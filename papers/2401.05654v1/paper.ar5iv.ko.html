<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.05654] Towards Conversational Diagnostic AI</title><meta property="og:description" content="At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable o…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Conversational Diagnostic AI">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Conversational Diagnostic AI">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.05654">

<!--Generated on Fri Feb  9 13:12:18 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Conversational Diagnostic AI</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tao Tu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anil Palepu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mike Schaekermann
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Khaled Saab
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jan Freyberg
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ryutaro Tanno
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amy Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Brenna Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohamed Amin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Nenad Tomasev
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shekoofeh Azizi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Karan Singhal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yong Cheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Le Hou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Albert Webson
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Kavita Kulkarni
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">S. Sara Mahdavi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christopher Semturs
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Juraj Gottweis
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joelle Barral
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Katherine Chou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Greg S. Corrado
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yossi Matias
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Alan Karthikesalingam
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vivek Natarajan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="1.1"><span class="ltx_text" id="1.1.1" lang="en">의학의 핵심에는 의사-환자 대화가 있으며, 여기서 숙련된 이력 수집은 정확한 진단, 효과적인 관리 및 지속적인 신뢰의 길을 열어준다. 진단 대화가 가능한 인공지능(AI) 시스템은 접근성과 일관성, 돌봄의 질을 높일 수 있다. 그러나 임상의의 전문 지식을 근사화하는 것은 눈에 띄는 큰 도전이다. 여기서는 진단대화에 최적화된 LLM(Large Language Model) 기반 AI 시스템인 AMIE(Articulate Medical Intelligence Explorer)를 소개한다. </span></p>
<p class="ltx_p" id="2.2"><span class="ltx_text" id="2.2.1" lang="en">AMIE는 다양한 질병 조건, 전문 분야 및 컨텍스트에 걸쳐 스케일링 학습을 위한 자동화된 피드백 메커니즘과 함께 새로운 셀프 플레이 기반 시뮬레이션 환경을 사용합니다. 우리는 병력 작성, 진단 정확도, 관리 추론, 의사소통 기술 및 공감을 포함한 임상적으로 의미 있는 수행 축을 평가하기 위한 프레임워크를 설계했다. 우리는 AMIE의 성능을 OSE(Objective Structured Clinical Examination) 스타일로 검증된 환자 행위자와의 텍스트 기반 상담의 무작위 이중 맹검 교차 연구에서 1차 진료 의사(PCP)의 성능과 비교했다. 이 연구에는 캐나다, 영국, 인도의 임상 제공자의 149개 사례 시나리오, AMIE와의 비교를 위한 20개의 PCP, 전문 의사 및 환자 행위자의 평가가 포함되었다. AMIE는 전문의에 따르면 32개 축 중 28개, 환자 행위자에 따르면 26개 축 중 24개에서 더 큰 진단 정확도와 우수한 성능을 보여주었다. 우리의 연구는 몇 가지 한계를 가지고 있으며 적절한 주의를 기울여 해석해야 한다. 임상의는 대규모 LLM-환자 상호 작용을 허용하지만 일반적인 임상 실습을 대표하지 않는 낯선 동기식 문자 채팅으로 제한되었다. AMIE가 실제 환경으로 변환되기 전에 추가 연구가 필요하지만 결과는 대화형 진단 AI를 향한 이정표를 나타낸다. </span></p>
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">의사와 환자 사이의 대화는 효과적이고 온정적인 치료의 기본이다. 의학 인터뷰는 “의사가 사용할 수 있는 가장 강력하고 민감하며 가장 다재다능한 도구”로 명명되었다. 일부 설정에서는 진단의 60-80%가 임상 병력 기록 단독 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx2" title="">2</a>, <a class="ltx_ref" href="#bib.bibx3" title="">3</a>, <a class="ltx_ref" href="#bib.bibx4" title="">4</a>, <a class="ltx_ref" href="#bib.bibx5" title="">5</a>, <a class="ltx_ref" href="#bib.bibx6" title="">6</a>]</cite>를 통해 이루어진다고 여겨진다. 의사-환자 대화는 병력 청취 및 진단을 넘어 확장되며, 라포와 신뢰를 확립하고 건강 요구를 해결하기 위한 도구 역할을 하며 환자가 선호, 기대 및 우려를 설명하는 정보에 입각한 결정을 내릴 수 있도록 권한을 부여할 수 있다. 임상의는 임상 병력 청취와 광범위한 "진단 대화"에서 상당한 기술을 구사하지만, 이 전문 기술에 대한 접근은 에피소드적이고 전 세계적으로 부족하다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">범용 대용량 언어 모델(LLM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx9" title="">9</a>, <a class="ltx_ref" href="#biba.bibx2" title="">119</a>, <a class="ltx_ref" href="#bib.bibx11" title="">11</a>]</cite>의 최근 발전은 인공지능(AI) 시스템이 자연주의적 대화를 수행하기 위해 관련 컨텍스트를 계획, 추론 및 통합할 수 있는 능력을 가지고 있음을 보여주었다. 이러한 발전은 완전한 대화형 AI의 개발을 위해 의학에서 AI의 가능성을 다시 생각할 수 있는 기회를 제공한다. 이러한 의료 AI 시스템은 임상 언어를 이해하고 불확실성 하에서 지능적으로 정보를 획득하고 환자 및 환자를 돌보는 사람들과 자연스럽고 진단적으로 유용한 의료 대화에 참여할 것이다. 임상 및 진단 대화를 할 수 있는 AI 시스템의 잠재적 실제 유용성은 광범위하며, 이러한 기능의 개발은 진단 및 예후 전문지식에 대한 접근성을 개선하고, 치료의 품질, 일관성, 가용성 및 경제성을 개선하고, (특히 의료 불균형에 직면한 인구의 경우) 더 나은 건강 결과를 실현하는 데 도움이 될 수 있기 때문이다.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2401.05654/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="367" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.3.1.1" style="font-size:90%;">Figure 1</span>:</span><span class="ltx_text ltx_font_bold" id="S1.F1.4.2" style="font-size:90%;">Overview of contributions. <span class="ltx_text ltx_font_medium" id="S1.F1.4.2.1">AMIE is a conversationational medical AI optimised for diagnostic dialogue. AMIE는 다양한 의료 추론, 질문 응답 및 요약 데이터 세트와 함께 실제 및 시뮬레이션된 의료 대화의 조합으로 미세 조정된다. 특히, 우리는 다양한 의학적 맥락과 전문 분야에 걸쳐 AMIE의 능력을 확장하기 위해 자동화된 피드백 메커니즘을 가진 셀프 플레이 기반 시뮬레이션 대화 환경을 설계했다. 구체적으로, 이 반복적인 자기 개선 프로세스는 두 개의 자기 재생 루프, 즉 (1) AMIE가 AI 환자 에이전트와의 시뮬레이션된 대화에서 행동을 개선하기 위해 맥락 내 비평 피드백을 활용하는 "내부" 자기 재생 루프, (2) 정제된 시뮬레이션된 대화 세트가 후속 미세 조정 반복에 통합되는 "외부" 자기 재생 루프로 구성되었다. 온라인 추론 동안 AMIE는 각 대화 턴에서 환자에 대한 정확하고 근거 있는 답변에 도달하기 위해 현재 대화에 조건화된 응답을 점진적으로 정제하기 위해 연쇄 추론 전략을 사용했다. </span></span></figcaption>
We designed and conducted a blinded remote Objective Structured Clinical Examination (OSCE) with validated simulated patient actors interacting with AMIE or Primary Care Physicians (PCPs) via a text interface. Across multiple axes corresponding to both specialist physician (28 out of 32) and patient actor (24 out of 26) perspective, AMIE was rated as superior to PCPs while being non-inferior on the rest.</span></span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">그러나 LLM은 임상 지식을 인코딩하고 매우 정확한 단일 회전 의료 질문 답변 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx12" title="">12</a>, <a class="ltx_ref" href="#bib.bibx13" title="">13</a>, <a class="ltx_ref" href="#bib.bibx14" title="">14</a>]</cite>가 가능한 것으로 입증되었지만, 이들의 대화 능력은 임상 의학 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx15" title="">15</a>, <a class="ltx_ref" href="#bib.bibx16" title="">16</a>]</cite> 이외의 도메인에 맞게 조정되었다. 건강을 위한 LLM에 대한 이전 작업 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx12" title="">12</a>, <a class="ltx_ref" href="#bib.bibx13" title="">13</a>, <a class="ltx_ref" href="#bib.bibx14" title="">14</a>, <a class="ltx_ref" href="#bib.bibx17" title="">17</a>, <a class="ltx_ref" href="#bib.bibx18" title="">18</a>]</cite>는 아직 AI 시스템의 임상 이력 작성 및 진단 대화 능력을 엄격하게 조사하거나 전문 임상의의 광범위한 능력과 비교하여 맥락화하지 않았다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">임상의가 진단 및 관리 계획을 도출하는 임상 병력 작성 및 진단 대화는 상황에 따라 최적의 행동이 크게 좌우되는 복잡한 기술<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx19" title="">19</a>]</cite>를 나타낸다. 따라서 진단 대화의 질을 평가하기 위해서는 도출된 이력의 구조와 완전성, 진단 정확도, 관리 계획의 적절성 및 그 근거, 관계 구축, 개인에 대한 존중 및 의사소통 효능과 같은 환자 중심 고려 사항 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx20" title="">20</a>]</cite>와 같은 여러 평가 축이 필요하다. LLM의 대화 가능성이 의학에서 실현되려면 임상의와 환자 사이의 역사 촬영 및 진단 대화에만 고유한 특성과 같은 특성에 대한 의료 AI 시스템의 개발 및 평가를 더 잘 최적화해야 할 상당한 충족되지 않은 필요성이 있다.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">이 작업에서는 임상 병력 작성 및 진단 추론을 위한 대화형 의료 AI 시스템으로의 진행 상황을 자세히 설명한다.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p" id="S1.p6.1">우리의 주요 기여는 다음과 같이 요약된다.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i1.p1.1">임상 병력 청취와 진단 대화에 최적화된 LLM 기반 AI 시스템인 AMIE(Articulate Medical Intelligence Explorer)를 선보였다.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i2.p1.1">다양한 전문 분야와 시나리오에 걸쳐 AMIE를 확장하기 위해 학습 프로세스를 강화하고 가속화하기 위해 자동화된 피드백 메커니즘과 함께 새로운 자체 플레이 기반 시뮬레이션 진단 대화 환경을 개발했다. 또한 AMIE의 진단 정확도와 대화 품질을 향상시키기 위해 추론 시간 연쇄 추론 전략을 도입했다.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i3.p1.1">우리는 임상의 중심 및 환자 중심 메트릭을 모두 포함하는 진단 대화형 의료 AI의 이력 작성, 진단 추론, 의사소통 기술 및 공감을 평가하기 위한 파일럿 평가 루브릭을 개발했다.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i4.p1.1">우리는 캐나다, 영국 및 인도의 임상 제공자로부터 149개의 사례 시나리오를 사용하여 블라인드 원격 OSCE 연구를 설계하고 수행하여 검증된 환자 행위자와 상담을 수행할 때 AMIE와 PCP의 무작위 및 균형 비교를 가능하게 했다. AMIE는 다양한 측정(예: 차등 진단 목록의 top-1 및 top-3 정확도)에 의해 평가된 PCP에 비해 우수한 진단 정확도를 나타냈다. 전문의 관점에서는 32개의 평가 축 중 28개, 환자 행위자 관점에서는 26개의 평가 축 중 24개에서 AMIE가 PCP보다 우수하면서도 나머지는 비열등하다는 평가를 받았다.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i5.p1.1">우리는 AMIE의 기능을 더 이해하고 특성화하기 위해 다양한 절제술을 수행하고 중요한 한계를 강조했으며 AMIE의 실제 임상 번역을 위한 주요 다음 단계를 제안했다.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para">
<p class="ltx_p" id="S1.p7.1">우리의 연구는 특히 진단 대화에 특화된 LLM과 환자 사이의 잠재적으로 대규모 상호 작용을 가능하게 하지만 원격 상담을 위한 PCP에는 익숙하지 않은 텍스트 채팅 인터페이스를 활용했다는 중요한 한계를 가지고 있다. 따라서 우리의 연구는 (원격)의료에서 일반적인 관행의 대표자로 간주되어서는 안 된다.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2401.05654/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.3.1.1" style="font-size:90%;">Figure 2</span>:</span><span class="ltx_text ltx_font_bold" id="S1.F2.4.2" style="font-size:90%;">Overview of randomized study design. <span class="ltx_text ltx_font_medium" id="S1.F2.4.2.1"> A primary care physician (PCP) 및 AMIE는 온라인 멀티턴 동기 텍스트 채팅을 통해 시뮬레이션된 환자들과 함께 가상 원격 Objective Structured Clinical Examination (OSCE)를 (무작위 순서로) 수행하고 사후 질문에 대한 답변을 생성한다. 그런 다음 PCP와 AMIE는 환자 행위자와 전문 의사 모두에 의해 평가된다. </span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>AMIE: An LLM based AI System for Diagnostic Dialogue</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p" id="S2.p1.1">다음 섹션에서는 진단 대화 기능 및 임상 의사 소통 기술을 위해 AMIE를 최적화하도록 설계된 실제 데이터 세트, 시뮬레이션된 셀프 플레이 환경, 미세 조정 프로세스 및 추론 시간 연쇄 추론에 대해 설명한다.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Real-world Datasets for AMIE</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p1.1">AMIE는 객관식 의료 질문 응답, 전문가 맞춤형 장기 형태 의료 추론, 전자 건강 기록(EHR) 노트 요약 및 대규모 전사 의료 대화 상호 작용을 포함한 다양한 실제 데이터 세트를 사용하여 개발되었다. 아래에서 자세히 설명하는 바와 같이, AMIE를 위한 훈련 과제 혼합은 대화 생성 과제 외에 의학 질문-답변, 추론, 요약 과제로 구성되었다.</p>
</div>
<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Medical Reasoning.</h5>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">미국 의료 면허 시험(USMLE) 객관식 개방형 도메인 질문으로 구성된 MedQA(객관식) 데이터 세트를 사용하여 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx21" title="">21</a>]</cite> 4개 또는 5개의 가능한 답변을 사용했다. 훈련 세트는 11,450개의 문항으로 구성되었고 테스트 세트는 1,273개의 문항으로 구성되었다. 또한 임상 전문가가 단계별 추론을 조작하여 정답 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx13" title="">13</a>]</cite>로 이어지는 훈련 세트에서 191개의 MedQA 질문을 선별했다.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Long-form Medical Question Answering.</h5>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">여기에 사용된 데이터 세트는 MultiMedBench<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx12" title="">12</a>]</cite>에서 HealthSearchQA, LiveQA 및 Medication QA의 64개 질문에 대한 전문가 조작된 긴 형식 응답으로 구성되었다.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Medical Summarization.</h5>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">IMIC-III의 65개의 임상 의사가 작성한 의료 노트 요약으로 구성된 데이터 세트, 중환자실 환자의 의료 기록을 포함하는 대규모 공개 데이터베이스 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx22" title="">22</a>]</cite>가 AMIE의 추가 훈련 데이터로 사용되었다. IMIC-III에는 심장학, 호흡기, 방사선학, 의사, 일반, 퇴원, 사례 관리, 상담, 간호, 약국, 영양, 재활 및 사회 사업을 포함한 13가지 유형에 걸쳐 약 200만 개의 메모가 포함되어 있다. 각 범주에서 5개의 노트가 선택되었으며 최소 총 길이는 400 토큰이고 환자당 최소 1개의 간호 노트가 선택되었다. 임상의는 개별 의료 노트의 추상적 요약을 작성하도록 지시받았고 주요 정보를 캡처하는 동시에 원본 노트에 존재하지 않는 새로운 정보 및 문장 포함을 허용했다.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Real-world Dialogue.</h5>

<div id="S2.SS1.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px4.p1.2">여기에서 우리는 미국에서 10년 동안 1,000명 이상의 임상의가 직접 임상 방문을 하는 동안 의료 대화의 98,919개의 오디오 녹취록으로 구성된 대화 연구 조직에서 허가된 비식별화된 데이터 세트를 사용했다. 51개의 의료 전문 분야(1차 진료, 류마티스, 혈액학, 종양학, 내과 및 정신과 등)와 168개의 의료 조건과 방문 이유(제2형 당뇨병, 류마티스 관절염, 천식, 우울증 등)를 다뤘다. 오디오 녹취록에는 의사, 환자 및 간호사와 같은 다양한 화자 역할의 발화가 포함되어 있다. 평균 149.8회의 회화가 있었다 (<math alttext="P_{0.25}=75.0" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.1.m1.1"><semantics id="S2.SS1.SSS0.Px4.p1.1.m1.1a"><mrow id="S2.SS1.SSS0.Px4.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.cmml"><msub id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml"><mi id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2.cmml">P</mi><mn id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3.cmml">0.25</mn></msub><mo id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml">75.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px4.p1.1.m1.1b"><apply id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1"><eq id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1"></eq><apply id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2">𝑃</ci><cn id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3.cmml" type="float" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3">0.25</cn></apply><cn id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml" type="float" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3">75.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px4.p1.1.m1.1c">P_{0.25}=75.0</annotation></semantics></math>, <math alttext="P_{0.75}=196.0" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px4.p1.2.m2.1"><semantics id="S2.SS1.SSS0.Px4.p1.2.m2.1a"><mrow id="S2.SS1.SSS0.Px4.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.cmml"><msub id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml"><mi id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2.cmml">P</mi><mn id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3.cmml">0.75</mn></msub><mo id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml">196.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px4.p1.2.m2.1b"><apply id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1"><eq id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1"></eq><apply id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2">𝑃</ci><cn id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3.cmml" type="float" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3">0.75</cn></apply><cn id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml" type="float" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3">196.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px4.p1.2.m2.1c">P_{0.75}=196.0</annotation></semantics></math>). 각 대화에 대해 메타데이터는 환자 인구 통계, 방문 이유(기존 상태에 대한 후속 조치, 급성 요구, 연간 검사 등), 진단 유형(신규, 기존 또는 기타 관련 없음)에 대한 정보를 포함했다. 자세한 내용은 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx23" title="">23</a>]</cite>를 참조한다.</p>
</div>
<div id="S2.SS1.SSS0.Px4.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px4.p2.1">이 연구를 위해 의사와 환자만 참여하는 대화를 선택했지만 간호사와 같은 다른 역할은 선택하지 않았다. 전처리 과정에서 전사체에서 “[LAUGHING]” 및 “[INAUDIBLE]”와 같은 파라버벌 주석을 제거했다. 그런 다음 상태 범주 및 방문 이유에 따라 계층화된 샘플링을 사용하여 데이터 세트를 훈련(90%) 및 검증(10%) 세트로 나누어 훈련을 위한 대화 89,027개, 검증을 위한 대화 9,892개를 생성했다.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Simulated Dialogue Learning Environment and Self-play for AMIE</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.p1.1">직접 임상 방문에서 실제 대화 내용을 수동적으로 수집하고 전사하는 것은 실현 가능하지만, 의료 대화를 위한 LLM을 훈련하는 데 있어 두 가지 실질적인 과제는 효과를 제한한다: (1) 기존 실제 데이터는 종종 광범위한 의료 조건과 시나리오를 캡처하지 못하여 확장성과 포괄성을 방해하고; (2) 실제 대화 녹취록에서 파생된 데이터는 모호한 언어(속어, 전문 용어 및 빈정거림 포함), 중단, 비문법적 발화 및 암묵적 참조를 포함하는 시끄러운 경향이 있다. 이는 결국 AMIE의 지식, 능력 및 적용 가능성을 제한할 수 있다.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS2.p2.1">이러한 한계를 해결하기 위해 가상 진료 환경에서 진단 의료 대화를 위한 셀프 플레이 기반 시뮬레이션 학습 환경을 설계하여 다양한 의료 조건과 컨텍스트에 걸쳐 AMIE의 지식과 기능을 확장할 수 있도록 했다. 이 환경을 사용하여 위에서 설명한 의료 QA, 추론, 요약 및 실제 대화 데이터의 정적 코퍼스 외에 진화하는 시뮬레이션 대화 세트를 사용하여 AMIE를 반복적으로 미세 조정했다(<a class="ltx_ref" href="#S1.F1" title="In 1 Introduction ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> 참조).</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS2.p3.1">이 과정은 두 개의 자기 재생 고리로 구성되었다:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">An "inner" self-play loop</span> where AMIE leveraged in-context critic feedback to refine its behavior on the simulated conversations on a AI patient agent.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">An "outer" self-play loop</span> where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. 그런 다음 AMIE의 새로운 버전은 내부 루프에 다시 참여하여 지속적인 학습 주기를 만들 수 있다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Simulated Dialogues.</h5>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">미세 조정을 반복할 때마다 5,230개의 다른 의료 조건에서 비롯된 11,686개의 대화를 생성했다. 조건은 3개의 데이터 세트에서 선택되었습니다.</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="S2.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i1.p1.1.1">Health QA dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx12" title="">12</a>]</cite> which contained 613 common medical conditions.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="S2.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i2.p1.1.1">MalaCards Human Disease Database<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnote1.1.1.1">1</span></span><a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/malacards-diseases.json" target="_blank" title="">https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/malacards-diseases.json</a></span></span></span></span> which contained 18,455 less common disease conditions.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="S2.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i3.p1.1.1">MedicineNet Diseases &amp; Conditions Index<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnote2.1.1.1">2</span></span><a class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium" href="https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/medicinenet-diseases.json" target="_blank" title="">https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/medicinenet-diseases.json</a></span></span></span></span> which contained 4,617 less common conditions.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p2.2">각 셀프 플레이 반복에서 613개의 공통 조건 각각에서 4개의 대화가 생성된 반면, 메디신넷 및 말라카드에서 무작위로 선택된 4,617개의 덜 공통 조건 각각에서 2개의 대화가 생성되었다. 시뮬레이션된 평균 대화 대화 길이는 21.28 턴(<math alttext="P_{0.25}=19.0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.1.m1.1"><semantics id="S2.SS2.SSS0.Px1.p2.1.m1.1a"><mrow id="S2.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml"><msub id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml"><mi id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2.cmml">P</mi><mn id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3.cmml">0.25</mn></msub><mo id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml">19.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1"><eq id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1"></eq><apply id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2">𝑃</ci><cn id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3.cmml" type="float" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3">0.25</cn></apply><cn id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml" type="float" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3">19.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.1.m1.1c">P_{0.25}=19.0</annotation></semantics></math>, <math alttext="P_{0.75}=25.0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p2.2.m2.1"><semantics id="S2.SS2.SSS0.Px1.p2.2.m2.1a"><mrow id="S2.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml"><msub id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml"><mi id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2.cmml">P</mi><mn id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3.cmml">0.75</mn></msub><mo id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml">25.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.2.m2.1b"><apply id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1"><eq id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1"></eq><apply id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.1.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2">𝑃</ci><cn id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3.cmml" type="float" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3">0.75</cn></apply><cn id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml" type="float" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3">25.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.2.m2.1c">P_{0.75}=25.0</annotation></semantics></math>)이었다.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p3.1">시뮬레이션된 대화를 사용하여 고품질의 라벨이 붙은 실제 대화 데이터의 제한된 가용성을 해결하고 다양한 의료 컨텍스트에 대한 모델의 일반화 및 적응성을 개선할 수 있었다. 이러한 셀프 플레이 패러다임을 활용하여 AMIE는 환자 상호 작용 동안 대화 및 진단 기능을 지속적으로 배우고 개선할 수 있다.</p>
</div>
</section>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Simulated Dialogue Data Curation</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">대규모의 고품질 시뮬레이션 대화를 생성하기 위해 우리는 세 가지 핵심 구성 요소로 구성된 새로운 다중 에이전트 프레임워크를 개발했다.</p>
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p class="ltx_p" id="S2.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i1.p1.1.1">Vignette Generator</span>: AMIE는 웹 검색을 활용하여 특정 의료 조건이 주어진 고유한 환자 vignettes를 공예합니다.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p class="ltx_p" id="S2.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i2.p1.1.1">Simulated Dialogue Generator</span>: 3개의 LLM 에이전트는 환자 에이전트, 의사 에이전트 및 중재자의 역할을 수행하여 현실적인 진단 상호 작용을 시뮬레이션하는 턴 바이 턴 대사에 참여한다.</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i3.p1" class="ltx_para">
<p class="ltx_p" id="S2.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i3.p1.1.1">Self-play Critic</span>: A fourth LLM 에이전트는 자기계발을 위해 의사 에이전트에 피드백을 주는 비평가 역할을 한다. 특히, AMIE는 이 프레임워크에서 모든 에이전트 역할을 했다. 아래에서는 각 구성 요소에 대해 자세히 설명합니다.</p>
</div>
</li>
</ul>
</div>
<section id="S2.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Vignette Generator.</h5>

<div id="S2.SS2.SSS1.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p1.1">비네트 생성기는 규모에서 다양하고 현실적인 환자 시나리오를 만드는 것을 목표로 했으며, 이는 후속적으로 시뮬레이션된 의사-환자 대화 생성을 위한 컨텍스트로 사용되어 AMIE가 더 많은 수의 조건 및 환자 배경에 대한 노출을 모방하는 훈련 프로세스를 겪을 수 있다. 환자 비네트(시나리오)에는 환자 인구 통계, 증상, 과거 병력, 과거 수술 이력, 과거 사회 이력 및 환자 질문과 같은 필수 배경 정보와 관련 진단 및 관리 계획이 포함되었다.</p>
</div>
<div id="S2.SS2.SSS1.Px1.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p2.1">주어진 조건에 대해 다음 과정을 사용하여 환자 비네트를 구성했다. 먼저 인터넷 검색엔진을 이용하여 인구통계, 증상, 상태와 관련된 관리계획의 범위에 대한 60개의 구절(각각 20개)을 검색하였다. 이러한 계대가 주어진 조건과 관련이 있는지 확인하기 위해 범용 LLM, PaLM-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#biba.bibx2" title="">119</a>]</cite>를 사용하여 이러한 검색된 계대를 필터링하여 주어진 조건과 관련이 없다고 간주되는 계대를 제거했다. 그런 다음 AMIE가 특정 비네트 형식을 강제하기 위한 원샷 예제를 제공하여 필터링된 패시지에서 검색된 인구통계, 증상 및 관리 계획과 정렬된 그럴듯한 환자 비네트를 생성하도록 유도했다. 이러한 각 단계에 대한 프롬프트는 다음과 같습니다.</p>
</div>
<div id="S2.SS2.SSS1.Px1.p3" class="ltx_para ltx_noindent">
<svg id="S2.SS2.SSS1.Px1.p3.pic1" class="ltx_picture" height="340.28" overflow="visible" version="1.1" width="600"><g transform="translate(0,340.28) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 334.37 C 0 337.63 2.64 340.28 5.91 340.28 L 594.09 340.28 C 597.36 340.28 600 337.63 600 334.37 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 334.37 C 1.97 336.54 3.73 338.31 5.91 338.31 L 594.09 338.31 C 596.27 338.31 598.03 336.54 598.03 334.37 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="312.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.1" class="ltx_p"><span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Search Retrieval Template
<br class="ltx_break"></span>What are the specific patient demographics/symptoms/management plan for the condition [Condition]?</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.2" class="ltx_p"><span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Passage Filtering Template
<br class="ltx_break"></span>For the clinical condition, [Condition], is the following a good description of common demographics/symptoms/management plans (Yes/No)?</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.3" class="ltx_p">Description: [Retrieved Passage]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.4" class="ltx_p">Answer (Yes/No):</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.5" class="ltx_p"><span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Vignette Generation Template
<br class="ltx_break"></span>The following are several passages about the demographics, symptoms, and management plan for a given condition. Generate 2 different patient vignettes consistent with these passages. Follow the format of the given example (just list N/A if a particular field is unavailable).</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.6" class="ltx_p">Condition: [Condition]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.7" class="ltx_p">Demographic Passages: [Retrieved Demographic Passages]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.8" class="ltx_p">Symptoms Passages: [Retrieved Symptom Passages]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.9" class="ltx_p">Management Plan Passages: [Retrieved Management Plan Passages]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.10" class="ltx_p">Example Format: [Oneshot example]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.11" class="ltx_p">Patient Vignettes for [Condition]:</span>
</span></foreignObject></g></g></svg>
</div>
</section>
<section id="S2.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Simulated Dialogue Generator.</h5>

<div id="S2.SS2.SSS1.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p1.1">특정 의학적 상태를 자세히 설명하는 환자 비네트(patient vignette)가 주어지면, 시뮬레이션 대화 생성기는 대면 신체 검사가 가능하지 않을 수 있는 온라인 채팅 환경에서 환자와 의사 사이의 현실적인 대화를 시뮬레이션하도록 설계되었다.</p>
</div>
<div id="S2.SS2.SSS1.Px2.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p2.1">AMIE가 각각 수행하는 3개의 특정 LLM 에이전트(환자 에이전트, 의사 에이전트 및 중재자)는 시뮬레이션된 대화를 생성하기 위해 서로 통신하는 작업을 수행했다. 각 에이전트는 서로 다른 지침을 가지고 있었습니다. 환자 에이전트는 비녯에 요약된 의학적 상태를 경험하는 개인을 구체화했다. 그들의 역할은 의사 요원의 질문에 정직하게 응답하는 것뿐만 아니라 그들이 가질 수 있는 추가 질문이나 우려를 제기하는 것과 관련이 있다. 의사 에이전트는 온라인 채팅 환경 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx24" title="">24</a>]</cite>에서 환자의 병력을 이해하려는 공감적 임상의 역할을 하였다. 그들의 목표는 환자의 증상과 배경을 효과적으로 드러낼 수 있는 질문을 공식화하여 정확한 진단과 효과적인 치료 계획으로 이어지는 것이었다. 중재자는 환자 에이전트와 의사 에이전트 사이의 진행 중인 대화를 지속적으로 평가하여 대화가 언제 자연스러운 결론에 도달했는지 결정했다.</p>
</div>
<div id="S2.SS2.SSS1.Px2.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p3.1">턴바이턴 대화 시뮬레이션은 의사 에이전트가 대화를 시작하는 것으로 시작되었습니다. "의사: 그래서, 오늘 무엇을 도와드릴까요?" 그 후, 환자 에이전트가 응답했고, 그들의 대답은 진행 중인 대화 이력에 통합되었다. 이어서, 의사 에이전트는 업데이트된 대화 이력에 기초하여 응답을 공식화하였다. 그런 다음 이 응답이 대화 기록에 추가되었습니다. 사회자가 대화가 자연스러운 결론에 도달했음을 감지할 때까지 대화는 의사 에이전트가 차등 진단, 치료 계획을 제공하고 나머지 환자 에이전트 질문을 적절하게 해결하거나 에이전트 중 하나가 이별을 시작할 때까지 진행되었다.</p>
</div>
<div id="S2.SS2.SSS1.Px2.p4" class="ltx_para ltx_noindent">
<svg id="S2.SS2.SSS1.Px2.p4.pic1" class="ltx_picture" height="261.48" overflow="visible" version="1.1" width="600"><g transform="translate(0,261.48) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 255.58 C 0 258.84 2.64 261.48 5.91 261.48 L 594.09 261.48 C 597.36 261.48 600 258.84 600 255.58 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 255.58 C 1.97 257.75 3.73 259.51 5.91 259.51 L 594.09 259.51 C 596.27 259.51 598.03 257.75 598.03 255.58 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="233.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.1" class="ltx_p"><span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Patient Agent Instruction:
<br class="ltx_break"></span>You are a patient chatting with a doctor over an online chat interface. The doctor has never met you before.
&lt;patient vignette&gt;
Respond to the doctor’s questions honestly as they interview you, asking any questions that may come up.</span>
<span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.2" class="ltx_p"><span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Doctor Agent Instruction:
<br class="ltx_break"></span>You are an empathetic clinician asking a patient about their medical history over an online chat interface. You know nothing about the patient in advance.
Respond to the patient with a single-turn response to better understand their history and symptoms. Do not ask more than two questions. If the patient asks a question, be sure to answer it appropriately.</span>
<span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.3" class="ltx_p"><span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Moderator Instruction:
<br class="ltx_break"></span>The following is a conversation between a doctor and a patient:
&lt;dialog&gt; 
The conversation should only come to an end if the doctor has finished giving the patient a diagnosis and treatment plan and the patient has no questions left. A conversation also comes to an end if the doctor or patient says goodbye.
Question: has the conversation come to an end? Yes or No.</span>
</span></foreignObject></g></g></svg>
</div>
</section>
<section id="S2.SS2.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Self-play Critic.</h5>

<div id="S2.SS2.SSS1.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p1.1">고품질 대화를 보장하기 위해 진단 대화의 자기 향상을 위한 맞춤형 셀프 플레이 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx25" title="">25</a>]</cite> 프레임워크를 구현했다. 이 프레임워크는 AMIE가 연기하고 그라운드 진실 진단을 인식하는 "비평가" 역할을 하는 네 번째 LLM 에이전트를 도입하여 의사 에이전트에게 상황 내 피드백을 제공하고 후속 대화에서 성능을 향상시켰다. 비평가 대리인은 의사 대리인의 반응을 다음과 같은 기준으로 평가하였다.</p>
<ul id="S2.I4" class="ltx_itemize">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i1.p1" class="ltx_para">
<p class="ltx_p" id="S2.I4.i1.p1.1">의사 에이전트는 환자 에이전트의 최신 질문이나 의견을 간결하게 다루면서 공감과 전문성을 발휘한다.</p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i2.p1" class="ltx_para">
<p class="ltx_p" id="S2.I4.i2.p1.1">의사 에이전트는 응답당 최대 1~2개에 초점을 맞추어 너무 많거나 반복적인 질문(이미 습득한 정보에 대한)을 피한다.</p>
</div>
</li>
<li id="S2.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i3.p1" class="ltx_para">
<p class="ltx_p" id="S2.I4.i3.p1.1">의사 에이전트가 AI 챗봇임을 밝혀서는 안 된다. 자연스럽게 흐르고 사실적 정확성을 유지하며 환자의 추가 참여를 촉진해야 한다.</p>
</div>
</li>
<li id="S2.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i4.p1" class="ltx_para">
<p class="ltx_p" id="S2.I4.i4.p1.1">의사 에이전트는 가장 가능성이 높은 감별 진단 중 적어도 두 가지를 식별하기에 충분한 질문을 한다. 그들은 그라운드 트루스 진단에 대한 표적 질문을 통해 이해를 더욱 구체화하고 그에 상응하는 치료를 제공한다.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.SSS1.Px3.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p2.1">비평가의 피드백에 따라 의사 에이전트는 동일한 환자 에이전트와의 후속 대화에서 반응을 개선하기 위해 제안을 처음부터 통합했다. 특히, 의사 요원은 매번 새로운 라운드에서 이전 대화 기록에 대한 액세스를 유지했다. 이러한 자기계발 과정을 두 번 반복하여 미세조정의 각 반복에 사용되는 대화문을 생성하였다.</p>
</div>
</section>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Instruction Fine-tuning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS3.p1.1">AMIE는 기본 LLM PaLM 2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#biba.bibx2" title="">119</a>]</cite>를 기반으로 의료 대화 및 추론 능력을 향상시키기 위해 미세 조정되었다. 기본 LLM 아키텍처에 대한 자세한 내용은 PaLM-2 기술 보고서를 참조한다.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS3.p2.1">우리는 의료 대화 내에서 환자 또는 의사 역할을 수행하고 의료 질문 응답 및 추론을 수행하고 EHR 노트를 요약할 때 AMIE를 미세 조정하기 위해 작업별 지침을 사용했다. 기본 LLM으로부터의 미세 조정의 첫 번째 라운드는 정적 데이터 세트만 사용했지만, 미세 조정의 후속 라운드는 <a class="ltx_ref" href="#S2.SS2.SSS1" title="2.2.1 Simulated Dialogue Data Curation ‣ 2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2.1</span></a>에 설명된 대로 자체 재생 내부 루프를 통해 생성된 시뮬레이션된 대화 상자를 활용했다.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS3.p3.1">대화 생성 작업의 경우 AMIE는 의사 또는 환자 역할을 가정하여 이전의 모든 상호 작용을 기반으로 다음 대화 순서를 예측하도록 훈련되었다. 환자 에이전트를 재생할 때 AMIE는 환자 시나리오에서 제공된 정보에 따라 증상에 대한 의사 에이전트의 질문에 답하라는 메시지가 표시되었다. 이러한 시나리오에는 시뮬레이션된 대화 또는 실제 대화 데이터 세트에 대한 인구 통계, 방문 이유 및 진단 유형과 같은 메타데이터에 대한 환자 vignettes(<a class="ltx_ref" href="#S2.SS2.SSS1.Px1" title="Vignette Generator. ‣ 2.2.1 Simulated Dialogue Data Curation ‣ 2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‣ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2.1</span></a> 참조)가 포함되었다. 의사 에이전트 역할에서 AMIE는 감정이입 임상의 역할을 하도록 자극되어 환자의 병력과 증상에 대해 환자를 인터뷰하여 궁극적으로 정확한 진단에 도달했다. 각 대화에서 우리는 목표 회전까지의 대화를 기반으로 예측하기 위해 목표 회전으로 의사 및 환자 역할 각각에 대해 평균 3턴을 샘플링했다. 대상 회전은 최소 30자 길이의 대화에서 모든 회전에서 무작위로 샘플링되었다.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p class="ltx_p" id="S2.SS3.p4.1">유사하게, EHR 노트 요약 작업의 경우 AMIE에 임상 노트가 제공되었고 노트의 요약을 생성하도록 프롬프트되었다. 의료 추론/QA 및 긴 형태 응답 생성 작업은 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx13" title="">13</a>]</cite>에서와 동일한 설정을 따랐다. 특히, 대화 생성 및 롱폼 응답 생성을 제외한 모든 태스크는 추가 컨텍스트에 대한 태스크-특정 명령 외에 소수의 샷(1-5) 예제를 통합했다.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Chain-of-reasoning for Online Inference</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS4.p1.1">진단 대화의 핵심 과제를 해결하기 위해 - 환자와 긍정적인 관계를 유지하면서 진단 정확성과 자신감을 향상시키기 위해 불확실성 하에서 정보를 효과적으로 획득함 - AMIE는 각 대화 전환에서 응답을 생성하기 전에 연쇄 추론 전략을 사용했다. 여기서, "추론 체인"은 일련의 순차적 모델 호출들을 지칭하며, 각각은 이전 단계들의 출력들에 의존한다. 구체적으로, 다음과 같이 설명된 3단계 추론 프로세스를 사용했다:</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<ol id="S2.I5" class="ltx_enumerate">
<li id="S2.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I5.i1.p1" class="ltx_para">
<p class="ltx_p" id="S2.I5.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I5.i1.p1.1.1">Analyzing patient information:</span> Given the current conversation history, AMIE was instructed to 1) summarize the positive and negative symptoms as any relevant medical/family/social history and demographic information, 2) produce a current differential diagnosis, 3) necessary missing information for the more accurate diagnosis, 4) assess confidence of the current differential and highlight its urgency.</p>
</div>
</li>
<li id="S2.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I5.i2.p1" class="ltx_para">
<p class="ltx_p" id="S2.I5.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I5.i2.p1.1.1">Formulating response and action:</span> Building on the conversation history and output of the step 1, AMIE performed the following: 1) Generate a response to the patient's last message and formulate further questions to acquire missing information and refine the differential diagnosis. 2) 필요한 경우 응급실 방문 등 즉각적인 조치를 권고한다. 사용 가능한 정보를 기반으로 진단을 확신하는 경우 감별을 제시합니다.</p>
</div>
</li>
<li id="S2.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I5.i3.p1" class="ltx_para">
<p class="ltx_p" id="S2.I5.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I5.i3.p1.1.1">Refining the response:</span> AMIE는 대화 이력과 이전 단계의 출력을 기반으로 특정 기준을 충족하도록 이전 출력을 수정합니다. 기준은 주로 사실성 및 응답의 포맷팅(예를 들어, 환자 사실 및 불필요한 반복에 대한 사실적 부정확성을 피하고, 공감을 보여주고, 명확한 포맷으로 표시)과 관련된다.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS4.p3.1">이 추론 연쇄 전략을 통해 AMIE는 현재 대화에 대해 조건화된 응답을 점진적으로 정제하여 정보에 입각하고 근거 있는 응답에 도달할 수 있었다.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">임상 대화를 위한 모델을 개발하기 위한 선행 연구들은 노트 대 대화 또는 대화 대 노트 세대 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx26" title="">26</a>, <a class="ltx_ref" href="#bib.bibx27" title="">27</a>]</cite>와 같은 메트릭이나 상담의 임상적 품질을 포착하지 못하는 BLEU 또는 ROUGE 점수 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx28" title="">28</a>, <a class="ltx_ref" href="#bib.bibx29" title="">29</a>]</cite>와 같은 자연어 생성 메트릭에 초점을 맞추었다.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p" id="S3.p2.1">이러한 이전 작업과 대조적으로 우리는 상담에서 의사 소통 기술을 포함하여 역사 촬영에 대한 의사의 전문 지식의 품질을 평가하는 데 더 일반적으로 사용되는 기준에 인간 평가를 고정하려고 했다. 의료 인터뷰에서 환자 중심 커뮤니케이션(PCCBP)에 대한 모범 사례에 대한 합의 검토에 발표된 원칙에서 프레임워크를 도출했다. 우리는 영국, 캐나다, 미국 및 인도에 기반을 둔 임상의 및 OSCE 검사관과의 인터뷰 및 포커스 그룹을 사용하여 포함 항목을 개선하고 평가를 위한 파일럿 척도 및 지침을 도출하기 위해 이러한 기준을 반복했다. 우리의 결과 파일럿 프레임워크는 임상의(보드 인증 의사)와 일반 평가자(환자 행위자)의 두 가지 관점에서 평가를 가능하게 했다. 프레임워크에는 상담 품질, 구조 및 완전성, 인터뷰어의 역할, 책임 및 기술(테이블 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:gmcpq_rubric_details</span>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:pccbp_rubric_details</span>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:diagnosis_management_rubric_details</span>)에 대한 고려가 포함되었다.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Objective Structured Clinical Examination</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p1.1">Objective Structured Clinical Examination (OSCE)는 표준화되고 객관적인 패션 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx31" title="">31</a>, <a class="ltx_ref" href="#bib.bibx32" title="">32</a>, <a class="ltx_ref" href="#bib.bibx33" title="">33</a>]</cite>에서 임상 기술과 역량을 평가하기 위해 의료에서 사용되는 실용적인 평가 형식이다. 주로 이론적 지식에 초점을 맞추고 대신 실제 임상 실습의 기술을 평가할 수 있는 환경을 제공하는 것을 목표로 하는 전통적인 필기 또는 구술 시험과 다르다.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p2.1">OSCE는 일반적으로 여러 스테이션(종종 8-12)으로 나뉘며, 각각은 미리 정의된 시나리오 설명에 기초하여 특정 증상 또는 상태를 묘사하도록 훈련된 표준화된 환자 행위자에 의해 제정된 실제 임상 시나리오를 시뮬레이션한다. 각 역에서 학생들은 임상 병력 청취나 진단 등 수행해야 할 구체적인 과제를 부여받는다. 각 스테이션에는 정해진 시간 제한이 있어 공정성과 효율적인 평가가 보장됩니다. 훈련된 검사자들은 미리 정의된 체크리스트 또는 마킹 체계를 사용하여 각 스테이션에서 학생들의 수행을 관찰한다. 의사소통, 이력 작성, 신체 검사 기술, 임상 추론 및 의사 결정과 같은 임상 기술을 평가한다.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Remote OSCE Study Design</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p1.1">AMIE의 성능을 실제 임상의의 성능과 비교하기 위해 원격 OSCE 스타일의 블라인드 상담에 대한 무작위 교차 연구를 수행했다. 우리의 OSCE 연구는 온라인 텍스트 기반 상담에 참여하기 위해 인도와 캐나다에서 각각 10명씩 20명의 이사회 인증 1차 진료 의사(PCP)와 20명의 검증된 환자 행위자를 포함했다. PCP는 거주 후 3년에서 25년(중앙값 7년)의 경험을 가지고 있었다. 환자 행위자는 OSCE 참여 경험이 있는 의대생, 레지던트 및 간호사 실무자의 혼합으로 구성되었다. 우리는 인도(75), 캐나다(60), 영국(14)에서 149개의 시나리오 팩을 공급했다.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p2.1">우리 연구의 시나리오 팩과 시뮬레이션 환자는 각각 의대에 소속되어 OSCE 검사를 위한 시나리오 팩과 시뮬레이션 환자를 준비한 경험이 풍부한 두 개의 OSCE 실험실(캐나다와 인도에 각각 하나씩)에서 준비했다. 영국 시나리오 팩은 MRCPUK 웹사이트에 제공된 샘플에서 조달되었다. 각 시나리오 팩은 그라운드 트루스 진단 및 허용 가능한 진단 세트와 연관되었다. 시나리오 팩은 심혈관(29), 호흡기(30), 위장학(31), 신경학(30), 비뇨기과, 산부인과 영역(15) 및 내과(14)의 조건을 포함한다. 집중 치료 또는 입원 환자 사례 관리 시나리오와 마찬가지로 소아 또는 정신 의학 영역은 본 연구에서 제외되었다.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p3.1">인도 환자 배우들은 모든 인도 시나리오 팩과 14개의 영국 시나리오 팩 중 7개에서 역할을 맡았다. 캐나다 환자 행위자는 캐나다와 영국 기반 시나리오 팩의 나머지 절반 모두에 대한 시나리오 팩에 참여했다. 이 할당 프로세스는 149명의 별개의 시뮬레이션 환자("시나리오")를 초래했다. 아래에서는 "OSCE 에이전트"라는 용어를 사용하여 환자 행위자를 인터뷰하는 대화 상대, 즉 PCP 또는 AMIE를 나타낸다. <a class="ltx_ref" href="#S3.T1" title="In 3.2 Remote OSCE Study Design ‣ 3 Evaluation ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>는 세 지리적 위치에 걸친 OSCE 할당 정보를 요약한다. 149명의 시뮬레이션 환자 각각은 그림 <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">2</span></a>에 묘사된 3단계 연구 흐름을 완료했다.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.5.1.1" style="font-size:113%;">Table 1</span>:</span><span class="ltx_text ltx_font_bold" id="S3.T1.6.2" style="font-size:113%;">OSCE study summary. <span class="ltx_text ltx_font_medium" id="S3.T1.6.2.1">원격 OSCE 연구에서 세 위치(캐나다, 인도 및 영국) 각각에서 시나리오 팩, 환자 행위자, 시뮬레이션 환자 및 1차 진료 의사(PCP)의 수. 20명의 보드 인증 PCP가 인도와 캐나다에서 각각 10명씩 AMIE와 비교하여 OSCE 에이전트로 연구에 참여했다. 20명의 훈련된 환자 배우가 참여했으며 각각 10명이 인도와 캐나다에서 왔다. </span></span></figcaption>
Indian patient actors played the roles in both India and UK scenario packs. Canadian patient actors participated in scenario packs for both Canada and the UK. This process resulted in 149 distinct simulated patients.</span></span></figcaption>
<table id="S3.T1.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.7.1.1" class="ltx_tr">
<th id="S3.T1.7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S3.T1.7.1.1.1.1" class="ltx_text" style="font-size:80%;">Location</span></th>
<td id="S3.T1.7.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.7.1.1.2.1" class="ltx_text" style="font-size:80%;"># of Scenario Packs</span></td>
<td id="S3.T1.7.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.7.1.1.3.1" class="ltx_text" style="font-size:80%;"># of Simulated Patients</span></td>
<td id="S3.T1.7.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.7.1.1.4.1" class="ltx_text" style="font-size:80%;"># of Patient Actors</span></td>
<td id="S3.T1.7.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.7.1.1.5.1" class="ltx_text" style="font-size:80%;"># of PCPs</span></td>
</tr>
<tr id="S3.T1.7.2.2" class="ltx_tr">
<th id="S3.T1.7.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T1.7.2.2.1.1" class="ltx_text" style="font-size:80%;">Canada</span></th>
<td id="S3.T1.7.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.2.2.2.1" class="ltx_text" style="font-size:80%;">60</span></td>
<td id="S3.T1.7.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.2.2.3.1" class="ltx_text" style="font-size:80%;">67</span></td>
<td id="S3.T1.7.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.2.2.4.1" class="ltx_text" style="font-size:80%;">10</span></td>
<td id="S3.T1.7.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.2.2.5.1" class="ltx_text" style="font-size:80%;">10</span></td>
</tr>
<tr id="S3.T1.7.3.3" class="ltx_tr">
<th id="S3.T1.7.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T1.7.3.3.1.1" class="ltx_text" style="font-size:80%;">India</span></th>
<td id="S3.T1.7.3.3.2" class="ltx_td ltx_align_center"><span id="S3.T1.7.3.3.2.1" class="ltx_text" style="font-size:80%;">75</span></td>
<td id="S3.T1.7.3.3.3" class="ltx_td ltx_align_center"><span id="S3.T1.7.3.3.3.1" class="ltx_text" style="font-size:80%;">82</span></td>
<td id="S3.T1.7.3.3.4" class="ltx_td ltx_align_center"><span id="S3.T1.7.3.3.4.1" class="ltx_text" style="font-size:80%;">10</span></td>
<td id="S3.T1.7.3.3.5" class="ltx_td ltx_align_center"><span id="S3.T1.7.3.3.5.1" class="ltx_text" style="font-size:80%;">10</span></td>
</tr>
<tr id="S3.T1.7.4.4" class="ltx_tr">
<th id="S3.T1.7.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T1.7.4.4.1.1" class="ltx_text" style="font-size:80%;">UK</span></th>
<td id="S3.T1.7.4.4.2" class="ltx_td ltx_align_center"><span id="S3.T1.7.4.4.2.1" class="ltx_text" style="font-size:80%;">14</span></td>
<td id="S3.T1.7.4.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.7.4.4.3.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S3.T1.7.4.4.4" class="ltx_td ltx_align_center"><span id="S3.T1.7.4.4.4.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S3.T1.7.4.4.5" class="ltx_td ltx_align_center"><span id="S3.T1.7.4.4.5.1" class="ltx_text" style="font-size:80%;">0</span></td>
</tr>
<tr id="S3.T1.7.5.5" class="ltx_tr">
<th id="S3.T1.7.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S3.T1.7.5.5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Total</span></th>
<td id="S3.T1.7.5.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.7.5.5.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">149</span></td>
<td id="S3.T1.7.5.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.7.5.5.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">149</span></td>
<td id="S3.T1.7.5.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.7.5.5.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">20</span></td>
<td id="S3.T1.7.5.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.7.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">20</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Online Text-based Consultation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">PCP와 환자 행위자는 샘플 시나리오와 지침으로 준비되었으며 인터페이스 및 실험 요구 사항을 숙지하기 위해 연구가 시작되기 전에 파일럿 상담에 참여했다.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">실험을 위해 시뮬레이션된 각 환자는 동기식 텍스트 채팅 인터페이스를 통해 두 개의 온라인 텍스트 기반 상담을 완료했다(그림 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:chat_interface</span>), 하나는 PCP(대조군)이고 하나는 AMIE(중재)이다. PCP와 AMIE의 순서는 무작위 배정되었고 환자 행위자는 각 상담에서 어떤 사람과 이야기하고 있는지 알 수 없었다. PCP는 환자 행위자와 동일한 국가에 위치했으며 상담을 위해 지정된 시간대에서 가용성을 기반으로 무작위로 추출되었다. 환자 배우들은 시나리오를 롤 플레이하고 20분 이내에 대화를 끝내도록 지시받았다. 두 OSCE 에이전트 모두(연구별 지침을 통한 PCP와 프롬프트 템플릿의 일부로 AMIE) 어떤 상황에서도 자신의 신원이나 인간 여부를 밝히지 않도록 요청했다.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Post-questionnaires</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">상담이 완료되면, 환자 행위자와 OSCE 에이전트는 결과적인 상담 전사체에 비추어 사후 질문지에 각각 채워졌다(도 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:patient_actor_rating_interface</span>). 환자 행위자에 대한 사후 질문은 완전한 GMCPQ(표 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:gmcpq_rubric_details</span>), "환자 관심 관리" 및 "환자 복지 유지"에 대한 PACES 구성 요소(표 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>), 및 "관계 육성"에 대한 PCCBP 범주의 체크리스트 표현으로 구성되었다(표 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:pccbp_rubric_details</span>). 사후 질문지에 제공된 환자 행위자에 대한 응답을 아래 "환자 행위자 등급"이라고 한다. OSCE 에이전트에 대한 사후 설문은 최소 3개 이상 10개 이하의 조건을 가진 순위 차등 진단(DDx) 목록과 대면 또는 영상 기반 상담, 조사, 치료, 관리 계획 및 후속 조치 필요성에 대한 권장 사항을 요청했다.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Specialist Physician Evaluation</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">마지막으로, 인도(14), 북미(6), 영국(3)의 23명의 전문 의사 풀이 상담의 질과 사후 질문에 대한 응답과 관련하여 PCP와 AMIE를 평가했다. 평가하는 동안 전문의는 또한 관련 그라운드 트루스 감별 및 추가로 승인된 감별과 함께 전체 시나리오 팩에 접근할 수 있었다. 전문의가 평가 중에 액세스할 수 있는 모든 데이터를 아래에서 총칭하여 "OSCE 데이터"라고 한다. 전문 의사는 연구에 포함된 시나리오 팩에 해당하는 전문 분야와 지리적 지역에 맞게 공급되었으며 거주 후 1년에서 36년(중앙값 5년)의 경험을 가지고 있었다. OSCE 데이터의 각 세트는 기본 시나리오의 전문 및 지리적 영역과 일치하도록 무작위로 할당된 한 전문 의사에 의해 평가되었다(예를 들어, 캐나다 호흡기 전문의는 캐나다 유래 호흡기 의학 시나리오의 OSCE 데이터를 평가했다). 각 전문가는 주어진 시나리오에 대해 PCP와 AMIE 모두의 OSCE 데이터를 평가했다. PCP 및 AMIE에 대한 평가는 무작위 및 블라인드 시퀀스에서 동일한 전문가에 의해 수행되었다.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">평가 기준은 제공된 DDx 목록의 정확도, 적절성 및 포괄성, 에스컬레이션, 조사, 치료, 관리 계획 및 후속 조치에 관한 권장 사항의 적절성(표 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:diagnosis_management_rubric_details</span>), 및 모든 PACES(표 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>) 및 PCCBP(표 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:pccbp_rubric_details</span>) 평가 항목을 포함하였다. 또한 전문 의사에게 상담 및 설문 응답, 즉 비사실적이거나 대화에서 제공되지 않는 정보에 참조된 텍스트 구절에서 혼란을 강조하도록 요청했다. 각 OSCE 시나리오 팩은 또한 전문가에게 시나리오별 임상 정보를 제공하여 이상적인 조사 또는 관리 계획과 같은 상담의 임상 품질을 평가하는 데 도움이 되거나 가능한 최고의 상담 품질을 위해 이상적으로 설명되었을 임상 이력의 중요한 측면을 제공했다.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Auto-evaluation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p1.1">인간 평가 외에도 전문가 평가에 대한 경제적인 일관된 대안으로 모델 기반 자동 평가 방법을 구현했다. 이러한 기술은 OSCE 에이전트의 대화 품질과 진단 정확도를 모두 평가하는 데 사용되었다. 대화 품질을 평가하기 위한 자동 평가 방법의 타당성을 확립하기 위해 처음에는 환자 행위자와 전문의 의사 모두가 평가한 PACES 루브릭(<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>)의 4개 평가 축 하위 집합에 초점을 맞췄다. AMIE와 함께 자체 CoT 전략(<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:auto-eval</span>)을 사용하여 대화 내용을 평가하는 자동 평가는 인간 평가자와 잘 일치했으며 이러한 기준에 대한 전문가 간 동의와 비슷했다. 차등 진단의 자동 평가를 위해 다른 LLM, Med-PaLM 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx13" title="">13</a>]</cite>를 대리인으로 활용하여 예측된 진단을 그라운드 진리 진단에 대해 등급을 매겼다(<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:auto-eval-ddx</span>의 더 자세한 내용). DDx 정확도에 대한 우리의 자동 평가는 전문가 등급과 비교하여 AMIE 및 OSCE 에이전트에 대해 유사한 경향을 보여주었다. 전반적으로 자동 평가 경향은 대화 품질과 진단 정확도 모두에 대해 인간 등급과 일치했다.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p2.1">또한 다음 목적을 위해 추가 자동 평가 분석을 수행했다.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i1.p1.1">AMIE 또는 PCP 상담에서 도출된 DDx 정확도의 성능을 비교하기 위해;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i2.p1.1">캐나다와 인도에서 수행된 시뮬레이션 환자 간의 DDx 정확도를 비교하고 두 위치 간에 체계적인 차이가 있는지 확인하는 단계;</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i3.p1.1">자체 대신 PCP 상담 제공 시 AMIE의 DDx 정확도를 분석하여 정보 획득 및 정보 해석의 효과를 격리하는 단계;</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i4.p1.1">대화 회전 횟수가 증가할수록 DDx 정확도를 분석하여 AMIE와 PCP 간의 정보 획득의 효율성을 평가하는 단계;</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i5.p1.1">비평가 피드백 전후의 대화 품질에 대한 내부 루프 셀프 플레이의 이점을 평가합니다.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Statistical Analysis</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p1.1">우리는 모든 149명의 시뮬레이션된 환자에 걸쳐 AMIE 및 PCP에 의해 생성된 DDx 목록의 top-k 정확도를 평가했다. Top-k 정확도는 DDx 목록의 top-k 위치 내에서 정확한 진단이 나타난 경우의 백분율로 정의되었다. 구체적으로, 전문가 평가자가 그라운드 트루스 진단과 매우 가깝거나 밀접하게 관련(또는 인정된 차등)된 정확한 일치로 표시한 경우 후보 진단을 일치로 간주했다. DDx 정확도에 대한 통계적 유의성은 10,000개의 샘플을 사용한 부트스트랩 테스트 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx34" title="">34</a>]</cite>와 모든 k에 걸쳐 FDR(false discovery rate) 수정 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx35" title="">35</a>]</cite>를 사용하여 결정되었다. 환자 배우 및 전문가 등급에 대한 통계적 유의성은 윌콕슨 부호 순위 테스트 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx36" title="">36</a>]</cite> FDR 보정을 사용하여 결정되었다. 에이전트가 "전송할 수 없음/적용되지 않음"을 받은 경우는 테스트에서 제외되었습니다. 아래 결과는 FDR 보정 후 <math alttext="p" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">p</annotation></semantics></math>-값을 참조한다.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Diagnostic Accuracy</h3>

<figure id="S4.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2401.05654/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.7.2.1" style="font-size:90%;">Figure 3</span>:</span><span class="ltx_text ltx_font_bold" id="S4.F3.2.1" style="font-size:90%;">Specialist-rated top-k diagnostic accuracy. <span class="ltx_text ltx_font_medium" id="S4.F3.2.1.2"> AMIE 및 PCPs top-k DDx 정확도는 Ground truth 진단(</span>a<span class="ltx_text ltx_font_medium" id="S4.F3.2.1.3">) 및 승인된 차등(</span>b<span class="ltx_text ltx_font_medium" id="S4.F3.2.1.1">)에서 모든 진단과 관련하여 149개의 시나리오에 걸쳐 비교된다. 부트스트래핑(n=10,000)은 AMIE와 PCP DDx 정확도 사이의 모든 top-k 차이가 FDR 보정 후 <math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S4.F3.2.1.1.m1.1"><semantics id="S4.F3.2.1.1.m1.1b"><mrow id="S4.F3.2.1.1.m1.1.1" xref="S4.F3.2.1.1.m1.1.1.cmml"><mi id="S4.F3.2.1.1.m1.1.1.2" xref="S4.F3.2.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.F3.2.1.1.m1.1.1.1" xref="S4.F3.2.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.F3.2.1.1.m1.1.1.3" xref="S4.F3.2.1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.2.1.1.m1.1c"><apply id="S4.F3.2.1.1.m1.1.1.cmml" xref="S4.F3.2.1.1.m1.1.1"><lt id="S4.F3.2.1.1.m1.1.1.1.cmml" xref="S4.F3.2.1.1.m1.1.1.1"></lt><ci id="S4.F3.2.1.1.m1.1.1.2.cmml" xref="S4.F3.2.1.1.m1.1.1.2">𝑝</ci><cn id="S4.F3.2.1.1.m1.1.1.3.cmml" type="float" xref="S4.F3.2.1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.2.1.1.m1.1d">p&lt;0.05</annotation></semantics></math>에서 유의함을 확인한다. </span></span></figcaption>
</figure>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>AMIE showed higher DDx accuracy than PCPs under specialist physician evaluation.</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">AMIE의 진단 정확도는 PCP보다 높은 것으로 평가되었다. <a class="ltx_ref" href="#S4.F3" title="In 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>는 AMIE 및 PCP에 대한 top-k 정확도를 나타내며, 그라운드 트루스 진단(a)과의 일치 및 허용된 차등(b) 상의 임의의 항목과의 일치 등을 고려한다. (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p1.1.m1.1"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mrow id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS1.SSS1.p1.1.m1.1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS1.SSS1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"><lt id="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.1"></lt><ci id="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.SSS1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">p&lt;0.05</annotation></semantics></math>). AMIE와 달리 PCP는 차등 진단에서 항상 10개의 진단을 제공하는 것은 아니다(최소: 3, 평균: 5.39). 또한 일치 여부를 결정하기 위한 일치 기준을 변경하여 AMIE와 PCP 간의 DDx 정확도를 비교했다. <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_specialist_match_cutoffs</span>에 묘사된 결과는 다양한 매칭 기준에 걸쳐 AMIE의 우수한 DDx 성능을 더욱 입증한다.</p>
</div>
<section id="S4.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Accuracy by Specialty.</h5>

<div id="S4.SS1.SSS1.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS1.Px1.p1.1"><span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:specialist_ddx_ratings_by_specialty</span>은 본 연구에서 시나리오가 다루는 6가지 의료 전문 분야에 걸쳐 AMIE 및 PCP에 의해 달성된 DDx 정확도를 보여준다. 우리는 AMIE의 성능이 호흡기 및 심혈관 전문 분야에서 가장 두드러진 개선과 함께 모든 전문 분야에 대한 PCP 성능과 일치하거나 능가하는 것을 관찰했다.</p>
</div>
</section>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Auto-evaluation suggested AMIE matched PCPs’ efficiency in acquiring information.</h4>

<section id="S4.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Auto-evaluation Accuracy.</h5>

<div id="S4.SS1.SSS2.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS2.Px1.p1.1"><a class="ltx_ref" href="#S4.F3" title="In 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>와 동일한 절차를 사용하여 전문가 평가자 대신 모델 기반 자동 평가기로 DDx 정확도 분석을 재현했다. 자동 평가기를 통해 얻은 전체 성능 경향은 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_autoeval</span>과 같이 계산된 정확도 값의 한계 차이에도 불구하고 전문가 평가와 잘 일치한다.</p>
</div>
</section>
<section id="S4.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Isolating the Source of Performance Gains.</h5>

<div id="S4.SS1.SSS2.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS2.Px2.p1.1"><a class="ltx_ref" href="#S4.F3" title="In 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>에서 관찰된 AMIE의 우수한 DDx 성능이 향상된 정보 획득에서 비롯되었는지 또는 더 나은 진단 추론 능력에서 비롯되었는지 조사하기 위해 DDx 자동 평가기를 사용하여 해당 PCP 상담에서 생성된 AMIE의 진단과 자체 상담을 기반으로 AMIE의 진단을 비교했다. <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_autoeval_AMIEvsAMIE</span>에 묘사된 결과는 현저하게 유사한 DDx 성능을 보여주었으며, 이는 AMIE가 자체 대화 또는 PCP의 대화에서 정보를 처리했는지 여부에 관계없이 진단 성능이 일관되게 유지되었음을 나타낸다. 두 방법 모두 PCP에 의해 생성된 차등 진단을 크게 능가했다. 이러한 결과는 AMIE가 정보 획득 시 PCP와 거의 동일하지만 정확한/완전한 감별 진단을 생성하기 위해 해당 정보를 해석하는 데 PCP보다 우수함을 시사한다.</p>
</div>
</section>
<section id="S4.SS1.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Efficiency of Information Acquisition.</h5>

<div id="S4.SS1.SSS2.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS2.Px3.p1.1">AMIE는 상담 중 응답에서 생성된 총 단어 수 측면에서 PCP에 비해 더 큰 장황함을 나타냈지만, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:number_of_words_and_turns</span>에 예시된 바와 같이 두 OSCE 에이전트 모두에서 대화 회전 수와 환자 행위자로부터 도출된 단어 수가 유사했다. 이것은 AMIE와 PCP가 만남 동안 환자로부터 유사한 양의 정보를 획득했음을 시사한다. AMIE 또는 PCP가 정확한 진단을 공식화하기 위한 충분한 정보를 수집하는 데 얼마나 효율적인지 조사하기 위해 다양한 턴 카운트에서 대화를 절단하고 AMIE를 사용하여 이러한 부분 대화를 기반으로 차등 진단을 생성했다. <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_autoeval_AMIEvsAMIE_turnsablation</span>은 모델에 제공된 턴 수의 함수로서 top-3 DDx 정확도를 묘사한다. 관찰된 정확도는 AMIE와 PCP 모두에 대해 초기 10회 대화 회전 내에서 안정되었다. 이는 AMIE와 PCP 모두 대화의 초기 단계에서 진단을 공식화하는 데 필요한 정보를 얻을 수 있었음을 시사한다. 또한 매회 유사한 성능은 AMIE나 PCP 모두 정보 획득의 효율성이나 품질에 큰 이점이 없음을 나타낸다.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2401.05654/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="355" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.11.5.1" style="font-size:90%;">Figure 4</span>:</span><span class="ltx_text ltx_font_bold" id="S4.F4.8.4" style="font-size:90%;">Patient actor ratings. <span class="ltx_text ltx_font_medium" id="S4.F4.8.4.4">상담 종료 시 환자 행위자가 평가한 대화 품질. 예시를 위해 5점 평가 척도의 모든 응답은 '매우 호의적'에서 '매우 비호의적'에 이르는 일반적인 5점 척도로 매핑되었다. 예/아니오 질문의 경우, (긍정적) '예' 응답은 '좋음'과 동일한 색으로 매핑되었고 (부정적) '아니오' 응답은 '좋음'과 동일한 색으로 매핑되었다. 평가 척도는 General Medical Council Patient Questionnaire (GMCPQ), Practical Assessment of Clinical Examination Skills (PACES), 그리고 Patient-Centered Communication Best Practice (PCCBP)에 대한 서술적 검토에서 채택되었다. 질문 문구와 응답 옵션에 대한 자세한 내용은 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:rubrics</span>에 나와 있습니다. 별표는 통계적 유의성(<math alttext="*:p&lt;0.05" class="ltx_math_unparsed" display="inline" id="S4.F4.5.1.1.m1.2"><semantics id="S4.F4.5.1.1.m1.2b"><mrow id="S4.F4.5.1.1.m1.2c"><mo id="S4.F4.5.1.1.m1.1.1" rspace="0em">*</mo><mo id="S4.F4.5.1.1.m1.2.2" rspace="0.278em">:</mo><mi id="S4.F4.5.1.1.m1.2.3">p</mi><mo id="S4.F4.5.1.1.m1.2.4">&lt;</mo><mn id="S4.F4.5.1.1.m1.2.5">0.05</mn></mrow><annotation encoding="application/x-tex" id="S4.F4.5.1.1.m1.2d">*:p&lt;0.05</annotation></semantics></math>, <math alttext="**:p&lt;0.01" class="ltx_math_unparsed" display="inline" id="S4.F4.6.2.2.m2.3"><semantics id="S4.F4.6.2.2.m2.3b"><mrow id="S4.F4.6.2.2.m2.3c"><mo id="S4.F4.6.2.2.m2.1.1" rspace="0em">*</mo><mo id="S4.F4.6.2.2.m2.2.2" lspace="0em" rspace="0em">*</mo><mo id="S4.F4.6.2.2.m2.3.3" rspace="0.278em">:</mo><mi id="S4.F4.6.2.2.m2.3.4">p</mi><mo id="S4.F4.6.2.2.m2.3.5">&lt;</mo><mn id="S4.F4.6.2.2.m2.3.6">0.01</mn></mrow><annotation encoding="application/x-tex" id="S4.F4.6.2.2.m2.3d">**:p&lt;0.01</annotation></semantics></math>, <math alttext="***:p&lt;0.001" class="ltx_math_unparsed" display="inline" id="S4.F4.7.3.3.m3.4"><semantics id="S4.F4.7.3.3.m3.4b"><mrow id="S4.F4.7.3.3.m3.4c"><mo id="S4.F4.7.3.3.m3.1.1" rspace="0em">*</mo><mo id="S4.F4.7.3.3.m3.2.2" lspace="0em" rspace="0em">*</mo><mo id="S4.F4.7.3.3.m3.3.3" lspace="0em" rspace="0em">*</mo><mo id="S4.F4.7.3.3.m3.4.4" rspace="0.278em">:</mo><mi id="S4.F4.7.3.3.m3.4.5">p</mi><mo id="S4.F4.7.3.3.m3.4.6">&lt;</mo><mn id="S4.F4.7.3.3.m3.4.7">0.001</mn></mrow><annotation encoding="application/x-tex" id="S4.F4.7.3.3.m3.4d">***:p&lt;0.001</annotation></semantics></math>, <math alttext="n.s.:" class="ltx_Math" display="inline" id="S4.F4.8.4.4.m4.3"><semantics id="S4.F4.8.4.4.m4.3b"><mrow id="S4.F4.8.4.4.m4.3.4.2" xref="S4.F4.8.4.4.m4.3.4.1.cmml"><mi id="S4.F4.8.4.4.m4.1.1" xref="S4.F4.8.4.4.m4.1.1.cmml">n</mi><mo id="S4.F4.8.4.4.m4.3.4.2.1" lspace="0em" rspace="0.167em" xref="S4.F4.8.4.4.m4.3.4.1a.cmml">.</mo><mi id="S4.F4.8.4.4.m4.2.2" xref="S4.F4.8.4.4.m4.2.2.cmml">s</mi><mo id="S4.F4.8.4.4.m4.3.4.2.2" lspace="0em" rspace="0.167em" xref="S4.F4.8.4.4.m4.3.4.1a.cmml">.</mo><mo id="S4.F4.8.4.4.m4.3.3" xref="S4.F4.8.4.4.m4.3.3.cmml">:</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.8.4.4.m4.3c"><apply id="S4.F4.8.4.4.m4.3.4.1.cmml" xref="S4.F4.8.4.4.m4.3.4.2"><csymbol cd="ambiguous" id="S4.F4.8.4.4.m4.3.4.1a.cmml" xref="S4.F4.8.4.4.m4.3.4.2.1">formulae-sequence</csymbol><ci id="S4.F4.8.4.4.m4.1.1.cmml" xref="S4.F4.8.4.4.m4.1.1">𝑛</ci><ci id="S4.F4.8.4.4.m4.2.2.cmml" xref="S4.F4.8.4.4.m4.2.2">𝑠</ci><ci id="S4.F4.8.4.4.m4.3.3.cmml" xref="S4.F4.8.4.4.m4.3.3">:</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.8.4.4.m4.3d">n.s.:</annotation></semantics></math> not significant)을 나타낸다. </span></span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Conversation Quality</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>AMIE surpassed PCPs in conversation quality, per specialists and patient actors.</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">대화 품질은 환자 배우 평가, 전문가 평가 및 자동 평가의 결과를 사용하여 평가되었다. <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:AMIE_example_osce</span> 및 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:pcp_example_osce</span>은 각각 AMIE 및 PCP로부터 동일한 시뮬레이션된 환자에 대한 두 가지 예시적인 상담을 보여준다.</p>
</div>
<section id="S4.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Patient Actor Ratings.</h5>

<div id="S4.SS2.SSS1.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS1.Px1.p1.1"><a class="ltx_ref" href="#S4.F4" title="In Efficiency of Information Acquisition. ‣ 4.1.2 Auto-evaluation suggested AMIE matched PCPs’ efficiency in acquiring information. ‣ 4.1 Diagnostic Accuracy ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>는 OSCE 에이전트와의 상담 후 평가된 다양한 대화 품질을 제시한다. 전반적으로 AMIE의 상담은 26개 축 중 24개 축에 걸쳐 PCP의 상담보다 환자 행위자에 의해 유의하게 더 나은(<수학 idx=0></math>) 평가를 받았다. 두 PCCBP 축 "환자의 개인 정보 존중"(N=108)과 "실수를 인정"(N=41)에 대해 등급의 유의한 차이는 감지되지 않았다. 후자의 기준은 OSCE 에이전트가 실수를 하고 대화에서 지적한 경우에만 적용되는 질문이기 때문에 제외 횟수가 실질적으로 더 많았다.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2401.05654/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="438" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.11.5.1" style="font-size:90%;">Figure 5</span>:</span><span class="ltx_text ltx_font_bold" id="S4.F5.8.4" style="font-size:90%;">전문 의사 등급. <span class="ltx_text ltx_font_medium" id="S4.F5.8.4.4"> 전문의가 평가한 대화 및 추론 품질. 예시를 위해 5점 평가 척도의 모든 응답은 '매우 호의적'에서 '매우 비호의적'에 이르는 일반적인 5점 척도로 매핑되었다. 유일한 4점 척도(DDx 포괄성)는 '유리하지도 않고 불리하지도 않은' 옵션을 무시하고 동일한 척도로 매핑되었다. 예/아니오 질문의 경우, (긍정적) '예' 응답은 '좋음'과 동일한 색으로 매핑되었고 (부정적) '아니오' 응답은 '좋음'과 동일한 색으로 매핑되었다. 평가 척도는 임상 검사 기술의 실용 평가(PACES), 환자 중심 의사소통 모범 사례(PCCBP) 및 기타 출처에 대한 서술적 검토에서 채택되었다. 질문 문구와 응답 옵션에 대한 자세한 내용은 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:rubrics</span>에 나와 있습니다. 별표는 통계적 유의성(<math alttext="*:p&lt;0.05" class="ltx_math_unparsed" display="inline" id="S4.F5.5.1.1.m1.2"><semantics id="S4.F5.5.1.1.m1.2b"><mrow id="S4.F5.5.1.1.m1.2c"><mo id="S4.F5.5.1.1.m1.1.1" rspace="0em">*</mo><mo id="S4.F5.5.1.1.m1.2.2" rspace="0.278em">:</mo><mi id="S4.F5.5.1.1.m1.2.3">p</mi><mo id="S4.F5.5.1.1.m1.2.4">&lt;</mo><mn id="S4.F5.5.1.1.m1.2.5">0.05</mn></mrow><annotation encoding="application/x-tex" id="S4.F5.5.1.1.m1.2d">*:p&lt;0.05</annotation></semantics></math>, <math alttext="**:p&lt;0.01" class="ltx_math_unparsed" display="inline" id="S4.F5.6.2.2.m2.3"><semantics id="S4.F5.6.2.2.m2.3b"><mrow id="S4.F5.6.2.2.m2.3c"><mo id="S4.F5.6.2.2.m2.1.1" rspace="0em">*</mo><mo id="S4.F5.6.2.2.m2.2.2" lspace="0em" rspace="0em">*</mo><mo id="S4.F5.6.2.2.m2.3.3" rspace="0.278em">:</mo><mi id="S4.F5.6.2.2.m2.3.4">p</mi><mo id="S4.F5.6.2.2.m2.3.5">&lt;</mo><mn id="S4.F5.6.2.2.m2.3.6">0.01</mn></mrow><annotation encoding="application/x-tex" id="S4.F5.6.2.2.m2.3d">**:p&lt;0.01</annotation></semantics></math>, <math alttext="***:p&lt;0.001" class="ltx_math_unparsed" display="inline" id="S4.F5.7.3.3.m3.4"><semantics id="S4.F5.7.3.3.m3.4b"><mrow id="S4.F5.7.3.3.m3.4c"><mo id="S4.F5.7.3.3.m3.1.1" rspace="0em">*</mo><mo id="S4.F5.7.3.3.m3.2.2" lspace="0em" rspace="0em">*</mo><mo id="S4.F5.7.3.3.m3.3.3" lspace="0em" rspace="0em">*</mo><mo id="S4.F5.7.3.3.m3.4.4" rspace="0.278em">:</mo><mi id="S4.F5.7.3.3.m3.4.5">p</mi><mo id="S4.F5.7.3.3.m3.4.6">&lt;</mo><mn id="S4.F5.7.3.3.m3.4.7">0.001</mn></mrow><annotation encoding="application/x-tex" id="S4.F5.7.3.3.m3.4d">***:p&lt;0.001</annotation></semantics></math>, <math alttext="n.s.:" class="ltx_Math" display="inline" id="S4.F5.8.4.4.m4.3"><semantics id="S4.F5.8.4.4.m4.3b"><mrow id="S4.F5.8.4.4.m4.3.4.2" xref="S4.F5.8.4.4.m4.3.4.1.cmml"><mi id="S4.F5.8.4.4.m4.1.1" xref="S4.F5.8.4.4.m4.1.1.cmml">n</mi><mo id="S4.F5.8.4.4.m4.3.4.2.1" lspace="0em" rspace="0.167em" xref="S4.F5.8.4.4.m4.3.4.1a.cmml">.</mo><mi id="S4.F5.8.4.4.m4.2.2" xref="S4.F5.8.4.4.m4.2.2.cmml">s</mi><mo id="S4.F5.8.4.4.m4.3.4.2.2" lspace="0em" rspace="0.167em" xref="S4.F5.8.4.4.m4.3.4.1a.cmml">.</mo><mo id="S4.F5.8.4.4.m4.3.3" xref="S4.F5.8.4.4.m4.3.3.cmml">:</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.8.4.4.m4.3c"><apply id="S4.F5.8.4.4.m4.3.4.1.cmml" xref="S4.F5.8.4.4.m4.3.4.2"><csymbol cd="ambiguous" id="S4.F5.8.4.4.m4.3.4.1a.cmml" xref="S4.F5.8.4.4.m4.3.4.2.1">formulae-sequence</csymbol><ci id="S4.F5.8.4.4.m4.1.1.cmml" xref="S4.F5.8.4.4.m4.1.1">𝑛</ci><ci id="S4.F5.8.4.4.m4.2.2.cmml" xref="S4.F5.8.4.4.m4.2.2">𝑠</ci><ci id="S4.F5.8.4.4.m4.3.3.cmml" xref="S4.F5.8.4.4.m4.3.3">:</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.8.4.4.m4.3d">n.s.:</annotation></semantics></math> not significant)을 나타낸다. </span></span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Specialist Physician Ratings.</h5>

<div id="S4.SS2.SSS1.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS1.Px2.p1.1">전문 의사는 도메인 전문 지식 내의 시나리오에 대해 대화 품질과 사후 질문에 대한 응답을 모두 평가했다(<a class="ltx_ref" href="#S4.F5" title="In Patient Actor Ratings. ‣ 4.2.1 AMIE surpassed PCPs in conversation quality, per specialists and patient actors. ‣ 4.2 Conversation Quality ‣ 4 Results ‣ Towards Conversational Diagnostic AI"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> 참조). 다시 한 번, AMIE의 응답은 32개의 평가 축 중 28개의 평가 축에서 전문가에 의해 PCP에 의한 응답보다 훨씬 더 나은 평가를 받았다; 전문가들은 AMIE의 상담, 진단 및 관리 계획을 PCP에 의한 응답보다 선호했다. 이 평가 세트에 대해 AMIE와 PCP 간의 전문가 등급 차이는 통계적으로 유의했다(<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S4.SS2.SSS1.Px2.p1.1.m1.1"><semantics id="S4.SS2.SSS1.Px2.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.Px2.p1.1.m1.1b"><apply id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1"><lt id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1"></lt><ci id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.Px2.p1.1.m1.1c">p&lt;0.05</annotation></semantics></math>). 진단 및 관리 루브릭의 4개 축, 즉 "에스컬레이션 권고 적절", "치료 부적절 회피", "후속 권고 적절" 및 "혼동 부재"에 대해 배제가 없음에도 불구하고 등급의 유의한 차이가 감지되지 않았다(N=149).</p>
</div>
</section>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Auto-evaluations demonstrated the effectiveness of inner self-play for AMIE.</h4>

<section id="S4.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Auto-evaluation of Conversation Ratings.</h5>

<div id="S4.SS2.SSS2.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS2.Px1.p1.1">PACES 루브릭에서 4개의 평가 축에 대한 대화를 평가하기 위해 모델 기반 자체 CoT 자동 평가 전략을 활용했으며 이러한 자동 평가 등급이 전문가 등급(<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:autoeval_ablation</span> 및 <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:autoeval_vs_specialist</span>)과 정확하고 잘 일치하는지 확인했다. 또한, 내적 자기 재생 루프가 모의 대화 품질을 향상시켰음을 입증하기 위해 자기 재생 절차 전후에 생성된 모의 대화문에 자동 평가 방법을 적용했다. <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:autoeval_selfplay</span>의 결과는 self-critique가 없는 baseline dialog보다 self-play 후 시뮬레이션된 dialog가 더 자주 선호된다는 것을 보여주었다.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Clinical History-taking and the Diagnostic Dialogue</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p1.1">역사와 임상 인터뷰는 의대와 대학원 커리큘럼 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx37" title="">37</a>, <a class="ltx_ref" href="#bib.bibx38" title="">38</a>, <a class="ltx_ref" href="#bib.bibx39" title="">39</a>, <a class="ltx_ref" href="#bib.bibx40" title="">40</a>, <a class="ltx_ref" href="#bib.bibx41" title="">41</a>, <a class="ltx_ref" href="#bib.bibx42" title="">42</a>]</cite>에서 널리 가르친다. 의사-환자 커뮤니케이션에 대한 합의는 환자 중심의 커뮤니케이션 관행을 수용하도록 발전했으며 임상 만남에서의 커뮤니케이션은 관계 촉진, 정보 수집, 정보 제공, 의사 결정, 감정 반응 및 질병 및 치료 관련 행동을 가능하게 하는 6가지 핵심 기능을 다루어야 한다고 권고했다. 이러한 목표를 달성하기 위한 특정 기술과 행동도 검증된 도구를 사용하여 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx45" title="">45</a>, <a class="ltx_ref" href="#bib.bibx20" title="">20</a>]</cite>를 설명, 가르치고 평가했다. 의학 컨벤션은 발표 불만, 과거 병력 및 약물 병력, 사회 및 가족 병력, 시스템 검토 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx46" title="">46</a>, <a class="ltx_ref" href="#bib.bibx47" title="">47</a>]</cite>와 같은 주제를 포함하여 임상 인터뷰 동안 특정 범주의 정보를 수집해야 한다고 일관되게 인용한다. 이러한 목표를 달성하는 임상의의 능력은 일반적으로 객관적인 구조화된 임상 검사(OSCE)의 프레임워크를 사용하여 평가된다. 이러한 평가는 재현성 또는 구현에서 다양하며 원격 의료 시나리오와 함께 가상 OSCE(vOSCE)로 원격 실습에 적용되기도 했으며 COVID-19 팬데믹 기간 동안 특히 관련성의 문제<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx48" title="">48</a>]</cite>이다.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Conversational AI and Goal-oriented Dialogue</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p1.1">목표 지향 대화와 과제 완성을 위한 대화형 AI 시스템은 풍부한 역사를 가지고 있다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx49" title="">49</a>, <a class="ltx_ref" href="#bib.bibx50" title="">50</a>, <a class="ltx_ref" href="#bib.bibx51" title="">51</a>]</cite> 트랜스포머 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx52" title="">52</a>]</cite>와 대형 언어 모델 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx15" title="">15</a>]</cite>의 출현은 이러한 방향에 대한 새로운 관심을 이끌었다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx53" title="">53</a>]</cite>, 자기 개선<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx54" title="">54</a>, <a class="ltx_ref" href="#bib.bibx55" title="">55</a>, <a class="ltx_ref" href="#bib.bibx56" title="">56</a>, <a class="ltx_ref" href="#bib.bibx57" title="">57</a>]</cite> 및 확장 가능한 감독 메커니즘 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx58" title="">58</a>]</cite>에 대한 전략 개발은 이러한 대화 시스템을 실제 세계에서 대규모로 배치할 수 있도록 했다. 그러나 이러한 AI 시스템의 대화 및 작업 완료 능력에 대한 엄격한 평가 및 탐색은 임상 응용을 위해 여전히 제한적이며, 연구는 주로 질문 응답 또는 요약과 같은 단일 회전 상호 작용 사용 사례에 초점을 맞추었다.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>AI for Medical Consultations and Diagnostic Dialogue</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p1.1">의료 상담을 수행하기 위한 도구로서 AI에 대한 대부분의 탐색은 완전한 자연스러운 대화가 아닌 "증상 체커" 응용 프로그램이나 임상 노트 또는 요약이 주어진 의료 오디오의 전사 또는 그럴듯한 대화 생성과 같은 주제에 초점을 맞추었다. 언어 모델은 임상 대화 데이터 세트를 사용하여 훈련되었지만 [cite idx=1></cite>]를 종합적으로 평가하지는 않았다. 연구는 상업적인 채팅 플랫폼에서 의사와 환자 사이의 메시지(1:1 의료 상담과 비교하여 의사-환자 참여를 변경했을 수 있음)에 기반을 두고 있다. 많은 사람들이 임상적으로 의미 있는 메트릭보다는 기록된 교환에서 다음 회전을 예측하는 데 주로 초점을 맞췄다. 그리고 현재까지 대화 및 의사 소통 기술에서 인간 의사를 검사하고 훈련하는 데 사용되는 동일한 기준을 사용하여 진단 대화를 위한 AI 모델의 품질을 조사한 연구 및 OSCE와 같은 공통 프레임워크에서 AI 시스템을 평가하는 연구는 보고되지 않았다.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Evaluation of Diagnostic Dialogue</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS4.p1.1">진단 대화에서 AI 시스템의 성능에 대한 인간 평가를 위한 이전 프레임워크는 세부적으로 제한적이었다. 그들은 의사소통 기술과 역사 촬영의 질을 평가하기 위한 확립된 기준에 고정되지 않았다. 예를 들어, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx29" title="">29</a>]</cite>는 전체 "인간 평가"를 설명하는 5점 척도를 보고했으며, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx65" title="">65</a>]</cite>는 "관련성, 정보성 및 인간 유사성", <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx66" title="">66</a>]</cite>는 "유창성, 전문성 및 관련성", <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx67" title="">67</a>]</cite>는 "유창성 및 적절성" 및 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx68" title="">68</a>]</cite>는 "유창성"을 보고했다. 이러한 기준은 의료 전문가가 가르치고 실행하는 기준보다 훨씬 덜 포괄적이고 구체적이다. LMs의 대화 능력을 평가하기 위한 다중 에이전트 프레임워크는 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx64" title="">64</a>]</cite>에 소개되어 있지만, 연구는 피부과의 제한된 환경에서 수행되었고, 시뮬레이션된 상호 작용의 의사와 환자 측면을 모방하기 위해 AI 모델을 사용했으며, "완전한" 또는 "완전하지 않은" 이력 작업에 대한 제한된 전문가 평가를 수행했다.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1">본 연구에서는 진단 추론 능력을 갖춘 임상 대화에 최적화된 LLM 기반 AI 시스템인 AMIE를 도입하였다. 우리는 AMIE 상담을 객관 구조 임상 검사(OSCE) 스타일의 인간 시뮬레이션 환자를 대상으로 무작위 이중 맹검 교차 연구를 사용하여 PCP에 의해 수행된 상담과 비교했다. 특히, 우리의 연구는 전통적인 OSCE 평가, 원격 또는 원격 의료 상담 관행 또는 임상의가 일반적으로 환자와 의사 소통하기 위해 텍스트 및 채팅 메시징을 사용하는 방식에 대한 임상 관례를 대표하도록 설계되지 않았다. 우리의 평가는 대신 오늘날 사람들이 LLM과 상호 작용하는 가장 일반적인 방식을 반영했으며 AI 시스템이 원격 진단 대화에 참여할 수 있도록 잠재적으로 확장 가능하고 친숙한 메커니즘을 활용했다. 이 환경에서 우리는 작업에 특별히 최적화된 AI 시스템인 AMIE가 임상적으로 의미 있는 상담 품질의 여러 축을 따라 평가할 때 시뮬레이션된 진단 대화에서 PCP를 능가하는 것을 관찰했다.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Diagnostic Performance.</h5>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">AMIE에서 제공한 차등 진단은 전문 의사가 둘 다 평가했을 때 보드 인증 PCP에서 제공한 진단보다 더 정확하고 완전했다. 이전 연구에서는 AI 시스템이 후향적 평가에서 특정, 좁은 작업 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx69" title="">69</a>, <a class="ltx_ref" href="#bib.bibx70" title="">70</a>, <a class="ltx_ref" href="#bib.bibx71" title="">71</a>]</cite>에서 인간 진단 성능과 일치하거나 초과할 수 있음을 보여주었다. 그러나 이러한 상황은 일반적으로 AI와 의사 모두가 동일한 고정 입력을 해석하는 것(예를 들어, 의료 이미지에서 특정 발견의 존재를 식별하는 것)을 포함했다. 우리의 연구는 인간의 노력으로 수집된 임상 정보에 의존하기보다는 AI 시스템이 대화를 통해 관련 정보를 적극적으로 획득하도록 요구했기 때문에 훨씬 더 어려웠다. 따라서 시스템의 다운스트림 차동 진단은 진단 추론 능력뿐만 아니라 자연스러운 대화와 라포 구축을 통해 불확실성 하에서 수집된 정보의 품질에 달려 있다.</p>
</div>
<div id="S6.SS0.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p2.1">우리의 결과는 AMIE가 시뮬레이션 상담 동안 관련 정보를 도출하는 데 PCP만큼 적합하고 동일한 양의 획득된 정보가 주어진 경우 완전한 차등 진단을 공식화하는 데 PCP보다 더 정확하다는 것을 시사했다. 이 발견은 LLM이 도전적인 경우 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx70" title="">70</a>]</cite>에서 의사와 동일한 임상 정보를 감안할 때 더 완전한 감별 진단을 생성할 수 있다는 다른 연구를 확증한다. 따라서 이 연구에서 탐구되지는 않았지만 AMIE의 보조 성능은 특히 의학과 같은 안전이 중요한 환경에서 AI 시스템에 대한 전문가 감독의 실제 중요성을 고려할 때 향후 연구를 위한 흥미롭고 중요한 방법을 나타낸다.</p>
</div>
<div id="S6.SS0.SSS0.Px1.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p3.1">우리의 연구는 캐나다와 인도에서 훈련된 행위자와 다양한 전문 분야에 걸친 시나리오로 구성된 다양한 시뮬레이션 환자를 활용했다. 이를 통해 전문 분야별, 시나리오가 도출되고 제정된 위치별, 다축을 따라 성능이 어떻게 달라지는지 탐구할 수 있었다. PCP와 AMIE 모두 다른 전문 분야의 시나리오보다 산과/진학 및 내과 시나리오에서 더 나쁜 성능을 보였다 (<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:specialist_ddx_ratings_by_specialty</span> 참조). 이 연구는 다른 전문 분야 주제 간의 성능을 비교하도록 구동되거나 설계되지 않았으며 일부 전문 분야의 시나리오가 다른 전문 분야보다 어려울 수 있음을 배제할 수 없다. 우리는 AMIE와 PCP 모두 인도 OSCE 실험실에서 제정된 상담에 비해 캐나다 OSCE 실험실에서 수행된 상담에서 진단 정확도가 더 높다는 것을 관찰했다(<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:per_location_specialist_eval</span> 참조). 그러나 그 차이는 통계적으로 유의하지 않았으며 캐나다 OSCE 랩과 인도 OSCE 랩 모두에서 제정된 40개 시나리오의 하위 집합에서 AMIE와 PCP의 성능은 동등했다(<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:same_scenario_location_ddx</span> 참조).</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Conversational Performance.</h5>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">환자 행위자와 전문가 평가자 모두 공감 및 의사소통 기술과 관련된 척도에 대해 AMIE의 성과가 PCP보다 높다고 평가했다. 이 축은 평가된 치수의 대부분을 구성했다. 이 일반적인 발견은 LLM 응답이 Reddit<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx73" title="">73</a>]</cite>에 게시된 건강 질문에 대한 임상의의 응답보다 더 공감하는 것으로 밝혀진 이전 연구와 일치한다. 그러나 해당 연구의 결과는 연구 설계의 차이로 인해 우리의 설정으로 직접 일반화되지 않을 수 있다. 특히, 이전 작업은 동일한 환자와의 다중 회전 대화의 전향적 시뮬레이션에서 의사와 AI 시스템의 직접 무작위 비교를 포함하지 않았다. 두 설정 모두에서 음성 기반 및 비언어적 시각적 커뮤니케이션의 부족은 임상의에게 불공평한 불이익이 될 수 있다.</p>
</div>
<div id="S6.SS0.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p2.1">본 연구에서 사용한 텍스트 기반 채팅 인터페이스는 장단점을 모두 소개한다. 오늘날 사람들은 동기식 문자-채팅 인터페이스 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx74" title="">74</a>]</cite>를 통해 LLMs에 가장 일반적으로 참여하며, 환자들은 종종 환자 포털을 사용하여 자신의 제공자에게 메시지를 보낸다. 따라서 우리는 이 상호작용 모드를 LLM이 멀티턴 대화를 수행하기 위한 대표적인 인터페이스로 선택했으며, 이에 따라 가상 OSCE 프레임워크를 적용했다. 이는 둘 다 동기식 문자 채팅으로 제한되었을 때 LLM과 임상의 사이의 진단 대화를 공정하게 비교할 수 있었지만, 우리의 실험이 실제 임상 실습(원격 의료 포함)에서 예상되는 진단 대화의 품질을 모방하지 않는다는 것을 인정하는 것이 중요하다. 의사는 동기식 문자-채팅 통신 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx75" title="">75</a>, <a class="ltx_ref" href="#bib.bibx76" title="">76</a>]</cite>보다 전화 또는 화상 상담에 의한 이력 작성 및 진단 대화에 더 익숙할 수 있다. 대신, 텍스트는 처방 리필 또는 특정 테스트 결과에 대한 통신과 같은 일화적 또는 비동기적 요구에 대해 환자와 의사 소통하기 위해 임상의에 의해 더 일반적으로 사용된다. 따라서 의사는 이 연구에서 사용한 동기식 문자 채팅 매체보다 문자/SMS 또는 이메일에 더 친숙할 수 있다. 텍스트/SMS와 이메일 모두에서 자연스럽고 공감적인 방식으로 소통하기 위한 관습과 기대는 다를 수 있다. 우리 연구의 PCP가 아직 설정에 익숙하지 않았을 수 있으며 특정 훈련 프로그램(AMIE를 위한 훈련 과정과 정신적으로 유사)을 받는 경우 다르게 수행되었을 수 있다. 연구에 참여한 임상의는 평가가 시작되기 전에 동기식 텍스트 인터페이스와 두 번의 준비 파일럿 세션의 상담을 수행했지만 이는 공식적인 훈련 프로그램이 아니며 임상의의 성능을 최적화하도록 설계되지 않았다. 향후 연구에서는 학습 곡선의 영향에 대한 모니터링을 포함하여 이 질문을 더 철저히 탐구하거나 참여 임상의 또는 시뮬레이션 환자가 원격 의료에 익숙한 정도에 따라 성능이 달라지는지 탐구할 수 있다.</p>
</div>
<div id="S6.SS0.SSS0.Px2.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p3.1">또한, 공감적 커뮤니케이션에 관한 우리의 연구 결과는 AMIE 응답이 임상의 응답보다 상당히 길었다는 사실(<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:number_of_words_and_turns</span>)에 부분적으로 기인할 수 있으며 더 큰 구조를 나타냈다. 이것은 잠재적으로 관찰자에게 환자의 만족도가 의사의 시간과 함께 증가함에 따라 증가한다는 알려진 발견과 유사하게 반응을 준비하는 데 더 많은 시간이 소요되었음을 시사할 수 있다.</p>
</div>
<div id="S6.SS0.SSS0.Px2.p4" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p4.1">종합적으로, 우리의 연구 결과는 언어 및 비언어 단서의 분석에서 임상의의 기술과 LLM의 잠재적 강점을 결합하여 공감 진술, 구조, 웅변 또는 보다 완전한 감별 진단을 포함한 보다 풍부한 대화 반응을 제안할 수 있는 인간-AI 상보성 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx82" title="">82</a>]</cite>를 활용할 수 있는 추가 연구를 위한 많은 방법을 제안한다.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Simulated Dialogue.</h5>

<div id="S6.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px3.p1.1">시뮬레이션된 데이터를 사용하여 광범위한 조건 및 환자 컨텍스트로 훈련을 신속하게 확장할 수 있었고 검색에서 지식을 주입하면 이러한 대화가 근거 있고 사실적으로 유지되도록 장려했다. 모의 환자는 광범위한 조건을 포함했지만 잠재적인 환자 배경, 성격 및 동기의 전체 범위를 포착하지 못했다. 내면적 자기놀이 절차를 통해 우리가 생성하고 미세 조정에 사용한 모의 대화를 반복적으로 개선할 수 있었다. 그러나 이러한 개선은 비평가 지시에서 좋은 대화를 만드는 것을 표현하는 우리의 능력, 효과적인 피드백을 생성하는 비평가의 능력, 그러한 피드백에 적응하는 AMIE의 능력에 의해 제한되었다. 예를 들어, 시뮬레이션된 환경에서 AMIE가 환자에 대해 제안된 차등 및 테스트/치료 계획에 도달한다고 부과하지만, 이러한 엔드포인트는 일부 조건, 특히 가상 채팅 기반 설정에서 비현실적일 수 있다.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Framework.</h5>

<div id="S6.SS0.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px4.p1.1">이전 작업과 대조적으로, 우리는 의사의 의사 소통 기술 및 역사 촬영 품질을 평가하는 것과 관련이 있는 것으로 이미 확립된 기준에 평가를 고정했다. 우리는 임상의와 모의 환자 관점 모두에서 평가를 통해 AI 시스템에 대한 이전 연구보다 더 광범위하고 다양한 인간 평가를 수행했다. 우리의 평가자와 시나리오는 북미, 인도 및 영국을 포함한 여러 지리적 위치에서 조달되었다. 우리의 파일럿 평가 루브릭은 우리가 아는 한 의사 자신을 위해 실제 세계에서도 측정되는 축을 사용하여 LLM의 역사 기록 및 의사 소통 기술을 처음으로 평가하여 연구의 임상적 관련성을 높이는 것이다. 우리의 평가 프레임워크는 환자 중심의 의사 소통 모범 사례나 상담 품질의 임상적으로 관련된 축을 고려하지 않은 AI 생성 임상 대화의 이전 작업보다 훨씬 더 세분화되고 구체적이다.</p>
</div>
<div id="S6.SS0.SSS0.Px4.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px4.p2.1">그러나 우리의 파일럿 프레임워크는 확정적이지 않으며 향후 연구에서 더 개선될 수 있다. 역사 촬영 자체는 맥락적이며 "좋은 역사"를 결정하는 것은 특정 임상 상황, 환자 및 의사 속성, 문화적 특성 및 기타 많은 요인에 따라 다르다. 임상 병력 조사 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx83" title="">83</a>, <a class="ltx_ref" href="#bib.bibx84" title="">84</a>, <a class="ltx_ref" href="#bib.bibx85" title="">85</a>, <a class="ltx_ref" href="#bib.bibx86" title="">86</a>]</cite>에 대한 모델의 변화에도 불구하고, 연구는 좋은 임상 인터뷰가 문제 감지 및 진단 정확도와 관련이 있을 뿐만 아니라 환자 및 의사 만족도, 스트레스 및 질병에 대한 회복력, 건강 결과 또는 비용에 이르는 치료 전달 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx87" title="">87</a>, <a class="ltx_ref" href="#bib.bibx88" title="">88</a>]</cite>를 목표로 한다는 것을 보여주었다. 따라서 LLM 이력 작성 품질에 대한 향후 연구는 실제 환경에서 이러한 결과의 전향적 측정(예: 환자 불만 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx89" title="">89</a>]</cite> 또는 비용 및 치료 효과, 환자 및 제공자 만족도의 개선)을 활용할 수 있지만, 이와 같은 평가는 동일한 개별 환자의 표준 관행과 비교하기에 어렵거나 비현실적일 수 있으며 다른 접근 방식의 무작위화도 실제 환경에서 어려울 수 있다.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Breadth of Evaluation.</h5>

<div id="S6.SS0.SSS0.Px5.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px5.p1.1">우리가 선택한 평가 축은 완전하지 않았으며 그 해석은 본질적으로 종종 주관적이었다. 우리는 북미와 인도 모두에 평가자가 있는 세 국가에서 시나리오 팩을 생성하여 임상의와 일반인의 평가를 수행했지만 모델을 평가하는 임상의와 일반인의 풀은 통찰력의 일반화를 개선하기 위해 더 확장될 수 있다. 우리의 실험은 또한 의도적으로 더 다양한 인간 평가자 풀(임상의 및 일반 사용자)을 사용한 향후 작업을 포함하여 관찰자 간 및 참가자 간 변동성과 같은 다른 측면을 탐색하기 위해 더 광범위한 복제를 겪을 수 있다. 임상 및 건강 형평성 영역 전문가뿐만 아니라 대표적인 환자 풀을 가진 모델 평가 도구 개발에 참여하는 설계도 가치가 있을 수 있다.</p>
</div>
<div id="S6.SS0.SSS0.Px5.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px5.p2.1">우리의 시나리오는 많은 다른 임상 조건과 전문 분야로 구성되었지만, 우리의 실험은 한 명의 의사(평균적으로 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx90" title="">90</a>]</cite>에서 수만 건의 상담을 수행할 수 있는)에 의해 축적된 수십 년의 임상 실습을 반드시 대표하는 것은 아니다. 의학에서 검사할 수 있는 조건의 범위는 개별 질병의 표현 변화와 마찬가지로 방대하다. 우리의 실험은 다중 이환 및 동시 발생 병리학, 종단 사례 제시 또는 임상 조사의 순차적 정보를 고려하도록 설계되지 않았다. 우리는 정신과, 소아과, 중환자, 입원 환자 사례 관리 시나리오와 같은 일부 임상 환경이나 전문 분야를 완전히 제외했다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx91" title="">91</a>, <a class="ltx_ref" href="#bib.bibx92" title="">92</a>]</cite>와 같은 고품질 역사 작업에 대한 요구 사항이 다를 수 있는 이러한 많은 환경에서 연구 결과의 적용 가능성을 이해하려면 추가 연구가 필요할 것이다. OSCE 프레임워크는 임상의의 기술 평가에 일반적으로 사용된다. 여기에는 실제 또는 시뮬레이션된 환자, 물리적 인공물 또는 임상 재료와의 상호 작용, 다양한 의료 전문 분야에 대한 적용, 작업 또는 설정, 원격 또는 직접 평가를 포함한 상당한 범위의 방법론이 포함된다. OSCE 접근법이 대중적이지만, 그 타당성<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx93" title="">93</a>]</cite>에는 상당한 한계가 있다. 우리는 비언어적 증상, 징후 및 커뮤니케이션 기능을 통합할 수 없는 것과 같은 "가상 OSCE"의 패러다임으로 알려져 있는 문제를 복제하여 원격 텍스트 기반 평가를 활용했다. 또한 이 형식은 PCP 참가자의 통신에 익숙하지 않은 제약을 도입할 수 있습니다 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx48" title="">48</a>]</cite>.</p>
</div>
<div id="S6.SS0.SSS0.Px5.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px5.p3.1">우리 연구에서 OSCE 대화의 톤, 내용 및 특성은 실제 환자 집단을 대표하지 않을 가능성이 있다. 예를 들어, 환자 행위자는 많은 상담에서 일상적으로 예상할 수 있는 것보다 더 큰 구조, 깊이 또는 임상 세부 사항으로 증상을 설명하거나 일반적으로 예상할 수 있는 것보다 임상적 맥락에 대한 이해가 더 클 수 있다. 또한 평가가 맹검되었지만 AMIE의 응답 스타일은 연구 설계에서 맹검의 실질적인 범위를 제한하는 PCP에 의한 응답 스타일과 현저하게 달랐다.</p>
</div>
<div id="S6.SS0.SSS0.Px5.p4" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px5.p4.1">따라서 우리가 다룬 질병과 전문 분야의 분포 내에서도 우리의 연구 결과는 겸손하고 신중하게 해석되어야 한다. 다양한 환자 요구, 선호도, 행동 및 상황의 상황에서 병력 작성 및 임상 대화를 평가하기 위한 대체 접근법의 탐색과 함께 동일한 질병의 다양한 프레젠테이션을 조사하기 위한 추가 연구가 필요하다.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fairness and Bias.</h5>

<div id="S6.SS0.SSS0.Px6.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px6.p1.1">본 논문에서 제시하는 평가 프로토콜은 공정성 및 편향과 관련된 잠재적인 문제를 포착할 수 있는 능력 측면에서 제한적이며, 이는 후속 시스템 평가에서 다루고자 하는 중요한 미해결 문제로 남아 있다. 대형 언어 모델 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx94" title="">94</a>, <a class="ltx_ref" href="#bib.bibx95" title="">95</a>]</cite>에서 편향 검출을 위한 포괄적인 프레임워크 개발의 최근 발전은 이러한 접근법을 확립하기 위한 유망한 출발점을 제시한다. 의료 진단 대화는 의료 영역의 복잡성, 대화의 대화형 정보 수집 특성 및 결과 기반 설정으로 인해 특히 어려운 사용 사례이며 잘못된 진단 또는 잘못된 의료 조언의 경우 관련 해악의 가능성이 있다는 점에 유의해야 한다. 그럼에도 불구하고, 영역의 LLM이 의료의 불평등을 전파하기보다는 극복해야 하는 경우 이러한 문제를 해결하는 것은 중요한 추가 연구 영역이다. 예를 들어, 이전 연구에서는 의사가 환자의 인종에 따라 평균적으로 환자와의 커뮤니케이션에 다르게 접근하여 흑인 환자가 환자 중심이 덜하고 긍정적인 영향을 받는 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx96" title="">96</a>]</cite>가 더 낮다는 것을 발견했다. 다른 연구에서는 성별 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx97" title="">97</a>]</cite>에 따라 의사의 의사 소통 스타일과 대화 길이에 차이가 있음을 발견했다. 효과적인 문화 간 의사소통 기술은 필수입니다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx91" title="">91</a>]</cite> 따라서 이러한 역사적 대화 편향이 AI 대화 시스템에서 복제되거나 증폭될 수 있는 무시할 수 없는 위험이 있지만 동시에 개별 환자의 요구에 더 포괄적이고 더 개인화될 수 있는 대화 시스템을 설계하는 방향으로 일할 기회도 있다.</p>
</div>
<div id="S6.SS0.SSS0.Px6.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px6.p2.1">필요한 공정성, 편견 및 형평성 프레임워크의 개발을 알리는 데 도움이 되려면 광범위한 환자 인구 통계와 임상 및 건강 형평성 영역 전문가에 걸쳐 대표 견해를 요청하는 참여 접근법을 사용하는 것이 중요하다. 이러한 평가 프레임워크는 광범위한 모델 레드 학습과 나머지 갭 및 고장 모드를 식별하기 위한 적대적 접근법으로 보완되어야 한다. 최근 적색 학습 LLM의 발전은 이 시나리오 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx98" title="">98</a>, <a class="ltx_ref" href="#bib.bibx99" title="">99</a>, <a class="ltx_ref" href="#bib.bibx100" title="">100</a>, <a class="ltx_ref" href="#bib.bibx101" title="">101</a>]</cite>에서 유용할 수 있다. 이러한 관행은 최종 모델의 평가뿐만 아니라 개발 및 반복적인 개선을 알려야 한다. 모델 개발은 확립된 데이터 및 모델 보고 관행을 따르고 훈련 데이터 및 관련 결정 프로세스 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx102" title="">102</a>, <a class="ltx_ref" href="#bib.bibx103" title="">103</a>, <a class="ltx_ref" href="#bib.bibx104" title="">104</a>]</cite>에 투명성을 제공해야 한다. 우리 연구에서 AMIE 훈련 데이터에 기여하는 대화 연구 데이터 세트는 비식별화되어 사회경제적 요인, 환자 인구 통계 및 임상 환경 및 위치에 대한 정보의 가용성을 줄였다.</p>
</div>
<div id="S6.SS0.SSS0.Px6.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px6.p3.1">다국어 설정 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx105" title="">105</a>, <a class="ltx_ref" href="#bib.bibx106" title="">106</a>, <a class="ltx_ref" href="#bib.bibx107" title="">107</a>, <a class="ltx_ref" href="#bib.bibx108" title="">108</a>]</cite>, 특히 자원이 적은 언어 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx109" title="">109</a>]</cite>에서 의료 LLM의 견고성을 보장하기 위한 추가 작업이 필요하다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx110" title="">110</a>]</cite>, 언어, 지역, 정체성 및 지역화된 의료 요구의 매우 다양한 문화는 선험적 정적이지만 포괄적인 공정성 벤치마크를 실질적으로 실행할 수 없게 만든다. 편향의 측정 및 완화는 전 세계적으로 확장되지 못하는 특정 축에 대한 전통적인 좁은 초점을 넘어 이동해야 한다. LLM 기반 평가자는 체계적인 벤치마크가 없는 언어에서 예비 평가를 위한 잠재적인 솔루션을 제시하지만, 선행 연구에서 이러한 자동 평가 프레임워크가 편향되어 원어민 평가에 대한 보정 필요성을 강조하고 주의해서 사용한다.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px7" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deployment.</h5>

<div id="S6.SS0.SSS0.Px7.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS0.SSS0.Px7.p1.1">이 연구는 진단 대화의 맥락에서 의료에서 향후 사용에 대한 LLM의 잠재력을 보여준다. 본 연구에서 평가된 LLM 연구 프로토타입에서 의료 제공자, 관리자 및 사람들이 사용할 수 있는 안전하고 강력한 도구로 전환하는 것은 기술의 안전성, 신뢰성, 효능 및 개인 정보를 보장하기 위해 상당한 추가 연구가 필요할 것이다. 다양한 임상 환경에 걸친 엄격한 품질 평가와 필요할 때 인간 임상 전문가에게 연기할 수 있는 신뢰할 수 있는 불확실성 추정 방법<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx113" title="">113</a>, <a class="ltx_ref" href="#bib.bibx114" title="">114</a>, <a class="ltx_ref" href="#bib.bibx115" title="">115</a>, <a class="ltx_ref" href="#bib.bibx116" title="">116</a>]</cite>에 대한 연구를 포함하여 이 기술의 윤리적 배치에 세심한 고려가 필요할 것이다. 이러한 및 기타 가드레일은 LLM 기술에 대한 잠재적인 과의존을 완화하기 위해 필요하며, 향후 사용 사례에 특정한 윤리적 및 규제 요구 사항에 주의를 기울이고 모델 출력을 보호하기 위해 루프에 자격을 갖춘 의사의 존재를 위한 기타 특정 조치가 필요하다. 이전 작업 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx12" title="">12</a>]</cite>에서 강조했듯이 기본 모델 또는 배포 사용 상황에서 편향 및 보안 취약성이 발생할 수 있는 정도를 평가하기 위한 추가 연구도 필요할 것이다. 임상 지식의 지속적인 진화를 감안할 때 LLM이 최신 임상 정보 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bibx117" title="">117</a>]</cite>를 활용할 수 있는 방법을 개발하는 것도 중요할 것이다.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p" id="S7.p1.1">의료 AI 시스템이 적절한 수준의 공감 및 신뢰와 소통하면서 대규모 의료 지식에 고정하면서 대화적으로 더 잘 상호작용할 수 있다면 활용도가 크게 향상될 수 있다. 이 연구는 임상 이력 작성 및 진단 대화와 관련된 설정을 위한 LLM 기반 AI 시스템의 상당한 잠재적 능력을 보여준다. 시뮬레이션 상담에서 AMIE의 성능은 대화형 진단 의료 AI에 대한 여러 임상 관련 축을 고려한 평가 프레임워크를 따라 평가되었기 때문에 현장의 이정표를 나타낸다. 그러나 결과는 적절한 주의를 기울여 해석해야 한다. 실험 시뮬레이션된 역사 촬영 및 진단 대화의 제한된 범위에서 사람과 이를 돌보는 사람들을 위한 실제 도구로 번역하려면 기술의 안전성, 신뢰성, 공정성, 효능 및 프라이버시를 보장하기 위한 상당한 추가 연구 개발이 필요하다. 성공적인 경우 AMIE와 같은 AI 시스템이 모든 사람에게 세계 수준의 헬스케어를 확장하는 데 도움이 되는 차세대 학습 건강 시스템의 핵심이 될 수 있다고 믿습니다.</p>
</div>
<section id="S7.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S7.SS0.SSSx1.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS0.SSSx1.p1.1">이 프로젝트는 구글 리서치와 구글 딥마인드의 많은 팀 간의 광범위한 협업이었습니다. 우리는 윤류, 다니엘 맥더프, 제이크 선샤인, 알리 코넬, 폴 맥거번, 주빈 가흐라마니가 원고에 대한 포괄적인 검토와 상세한 피드백에 감사한다. 우리는 또한 사미 라흐가, 로렌 와이너, 존 길리어드 그리고 매기 시엘스가 서사와 시각에 기여한 것에 대해 감사한다. 우리는 줄리 앤 세긴, 샐리 골드만, 유리 바실레프스키, 신잉 송, 악샤이 고엘, 추링 고, 아비나브 다스, 하이양 유, 창 류, 유첸 류, 시와이 맨, 브렛 해트필드, 션 리, 아제이 조시, 고든 터너, 아니사 엄라니, 디브야 판디아 및 프리티 싱에게 우리의 연구 동안 귀중한 통찰력, 기술적 지원 및 피드백에 감사한다. 또한 OSCE 연구를 수행하는 데 파트너십을 맺은 캐나다와 인도의 임상 공급자 파트너에 감사드립니다. 마지막으로, 우리는 이 프로젝트 과정에서 그들의 지원에 대해 데일 웹스터, 에와 도미노프스카, 데이비드 플릿, 필립 맨스필드, 수샨트 프라카시, 르네 웡, 수잔 토마스, 마이클 하웰, 카렌 드살보, 제프 딘, 제임스 마니카, 주빈 가흐라마니, 데미스 하사비스에게 감사한다.</p>
</div>
</section>
<section id="S7.SS0.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Data Availability</h4>

<div id="S7.SS0.SSSx2.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS0.SSSx2.p1.1">AMIE 개발에 사용되는 실제 데이터 세트 중 일부는 오픈 소스(MedQA)이다. OSCE 연구에 사용된 영국의 시나리오 팩은 또한 인터넷에서 다운로드할 수 있다.</p>
</div>
</section>
<section id="S7.SS0.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Code Availability</h4>

<div id="S7.SS0.SSSx3.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS0.SSSx3.p1.1">AMIE는 진단 대화를 위한 LLM 기반 연구 AI 시스템이다. 우리는 의료 환경에서 모니터링되지 않은 시스템 사용의 안전 문제로 인해 오픈 소싱 모델 코드와 가중치가 아니다. 책임 있는 혁신을 위해, 우리는 AMIE의 안전한 향후 사용을 검증하고 탐색하기 위해 연구 파트너, 규제 기관 및 제공자와 협력할 것입니다. 재현성을 위해 임상 및 일반 과학 청중이 논문에 액세스할 수 있도록 유지하면서 기술적 딥러닝 방법을 문서화했다. 우리의 작업은 PaLM 2를 기반으로 하며, 기술 세부 사항은 기술 보고서 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#biba.bibx2" title="">119</a>]</cite>에서 광범위하게 설명되었다.</p>
</div>
</section>
<section id="S7.SS0.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Competing Interests</h4>

<div id="S7.SS0.SSSx4.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS0.SSSx4.p1.1">이 연구는 알파벳 주식회사 및/또는 그 자회사인 '알파벳'에서 자금을 지원받았다. 모든 저자는 알파벳의 직원이며 표준 보상 패키지의 일부로 주식을 소유할 수 있다.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">George Libman Engel and William L Morgan
</span>
<span class="ltx_bibblock">“Interviewing the patient”
</span>
<span class="ltx_bibblock">Saunders, Philadelphia, London, 1973
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">MICHAEL C Peterson, JOHN H Holbrook, DE Von Hales, NL Smith and LV Staker
</span>
<span class="ltx_bibblock">“Contributions of the history, physical examination, and laboratory investigation in making medical diagnoses.”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">Western Journal of Medicine</em> <span id="bib.bibx2.2.2" class="ltx_text ltx_font_bold">156.2</span>
</span>
<span class="ltx_bibblock">BMJ Publishing Group, 1992, pp. 163
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">John R Hampton, MJ Harrison, John R Mitchell, Jane S Prichard and Carol Seymour
</span>
<span class="ltx_bibblock">“Relative contributions of history-taking, physical examination, and laboratory investigation to diagnosis and management of medical outpatients.”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Br Med J</em> <span id="bib.bibx3.2.2" class="ltx_text ltx_font_bold">2.5969</span>
</span>
<span class="ltx_bibblock">British Medical Journal Publishing Group, 1975, pp. 486–489
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Jerome P Kassirer
</span>
<span class="ltx_bibblock">“Teaching clinical medicine by iterative hypothesis testing: let’s preach what we practice”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">New England Journal of Medicine</em> <span id="bib.bibx4.2.2" class="ltx_text ltx_font_bold">309.15</span>
</span>
<span class="ltx_bibblock">Mass Medical Soc, 1983, pp. 921–923
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">M Roshan and AP Rao
</span>
<span class="ltx_bibblock">“A study on relative contributions of the history, physical examination and investigations in making medical diagnosis.”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">The Journal of the Association of Physicians of India</em> <span id="bib.bibx5.2.2" class="ltx_text ltx_font_bold">48.8</span>, 2000, pp. 771–775
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Gerald Sandler
</span>
<span class="ltx_bibblock">“The importance of the history in the medical clinic and the cost of unnecessary tests”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">American heart journal</em> <span id="bib.bibx6.2.2" class="ltx_text ltx_font_bold">100.6</span>
</span>
<span class="ltx_bibblock">Elsevier, 1980, pp. 928–931
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Jonathan Silverman, Suzanne Kurtz and Juliet Draper
</span>
<span class="ltx_bibblock">“Skills for communicating with patients”
</span>
<span class="ltx_bibblock">crc press, 2016
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Timothy Rennie, Jennifer Marriott and Tina P Brock
</span>
<span class="ltx_bibblock">“Global supply of health professionals”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">N Engl J Med</em> <span id="bib.bibx8.2.2" class="ltx_text ltx_font_bold">370.23</span>, 2014, pp. 2246–7
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">“GPT-4 Technical Report”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2303.08774" title="" class="ltx_ref ltx_href">2303.08774 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> Google
</span>
<span class="ltx_bibblock">“PaLM 2 Technical Report”, <a target="_blank" href="https://ai.google/static/documents/palm2techreport.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.google/static/documents/palm2techreport.pdf</a>, 2023
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Google Deepmind
</span>
<span class="ltx_bibblock">“Gemini: A Family of Highly Capable Multimodal Models”, <a target="_blank" href="https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/r7G7RrtT6rnM/v0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/r7G7RrtT6rnM/v0</a>, 2023
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis and Stephen Pfohl
</span>
<span class="ltx_bibblock">“Large Language Models Encode Clinical Knowledge”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.13138</em>, 2022
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis and Darlene Neal
</span>
<span class="ltx_bibblock">“Towards expert-level medical question answering with large language models”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.09617</em>, 2023
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li and Weishung Liu
</span>
<span class="ltx_bibblock">“Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.16452</em>, 2023
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker and Yu Du
</span>
<span class="ltx_bibblock">“LaMDA: Language models for dialog applications”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.08239</em>, 2022
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">“Introducing ChatGPT”, 2022
</span>
<span class="ltx_bibblock">OpenAI
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a>
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Augustin Toma, Patrick R Lawler, Jimmy Ba, Rahul G Krishnan, Barry B Rubin and Bo Wang
</span>
<span class="ltx_bibblock">“Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.12031</em>, 2023
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf and Amirkeivan Mohtashami
</span>
<span class="ltx_bibblock">“MEDITRON-70B: Scaling Medical Pretraining for Large Language Models”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.16079</em>, 2023
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">David Levine
</span>
<span class="ltx_bibblock">“History taking is a complex skill”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">BMJ</em> <span id="bib.bibx19.2.2" class="ltx_text ltx_font_bold">358</span>
</span>
<span class="ltx_bibblock">British Medical Journal Publishing Group, 2017
</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Ann King and Ruth B Hoppe
</span>
<span class="ltx_bibblock">““Best practice” for patient-centered communication: a narrative review”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx20.1.1" class="ltx_emph ltx_font_italic">Journal of graduate medical education</em> <span id="bib.bibx20.2.2" class="ltx_text ltx_font_bold">5.3</span>
</span>
<span class="ltx_bibblock">The Accreditation Council for Graduate Medical Education Suite 2000, 515&nbsp;…, 2013, pp. 385–393
</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang and Peter Szolovits
</span>
<span class="ltx_bibblock">“What disease does this patient have? a large-scale open domain question answering dataset from medical exams”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx21.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em> <span id="bib.bibx21.2.2" class="ltx_text ltx_font_bold">11.14</span>
</span>
<span class="ltx_bibblock">MDPI, 2021, pp. 6421
</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi and Roger G Mark
</span>
<span class="ltx_bibblock">“MIMIC-III, a freely accessible critical care database”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">Scientific data</em> <span id="bib.bibx22.2.2" class="ltx_text ltx_font_bold">3.1</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group, 2016, pp. 1–9
</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Chung-Cheng Chiu, Anshuman Tripathi, Katherine Chou, Chris Co, Navdeep Jaitly, Diana Jaunzeikare, Anjuli Kannan, Patrick Nguyen, Hasim Sak and Ananth Sankar
</span>
<span class="ltx_bibblock">“Speech recognition for medical conversations”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.07274</em>, 2017
</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Ashish Sharma, Adam S Miner, David C Atkins and Tim Althoff
</span>
<span class="ltx_bibblock">“A computational approach to understanding empathy expressed in text-based mental health support”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.08441</em>, 2020
</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Yao Fu, Hao Peng, Tushar Khot and Mirella Lapata
</span>
<span class="ltx_bibblock">“Improving language model negotiation with self-play and in-context learning from ai feedback”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.10142</em>, 2023
</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Asma Ben Abacha, Wen-Wai Yim, Griffin Adams, Neal Snider and Meliha Yetisgen-Yildiz
</span>
<span class="ltx_bibblock">“Overview of the mediqa-chat 2023 shared tasks on the summarization &amp; generation of doctor-patient conversations”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th Clinical Natural Language Processing Workshop</em>, 2023, pp. 503–513
</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Bogdan Ionescu, Henning Müller, Ana-Maria Drăgulinescu, Wen-Wai Yim, Asma Ben Abacha, Neal Snider, Griffin Adams, Meliha Yetisgen, Johannes Rückert and Alba G. de Herrera
</span>
<span class="ltx_bibblock">“Overview of the ImageCLEF 2023: Multimedia Retrieval in Medical, Social Media and Internet Applications”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx27.1.1" class="ltx_emph ltx_font_italic">International Conference of the Cross-Language Evaluation Forum for European Languages</em>, 2023, pp. 370–396
</span>
<span class="ltx_bibblock">Springer
</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Zhenfeng He, Yuqiang Han, Zhenqiu Ouyang, Wei Gao, Hongxu Chen, Guandong Xu and Jian Wu
</span>
<span class="ltx_bibblock">“DialMed: A Dataset for Dialogue-based Medication Recommendation”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.07094</em>, 2022
</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Usman Naseem, Ajay Bandi, Shaina Raza, Junaid Rashid and Bharathi Raja Chakravarthi
</span>
<span class="ltx_bibblock">“Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st Workshop on Biomedical Language Processing</em>, 2022, pp. 110–115
</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Jane Dacre, Mike Besser and Patricia White
</span>
<span class="ltx_bibblock">“MRCP (UK) PART 2 Clinical Examination (PACES): a review of the first four examination sessions (June 2001–July 2002)”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx30.1.1" class="ltx_emph ltx_font_italic">Clinical Medicine</em> <span id="bib.bibx30.2.2" class="ltx_text ltx_font_bold">3.5</span>
</span>
<span class="ltx_bibblock">Royal College of Physicians, 2003, pp. 452
</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">David A Sloan, Michael B Donnelly, Richard W Schwartz and William E Strodel
</span>
<span class="ltx_bibblock">“The Objective Structured Clinical Examination. The new gold standard for evaluating postgraduate clinical performance.”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx31.1.1" class="ltx_emph ltx_font_italic">Annals of surgery</em> <span id="bib.bibx31.2.2" class="ltx_text ltx_font_bold">222.6</span>
</span>
<span class="ltx_bibblock">Lippincott, Williams,Wilkins, 1995, pp. 735
</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Carol Carraccio and Robert Englander
</span>
<span class="ltx_bibblock">“The objective structured clinical examination: a step in the direction of competency-based evaluation”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx32.1.1" class="ltx_emph ltx_font_italic">Archives of pediatrics &amp; adolescent medicine</em> <span id="bib.bibx32.2.2" class="ltx_text ltx_font_bold">154.7</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2000, pp. 736–741
</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Ronald M Epstein and Edward M Hundert
</span>
<span class="ltx_bibblock">“Defining and assessing professional competence”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx33.1.1" class="ltx_emph ltx_font_italic">Jama</em> <span id="bib.bibx33.2.2" class="ltx_text ltx_font_bold">287.2</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2002, pp. 226–235
</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">Joel L Horowitz
</span>
<span class="ltx_bibblock">“The bootstrap”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx34.1.1" class="ltx_emph ltx_font_italic">Handbook of econometrics</em> <span id="bib.bibx34.2.2" class="ltx_text ltx_font_bold">5</span>
</span>
<span class="ltx_bibblock">Elsevier, 2001, pp. 3159–3228
</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">Yoav Benjamini and Yosef Hochberg
</span>
<span class="ltx_bibblock">“Controlling the false discovery rate: a practical and powerful approach to multiple testing”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx35.1.1" class="ltx_emph ltx_font_italic">Journal of the Royal statistical society: series B (Methodological)</em> <span id="bib.bibx35.2.2" class="ltx_text ltx_font_bold">57.1</span>
</span>
<span class="ltx_bibblock">Wiley Online Library, 1995, pp. 289–300
</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">Robert F Woolson
</span>
<span class="ltx_bibblock">“Wilcoxon signed-rank test”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx36.1.1" class="ltx_emph ltx_font_italic">Wiley encyclopedia of clinical trials</em>
</span>
<span class="ltx_bibblock">Wiley Online Library, 2007, pp. 1–3
</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">Katharina E Keifenheim, Martin Teufel, Julianne Ip, Natalie Speiser, Elisabeth J Leehr, Stephan Zipfel and Anne Herrmann-Werner
</span>
<span class="ltx_bibblock">“Teaching history taking to medical students: a systematic review”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx37.1.1" class="ltx_emph ltx_font_italic">BMC medical education</em> <span id="bib.bibx37.2.2" class="ltx_text ltx_font_bold">15.1</span>
</span>
<span class="ltx_bibblock">BioMed Central, 2015, pp. 1–12
</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">Michael J Yedidia, Colleen C Gillespie, Elizabeth Kachur, Mark D Schwartz, Judith Ockene, Amy E Chepaitis, Clint W Snyder, Aaron Lazare and Mack Lipkin Jr
</span>
<span class="ltx_bibblock">“Effect of communications training on medical student performance”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx38.1.1" class="ltx_emph ltx_font_italic">Jama</em> <span id="bib.bibx38.2.2" class="ltx_text ltx_font_bold">290.9</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2003, pp. 1157–1165
</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">Gregory Makoul
</span>
<span class="ltx_bibblock">“Communication skills education in medical school and beyond”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx39.1.1" class="ltx_emph ltx_font_italic">Jama</em> <span id="bib.bibx39.2.2" class="ltx_text ltx_font_bold">289.1</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2003, pp. 93–93
</span>
</li>
<li id="bib.bibx40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">Xiu Hui Tan, Malia Alexandra Foo, Shaun Li He Lim, Marie Bernadette Xin Yi Lim, Annelissa Mien Chew Chin, Jamie Zhou, Min Chiam and Lalit Kumar Radha Krishna
</span>
<span class="ltx_bibblock">“Teaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx40.1.1" class="ltx_emph ltx_font_italic">BMC medical education</em> <span id="bib.bibx40.2.2" class="ltx_text ltx_font_bold">21</span>
</span>
<span class="ltx_bibblock">Springer, 2021, pp. 1–19
</span>
</li>
<li id="bib.bibx41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">Steven E Raper, Meera Gupta, Olugbenga Okusanya and Jon B Morris
</span>
<span class="ltx_bibblock">“Improving communication skills: a course for academic medical center surgery residents and faculty”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx41.1.1" class="ltx_emph ltx_font_italic">Journal of Surgical education</em> <span id="bib.bibx41.2.2" class="ltx_text ltx_font_bold">72.6</span>
</span>
<span class="ltx_bibblock">Elsevier, 2015, pp. e202–e211
</span>
</li>
<li id="bib.bibx42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">Martin Von Fragstein, Jonathan Silverman, Annie Cushing, Sally Quilligan, Helen Salisbury, Connie Wiskin and UK Council Clinical Communication Skills Teaching in Undergraduate Medical Education
</span>
<span class="ltx_bibblock">“UK consensus statement on the content of communication curricula in undergraduate medical education”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx42.1.1" class="ltx_emph ltx_font_italic">Medical education</em> <span id="bib.bibx42.2.2" class="ltx_text ltx_font_bold">42.11</span>
</span>
<span class="ltx_bibblock">Wiley Online Library, 2008, pp. 1100–1107
</span>
</li>
<li id="bib.bibx43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">Hanneke De Haes and Jozien Bensing
</span>
<span class="ltx_bibblock">“Endpoints in medical communication research, proposing a framework of functions and outcomes”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx43.1.1" class="ltx_emph ltx_font_italic">Patient education and counseling</em> <span id="bib.bibx43.2.2" class="ltx_text ltx_font_bold">74.3</span>
</span>
<span class="ltx_bibblock">Elsevier, 2009, pp. 287–294
</span>
</li>
<li id="bib.bibx44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">Ronald M Epstein and Richard L Street Jr
</span>
<span class="ltx_bibblock">“Patient-centered communication in cancer care: promoting healing and reducing suffering”, 2007
</span>
</li>
<li id="bib.bibx45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">Julie M Schirmer, Larry Mauksch, Forrest Lang, M Kim Marvel, Kathy Zoppi, Ronald M Epstein, Doug Brock and Michael Pryzbylski
</span>
<span class="ltx_bibblock">“Assessing communication competence: a review of current tools”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx45.1.1" class="ltx_emph ltx_font_italic">Family Medicine</em> <span id="bib.bibx45.2.2" class="ltx_text ltx_font_bold">37.3</span>, 2005, pp. 184–92
</span>
</li>
<li id="bib.bibx46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">Jonathan R Nichol, Joshua Henrina Sundjaja and Grant Nelson
</span>
<span class="ltx_bibblock">“Medical history”
</span>
<span class="ltx_bibblock">StatPearls Publishing, Treasure Island (FL), 2018
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://europepmc.org/books/NBK534249" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://europepmc.org/books/NBK534249</a>
</span>
</li>
<li id="bib.bibx47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">Claire Denness
</span>
<span class="ltx_bibblock">“What are consultation models for?”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx47.1.1" class="ltx_emph ltx_font_italic">InnovAiT</em> <span id="bib.bibx47.2.2" class="ltx_text ltx_font_bold">6.9</span>
</span>
<span class="ltx_bibblock">Sage Publications Sage UK: London, England, 2013, pp. 592–599
</span>
</li>
<li id="bib.bibx48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">See Chai Carol Chan, George Choa, James Kelly, Devina Maru and Mohammed Ahmed Rashid
</span>
<span class="ltx_bibblock">“Implementation of virtual OSCE in health professions education: A systematic review”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx48.1.1" class="ltx_emph ltx_font_italic">Medical Education</em>
</span>
<span class="ltx_bibblock">Wiley Online Library, 2023
</span>
</li>
<li id="bib.bibx49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan and Milica Gašić
</span>
<span class="ltx_bibblock">“Multiwoz–a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.00278</em>, 2018
</span>
</li>
<li id="bib.bibx50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">Wei Wei, Quoc Le, Andrew Dai and Jia Li
</span>
<span class="ltx_bibblock">“Airdialogue: An environment for goal-oriented dialogue research”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, 2018, pp. 3844–3854
</span>
</li>
<li id="bib.bibx51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">Jessy Lin, Nicholas Tomlin, Jacob Andreas and Jason Eisner
</span>
<span class="ltx_bibblock">“Decision-Oriented Dialogue for Human-AI Collaboration”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2305.20076" title="" class="ltx_ref ltx_href">2305.20076 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser and Illia Polosukhin
</span>
<span class="ltx_bibblock">“Attention is all you need”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx52.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> <span id="bib.bibx52.2.2" class="ltx_text ltx_font_bold">30</span>, 2017
</span>
</li>
<li id="bib.bibx53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama and Alex Ray
</span>
<span class="ltx_bibblock">“Training language models to follow instructions with human feedback”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.02155</em>, 2022
</span>
</li>
<li id="bib.bibx54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">Jieyu Zhao, Daniel Khashabi, Tushar Khot, Ashish Sabharwal and Kai-Wei Chang
</span>
<span class="ltx_bibblock">“Ethical-advice taker: Do language models understand natural language interventions?”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.01465</em>, 2021
</span>
</li>
<li id="bib.bibx55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward and Jan Leike
</span>
<span class="ltx_bibblock">“Self-critiquing models for assisting human evaluators”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx55.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.05802</em>, 2022
</span>
</li>
<li id="bib.bibx56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho and Ethan Perez
</span>
<span class="ltx_bibblock">“Training language models with language feedback at scale”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.16755</em>, 2023
</span>
</li>
<li id="bib.bibx57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick and Phoebe Thacker
</span>
<span class="ltx_bibblock">“Improving alignment of dialogue agents via targeted human judgements”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.14375</em>, 2022
</span>
</li>
<li id="bib.bibx58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini and Cameron McKinnon
</span>
<span class="ltx_bibblock">“Constitutional AI: Harmlessness from AI feedback”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.08073</em>, 2022
</span>
</li>
<li id="bib.bibx59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann and Nova DasSarma
</span>
<span class="ltx_bibblock">“A general language assistant as a laboratory for alignment”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.00861</em>, 2021
</span>
</li>
<li id="bib.bibx60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">Joel Shor, Ruyue Agnes Bi, Subhashini Venugopalan, Steven Ibara, Roman Goldenberg and Ehud Rivlen
</span>
<span class="ltx_bibblock">“Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.05737</em>, 2023
</span>
</li>
<li id="bib.bibx61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">Asma Ben Abacha, Eugene Agichtein, Yuval Pinter and Dina Demner-Fushman
</span>
<span class="ltx_bibblock">“Overview of the medical question answering task at TREC 2017 LiveQA.”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx61.1.1" class="ltx_emph ltx_font_italic">TREC</em>, 2017, pp. 1–12
</span>
</li>
<li id="bib.bibx62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">William Wallace, Calvin Chan, Swathikan Chidambaram, Lydia Hanna, Fahad Mujtaba Iqbal, Amish Acharya, Pasha Normahani, Hutan Ashrafian, Sheraz R Markar and Viknesh Sounderajah
</span>
<span class="ltx_bibblock">“The diagnostic and triage accuracy of digital and online symptom checker tools: a systematic review”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx62.1.1" class="ltx_emph ltx_font_italic">NPJ Digital Medicine</em> <span id="bib.bibx62.2.2" class="ltx_text ltx_font_bold">5.1</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group UK London, 2022, pp. 118
</span>
</li>
<li id="bib.bibx63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">Dan Zeltzer, Lee Herzog, Yishai Pickman, Yael Steuerman, Ran Ilan Ber, Zehavi Kugler, Ran Shaul and Jon O Ebbert
</span>
<span class="ltx_bibblock">“Diagnostic accuracy of artificial intelligence in virtual primary care”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx63.1.1" class="ltx_emph ltx_font_italic">Mayo Clinic Proceedings: Digital Health</em> <span id="bib.bibx63.2.2" class="ltx_text ltx_font_bold">1.4</span>
</span>
<span class="ltx_bibblock">Elsevier, 2023, pp. 480–489
</span>
</li>
<li id="bib.bibx64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">Shreya Johri, Jaehwan Jeong, Benjamin A Tran, Daniel I Schlessinger, Shannon Wongvibulsin, Zhuo Ran Cai, Roxana Daneshjou and Pranav Rajpurkar
</span>
<span class="ltx_bibblock">“Testing the Limits of Language Models: A Conversational Framework for Medical AI Assessment”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx64.1.1" class="ltx_emph ltx_font_italic">medRxiv</em>
</span>
<span class="ltx_bibblock">Cold Spring Harbor Laboratory Press, 2023, pp. 2023–09
</span>
</li>
<li id="bib.bibx65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong and Ruoyu Zhang
</span>
<span class="ltx_bibblock">“MedDialog: Large-scale medical dialogue datasets”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx65.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2020, pp. 9241–9250
</span>
</li>
<li id="bib.bibx66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">Wenge Liu, Jianheng Tang, Yi Cheng, Wenjie Li, Yefeng Zheng and Xiaodan Liang
</span>
<span class="ltx_bibblock">“MedDG: an entity-centric medical consultation dataset for entity-aware medical dialogue generation”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx66.1.1" class="ltx_emph ltx_font_italic">CCF International Conference on Natural Language Processing and Chinese Computing</em>, 2022, pp. 447–459
</span>
<span class="ltx_bibblock">Springer
</span>
</li>
<li id="bib.bibx67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">Deeksha Varshney, Aizan Zafar, Niranshu Kumar Behra and Asif Ekbal
</span>
<span class="ltx_bibblock">“Cdialog: A multi-turn COVID-19 conversation dataset for entity-aware dialog generation”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.06049</em>, 2022
</span>
</li>
<li id="bib.bibx68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">Guojun Yan, Jiahuan Pei, Pengjie Ren, Zhaochun Ren, Xin Xin, Huasheng Liang, Maarten Rijke and Zhumin Chen
</span>
<span class="ltx_bibblock">“ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx68.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 2022, pp. 3013–3024
</span>
</li>
<li id="bib.bibx69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">Christopher J Kelly, Alan Karthikesalingam, Mustafa Suleyman, Greg Corrado and Dominic King
</span>
<span class="ltx_bibblock">“Key challenges for delivering clinical impact with artificial intelligence”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx69.1.1" class="ltx_emph ltx_font_italic">BMC medicine</em> <span id="bib.bibx69.2.2" class="ltx_text ltx_font_bold">17</span>
</span>
<span class="ltx_bibblock">Springer, 2019, pp. 1–9
</span>
</li>
<li id="bib.bibx70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">Daniel McDuff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, Yash Sharma, Shekoofeh Azizi and Kavita Kulkarni
</span>
<span class="ltx_bibblock">“Towards Accurate Differential Diagnosis with Large Language Models”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx70.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.00164</em>, 2023
</span>
</li>
<li id="bib.bibx71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">Zahir Kanjee, Byron Crowe and Adam Rodman
</span>
<span class="ltx_bibblock">“Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx71.1.1" class="ltx_emph ltx_font_italic">JAMA</em>, 2023
</span>
</li>
<li id="bib.bibx72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">Hannah L Semigran, Jeffrey A Linder, Courtney Gidengil and Ateev Mehrotra
</span>
<span class="ltx_bibblock">“Evaluation of symptom checkers for self diagnosis and triage: audit study”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx72.1.1" class="ltx_emph ltx_font_italic">BMJ</em> <span id="bib.bibx72.2.2" class="ltx_text ltx_font_bold">351</span>
</span>
<span class="ltx_bibblock">British Medical Journal Publishing Group, 2015
</span>
</li>
<li id="bib.bibx73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">John W Ayers, Adam Poliak, Mark Dredze, Eric C Leas, Zechariah Zhu, Jessica B Kelley, Dennis J Faix, Aaron M Goodman, Christopher A Longhurst and Michael Hogarth
</span>
<span class="ltx_bibblock">“Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx73.1.1" class="ltx_emph ltx_font_italic">JAMA Internal Medicine</em>, 2023
</span>
</li>
<li id="bib.bibx74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">“ChatGPT”, 2023
</span>
<span class="ltx_bibblock">OpenAI
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://chat.openai.com/chat" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://chat.openai.com/chat</a>
</span>
</li>
<li id="bib.bibx75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">Sara Carrillo de Albornoz, Kah-Ling Sia and Anthony Harris
</span>
<span class="ltx_bibblock">“The effectiveness of teleconsultations in primary care: systematic review”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx75.1.1" class="ltx_emph ltx_font_italic">Family Practice</em> <span id="bib.bibx75.2.2" class="ltx_text ltx_font_bold">39.1</span>
</span>
<span class="ltx_bibblock">Oxford University Press UK, 2022, pp. 168–182
</span>
</li>
<li id="bib.bibx76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">George A Wharton, Harpreet S Sood, Amanda Sissons and Elias Mossialos
</span>
<span class="ltx_bibblock">“Virtual primary care: fragmentation or integration?”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx76.1.1" class="ltx_emph ltx_font_italic">The Lancet Digital Health</em> <span id="bib.bibx76.2.2" class="ltx_text ltx_font_bold">1.7</span>
</span>
<span class="ltx_bibblock">Elsevier, 2019, pp. e330–e331
</span>
</li>
<li id="bib.bibx77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">Aı̈na Fuster-Casanovas and Josep Vidal-Alaball
</span>
<span class="ltx_bibblock">“Asynchronous Remote Communication as a Tool for Care Management in Primary Care: A Rapid Review of the Literature”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx77.1.1" class="ltx_emph ltx_font_italic">International Journal of Integrated Care</em> <span id="bib.bibx77.2.2" class="ltx_text ltx_font_bold">22.3</span>
</span>
<span class="ltx_bibblock">Ubiquity Press, 2022
</span>
</li>
<li id="bib.bibx78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">Victoria Hammersley, Eddie Donaghy, Richard Parker, Hannah McNeilly, Helen Atherton, Annemieke Bikker, John Campbell and Brian McKinstry
</span>
<span class="ltx_bibblock">“Comparing the content and quality of video, telephone, and face-to-face consultations: a non-randomised, quasi-experimental, exploratory study in UK primary care”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx78.1.1" class="ltx_emph ltx_font_italic">British Journal of General Practice</em> <span id="bib.bibx78.2.2" class="ltx_text ltx_font_bold">69.686</span>
</span>
<span class="ltx_bibblock">British Journal of General Practice, 2019, pp. e595–e604
</span>
</li>
<li id="bib.bibx79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">David A Gross, Stephen J Zyzanski, Elaine A Borawski, Randall D Cebul and Kurt C Stange
</span>
<span class="ltx_bibblock">“Patient satisfaction with time spent with their physician”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx79.1.1" class="ltx_emph ltx_font_italic">Journal of Family Practice</em> <span id="bib.bibx79.2.2" class="ltx_text ltx_font_bold">47.2</span>
</span>
<span class="ltx_bibblock">[New York, Appleton-Century-Crofts], 1998, pp. 133–138
</span>
</li>
<li id="bib.bibx80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">Kiek Tates, Marjolijn L Antheunis, Saskia Kanters, Theodoor E Nieboer and Maria BE Gerritse
</span>
<span class="ltx_bibblock">“The effect of screen-to-screen versus face-to-face consultation on doctor-patient communication: an experimental study with simulated patients”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx80.1.1" class="ltx_emph ltx_font_italic">Journal of medical Internet research</em> <span id="bib.bibx80.2.2" class="ltx_text ltx_font_bold">19.12</span>
</span>
<span class="ltx_bibblock">JMIR Publications Toronto, Canada, 2017, pp. e421
</span>
</li>
<li id="bib.bibx81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">Stephen J Zyzanski, Kurt C Stange, Doreen M Langa and Susan A Flocke
</span>
<span class="ltx_bibblock">“Trade-offs in high-volume primary care practice”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx81.1.1" class="ltx_emph ltx_font_italic">Journal of Family Practice</em> <span id="bib.bibx81.2.2" class="ltx_text ltx_font_bold">46.5</span>
</span>
<span class="ltx_bibblock">[New York, Appleton-Century-Crofts], 1998, pp. 397–402
</span>
</li>
<li id="bib.bibx82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">Krishnamurthy Dvijotham, Jim Winkens, Melih Barsbey, Sumedh Ghaisas, Robert Stanforth, Nick Pawlowski, Patricia Strachan, Zahra Ahmed, Shekoofeh Azizi and Yoram Bachrach
</span>
<span class="ltx_bibblock">“Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx82.1.1" class="ltx_emph ltx_font_italic">Nature Medicine</em> <span id="bib.bibx82.2.2" class="ltx_text ltx_font_bold">29.7</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group US New York, 2023, pp. 1814–1820
</span>
</li>
<li id="bib.bibx83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">Julian Bird and Steven A Cohen-Cole
</span>
<span class="ltx_bibblock">“The three-function model of the medical interview”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx83.1.1" class="ltx_emph ltx_font_italic">Methods in teaching consultation-liaison psychiatry</em> <span id="bib.bibx83.2.2" class="ltx_text ltx_font_bold">20</span>
</span>
<span class="ltx_bibblock">Karger Publishers, 1990, pp. 65–88
</span>
</li>
<li id="bib.bibx84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">Agnes G Rezler, James A Woolliscroft and Summers G Kalishman
</span>
<span class="ltx_bibblock">“What is missing from patient histories?”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx84.1.1" class="ltx_emph ltx_font_italic">Medical Teacher</em> <span id="bib.bibx84.2.2" class="ltx_text ltx_font_bold">13.3</span>
</span>
<span class="ltx_bibblock">Taylor &amp; Francis, 1991, pp. 245–252
</span>
</li>
<li id="bib.bibx85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">Ellen E Rosenberg
</span>
<span class="ltx_bibblock">“Lessons for Clinicians From Physician-Patient”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx85.1.1" class="ltx_emph ltx_font_italic">Arch Fam Med</em> <span id="bib.bibx85.2.2" class="ltx_text ltx_font_bold">6</span>, 1997, pp. 279–283
</span>
</li>
<li id="bib.bibx86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">Robert Charles Smith
</span>
<span class="ltx_bibblock">“Patient-centered interviewing: an evidence-based method”
</span>
<span class="ltx_bibblock">Lippincott Williams &amp; Wilkins, 2002
</span>
</li>
<li id="bib.bibx87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">Donald M Berwick, Thomas W Nolan and John Whittington
</span>
<span class="ltx_bibblock">“The triple aim: care, health, and cost”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx87.1.1" class="ltx_emph ltx_font_italic">Health affairs</em> <span id="bib.bibx87.2.2" class="ltx_text ltx_font_bold">27.3</span>
</span>
<span class="ltx_bibblock">Project HOPE-The People-to-People Health Foundation, Inc., 2008, pp. 759–769
</span>
</li>
<li id="bib.bibx88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">Thomas Bodenheimer and Christine Sinsky
</span>
<span class="ltx_bibblock">“From triple to quadruple aim: care of the patient requires care of the provider”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx88.1.1" class="ltx_emph ltx_font_italic">The Annals of Family Medicine</em> <span id="bib.bibx88.2.2" class="ltx_text ltx_font_bold">12.6</span>
</span>
<span class="ltx_bibblock">Annals Family Med, 2014, pp. 573–576
</span>
</li>
<li id="bib.bibx89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">T Elaine Adamson, Jeane M Tschann, DS Gullion and AA Oppenberg
</span>
<span class="ltx_bibblock">“Physician communication skills and malpractice claims. A complex relationship.”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx89.1.1" class="ltx_emph ltx_font_italic">Western Journal of Medicine</em> <span id="bib.bibx89.2.2" class="ltx_text ltx_font_bold">150.3</span>
</span>
<span class="ltx_bibblock">BMJ Publishing Group, 1989, pp. 356
</span>
</li>
<li id="bib.bibx90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">Jonathan Silverman and Paul Kinnersley
</span>
<span class="ltx_bibblock">“Doctors’ non-verbal behaviour in consultations: look at the patient before you look at the computer”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx90.1.1" class="ltx_emph ltx_font_italic">British Journal of General Practice</em> <span id="bib.bibx90.2.2" class="ltx_text ltx_font_bold">60.571</span>
</span>
<span class="ltx_bibblock">British Journal of General Practice, 2010, pp. 76–78
</span>
</li>
<li id="bib.bibx91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">Urna Rahman and Nick Cooling
</span>
<span class="ltx_bibblock">“Inter-Cultural Communication Skills Training in Medical Schools: A Systematic Review”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx91.1.1" class="ltx_emph ltx_font_italic">Medical Research Archives</em> <span id="bib.bibx91.2.2" class="ltx_text ltx_font_bold">11.4</span>, 2023
</span>
</li>
<li id="bib.bibx92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">Ahmad Kantar, Julie M Marchant, Woo-Jung Song, Michael D Shields, Grigorios Chatziparasidis, Angela Zacharasiewicz, Alexander Moeller and Anne B Chang
</span>
<span class="ltx_bibblock">“History taking as a diagnostic tool in children with chronic cough”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx92.1.1" class="ltx_emph ltx_font_italic">Frontiers in pediatrics</em> <span id="bib.bibx92.2.2" class="ltx_text ltx_font_bold">10</span>
</span>
<span class="ltx_bibblock">Frontiers, 2022, pp. 850912
</span>
</li>
<li id="bib.bibx93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">Winny Setyonugroho, Kieran M Kennedy and Thomas JB Kropmans
</span>
<span class="ltx_bibblock">“Reliability and validity of OSCE checklists used to assess the communication skills of undergraduate medical students: a systematic review”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx93.1.1" class="ltx_emph ltx_font_italic">Patient education and counseling</em> <span id="bib.bibx93.2.2" class="ltx_text ltx_font_bold">98.12</span>
</span>
<span class="ltx_bibblock">Elsevier, 2015, pp. 1482–1491
</span>
</li>
<li id="bib.bibx94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle and Atoosa Kasirzadeh
</span>
<span class="ltx_bibblock">“Taxonomy of risks posed by language models”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx94.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2022, pp. 214–229
</span>
</li>
<li id="bib.bibx95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang and Nesreen K. Ahmed
</span>
<span class="ltx_bibblock">“Bias and Fairness in Large Language Models: A Survey”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2309.00770" title="" class="ltx_ref ltx_href">2309.00770 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">Rachel L Johnson, Debra Roter, Neil R Powe and Lisa A Cooper
</span>
<span class="ltx_bibblock">“Patient race/ethnicity and quality of patient–physician communication during medical visits”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx96.1.1" class="ltx_emph ltx_font_italic">American journal of public health</em> <span id="bib.bibx96.2.2" class="ltx_text ltx_font_bold">94.12</span>
</span>
<span class="ltx_bibblock">American Public Health Association, 2004, pp. 2084–2090
</span>
</li>
<li id="bib.bibx97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">Debra L Roter, Judith A Hall and Yutaka Aoki
</span>
<span class="ltx_bibblock">“Physician gender effects in medical communication: a meta-analytic review”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx97.1.1" class="ltx_emph ltx_font_italic">Jama</em> <span id="bib.bibx97.2.2" class="ltx_text ltx_font_bold">288.6</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2002, pp. 756–764
</span>
</li>
<li id="bib.bibx98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese and Geoffrey Irving
</span>
<span class="ltx_bibblock">“Red teaming language models with language models”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx98.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.03286</em>, 2022
</span>
</li>
<li id="bib.bibx99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer and Kamal Ndousse
</span>
<span class="ltx_bibblock">“Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx99.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.07858</em>, 2022
</span>
</li>
<li id="bib.bibx100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">Jiahao Yu, Xingwei Lin and Xinyu Xing
</span>
<span class="ltx_bibblock">“Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx100.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10253</em>, 2023
</span>
</li>
<li id="bib.bibx101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">Suyu Ge, Chunting Zhou, Rui Hou, Madian Khabsa, Yi-Chia Wang, Qifan Wang, Jiawei Han and Yuning Mao
</span>
<span class="ltx_bibblock">“MART: Improving LLM Safety with Multi-round Automatic Red-Teaming”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx101.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.07689</em>, 2023
</span>
</li>
<li id="bib.bibx102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji and Timnit Gebru
</span>
<span class="ltx_bibblock">“Model cards for model reporting”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx102.1.1" class="ltx_emph ltx_font_italic">Proceedings of the conference on fairness, accountability, and transparency</em>, 2019, pp. 220–229
</span>
</li>
<li id="bib.bibx103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">Anamaria Crisan, Margaret Drouhard, Jesse Vig and Nazneen Rajani
</span>
<span class="ltx_bibblock">“Interactive model cards: A human-centered approach to model documentation”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx103.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2022, pp. 427–439
</span>
</li>
<li id="bib.bibx104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">Mahima Pushkarna, Andrew Zaldivar and Oddur Kjartansson
</span>
<span class="ltx_bibblock">“Data cards: Purposeful and transparent dataset documentation for responsible ai”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx104.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2022, pp. 1776–1826
</span>
</li>
<li id="bib.bibx105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">Monojit Choudhury and Amit Deshpande
</span>
<span class="ltx_bibblock">“How Linguistically Fair Are Multilingual Pre-Trained Language Models?”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx105.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em> <span id="bib.bibx105.2.2" class="ltx_text ltx_font_bold">35.14</span>, 2021, pp. 12710–12718
</span>
</li>
<li id="bib.bibx106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">Zeerak Talat, Aurélie Névéol, Stella Biderman, Miruna Clinciu, Manan Dey, Shayne Longpre, Sasha Luccioni, Maraim Masoud, Margaret Mitchell and Dragomir Radev
</span>
<span class="ltx_bibblock">“You reap what you sow: On the challenges of bias evaluation under multilingual settings”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx106.1.1" class="ltx_emph ltx_font_italic">Proceedings of BigScience Episode# 5–Workshop on Challenges &amp; Perspectives in Creating Large Language Models</em>, 2022, pp. 26–41
</span>
</li>
<li id="bib.bibx107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali and Sunayana Sitaram
</span>
<span class="ltx_bibblock">“MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2311.07463" title="" class="ltx_ref ltx_href">2311.07463 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">Ayyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, André Martins, François Yvon and Hinrich Schütze
</span>
<span class="ltx_bibblock">“Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx108.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
</span>
<span class="ltx_bibblock">Association for Computational Linguistics, 2023
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.18653/v1/2023.acl-long.61" title="" class="ltx_ref ltx_href">10.18653/v1/2023.acl-long.61</a>
</span>
</li>
<li id="bib.bibx109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">Xuan-Phi Nguyen, Sharifah Mahani Aljunied, Shafiq Joty and Lidong Bing
</span>
<span class="ltx_bibblock">“Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2306.11372" title="" class="ltx_ref ltx_href">2306.11372 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">Tarek Naous, Michael J. Ryan, Alan Ritter and Wei Xu
</span>
<span class="ltx_bibblock">“Having Beer after Prayer? Measuring Cultural Bias in Large Language Models”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2305.14456" title="" class="ltx_ref ltx_href">2305.14456 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">Krithika Ramesh, Sunayana Sitaram and Monojit Choudhury
</span>
<span class="ltx_bibblock">“Fairness in Language Models Beyond English: Gaps and Challenges”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2302.12578" title="" class="ltx_ref ltx_href">2302.12578 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">Rishav Hada, Varun Gumma, Adrian Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali and Sunayana Sitaram
</span>
<span class="ltx_bibblock">“Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2309.07462" title="" class="ltx_ref ltx_href">2309.07462 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S. Jaakkola and Regina Barzilay
</span>
<span class="ltx_bibblock">“Conformal Language Modeling”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2306.10193" title="" class="ltx_ref ltx_href">2306.10193 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">Jiuhai Chen and Jonas Mueller
</span>
<span class="ltx_bibblock">“Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2308.16175" title="" class="ltx_ref ltx_href">2308.16175 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu and Lei Ma
</span>
<span class="ltx_bibblock">“Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2307.10236" title="" class="ltx_ref ltx_href">2307.10236 [cs.SE]</a>
</span>
</li>
<li id="bib.bibx116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">Qi Yang, Shreya Ravikumar, Fynn Schmitt-Ulms, Satvik Lolla, Ege Demir, Iaroslav Elistratov, Alex Lavaee, Sadhana Lolla, Elaheh Ahmadi, Daniela Rus, Alexander Amini and Alejandro Perez
</span>
<span class="ltx_bibblock">“Uncertainty-aware Language Modeling for Selective Question Answering”, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2311.15451" title="" class="ltx_ref ltx_href">2311.15451 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien Masson d’Autume, Tomas Kocisky and Sebastian Ruder
</span>
<span class="ltx_bibblock">“Mind the gap: Assessing temporal generalization in neural language models”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx117.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> <span id="bib.bibx117.2.2" class="ltx_text ltx_font_bold">34</span>, 2021, pp. 29348–29363
</span>
</li>
</ul>
</section>
<section id="biba" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="biba.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le and Denny Zhou
</span>
<span class="ltx_bibblock">“Chain-of-thought prompting elicits reasoning in large language models”
</span>
<span class="ltx_bibblock">In <em id="biba.bibx1.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> <span id="biba.bibx1.2.2" class="ltx_text ltx_font_bold">35</span>, 2022, pp. 24824–24837
</span>
</li>
<li id="biba.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock"> Google
</span>
<span class="ltx_bibblock">“PaLM 2 Technical Report”, <a target="_blank" href="https://ai.google/static/documents/palm2techreport.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.google/static/documents/palm2techreport.pdf</a>, 2023
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para">
<p class="ltx_p" id="p1.1"><span class="ltx_text" id="p1.1.1" lang="en">appendix</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2401.05652" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2401.05654" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2401.05654">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.05654" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2401.05655" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Feb  9 13:12:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
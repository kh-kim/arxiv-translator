<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.05654] Towards Conversational Diagnostic AI</title><meta property="og:description" content="At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable o‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Conversational Diagnostic AI">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Conversational Diagnostic AI">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.05654">

<!--Generated on Fri Feb  9 13:12:18 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Conversational Diagnostic AI</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tao Tu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anil Palepu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mike Schaekermann
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Khaled Saab
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jan Freyberg
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ryutaro Tanno
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amy Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Brenna Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohamed Amin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Nenad Tomasev
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shekoofeh Azizi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Karan Singhal
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yong Cheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Le Hou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Albert Webson
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Kavita Kulkarni
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">S. Sara Mahdavi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christopher Semturs
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Juraj Gottweis
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joelle Barral
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google DeepMind
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Katherine Chou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Greg S. Corrado
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yossi Matias
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Alan Karthikesalingam
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vivek Natarajan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span>
<span class="ltx_contact ltx_role_affiliation">Google Research, 
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="1.1" class="ltx_p"><span id="1.1.1" class="ltx_text" lang="en">At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians‚Äô expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.</span></p>
<p id="2.2" class="ltx_p"><span id="2.2.1" class="ltx_text" lang="en">AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE‚Äôs performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.</span></p>
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The dialogue between the physician and the patient is fundamental to effective and compassionate care. The medical interview has been termed ‚Äúthe most powerful, sensitive, and most versatile instrument available to the physician‚Äù&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>. In some settings, it is believed that 60-80% of diagnoses are made through clinical history-taking alone&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>, <a href="#bib.bibx3" title="" class="ltx_ref">3</a>, <a href="#bib.bibx4" title="" class="ltx_ref">4</a>, <a href="#bib.bibx5" title="" class="ltx_ref">5</a>, <a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>. The physician-patient dialogue extends beyond history-taking and diagnosis; it is a complex interaction which establishes rapport and trust, serves as a tool for addressing health needs and can empower patients to make informed decisions that account for their preferences, expectations, and concerns&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">7</a>]</cite>. Clinicians wield considerable skills in clinical history-taking and the wider ‚Äúdiagnostic dialogue‚Äù, but access to this expertise remains episodic and globally scarce&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recent progress in general-purpose large language models (LLMs)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>, <a href="#biba.bibx2" title="" class="ltx_ref">119</a>, <a href="#bib.bibx11" title="" class="ltx_ref">11</a>]</cite> has shown that artificial intelligence (AI) systems have capabilities to plan, reason, and incorporate relevant context to hold naturalistic conversations. This progress affords an opportunity to rethink the possibilities of AI in medicine towards the development of fully interactive conversational AI. Such medical AI systems would understand clinical language, intelligently acquire information under uncertainty, and engage in natural, diagnostically useful medical conversations with patients and those who care for them. The potential real-world utility of AI systems capable of clinical and diagnostic dialogue is broad, as the development of such capabilities might improve access to diagnostic and prognostic expertise, to improved quality, consistency, availability, and affordability of care, and to help realize better health outcomes (particularly for populations facing healthcare disparities).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2401.05654/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="367" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Overview of contributions.<span id="S1.F1.4.2.1" class="ltx_text ltx_font_medium"> AMIE is a conversational medical AI optimised for diagnostic dialogue. AMIE is instruction fine-tuned with a combination of real-world and simulated medical dialogues, alongside a diverse set of medical reasoning, question answering, and summarization datasets. Notably, we designed a self-play based simulated dialogue environment with automated feedback mechanisms to scale AMIE‚Äôs capabilities across various medical contexts and specialities. Specifically, this iterative self-improvement process consisted of two self-play loops: (1) An ‚Äúinner‚Äù self-play loop, where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient agent; (2) An ‚Äúouter‚Äù self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. During online inference, AMIE used a chain-of-reasoning strategy to progressively refine its response conditioned on the current conversation to arrive at an accurate and grounded reply to the patient in each dialogue turn.
We designed and conducted a blinded remote Objective Structured Clinical Examination (OSCE) with validated simulated patient actors interacting with AMIE or Primary Care Physicians (PCPs) via a text interface. Across multiple axes corresponding to both specialist physician (28 out of 32) and patient actor (24 out of 26) perspective, AMIE was rated as superior to PCPs while being non-inferior on the rest.</span></span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, while LLMs have been shown to encode clinical knowledge and proven capable of highly accurate single-turn medical question-answering&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>, <a href="#bib.bibx13" title="" class="ltx_ref">13</a>, <a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite>, their conversational capabilities have been tailored to domains outside clinical medicine&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>, <a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>. Prior work in LLMs for health &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>, <a href="#bib.bibx13" title="" class="ltx_ref">13</a>, <a href="#bib.bibx14" title="" class="ltx_ref">14</a>, <a href="#bib.bibx17" title="" class="ltx_ref">17</a>, <a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite> has not yet rigorously examined the clinical history-taking and diagnostic dialogue capabilities of AI systems or contextualized this by comparison to the extensive capabilities of expert clinicians.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Clinical history-taking and diagnostic dialogue through which clinicians derive diagnosis and management plans represent a complex skill&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite> whose optimal conduct is highly dependent on context. Thus, multiple evaluation axes are needed to assess the quality of a diagnostic dialogue, including the structure and completeness of the elicited history, diagnostic accuracy, the appropriateness of management plans and their rationale, and patient-centred considerations such as relationship-building, respect for the individual and communication efficacy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite>. If the conversational potential of LLMs is to be realized in medicine, there is a significant unmet need to better optimize development and evaluation of medical AI systems for characteristics such as these, which are unique to history-taking and diagnostic dialogue between clinicians and patients.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this work, we detail our progress towards a conversational medical AI system for clinical history-taking and diagnostic reasoning.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our key contributions are summarized as:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduced AMIE (Articulate Medical Intelligence Explorer), an LLM based AI system optimized for clinical history-taking and diagnostic dialogue.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">To scale AMIE across a multitude of specialties and scenarios, we developed a novel self-play based simulated diagnostic dialogue environment with automated feedback mechanisms to enrich and accelerate its learning process. We also introduced an inference time chain-of-reasoning strategy to improve AMIE‚Äôs diagnostic accuracy and conversation quality.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We developed a pilot evaluation rubric to assess the history-taking, diagnostic reasoning, communication skills and empathy of diagnostic conversational medical AI, encompassing both clinician-centred and patient-centred metrics.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We designed and conducted a blinded remote OSCE study with 149 case scenarios from clinical providers in Canada, the UK, and India, enabling randomized and counterbalanced comparison of AMIE to PCPs when performing consultations with validated patient actors. AMIE exhibited superior diagnostic accuracy compared to PCPs as assessed by various measures (e.g., top-1 and top-3 accuracy of the differential diagnosis list). Across 28 out of 32 evaluation axes from the specialist physician perspective and 24 out of 26 evaluation axes from the patient actor perspective, AMIE was rated superior to PCPs while being non-inferior on the rest.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">We performed a range of ablations to further understand and characterize the capabilities of AMIE, highlighted important limitations, and proposed key next steps for real-world clinical translation of AMIE.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Our research has important limitations, most notably that we utilized a text-chat interface, which although enabling potentially large-scale interaction between patients and LLMs specialized for diagnostic dialogue, was unfamiliar to PCPs for remote consultation. Thus our study should not be regarded as representative of usual practice in (tele)medicine.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2401.05654/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Overview of randomized study design.<span id="S1.F2.4.2.1" class="ltx_text ltx_font_medium"> A primary care physician (PCP) and AMIE perform (in a randomized order) a virtual remote Objective Structured Clinical Examination (OSCE) with simulated patients via online multi-turn synchronous text chat and produce answers to a post-questionnaire. Both the PCP and AMIE are then evaluated by both the patient actors as well as specialist physicians.</span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>AMIE: An LLM based AI System for Diagnostic Dialogue</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In the following sections, we describe the real-world datasets, simulated self-play environment, fine-tuning process, and inference time chain-of-reasoning that we designed to optimize AMIE for diagnostic conversation capabilities and clinical communication skills.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Real-world Datasets for AMIE</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">AMIE was developed using a diverse suite of real-world datasets including multiple-choice medical question-answering, expert-curated long-form medical reasoning, electronic health record (EHR) note summaries, and large-scale transcribed medical conversation interactions. As described in detail below, in addition to dialogue generation tasks, the training task mixture for AMIE consisted of medical question-answering, reasoning, and summarization tasks.</p>
</div>
<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Medical Reasoning.</h5>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px1.p1.1" class="ltx_p">We used the MedQA (multiple-choice) dataset consisting of US Medical Licensing Examination (USMLE) multiple-choice style open domain questions with four or five possible answers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">21</a>]</cite>. The training set consisted of 11,450 questions and the test set had 1,273 questions. We also curated 191 MedQA questions from the training set where clinical experts crafted step-by-step reasoning leading to the correct answer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Long-form Medical Question Answering.</h5>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p1.1" class="ltx_p">The dataset used here consisted of expert-crafted long-form responses to 64 questions from HealthSearchQA, LiveQA, and Medication QA in MultiMedBench&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Medical Summarization.</h5>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px3.p1.1" class="ltx_p">A dataset consisting of 65 clinician-written summaries of medical notes from MIMIC-III, a large, publicly available database containing medical records of intensive care unit patients&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">22</a>]</cite>, was used as additional training data for AMIE. MIMIC-III contains approximately 2 million notes spanning 13 types including cardiology, respiratory, radiology, physician, general, discharge, case management, consult, nursing, pharmacy, nutrition, rehabilitation and social work. 5 notes from each category were selected, with a minimum total length of 400 tokens and at least one nursing note per patient. Clinicians were instructed to write abstractive summaries of individual medical notes, capturing key information while also permitting the inclusion of new informative and clarifying phrases and sentences not present in the original note.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Real-world Dialogue.</h5>

<div id="S2.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px4.p1.2" class="ltx_p">Here, we used a de-identified dataset licensed from a dialogue research organisation comprising 98,919 audio transcripts of medical conversations during in-person clinical visits from over 1,000 clinicians over a 10-year period in the United States&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite>. It covered 51 medical specialties (primary care, rheumatology, hematology, oncology, internal medicine and psychiatry among others) and 168 medical conditions and visit reasons (type II diabetes, rheumatoid arthritis, asthma, depression among the common conditions). Audio transcripts contained utterances from different speaker roles such as doctors, patients, and nurses. On average a conversation had 149.8 turns (<math id="S2.SS1.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="P_{0.25}=75.0" display="inline"><semantics id="S2.SS1.SSS0.Px4.p1.1.m1.1a"><mrow id="S2.SS1.SSS0.Px4.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.cmml"><msub id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml"><mi id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2.cmml">P</mi><mn id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3.cmml">0.25</mn></msub><mo id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml">75.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px4.p1.1.m1.1b"><apply id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1"><eq id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.1"></eq><apply id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.2">ùëÉ</ci><cn type="float" id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.2.3">0.25</cn></apply><cn type="float" id="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS0.Px4.p1.1.m1.1.1.3">75.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px4.p1.1.m1.1c">P_{0.25}=75.0</annotation></semantics></math>, <math id="S2.SS1.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="P_{0.75}=196.0" display="inline"><semantics id="S2.SS1.SSS0.Px4.p1.2.m2.1a"><mrow id="S2.SS1.SSS0.Px4.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.cmml"><msub id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml"><mi id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2.cmml">P</mi><mn id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3.cmml">0.75</mn></msub><mo id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml">196.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px4.p1.2.m2.1b"><apply id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1"><eq id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.1"></eq><apply id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.2">ùëÉ</ci><cn type="float" id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.2.3">0.75</cn></apply><cn type="float" id="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S2.SS1.SSS0.Px4.p1.2.m2.1.1.3">196.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px4.p1.2.m2.1c">P_{0.75}=196.0</annotation></semantics></math>). For each conversation, the metadata contained information about patient demographics, reason for the visit (follow-up for pre-existing condition, acute needs, annual exam and more), and diagnosis type (new, existing or other unrelated). We refer to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite> for more details.</p>
</div>
<div id="S2.SS1.SSS0.Px4.p2" class="ltx_para">
<p id="S2.SS1.SSS0.Px4.p2.1" class="ltx_p">For this study, we selected dialogues involving only doctors and patients, but not other roles such as nurses. During preprocessing, we removed paraverbal annotations such as ‚Äú[LAUGHING]‚Äù and ‚Äú[INAUDIBLE]‚Äù from the transcripts. We then divided the dataset into training (90%) and validation (10%) sets using stratified sampling based on condition categories and reasons for visits, resulting in 89,027 conversations for training and 9,892 for validation.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Simulated Dialogue Learning Environment and Self-play for AMIE</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">While passively collecting and transcribing real-world dialogues from in-person clinical visits is feasible, two substantial challenges limit its effectiveness in training LLMs for medical conversations: (1) existing real-world data often fails to capture the vast range of medical conditions and scenarios, hindering its scalability and comprehensiveness; (2) the data derived from real-world dialogue transcripts tends to be noisy, containing ambiguous language (including slang, jargon, and sarcasm), interruptions, ungrammatical utterances, and implicit references. This in turn, may limit AMIE‚Äôs knowledge, capabilities, and applicability.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">To address these limitations, we designed a self-play based simulated learning environment for diagnostic medical dialogues in a virtual care setting, enabling us to scale AMIE‚Äôs knowledge and capabilities across a multitude of medical conditions and contexts. We used this environment to iteratively fine-tune AMIE with an evolving set of simulated dialogues in addition to the static corpus of medical QA, reasoning, summarization, and real-world dialogue data described above (see <a href="#S1.F1" title="In 1 Introduction ‚Ä£ Towards Conversational Diagnostic AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">This process consisted of two self-play loops:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">An ‚Äúinner‚Äù self-play loop</span> where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient agent.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">An ‚Äúouter‚Äù self-play loop</span> where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. The resulting new version of AMIE could then participate in the inner loop again, creating a continuous learning cycle.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Simulated Dialogues.</h5>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">At each iteration of fine-tuning, we produced 11,686 dialogues, stemming from 5,230 different medical conditions. Conditions were selected from three datasets:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Health QA dataset</span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite> which contained 613 common medical conditions.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">MalaCards Human Disease Database<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_medium">1</span></span><a target="_blank" href="https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/malacards-diseases.json" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium">https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/malacards-diseases.json</a></span></span></span></span> which contained 18,455 less common disease conditions.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p"><span id="S2.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">MedicineNet Diseases &amp; Conditions Index<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_medium">2</span></span><a target="_blank" href="https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/medicinenet-diseases.json" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_medium">https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/medicinenet-diseases.json</a></span></span></span></span> which contained 4,617 less common conditions.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p2.2" class="ltx_p">At each self-play iteration, four conversations were generated from each of the 613 common conditions, while two conversations were generated from each of the 4,617 less common conditions randomly chosen from
MedicineNet and MalaCards. The average simulated dialogue conversation length was 21.28 turns (<math id="S2.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="P_{0.25}=19.0" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.1.m1.1a"><mrow id="S2.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml"><msub id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml"><mi id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2.cmml">P</mi><mn id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3.cmml">0.25</mn></msub><mo id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml">19.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1"><eq id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.1"></eq><apply id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.1.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.2">ùëÉ</ci><cn type="float" id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.2.3">0.25</cn></apply><cn type="float" id="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px1.p2.1.m1.1.1.3">19.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.1.m1.1c">P_{0.25}=19.0</annotation></semantics></math>, <math id="S2.SS2.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="P_{0.75}=25.0" display="inline"><semantics id="S2.SS2.SSS0.Px1.p2.2.m2.1a"><mrow id="S2.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml"><msub id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml"><mi id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2.cmml">P</mi><mn id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3.cmml">0.75</mn></msub><mo id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml">25.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p2.2.m2.1b"><apply id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1"><eq id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.1"></eq><apply id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.1.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.2">ùëÉ</ci><cn type="float" id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.2.3">0.75</cn></apply><cn type="float" id="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S2.SS2.SSS0.Px1.p2.2.m2.1.1.3">25.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p2.2.m2.1c">P_{0.75}=25.0</annotation></semantics></math>).</p>
</div>
<div id="S2.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p3.1" class="ltx_p">Using simulated dialogues allowed us to address the limited availability of high-quality, labelled real-world conversation data and improved the model‚Äôs generalization and adaptability to diverse medical contexts. By leveraging this self-play paradigm, AMIE could continuously learn and refine its conversational and diagnostic capabilities during patient interactions.</p>
</div>
</section>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Simulated Dialogue Data Curation</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">In order to produce high-quality simulated dialogues at scale, we developed a novel multi-agent framework which comprised three key components:</p>
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p"><span id="S2.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Vignette Generator</span>: AMIE leverages web searches to craft unique patient vignettes given a specific medical condition.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p"><span id="S2.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Simulated Dialogue Generator</span>: Three LLM agents play the roles of patient agent, doctor agent, and moderator, engaging in a turn-by-turn dialogue simulating realistic diagnostic interactions.</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I3.i3.p1" class="ltx_para">
<p id="S2.I3.i3.p1.1" class="ltx_p"><span id="S2.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">Self-play Critic</span>: A fourth LLM agent acts as a critic to give feedback to the doctor agent for self-improvement. Notably, AMIE acted as all agents in this framework. We describe each component in detail below.</p>
</div>
</li>
</ul>
</div>
<section id="S2.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Vignette Generator.</h5>

<div id="S2.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.Px1.p1.1" class="ltx_p">The vignette generator aimed to create varied and realistic patient scenarios at scale, which could be subsequently used as context for generating simulated doctor-patient dialogues thereby allowing AMIE to undergo a training process emulating exposure to a greater number of conditions and patient backgrounds. The patient vignette (scenario) included essential background information such as patient demographics, symptoms, past medical history, past surgical history, past social history, and patient questions, as well as an associated diagnosis and management plan.</p>
</div>
<div id="S2.SS2.SSS1.Px1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.Px1.p2.1" class="ltx_p">For a given condition, patient vignettes were constructed using the following process. First, we retrieved 60 passages (20 each) on the range of demographics, symptoms, and management plans associated with the condition from using an internet search engine. To ensure these passages were relevant to the given condition, we used the general-purpose LLM, PaLM-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#biba.bibx2" title="" class="ltx_ref">119</a>]</cite>, to filter these retrieved passages, removing any passages deemed unrelated to the given condition. We then prompted AMIE to generate plausible patient vignettes aligned with the demographics, symptoms, and management plans retrieved from the filtered passages, by providing a one-shot exemplar to enforce a particular vignette format. The prompts for each of these steps are as follows:</p>
</div>
<div id="S2.SS2.SSS1.Px1.p3" class="ltx_para ltx_noindent">
<svg id="S2.SS2.SSS1.Px1.p3.pic1" class="ltx_picture" height="340.28" overflow="visible" version="1.1" width="600"><g transform="translate(0,340.28) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 334.37 C 0 337.63 2.64 340.28 5.91 340.28 L 594.09 340.28 C 597.36 340.28 600 337.63 600 334.37 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 334.37 C 1.97 336.54 3.73 338.31 5.91 338.31 L 594.09 338.31 C 596.27 338.31 598.03 336.54 598.03 334.37 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="312.72" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.1" class="ltx_p"><span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Search Retrieval Template
<br class="ltx_break"></span>What are the specific patient demographics/symptoms/management plan for the condition [Condition]?</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.2" class="ltx_p"><span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Passage Filtering Template
<br class="ltx_break"></span>For the clinical condition, [Condition], is the following a good description of common demographics/symptoms/management plans (Yes/No)?</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.3" class="ltx_p">Description: [Retrieved Passage]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.4" class="ltx_p">Answer (Yes/No):</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.5" class="ltx_p"><span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Vignette Generation Template
<br class="ltx_break"></span>The following are several passages about the demographics, symptoms, and management plan for a given condition. Generate 2 different patient vignettes consistent with these passages. Follow the format of the given example (just list N/A if a particular field is unavailable).</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.6" class="ltx_p">Condition: [Condition]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.7" class="ltx_p">Demographic Passages: [Retrieved Demographic Passages]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.8" class="ltx_p">Symptoms Passages: [Retrieved Symptom Passages]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.9" class="ltx_p">Management Plan Passages: [Retrieved Management Plan Passages]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.10" class="ltx_p">Example Format: [Oneshot example]</span>
<span id="S2.SS2.SSS1.Px1.p3.pic1.1.1.1.1.1.11" class="ltx_p">Patient Vignettes for [Condition]:</span>
</span></foreignObject></g></g></svg>
</div>
</section>
<section id="S2.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Simulated Dialogue Generator.</h5>

<div id="S2.SS2.SSS1.Px2.p1" class="ltx_para">
<p id="S2.SS2.SSS1.Px2.p1.1" class="ltx_p">Given a patient vignette detailing a specific medical condition, the simulated dialogue generator was designed to simulate a realistic dialogue between a patient and a doctor in an online chat setting where in-person physical examination may not be feasible.</p>
</div>
<div id="S2.SS2.SSS1.Px2.p2" class="ltx_para">
<p id="S2.SS2.SSS1.Px2.p2.1" class="ltx_p">Three specific LLM agents (patient agent, doctor agent, and moderator), each played by AMIE, were tasked with communicating amongst each other to generate the simulated dialogues. Each agent had distinct instructions. The patient agent embodied the individual experiencing the medical condition outlined in the vignette. Their role involved truthfully responding to the doctor agent‚Äôs inquiries as well as raising any additional questions or concerns they may have had. The doctor agent played the role of an empathetic clinician seeking to comprehend the patient‚Äôs medical history within the online chat environment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">24</a>]</cite>. Their objective was to formulate questions that could effectively reveal the patient‚Äôs symptoms and background, leading to an accurate diagnosis and an effective treatment plan. The moderator continually assessed the ongoing dialogue between the patient agent and doctor agent, determining when the conversation had reached a natural conclusion.</p>
</div>
<div id="S2.SS2.SSS1.Px2.p3" class="ltx_para">
<p id="S2.SS2.SSS1.Px2.p3.1" class="ltx_p">The turn-by-turn dialogue simulation started with the doctor agent initiating the conversation: ‚ÄúDoctor: So, how can I help you today?‚Äù. Following this, the patient agent responded, and their answer was incorporated into the ongoing dialogue history. Subsequently, the doctor agent formulated a response based on the updated dialogue history. This response was then appended to the conversation history. The conversation progressed until the moderator detected the dialogue had reached a natural conclusion, when the doctor agent had provided a differential diagnosis, treatment plan, and adequately addressed any remaining patient agent questions, or if either agent initiated a farewell.</p>
</div>
<div id="S2.SS2.SSS1.Px2.p4" class="ltx_para ltx_noindent">
<svg id="S2.SS2.SSS1.Px2.p4.pic1" class="ltx_picture" height="261.48" overflow="visible" version="1.1" width="600"><g transform="translate(0,261.48) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 255.58 C 0 258.84 2.64 261.48 5.91 261.48 L 594.09 261.48 C 597.36 261.48 600 258.84 600 255.58 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 255.58 C 1.97 257.75 3.73 259.51 5.91 259.51 L 594.09 259.51 C 596.27 259.51 598.03 257.75 598.03 255.58 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="233.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.1" class="ltx_p"><span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Patient Agent Instruction:
<br class="ltx_break"></span>You are a patient chatting with a doctor over an online chat interface. The doctor has never met you before.
&lt;patient vignette&gt;
Respond to the doctor‚Äôs questions honestly as they interview you, asking any questions that may come up.</span>
<span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.2" class="ltx_p"><span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Doctor Agent Instruction:
<br class="ltx_break"></span>You are an empathetic clinician asking a patient about their medical history over an online chat interface. You know nothing about the patient in advance.
Respond to the patient with a single-turn response to better understand their history and symptoms. Do not ask more than two questions. If the patient asks a question, be sure to answer it appropriately.</span>
<span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.3" class="ltx_p"><span id="S2.SS2.SSS1.Px2.p4.pic1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Moderator Instruction:
<br class="ltx_break"></span>The following is a conversation between a doctor and a patient:
&lt;dialog&gt; 
The conversation should only come to an end if the doctor has finished giving the patient a diagnosis and treatment plan and the patient has no questions left. A conversation also comes to an end if the doctor or patient says goodbye.
Question: has the conversation come to an end? Yes or No.</span>
</span></foreignObject></g></g></svg>
</div>
</section>
<section id="S2.SS2.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Self-play Critic.</h5>

<div id="S2.SS2.SSS1.Px3.p1" class="ltx_para">
<p id="S2.SS2.SSS1.Px3.p1.1" class="ltx_p">To ensure high-quality dialogues, we implemented a tailored self-play&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">25</a>]</cite> framework specifically for self-improvement of diagnostic conversations. This framework introduced a fourth LLM agent, acting as a ‚Äúcritic‚Äù which was also played by AMIE and aware of the ground truth diagnosis, to provide in-context feedback to the doctor agent and enhance its performance in subsequent conversations. The critic agent evaluated the doctor agent‚Äôs responses based on the following criteria:</p>
<ul id="S2.I4" class="ltx_itemize">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I4.i1.p1" class="ltx_para">
<p id="S2.I4.i1.p1.1" class="ltx_p">The doctor agent exhibits empathy and professionalism while addressing the patient agent‚Äôs latest questions or comments in a concise manner.</p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I4.i2.p1" class="ltx_para">
<p id="S2.I4.i2.p1.1" class="ltx_p">The doctor agent avoids asking too many or repetitive questions (about information already acquired), focusing on a maximum of one or two per response.</p>
</div>
</li>
<li id="S2.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I4.i3.p1" class="ltx_para">
<p id="S2.I4.i3.p1.1" class="ltx_p">The responses should not reveal that the doctor agent is an AI chatbot. They should flow naturally, maintain factual accuracy, and facilitate further engagement from the patient.</p>
</div>
</li>
<li id="S2.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I4.i4.p1" class="ltx_para">
<p id="S2.I4.i4.p1.1" class="ltx_p">The doctor agent asks sufficient questions to identify at least two of the most likely differential diagnoses. They further refine their understanding through targeted questions towards the ground truth diagnosis and offer the corresponding treatment.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.SSS1.Px3.p2" class="ltx_para">
<p id="S2.SS2.SSS1.Px3.p2.1" class="ltx_p">Following the critic‚Äôs feedback, the doctor agent incorporated the suggestions to improve its responses in subsequent rounds of dialogue with the same patient agent from scratch. Notably, the doctor agent retained access to its previous dialogue history at each new round. This self-improvement process was repeated twice to generate the dialogues used for each iteration of fine-tuning.</p>
</div>
</section>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Instruction Fine-tuning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">AMIE, built upon the base LLM PaLM 2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#biba.bibx2" title="" class="ltx_ref">119</a>]</cite>, was instruction fine-tuned to enhance its capabilities for medical dialogue and reasoning. We refer to the PaLM-2 technical report for more details on the base LLM architecture.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">We employed task-specific instructions to fine-tune AMIE in playing either the patient or doctor role within medical dialogues, performing medical question answering and reasoning, and summarizing EHR notes. While the first round of fine-tuning from the base LLM only used the static datasets, subsequent rounds of fine-tuning leveraged the simulated dialogues generated through the self-play inner loop as described in <a href="#S2.SS2.SSS1" title="2.2.1 Simulated Dialogue Data Curation ‚Ä£ 2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‚Ä£ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‚Ä£ Towards Conversational Diagnostic AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">2.2.1</span></a>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">For dialogue generation tasks, AMIE was trained to predict the next conversational turn based on all previous interactions, assuming either the doctor or patient role. When playing the patient agent, AMIE was prompted to reply to the doctor agent‚Äôs questions about their symptoms, drawing upon information provided in patient scenarios. These scenarios included patient vignettes (see <a href="#S2.SS2.SSS1.Px1" title="Vignette Generator. ‚Ä£ 2.2.1 Simulated Dialogue Data Curation ‚Ä£ 2.2 Simulated Dialogue Learning Environment and Self-play for AMIE ‚Ä£ 2 AMIE: An LLM based AI System for Diagnostic Dialogue ‚Ä£ Towards Conversational Diagnostic AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">2.2.1</span></a>) for simulated dialogues or metadata such as demographics, visit reason, and diagnosis type for the real-world dialogue dataset. In the doctor agent role, AMIE was prompted to act as an empathetic clinician, interviewing patients about their medical history and symptoms to ultimately arrive at an accurate diagnosis. From each dialogue, we sampled on average 3 turns for each the doctor and patient roles as the target turns to predict based on the conversation leading up to that target turn. Target turns were randomly sampled from all turns in the dialogue that had a minimum length of 30 characters.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Similarly, for the EHR note summarization task, AMIE was provided with a clinical note and prompted to generate a summary of the note. Medical reasoning/QA and long-form response generation tasks followed the same setup as in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>. Notably, all tasks except dialogue generation and long-form response generation incorporated few-shot (1-5) exemplars in addition to task-specific instructions for additional context.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Chain-of-reasoning for Online Inference</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">To address the core challenge in diagnostic dialogue - effectively acquiring information under uncertainty to enhance diagnostic accuracy and confidence while maintaining positive rapport with the patient - AMIE employed a chain-of-reasoning strategy before generating a response in each dialogue turn. Here, ‚Äúchain-of-reasoning‚Äù refers to a series of sequential model calls, each dependent on the outputs of prior steps. Specifically, we used a three-step reasoning process, described as follows:</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<ol id="S2.I5" class="ltx_enumerate">
<li id="S2.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I5.i1.p1" class="ltx_para">
<p id="S2.I5.i1.p1.1" class="ltx_p"><span id="S2.I5.i1.p1.1.1" class="ltx_text ltx_font_bold">Analyzing patient information:</span> Given the current conversation history, AMIE was instructed to 1) summarize the positive and negative symptoms of the patient as well as any relevant medical/family/social history and demographic information, 2) produce a current differential diagnosis, 3) note missing information needed for a more accurate diagnosis and 4) assess confidence in the current differential and highlight its urgency.</p>
</div>
</li>
<li id="S2.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I5.i2.p1" class="ltx_para">
<p id="S2.I5.i2.p1.1" class="ltx_p"><span id="S2.I5.i2.p1.1.1" class="ltx_text ltx_font_bold">Formulating response and action:</span> Building upon the conversation history and the output of step 1, AMIE performed the following: 1) Generate a response to the patient‚Äôs last message and formulate further questions to acquire missing information and refine the differential diagnosis. 2) If necessary, recommend immediate action, such as an emergency room visit. If confident in the diagnosis based on available information, present the differential.</p>
</div>
</li>
<li id="S2.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I5.i3.p1" class="ltx_para">
<p id="S2.I5.i3.p1.1" class="ltx_p"><span id="S2.I5.i3.p1.1.1" class="ltx_text ltx_font_bold">Refining the response:</span> AMIE revises its previous output to meet specific criteria based on the conversation history and outputs from earlier steps. The criteria are primarily related to factuality and formatting of the response (e.g., avoid factual inaccuracies on patient facts and unnecessary repetition, show empathy, and display in a clear format).</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">This chain-of-reasoning strategy enabled AMIE to progressively refine its response conditioned on the current conversation to arrive at an informed and grounded reply.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Prior works developing models for clinical dialogue have focused on metrics such as the accuracy of note-to-dialogue or dialogue-to-note generations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>, <a href="#bib.bibx27" title="" class="ltx_ref">27</a>]</cite>, or natural language generation metrics such as BLEU or ROUGE scores that fail to capture the clinical quality of a consultation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">28</a>, <a href="#bib.bibx29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">In contrast to these prior works we sought to anchor our human evaluation in criteria more commonly used for evaluating the quality of physicians‚Äô expertise in history-taking, including their communication skills in consultation. We derived a framework from principles published in reviews of the consensus for best practices for patient-centered communication (PCCBP) in medical interviews&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite>, criteria examined for history-taking skills by the Royal College of Physicians in the UK as part of their Practical Assessment of Clinical Examination Skills (PACES)<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://www.mrcpuk.org/mrcpuk-examinations/paces/marksheets" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mrcpuk.org/mrcpuk-examinations/paces/marksheets</a></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">30</a>]</cite>, and criteria proposed by the UK General Medical Council Patient Questionnaire (GMCPQ)<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://www.ed.ac.uk/sites/default/files/imports/fileManager/patient_questionnaire%20pdf_48210488.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ed.ac.uk/sites/default/files/imports/fileManager/patient_questionnaire%20pdf_48210488.pdf</a></span></span></span> for doctors seeking patient feedback as part of professional re-validation<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://www.gmc-uk.org/registration-and-licensing/managing-your-registration/revalidation/revalidation-resources/collecting-colleague-and-patient-feedback-for-revalidation" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.gmc-uk.org/registration-and-licensing/managing-your-registration/revalidation/revalidation-resources/collecting-colleague-and-patient-feedback-for-revalidation</a></span></span></span>. We iterated upon these criteria to refine items for inclusion and derived pilot scales and instructions for assessment by using focus groups and interviews with clinicians and OSCE examiners based in the UK, Canada, US, and India.
Our resulting pilot framework enabled assessment from two perspectives: clinician (board-certified physicians) and lay raters (patient actors). The framework included consideration of consultation quality, structure and completeness, the roles, responsibilities, and skills of the interviewer (Tables <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:gmcpq_rubric_details</span>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>, <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:pccbp_rubric_details</span>, and <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:diagnosis_management_rubric_details</span>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Objective Structured Clinical Examination</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Objective Structured Clinical Examination (OSCE) is a practical assessment format used in healthcare to assess clinical skills and competencies in a standardized and objective fashion&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">31</a>, <a href="#bib.bibx32" title="" class="ltx_ref">32</a>, <a href="#bib.bibx33" title="" class="ltx_ref">33</a>]</cite>.
It differs from traditional written or oral exams that focus primarily on theoretical knowledge and instead aims to provide an environment in which the skills of real-world clinical practice might be assessed.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The OSCE is typically divided into multiple stations (often 8-12), each simulating a real-life clinical scenario enacted by standardized patient actors trained to portray specific symptoms or conditions based on pre-defined scenario descriptions.
At each station, students are given specific tasks to perform, such as taking a clinical history, or making a diagnosis.
Each station has a set time limit, ensuring fairness and efficient assessment.
Trained examiners observe students‚Äô performance at each station using a pre-defined checklist or marking scheme.
They assess clinical skills like communication, history-taking, physical examination techniques, clinical reasoning, and decision-making.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Remote OSCE Study Design</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To compare AMIE‚Äôs performance to that of real clinicians, we conducted a randomized crossover study of blinded consultations in the style of a remote OSCE.
Our OSCE study involved 20 board-certified primary care physicians (PCPs) and 20 validated patient actors, 10 each from India and Canada, respectively, to partake in online text-based consultations.
PCPs had between 3 and 25 years of post-residency experience (median 7 years).
Patient actors comprised of a mix of medical students, residents, and nurse practitioners with experience in OSCE participation.
We sourced 149 scenario packs from India (75), Canada (60), and the UK (14).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The scenario packs and simulated patients in our study were prepared by two OSCE laboratories (one each in Canada and India), each affiliated to a medical school and with extensive experience in preparing scenario packs and simulated patients for OSCE examinations. UK scenario packs were sourced from the samples provided on the MRCPUK website. Each scenario pack was associated with a ground truth diagnosis and a set of acceptable diagnoses. The scenario packs covered conditions from cardiovascular (29), respiratory (30), gastroenterology (31), neurology (30), urology, obstetric, and gynecology domains (15), and internal medicine (14). Pediatric or psychiatry domains were excluded from this study, as were intensive care or inpatient case management scenarios.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Indian patient actors played the roles in all India scenario packs and 7 of the 14 UK scenario packs. Canadian patient actors participated in scenario packs for both Canada and the other half of UK-based scenario packs. This assignment process resulted in 149 distinct simulated patients (‚Äúscenarios‚Äù). Below, we use the term ‚ÄúOSCE agent‚Äù to refer to the conversational counterpart interviewing the patient actor, i.e., either PCP or AMIE. &nbsp;<a href="#S3.T1" title="In 3.2 Remote OSCE Study Design ‚Ä£ 3 Evaluation ‚Ä£ Towards Conversational Diagnostic AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a> summarizes the OSCE assignment information across three geographical locations. Each of the 149 simulated patients completed the three-step study flow depicted in Figure <a href="#S1.F2" title="Figure 2 ‚Ä£ 1 Introduction ‚Ä£ Towards Conversational Diagnostic AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.5.1.1" class="ltx_text" style="font-size:113%;">Table 1</span>: </span><span id="S3.T1.6.2" class="ltx_text ltx_font_bold" style="font-size:113%;">OSCE study summary.<span id="S3.T1.6.2.1" class="ltx_text ltx_font_medium"> Number of scenario packs, patient actors, simulated patients, and primary care physicians (PCPs) in each of the three locations (Canada, India, and the UK) in the remote OSCE study. 20 board-certified PCPs participated in the study as OSCE agents in comparison with AMIE, 10 each from India and Canada. 20 trained patient actors were involved, with 10 each from India and Canada.
Indian patient actors played the roles in both India and UK scenario packs. Canadian patient actors participated in scenario packs for both Canada and the UK. This process resulted in 149 distinct simulated patients.</span></span></figcaption>
<table id="S3.T1.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.7.1.1" class="ltx_tr">
<th id="S3.T1.7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S3.T1.7.1.1.1.1" class="ltx_text" style="font-size:80%;">Location</span></th>
<td id="S3.T1.7.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.7.1.1.2.1" class="ltx_text" style="font-size:80%;"># of Scenario Packs</span></td>
<td id="S3.T1.7.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.7.1.1.3.1" class="ltx_text" style="font-size:80%;"># of Simulated Patients</span></td>
<td id="S3.T1.7.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.7.1.1.4.1" class="ltx_text" style="font-size:80%;"># of Patient Actors</span></td>
<td id="S3.T1.7.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.7.1.1.5.1" class="ltx_text" style="font-size:80%;"># of PCPs</span></td>
</tr>
<tr id="S3.T1.7.2.2" class="ltx_tr">
<th id="S3.T1.7.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T1.7.2.2.1.1" class="ltx_text" style="font-size:80%;">Canada</span></th>
<td id="S3.T1.7.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.2.2.2.1" class="ltx_text" style="font-size:80%;">60</span></td>
<td id="S3.T1.7.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.2.2.3.1" class="ltx_text" style="font-size:80%;">67</span></td>
<td id="S3.T1.7.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.2.2.4.1" class="ltx_text" style="font-size:80%;">10</span></td>
<td id="S3.T1.7.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.2.2.5.1" class="ltx_text" style="font-size:80%;">10</span></td>
</tr>
<tr id="S3.T1.7.3.3" class="ltx_tr">
<th id="S3.T1.7.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T1.7.3.3.1.1" class="ltx_text" style="font-size:80%;">India</span></th>
<td id="S3.T1.7.3.3.2" class="ltx_td ltx_align_center"><span id="S3.T1.7.3.3.2.1" class="ltx_text" style="font-size:80%;">75</span></td>
<td id="S3.T1.7.3.3.3" class="ltx_td ltx_align_center"><span id="S3.T1.7.3.3.3.1" class="ltx_text" style="font-size:80%;">82</span></td>
<td id="S3.T1.7.3.3.4" class="ltx_td ltx_align_center"><span id="S3.T1.7.3.3.4.1" class="ltx_text" style="font-size:80%;">10</span></td>
<td id="S3.T1.7.3.3.5" class="ltx_td ltx_align_center"><span id="S3.T1.7.3.3.5.1" class="ltx_text" style="font-size:80%;">10</span></td>
</tr>
<tr id="S3.T1.7.4.4" class="ltx_tr">
<th id="S3.T1.7.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T1.7.4.4.1.1" class="ltx_text" style="font-size:80%;">UK</span></th>
<td id="S3.T1.7.4.4.2" class="ltx_td ltx_align_center"><span id="S3.T1.7.4.4.2.1" class="ltx_text" style="font-size:80%;">14</span></td>
<td id="S3.T1.7.4.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.7.4.4.3.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S3.T1.7.4.4.4" class="ltx_td ltx_align_center"><span id="S3.T1.7.4.4.4.1" class="ltx_text" style="font-size:80%;">0</span></td>
<td id="S3.T1.7.4.4.5" class="ltx_td ltx_align_center"><span id="S3.T1.7.4.4.5.1" class="ltx_text" style="font-size:80%;">0</span></td>
</tr>
<tr id="S3.T1.7.5.5" class="ltx_tr">
<th id="S3.T1.7.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S3.T1.7.5.5.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Total</span></th>
<td id="S3.T1.7.5.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.7.5.5.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">149</span></td>
<td id="S3.T1.7.5.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.7.5.5.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">149</span></td>
<td id="S3.T1.7.5.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.7.5.5.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">20</span></td>
<td id="S3.T1.7.5.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.7.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">20</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Online Text-based Consultation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">PCPs and patient actors were primed with sample scenarios and instructions, and participated in pilot consultations prior to the study commencing in order to familiarize themselves with the interface and experiment requirements.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">For the experiment, each simulated patient completed two online text-based consultations via a synchronous text chat interface (Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:chat_interface</span>), one with a PCP (control) and one with AMIE (intervention).
The ordering of PCP and AMIE was randomized and patient actors were not informed as to which they were talking to in each consultation.
PCPs were located in the same country as patient actors, and were randomly drawn based on availability at the specified time slot for the consultation.
Patient actors role-played the scenario and were instructed to conclude the conversation after no more than 20 minutes. Both OSCE agents were asked (PCPs via study-specific instructions, and AMIE as part of the prompt template) to not reveal their identity, or whether they were human, under any circumstances.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Post-questionnaires</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Upon conclusion of the consultation, the patient actor and OSCE agent each filled in a post-questionnaire in light of the resulting consultation transcript (Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:patient_actor_rating_interface</span>).
The post-questionnaire for patient actors consisted of the complete GMCPQ (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:gmcpq_rubric_details</span>), the PACES components for ‚ÄúManaging Patient Concerns‚Äù and ‚ÄúMaintaining Patient Welfare‚Äù (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>), and a checklist representation of the PCCBP category for ‚ÄúFostering the Relationship‚Äù (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:pccbp_rubric_details</span>).
Responses patient actors provided to the post-questionnaire are referred to as ‚Äúpatient actor ratings‚Äù below.
The post-questionnaire for the OSCE agent asked for a ranked differential diagnosis (DDx) list with a minimum of 3 and no more than 10 conditions, as well as recommendations for escalation to in-person or video-based consultation, investigations, treatments, management plan, and the need for a follow-up.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Specialist Physician Evaluation</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Finally, a pool of 23 specialist physicians from India (14), North America (6), and the UK (3) evaluated PCPs and AMIE with respect to the quality of their consultation, and their responses to the post-questionnaire.
During evaluation, specialist physicians also had access to the full scenario pack along with its associated ground truth differential and additional accepted differentials.
All of the data the specialist physicians had access to during evaluation are collectively referred to as ‚ÄúOSCE data‚Äù below.
Specialist physicians were sourced to match the specialties and geographic regions corresponding to the scenario packs included in our study, and had between 1 and 36 years of post-residency experience (median 5 years).
Each set of OSCE data was evaluated by one specialist physician randomly assigned to match the specialty and geographic region of the underlying scenario (e.g., Canadian pulmonologist evaluated OSCE data from Canada-sourced respiratory medicine scenario).
Each specialist evaluated OSCE data from both PCP and AMIE for a given scenario.
Evaluations for PCP and AMIE were conducted by the same specialist in a randomized and blinded sequence.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">Evaluation criteria included the accuracy, appropriateness and comprehensiveness of the provided DDx list, appropriateness of recommendations regarding escalation, investigation, treatment, management plan and follow-up (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:diagnosis_management_rubric_details</span>), and all PACES (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>) and PCCBP (Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:pccbp_rubric_details</span>) rating items.
We also asked specialist physicians to highlight confabulations in the consultations and questionnaire responses, i.e., text passages that were non-factual or referred to information not provided in the conversation.
Each OSCE scenario pack additionally supplied specialists with scenario-specific clinical information to assist with rating the clinical quality of the consultation, such as the ideal investigation or management plans; or important aspects of the clinical history that would ideally have been elucidated for the highest quality of consultation possible.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Auto-evaluation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In addition to human evaluations, we implemented model-based auto-evaluation methods as economical consistent alternatives to specialist assessments. These techniques were employed to evaluate both dialogue quality and diagnostic accuracy of the OSCE agent.
To establish the validity of our auto-evaluation methods for assessing dialogue quality, we initially focused on a subset of four evaluation axes from the PACES rubric (<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:paces_rubric_details</span>) that were assessed by both the patient actors and the specialist physicians. The auto-evaluation, which uses a self-CoT strategy (details described in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:auto-eval</span>) with AMIE to rate dialogues, was in good alignment with human raters and comparable to the inter-specialist agreement on these criteria. For the auto-evaluation of differential diagnoses, we leveraged another LLM, Med-PaLM 2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite> as a surrogate for a specialist rater to grade the predicted diagnoses against the ground truth diagnoses (more details in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:auto-eval-ddx</span>). Our auto-evaluation on DDx accuracy showed a similar trend for AMIE and OSCE agents compared to the specialist ratings. Overall, auto-evaluation trends aligned with human ratings for both dialogue quality and diagnostic accuracy.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We also conducted additional auto-evaluation analyses for the following purposes:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">To compare the performance of the DDx accuracy derived from AMIE or PCP consultations;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">To compare the DDx accuracy between simulated patients performed in Canada and India and determine if there is systematic differences between the two locations;</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">To isolate the effects of information acquisition and information interpretation by analyzing the DDx accuracy of AMIE when provided the PCP consultation instead of its own;</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">To evaluate the efficiency of information acquisition between AMIE and PCPs by analyzing the DDx accuracy as the number of conversation turns increases;</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">To evaluate the benefit of inner-loop self-play on dialogue quality before and after critic feedback.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Statistical Analysis</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We evaluated the top-k accuracy of the DDx lists generated by AMIE and PCPs across all 149 simulated patients.
Top-k accuracy was defined as the percentage of cases where the correct diagnosis appeared within the top-k positions of the DDx list.
Specifically, a candidate diagnosis was considered a match if the specialist rater marked it as either an exact match with, very close to or closely related to the ground truth diagnosis (or accepted differential).
Statistical significance for DDx accuracy was determined using bootstrap tests&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">34</a>]</cite> with 10,000 samples and false discovery rate (FDR) correction&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">35</a>]</cite> across all k.
Statistical significance for patient actor and specialist ratings was determined using Wilcoxon signed-rank tests&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">36</a>]</cite> FDR correction.
Cases where either agent received ‚ÄúCannot rate / Does not apply‚Äù were excluded from the test.
Results below refer to <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">ùëù</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">p</annotation></semantics></math>-values after FDR correction.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Diagnostic Accuracy</h3>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2401.05654/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.7.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Specialist-rated top-k diagnostic accuracy.<span id="S4.F3.2.1.2" class="ltx_text ltx_font_medium"> AMIE and PCPs top-k DDx accuracy are compared across 149 scenarios with respect to the ground truth diagnosis (</span>a<span id="S4.F3.2.1.3" class="ltx_text ltx_font_medium">) and all diagnoses in the accepted differential (</span>b<span id="S4.F3.2.1.1" class="ltx_text ltx_font_medium">). Bootstrapping (n=10,000) confirms all top-k differences between AMIE and PCP DDx accuracy are significant with <math id="S4.F3.2.1.1.m1.1" class="ltx_Math" alttext="p<0.05" display="inline"><semantics id="S4.F3.2.1.1.m1.1b"><mrow id="S4.F3.2.1.1.m1.1.1" xref="S4.F3.2.1.1.m1.1.1.cmml"><mi id="S4.F3.2.1.1.m1.1.1.2" xref="S4.F3.2.1.1.m1.1.1.2.cmml">p</mi><mo id="S4.F3.2.1.1.m1.1.1.1" xref="S4.F3.2.1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.F3.2.1.1.m1.1.1.3" xref="S4.F3.2.1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.2.1.1.m1.1c"><apply id="S4.F3.2.1.1.m1.1.1.cmml" xref="S4.F3.2.1.1.m1.1.1"><lt id="S4.F3.2.1.1.m1.1.1.1.cmml" xref="S4.F3.2.1.1.m1.1.1.1"></lt><ci id="S4.F3.2.1.1.m1.1.1.2.cmml" xref="S4.F3.2.1.1.m1.1.1.2">ùëù</ci><cn type="float" id="S4.F3.2.1.1.m1.1.1.3.cmml" xref="S4.F3.2.1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.2.1.1.m1.1d">p&lt;0.05</annotation></semantics></math> after FDR correction.</span></span></figcaption>
</figure>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>AMIE showed higher DDx accuracy than PCPs under specialist physician evaluation.</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">AMIE‚Äôs diagnostic accuracy was assessed as higher than that of PCPs.&nbsp;<a href="#S4.F3" title="In 4.1 Diagnostic Accuracy ‚Ä£ 4 Results ‚Ä£ Towards Conversational Diagnostic AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> shows the top-k accuracy for AMIE and PCPs, considering matches with the ground truth diagnosis (a) and matches with any item on the accepted differential (b). AMIE showed significantly higher top-k accuracy than that of PCPs across all values of k (<math id="S4.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="p<0.05" display="inline"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mrow id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS1.SSS1.p1.1.m1.1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS1.SSS1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"><lt id="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.1"></lt><ci id="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.2">ùëù</ci><cn type="float" id="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">p&lt;0.05</annotation></semantics></math>). Note that unlike AMIE, PCPs did not always provide 10 diagnoses in their differential diagnoses (min: 3, mean: 5.39). Additionally, we performed a comparison of DDx accuracy between AMIE and PCP by varying the matching criteria for determining a match. Results depicted in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_specialist_match_cutoffs</span> further substantiate AMIE‚Äôs superior DDx performance across various matching criteria.</p>
</div>
<section id="S4.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Accuracy by Specialty.</h5>

<div id="S4.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px1.p1.1" class="ltx_p"><span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:specialist_ddx_ratings_by_specialty</span> illustrates the DDx accuracy achieved by AMIE and PCPs across the six medical specialties covered by scenarios in our study. We observed that AMIE‚Äôs performance matched or surpassed PCP performance for all specialties with the most pronounced improvements in the respiratory and cardiovascular specialities.</p>
</div>
</section>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Auto-evaluation suggested AMIE matched PCPs‚Äô efficiency in acquiring information.</h4>

<section id="S4.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Auto-evaluation Accuracy.</h5>

<div id="S4.SS1.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS2.Px1.p1.1" class="ltx_p">We reproduced the DDx accuracy analysis with our model-based auto-evaluator instead of the specialist raters using the same procedure as in&nbsp;<a href="#S4.F3" title="In 4.1 Diagnostic Accuracy ‚Ä£ 4 Results ‚Ä£ Towards Conversational Diagnostic AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>. The overall performance trends obtained through the auto-evaluator align well with specialist assessments despite marginal differences in the computed accuracy values, as shown in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_autoeval</span>.</p>
</div>
</section>
<section id="S4.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Isolating the Source of Performance Gains.</h5>

<div id="S4.SS1.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.Px2.p1.1" class="ltx_p">To investigate whether AMIE‚Äôs superior DDx performance observed in&nbsp;<a href="#S4.F3" title="In 4.1 Diagnostic Accuracy ‚Ä£ 4 Results ‚Ä£ Towards Conversational Diagnostic AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> stemmed from improved information acquisition or from better diagnostic reasoning capability, we compared AMIE‚Äôs diagnoses based on its own consultations with AMIE‚Äôs diagnoses generated from the corresponding PCP consultations, using the DDx auto-evaluator. Results depicted in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_autoeval_AMIEvsAMIE</span> revealed markedly similar DDx performance, indicating that the diagnostic performance remained consistent regardless of whether AMIE processed information from its own dialogue or from the PCP‚Äôs conversation. Both methods significantly outperformed the differential diagnoses produced by PCPs. These results suggest that AMIE was approximately equivalent to PCPs at information acquisition but better than PCPs at interpreting that information to produce an accurate/complete differential diagnosis.</p>
</div>
</section>
<section id="S4.SS1.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Efficiency of Information Acquisition.</h5>

<div id="S4.SS1.SSS2.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS2.Px3.p1.1" class="ltx_p">Although AMIE displayed greater verbosity compared to PCPs in terms of total number of words generated in their responses during the consultation, the number of conversational turns and the number of words elicited from the patient actors were similar across both OSCE agents, as illustrated in <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:number_of_words_and_turns</span>. This suggests that both AMIE and PCPs acquired a similar amount of information from the patients during the encounter. To investigate how efficient AMIE or PCPs were at gathering sufficient information to formulate a correct diagnosis, we truncated the conversations at various turn counts and used AMIE to generate differential diagnoses based on these partial conversations. &nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:all_cases_autoeval_AMIEvsAMIE_turnsablation</span> depicts the top-3 DDx accuracy as a function of the number of turns provided to the model. The observed accuracies plateaued within the initial 10 conversational turns for both AMIE and PCPs. This suggests that both AMIE and PCPs were able to acquire the information necessary for formulating a diagnosis within the early stages of the conversation. Additionally, the comparable performance at every turn indicates that neither AMIE nor PCPs had a significant advantage in the efficiency or quality of information acquisition.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2401.05654/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="355" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.11.5.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.8.4" class="ltx_text ltx_font_bold" style="font-size:90%;">Patient actor ratings.<span id="S4.F4.8.4.4" class="ltx_text ltx_font_medium"> Conversation qualities as assessed by patient actors upon conclusion of the consultation. For illustration purposes, all responses from five-point rating scales were mapped to a generic five-point scale ranging from ‚ÄòVery favorable‚Äô to ‚ÄòVery unfavorable‚Äô. For Yes/No questions, a (positive) ‚ÄòYes‚Äô response was mapped to the same color as ‚ÄòFavorable‚Äô and a (negative) ‚ÄôNo‚Äô response to the same color as ‚ÄòUnfavorable‚Äô. Rating scales were adapted from the General Medical Council Patient Questionnaire (GMCPQ), the Practical Assessment of Clinical Examination Skills (PACES), and a narrative review about Patient-Centered Communication Best Practice (PCCBP). Details on question wording and response options are provided in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:rubrics</span>. Asterisks represent statistical significance (<math id="S4.F4.5.1.1.m1.2" class="ltx_math_unparsed" alttext="*:p<0.05" display="inline"><semantics id="S4.F4.5.1.1.m1.2b"><mrow id="S4.F4.5.1.1.m1.2c"><mo rspace="0em" id="S4.F4.5.1.1.m1.1.1">*</mo><mo rspace="0.278em" id="S4.F4.5.1.1.m1.2.2">:</mo><mi id="S4.F4.5.1.1.m1.2.3">p</mi><mo id="S4.F4.5.1.1.m1.2.4">&lt;</mo><mn id="S4.F4.5.1.1.m1.2.5">0.05</mn></mrow><annotation encoding="application/x-tex" id="S4.F4.5.1.1.m1.2d">*:p&lt;0.05</annotation></semantics></math>, <math id="S4.F4.6.2.2.m2.3" class="ltx_math_unparsed" alttext="**:p<0.01" display="inline"><semantics id="S4.F4.6.2.2.m2.3b"><mrow id="S4.F4.6.2.2.m2.3c"><mo rspace="0em" id="S4.F4.6.2.2.m2.1.1">*</mo><mo lspace="0em" rspace="0em" id="S4.F4.6.2.2.m2.2.2">*</mo><mo rspace="0.278em" id="S4.F4.6.2.2.m2.3.3">:</mo><mi id="S4.F4.6.2.2.m2.3.4">p</mi><mo id="S4.F4.6.2.2.m2.3.5">&lt;</mo><mn id="S4.F4.6.2.2.m2.3.6">0.01</mn></mrow><annotation encoding="application/x-tex" id="S4.F4.6.2.2.m2.3d">**:p&lt;0.01</annotation></semantics></math>, <math id="S4.F4.7.3.3.m3.4" class="ltx_math_unparsed" alttext="***:p<0.001" display="inline"><semantics id="S4.F4.7.3.3.m3.4b"><mrow id="S4.F4.7.3.3.m3.4c"><mo rspace="0em" id="S4.F4.7.3.3.m3.1.1">*</mo><mo lspace="0em" rspace="0em" id="S4.F4.7.3.3.m3.2.2">*</mo><mo lspace="0em" rspace="0em" id="S4.F4.7.3.3.m3.3.3">*</mo><mo rspace="0.278em" id="S4.F4.7.3.3.m3.4.4">:</mo><mi id="S4.F4.7.3.3.m3.4.5">p</mi><mo id="S4.F4.7.3.3.m3.4.6">&lt;</mo><mn id="S4.F4.7.3.3.m3.4.7">0.001</mn></mrow><annotation encoding="application/x-tex" id="S4.F4.7.3.3.m3.4d">***:p&lt;0.001</annotation></semantics></math>, <math id="S4.F4.8.4.4.m4.3" class="ltx_Math" alttext="n.s.:" display="inline"><semantics id="S4.F4.8.4.4.m4.3b"><mrow id="S4.F4.8.4.4.m4.3.4.2" xref="S4.F4.8.4.4.m4.3.4.1.cmml"><mi id="S4.F4.8.4.4.m4.1.1" xref="S4.F4.8.4.4.m4.1.1.cmml">n</mi><mo lspace="0em" rspace="0.167em" id="S4.F4.8.4.4.m4.3.4.2.1" xref="S4.F4.8.4.4.m4.3.4.1a.cmml">.</mo><mi id="S4.F4.8.4.4.m4.2.2" xref="S4.F4.8.4.4.m4.2.2.cmml">s</mi><mo lspace="0em" rspace="0.167em" id="S4.F4.8.4.4.m4.3.4.2.2" xref="S4.F4.8.4.4.m4.3.4.1a.cmml">.</mo><mo id="S4.F4.8.4.4.m4.3.3" xref="S4.F4.8.4.4.m4.3.3.cmml">:</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.8.4.4.m4.3c"><apply id="S4.F4.8.4.4.m4.3.4.1.cmml" xref="S4.F4.8.4.4.m4.3.4.2"><csymbol cd="ambiguous" id="S4.F4.8.4.4.m4.3.4.1a.cmml" xref="S4.F4.8.4.4.m4.3.4.2.1">formulae-sequence</csymbol><ci id="S4.F4.8.4.4.m4.1.1.cmml" xref="S4.F4.8.4.4.m4.1.1">ùëõ</ci><ci id="S4.F4.8.4.4.m4.2.2.cmml" xref="S4.F4.8.4.4.m4.2.2">ùë†</ci><ci id="S4.F4.8.4.4.m4.3.3.cmml" xref="S4.F4.8.4.4.m4.3.3">:</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.8.4.4.m4.3d">n.s.:</annotation></semantics></math> not significant).</span></span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Conversation Quality</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>AMIE surpassed PCPs in conversation quality, per specialists and patient actors.</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Conversation quality was assessed using patient actor ratings, specialist ratings, and outputs from auto-evaluation.
&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:AMIE_example_osce</span> and <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:pcp_example_osce</span> show two example consultations for the same simulated patient from AMIE and PCP, respectively.</p>
</div>
<section id="S4.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Patient Actor Ratings.</h5>

<div id="S4.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.p1.1" class="ltx_p"><a href="#S4.F4" title="In Efficiency of Information Acquisition. ‚Ä£ 4.1.2 Auto-evaluation suggested AMIE matched PCPs‚Äô efficiency in acquiring information. ‚Ä£ 4.1 Diagnostic Accuracy ‚Ä£ 4 Results ‚Ä£ Towards Conversational Diagnostic AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a> presents the various conversation qualities patient actors assessed following their consultations with the OSCE agents.
Overall, AMIE‚Äôs consultations were rated significantly better (<math id="S4.SS2.SSS1.Px1.p1.1.m1.1" class="ltx_Math" alttext="p<0.05" display="inline"><semantics id="S4.SS2.SSS1.Px1.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.Px1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1"><lt id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.1"></lt><ci id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.2">ùëù</ci><cn type="float" id="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.Px1.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.Px1.p1.1.m1.1c">p&lt;0.05</annotation></semantics></math>) by patient actors than those from PCPs across 24 of 26 axes. No significant differences in ratings were detected for the two PCCBP axes ‚ÄúRespecting Patient‚Äôs Privacy‚Äù (N=108) and ‚ÄúAcknowledging Mistakes‚Äù (N=41). For the latter criterion, the number of exclusions was substantially higher since the question applied only when mistakes were made by the OSCE agent and pointed out in the conversation.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2401.05654/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="438" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.11.5.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.8.4" class="ltx_text ltx_font_bold" style="font-size:90%;">Specialist physician ratings.<span id="S4.F5.8.4.4" class="ltx_text ltx_font_medium"> Conversation and reasoning qualities as assessed by specialist physicians. For illustration purposes, all responses from five-point rating scales were mapped to a generic five-point scale ranging from ‚ÄòVery favorable‚Äô to ‚ÄòVery unfavorable‚Äô. The only four-point scale (DDx Comprehensiveness) was mapped to the same scale, ignoring the ‚ÄòNeither favorable nor unfavorable‚Äô option. For Yes/No questions, a (positive) ‚ÄòYes‚Äô response was mapped to the same color as ‚ÄòFavorable‚Äô and a (negative) ‚ÄôNo‚Äô response to the same color as ‚ÄòUnfavorable‚Äô. Rating scales were adapted from the Practical Assessment of Clinical Examination Skills (PACES), a narrative review about Patient-Centered Communication Best Practice (PCCBP), and other sources. Details on question wording and response options are provided in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:appendix:rubrics</span>. Asterisks represent statistical significance (<math id="S4.F5.5.1.1.m1.2" class="ltx_math_unparsed" alttext="*:p<0.05" display="inline"><semantics id="S4.F5.5.1.1.m1.2b"><mrow id="S4.F5.5.1.1.m1.2c"><mo rspace="0em" id="S4.F5.5.1.1.m1.1.1">*</mo><mo rspace="0.278em" id="S4.F5.5.1.1.m1.2.2">:</mo><mi id="S4.F5.5.1.1.m1.2.3">p</mi><mo id="S4.F5.5.1.1.m1.2.4">&lt;</mo><mn id="S4.F5.5.1.1.m1.2.5">0.05</mn></mrow><annotation encoding="application/x-tex" id="S4.F5.5.1.1.m1.2d">*:p&lt;0.05</annotation></semantics></math>, <math id="S4.F5.6.2.2.m2.3" class="ltx_math_unparsed" alttext="**:p<0.01" display="inline"><semantics id="S4.F5.6.2.2.m2.3b"><mrow id="S4.F5.6.2.2.m2.3c"><mo rspace="0em" id="S4.F5.6.2.2.m2.1.1">*</mo><mo lspace="0em" rspace="0em" id="S4.F5.6.2.2.m2.2.2">*</mo><mo rspace="0.278em" id="S4.F5.6.2.2.m2.3.3">:</mo><mi id="S4.F5.6.2.2.m2.3.4">p</mi><mo id="S4.F5.6.2.2.m2.3.5">&lt;</mo><mn id="S4.F5.6.2.2.m2.3.6">0.01</mn></mrow><annotation encoding="application/x-tex" id="S4.F5.6.2.2.m2.3d">**:p&lt;0.01</annotation></semantics></math>, <math id="S4.F5.7.3.3.m3.4" class="ltx_math_unparsed" alttext="***:p<0.001" display="inline"><semantics id="S4.F5.7.3.3.m3.4b"><mrow id="S4.F5.7.3.3.m3.4c"><mo rspace="0em" id="S4.F5.7.3.3.m3.1.1">*</mo><mo lspace="0em" rspace="0em" id="S4.F5.7.3.3.m3.2.2">*</mo><mo lspace="0em" rspace="0em" id="S4.F5.7.3.3.m3.3.3">*</mo><mo rspace="0.278em" id="S4.F5.7.3.3.m3.4.4">:</mo><mi id="S4.F5.7.3.3.m3.4.5">p</mi><mo id="S4.F5.7.3.3.m3.4.6">&lt;</mo><mn id="S4.F5.7.3.3.m3.4.7">0.001</mn></mrow><annotation encoding="application/x-tex" id="S4.F5.7.3.3.m3.4d">***:p&lt;0.001</annotation></semantics></math>, <math id="S4.F5.8.4.4.m4.3" class="ltx_Math" alttext="n.s.:" display="inline"><semantics id="S4.F5.8.4.4.m4.3b"><mrow id="S4.F5.8.4.4.m4.3.4.2" xref="S4.F5.8.4.4.m4.3.4.1.cmml"><mi id="S4.F5.8.4.4.m4.1.1" xref="S4.F5.8.4.4.m4.1.1.cmml">n</mi><mo lspace="0em" rspace="0.167em" id="S4.F5.8.4.4.m4.3.4.2.1" xref="S4.F5.8.4.4.m4.3.4.1a.cmml">.</mo><mi id="S4.F5.8.4.4.m4.2.2" xref="S4.F5.8.4.4.m4.2.2.cmml">s</mi><mo lspace="0em" rspace="0.167em" id="S4.F5.8.4.4.m4.3.4.2.2" xref="S4.F5.8.4.4.m4.3.4.1a.cmml">.</mo><mo id="S4.F5.8.4.4.m4.3.3" xref="S4.F5.8.4.4.m4.3.3.cmml">:</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.8.4.4.m4.3c"><apply id="S4.F5.8.4.4.m4.3.4.1.cmml" xref="S4.F5.8.4.4.m4.3.4.2"><csymbol cd="ambiguous" id="S4.F5.8.4.4.m4.3.4.1a.cmml" xref="S4.F5.8.4.4.m4.3.4.2.1">formulae-sequence</csymbol><ci id="S4.F5.8.4.4.m4.1.1.cmml" xref="S4.F5.8.4.4.m4.1.1">ùëõ</ci><ci id="S4.F5.8.4.4.m4.2.2.cmml" xref="S4.F5.8.4.4.m4.2.2">ùë†</ci><ci id="S4.F5.8.4.4.m4.3.3.cmml" xref="S4.F5.8.4.4.m4.3.3">:</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.8.4.4.m4.3d">n.s.:</annotation></semantics></math> not significant).</span></span></figcaption>
</figure>
</section>
<section id="S4.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Specialist Physician Ratings.</h5>

<div id="S4.SS2.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px2.p1.1" class="ltx_p">Specialist physicians evaluated both the conversational quality as well as the responses to the post-questionnaire for scenarios within their domain expertise (see <a href="#S4.F5" title="In Patient Actor Ratings. ‚Ä£ 4.2.1 AMIE surpassed PCPs in conversation quality, per specialists and patient actors. ‚Ä£ 4.2 Conversation Quality ‚Ä£ 4 Results ‚Ä£ Towards Conversational Diagnostic AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a>). Again, AMIE‚Äôs responses were rated significantly better by specialists than those from PCPs on 28 of 32 evaluation axes; Specialists preferred AMIE‚Äôs consultation, diagnoses, and management plan over those from PCPs. For this set of evaluations, differences in specialist ratings between AMIE and PCPs were statistically significant (<math id="S4.SS2.SSS1.Px2.p1.1.m1.1" class="ltx_Math" alttext="p<0.05" display="inline"><semantics id="S4.SS2.SSS1.Px2.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.Px2.p1.1.m1.1b"><apply id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1"><lt id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.1"></lt><ci id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.2">ùëù</ci><cn type="float" id="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.Px2.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.Px2.p1.1.m1.1c">p&lt;0.05</annotation></semantics></math>).
No significant differences in ratings were detected for four of the axes in the Diagnosis &amp; Management rubric, namely, ‚ÄúEscalation Recommendation Appropriate‚Äù, ‚ÄúTreatment Inappropriate Avoided‚Äù, ‚ÄúFollowup Recommendation Appropriate‚Äù and ‚ÄúConfabulation Absent‚Äù, despite no exclusions (N=149).</p>
</div>
</section>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Auto-evaluations demonstrated the effectiveness of inner self-play for AMIE.</h4>

<section id="S4.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Auto-evaluation of Conversation Ratings.</h5>

<div id="S4.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px1.p1.1" class="ltx_p">We leveraged the model-based self-CoT auto-evaluation strategy to rate conversations on four evaluation axes from the PACES rubric, and validated that these auto-evaluation ratings were accurate and well aligned with the specialist ratings (<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:autoeval_ablation</span> and&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:autoeval_vs_specialist</span>). Furthermore, to demonstrate that the inner self-play loop improved simulated dialogue quality, we applied the auto-evaluation method to the simulated dialogues generated before and after the self-play procedure. Results in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:autoeval_selfplay</span> revealed that the simulated dialogues after self-play were preferred more often than the baseline dialogues without self-critique.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Clinical History-taking and the Diagnostic Dialogue</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">History-taking and the clinical interview are widely taught in both medical schools‚Äô and postgraduate curricula&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx37" title="" class="ltx_ref">37</a>, <a href="#bib.bibx38" title="" class="ltx_ref">38</a>, <a href="#bib.bibx39" title="" class="ltx_ref">39</a>, <a href="#bib.bibx40" title="" class="ltx_ref">40</a>, <a href="#bib.bibx41" title="" class="ltx_ref">41</a>, <a href="#bib.bibx42" title="" class="ltx_ref">42</a>]</cite>. Consensus on physician-patient communication has evolved to embrace patient-centred communication practices, with recommendations that communication in clinical encounters should address six core functions: fostering the relationship, gathering information, providing information, making decisions, responding to emotions and enabling disease- and treatment-related behavior&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">20</a>, <a href="#bib.bibx43" title="" class="ltx_ref">43</a>, <a href="#bib.bibx44" title="" class="ltx_ref">44</a>]</cite>. Specific skills and behaviours for meeting these goals have also been described, taught and assessed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx45" title="" class="ltx_ref">45</a>, <a href="#bib.bibx20" title="" class="ltx_ref">20</a>]</cite> with validated tools&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx45" title="" class="ltx_ref">45</a>]</cite>. Medical conventions consistently cite that certain categories of information should be gathered during a clinical interview, comprising topics such as the presenting complaint, past medical history and medication history, social and family history, and systems review&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx46" title="" class="ltx_ref">46</a>, <a href="#bib.bibx47" title="" class="ltx_ref">47</a>]</cite>. Clinicians‚Äô ability to meet these goals is commonly assessed using the framework of an objective structured clinical examination (OSCE)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">31</a>, <a href="#bib.bibx32" title="" class="ltx_ref">32</a>, <a href="#bib.bibx33" title="" class="ltx_ref">33</a>]</cite>. Such assessments vary in their reproducibility or implementation and have even been adapted for remote practice as virtual OSCEs (vOSCEs) with telemedical scenarios, an issue of particular relevance during the COVID-19 pandemic&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx48" title="" class="ltx_ref">48</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Conversational AI and Goal-oriented Dialogue</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Conversational AI systems for goal-oriented dialogue and task completion have a rich history&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx49" title="" class="ltx_ref">49</a>, <a href="#bib.bibx50" title="" class="ltx_ref">50</a>, <a href="#bib.bibx51" title="" class="ltx_ref">51</a>]</cite>. The emergence of transformers&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx52" title="" class="ltx_ref">52</a>]</cite> and large language models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite> have led to renewed interest in this direction. The development of strategies for alignment&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx53" title="" class="ltx_ref">53</a>]</cite>, self-improvement&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx54" title="" class="ltx_ref">54</a>, <a href="#bib.bibx55" title="" class="ltx_ref">55</a>, <a href="#bib.bibx56" title="" class="ltx_ref">56</a>, <a href="#bib.bibx57" title="" class="ltx_ref">57</a>]</cite> and scalable oversight mechanisms&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx58" title="" class="ltx_ref">58</a>]</cite> have enabled large scale deployment of such conversational systems in the real world&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>, <a href="#bib.bibx59" title="" class="ltx_ref">59</a>]</cite>.
However, the rigorous evaluation and exploration of conversational and task-completion capabilities of such AI systems remains limited for clinical applications, where studies have largely focused on single-turn interaction use cases such as question-answering or summarization.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>AI for Medical Consultations and Diagnostic Dialogue</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The majority of explorations of AI as tools for conducting medical consultations have focused on ‚Äúsymptom checker‚Äù applications rather than a full natural dialogue, or on topics such as transcription of medical audio or the generation of plausible dialogue given clinical notes or summaries&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">60</a>, <a href="#bib.bibx61" title="" class="ltx_ref">61</a>, <a href="#bib.bibx62" title="" class="ltx_ref">62</a>, <a href="#bib.bibx63" title="" class="ltx_ref">63</a>]</cite>. Language models have been trained using clinical dialogue datasets but not comprehensively evaluated&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx64" title="" class="ltx_ref">64</a>]</cite>. Studies have been grounded in messages between doctors and patients in commercial chat platforms (which may have altered doctor-patient engagement compared to 1:1 medical consultations)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx65" title="" class="ltx_ref">65</a>, <a href="#bib.bibx66" title="" class="ltx_ref">66</a>, <a href="#bib.bibx28" title="" class="ltx_ref">28</a>]</cite>. Many focused largely on predicting next turns in the recorded exchanges rather than clinically meaningful metrics. And to date, there have been no reported studies that have examined the quality of AI models for diagnostic dialogue using the same criteria that are used to examine and train human physicians in dialogue and communication skills; nor evaluating AI systems in common frameworks such as the OSCE.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Evaluation of Diagnostic Dialogue</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Prior frameworks for human evaluation of AI systems‚Äô performance in diagnostic dialogue have been limited in detail. They have not been anchored in established criteria for assessing communication skills and the quality of history-taking. For example,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">29</a>]</cite> reported a 5-point scale describing overall ‚Äúhuman evaluation‚Äù,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx65" title="" class="ltx_ref">65</a>]</cite> reported ‚Äúrelevance, informativeness and human likeness‚Äù,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx66" title="" class="ltx_ref">66</a>]</cite> reported ‚Äúfluency, expertise and relevance‚Äù,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx67" title="" class="ltx_ref">67</a>]</cite> ‚Äúfluency and adequacy‚Äù and&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx68" title="" class="ltx_ref">68</a>]</cite> ‚Äúfluency‚Äù. These criteria are far less comprehensive and specific than those taught and practiced by medical professionals. A multi-agent framework for assessing conversational capabilities of LLMs is introduced in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx64" title="" class="ltx_ref">64</a>]</cite>, however, the study was performed in the restricted setting of dermatology, used AI models to emulate both doctor and patient sides of simulated interactions, and performed limited expert evaluation of history-taking as ‚Äúcomplete‚Äù or not.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this study, we introduced AMIE, an LLM based AI system optimised for clinical dialogue with diagnostic reasoning capabilities. We compared AMIE consultations to those performed by PCPs using a randomized, double-blind crossover study with human simulated patients in the style of an Objective Structured Clinical Examination (OSCE). Notably, our study was not designed to be representative of clinical conventions either for traditional OSCE evaluations, for remote- or tele-medical consultation practices, or for the ways clinicians usually use text and chat messaging to communicate with patients. Our evaluation instead mirrored the most common way by which people interact with LLMs today, leveraging a potentially scalable and familiar mechanism for AI systems to engage in remote diagnostic dialogue. In this setting, we observed that AMIE, an AI system optimised specifically for the task, outperformed PCPs on simulated diagnostic conversations when evaluated along multiple clinically-meaningful axes of consultation quality.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Diagnostic Performance.</h5>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">The differential diagnoses provided by AMIE were more accurate and complete than those provided by board-certified PCPs, when both were evaluated by specialist physicians. Previous research has shown that AI systems may match or exceed human diagnostic performance in specific, narrow tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx69" title="" class="ltx_ref">69</a>, <a href="#bib.bibx70" title="" class="ltx_ref">70</a>, <a href="#bib.bibx71" title="" class="ltx_ref">71</a>]</cite> in retrospective evaluation. However, these situations typically involved both AI and physicians interpreting the same fixed input (for example, identifying the presence of a specific finding in a medical image). Our study was significantly more challenging because it required the AI system to actively acquire relevant information through conversation rather than relying on clinical information collated by human efforts&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx72" title="" class="ltx_ref">72</a>]</cite>. Therefore the system‚Äôs downstream differential diagnoses depended on not only its diagnostic inference capability, but also the quality of information gathered under uncertainty through natural conversation and building rapport.</p>
</div>
<div id="S6.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p2.1" class="ltx_p">Our results suggested that AMIE was as adept as PCPs in eliciting pertinent information during the simulated consultations and was more accurate than PCPs in formulating a complete differential diagnosis if given the same amount of acquired information. This finding corroborates other work that LLMs may be able to produce more complete differential diagnoses given the same clinical information as physicians in challenging cases &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx70" title="" class="ltx_ref">70</a>]</cite>. Though not explored in this study, the assistive performance of AMIE therefore represents an interesting and important avenue for future research, particularly given the real-world importance of expert oversight for AI systems in safety-critical settings such as medicine.</p>
</div>
<div id="S6.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p3.1" class="ltx_p">Our study utilized a wide variety of simulated patients, comprising actors trained in both Canada and India and scenarios across a range of specialties. This allowed us to explore how performance varied along multiple axes: by specialty, and by the locations in which the scenario was derived and enacted. We observed that both PCPs and AMIE performed worse in obstetric/gynecology and internal medicine scenarios than those from other specialties (see&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:specialist_ddx_ratings_by_specialty</span>). The study was not powered or designed to compare performance between different specialty topics, and we cannot exclude that the scenarios in some specialties might be harder than others. We observed that both AMIE and PCPs had higher diagnostic accuracy in consultations performed in the Canada OSCE lab compared to those enacted in the India OSCE lab (see&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:per_location_specialist_eval</span>). However, the differences were not statistically significant and in a subset of 40 scenarios enacted in both the Canada OSCE lab and the India OSCE lab, the performance of both AMIE and PCPs was equivalent (see&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:same_scenario_location_ddx</span>).</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Conversational Performance.</h5>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p">Patient actors and specialist raters both evaluated AMIE‚Äôs performance to be higher than PCPs on metrics related to empathy and communication skills. These axes comprised a majority of the dimensions that were evaluated. This general finding is consistent with a prior study where LLM responses were found to be more empathetic than the responses from clinicians to health questions posted on Reddit&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx73" title="" class="ltx_ref">73</a>]</cite>. However, the findings in that study may not be generalised directly to our setting due to the differences in study design. Specifically, prior work has not involved a direct, randomised comparison of physicians and AI systems in a prospective simulation of multi-turn dialogue with the same patient. In both settings, the lack of voice-based and non-verbal visual communication may be an unfair disadvantage to clinicians.</p>
</div>
<div id="S6.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p2.1" class="ltx_p">The text-based chat interface used in this study introduces both advantages and disadvantages. People today most commonly engage with LLMs through synchronous text-chat interfaces&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx74" title="" class="ltx_ref">74</a>]</cite>, and patients often use patient portals to send messages to their providers. We therefore chose this mode of interaction as a representative interface for LLMs to perform multi-turn conversation, adapting the virtual OSCE framework accordingly. While this allowed a fair comparison of diagnostic dialogue between LLMs and clinicians when both were restricted to a synchronous text-chat, it is important to acknowledge that our experiments do not emulate the expected quality of diagnostic dialogue in real clinical practice (including telemedicine). Physicians may be more used to history-taking and diagnostic dialogue by telephone or video consultation than synchronous text-chat communication&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx75" title="" class="ltx_ref">75</a>, <a href="#bib.bibx76" title="" class="ltx_ref">76</a>]</cite>. Instead, text is more commonly used by clinicians to communicate with patients for episodic or asynchronous needs such as prescription refills or communication about specific test results&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx77" title="" class="ltx_ref">77</a>]</cite>. Physicians may thus be more familiar with text/SMS or email rather than the synchronous text-chat medium we employed in this study. In both text/SMS and email, the conventions and expectations for communicating naturally and with empathic style might be different&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx78" title="" class="ltx_ref">78</a>]</cite>. It is possible that the PCPs in our study had not yet become accustomed to the setting, and may have performed differently if subjected to a specific training program (similar in spirit to the training process for AMIE). Clinicians participating in the study undertook two preparatory pilot sessions of consultations with our synchronous text interface before the evaluation began, but this was not a formal training program, nor was it designed to optimize clinicians‚Äô performance. Future research could explore this question more thoroughly including monitoring for the impact of a learning curve, or exploring whether performance varies according to the extent to which participating clinicians or simulated patients are familiar with telemedicine.</p>
</div>
<div id="S6.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p3.1" class="ltx_p">Additionally, our findings regarding empathic communication could also be partially attributed to the fact that AMIE responses were significantly longer than clinician responses (shown in&nbsp;<span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:number_of_words_and_turns</span>), and presented with greater structure. This could potentially suggest to an observer that more time was spent preparing the response, analogous to known findings that patient satisfaction increases with time spend with their physicians&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx79" title="" class="ltx_ref">79</a>, <a href="#bib.bibx80" title="" class="ltx_ref">80</a>, <a href="#bib.bibx81" title="" class="ltx_ref">81</a>]</cite>.</p>
</div>
<div id="S6.SS0.SSS0.Px2.p4" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p4.1" class="ltx_p">Collectively, our findings suggest many avenues for further research that might leverage human-AI complementarity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx82" title="" class="ltx_ref">82</a>]</cite>, combining clinicians‚Äô skills in the analysis of verbal and non-verbal cues with the potential strengths of LLMs to suggest more enriched conversational responses including empathic statements, structure, eloquence, or more complete differential diagnoses.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Simulated Dialogue.</h5>

<div id="S6.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px3.p1.1" class="ltx_p">The use of simulated data allowed us to quickly scale training to a broad set of conditions and patient contexts, while the injection of knowledge from search encouraged these dialogues to remain grounded and realistic. Though the simulated patients encompassed a wide range of conditions, they failed to capture the full range of potential patient backgrounds, personalities, and motivations. Through the inner self-play procedure, we were able to iteratively improve the simulated dialogue we generated and used in fine-tuning. However, these improvements were limited by our ability to articulate what makes a good dialogue in the critic instructions, the critic‚Äôs ability to produce effective feedback, and AMIE‚Äôs ability to adapt to such feedback. For example, in the simulated environment we impose that AMIE reaches a proposed differential and testing/treatment plan for the patient, but such an endpoint may be unrealistic for some conditions, especially in the virtual chat-based setting.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Framework.</h5>

<div id="S6.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px4.p1.1" class="ltx_p">In contrast to prior works, we anchored our evaluation in criteria already established to be relevant for assessing physicians‚Äô communication skills and history-taking quality. We performed more extensive and diverse human evaluation than prior studies of AI systems, with ratings from both clinicians and simulated patients perspective. Our raters and scenarios were sourced from multiple geographic locations, including North America, India and the UK. Our pilot evaluation rubric is, to our knowledge, the first to evaluate LLMs‚Äô history-taking and communication skills using axes that are also measured in the real world for physicians themselves, increasing the clinical relevance of our research. Our evaluation framework is considerably more granular and specific than prior works on AI-generated clinical dialogue, which have not considered patient-centred communication best practice or clinically-relevant axes of consultation quality&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">29</a>, <a href="#bib.bibx65" title="" class="ltx_ref">65</a>, <a href="#bib.bibx66" title="" class="ltx_ref">66</a>, <a href="#bib.bibx67" title="" class="ltx_ref">67</a>, <a href="#bib.bibx68" title="" class="ltx_ref">68</a>, <a href="#bib.bibx64" title="" class="ltx_ref">64</a>]</cite>.</p>
</div>
<div id="S6.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S6.SS0.SSS0.Px4.p2.1" class="ltx_p">However, our pilot framework is not definitive and can be further improved in future research. History-taking itself is contextual and what determines a ‚Äúgood history‚Äù is dependent on the specific clinical situation, patient and physician attributes, cultural characteristics, and many other factors. Despite variation in models for clinical history-taking&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx83" title="" class="ltx_ref">83</a>, <a href="#bib.bibx84" title="" class="ltx_ref">84</a>, <a href="#bib.bibx85" title="" class="ltx_ref">85</a>, <a href="#bib.bibx86" title="" class="ltx_ref">86</a>]</cite>, studies have shown that good clinical interviews are associated with not only problem detection and diagnostic accuracy, but also quadruple aims for care delivery&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx87" title="" class="ltx_ref">87</a>, <a href="#bib.bibx88" title="" class="ltx_ref">88</a>]</cite> ranging from patient and physician satisfaction, resilience to stress and illness, and health outcomes or cost.
Future studies on the quality of LLM history-taking might therefore utilise prospective measures of these outcomes in real-world settings (for example reductions in patient complaints&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx89" title="" class="ltx_ref">89</a>]</cite>, or improvements in cost and care effectiveness, patient and provider satisfaction), though evaluations as such may be challenging or impractical to compare to standard practice in the same individual patient, and randomisation of different approaches may also be challenging in real-world settings.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Breadth of Evaluation.</h5>

<div id="S6.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px5.p1.1" class="ltx_p">Our chosen axes of evaluation were not exhaustive and their interpretation was often subjective in nature. Although we conducted evaluations from both clinician and lay-perspectives, generating scenario-packs in three countries with assessors in both North America and India, the pool of clinicians and lay-people assessing the models could be expanded further to improve generalization of our insights. Our experiments could also undergo more extensive replication to explore other aspects such as inter-observer and inter-participant variability, including future work with an intentionally further diversified pool of human raters (clinicians and lay users). Participatory design in the development of model evaluation tools with a representative pool of patients, as well as clinical and health equity domain experts, could also be valuable.</p>
</div>
<div id="S6.SS0.SSS0.Px5.p2" class="ltx_para">
<p id="S6.SS0.SSS0.Px5.p2.1" class="ltx_p">Although our scenarios comprised many different clinical conditions and specialties, our experiments were not necessarily representative of the decades of clinical practice accumulated by even a single doctor (who on average may perform tens of thousands of consultations in a career&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx90" title="" class="ltx_ref">90</a>]</cite>). The range of conditions possible to examine in medicine is vast as is the variation in presentation of individual diseases. Our experiments were not designed to examine multi-morbidity and co-incident pathology, longitudinal case presentation or the consideration of sequential information from clinical investigations. We excluded entirely some clinical settings or specialties such as psychiatry, pediatrics, intensive care, and inpatient case management scenarios. Further research would be needed to understand the applicability of our findings in many settings such as these, where the requirements for high-quality history-taking might differ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx91" title="" class="ltx_ref">91</a>, <a href="#bib.bibx92" title="" class="ltx_ref">92</a>]</cite>. The OSCE framework is commonly used in the assessment of clinicians‚Äô skills. It encompasses a significant range of methodologies including real or simulated patients, interaction with physical artefacts or clinical materials, applications to a variety of medical specialties, tasks or settings; and both remote or in-person assessments. Although the OSCE approach is popular, there are significant limitations to its validity&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx93" title="" class="ltx_ref">93</a>]</cite>. We utilised a remote text-based assessment, replicating known issues with the paradigm of ‚Äúvirtual OSCE‚Äù such as the inability to incorporate non-verbal symptoms, signs and communication features. Additionally, this format could introduce unfamiliar constraints to the communication of PCP participants &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx48" title="" class="ltx_ref">48</a>]</cite>.</p>
</div>
<div id="S6.SS0.SSS0.Px5.p3" class="ltx_para">
<p id="S6.SS0.SSS0.Px5.p3.1" class="ltx_p">The tone, content, and nature of the OSCE dialogues in our study are likely not to be representative of real-world patient populations. For example, patient actors may have described their symptoms with greater structure, depth or clinical detail than could be routinely expected in many consultations, or had greater comprehension of clinical context than would be ordinarily expected. Furthermore, although evaluation was blinded, the style of responses from AMIE was notably different to that by PCPs which limits the practical extent of blinding in study design.</p>
</div>
<div id="S6.SS0.SSS0.Px5.p4" class="ltx_para">
<p id="S6.SS0.SSS0.Px5.p4.1" class="ltx_p">Therefore even within the distribution of diseases and specialties we addressed, our findings should be interpreted with humility and caution. There is a need for further research to examine varied presentations of the same diseases, alongside exploration of alternate approaches to evaluating history-taking and clinical dialogue in situations of different patient needs, preferences, behaviours and circumstances.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fairness and Bias.</h5>

<div id="S6.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px6.p1.1" class="ltx_p">The evaluation protocol presented in this paper is limited in terms of its ability to capture potential issues related to fairness and bias, which remains an important open question that we will aim to address in subsequent system evaluations. Recent advances in the development of comprehensive frameworks for bias detection in large language models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx94" title="" class="ltx_ref">94</a>, <a href="#bib.bibx95" title="" class="ltx_ref">95</a>]</cite> present a promising starting point for establishing such an approach. It should be noted that medical diagnostic dialogue is a particularly challenging use case, due to the complexity of the medical domain, the interactive information gathering nature of the dialogue, and the outcome-driven setting, with the potential of associated harms in case of incorrect diagnosis or incorrect medical advice. Nevertheless, disentangling these issues is an important further research area if LLMs in the domain are to overcome rather than propagate inequities in healthcare. For example, previous studies have found that physicians approach communication with their patients differently, on average, depending on patients‚Äô race, resulting in Black patients receiving communication that was less patient-centered, and with a lower positive affect&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx96" title="" class="ltx_ref">96</a>]</cite>. Other studies have found differences in physicians‚Äô communication styles and conversation length based on gender&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx97" title="" class="ltx_ref">97</a>]</cite>. Effective intercultural communication skills are essential&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx91" title="" class="ltx_ref">91</a>]</cite>. There is therefore a non-negligible risk that such historical conversational biases may be replicated or amplified in an AI dialogue system, but at the same time there is also an opportunity to work towards designing conversational systems that can be more inclusive, and more personalized to the individual patient‚Äôs needs.</p>
</div>
<div id="S6.SS0.SSS0.Px6.p2" class="ltx_para">
<p id="S6.SS0.SSS0.Px6.p2.1" class="ltx_p">To help inform the development of the necessary fairness, bias, and equity frameworks, it is important to employ a participatory approach to solicit representative views across a wide range of patient demographics, as well as clinical and health equity domain experts. Such evaluation frameworks should be complemented by extensive model red teaming and an adversarial approach to identifying any remaining gaps and failure modes. Recent advances in red teaming LLMs could be useful in this scenario&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx98" title="" class="ltx_ref">98</a>, <a href="#bib.bibx99" title="" class="ltx_ref">99</a>, <a href="#bib.bibx100" title="" class="ltx_ref">100</a>, <a href="#bib.bibx101" title="" class="ltx_ref">101</a>]</cite>. These practices should not only inform the evaluation of the final model, but also its development and iterative refinement. Model development should follow the established data and model reporting practices and provide transparency into the training data and the associated decision processes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx102" title="" class="ltx_ref">102</a>, <a href="#bib.bibx103" title="" class="ltx_ref">103</a>, <a href="#bib.bibx104" title="" class="ltx_ref">104</a>]</cite>. The dialogue research dataset contributing to AMIE training data in our study was de-identified, reducing the availability of socio-economic factors, patient demographics, and information about clinical settings and locations.</p>
</div>
<div id="S6.SS0.SSS0.Px6.p3" class="ltx_para">
<p id="S6.SS0.SSS0.Px6.p3.1" class="ltx_p">Further work is also needed to ensure the robustness of medical LLMs in multilingual settings&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx105" title="" class="ltx_ref">105</a>, <a href="#bib.bibx106" title="" class="ltx_ref">106</a>, <a href="#bib.bibx107" title="" class="ltx_ref">107</a>, <a href="#bib.bibx108" title="" class="ltx_ref">108</a>]</cite>, and particularly their performance in low-resource languages&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx109" title="" class="ltx_ref">109</a>]</cite>. The great variety of cultures&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx110" title="" class="ltx_ref">110</a>]</cite>, languages, localities, identities, and localized medical needs, makes the task of generating a priori static yet comprehensive fairness benchmarks practically infeasible. Measurement and mitigation of bias must move beyond the traditional narrow focus on specific axes that fails to scale globally&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx111" title="" class="ltx_ref">111</a>]</cite>. LLM-based evaluators present a potential solution for preliminary assessments in languages where there are no systematic benchmarks, though prior studies have found these auto-evaluation frameworks to be biased, underscoring the need for calibrating them on native speaker evaluations, and using them with caution&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx112" title="" class="ltx_ref">112</a>]</cite>.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px7" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deployment.</h5>

<div id="S6.SS0.SSS0.Px7.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px7.p1.1" class="ltx_p">This research demonstrates the potential of LLMs for future use in healthcare in the context of diagnostic dialogue. Transitioning from an LLM research prototype that has been evaluated in this study to a safe and robust tool that can be used by healthcare providers, administrators, and people will require significant additional research to ensure the safety, reliability, efficacy, and privacy of the technology. Careful consideration will need to be given to the ethical deployment of this technology including rigorous quality assessment across different clinical settings and research into reliable uncertainty estimation methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx113" title="" class="ltx_ref">113</a>, <a href="#bib.bibx114" title="" class="ltx_ref">114</a>, <a href="#bib.bibx115" title="" class="ltx_ref">115</a>, <a href="#bib.bibx116" title="" class="ltx_ref">116</a>]</cite> that would allow for deferral to human clinical experts when needed. These and other guardrails are needed to mitigate potential overreliance on LLM technologies, with other specific measures for attention to ethical and regulatory requirements particular to future use-cases and the presence of qualified physicians in the loop to safeguard any model outputs. Additional research will also be needed to assess the extent to which biases and security vulnerabilities might arise either from base models or the circumstances of use in deployment, as we have highlighted in our prior work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite>. Given the continuous evolution of clinical knowledge, it will also be important to develop ways for LLMs to utilize up-to-date clinical information&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx117" title="" class="ltx_ref">117</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">The utility of medical AI systems could be greatly improved if they are better able to interact conversationally, anchoring on large-scale medical knowledge while communicating with appropriate levels of empathy and trust.
This research demonstrates the significant potential capabilities of LLM based AI systems for settings involving clinical history-taking and diagnostic dialogue. The performance of AMIE in simulated consultations represents a milestone for the field, as it was assessed along an evaluation framework that considered multiple clinically-relevant axes for conversational diagnostic medical AI. However, the results should be interpreted with appropriate caution. Translating from this limited scope of experimental simulated history-taking and diagnostic dialogue, towards real-world tools for people and those who provide care for them, requires significant additional research and development to ensure the safety, reliability, fairness, efficacy, and privacy of the technology. If successful, we believe AI systems such as AMIE can be at the core of next generation learning health systems that help scale world class healthcare to everyone.</p>
</div>
<section id="S7.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S7.SS0.SSSx1.p1" class="ltx_para">
<p id="S7.SS0.SSSx1.p1.1" class="ltx_p">This project was an extensive collaboration between many teams at Google Research and Google DeepMind.
We thank Yun Liu, Daniel McDuff, Jake Sunshine, Ali Connell, Paul McGovern and Zoubin Ghahramani for their comprehensive review and detailed feedback on the manuscript. We also thank Sami Lachgar, Lauren Winer, John Guilyard and Maggie Shiels for contributions to the narratives and visuals.
We are grateful to Julie Anne Seguin, Sally Goldman, Yuri Vasilevski, Xinying Song, Akshay Goel, Chu-ling Ko, Abhinav Das, Haiyang Yu, Chang Liu, Yuchen Liu, SiWai Man, Brett Hatfield, Sean Li, Ajay Joshi, Gordon Turner, Annisah Um‚Äôrani, Divya Pandya and Preeti Singh for their valuable insights, technical support and
feedback during our research.
We also thank our clinical provider partners in Canada and India for their partnership in conducting the OSCE study. Finally, we are grateful to Dale Webster, Ewa Dominowska, David Fleet, Philip Mansfield, Sushant Prakash, Renee Wong, Susan Thomas, Michael Howell, Karen DeSalvo, Jeff Dean, James Manyika, Zoubin Ghahramani and Demis Hassabis for their support during the course of this project.</p>
</div>
</section>
<section id="S7.SS0.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Data Availability</h4>

<div id="S7.SS0.SSSx2.p1" class="ltx_para">
<p id="S7.SS0.SSSx2.p1.1" class="ltx_p">Some of the real-world datasets used in the development of AMIE are open-source (MedQA). The scenario packs from UK used in the OSCE study are also available for download on the internet.</p>
</div>
</section>
<section id="S7.SS0.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Code Availability</h4>

<div id="S7.SS0.SSSx3.p1" class="ltx_para">
<p id="S7.SS0.SSSx3.p1.1" class="ltx_p">AMIE is an LLM based research AI system for diagnostic dialogue. We are not open-sourcing model code and weights due to the safety implications of unmonitored use of such a system in medical settings. In the interest of responsible innovation, we will be working with research partners, regulators, and providers to validate and explore safe onward uses of AMIE. For reproducibility, we have documented technical deep learning methods while keeping the paper accessible to a clinical and general scientific audience. Our work builds upon PaLM 2, for which technical details have been described extensively in the technical report <cite class="ltx_cite ltx_citemacro_cite">[<a href="#biba.bibx2" title="" class="ltx_ref">119</a>]</cite>.</p>
</div>
</section>
<section id="S7.SS0.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Competing Interests</h4>

<div id="S7.SS0.SSSx4.p1" class="ltx_para">
<p id="S7.SS0.SSSx4.p1.1" class="ltx_p">This study was funded by Alphabet Inc and/or a subsidiary thereof (‚ÄòAlphabet‚Äô). All authors are employees of Alphabet and may own stock as part of the standard compensation package.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">George Libman Engel and William L Morgan
</span>
<span class="ltx_bibblock">‚ÄúInterviewing the patient‚Äù
</span>
<span class="ltx_bibblock">Saunders, Philadelphia, London, 1973
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">MICHAEL C Peterson, JOHN H Holbrook, DE Von Hales, NL Smith and LV Staker
</span>
<span class="ltx_bibblock">‚ÄúContributions of the history, physical examination, and laboratory investigation in making medical diagnoses.‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">Western Journal of Medicine</em> <span id="bib.bibx2.2.2" class="ltx_text ltx_font_bold">156.2</span>
</span>
<span class="ltx_bibblock">BMJ Publishing Group, 1992, pp. 163
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">John R Hampton, MJ Harrison, John R Mitchell, Jane S Prichard and Carol Seymour
</span>
<span class="ltx_bibblock">‚ÄúRelative contributions of history-taking, physical examination, and laboratory investigation to diagnosis and management of medical outpatients.‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Br Med J</em> <span id="bib.bibx3.2.2" class="ltx_text ltx_font_bold">2.5969</span>
</span>
<span class="ltx_bibblock">British Medical Journal Publishing Group, 1975, pp. 486‚Äì489
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Jerome P Kassirer
</span>
<span class="ltx_bibblock">‚ÄúTeaching clinical medicine by iterative hypothesis testing: let‚Äôs preach what we practice‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">New England Journal of Medicine</em> <span id="bib.bibx4.2.2" class="ltx_text ltx_font_bold">309.15</span>
</span>
<span class="ltx_bibblock">Mass Medical Soc, 1983, pp. 921‚Äì923
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">M Roshan and AP Rao
</span>
<span class="ltx_bibblock">‚ÄúA study on relative contributions of the history, physical examination and investigations in making medical diagnosis.‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">The Journal of the Association of Physicians of India</em> <span id="bib.bibx5.2.2" class="ltx_text ltx_font_bold">48.8</span>, 2000, pp. 771‚Äì775
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Gerald Sandler
</span>
<span class="ltx_bibblock">‚ÄúThe importance of the history in the medical clinic and the cost of unnecessary tests‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">American heart journal</em> <span id="bib.bibx6.2.2" class="ltx_text ltx_font_bold">100.6</span>
</span>
<span class="ltx_bibblock">Elsevier, 1980, pp. 928‚Äì931
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Jonathan Silverman, Suzanne Kurtz and Juliet Draper
</span>
<span class="ltx_bibblock">‚ÄúSkills for communicating with patients‚Äù
</span>
<span class="ltx_bibblock">crc press, 2016
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Timothy Rennie, Jennifer Marriott and Tina P Brock
</span>
<span class="ltx_bibblock">‚ÄúGlobal supply of health professionals‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">N Engl J Med</em> <span id="bib.bibx8.2.2" class="ltx_text ltx_font_bold">370.23</span>, 2014, pp. 2246‚Äì7
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">‚ÄúGPT-4 Technical Report‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2303.08774" title="" class="ltx_ref ltx_href">2303.08774 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> Google
</span>
<span class="ltx_bibblock">‚ÄúPaLM 2 Technical Report‚Äù, <a target="_blank" href="https://ai.google/static/documents/palm2techreport.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.google/static/documents/palm2techreport.pdf</a>, 2023
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Google Deepmind
</span>
<span class="ltx_bibblock">‚ÄúGemini: A Family of Highly Capable Multimodal Models‚Äù, <a target="_blank" href="https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/r7G7RrtT6rnM/v0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/r7G7RrtT6rnM/v0</a>, 2023
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis and Stephen Pfohl
</span>
<span class="ltx_bibblock">‚ÄúLarge Language Models Encode Clinical Knowledge‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.13138</em>, 2022
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis and Darlene Neal
</span>
<span class="ltx_bibblock">‚ÄúTowards expert-level medical question answering with large language models‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.09617</em>, 2023
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li and Weishung Liu
</span>
<span class="ltx_bibblock">‚ÄúCan Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.16452</em>, 2023
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker and Yu Du
</span>
<span class="ltx_bibblock">‚ÄúLaMDA: Language models for dialog applications‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.08239</em>, 2022
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">‚ÄúIntroducing ChatGPT‚Äù, 2022
</span>
<span class="ltx_bibblock">OpenAI
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a>
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Augustin Toma, Patrick R Lawler, Jimmy Ba, Rahul G Krishnan, Barry B Rubin and Bo Wang
</span>
<span class="ltx_bibblock">‚ÄúClinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.12031</em>, 2023
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Zeming Chen, Alejandro Hern√°ndez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K√∂pf and Amirkeivan Mohtashami
</span>
<span class="ltx_bibblock">‚ÄúMEDITRON-70B: Scaling Medical Pretraining for Large Language Models‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.16079</em>, 2023
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">David Levine
</span>
<span class="ltx_bibblock">‚ÄúHistory taking is a complex skill‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">BMJ</em> <span id="bib.bibx19.2.2" class="ltx_text ltx_font_bold">358</span>
</span>
<span class="ltx_bibblock">British Medical Journal Publishing Group, 2017
</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Ann King and Ruth B Hoppe
</span>
<span class="ltx_bibblock">‚Äú‚ÄúBest practice‚Äù for patient-centered communication: a narrative review‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx20.1.1" class="ltx_emph ltx_font_italic">Journal of graduate medical education</em> <span id="bib.bibx20.2.2" class="ltx_text ltx_font_bold">5.3</span>
</span>
<span class="ltx_bibblock">The Accreditation Council for Graduate Medical Education Suite 2000, 515&nbsp;‚Ä¶, 2013, pp. 385‚Äì393
</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang and Peter Szolovits
</span>
<span class="ltx_bibblock">‚ÄúWhat disease does this patient have? a large-scale open domain question answering dataset from medical exams‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx21.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em> <span id="bib.bibx21.2.2" class="ltx_text ltx_font_bold">11.14</span>
</span>
<span class="ltx_bibblock">MDPI, 2021, pp. 6421
</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi and Roger G Mark
</span>
<span class="ltx_bibblock">‚ÄúMIMIC-III, a freely accessible critical care database‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.1.1" class="ltx_emph ltx_font_italic">Scientific data</em> <span id="bib.bibx22.2.2" class="ltx_text ltx_font_bold">3.1</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group, 2016, pp. 1‚Äì9
</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Chung-Cheng Chiu, Anshuman Tripathi, Katherine Chou, Chris Co, Navdeep Jaitly, Diana Jaunzeikare, Anjuli Kannan, Patrick Nguyen, Hasim Sak and Ananth Sankar
</span>
<span class="ltx_bibblock">‚ÄúSpeech recognition for medical conversations‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.07274</em>, 2017
</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Ashish Sharma, Adam S Miner, David C Atkins and Tim Althoff
</span>
<span class="ltx_bibblock">‚ÄúA computational approach to understanding empathy expressed in text-based mental health support‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.08441</em>, 2020
</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Yao Fu, Hao Peng, Tushar Khot and Mirella Lapata
</span>
<span class="ltx_bibblock">‚ÄúImproving language model negotiation with self-play and in-context learning from ai feedback‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.10142</em>, 2023
</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Asma Ben Abacha, Wen-Wai Yim, Griffin Adams, Neal Snider and Meliha Yetisgen-Yildiz
</span>
<span class="ltx_bibblock">‚ÄúOverview of the mediqa-chat 2023 shared tasks on the summarization &amp; generation of doctor-patient conversations‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th Clinical Natural Language Processing Workshop</em>, 2023, pp. 503‚Äì513
</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Bogdan Ionescu, Henning M√ºller, Ana-Maria DrƒÉgulinescu, Wen-Wai Yim, Asma Ben Abacha, Neal Snider, Griffin Adams, Meliha Yetisgen, Johannes R√ºckert and Alba G. de Herrera
</span>
<span class="ltx_bibblock">‚ÄúOverview of the ImageCLEF 2023: Multimedia Retrieval in Medical, Social Media and Internet Applications‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx27.1.1" class="ltx_emph ltx_font_italic">International Conference of the Cross-Language Evaluation Forum for European Languages</em>, 2023, pp. 370‚Äì396
</span>
<span class="ltx_bibblock">Springer
</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Zhenfeng He, Yuqiang Han, Zhenqiu Ouyang, Wei Gao, Hongxu Chen, Guandong Xu and Jian Wu
</span>
<span class="ltx_bibblock">‚ÄúDialMed: A Dataset for Dialogue-based Medication Recommendation‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.07094</em>, 2022
</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Usman Naseem, Ajay Bandi, Shaina Raza, Junaid Rashid and Bharathi Raja Chakravarthi
</span>
<span class="ltx_bibblock">‚ÄúIncorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st Workshop on Biomedical Language Processing</em>, 2022, pp. 110‚Äì115
</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Jane Dacre, Mike Besser and Patricia White
</span>
<span class="ltx_bibblock">‚ÄúMRCP (UK) PART 2 Clinical Examination (PACES): a review of the first four examination sessions (June 2001‚ÄìJuly 2002)‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx30.1.1" class="ltx_emph ltx_font_italic">Clinical Medicine</em> <span id="bib.bibx30.2.2" class="ltx_text ltx_font_bold">3.5</span>
</span>
<span class="ltx_bibblock">Royal College of Physicians, 2003, pp. 452
</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">David A Sloan, Michael B Donnelly, Richard W Schwartz and William E Strodel
</span>
<span class="ltx_bibblock">‚ÄúThe Objective Structured Clinical Examination. The new gold standard for evaluating postgraduate clinical performance.‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx31.1.1" class="ltx_emph ltx_font_italic">Annals of surgery</em> <span id="bib.bibx31.2.2" class="ltx_text ltx_font_bold">222.6</span>
</span>
<span class="ltx_bibblock">Lippincott, Williams,Wilkins, 1995, pp. 735
</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Carol Carraccio and Robert Englander
</span>
<span class="ltx_bibblock">‚ÄúThe objective structured clinical examination: a step in the direction of competency-based evaluation‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx32.1.1" class="ltx_emph ltx_font_italic">Archives of pediatrics &amp; adolescent medicine</em> <span id="bib.bibx32.2.2" class="ltx_text ltx_font_bold">154.7</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2000, pp. 736‚Äì741
</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Ronald M Epstein and Edward M Hundert
</span>
<span class="ltx_bibblock">‚ÄúDefining and assessing professional competence‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx33.1.1" class="ltx_emph ltx_font_italic">Jama</em> <span id="bib.bibx33.2.2" class="ltx_text ltx_font_bold">287.2</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2002, pp. 226‚Äì235
</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">Joel L Horowitz
</span>
<span class="ltx_bibblock">‚ÄúThe bootstrap‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx34.1.1" class="ltx_emph ltx_font_italic">Handbook of econometrics</em> <span id="bib.bibx34.2.2" class="ltx_text ltx_font_bold">5</span>
</span>
<span class="ltx_bibblock">Elsevier, 2001, pp. 3159‚Äì3228
</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">Yoav Benjamini and Yosef Hochberg
</span>
<span class="ltx_bibblock">‚ÄúControlling the false discovery rate: a practical and powerful approach to multiple testing‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx35.1.1" class="ltx_emph ltx_font_italic">Journal of the Royal statistical society: series B (Methodological)</em> <span id="bib.bibx35.2.2" class="ltx_text ltx_font_bold">57.1</span>
</span>
<span class="ltx_bibblock">Wiley Online Library, 1995, pp. 289‚Äì300
</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">Robert F Woolson
</span>
<span class="ltx_bibblock">‚ÄúWilcoxon signed-rank test‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx36.1.1" class="ltx_emph ltx_font_italic">Wiley encyclopedia of clinical trials</em>
</span>
<span class="ltx_bibblock">Wiley Online Library, 2007, pp. 1‚Äì3
</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">Katharina E Keifenheim, Martin Teufel, Julianne Ip, Natalie Speiser, Elisabeth J Leehr, Stephan Zipfel and Anne Herrmann-Werner
</span>
<span class="ltx_bibblock">‚ÄúTeaching history taking to medical students: a systematic review‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx37.1.1" class="ltx_emph ltx_font_italic">BMC medical education</em> <span id="bib.bibx37.2.2" class="ltx_text ltx_font_bold">15.1</span>
</span>
<span class="ltx_bibblock">BioMed Central, 2015, pp. 1‚Äì12
</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">Michael J Yedidia, Colleen C Gillespie, Elizabeth Kachur, Mark D Schwartz, Judith Ockene, Amy E Chepaitis, Clint W Snyder, Aaron Lazare and Mack Lipkin Jr
</span>
<span class="ltx_bibblock">‚ÄúEffect of communications training on medical student performance‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx38.1.1" class="ltx_emph ltx_font_italic">Jama</em> <span id="bib.bibx38.2.2" class="ltx_text ltx_font_bold">290.9</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2003, pp. 1157‚Äì1165
</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">Gregory Makoul
</span>
<span class="ltx_bibblock">‚ÄúCommunication skills education in medical school and beyond‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx39.1.1" class="ltx_emph ltx_font_italic">Jama</em> <span id="bib.bibx39.2.2" class="ltx_text ltx_font_bold">289.1</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2003, pp. 93‚Äì93
</span>
</li>
<li id="bib.bibx40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">Xiu Hui Tan, Malia Alexandra Foo, Shaun Li He Lim, Marie Bernadette Xin Yi Lim, Annelissa Mien Chew Chin, Jamie Zhou, Min Chiam and Lalit Kumar Radha Krishna
</span>
<span class="ltx_bibblock">‚ÄúTeaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx40.1.1" class="ltx_emph ltx_font_italic">BMC medical education</em> <span id="bib.bibx40.2.2" class="ltx_text ltx_font_bold">21</span>
</span>
<span class="ltx_bibblock">Springer, 2021, pp. 1‚Äì19
</span>
</li>
<li id="bib.bibx41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">Steven E Raper, Meera Gupta, Olugbenga Okusanya and Jon B Morris
</span>
<span class="ltx_bibblock">‚ÄúImproving communication skills: a course for academic medical center surgery residents and faculty‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx41.1.1" class="ltx_emph ltx_font_italic">Journal of Surgical education</em> <span id="bib.bibx41.2.2" class="ltx_text ltx_font_bold">72.6</span>
</span>
<span class="ltx_bibblock">Elsevier, 2015, pp. e202‚Äìe211
</span>
</li>
<li id="bib.bibx42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">Martin Von Fragstein, Jonathan Silverman, Annie Cushing, Sally Quilligan, Helen Salisbury, Connie Wiskin and UK Council Clinical Communication Skills Teaching in Undergraduate Medical Education
</span>
<span class="ltx_bibblock">‚ÄúUK consensus statement on the content of communication curricula in undergraduate medical education‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx42.1.1" class="ltx_emph ltx_font_italic">Medical education</em> <span id="bib.bibx42.2.2" class="ltx_text ltx_font_bold">42.11</span>
</span>
<span class="ltx_bibblock">Wiley Online Library, 2008, pp. 1100‚Äì1107
</span>
</li>
<li id="bib.bibx43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">Hanneke De Haes and Jozien Bensing
</span>
<span class="ltx_bibblock">‚ÄúEndpoints in medical communication research, proposing a framework of functions and outcomes‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx43.1.1" class="ltx_emph ltx_font_italic">Patient education and counseling</em> <span id="bib.bibx43.2.2" class="ltx_text ltx_font_bold">74.3</span>
</span>
<span class="ltx_bibblock">Elsevier, 2009, pp. 287‚Äì294
</span>
</li>
<li id="bib.bibx44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">Ronald M Epstein and Richard L Street Jr
</span>
<span class="ltx_bibblock">‚ÄúPatient-centered communication in cancer care: promoting healing and reducing suffering‚Äù, 2007
</span>
</li>
<li id="bib.bibx45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">Julie M Schirmer, Larry Mauksch, Forrest Lang, M Kim Marvel, Kathy Zoppi, Ronald M Epstein, Doug Brock and Michael Pryzbylski
</span>
<span class="ltx_bibblock">‚ÄúAssessing communication competence: a review of current tools‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx45.1.1" class="ltx_emph ltx_font_italic">Family Medicine</em> <span id="bib.bibx45.2.2" class="ltx_text ltx_font_bold">37.3</span>, 2005, pp. 184‚Äì92
</span>
</li>
<li id="bib.bibx46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">Jonathan R Nichol, Joshua Henrina Sundjaja and Grant Nelson
</span>
<span class="ltx_bibblock">‚ÄúMedical history‚Äù
</span>
<span class="ltx_bibblock">StatPearls Publishing, Treasure Island (FL), 2018
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://europepmc.org/books/NBK534249" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://europepmc.org/books/NBK534249</a>
</span>
</li>
<li id="bib.bibx47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">Claire Denness
</span>
<span class="ltx_bibblock">‚ÄúWhat are consultation models for?‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx47.1.1" class="ltx_emph ltx_font_italic">InnovAiT</em> <span id="bib.bibx47.2.2" class="ltx_text ltx_font_bold">6.9</span>
</span>
<span class="ltx_bibblock">Sage Publications Sage UK: London, England, 2013, pp. 592‚Äì599
</span>
</li>
<li id="bib.bibx48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">See Chai Carol Chan, George Choa, James Kelly, Devina Maru and Mohammed Ahmed Rashid
</span>
<span class="ltx_bibblock">‚ÄúImplementation of virtual OSCE in health professions education: A systematic review‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx48.1.1" class="ltx_emph ltx_font_italic">Medical Education</em>
</span>
<span class="ltx_bibblock">Wiley Online Library, 2023
</span>
</li>
<li id="bib.bibx49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">Pawe≈Ç Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan and Milica Ga≈°iƒá
</span>
<span class="ltx_bibblock">‚ÄúMultiwoz‚Äìa large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.00278</em>, 2018
</span>
</li>
<li id="bib.bibx50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">Wei Wei, Quoc Le, Andrew Dai and Jia Li
</span>
<span class="ltx_bibblock">‚ÄúAirdialogue: An environment for goal-oriented dialogue research‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, 2018, pp. 3844‚Äì3854
</span>
</li>
<li id="bib.bibx51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">Jessy Lin, Nicholas Tomlin, Jacob Andreas and Jason Eisner
</span>
<span class="ltx_bibblock">‚ÄúDecision-Oriented Dialogue for Human-AI Collaboration‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2305.20076" title="" class="ltx_ref ltx_href">2305.20076 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser and Illia Polosukhin
</span>
<span class="ltx_bibblock">‚ÄúAttention is all you need‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx52.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> <span id="bib.bibx52.2.2" class="ltx_text ltx_font_bold">30</span>, 2017
</span>
</li>
<li id="bib.bibx53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama and Alex Ray
</span>
<span class="ltx_bibblock">‚ÄúTraining language models to follow instructions with human feedback‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.02155</em>, 2022
</span>
</li>
<li id="bib.bibx54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">Jieyu Zhao, Daniel Khashabi, Tushar Khot, Ashish Sabharwal and Kai-Wei Chang
</span>
<span class="ltx_bibblock">‚ÄúEthical-advice taker: Do language models understand natural language interventions?‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.01465</em>, 2021
</span>
</li>
<li id="bib.bibx55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward and Jan Leike
</span>
<span class="ltx_bibblock">‚ÄúSelf-critiquing models for assisting human evaluators‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx55.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.05802</em>, 2022
</span>
</li>
<li id="bib.bibx56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">J√©r√©my Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho and Ethan Perez
</span>
<span class="ltx_bibblock">‚ÄúTraining language models with language feedback at scale‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.16755</em>, 2023
</span>
</li>
<li id="bib.bibx57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">Amelia Glaese, Nat McAleese, Maja Trƒôbacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick and Phoebe Thacker
</span>
<span class="ltx_bibblock">‚ÄúImproving alignment of dialogue agents via targeted human judgements‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.14375</em>, 2022
</span>
</li>
<li id="bib.bibx58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini and Cameron McKinnon
</span>
<span class="ltx_bibblock">‚ÄúConstitutional AI: Harmlessness from AI feedback‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.08073</em>, 2022
</span>
</li>
<li id="bib.bibx59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann and Nova DasSarma
</span>
<span class="ltx_bibblock">‚ÄúA general language assistant as a laboratory for alignment‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.00861</em>, 2021
</span>
</li>
<li id="bib.bibx60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">Joel Shor, Ruyue Agnes Bi, Subhashini Venugopalan, Steven Ibara, Roman Goldenberg and Ehud Rivlen
</span>
<span class="ltx_bibblock">‚ÄúClinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.05737</em>, 2023
</span>
</li>
<li id="bib.bibx61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">Asma Ben Abacha, Eugene Agichtein, Yuval Pinter and Dina Demner-Fushman
</span>
<span class="ltx_bibblock">‚ÄúOverview of the medical question answering task at TREC 2017 LiveQA.‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx61.1.1" class="ltx_emph ltx_font_italic">TREC</em>, 2017, pp. 1‚Äì12
</span>
</li>
<li id="bib.bibx62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">William Wallace, Calvin Chan, Swathikan Chidambaram, Lydia Hanna, Fahad Mujtaba Iqbal, Amish Acharya, Pasha Normahani, Hutan Ashrafian, Sheraz R Markar and Viknesh Sounderajah
</span>
<span class="ltx_bibblock">‚ÄúThe diagnostic and triage accuracy of digital and online symptom checker tools: a systematic review‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx62.1.1" class="ltx_emph ltx_font_italic">NPJ Digital Medicine</em> <span id="bib.bibx62.2.2" class="ltx_text ltx_font_bold">5.1</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group UK London, 2022, pp. 118
</span>
</li>
<li id="bib.bibx63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">Dan Zeltzer, Lee Herzog, Yishai Pickman, Yael Steuerman, Ran Ilan Ber, Zehavi Kugler, Ran Shaul and Jon O Ebbert
</span>
<span class="ltx_bibblock">‚ÄúDiagnostic accuracy of artificial intelligence in virtual primary care‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx63.1.1" class="ltx_emph ltx_font_italic">Mayo Clinic Proceedings: Digital Health</em> <span id="bib.bibx63.2.2" class="ltx_text ltx_font_bold">1.4</span>
</span>
<span class="ltx_bibblock">Elsevier, 2023, pp. 480‚Äì489
</span>
</li>
<li id="bib.bibx64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">Shreya Johri, Jaehwan Jeong, Benjamin A Tran, Daniel I Schlessinger, Shannon Wongvibulsin, Zhuo Ran Cai, Roxana Daneshjou and Pranav Rajpurkar
</span>
<span class="ltx_bibblock">‚ÄúTesting the Limits of Language Models: A Conversational Framework for Medical AI Assessment‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx64.1.1" class="ltx_emph ltx_font_italic">medRxiv</em>
</span>
<span class="ltx_bibblock">Cold Spring Harbor Laboratory Press, 2023, pp. 2023‚Äì09
</span>
</li>
<li id="bib.bibx65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong and Ruoyu Zhang
</span>
<span class="ltx_bibblock">‚ÄúMedDialog: Large-scale medical dialogue datasets‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx65.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2020, pp. 9241‚Äì9250
</span>
</li>
<li id="bib.bibx66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">Wenge Liu, Jianheng Tang, Yi Cheng, Wenjie Li, Yefeng Zheng and Xiaodan Liang
</span>
<span class="ltx_bibblock">‚ÄúMedDG: an entity-centric medical consultation dataset for entity-aware medical dialogue generation‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx66.1.1" class="ltx_emph ltx_font_italic">CCF International Conference on Natural Language Processing and Chinese Computing</em>, 2022, pp. 447‚Äì459
</span>
<span class="ltx_bibblock">Springer
</span>
</li>
<li id="bib.bibx67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">Deeksha Varshney, Aizan Zafar, Niranshu Kumar Behra and Asif Ekbal
</span>
<span class="ltx_bibblock">‚ÄúCdialog: A multi-turn COVID-19 conversation dataset for entity-aware dialog generation‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.06049</em>, 2022
</span>
</li>
<li id="bib.bibx68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">Guojun Yan, Jiahuan Pei, Pengjie Ren, Zhaochun Ren, Xin Xin, Huasheng Liang, Maarten Rijke and Zhumin Chen
</span>
<span class="ltx_bibblock">‚ÄúReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx68.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 2022, pp. 3013‚Äì3024
</span>
</li>
<li id="bib.bibx69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">Christopher J Kelly, Alan Karthikesalingam, Mustafa Suleyman, Greg Corrado and Dominic King
</span>
<span class="ltx_bibblock">‚ÄúKey challenges for delivering clinical impact with artificial intelligence‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx69.1.1" class="ltx_emph ltx_font_italic">BMC medicine</em> <span id="bib.bibx69.2.2" class="ltx_text ltx_font_bold">17</span>
</span>
<span class="ltx_bibblock">Springer, 2019, pp. 1‚Äì9
</span>
</li>
<li id="bib.bibx70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">Daniel McDuff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, Yash Sharma, Shekoofeh Azizi and Kavita Kulkarni
</span>
<span class="ltx_bibblock">‚ÄúTowards Accurate Differential Diagnosis with Large Language Models‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx70.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.00164</em>, 2023
</span>
</li>
<li id="bib.bibx71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">Zahir Kanjee, Byron Crowe and Adam Rodman
</span>
<span class="ltx_bibblock">‚ÄúAccuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx71.1.1" class="ltx_emph ltx_font_italic">JAMA</em>, 2023
</span>
</li>
<li id="bib.bibx72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">Hannah L Semigran, Jeffrey A Linder, Courtney Gidengil and Ateev Mehrotra
</span>
<span class="ltx_bibblock">‚ÄúEvaluation of symptom checkers for self diagnosis and triage: audit study‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx72.1.1" class="ltx_emph ltx_font_italic">BMJ</em> <span id="bib.bibx72.2.2" class="ltx_text ltx_font_bold">351</span>
</span>
<span class="ltx_bibblock">British Medical Journal Publishing Group, 2015
</span>
</li>
<li id="bib.bibx73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">John W Ayers, Adam Poliak, Mark Dredze, Eric C Leas, Zechariah Zhu, Jessica B Kelley, Dennis J Faix, Aaron M Goodman, Christopher A Longhurst and Michael Hogarth
</span>
<span class="ltx_bibblock">‚ÄúComparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx73.1.1" class="ltx_emph ltx_font_italic">JAMA Internal Medicine</em>, 2023
</span>
</li>
<li id="bib.bibx74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"> OpenAI
</span>
<span class="ltx_bibblock">‚ÄúChatGPT‚Äù, 2023
</span>
<span class="ltx_bibblock">OpenAI
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://chat.openai.com/chat" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://chat.openai.com/chat</a>
</span>
</li>
<li id="bib.bibx75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">Sara Carrillo de Albornoz, Kah-Ling Sia and Anthony Harris
</span>
<span class="ltx_bibblock">‚ÄúThe effectiveness of teleconsultations in primary care: systematic review‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx75.1.1" class="ltx_emph ltx_font_italic">Family Practice</em> <span id="bib.bibx75.2.2" class="ltx_text ltx_font_bold">39.1</span>
</span>
<span class="ltx_bibblock">Oxford University Press UK, 2022, pp. 168‚Äì182
</span>
</li>
<li id="bib.bibx76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">George A Wharton, Harpreet S Sood, Amanda Sissons and Elias Mossialos
</span>
<span class="ltx_bibblock">‚ÄúVirtual primary care: fragmentation or integration?‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx76.1.1" class="ltx_emph ltx_font_italic">The Lancet Digital Health</em> <span id="bib.bibx76.2.2" class="ltx_text ltx_font_bold">1.7</span>
</span>
<span class="ltx_bibblock">Elsevier, 2019, pp. e330‚Äìe331
</span>
</li>
<li id="bib.bibx77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">Aƒ±Ãàna Fuster-Casanovas and Josep Vidal-Alaball
</span>
<span class="ltx_bibblock">‚ÄúAsynchronous Remote Communication as a Tool for Care Management in Primary Care: A Rapid Review of the Literature‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx77.1.1" class="ltx_emph ltx_font_italic">International Journal of Integrated Care</em> <span id="bib.bibx77.2.2" class="ltx_text ltx_font_bold">22.3</span>
</span>
<span class="ltx_bibblock">Ubiquity Press, 2022
</span>
</li>
<li id="bib.bibx78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">Victoria Hammersley, Eddie Donaghy, Richard Parker, Hannah McNeilly, Helen Atherton, Annemieke Bikker, John Campbell and Brian McKinstry
</span>
<span class="ltx_bibblock">‚ÄúComparing the content and quality of video, telephone, and face-to-face consultations: a non-randomised, quasi-experimental, exploratory study in UK primary care‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx78.1.1" class="ltx_emph ltx_font_italic">British Journal of General Practice</em> <span id="bib.bibx78.2.2" class="ltx_text ltx_font_bold">69.686</span>
</span>
<span class="ltx_bibblock">British Journal of General Practice, 2019, pp. e595‚Äìe604
</span>
</li>
<li id="bib.bibx79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">David A Gross, Stephen J Zyzanski, Elaine A Borawski, Randall D Cebul and Kurt C Stange
</span>
<span class="ltx_bibblock">‚ÄúPatient satisfaction with time spent with their physician‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx79.1.1" class="ltx_emph ltx_font_italic">Journal of Family Practice</em> <span id="bib.bibx79.2.2" class="ltx_text ltx_font_bold">47.2</span>
</span>
<span class="ltx_bibblock">[New York, Appleton-Century-Crofts], 1998, pp. 133‚Äì138
</span>
</li>
<li id="bib.bibx80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">Kiek Tates, Marjolijn L Antheunis, Saskia Kanters, Theodoor E Nieboer and Maria BE Gerritse
</span>
<span class="ltx_bibblock">‚ÄúThe effect of screen-to-screen versus face-to-face consultation on doctor-patient communication: an experimental study with simulated patients‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx80.1.1" class="ltx_emph ltx_font_italic">Journal of medical Internet research</em> <span id="bib.bibx80.2.2" class="ltx_text ltx_font_bold">19.12</span>
</span>
<span class="ltx_bibblock">JMIR Publications Toronto, Canada, 2017, pp. e421
</span>
</li>
<li id="bib.bibx81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">Stephen J Zyzanski, Kurt C Stange, Doreen M Langa and Susan A Flocke
</span>
<span class="ltx_bibblock">‚ÄúTrade-offs in high-volume primary care practice‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx81.1.1" class="ltx_emph ltx_font_italic">Journal of Family Practice</em> <span id="bib.bibx81.2.2" class="ltx_text ltx_font_bold">46.5</span>
</span>
<span class="ltx_bibblock">[New York, Appleton-Century-Crofts], 1998, pp. 397‚Äì402
</span>
</li>
<li id="bib.bibx82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">Krishnamurthy Dvijotham, Jim Winkens, Melih Barsbey, Sumedh Ghaisas, Robert Stanforth, Nick Pawlowski, Patricia Strachan, Zahra Ahmed, Shekoofeh Azizi and Yoram Bachrach
</span>
<span class="ltx_bibblock">‚ÄúEnhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx82.1.1" class="ltx_emph ltx_font_italic">Nature Medicine</em> <span id="bib.bibx82.2.2" class="ltx_text ltx_font_bold">29.7</span>
</span>
<span class="ltx_bibblock">Nature Publishing Group US New York, 2023, pp. 1814‚Äì1820
</span>
</li>
<li id="bib.bibx83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">Julian Bird and Steven A Cohen-Cole
</span>
<span class="ltx_bibblock">‚ÄúThe three-function model of the medical interview‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx83.1.1" class="ltx_emph ltx_font_italic">Methods in teaching consultation-liaison psychiatry</em> <span id="bib.bibx83.2.2" class="ltx_text ltx_font_bold">20</span>
</span>
<span class="ltx_bibblock">Karger Publishers, 1990, pp. 65‚Äì88
</span>
</li>
<li id="bib.bibx84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">Agnes G Rezler, James A Woolliscroft and Summers G Kalishman
</span>
<span class="ltx_bibblock">‚ÄúWhat is missing from patient histories?‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx84.1.1" class="ltx_emph ltx_font_italic">Medical Teacher</em> <span id="bib.bibx84.2.2" class="ltx_text ltx_font_bold">13.3</span>
</span>
<span class="ltx_bibblock">Taylor &amp; Francis, 1991, pp. 245‚Äì252
</span>
</li>
<li id="bib.bibx85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">Ellen E Rosenberg
</span>
<span class="ltx_bibblock">‚ÄúLessons for Clinicians From Physician-Patient‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx85.1.1" class="ltx_emph ltx_font_italic">Arch Fam Med</em> <span id="bib.bibx85.2.2" class="ltx_text ltx_font_bold">6</span>, 1997, pp. 279‚Äì283
</span>
</li>
<li id="bib.bibx86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">Robert Charles Smith
</span>
<span class="ltx_bibblock">‚ÄúPatient-centered interviewing: an evidence-based method‚Äù
</span>
<span class="ltx_bibblock">Lippincott Williams &amp; Wilkins, 2002
</span>
</li>
<li id="bib.bibx87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">Donald M Berwick, Thomas W Nolan and John Whittington
</span>
<span class="ltx_bibblock">‚ÄúThe triple aim: care, health, and cost‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx87.1.1" class="ltx_emph ltx_font_italic">Health affairs</em> <span id="bib.bibx87.2.2" class="ltx_text ltx_font_bold">27.3</span>
</span>
<span class="ltx_bibblock">Project HOPE-The People-to-People Health Foundation, Inc., 2008, pp. 759‚Äì769
</span>
</li>
<li id="bib.bibx88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">Thomas Bodenheimer and Christine Sinsky
</span>
<span class="ltx_bibblock">‚ÄúFrom triple to quadruple aim: care of the patient requires care of the provider‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx88.1.1" class="ltx_emph ltx_font_italic">The Annals of Family Medicine</em> <span id="bib.bibx88.2.2" class="ltx_text ltx_font_bold">12.6</span>
</span>
<span class="ltx_bibblock">Annals Family Med, 2014, pp. 573‚Äì576
</span>
</li>
<li id="bib.bibx89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">T Elaine Adamson, Jeane M Tschann, DS Gullion and AA Oppenberg
</span>
<span class="ltx_bibblock">‚ÄúPhysician communication skills and malpractice claims. A complex relationship.‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx89.1.1" class="ltx_emph ltx_font_italic">Western Journal of Medicine</em> <span id="bib.bibx89.2.2" class="ltx_text ltx_font_bold">150.3</span>
</span>
<span class="ltx_bibblock">BMJ Publishing Group, 1989, pp. 356
</span>
</li>
<li id="bib.bibx90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">Jonathan Silverman and Paul Kinnersley
</span>
<span class="ltx_bibblock">‚ÄúDoctors‚Äô non-verbal behaviour in consultations: look at the patient before you look at the computer‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx90.1.1" class="ltx_emph ltx_font_italic">British Journal of General Practice</em> <span id="bib.bibx90.2.2" class="ltx_text ltx_font_bold">60.571</span>
</span>
<span class="ltx_bibblock">British Journal of General Practice, 2010, pp. 76‚Äì78
</span>
</li>
<li id="bib.bibx91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">Urna Rahman and Nick Cooling
</span>
<span class="ltx_bibblock">‚ÄúInter-Cultural Communication Skills Training in Medical Schools: A Systematic Review‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx91.1.1" class="ltx_emph ltx_font_italic">Medical Research Archives</em> <span id="bib.bibx91.2.2" class="ltx_text ltx_font_bold">11.4</span>, 2023
</span>
</li>
<li id="bib.bibx92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">Ahmad Kantar, Julie M Marchant, Woo-Jung Song, Michael D Shields, Grigorios Chatziparasidis, Angela Zacharasiewicz, Alexander Moeller and Anne B Chang
</span>
<span class="ltx_bibblock">‚ÄúHistory taking as a diagnostic tool in children with chronic cough‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx92.1.1" class="ltx_emph ltx_font_italic">Frontiers in pediatrics</em> <span id="bib.bibx92.2.2" class="ltx_text ltx_font_bold">10</span>
</span>
<span class="ltx_bibblock">Frontiers, 2022, pp. 850912
</span>
</li>
<li id="bib.bibx93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">Winny Setyonugroho, Kieran M Kennedy and Thomas JB Kropmans
</span>
<span class="ltx_bibblock">‚ÄúReliability and validity of OSCE checklists used to assess the communication skills of undergraduate medical students: a systematic review‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx93.1.1" class="ltx_emph ltx_font_italic">Patient education and counseling</em> <span id="bib.bibx93.2.2" class="ltx_text ltx_font_bold">98.12</span>
</span>
<span class="ltx_bibblock">Elsevier, 2015, pp. 1482‚Äì1491
</span>
</li>
<li id="bib.bibx94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle and Atoosa Kasirzadeh
</span>
<span class="ltx_bibblock">‚ÄúTaxonomy of risks posed by language models‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx94.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2022, pp. 214‚Äì229
</span>
</li>
<li id="bib.bibx95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang and Nesreen K. Ahmed
</span>
<span class="ltx_bibblock">‚ÄúBias and Fairness in Large Language Models: A Survey‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2309.00770" title="" class="ltx_ref ltx_href">2309.00770 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">Rachel L Johnson, Debra Roter, Neil R Powe and Lisa A Cooper
</span>
<span class="ltx_bibblock">‚ÄúPatient race/ethnicity and quality of patient‚Äìphysician communication during medical visits‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx96.1.1" class="ltx_emph ltx_font_italic">American journal of public health</em> <span id="bib.bibx96.2.2" class="ltx_text ltx_font_bold">94.12</span>
</span>
<span class="ltx_bibblock">American Public Health Association, 2004, pp. 2084‚Äì2090
</span>
</li>
<li id="bib.bibx97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">Debra L Roter, Judith A Hall and Yutaka Aoki
</span>
<span class="ltx_bibblock">‚ÄúPhysician gender effects in medical communication: a meta-analytic review‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx97.1.1" class="ltx_emph ltx_font_italic">Jama</em> <span id="bib.bibx97.2.2" class="ltx_text ltx_font_bold">288.6</span>
</span>
<span class="ltx_bibblock">American Medical Association, 2002, pp. 756‚Äì764
</span>
</li>
<li id="bib.bibx98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese and Geoffrey Irving
</span>
<span class="ltx_bibblock">‚ÄúRed teaming language models with language models‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx98.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.03286</em>, 2022
</span>
</li>
<li id="bib.bibx99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer and Kamal Ndousse
</span>
<span class="ltx_bibblock">‚ÄúRed teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx99.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.07858</em>, 2022
</span>
</li>
<li id="bib.bibx100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">Jiahao Yu, Xingwei Lin and Xinyu Xing
</span>
<span class="ltx_bibblock">‚ÄúGptfuzzer: Red teaming large language models with auto-generated jailbreak prompts‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx100.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10253</em>, 2023
</span>
</li>
<li id="bib.bibx101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">Suyu Ge, Chunting Zhou, Rui Hou, Madian Khabsa, Yi-Chia Wang, Qifan Wang, Jiawei Han and Yuning Mao
</span>
<span class="ltx_bibblock">‚ÄúMART: Improving LLM Safety with Multi-round Automatic Red-Teaming‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx101.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.07689</em>, 2023
</span>
</li>
<li id="bib.bibx102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji and Timnit Gebru
</span>
<span class="ltx_bibblock">‚ÄúModel cards for model reporting‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx102.1.1" class="ltx_emph ltx_font_italic">Proceedings of the conference on fairness, accountability, and transparency</em>, 2019, pp. 220‚Äì229
</span>
</li>
<li id="bib.bibx103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">Anamaria Crisan, Margaret Drouhard, Jesse Vig and Nazneen Rajani
</span>
<span class="ltx_bibblock">‚ÄúInteractive model cards: A human-centered approach to model documentation‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx103.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2022, pp. 427‚Äì439
</span>
</li>
<li id="bib.bibx104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">Mahima Pushkarna, Andrew Zaldivar and Oddur Kjartansson
</span>
<span class="ltx_bibblock">‚ÄúData cards: Purposeful and transparent dataset documentation for responsible ai‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx104.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>, 2022, pp. 1776‚Äì1826
</span>
</li>
<li id="bib.bibx105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">Monojit Choudhury and Amit Deshpande
</span>
<span class="ltx_bibblock">‚ÄúHow Linguistically Fair Are Multilingual Pre-Trained Language Models?‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx105.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em> <span id="bib.bibx105.2.2" class="ltx_text ltx_font_bold">35.14</span>, 2021, pp. 12710‚Äì12718
</span>
</li>
<li id="bib.bibx106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">Zeerak Talat, Aur√©lie N√©v√©ol, Stella Biderman, Miruna Clinciu, Manan Dey, Shayne Longpre, Sasha Luccioni, Maraim Masoud, Margaret Mitchell and Dragomir Radev
</span>
<span class="ltx_bibblock">‚ÄúYou reap what you sow: On the challenges of bias evaluation under multilingual settings‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx106.1.1" class="ltx_emph ltx_font_italic">Proceedings of BigScience Episode# 5‚ÄìWorkshop on Challenges &amp; Perspectives in Creating Large Language Models</em>, 2022, pp. 26‚Äì41
</span>
</li>
<li id="bib.bibx107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali and Sunayana Sitaram
</span>
<span class="ltx_bibblock">‚ÄúMEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2311.07463" title="" class="ltx_ref ltx_href">2311.07463 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">Ayyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, Andr√© Martins, Fran√ßois Yvon and Hinrich Sch√ºtze
</span>
<span class="ltx_bibblock">‚ÄúGlot500: Scaling Multilingual Corpora and Language Models to 500 Languages‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx108.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
</span>
<span class="ltx_bibblock">Association for Computational Linguistics, 2023
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.18653/v1/2023.acl-long.61" title="" class="ltx_ref ltx_href">10.18653/v1/2023.acl-long.61</a>
</span>
</li>
<li id="bib.bibx109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">Xuan-Phi Nguyen, Sharifah Mahani Aljunied, Shafiq Joty and Lidong Bing
</span>
<span class="ltx_bibblock">‚ÄúDemocratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2306.11372" title="" class="ltx_ref ltx_href">2306.11372 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">Tarek Naous, Michael J. Ryan, Alan Ritter and Wei Xu
</span>
<span class="ltx_bibblock">‚ÄúHaving Beer after Prayer? Measuring Cultural Bias in Large Language Models‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2305.14456" title="" class="ltx_ref ltx_href">2305.14456 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">Krithika Ramesh, Sunayana Sitaram and Monojit Choudhury
</span>
<span class="ltx_bibblock">‚ÄúFairness in Language Models Beyond English: Gaps and Challenges‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2302.12578" title="" class="ltx_ref ltx_href">2302.12578 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">Rishav Hada, Varun Gumma, Adrian Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali and Sunayana Sitaram
</span>
<span class="ltx_bibblock">‚ÄúAre Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2309.07462" title="" class="ltx_ref ltx_href">2309.07462 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S. Jaakkola and Regina Barzilay
</span>
<span class="ltx_bibblock">‚ÄúConformal Language Modeling‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2306.10193" title="" class="ltx_ref ltx_href">2306.10193 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">Jiuhai Chen and Jonas Mueller
</span>
<span class="ltx_bibblock">‚ÄúQuantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2308.16175" title="" class="ltx_ref ltx_href">2308.16175 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu and Lei Ma
</span>
<span class="ltx_bibblock">‚ÄúLook Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2307.10236" title="" class="ltx_ref ltx_href">2307.10236 [cs.SE]</a>
</span>
</li>
<li id="bib.bibx116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">Qi Yang, Shreya Ravikumar, Fynn Schmitt-Ulms, Satvik Lolla, Ege Demir, Iaroslav Elistratov, Alex Lavaee, Sadhana Lolla, Elaheh Ahmadi, Daniela Rus, Alexander Amini and Alejandro Perez
</span>
<span class="ltx_bibblock">‚ÄúUncertainty-aware Language Modeling for Selective Question Answering‚Äù, 2023
</span>
<span class="ltx_bibblock">arXiv:<a target="_blank" href="https://arxiv.org/abs/2311.15451" title="" class="ltx_ref ltx_href">2311.15451 [cs.CL]</a>
</span>
</li>
<li id="bib.bibx117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien Masson d‚ÄôAutume, Tomas Kocisky and Sebastian Ruder
</span>
<span class="ltx_bibblock">‚ÄúMind the gap: Assessing temporal generalization in neural language models‚Äù
</span>
<span class="ltx_bibblock">In <em id="bib.bibx117.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> <span id="bib.bibx117.2.2" class="ltx_text ltx_font_bold">34</span>, 2021, pp. 29348‚Äì29363
</span>
</li>
</ul>
</section>
<section id="biba" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="biba.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le and Denny Zhou
</span>
<span class="ltx_bibblock">‚ÄúChain-of-thought prompting elicits reasoning in large language models‚Äù
</span>
<span class="ltx_bibblock">In <em id="biba.bibx1.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> <span id="biba.bibx1.2.2" class="ltx_text ltx_font_bold">35</span>, 2022, pp. 24824‚Äì24837
</span>
</li>
<li id="biba.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock"> Google
</span>
<span class="ltx_bibblock">‚ÄúPaLM 2 Technical Report‚Äù, <a target="_blank" href="https://ai.google/static/documents/palm2techreport.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.google/static/documents/palm2techreport.pdf</a>, 2023
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text" lang="en">appendix</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.05652" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.05654" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2401.05654">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.05654" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.05655" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Feb  9 13:12:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
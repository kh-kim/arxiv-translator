<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '###### Contents\n' +
      '\n' +
      '* 1 Introduction\n' +
      '	* 1.1 Summary\n' +
      '* 2 Source Corpora\n' +
      '	* 2.1 Corpora Selection Criteria\n' +
      '	* 2.2 Selected Corpora\n' +
      '		* 2.2.1 Potential Concerns\n' +
      '	* 2.3 Preprocessing\n' +
      '	* 2.4 Task Assignment\n' +
      '* 3 KLUE Benchmark\n' +
      '	* 3.1 Topic Classification (TC)\n' +
      '		* 3.1.1 Dataset Construction\n' +
      '		* 3.1.2 Evaluation Metric\n' +
      '		* 3.1.3 Related Work\n' +
      '		* 3.1.4 Conclusion\n' +
      '	* 3.2 Semantic Textual Similarity (STS)\n' +
      '		* 3.2.1 Dataset Construction\n' +
      '		* 3.2.2 Evaluation Metrics\n' +
      '		* 3.2.3 Related Work\n' +
      '		* 3.2.4 Conclusion\n' +
      '	* 3.3 Natural Language Inference (NLI)\n' +
      '		* 3.3.1 Dataset Construction\n' +
      '		* 3.3.2 Evaluation Metric\n' +
      '		* 3.3.3 Related Work\n' +
      '		* 3.3.4 Conclusion\n' +
      '	* 3.4 Named Entity Recognition (NER)\n' +
      '		* 3.4.1 Dataset Construction\n' +
      '		* 3.4.2 Evaluation Metrics\n' +
      '		* 3.4.3 Related Work\n' +
      '		* 3.4.4 Conclusion\n' +
      '	* 3.5 Relation Extraction (RE)\n' +
      '		* 3.5.1 Data Construction\n' +
      '		* 3.5.2 Evaluation Metrics\n' +
      '		* 3.5.3 Related Work\n' +
      '		* 3.5.4 Conclusion\n' +
      '	* 3.6 Dependency Parsing (DP)\n' +
      '		* 3.6.1 Dataset Construction\n' +
      '		* 3.6.2 Evaluation Metrics\n' +
      '		* 3.6.3 Related Work\n' +
      '		* 3.6.4 Conclusion\n' +
      '	* 3.7 Machine Reading Comprehension (MRC)\n' +
      '		* 3.7.1 Dataset Construction\n' +
      '		* 3.7.2 Evaluation Metrics\n' +
      '		* 3.7.3 Analysis\n' +
      '		* 3.7.4 Related Work\n' +
      '		* 3.7.5 Conclusion\n' +
      '	* 3.8 Dialogue State Tracking (DST)\n' +
      '		* 3.8.1 Dataset Construction\n' +
      '		* 3.8.2 Evaluation Metrics\n' +
      '		* 3.8.3 Analysis\n' +
      '		* 3.8.4 Related Work\n' +
      '		* 3.8.5 Conclusion\n' +
      '* 4 Pretrained Language Models\n' +
      '	* 4.1 Language Models\n' +
      '	* 4.2 Existing Language Models\n' +
      '* 5 Fine-tuning Language Models\n' +
      '	* 5.1 Task-Specific Architectures\n' +
      '		* 5.1.1 Single Sentence Classification\n' +
      '		* 5.1.2 Sentence Pair Classification / Regression\n' +
      '		* 5.1.3 Multiple-Sentence Slot-Value Prediction\n' +
      '		* 5.1.4 Sequence Tagging\n' +
      '	* 5.2 Fine-Tuning Configurations\n' +
      '	* 5.3 Evaluation Results\n' +
      '	* 5.4 Analysis of Models\n' +
      '* 6 Ethical Considerations\n' +
      '	* 6.1 Copyright and Accessibility\n' +
      '	* 6.2 Toxic Content\n' +
      '	* 6.3 Personally Identifiable Information\n' +
      '* 7 Related Work\n' +
      '* 8 Discussion\n' +
      '* 9 Conclusion\n' +
      '\n' +
      'Index\n' +
      'Introduction\n' +
      '\n' +
      'A major factor behind recent success of pretrained language models, such as BERT [30] and its variants [82; 22; 49] as well as GPT-3 [110] and its variants [111; 76; 9], has been the availability of well-designed benchmark suites for evaluating their effectiveness in natural language understanding (NLU). GLUE [133] and SuperGLUE [132] are representative examples of such suites and were designed to evaluate diverse aspects of NLU, including syntax, semantics and pragmatics. The research community has embraced GLUE and SuperGLUE, and has made rapid progress in developing better model architectures as well as learning algorithms for NLU.\n' +
      '\n' +
      'The success of GLUE and SuperGLUE has sparked interest in building such a standardized benchmark suite for other languages, in order to better measure the progress in NLU in languages beyond English. Such efforts have been pursued along two directions. First, various groups in the world have independently created language-specific benchmark suites; a Chinese version of GLUE (CLUE [142]), a French version of GLUE (FLUE [72]), an Indonesian variant [137], an Indic version [57] and a Russian variant of SuperGLUE [125]. On the other hand, some have relied on both machine and human translation of existing benchmark suites for building multilingual version of the benchmark suites which were often created initially in English. These include for instance XGLUE [78] and XTREME [54]. Although the latter approach scales much better than the former does, the latter often fails to capture societal aspects of NLU and also introduces various artifacts arising from translation.\n' +
      '\n' +
      'To this end, we build a new benchmark suite for evaluating NLU in Korean which is the 13-th most used language in the world according to [34] but lacks a unified benchmark suite for NLU. Instead of starting from existing benchmark tasks or corpora, we build this benchmark suite from ground up by determining and collecting base corpora, identifying a set of benchmark tasks, designing appropriate annotation protocols and finally validating collected annotation. This allows us to preemptively address and avoid properties that may have undesirable consequences, such as copyright infringement, annotation artifacts, social biases and privacy violations.\n' +
      '\n' +
      'In the rest of this section, we summarize a series of decisions and principles that went behind creating KLUE.\n' +
      '\n' +
      '### Summary\n' +
      '\n' +
      'In designing the Korean Language Understanding Evaluation (KLUE), we aim to make KLUE; 1) cover diverse tasks and corpora, 2) accessible to everyone without any restriction, 3) include accurate and unambiguous annotations, 4) mitigate AI ethical issues. KLUE is safe to use for both building and evaluating systems, because KLUE has proactively addressed potential _ethical_ issues. Here, we describe more in detail how these principles have guided creating KLUE from task selection, corpus selection, annotation protocols, determining evaluation metrics to baseline construction.\n' +
      '\n' +
      'Design PrinciplesFirst, let us describe each design principle in detail:\n' +
      '\n' +
      '* _Covering diverse tasks and corpora_: To cover diverse aspects of language understanding, we choose eight tasks that cover diverse domain, including news, encyclopedia, user review, smart home queries and task-oriented dialogue, and diverse style, both formal and colloquial.\n' +
      '* _Accessible to everyone without any restriction_: It is critical for a benchmark suite to be accessible by everyone for it to serve as a true guideline in evaluating and improving NLU systems. We thus use only corpora and resources that can be freely copied, redistributed, remixed and transformed for the purpose of benchmarking NLU systems.\n' +
      '* _Obtaining accurate and unambiguous annotations_: Ambiguity in benchmark tasks leads to ambiguity in evaluation, which often leads to the discrepancy between the quality of an NLU system measured by the benchmark and its true quality. In order to minimize such discrepancy, we carefully design annotation guidelines of all tasks and improve them over multiple iterations, to avoid accurate annotations.\n' +
      '* _Mitigating AI ethical issues_: It has been repeatedly observed that large-scale language models can and often do amplify social biases embedded in text used to train them [95]. In order to disincentivize such behaviors, we proactively remove examples, from both unlabeled and labeled corpora, that reflect social biases, contain toxic content and have personally identifiable information (PII), both manually and automatically. Social biases are defined as overgeneralized judgment on certain individuals or group based on social attributes (e.g., gender, ethnicity, religion). Toxic contents include results, sexual harassment and offensive expressions.\n' +
      '\n' +
      'Diverse Task SelectionWe carefully choose the following eight NLU tasks with two goals; 1) to cover as diverse aspects of NLU in Korean, and 2) to minimize redundancy among the tasks. See Table 1 for their formats, evaluation granularity and other properties:\n' +
      '\n' +
      '* Topic Classification (TC): classify a single sentence into a single class.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      'Evaluation MetricsThe diversity of tasks in KLUE implies that we must choose a proper set of evaluation metrics for each task carefully and separately. Here, we list the tasks and describe how we choose the evaluation metrics for each of these tasks.\n' +
      '\n' +
      '* _KLUE-TC_ (Yonhap News Agency Topic Classification (YNAT)): We formulate KLUE-TC as a multi-class classification problem with seven classes. Because the headline alone is often not enough to precisely identify the proper class to which it belongs, we manually annotate and keep 70,000 headlines, for each of which there was a majority consensus on the class by the annotators. We then use the consensus classes as ground-truth classes and use macro F1 score as an evaluation metric.\n' +
      '* _KLUE-STS_: In KLUE-STS the similarity between each pair of sentences is annotated with the average (real-valued) similarity rating (between 0 and 5). We measure the quality of an NLU model in two different ways. First, we use the Pearson correlation coefficient between the real-valued target and prediction. Second, we compute the F1 score after binarizing the real-valued similarity rating as in paraphrase detection.\n' +
      '* _KLUE-NLI_: Similar to existing NLI datasets, such as SNLI [8] and MNLI [138], we use classification accuracy, and this is appropriate, as we create KLUE-NLI dev/test set to have a balanced class distribution.\n' +
      '* _KLUE-NER_: In KLUE-NER, a named entity recognizer is expected to output BIO tags and also categorize each detected entity into one of six types; person, location, organization, date, time and quantity. To account for rich morphology in Korean, we use entity-level and character-level F1 score to evaluate the quality of the detection to evaluate the recognizer\'s ability in determining the type of each entity.\n' +
      '* _KLUE-RE_: KLUE-RE is designed as a sentence classification task in which the input is a single sentence with two marked entities and the output is their relationship out of 30 types. We use two evaluation metrics. The first one is micro F1 score, considering only meaningful types (excluding no relationship), which allows us to evaluate the NLU system\'s ability to identify a fine-grained relationship between a pair of entities. The second one is the area under the precision-recall curve (AUPRC), which gives us a holistic view into the quality of the relation extraction model in question.\n' +
      '* _KLUE-DP_: Following standard practice in dependency parsing, we use both unlabeled attachment score (UAS) and labeled attachment score (LAS) to evaluate a dependency parser. We annotate and use both formal and informal text\n' +
      '\n' +
      '(subsets from the news corpora and colloquial review corpora, respectively), which allows us to perform fine-grained analysis across multiple domains.\n' +
      '* _KLUE-MRC_: Similarly to KLUE-NER, KLUE-MRC is framed as a span prediction problem. We keep character-level exact match (EM) for comparison against existing datasets, while we propose to use ROUGE-W which measures the F1 score based on the longest common consecutive subsequence (LCCS) between the ground-truth and predicted answer spans. The latter handles rich morphology of Korean as well as the former does while being more interpretable.\n' +
      '* _KLUE-DST_ (Wizard of Seoul, WoS): We formulate KLUE-DST as a multiple-sentence slot-value prediction task, and evaluate an NLU system using two metrics. The first metric is the joint goal accuracy which measures whether all the slots were correctly predicted, while the other metric is average F1 score. Because the former treats all examples for which not all slots were correctly filled in, it often fails to distinguish similarly performing NLU systems. We address this shortcoming by reporting both the joint goal accuracy and slot F1 score. We furthermore build it using multiple domains in order to facilitate finer-grain analysis.\n' +
      '\n' +
      'BaselinesIn addition to creating a benchmark suite, we also build and publicly release a set of strong baselines based on large-scale pretrained language models. In due course, we pretrain and release large-scale language models for Korean ourselves, which will reduce the burden of retraining these large-scale models from individual researchers. We also use several existing multilingual pretrained language models and open-source Korean-specific models in addition to our own models, to gain further insights into the proposed KLUE benchmark. We present all the results in Table 32 and summarize a few interesting observations here. First, Korean-specific language models generally outperform multilingual models. Second, different models perform best on different tasks when controlled for their sizes; KLUE-BERT performs best for YNAT and WoS, KLUE-RoBERTa for KLUE-RE and KLUE-MRC, and KoELECTRABASE for KLUE-STS and KLUE-NLI. Third, as we increase the model size, KLUE-RoBERTaLARGE ends up outperforming all the other models in all the tasks other than KLUE-NER. Lastly, we observe that removing PII has minimal effect on the downstream task performances, and our tokenization scheme, morpheme-based subword tokenization, is effective in tasks involving tagging, detection and even generation at the morpheme level.\n' +
      '\n' +
      'Task OverviewIn Table 1, we summarize the resulting eight KLUE tasks, listing important properties, such as type, format, evaluation metrics and annotated data sizes. In the rest of the paper, we will walk through the process by which each and every one of these tasks was constructed much more in detail.\n' +
      '\n' +
      'Source Corpora\n' +
      '\n' +
      'We build KLUE from scratch, instead of putting together existing datasets, which has been a common practice in setting up benchmarks. We investigate available textual resources, and document the process in order to provide better understanding on how and why we select some corpora but not others. We adopt the recently proposed documentation frameworks; _datasheets_[41] and _data statements_[6]. Based on these frameworks, we document and provide more information to carefully describe our protocol.\n' +
      '\n' +
      '### Corpora Selection Criteria\n' +
      '\n' +
      'We consider two criteria when sourcing a set of corpora to build a source corpus from which task-specific corpora are derived and annotated. The first criterion is accessibility. As the main purpose of KLUE is to facilitate future NLP research and development, we ensure KLUE comes with data that can be used and shared as freely as possible to all. The second criterion is the quality and diversity. We ensure each example with these corpora is of certain quality by removing low-quality text and also the balance is met between formal and colloquial text within these corpora.\n' +
      '\n' +
      'AccessibilityUnlike Wang et al. [132], Hu et al. [54], Kakwani et al. [57], we design KLUE to reach as broad and diverse researchers as possible by avoiding any restriction on affiliations of users as well as the purpose of its use. Furthermore, we acknowledge the rapid pace of advances in the field and allow users to reproduce and redistribute KLUE to prolong its usability as a standard benchmark of NLU. To do so, we build and release the source corpus with CC BY-SA.4\n' +
      '\n' +
      'Footnote 4: [https://creativecommons.org/licenses/by-sa/4.0/](https://creativecommons.org/licenses/by-sa/4.0/)\n' +
      '\n' +
      'The source corpus, or a set of source corpora, satisfies the following conditions:\n' +
      '\n' +
      '* **No restriction on the use:** We allow both non-commercial and commercial use of KLUE, in order to accommodate the recent trend of fundamental research from industry labs.\n' +
      '* **Derivatives:** We allow users to freely refurbish any part of KLUE to first address any shortcomings, such as unanticipated artifacts, ethical issues and annotation mistakes, and second derive more challenging benchmarks for the future. This is similar to what has been done with SQuAD 2.0 [113] which was created to include SQuAD 1.1 [112].\n' +
      '* **Redistributable:** We allow KLUE benchmark datasets to be distributed by anyone via any channel as long as the proper attribution is given to the original creators of KLUE. We deliberately make this decision to avoid situations where only a limited and select group of researchers have a monopoly on resources, ultimately hindering the progress overall. This is in reaction to some of the existing Korean corpora which come together with restrictive policies, often preventing derivatives as well as redistribution, and are only accessible by researchers in Korea after acquiring permissions from the corpus publishers who are often public institutions in Korea. KLUE avoids such preventive policies in order to maximally facilitate the progress in Korean NLP.\n' +
      '\n' +
      'Because most of the existing datasets do not meet these conditions, we curate the source corpus from scratch by considering only those resources that either come with one of the following licenses: CC0,5 CC BY,6 CC BY-SA,7 and other similar licenses such as KOGL Type 1,8 are not protected by the copyright act according to the latest copyright act in Korea,9 or have been explicitly provided to us by copyright holders under contracts. We end up 20 candidate corpora in total, of which subset is selected to form a source corpus set of KLUE. They are listed in Table 2.\n' +
      '\n' +
      'Footnote 5: [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)\n' +
      '\n' +
      'Footnote 6: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)\n' +
      '\n' +
      'Footnote 7: [https://creativecommons.org/licenses/by-sa/4.0/](https://creativecommons.org/licenses/by-sa/4.0/)\n' +
      '\n' +
      'Footnote 8: [https://www.kogl.or.kr/info/license.do#05-tab](https://www.kogl.or.kr/info/license.do#05-tab)\n' +
      '\n' +
      'Footnote 9: See [https://www.law.go.kr/KEB/%2%95%EB%AA%B9/%EC%AA0%0%EC%9E%91%EA%BE%AC%EB%2%95](https://www.law.go.kr/KEB/%2%95%EB%AA%B9/%EC%AA0%0%EC%9E%91%EA%BE%AC%EB%2%95) for the copyright act which went effective as of Dec 8 2020.\n' +
      '\n' +
      'Quality and DiversityAmong these 20 source corpora, we select a subset of ten corpora to form the source corpus and to build the KLUE benchmark. In doing so, we consider the following criteria; 1) the corpus should not be specific to narrow domains (diversity), 2) the corpus must be written in contemporary Korean (quality), 3) the corpus should not be dominated by contents that have privacy or toxicity concerns (quality) and 4) the corpus must be amenable to annotation for at least one of the eight benchmark tasks. Furthermore, we select the subset of corpora to cover both formal and colloquial uses.\n' +
      '\n' +
      'The Final Source CorporaBased on these criteria and decisions, we choose News Headlines, Wikipedia, Wikinews, Policy News, The Korea Economics Daily News, and Acrofan News for (relatively) formal text.10 For more colloquial text, we use ParaKQC, Airbnb Reviews, and NAVER Sentiment Movie Corpus. These are marked bold in Table 2.\n' +
      '\n' +
      'Footnote 10: Although Wikitree was found to include some contents that could be considered unethical, socially biased and/or of low quality in general, we include it, as Wikitree is the largest source of license-free news articles. We address these problematic contents via annotation.\n' +
      '\n' +
      '### Selected Corpora\n' +
      '\n' +
      'Here, we describe in more detail general characteristics and potential concerns of each source corpus. We document the collection mechanisms, timeframe, domain, style, license, and background of each corpus as well.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline\n' +
      '**Dataset** & **License** & **Domain** & **Style** & \\begin{tabular}{c} **Ethical** \\\\ **Risks** \\\\ \\end{tabular} & **Volume** & \n' +
      '\\begin{tabular}{c} **Contemporary** \\\\ **Korean** \\\\ \\end{tabular} \\\\ \\hline\n' +
      '**News Headlines** & **N/A** & **News (Headline)** & **Formal** & **Low** & **Large** & **o** \\\\ Judgments & Public Domain & Law & Formal & Low & Large & o \\\\ National Assembly & Public Domain & Politics & Colloquial & Medium & Large & o \\\\ Patents & Public Domain & Patent & Formal & Low & Large & o \\\\ \\hline\n' +
      '**Wikipedia** & **CC BY-SA 3.0** & **Wikipedia** & **Formal** & **Low** & **Large** & **o** \\\\ Wikibooks & CC BY-SA 3.0 & Book & Formal & Low & Medium & x \\\\ Wikisource & CC BY-SA 3.0 & Law & Formal & Low & Medium & x \\\\\n' +
      '**Wikinews** & **CC BY 2.5** & **News** & **Formal** & **Low** & **Small** & **o** \\\\\n' +
      '**Wikitree** & **CC BY-SA 2.0** & **News** & **Formal** & **Medium** & **Large** & **o** \\\\ Librewiki & CC BY-SA 3.0 & Wiki & Formal & Medium & Large & o \\\\ Zetawiki & CC BY-SA 3.0 & Wiki & Formal & Medium & Large & o \\\\\n' +
      '**Policy News** & **KOGL Type 1** & **News** & **Formal** & **Low** & **Medium** & **o** \\\\ NIKL Standard & CC BY-SA 2.0 & Dictionary & Formal & Low & Large & o \\\\ Korean Dictionary & CC BY-SA 2.0 & Dictionary & Formal & Low & Large & o \\\\\n' +
      '**ParaKQC** & **CC BY-SA 4.0** & \n' +
      '\\begin{tabular}{c} **Smart Home** \\\\ **Uterances** \\\\ \\end{tabular} & **Colloquial** & **Low** & **Medium** & **o** \\\\\n' +
      '**Airbnb Reviews** & **CC0 1.0** & **Review** & **Colloquial** & **Medium** & **Large** & **o** \\\\\n' +
      '**NAVER Sentiment** & **CC0 1.0** & **Review** & **Colloquial** & **Medium** & **Large** & **o** \\\\\n' +
      '**Movie Corpus (NSMC)** & **CC BY-SA 4.0** & Review & Colloquial & High & Large & o \\\\ NAVER Entertainment & CC BY-SA 4.0 & **\n' +
      '\\begin{tabular}{c} **CC BY-SA 4.0** \\\\ **for KLUE-MRC by Contract** \\\\ \\end{tabular} & **News** & **Formal** & **Low** & **Large** & **o** \\\\\n' +
      '**The Korea Economics** & \n' +
      '\\begin{tabular}{c} **CC BY-SA 4.0** \\\\ **for KLUE-MRC by Contract** \\\\ \\end{tabular} & **News** & **Formal** & **Low** & **Large** & **o** \\\\\n' +
      '**Daily News** & **for KLUE-MRC by Contract** & **News** & **Formal** & **Low** & **Large** & **o** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Collected source corpora. The corpora in the first section are not protected by copyright act. Specifically, _News Headlines_ are not classified as a work due to their lack of creativity and _Judgements_ are not protected works under Article 7, Act 3. _National Assembly Minutes_ and _Patents_, made in National Assembly, shall not apply the copyright act by Article 24, Act 2. The second section is a collection of corpora under the permissive licenses. The last section corpora, KED and Acrofan, are originally prohibited from creating derivative works, however, we release such condition by exclusive contract. For the column, _Volume_, we denote _Small_ as corpus size under 1k, _Medium_ as in between 1k and 50k, and _Large_ as over 50k. Bold represents our final source corpora to build KLUE benchmark.\n' +
      '\n' +
      'News Headlinesfrom YNA.\n' +
      '\n' +
      'YNA is a dataset of news headlines from Ynahap News Agency, one of the representative news agencies in South Korea. Using news headlines does not infringe on copyrights, unlike the actual contents of news articles. We include YNA from 2016 to 2020 with a main purpose of using it for a single sentence classification task.\n' +
      '\n' +
      'Wikipedia (WIKIPEDIA)WIKIPEDIA is an open encyclopedia written in a formal style and has been widely used for language modeling and dataset construction across many languages, because of its high-quality and well-curated text. The Wikipedia articles in Korean are released under CC BY-SA. We use the dump of Korean Wikipedia released on December 1st, 2020.\n' +
      '\n' +
      'Wikinews (WIKINEWS)WIKINEWS implements collective journalism and provides news articles for free under CC BY, both of which are rare for news articles. Due to these properties, we include it in the source corpora despite its limited number of articles (approximately 500 of them).\n' +
      '\n' +
      'Wikitree (WIKITREE)WIKITREE is a dataset of news articles derived from Wikitree, the first Korean social media-based news platform that started in 2010. Although there are concerns that the articles on Wikitree are in many cases advertisement-in-disguise or click-bait headlines and express undesirable biases, we include WIKITREE, as it is the only large-scale source of news articles that are freely distributed under CC BY-SA, to the best of our knowledge. It also covers a broad spectrum of topics, including politics, economics, culture and life. We use the articles published between 2016 and 2020. We conduct more thorough manual inspection of WIKITREE is more thoroughly conducted. See Section 2.2.1 for more details.\n' +
      '\n' +
      'Policy News (POLICY)POLICY is a dataset of various articles distributed by ministries, national offices, and national commissions of South Korea. It covers statements, notices, or media notes reported by the government agencies. POLICY is protected under the Korea Open Government License (KOGL) Type 1, which permits users to share and remix even for commercial purposes, if attribution is properly done. We include articles released up to the end of 2020.\n' +
      '\n' +
      'ParaKQC (PARAKQC)PARAKQC is a dataset of 10,000 utterances aimed at smart home devices, consisting of 1,000 intents of 10 similar queries [18]. It covers various topics which are probable when interacting with smart home speakers, such as scheduling an appointment and asking about the weather. PARAKQC is available under CC BY-SA.\n' +
      '\n' +
      'Airbnb Reviews (AIRBNB)AIRBNB is a review dataset sourced from the publicly accessible portion of the Airbnb website. More specifically, we start from the existing multilingual Airbnb reviews collected and preprocessed by Inside Airbnb.11 We identify a subset of reviews written in Korean from this multilingual Airbnb corpus, using regular expressions. Reviews are from hosts and guests who have completed their stays. AIRBNB is available under CC0.\n' +
      '\n' +
      'Footnote 11: [http://insideairbnb.com/get-the-data.html](http://insideairbnb.com/get-the-data.html)\n' +
      '\n' +
      'NAVER Sentiment Movie Corpus (NSMC)NSMC is a movie review dataset scraped from NAVER Movies.12 The reviews are written by online users. Each review comes with both the textual content and the binary sentiment label. There are 200,000 reviews in total. The numbers of positive and negative reviewers are balanced. NSMC is available under CC0.\n' +
      '\n' +
      'Footnote 12: [https://movie.naver.com/movie/point/af/list.nhn](https://movie.naver.com/movie/point/af/list.nhn)\n' +
      '\n' +
      'Acrofan News (ACROFAN)ACROFAN is a corpus consisting of news articles released by ACROFAN. Most articles are press release-like in that they often introduce new products or events of companies. The formats and styles are quite templated, although the articles cover a broad set of categories including automobiles, IT, startups, big companies, energy, beauty and fashion. We obtain the permission and use of the articles from ACROFMAN for KLUE. We include news articles published between Dec 2020 and Jan 2021.\n' +
      '\n' +
      'The Korea Economics Daily News (The Korea Economy Daily)The Korea Economy Daily is a news corpus consisting of articles from the Korea Economics Daily owned by Hankyung corporation. Korea Economics Daily is a newspaper that mainly covers economic issues, but also publishes various topics such as politics, culture and IT topics. The owner of the Korea Economics Daily and we have entered a contract to use news articles published between Jan 2013 and Dec 2015, provided by the Hankyung corporation, as a part of KLUE. This allows us to ensure high-quality, well-curated news articles are included in KLUE. We release The Korea Economy Daily under CC BY-SA, with the condition that these articles are used for the purpose of machine learning research.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      'the predictive score of \\(0.9\\) or above. The thresholds are manually determined for each corpus. This approach work well for online text, such as reviews, because the Korean hate speech dataset was constructed using online reviews. It however does not work well for more formal text, such as found in news articles, based on which we decide against using this strategy on The Korea Economy Daily, ACROFAN, and YNA.\n' +
      '\n' +
      'PII RemovalTo mitigate potential privacy issues, we get rid of sentences that contain private information. We detect such sentences using regular expressions that match email addresses, URL and user-mentioning keywords, such as \'@gildong\'.\n' +
      '\n' +
      '### Task Assignment\n' +
      '\n' +
      'We use these source corpora to build the datasets for the seven KLUE tasks, except for the DST. DST is built from simulated dialogues by crowdworkers and does not require access to offline text. For each downstream task, we use a subset of the source corpora, as described below:\n' +
      '\n' +
      '* Topic Classification (TC): We use YNA, which has been widely studied for a single sentence topic classification task.\n' +
      '* Semantic Textual Similarity (STS): We use AIRBNB, POLICY, and PARAKQC to include diverse semantic contexts. Intent queries and topic information of PARAKQC are useful when generating semantically related sentence pairs.\n' +
      '* Natural Language Inference (NLI): Following MNLI [138], we use multiple sources to construct NLI. We use WIKITREE, POLICY, WIKINEWS, WIKIPEDIA, NSMC and AIRBNB.\n' +
      '* Named Entity Recognition (NER): Due to the nature of NER, we must build a corpus in which (named) entities frequently appear. We thus use WIKITREE and NSMC, which enables us to include both formal and informal writing styles.\n' +
      '* Relation Extraction (RE): We use WIKIPEDIA, WIKITREE and POLICY. These corpora tend to have long complete sentences with the names of public figures and their relationships to various organizations.\n' +
      '* Dependency Parsing (DP): We balance formal and colloquial writing styles, while ensuring most of sentences from selected corpora are complete. We end up using WIKITREE and AIRBNB. We choose AIRBNB over NSMC, because the former has better-formed sentences.\n' +
      '* Machine Reading Comprehension (MRC): To provide informative passages, we use WIKIPEDIA, The Korea Economy Daily, and ACROFAN.\n' +
      '\n' +
      'KLUE Benchmark\n' +
      '\n' +
      'The goal of KLUE is to provide high quality evaluation datasets and suitable automatic metrics to test a system\'s ability to understand Korean language. We provide comprehensive details on how we construct our 8 benchmark datasets. We document 1) background of source corpus selection, 2) annotation protocol, 3) annotation process, 4) dataset split strategy, and 5) design process of the metrics. In the annotation process, we guide workers to identify texts containing potential ethical issues. See Section 1.1 for our definitions on bias, hate, and PII.\n' +
      '\n' +
      '### Topic Classification (TC)\n' +
      '\n' +
      'In topic classification (TC), the goal is to train a classifier to predict the topic of a given text snippet. Topic classification datasets typically consist of news or Wikipedia articles and their predefined categories, because the categories often represent topics [151].\n' +
      '\n' +
      'We include TC in our KLUE benchmark, as inferring the topic of a text is a key capability that should be possessed by a language understanding system. As a typical single sentence classification task, other NLU benchmarks such as CLUE [142] and IndicGLUE [57] also contain TNEWS and News Category Classification. For Korean, no dataset has been proposed for the task, which motivates us to construct the first Korean topic classification benchmark.\n' +
      '\n' +
      'In this task, given a news headline, a text classifier must predict a topic which is one of {politics, economy, society, culture, world, IT/science, sports}. We formulate TC as single sentence classification task following previous works and use macro-F1 score as an evaluation metric.\n' +
      '\n' +
      '#### 3.1.1 Dataset Construction\n' +
      '\n' +
      'Our TC benchmark is constructed in three stages. First, we collect headlines and their corresponding categories, then we annotate the topics without looking at the categories and we finalize the dataset by defining its split into training, development and test splits considering the publication date and term appearances.\n' +
      '\n' +
      'Source CorporaWe collect news headlines from online articles distributed by YNA, the largest news agency in Korea. Specifically, we collect the headlines of the published articles from January 2016 to December 2020 from Naver News.17 These articles belong to one of the following seven sections: politics, economy, society, culture, world, IT/science, and sports. To balance the data across the different sections, we randomly sample 10,000 articles from each section, except for the sports and IT/science section. We collect 9,000 sports articles and 11,000 IT/science articles.\n' +
      '\n' +
      'Footnote 17: [https://news.naver.com/](https://news.naver.com/)\n' +
      '\n' +
      'Unlike other benchmarks such as TNEWS in CLUE [142] or AG News [151], we exclude contents of the articles to avoid infringement of copyright. Since the contents are protected as copyrighted work, we cannot freely use them without permission. Headlines, on the other hand, are not considered copyrighted work based on a legal precedent [23].\n' +
      '\n' +
      'Annotation ProtocolThe headline of each article may not reflect all of the main content, such that the _topic_ of the headline may be different from the original news section of the article. To address this gap between the headline and the corresponding article, we manually annotate the topics of the headlines.\n' +
      '\n' +
      'We use SelectStar,18 a crowdsourcing platform in Korea, to annotate topics of the headlines. For each headline, three annotators label topics independently from each other. Each annotator picks at most three topics in the order of relevance among the seven categories. For precise annotation, we also present _key terms_ of each topic to annotators. The terms are subsections of corresponding topics in NAVER news platform as shown in Table 3.\n' +
      '\n' +
      'Footnote 18: [https://selectstar.ai/](https://selectstar.ai/)\n' +
      '\n' +
      'An annotator may choose _unable-to-decide_ if the headline does not contain sufficient information to identify the appropriate categories. Such an example is "Youngsoo Kim awards an appreciation plaque". There is no clue about who "Youngsoo Kim" is nor why he is awarding the appreciation plaque, in this headline.\n' +
      '\n' +
      'We request the workers to report any headline that includes personally identifiable information (PII), expresses social bias, or is hate speech. We discard the reported headline after manually reviewing them.\n' +
      '\n' +
      'Annotation ProcessWe run a pilot study to select workers, before commencing the main annotation process. We exclude workers who have continuously failed to assign a topic or have failed to agree with the other workers during the pilot stage. As a result, 13 workers have passed this stage of pilot study.\n' +
      '\n' +
      'In the main annotation, the 13 selected workers labeled topics for all 70,000 headlines. During the annotation, they reported 650 headlines are including potential PIIs (0.93%), 194 toxic contents (0.28%), and 2,515 _unable-to-decides_ (3.59%). We first exclude such invalid 2,953 headlines. The sum of the three type of problematic headlines are larger than the total value because of the intersection among them. After filtering them, 67,047 headlines remain.\n' +
      '\n' +
      'We look at agreements between three annotators in valid headlines. We consider each of the first relevant topics chosen by three annotators. In 40,359 (60.5%) headlines, all three annotators agree to a single topic. 23,353 (34.8%) had two majority votes, and the other 3,155 (4.7%) did not reached to agreement. To make the headlines classified to a single topic, we remove the others, leaving 63,892 headlines.\n' +
      '\n' +
      'We examine the second and third relevant topics within an annotator. For 48,885 (69.8%) of headlines, three annotators did not choose any second and third most relevant topic. Only 5,088 (7.3%) of headlines have the second topic in three annotators. We thus assume that headlines are sufficiently represented by the first relevant topics within an annotator.\n' +
      '\n' +
      'We thus keep only a single topic for each headline, selected by at least two annotators out of three. The annotator agreement on the resulting 63,892 headlines is fairly high (Krippendorff\'s \\(\\alpha=0.713\\)) [67].\n' +
      '\n' +
      'Final DatasetWe partition the final dataset, named YNAT (Yonhap News Agency dataset for Topic classification), into train, development, and test sets. We split the dataset based on the publication date. We include headlines published after 2020 in the development and test sets, while those published before 2020 in the training set. To prevent TC models attending specific keyword to classify the headlines, we also include headlines containing terms that have not appeared in the train set in the development and test set. As shown in the Table 3, train, development, and test sets consist of 45,678, 9,107, and 9,107 examples, respectively.\n' +
      '\n' +
      '#### 3.1.2 Evaluation Metric\n' +
      '\n' +
      'The evaluation metric for YNAT is macro F1 score. Macro F1 score is defined as the mean of topic-wise F1 scores, giving the same importance to each topic. Topic-wise F1 score weights recall and precision equally.\n' +
      '\n' +
      '#### 3.1.3 Related Work\n' +
      '\n' +
      'Although many topic classification datasets have been proposed in various languages, we are not aware of any public TC benchmark in Korea. AG News [151], a widely used benchmark for topic classification in English, consists of more than a million of news articles collected from the news search engine ComeToMyHead,19 and categorizes articles into four sections: world, sports, business, and science/technology. More recently, a number of TC benchmark datasets in languages other than English were proposed. IndicGLUE [57] includes News Genre Classification in Indian languages, in which the goal is to classify a news article or news headline into seven categories; entertainment, sports, business,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c} \\hline \\hline\n' +
      '**Topic** & **Key Terms** & **lTrain** & **lDevl** & **lTestl** & **Total** \\\\ \\hline Politics & Blue House, Ministry, Parliament, North Korea & 7,379 & 750 & 722 & 8,851 \\\\  & Political parties, Defense, Diplomacy & & 6,118 & 1,268 & 1,348 & 8,734 \\\\ Economy & Stock, Finance, Industry Enterprise, Real estate & 5,133 & 3,740 & 3,701 & 12,574 \\\\ Society & Education, Labor, Journalism & 5,751 & 1,387 & 1,369 & 8,507 \\\\  & Environment, Human rights, Food and drugs & & 5,751 & 1,387 & 1,369 & 8,507 \\\\ Culture & Health, Transportation, Leisure, Hot places, Fashion, Beauty, Performance, Exhibition, Books, Weather & & 8,320 & 776 & 835 & 9,931 \\\\ World & Asia/Australia, America, Europe, Middle East/Africa & & 5,235 & 587 & 554 & 6,376 \\\\ IT/Science & Mobile, IT, Internet Social media, Communication & & 7,742 & 599 & 578 & 8,919 \\\\ Sports & Baseball, Basketball, Volleyball, E-sports & & **45,678** & **9,107** & **9,107** & **63,892** \\\\ \\hline\n' +
      '**Total** & & & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: The final statistics of YNAT (KLUE-TC), provided with the key terms of each category.\n' +
      '\n' +
      'lifestyle, technology, politics, and crime. TNEWS from CLUE [142] is a news topic classification task in Mandarin and consists of 73K titles with 15 news categories, published in Toutiao.\n' +
      '\n' +
      'Since a large language model fine-tuned on TC benchmark can closely reach 100% accuracy as in IndicGLUE [57], some researchers focus on making challenging TC benchmark to leave a room for improvement. CLUE [142] filters easy examples in TNEWS by using 4-fold cross-validation, and then randomly shuffle and split the dataset. Instead of designing our benchmark artificially more difficult, we reflect how topic classification is done in practice even a baseline model reaches to good performance with relatively easy examples in our benchmark.\n' +
      '\n' +
      '#### 3.1.4 Conclusion\n' +
      '\n' +
      'We introduce YNAT, the first Korean topic classification benchmark. The benchmark includes 63,892 news headlines classified to a single hand-labeled topic among 7 categories. We assume each headline has only a single topic, but it could be formulated as multi-label classification. We thus open the second and third relevant topic annotations. Also, URLs for each headlines are accompanied for future work if metadata is needed. If some of them requires permission to use, one should contact to the agency. We expect YNAT to serve as a simple and basic NLU task compared to others in KLUE.\n' +
      '\n' +
      '### Semantic Textual Similarity (STS)\n' +
      '\n' +
      'Semantic textual similarity (STS) is to measure the degree of semantic equivalence between two sentences. We include STS in our benchmark because it is essential to other NLP tasks such as machine translation, summarization, and question answering. Like STS [13] in GLUE [133], many NLU benchmarks include comparing semantic similarity of text snippets such as semantic similarity [142], paraphrase detection [133, 57], or word sense disambiguation [125, 72].\n' +
      '\n' +
      'We formulate STS as a sentence pair regression task which predicts the semantic similarity of two input sentences as a real value from 0 (no meaning overlap) to 5 (meaning equivalence). A model performance is measured by Pearson\'s correlation coefficient following the evaluation scheme of STS-b [13]. We additionally binarize the real numbers into two classes with a threshold score 3.0 (paraphrased or not), and use F1 score to evaluate the model.\n' +
      '\n' +
      '#### 3.2.1 Dataset Construction\n' +
      '\n' +
      'Source CorporaTo diversify domain and style of source corpora, we collect sentences from AIRBNB (colloquial review), POLICY (formal news), and PARAKQC [18] (smart home utterances). We carefully match them to sentence pairs.\n' +
      '\n' +
      'For each corpus, we design a sampling strategy of sentence pairs to uniformly cover all range of the similarity scores. Without a sophisticated strategy, simple random sampling and matching sentence to pairs would result in a majority of the score zero. To alleviate this skewness, _potentially_ similar and less similar sentences are separately paired by using various methods. For instance, if two descriptions are depicting the same image or headlines referring to the same event, they are likely to be similar because of the additional information. Otherwise, they would not be similar [2]. Inspired from these, we use available additional information to pair sentences as similar or not. If not available, we use round-trip translation (RTT) to obtain the similar pairs and _greedy sentence matching_ for the less similar pairs.\n' +
      '\n' +
      'We specify the strategy for PARAKQC where the intent of each sentence is available. All sentences are queries for a smart home domain and their intent are shared among some queries. For example, "_How\'s the weather today in Seoul?_" and "_You know what the weather is like in Seoul today?_" share the same intent which is asking "The weather of Seoul today". We pair two sentences with the same intent as similar pairs and different intent as the less similar. Note that even the less similar pairs share topic to avoid making too many mutually dissimilar pairs.\n' +
      '\n' +
      'For AIRBNB and POLICY, we cannot find meaningful metadata to estimate similarity between sentences. So we adopt RTT technique using NAVER Papago20 to generate the similar sentence pairs, since RTT is known to yield sentences with slightly different lexical representation while preserving the core meaning of the original sentence. We set English as an intermediate language. We choose a honorific option when translating back to Korean because the option tends to preserve the meaning of the sentences empirically. For less similar pairs, we first compute ROUGE [80] of all possible sentence pairs, by assuming the higher score correlates with higher semantic similarity.21 Then we draw a pair with the largest score from all possible pairs and the draw is repeated over remaining pairs until all of sentences are matched. As it progresses, the score declines as the number of remaining pairs becomes smaller, producing less similar pairs. We summarize this process as _greedy sentence matching_ (GSM), as presented in Algorithm 1.\n' +
      '\n' +
      'Footnote 20: [https://papago.naver.com/](https://papago.naver.com/)\n' +
      '\n' +
      'Footnote 21: This might be replaced to any other similarity measures.\n' +
      '\n' +
      '**Result:** Set of sentence pairs SET in a corpus C\n' +
      '\n' +
      'Prepare corpus C, Let SET = [];\n' +
      '\n' +
      '**while**_size of C \\(\\geq\\) 2_**do**\n' +
      '\n' +
      '1. Choose a random sentence S from C;\n' +
      '\n' +
      '2. Find a sentence T where ROUGE(S, T) is maximized and T \\(\\in\\) C\\(\\backslash\\)S;\n' +
      '\n' +
      '3. Remove {S, T} from C;\n' +
      '\n' +
      '4. Add matched pair {(S, T)} to SET\n' +
      '\n' +
      '**end**\n' +
      '\n' +
      '**Annotation Protocol** We modify the original annotation guide used in SemEval-2015 [2]. It suggests chunking both sentences and compares similarity in chunk-level (e.g., NP, verb chain, PP, etc.). Then an annotator should sum up their judgement to sentence-level similarity. However, we could not directly apply the guide because chunking is highly challenging in Korean. In chunking, tokenization and morpheme-level decomposition of words are required, but they are difficult and even not deterministic in some cases [103]. We thus guide an annotator to evaluate the similarity without chunking and stick to sentence-level comparison.\n' +
      '\n' +
      'We give crowdworkers additional cues what is _important_ or _unimportant_ for sentence-level similarity evaluation. _Important_ content indicates the main idea in a sentence. If it is a declarative sentence, its providing facts, explanation, or information is the main idea. For an interrogative and imperative sentence, conveying a request or command is important. In exclamatory sentence, feelings or opinion is the main content [4]. Other components than these _important_ contents are regarded as _unimportant_. For example, they are auxiliary verbs or function words which affect its nuance or politeness. An annotator should score the similarity as follows:\n' +
      '\n' +
      '* 5: Two sentences are equivalent in terms of _important_ and _unimportant_ content.\n' +
      '* 4: Two sentences are closely equivalent. Some _unimportant_ content differ.\n' +
      '* 3: Two sentences are roughly equivalent. _Important_ content are similar to each other, but difference between _unimportant_ content is not ignorable.\n' +
      '* 2: Two sentences are not equivalent. _Important_ content are not similar to each other, only sharing some _unimportant_ contents.\n' +
      '* 1: Two sentences are not equivalent. _Important_ and _unimportant_ content are not similar to each other. Two sentences only share their topics.\n' +
      '* 0: Two sentences are not equivalent. They are not sharing any _important_ and _unimportant_ contents and even topics.\n' +
      '\n' +
      'We also guide crowdworkers to consider the context of sentences. If it significantly affects distinguishing the meaning of two sentences, the score should be low. For example, let two sentences contain important information \'check-in\' such as "Check-in was done by someone other than the host." and "Check-in was done by someone." In the latter sentence,\'someone\' might be the host. Since we lose information by dropping \'other than the host\' from the former, difference of meaning between the two sentence is not ignorable. We score this pair to 3. Furthermore, if the former sentence is compared to \'Check-out was done by someone other than the host.\', _important_ information differ so we give score 2.\n' +
      '\n' +
      'Annotation ProcessWe recruit workers from SelectStar,22 a crowdsourcing platform in Korea and familiarize them to our annotation protocol. We run pilot annotation to select qualified workers. If a crowdworker\'s judgement is frequently disagreed against that of other workers, the person is excluded from the main annotation process. As a result, 19 out of the initial 20 workers participate in the main annotation. After removing the sentence pairs used in the pilot, we use 14,869 pairs for the main annotation, consisting of 7,375 for AIRBNB, 2,956 for POLICY, and 4,538 for PARAKQC. 7 different workers labeled all sentence pairs independently.\n' +
      '\n' +
      'Footnote 22: [https://selectstar.ai/](https://selectstar.ai/)\n' +
      '\n' +
      'We average 7 labels for each sentence pair and remove outliers following Agirre et al. [3], Cer et al. [13]. First, we filter out annotators showing Pearson\'s correlation < 0.80 or Krippendorff\'s alpha < 0.20 (nominal) [67] with others\' annotations. We exclude two annotators with this criteria so all sentence pairs have annotations from at least five people. Lastly, similarity score is rounded up to the first decimal place.\n' +
      '\n' +
      'A few more filtering schemes are applied. First, we drop 14 pairs whose annotations are showing larger than 2 standard deviation. Those pairs might contain ambiguous expressions interpreted in various ways, or misannotations. Second, we ask workers to report the sentences including translation error or misinformation caused by RTT. We inspect the reported sentences and remove 418 sentence pairs. Third, we drop sentences involving ethical issues. Workers report the pairs if they are including any kind of hate speech, social bias, and potential personally identifiable information (PII). 1,213 sentence pairs were additionally removed after inspection. As a result, we have 13,224 sentence pairs in total. We report inter-annotator agreement (IAA) by using Krippendorff\'s alpha instead of Pearson\'s correlation because 7 annotators (or less) differ by pairs. The annotator agreed to each other\'s annotations. (Krippendorff\'s alpha (interval) = 0.85).\n' +
      '\n' +
      'We observe the distribution of similarity score annotations differ between the _potentially_ similar sentence pairs and the less similar pairs. Figure 1 illustrates label distributions generated by RTT (top) and GSM (bottom) in AIRBNB. As expected, RTT pairs tend to show high similarity (from 3 to 5) while GSM pairs are considered less similar (from 0 to 3). Note that the number of GSM pairs scored 0 is high even we employ similarity-based matching. Similar tendencies are observed in POLICY and PARAKQC. By combining two distributions, we manage to obtain various sentence pairs in terms of similarity scores.\n' +
      '\n' +
      'Final DatasetWe collect 13,224 sentence pairs and corresponding similarity scores. We split them to training, development, and test sets, considering the distribution of the scores. Even if we carefully sampled the pairs, the overall score distribution is not uniform across \\(0-5\\) as shown in Figure 1. However, we prefer uniform distribution at least in evaluation (development and test) set, in order to prevent evaluation bias toward a specific score. We therefore construct the evaluation set having approximately uniform distribution as shown in Figure 2. To this end, we divide the score range 0\\(-\\)5 to 51 bins, rounding up to the first decimal place of every scores. We try to balance the number of pairs across bins. Since some of them have small number of pairs, we try to fit all number of the pairs close to that number.\n' +
      '\n' +
      'We also consider word overlap between sentences in each pair for evaluation set. Since larger word overlap might indicate higher semantic similarity, we try to reduce pairs satisfying such tendency to prevent the model from predicting similarity simply using word overlap. The overlap is measured by morpheme-level Jaccard distance by using MeCab [68]. We choose the pairs with the least word overlap from score 3\\(-\\)5, and the pairs with most word overlap from the rest. Such pairs are prioritized to be included to every bins in the dev and the test sets.\n' +
      '\n' +
      'We split the evaluation set with 1:2 ratio to construct the dev and the test sets, resulting in 519 and 1,037 pairs, respectively. The rest 11,668 pairs comprise the train set. Detailed numbers for each corpus are presented in Table 4. For all the sets, we balance the ratio between source corpora with that of the original pairs. Additionally, the scores are binarized with a threshold 3.0 same as paraphrase detection task.\n' +
      '\n' +
      '#### 3.2.2 Evaluation Metrics\n' +
      '\n' +
      'The evaluation metrics for KLUE-STS is 1) Pearson\'s correlation coefficient (Pearson\' \\(r\\)), and 2) F1 score. Pearson\'s \\(r\\) is a measure of linear correlation between human-labeled sentence-similarity scores and model predicted scores, adopted in STS-b [13]. Since our dev and test set have a balanced score distribution, the coefficient correctly gives the magnitude of the relationship. F1 score is adopted to measure binarized results (_paraphrased / not paraphrased_). Specifically, our F1 reports results for the _paraphrased_ class.\n' +
      '\n' +
      'Figure 1: Label distributions generated by RTT (top) and GSM (bottom) in AIRBNB.\n' +
      '\n' +
      'Figure 2: Similarity score distribution of the train (top) and dev (bottom) set. The scores of dev set is close to uniform distribution across range 0\\(-\\)5. The scores are rounded to the first decimal place.\n' +
      '\n' +
      '#### 3.2.3 Related Work\n' +
      '\n' +
      'Measuring similarity between sentences is a fundamental natural language understanding problem so that closely related to various NLP applications. Because of its importance, STS is included in various NLU benchmarks [133, 142]. To facilitate research in this area, many shared tasks have been held and annotated corpora are released [2, 3, 13]. Typically, they cover multiple text domains such as question pairs, image descriptions, news headlines, annotated with a real value from 0 (no meaning overlap) to 5 (meaning equivalence).\n' +
      '\n' +
      'Recently, Ham et al. [45] introduces a machine-translated Korean STS benchmark. This is a translation of [13] in GLUE, which contains around 8,600 sentence pairs in total. All examples are solely relying on machine translation, and sentence pairs in evaluation (dev and test) set are further post-edited by human. However, corresponding labels were not adjusted to translated meanings. Lack of re-labeling process would be problematic because Korean speakers would judge the similarity between them differently.\n' +
      '\n' +
      'If similarity labels are binarized by a certain threshold, STS also could be seen as paraphrase detection task such as Microsoft Research Paraphrase Corpus (MRPC) [32], Quora Question Pairs (QQP) [133], or PAWS [152] and PAWS-X [144]. Thus we additionally binarize our ground truths and predictions, reporting binary classification performance to see how well a model performs in paraphrase detection.\n' +
      '\n' +
      'In paraphrase detection, Cho et al. [18] presents a benchmark that includes the human-generated queries for smart home, where ten paraphrase sentences are grouped together to make up a total of 1,000 groups. The granularity of scale is from 0 to 5, but the semantic similarity is judged only with attributes such as topic (smart home, weather, etc.) and speech act (question, prohibition, etc.), which does not consider other details such as nuance and syntactic structure because it lacks direct human judgement of similarity. PAWS-X [144] provides a translated version of PAWS [152] of Korean. Like KorSTS, the train split is machine-translated and its dev and test splits are human-translated, and corresponding labels are preserved without human inspection. There are also paraphrase corpora provided by government-funded institutions such as National Institute of Korean Language (NIKL) [98], but it simply provides human-generated and machine-paraphrased sentences with limited accessibility.\n' +
      '\n' +
      '#### 3.2.4 Conclusion\n' +
      '\n' +
      'We create the first human-annotated Korean STS benchmark, KLUE-STS, that covers multiple domains and styles with free accessibility to everyone. The similarity score annotation process is specially designed to capture the characteristics of the Korean language. Covering the expressions from various domains, our benchmark is expected to be a useful resource for further research, beyond serving as a benchmark. Our benchmark helps to develop numerous models established on STS resources, such as SentenceBERT [115].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Source** & **|Train** & **|Devl** & **|Testl** & **Total** \\\\ \\hline AIRBNB & 5,371 & 255 & 510 & 6,136 \\\\ POLICY & 2,344 & 132 & 264 & 2,740 \\\\ PARAKQC & 3,953 & 132 & 263 & 4,348 \\\\ \\hline\n' +
      '**Overall** & **11,668** & **519** & **1,037** & **13,224** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Statistics for KLUE-STS. The first three columns provide the number of examples in train, dev, and test sets of each source corpus and the final data.\n' +
      '\n' +
      '### Natural Language Inference (NLI)\n' +
      '\n' +
      'The goal of natural language inference (NLI) is to train a model to infer the relationship between the _hypothesis_ sentence and the _premise_ sentence. Given a _premise_, an NLI model determines if _hypothesis_ is true (entailment), false (contradiction), or undetermined (neutral). The task is also known as recognizing textual entailment (RTE) [27].\n' +
      '\n' +
      'Understanding entailment and contradiction between sentences is fundamental to NLU. NLI datasets are also included in various NLU benchmarks such as GLUE [133] and superGLUE [132], and they are valuable as training data for other NLU tasks [24, 107, 115].\n' +
      '\n' +
      'We formulate NLI as a classification task where an NLI model reads each pair of _premise_ and _hypothesis_ sentences and predicts whether the relationship is entailment, contradiction, or neutral. We use the classification accuracy to measure the model performance.\n' +
      '\n' +
      '#### 3.3.1 Dataset Construction\n' +
      '\n' +
      'We construct KLUE-NLI by using a collection method similar to that of SNLI [8] and MNLI [138]. First, we collect premise sentences from existing corpora. Then for each premise sentence, we ask one annotator to generate three new hypothesis sentences, one for each of the three relationship classes. Then for each pair of premise and hypothesis sentences, we ask four additional annotators to label the relationship for validation. We follow the criteria proposed by Williams et al. [138] to describe the three labels to the annotators. For both hypothesis generation and pair validation, we recruit workers from SelectStar,23 a Korean crowdsourcing platform.\n' +
      '\n' +
      'Footnote 23: [https://selectstar.ai/](https://selectstar.ai/)\n' +
      '\n' +
      'Source Corpora for Premise SentencesWe use six corpora for the set of premise sentences: WIKITREE, POLICY, WIKINEWS, WIKIPEDIA, NSMC and AIRBNB. They cover diverse topics and writing styles of contemporary Korean. WIKITREE, POLICY and WIKINEWS are news articles and WIKIPEDIA is a crowd-sourced encyclopedia, all of which are written in formal Korean. NSMC and AIRBNB consist of colloquial reviews in the domains of movies and travel, respectively.\n' +
      '\n' +
      'From the six corpora, we extract 10,000 premises with which we elicit hypotheses. A valid premise should satisfy three conditions. First, premise is a proposition, a declarative sentence to which we can assign a truth value, excluding mathematical formulae and lists. Second, a premise must include at least one predicate, and the predicate can be of diverse types such as states (e.g., be, believe, know), activities (e.g., play, smile, walk), achievements (e.g. realize, reach, break), and accomplishments (e.g. eat, build, paint). Third, the length of a premise should be from 20 to 90 characters including whitespace.\n' +
      '\n' +
      'Annotation Protocol for Hypothesis GenerationWe show annotators a premise and ask them to write three hypotheses that correspond to each label. This allows us to collect nearly equal number of the (premise, hypothesis) pairs for each labels. We maintain the outline of the criteria as follows:\n' +
      '\n' +
      '* ENTAILMENT: The hypothesis is necessarily true given the premise is true\n' +
      '* CONTRADICITION: The hypothesis is necessarily false given the premise is true\n' +
      '* NEUTRAL: The hypothesis may or may not be true given the premise is true\n' +
      '\n' +
      'We are aware of the annotation artifacts coming from human writing-based hypothesis generation. Sentence length and explicit lexical patterns are highly associated with certain classes. Neutral sentences tend to be the longest among all classes, since workers can produce neutral hypothesis simply by introducing additional phrase or clause not stated in the premise. Negations such as "no", "never" and "nothing" are often accompanied with the class CONTRADICION [44, 108].\n' +
      '\n' +
      'Despite the concerns of such artifacts, we stick to such a writing-based annotation procedure. Compared to automatic pipelines to collect hypotheses, human writing yields higher quality data and is still an effective protocol [131]. We focus on ways to encourage annotators to avoid injecting trivial patterns. We prepare guidelines with specific _Do_s and _Don_\'ts, and rigorously train the workers in advance. To minimize annotation artifacts, we instruct the annotators to write sentences with similar lengths across the classes, refrain from inserting certain lexical items repeatedly, and use as diverse strategies as possible when making inferences.\n' +
      '\n' +
      'Specifically, we provide detailed guidelines for hypothesis generation together with examples. We encourage annotators to create hypotheses that exhibit diverse linguistic phenomena, in terms of 1) lexical choice, 2) syntactic structures and 3) world knowledge. In the case of lexical choice, our guideline suggests annotators use synonyms/antonyms,hypernyms/hyponyms, and auxiliary particles. To introduce various syntactic structures, we provide several syntactic transformation strategies such as word scrambling, voice alteration, and causative alternation. Methods like subject/object swapping or passivization is motivated by existing NLI data augmentation strategies [89, 42]. We also encourage using expressions that reflect world knowledge such as time, quantity and geography in order to create a dataset grounded to the real world.\n' +
      '\n' +
      'There are a few more details in the guideline. We instruct annotators to maintain the writing style of the premise to create a balanced dataset in terms of the style as well. We also instruct them to skip sentences that are difficult to understand either due to the ungrammaticality or the complexity of the content. They are also instructed to skip and report sentences that contain ethical issues such as hate speech, social bias, or personally identifiable information. We examine all reported sentences and make final decisions whether to include the sentences in the dataset.\n' +
      '\n' +
      'Annotation Protocol for Label ValidationCrowdworkers annotate the relations of the resulting premise-hypothesis pairs for validation. For each of the pairs created, we ask four crowdworkers to supply a single label among (ENTAILMENT, CONTRADICTION, NEUTRAL). This yields a total of five labels per pair, including the initial label intended by the annotator who wrote the hypothesis sentence. For each validated sentence pair, we assign a gold label representing the majority of three or more votes out of five.\n' +
      '\n' +
      'Annotation ProcessFor hypothesis generation, we go through a pilot phase where we iteratively update the guidelines and train the workers. During the pilot, we find writing a semantically unacceptable sentence or introducing a demonstrative pronoun not used in the premise could be potential problems. Since they might alter the intended label, we ask workers to avoid writing such sentences. The number of workers for this part of the annotation process is 11.\n' +
      '\n' +
      'We then validate the relation labels for every pair. We go through a pilot phase, starting with 2,604 applicants in the pilot, then select 684 who passed the test to participate in the validation step. With 138 workers dropping out, the final number of workers is 546.\n' +
      '\n' +
      'Validation results are summarized in Table 5. They suggest that our writing protocol is effective in producing a high quality corpus. The rate of unanimous gold labeled examples in KLUE-NLI is 18% higher than SNLI and MNLI. The higher the rate of such examples, the clearer the relationship between the generated hypothesis sentences and the original premise sentences. Individual annotator\'s agreement with the gold label and the author\'s label are also higher than SNLI and MNLI, and almost all pairs receive the gold label. Only a few sentence pairs (0.53%) lack the gold label, and we remove those before finalizing our dataset.\n' +
      '\n' +
      'Final DatasetThe final dataset consists of 30,998 sentence pairs that are divided into train/development/test sets. Table 6 shows the basic statistics of the dataset. As observed in SNLI and MNLI, our premise sentences also tend to be longer than the corresponding hypothesis sentences. This is because workers generally use partial information of a premise to write a hypothesis.\n' +
      '\n' +
      'Note that we deliberately form the development and test sets in a way to 1) contain balanced source styles and 2) disincentivize models exploiting annotation artifacts. The development and the test set each contains 3,000 sentence pairs.\n' +
      '\n' +
      'To maintain consistency of style in development and test sets, we include in each set 60% formal and 40% colloquial sentences. We sample 450 sentences each from formal text WIKITREE, POLICY, WIKINEWS, WIKIPEDIA, and 600 sentences each from colloquial text NSMC, AIRBNB.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Statistics** & **SNLI** & **MNLI** & **KLUE-NLI** \\\\ \\hline Unanimous Gold Label & 58.30\\% & 58.20\\% & **76.29\\%** \\\\ \\hline Individual Label = Gold Label & 89.00\\% & 88.70\\% & **92.63\\%** \\\\ Individual Label = Authors Label & 85.80\\% & 85.20\\% & **90.92\\%** \\\\ \\hline Gold Label = Authors Label & 91.20\\% & 92.60\\% & **96.76\\%** \\\\ Gold Label \\(\\neq\\) Authors Label & 6.80\\% & 5.60\\% & **2.71\\%** \\\\ No Gold Label (No 3 Labels Match) & 2.00\\% & 1.80\\% & **0.53\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Summary of validation statistics for KLUE-NLI compared to SNLI and MNLI [138]. We call the label intended by the original annotator in writing the hypothesis authors label. Consensus among three out of five annotators is gold label.To prevent our NLI benchmark from incentivizing a model that predicts a label using a spurious cue in the hypothesis, we first fine-tune the KLUE-RoBERTa-base model using only the hypothesis sentences with their corresponding labels. If the model finds no clue between the hypothesis and the label, the predicted probability scores for each label should be uniform (i.e., one-third (\\(\\frac{1}{3}\\)) when classified 3-way). Assuming that such score distribution is ideal, we prefer the pairs for development/test sets whose hypothesis-only model\'s predictions are closest to the ideal. We compute the distance between the prediction and the ideal using cross entropy. To preserve the intact sets of a premise and its three hypotheses, we calculate the mean distance of each set. We extract the sets whose mean distance is among the lowest 20%, and randomly split them into dev and test sets.\n' +
      '\n' +
      'Our idea can be viewed as an extension of pointwise mutual information (PMI). PMI between each hypothesis word (\\(w\\)) and class label (\\(c\\)) has been used to discover the association of the word with each class [44, 131]. If PMI is expanded to the sentence-level association, the metric provides a similar measure to the hypothesis-only model prediction probability as below.\n' +
      '\n' +
      '\\[\\text{PMI}(w,c)=\\log\\frac{P(w,c)}{P(w)P(c)}=\\log\\frac{P(c|w)P(w)}{P(w)P(c)}= \\log\\frac{P(c|w)}{P(c)}\\propto P(c|w)\\]\n' +
      '\n' +
      'To measure human performance and examine whether KLUE-NLI test set improves upon KorNLI [45] test set, a machine-translation of the XNLI [25] test set, we conduct a round of human evaluation. We employ four native Korean undergraduates who major in Korean linguistics and did not participate in the KLUE-NLI construction process. We randomly sample 100 sentence pairs from KLUE-NLI test set and ask the workers to annotate them. We check the agreement of their annotations with the given gold label. We do the same on the subset of the KorNLI test set, to examine whether the human-elicited dataset improves the quality of the dataset. The results are shown in Table 7.\n' +
      '\n' +
      'For KorNLI, 38% of the sentence pairs have responses from all four annotators that match with the gold labels. There are 18%, 18%, and 16% of sentences, respectively, when three, and two, and one response match with the gold label. 10 pairs do not match with the gold label. On the other hand, KLUE-NLI shows much higher agreement with the given gold label. All annotators agree with the gold label in 71% of the pairs, and 95% obtain at least three agreements. Furthermore, only 258 out of 400 (64.50%) individual annotations are the same as the gold label in KorNLI. Again, KLUE-NLI shows better agreement with the gold labels. 360 (91.00%) annotations are the same as the gold label.\n' +
      '\n' +
      'These numbers in annotation quality of KLUE-NLI are better than KorNLI as well as SNLI and MLNI. In KorNLI, annotators often report that they do not quite understand at least one of the two sentences or choose NEUTRAL because it is difficult to distinguish the semantic relationships of the sentences. Although the distribution of the gold label is uniform (respectively 33, 33, and 34% of entailment, contradiction, and neutral sentences), the label chosen most frequently by the annotators is NEUTRAL (56.75% on average). There are 26% of cases where the gold labels are different from the majority vote by the annotator. These results suggest that the annotators struggle to grasp the logical semantic relationship of KorNLI sentences.\n' +
      '\n' +
      'On the other hand, for KLUE-NLI, there is no case where none of the four responses matches the gold label. Considering the cases where more than two of the responses match the gold label, there is a 98% chance of the gold label to be re-selected as the majority tag. Compared to KorNLI, we can see that KLUE-NLI is a much more reliable dataset. This result also confirms that the headroom of our current best model (accuracy: 89.77%) is still there, given that the human accuracy, represented by the majority tag, is 98%.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline\n' +
      '**Source** & **|Train** & **|Devl** & **|Testl** & **Total** & **Avg Len Prem** & **Avg Len Hyp** \\\\ \\hline WIKITREE & 3,838 & 450 & 450 & 4,738 & 52.81 & 26.86 \\\\ POLICY & 3,833 & 450 & 450 & 4,733 & 56.73 & 32.93 \\\\ WIKINEWS & 3,824 & 450 & 450 & 4,724 & 64.17 & 29.11 \\\\ WIKIPEDIA & 3,780 & 450 & 450 & 4,680 & 57.45 & 23.70 \\\\ \\hline NSMC & 4,899 & 600 & 600 & 6,099 & 27.48 & 21.49 \\\\ AIRBNB & 4,824 & 600 & 600 & 6,024 & 24.28 & 18.65 \\\\ \\hline\n' +
      '**Overall** & **24,998** & **3,000** & **3,000** & **30,998** & **47.15** & **25.46** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: Statistics for KLUE-NLI. The first three columns provide the number of sentence pairs in train, dev, and test sets. _Avg Len Prem_ and _Avg Len Hyp_ are the mean character counts of premise and hypothesis sentences, respectively.\n' +
      '\n' +
      '#### 3.3.2 Evaluation Metric\n' +
      '\n' +
      'The evaluation metric for KLUE-NLI is accuracy, following SNLI [8] and MNLI [138]. Accuracy measures how well a classifier correctly identifies the results. The class labels are almost equally distributed, thus higher accuracy will correctly represent performances of a model.\n' +
      '\n' +
      '#### 3.3.3 Related Work\n' +
      '\n' +
      'Recognizing Textual Entailment (RTE) [27] is a task similar to NLI and was introduced in a series of textual entailment challenges. In the RTE task, two sentences are given, and the model decides whether the meaning of one sentence can be entailed from the other sentence. In earlier RTE 1-3, the task is binary, \'ENTAILMENT\' and \'NO ENTAILMENT\'. In RTE 4-5, a new class \'UNKNOWN\' is introduced, and the task is formulated as a three-way classification.\n' +
      '\n' +
      'Two major datasets for NLI in English are Stanford Natural Language Inference (SNLI) [8] and Multi-Genre Natural Language Inference (MNLI) [138]. Hypothesis sentences in SNLI and MNLI are labeled ENTAILMENT, CONTRA-DICTION, or NEUTRAL. SNLI is two orders of magnitude larger than the RTE corpora, made from 570,152 image captions in Flickr30k [148]. MNLI premise sentences are derived from 10 different sources, covering a wider range of styles, degrees of formality, and topics.\n' +
      '\n' +
      'Most of the existing NLI datasets are in English, including SNLI and MNLI, and one common approach for constructing NLI datasets in other languages is to translate the existing English corpora to the language of interest. Conneau et al. [25] provides XNLI (Cross-lingual natural language inference) by employing professional translators to translate the development and test sets of MNLI into 15 languages. One main concern of the translation-based approach is whether the relation of the original sentence pair is maintained in the process. Conneau et al. [25] find some translated pairs lose the initial semantic relationship, validated by human annotators who re-annotate a sample of the dataset. The result demonstrates that human translations cause 2% misannotations given the 85% correct examples in the MNLI and 83% in XNLI.\n' +
      '\n' +
      'Motivated by the fact that Korean is not included in XNLI, KorNLI [45] is introduced. KorNLI [45] is a translation of existing English corpora whose train set is created through machine translation of training sets of SNLI and MNLI, and the development and test sets through machine translation of development and tests sets of XNLI and post-editing by professional translators. Although Ham et al. [45] also investigate the data manually and acknowledge some incorrect examples after the translation, no human validation process is performed to quantify the observation and leave analyzing such errors to future work. Moreover, even with post-editing, there are some sentences that are either unnatural in terms of syntactic structure or word choice.\n' +
      '\n' +
      'Many studies have been proposed based on SNLI and MNLI; however, SNLI and MNLI are known to have annotation artifacts [44, 108]. Annotation artifacts are the product of certain types of annotation strategies and heuristics naturally arising from the crowdsourcing process. Such artifacts are problematic as they may lead models to adopt heuristics rather than to actually learn the relationship.\n' +
      '\n' +
      'There have been some efforts to reduce annotation artifacts in NLI. Vania et al. [131] experiment with two fully automated protocols for creating premise-hypothesis pairs, but find that the methods yield poor-quality data and mixed results on annotation artifacts. OCNLI [53] enhance writing-base protocol with some interventions to control the bias: encouraging writers to use diverse ways of making inference, and putting constraints on overused words. Despite partial effects on reducing negators, the explicit constraint gives rise to other words of correlation, and the final OCNLI dataset exhibit similar level of hypothesis-only test scores to most benchmark NLI datasets.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Statistics** & **KorNLI** & **KLUE-NLI** \\\\ \\hline Unanimous Gold Label (4 Agree) & 38.00\\% & **71.00\\%** \\\\\n' +
      '3 Agree with Gold Label & 18.00\\% & 24.00\\% \\\\\n' +
      '2 Agree with Gold Label & 18.00\\% & 3.00\\% \\\\\n' +
      '1 Agrees with Gold Label & 16.00\\% & 2.00\\% \\\\\n' +
      '0 Agrees with Gold Label & 10.00\\% & 0.00\\% \\\\ \\hline Individual Label = Gold Label & 64.50\\% & **91.00\\%** \\\\ \\hline No Gold Label (No 3 Labels Match) & 4.00\\% & **0.00\\%** \\\\ Majority Vote \\(\\neq\\) Gold Label & 26.00\\% & **0.00\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Statistics for human evaluation results of KorNLI and KLUE-NLI. We compare the labels of four annotators with gold labels of korNLI and KLUE-NLI test data.\n' +
      '\n' +
      '#### 3.3.4 Conclusion\n' +
      '\n' +
      'Our new dataset, KLUE-NLI, is the first resource constructed upon naturally occurring Korean sentences. KLUE-NLI represents diverse linguistic phenomena, writing style, degree of formality and contents that are most natural and suitable for Korean. The premise sentences of our dataset come from six Korean corpora, and the hypothesis sentences are written by well-trained workers.\n' +
      '\n' +
      'By keeping the writing-based protocol and thoroughly training workers based on detailed guidelines, we improve upon the existing NLI datasets in the reliability of the labels. KLUE-NLI shows much higher inter-annotator agreement rate than both the MNLI and the translation-based Korean dataset, KorNLI. The gap between the human performance scores of KLUE-NLI and KorNLI also provides evidence that KLUE-NLI is currently the optimal Korean NLI dataset.\n' +
      '\n' +
      'Beyond its main purpose as an NLI benchmark dataset, we hope KLUE-NLI will be a useful resource for future NLU research, as English dataset such as MNLI and SNLI are extended [24, 107, 115].\n' +
      '\n' +
      '### Named Entity Recognition (NER)\n' +
      '\n' +
      'The goal of named entity recognition (NER) is to detect the boundaries of named entities in unstructured text and classify the types. An entity can be series of words that refers to the person, location, organization, time expressions, quantities, monetary values.\n' +
      '\n' +
      'Since NER is an important for application fields like syntax analysis, goal-oriented dialog system, question and answering chatbot and information extraction, various NLU benchmarks contains NER datasets [137, 57, 78, 54]. Despite the rise of necessity of NER datasets in various domains and styles, there are few existing Korean NER datasets to cover such need. Therefore, we annotate corpora including web texts that can be applied to real-word applications.\n' +
      '\n' +
      'In KLUE-NER, a model should detect the spans and classify the types of entities included in an input sentence. The six entity types used in KLUE-NER are person, location, organization, date, time, and quantity. They are tagged via character-level BIO (Begin-Inside-Outside) tagging scheme, and thus we evaluate a model\'s performance using entity-level and character-level F1 score.\n' +
      '\n' +
      '#### 3.4.1 Dataset Construction\n' +
      '\n' +
      'Source CorporaTo incorporate both formal and informal writing styles, we use two corpora, WIKITREE and NSMC for annotation. WIKITREE is a news article corpus and thus contains formal sentences with many entity types, which suits well as a source corpus for NER. NSMC includes colloquial reviews of movies or TV shows. Since the texts in NSMC are user-generated comments, they contain errata and non-normalized expressions, along with emojis and slang. Such a noisy dataset will help broaden the application field of NER models.\n' +
      '\n' +
      'The preprocessing of the two corpora is performed differently considering the characteristics of each corpus. For WIKITREE, since the news articles are mainly composed of well-written sentences, we simply split the articles into sentences. In contrast, the web texts from NSMC are written in the style of spoken language with blurry sentence boundaries. As each review is generally quite short and the sentences consisting it are on the same topic, we use each review as a single unit of input. In addition, the sentences that contain hate speech or socially biased terms are removed manually. For both corpora, we remove sentences longer than 400 characters.\n' +
      '\n' +
      'For efficient annotation, we perform pseudo-labeling with a pretrained model. The model is trained with BERT-CRF using a publicly available dataset KMOU-NER corpus,24 to support fast and accurate entity tagging for annotators. We also filter out the sentences with no pseudo-labeled entity assuming they do not include any of the entities. Remaining sentences account for about 80% in WIKITREE and 41% in NSMC, leaving a total of 36,515 sentences.\n' +
      '\n' +
      'Footnote 24: [https://github.com/kmounlp/NER](https://github.com/kmounlp/NER)\n' +
      '\n' +
      'Annotation ProtocolWe use six entity types for KLUE-NER annotation: PS (Person), LC (Location), OG (Organization), DT (Date), TI (Time), and QT (Quantity). The description of each entity type is as follows.\n' +
      '\n' +
      '* PS (Person): Name of an individual or a group\n' +
      '* LC (Location): Name of a district/province or a geographical location\n' +
      '* OG (Organization): Name of an organization or an enterprise\n' +
      '* DT (Date): Expressions related to date/period/era/age\n' +
      '* TI (Time): Expressions related to time\n' +
      '* QT (Quantity): Expressions related to quantity or number including units\n' +
      '\n' +
      'We employ the above sets following the convention of two existing tag sets: Korean Telecommunications Technology Association (TTA) NER guidelines25 and MUC-7 [16]. TTA guideline is a standardized NER tagging scheme for Korean language and we follow the names and the definitions of its entity types. Among the 15 entity types of TTA, we select our six types that correspond with tagsets used in MUC-7 (DATE, LOCATION, MONEY, ORGANIZATION, PERCENT, PERSON and TIME). As MONEY and PERCENT types are included in QT (QUANTITY) type from TTA set, we instead adopt an entity type QT.\n' +
      '\n' +
      'Footnote 25: [https://committee.tta.or.kr/data/standard_view.jsp?nowPage=2&pk_num=TTAK.KO-10.0852&commit_code=PG606](https://committee.tta.or.kr/data/standard_view.jsp?nowPage=2&pk_num=TTAK.KO-10.0852&commit_code=PG606)\n' +
      '\n' +
      'In the case of entities with multiple possible entity types, instead of assigning a unique tag for all use cases, we determine their tags based on the context. One example is _Cine21_, which, in Korean, can either refer to the name of a magazine or the publisher of the magazine. In a sentence like "I bought a Cine21 from a bookstore and read it page by page," Buy something from a bookstore\' and\'read page by page\' are properties regarding media (magazine), rather than an organization; thus we do not assign an OG tag.\n' +
      '\n' +
      'We guide crowdworkers to report if the text for annotation does not meet certain conditions. For example, texts consisting of multiple sentences, texts that are not in a sentence form, a fragment, and a simple sequence of nouns are discarded. Workers are also required to report sentences that include hate speech and various biases in tagging process.\n' +
      '\n' +
      'In terms of personally identifiable information, we cannot simply drop or pseudonymize the information because the very task of NER often requires the specific information of proper nouns such as person names (PS). In order to minimize the loss of sentences, we inspect through the sentences after the annotation process. We investigate the sentences that include PS tags, and keep the ones that contain the name of public figures that appear in Korean search engines.26 Other sentences are removed if it has potential privacy issues.\n' +
      '\n' +
      'Footnote 26: Daum: [http://search.daum.net/search?nil_suggest=btn&nil_ch=&rtupcoll=&w=tot&m=&f=&lpp=&q=%CO%CE%B9%BO%CB%BB%F6/Naver](http://search.daum.net/search?nil_suggest=btn&nil_ch=&rtupcoll=&w=tot&m=&f=&lpp=&q=%CO%CE%B9%BO%CB%BB%F6/Naver): [https://people.search.naver.com/](https://people.search.naver.com/)\n' +
      '\n' +
      'Annotation Process51 qualified crowdworkers recruited by a Korean crowdsourcing platform, DeepNatural27 participate in the annotation process. The qualification is given when passing a pilot entity tagging test. Then two linguists check whether the crowdworkers\' annotations are correct or not. We find some erroneous annotations remaining even after validation. Therefore, six NLP researchers manually correct the annotation errors.\n' +
      '\n' +
      'Footnote 27: [https://deepnatural.ai/](https://deepnatural.ai/)\n' +
      '\n' +
      'During the annotation process, 5,354 sentences are dropped by workers due to their inadequacy. 118 sentences are dropped due to the privacy issue, and 35 sentences are removed after the inspection by the researchers because all annotations are false positives. A total of 5,507 sentences are dropped in the inspection process, resulting in 31,008 sentences.\n' +
      '\n' +
      'Final DatasetThe resulting corpus is split into train/dev/test sets, each consisting of 21,008, 5,000, and 5,000 sentences (Table 8). The entity-wise statistics is provided in Table 9. We design the test set to include unseen entities to check the robustness of the models in terms of domain transitions and generalization.\n' +
      '\n' +
      'The finalized entity types are tagged in the character level BIO tagging scheme (Figure 3). In most English and Korean NER datasets, the entities are tagged with the word-level BIO scheme, following CoNLL 2003 dataset [129]. In Korean, however, it is difficult to adhere to the word level tagging scheme based on whitespace for two reasons. First, whitespace-split units (eojeols) are often not a single word and are a composite of content words and functional words (e.g., "E1-2-7" (the next week is) "E1-2" (the next week) "E1-2" (is)\') [46]. Second, many compound words in Korean contain whitespaces. Therefore, we choose to tag in character level.\n' +
      '\n' +
      '#### 3.4.2 Evaluation Metrics\n' +
      '\n' +
      'The evaluation metrics for KLUE-NER are 1) entity-level macro F1 (Entity F1) and 2) character-level macro F1 (Char F1) scores. Entity F1 score measures how many predicted entities and types are exactly matched with the ground truths\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Source** & **|Train** & **|Devl** & **|Testl** & **Total** \\\\ \\hline WIKITREE & 11,435 & 2,534 & 2,685 & 16,664 \\\\ NSMC & 9,573 & 2,466 & 2,315 & 14,354 \\\\ \\hline\n' +
      '**Total** & **21,008** & **5,000** & **5,000** & **31,008** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Statistics for KLUE-NER.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Source** & **|Train**| & **|Devl** & **|Testl** & **Total** \\\\ \\hline PS & 14,453 (5,428) & 4,418 (2,706) & 4,830 (3,063) & 23,289 (7,124) \\\\ LC & 6,663 (2,068) & 1,649 (896) & 2,064 (1,130) & 9,961 (2,650) \\\\ OG & 8,491 (3,008) & 2,182 (1,291) & 2,514 (1,579) & 12,855 (3,796) \\\\ DT & 8,029 (1,608) & 2,312 (835) & 2,498 (933) & 12,653 (2,060) \\\\ TI & 2,020 (573) & 5,45 (268) & 579 (316) & 3,110 (730) \\\\ QT & 11,717 (3,628) & 3,151 (1,763) & 3,827 (2,369) & 18,019 (4,776) \\\\ \\hline\n' +
      '**Total** & **51,373 (16,313)** & **14,257 (7,759)** & **16,312 (9,390)** & **79,887 (21,136)** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Entity-wise statistics for KLUE-NER. Note that the numbers in parentheses denote the number of types. The total number does not match Table 8 since this table does not remove duplication.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:27]\n' +
      '\n' +
      '### Relation Extraction (RE)\n' +
      '\n' +
      'Relation extraction (RE) identifies semantic relations between entity pairs in a text. The relation is defined between an entity pair consisting of _subject entity_ (\\(e_{\\text{subj}}\\)) and _object entity_ (\\(e_{\\text{obj}}\\)). For example, in a sentence \'Kierkegaard\' was born to an affluent family in Copenhagen\', the subject entity is \'Kierkegaard\' and the object entity is \'Copenhagen\'. The goal is then to pick an appropriate relationship between these two entities; \'_place_of_birth_\'.\n' +
      '\n' +
      'RE is a task suitable for evaluating whether a model correctly understands the relationships between entities. In order to ensure KLUE-RE captures this aspect of language understanding, we include a large-scale RE benchmark. Because there is no large-scale RE benchmark publicly available in Korean, we collect and annotate our own dataset.\n' +
      '\n' +
      'We formulate RE as a single sentence classification task. A model picks one of predefined relation classes describing the relation between two entities within a given sentence. In other words, the RE model predicts an appropriate relation \\(r\\) of entity pair \\((e_{\\text{subj}},e_{\\text{obj}})\\) in a sentence \\(s\\), where \\(e_{\\text{subj}}\\) is the subject entity and \\(e_{\\text{obj}}\\) is the object entity. We refer to \\((e_{\\text{subj}},r,e_{\\text{obj}})\\) as a relation triplet. The entities are marked as corresponding spans in each sentence \\(s\\). There are 30 relation classes that consist of 18 person-related relations, 11 organization-related relations, and _no_relation_. Detailed explanation of these classes are presented in Table 10. We evaluate a model using micro F1 score, computed after excluding _no_relation_, and area under the precision-recall curve including all 30 classes.\n' +
      '\n' +
      '#### 3.5.1 Data Construction\n' +
      '\n' +
      'Distant supervision [91] is a popular way to build a large-scale RE benchmark. It leverages relation triplets \\((e_{\\text{subj}},r,e_{\\text{obj}})\\) in existing large-scale knowledge base (KB) such as Freebase. If a sentence \\(s\\) in a large corpora includes \\((e_{\\text{subj}},e_{\\text{obj}})\\) detected by an NER model simultaneously, it is added to the dataset with relation label \\(r\\) by assuming any sentence which contains the pair will express that relation. This approach does not require expensive human annotation, thus allowing us to build a large-scale RE benchmark in a cost-effective way.\n' +
      '\n' +
      'Despite this advantage, distant supervision often ends up with incorrect relation labels when the assumption is not satisfied. In particular, it only considers pairs of entities which are related to each other, which results in an RE model trained on such corpus to over-predict the existence of some relationship between any given pair of entities. In other words, the predicted relation class distribution from such predictors is not realistic [116]. Zhang et al. [153] and Nam et al. [93] thus propose to employ crowdworkers to alleviate erroneous relations extracted by distant supervision. Riedel et al. [116] furthermore intentionally collect irrelevant entity pairs to prevent RE models from overly predicting false positives relations.\n' +
      '\n' +
      'OverviewWe modify the original strategy of distant supervision above, to address this weakness and to better fit our situation. First, we collect triplets \\((e_{\\text{subj}},r,e_{\\text{obj}})\\) from a small Korean KB29 and build additional ones by parsing the infoboxes in WIKIPEDIA and NAMUWIKI30 to enlarge the pool of the candidate triplets. We then ask crowdworkers to select the correct relation class of each candidate triplet within a sentence, compared to distant supervision which directly uses automatically generated relation labels. In addition, we randomly sample entity pairs in \\(s\\) to obtain more realistic relation class distribution in our benchmark. Those examples would include unseen entities in existing KB as well as have higher chance to be irrelevant (_no_relation_).\n' +
      '\n' +
      'Footnote 29: [https://aihub.or.kr/aidata/84](https://aihub.or.kr/aidata/84)\n' +
      '\n' +
      'Footnote 30: [https://namu.wiki](https://namu.wiki)\n' +
      '\n' +
      'This procedure can be divided into five steps; (1) candidate sentence collection, (2) relation schema definition, (3) entity detection, (4) entity pair selection and (5) relation annotation. We elaborate each step in the rest of this section.\n' +
      '\n' +
      '1. Collect Candidate SentencesWe sample candidate sentences from WIKIPEDIA, WIKITREE and POLICY corpora to cover a diverse set of named entities and relational facts. Since our task deals with single sentences, we exploit individual sentences split by Korean Sentence Splitter 31 at the preprocessing step. We filter out sentences that contain undesirable social bias and are considered hate speech, using a classifier trained on the Korean hate speech dataset [92].\n' +
      '\n' +
      'Footnote 31: [https://github.com/hyunwoongko/kss](https://github.com/hyunwoongko/kss)\n' +
      '\n' +
      '2. Define Relation SchemaWe design a relation schema based on the schema from Text Analysis Conference Knowledge Base Population (TAC-KBP) [87]. Our schema defines entity types and relation classes. Similar to TAC-KBP, we constrain \\(e_{\\text{subj}}\\) to be of either PER (Person) or ORG (Organization) type. \\(e_{\\text{obj}}\\) can have one of the following types: PER, ORG, LOC (Location), DAT (Date and time), POH (Other proper nouns), and NOH (Other numerals). For the relation classes, we adapt the original classes in TAC-KBP to our corpus, following Yu et al. [149].\n' +
      '\n' +
      'We remove rarely appearing relation classes in our corpus such as _org:website_, _per:shareholders_, _per:cause_of_death_, _per:charges_, and _per:age_. For the same reason, we incorporate _org:parents_ into _org:member_of_ and _org:subsidiaries_ into _org:members_. Since the taxonomy of TAC-KBP does not precisely reflect the regional hierarchy of Korea, we integrate the prefixes _country_of_, _city_of_, and _stateoprovince_of_ into _place_of_. We introduce additional classes frequently appearing in our corpus such as _org:product_, _per:product_ and _per:colleague_:\n' +
      '\n' +
      '* _org:product_: A product or merchandise produced by an organization. This includes intangible goods such as an event hosted and a business launched by the organization.\n' +
      '* _per:product_: A product produced by a person. Artworks (e.g. book, music, movie) or contribution to producing them.\n' +
      '* _per:colleague_: A person could be a colleague of someone if they work together. Two people in the same group such as political party or alliance are colleagues as well.\n' +
      '\n' +
      '3. Detect EntitiesWe automatically detect named entities in all candidate sentences. We fine-tune a pre-trained ELECTRA for Korean32 to build two named entity recognition (NER) models on two existing Korean NER resources respectively. One is provided by National Institute of Korean Language [98], and the other is built by Korea Maritime & Ocean University.33 We modify the named entity types defined in these resources to be compatible with our own entity types previously defined in the schema. We take the union of both models\' predictions to extract as many entities as possible. We use crowdsourcing to correct incorrect boundaries of the detected entities, as described later.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Relation Class** & **Description** \\\\ \\hline _no\\_relation_ & No relation in between \\((e_{\\text{subl}},e_{\\text{obj}})\\) \\\\ \\hline _org:dissolved_ & The date when the specified organization was dissolved \\\\ _org:founded_ & The date when the specified organization was founded \\\\ _org:place\\_of\\_headquarters_ & The place which the headquarters of the specified organization are located in \\\\ _org:alternate\\_names_ & Alternative names called instead of the official name to refer to the specified organization \\\\ _org:member\\_of_ & Organizations to which the specified organization belongs \\\\ _org:members_ & Organizations which belong to the specified organization \\\\ _org:political/religious\\_affiliation_ & Political/religious groups which the specified organization is affiliated in \\\\ _org:product_ & Products or merchandise produced by the specified organization \\\\ _org:founded\\_by_ & The person or organization that founded the specified organization \\\\ _org:top\\_members/employees_ & The representative(s) or members of the specified organization \\\\ _org:number\\_of\\_employees/members_ & The total number of members that are affiliated in the specified organization \\\\ \\hline _per:date\\_of\\_birth_ & The date when the specified person was born \\\\ _per:date\\_of\\_death_ & The date when the specified person died \\\\ _per:place\\_of\\_birth_ & The place where the specified person was born \\\\ _per:place\\_of\\_death_ & The place where the specified person died \\\\ _per:place\\_of\\_residence_ & The place where the specified person lives \\\\ _per:origin_ & The origins or the nationality of the specified person \\\\ _per:employee\\_of_ & The organization where the specified person works \\\\ _per:schools\\_attended_ & A school where the specified person attended \\\\ _per:alternate\\_names_ & Alternative names called instead of the official name to refer to the specified person \\\\ _per:parents_ & The parents of the specified person \\\\ _per:children_ & The children of the specified person \\\\ _per:siblings_ & The brothers and sisters of the specified person \\\\ _per:spouse_ & The spouse(s) of the specified person \\\\ _per:other\\_family_ & Family members of the specified person other than parents, children, siblings, and spouse(s) \\\\ _per:colleagues_ & People who work together with the specified person \\\\ _per:product_ & Products or artworks produced by the specified person \\\\ _per:religion_ & The religion in which the specified person believes \\\\ _per:title_ & Official or unofficial names that represent the occupational position of the specified person \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 10: 30 relation classes defined in the relation schema of KLUE-RE. Relation class \\(r\\) should be one of the followings which consist of 18 person-related relations, 11 organization-related relations, and _no_relation_.\n' +
      '\n' +
      '## 4 Select Entity Pairs\n' +
      '\n' +
      'We select two entities from the entity set \\(E\\) of a given sentence \\(s\\) to make an entity pair \\((e_{\\text{subj}},e_{\\text{obj}})\\). In doing so, we take two distinct approaches; (1) KB-based sampling and (2) uniform sampling.\n' +
      '\n' +
      'For the first approach, we only consider the subset of entities such that each entity pair \\((e_{\\text{subj}},e_{\\text{obj}})\\) appears in the pool of triplets \\((e_{\\text{subj}},r,e_{\\text{obj}})\\). We collect these triplets from two sources. First, we create the initial pool of triplets, using a Korean KB.34 Because the number of triplets (\\(\\sim\\)800k) from the Korean KB is small compared to, for instance, that of Freebase (\\(\\sim\\)2b), we enlarge this pool of triplets by gathering and then parsing infoboxes in WIKIPEDIA and Namuwiki. In order to avoid over-inclusion of frequent entities, such as the President of Korea, we set an upper bound to the number of co-occurrence between \\((e_{\\text{subj}},e_{\\text{obj}})\\) during sampling [153].\n' +
      '\n' +
      'Footnote 34: Released by NIA, a government-funded institution. Available at [https://aihub.or.kr/aidata/84](https://aihub.or.kr/aidata/84).\n' +
      '\n' +
      'In the second approach, \\((e_{\\text{subj}},e_{\\text{obj}})\\) is uniformly sampled from the entire entity set \\(E\\) of a given sentence \\(s\\), at random. Because there is no cue whether a sampled pair has any relation between them, the pair is highly likely to be irrelevant (_no_relation_). Irrelevant pairs will account for a large portion of realistic relation distribution between two arbitrary entities. Therefore, this approach helps to set up real-world scenario. Such a pair is also likely to contain entities that are not selected in the first approach. This leads to capturing entity pairs and their relations independent of KBs.\n' +
      '\n' +
      '## 5 Annotate Relations\n' +
      '\n' +
      'We ask workers recruited by DeepNatural,35 a Korean crowdsourcing platform, to annotate each entity pair \\((e_{\\text{subj}},e_{\\text{obj}})\\) with a relation label \\(r\\). We instruct workers to focus on the current relationship, not ones from the past. For instance, if a person described in a sentence is a former member of a certain organization, workers are asked not to choose the relation _per:employee_of_. We also ask them to avoid relying on external knowledge, or common sense, to infer the relation from the context solely within a given sentence. Workers report examples that contain hate speech, biased expressions, or personally identifiable information. In addition, they are asked to report sentences with incorrect entity boundaries.\n' +
      '\n' +
      'Footnote 35: [https://deepnatural.ai/](https://deepnatural.ai/)\n' +
      '\n' +
      'We employ 163 qualified workers, each of which correctly labelled at least 4 out of 5 questions during the pilot annotation phase. After the pilot phase, 3 workers are assigned to each example independently to label the relation. Figure 3.5.1 shows the annotation tool for crowdsourcing. To reduce cognitive burden of annotators, we provide a small number of candidate relations at first. The candidates consist of relations that can be defined between types of entity pair predicted by the NER models. If one cannot find appropriate \\(r\\) in the candidates, they are expanded to all relation classes.\n' +
      '\n' +
      'Figure 4: Annotation tool for crowdsourcing. Main features are translated in English with red color.\n' +
      '\n' +
      'We take majority-voted labels as gold labels. For each example without a majority label, the top 30 annotators select the final label from the annotated labels. We do not include examples reported as hate speech, biased, or to have privacy issues. The inter-annotator agreement (Krippendorff\'s \\(\\alpha\\)) on the annotated dataset is 0.701 [67].\n' +
      '\n' +
      'Final DatasetKLUE-RE consists of 32,470 training, 7,765 development and 7,766 test examples. For real-world scenario, we only use examples created from uniform sampling when building the development and test sets. In the test set, we only include sentences with entities that do not appear in the training set.\n' +
      '\n' +
      'The average length of a sentence in KLUE-RE is 95.9 characters including whitespaces. The proportions of the entity types are: PER (38.1%), ORG (36.3%), LOC (6.2%), DAT (6.2%), POH (11.9%), and NOH (1.3%). The distribution of the relation classes is shown in Table 11.\n' +
      '\n' +
      '#### 3.5.2 Evaluation Metrics\n' +
      '\n' +
      'The evaluation metrics for KLUE-RE are 1) micro F1 score on relation existing cases, and 2) area under the precision-recall curve (AUPRC) on all classes. Micro F1 score is a harmonic mean of micro-precision and micro-recall. It measures the F1 score of the aggregated contributions of all classes. It gives each sample the same importance, thus naturally weighting more on the majority class. We remove the dominant class (\\(no\\_relation\\)) for this metric to not incentivize the model that focus more on predicting negative class. AUPRC is an averaged area under the precision-recall curves whose x-axis is recall and y-axis is the precision of all relation classes. It is a useful metric for this imbalanced data setting where important positive examples are rarely occurred.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r} \\hline \\hline  & \\multicolumn{2}{c}{**Train**} & \\multicolumn{2}{c}{**Dev**} & \\multicolumn{2}{c}{**Test**} \\\\ \\cline{2-7}\n' +
      '**Relation Class** & Count & Ratio & Count & Ratio & Count & Ratio \\\\ \\hline _no\\_relation_ & 9,534 & 29.36\\% & 4,631 & 59.64\\% & 4,632 & 59.64\\% \\\\ \\hline _org:dissolved_ & 66 & 0.20\\% & 11 & 0.14\\% & 10 & 0.13\\% \\\\ _org:founded_ & 450 & 1.39\\% & 20 & 0.26\\% & 20 & 0.26\\% \\\\ _org:place\\_of\\_headquarters_ & 1,195 & 3.68\\% & 194 & 2.50\\% & 193 & 2.49\\% \\\\ _org:alternate\\_names_ & 1,320 & 4.07\\% & 78 & 1.00\\% & 77 & 0.99\\% \\\\ _org:member\\_of_ & 1,866 & 5.75\\% & 104 & 1.34\\% & 105 & 1.35\\% \\\\ _org:members_ & 420 & 1.29\\% & 122 & 1.57\\% & 122 & 1.57\\% \\\\ _org:political/religious\\_affiliation_ & 98 & 0.30\\% & 13 & 0.17\\% & 13 & 0.17\\% \\\\ _org:product_ & 380 & 1.17\\% & 235 & 3.03\\% & 235 & 3.03\\% \\\\ _org:founded\\_by_ & 155 & 0.48\\% & 11 & 0.14\\% & 11 & 0.14\\% \\\\ _org:top\\_members/employees_ & 4,284 & 13.19\\% & 513 & 6.61\\% & 514 & 6.62\\% \\\\ _org:number\\_of\\_employees/members_ & 48 & 0.15\\% & 17 & 0.22\\% & 18 & 0.23\\% \\\\ \\hline _per:date\\_of\\_birth_ & 1,130 & 3.48\\% & 12 & 0.15\\% & 12 & 0.15\\% \\\\ _per:date\\_of\\_death_ & 418 & 1.29\\% & 13 & 0.17\\% & 13 & 0.17\\% \\\\ _per:place\\_of\\_birth_ & 166 & 0.51\\% & 11 & 0.14\\% & 10 & 0.13\\% \\\\ _per:place\\_of\\_death_ & 40 & 0.12\\% & 10 & 0.13\\% & 11 & 0.14\\% \\\\ _per:place\\_of\\_residence_ & 193 & 0.59\\% & 124 & 1.60\\% & 125 & 1.61\\% \\\\ _per:origin_ & 1,234 & 3.80\\% & 118 & 1.52\\% & 118 & 1.52\\% \\\\ _per:employee\\_of_ & 3,573 & 11.00\\% & 242 & 3.12\\% & 241 & 3.10\\% \\\\ _per:schools\\_attended_ & 82 & 0.25\\% & 11 & 0.14\\% & 11 & 0.14\\% \\\\ _per:alternate\\_names_ & 1,001 & 3.08\\% & 104 & 1.34\\% & 103 & 1.33\\% \\\\ _per:parents_ & 520 & 1.60\\% & 27 & 0.35\\% & 27 & 0.35\\% \\\\ _per:children_ & 304 & 0.94\\% & 27 & 0.35\\% & 27 & 0.35\\% \\\\ _per:siblings_ & 136 & 0.42\\% & 24 & 0.31\\% & 24 & 0.31\\% \\\\ _per:spouse_ & 795 & 2.45\\% & 41 & 0.53\\% & 40 & 0.52\\% \\\\ _per:other\\_family_ & 190 & 0.59\\% & 34 & 0.44\\% & 35 & 0.45\\% \\\\ _per:colleagues_ & 534 & 1.64\\% & 220 & 2.83\\% & 220 & 2.83\\% \\\\ _per:product_\n' +
      '\n' +
      '#### 3.5.3 Related Work\n' +
      '\n' +
      'Many researchers attempt to build KBs from unstructured text through automatically identifying relational facts between entity pairs in plain text by applying machine learning techniques. Doddington et al. [31] and Hendrickx et al. [52] construct English datasets to train such models, including a relatively small number of relation classes for general domain text. Mintz et al. [91] further propose distant supervision to automatically annotate plain text by aligning it to the schema of KBs. This allows researchers to scale up the size of RE datasets [116, 147, 149, 153, 48]. Among these recent studies, TACRED [153] is the most widely used dataset, built based on the popular relation schema TAC-KBP [87] which mainly focuses on person and organization entities. Specifically, TACRED contains 106,264 examples annotated with the 42 relation classes. Yu et al. [149] also proposes a dialogue-based RE task by refining TAC-KBP to obtain 36 relation classes adapted for the dialogue domain. We also follow TAC-KBP to build the relation schema and modify them suitable for our situation.\n' +
      '\n' +
      'In the cases of languages other than English, there are only a few existing benchmarks, including one in Chinese [141], one in German [121], and one in French [55]. Nam et al. [93] propose an RE dataset in Korean using distant supervision to automatically generate and annotate examples. It however has a relatively small test set (\\(\\sim\\)3k), making it difficult to evaluate performance for the total 49 relation classes properly. Moreover, since there is no negative class (_no_relation_ in ours), it is likely to encourage models to overly predict false positives [153]. We thus consider KLUE-RE as a standard large-scale RE benchmark to properly evaluate Korean language models.\n' +
      '\n' +
      '#### 3.5.4 Conclusion\n' +
      '\n' +
      'We propose KLUE-RE, a large-scale human-annotated RE benchmark for Korean. To overcome the lack of large-scale and up-to-date Korean KBs, we design an efficient candidate collection method, coupled with an effective annotation scheme. KLUE-RE can not only be used for online information extraction but also contribute to building a large-scale knowledge graph from unstructured texts. We therefore expect KLUE-RE to be a starting point for building a large-scale, ever-growing public KB in Korean, as well as a valuable Korean NLU benchmark.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:33]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:34]\n' +
      '\n' +
      'UAS calculates macro F1 score on HEAD prediction, while LAS calculates macro F1 score on DEPREL whose HEAD prediction is correct. Both scores give the same importance to all classes. For LAS, since DEPREL distribution is highly skewed, we combine the predictions on the labels with a cumulative frequency of 1% from the bottom into a single label (OTHERS) and then calculate F1 score. The less-appeared labels are referred in Table 14.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Source** & **|Train** & **|Devl** & **|Testl** & **Total** \\\\ \\hline WIKITREE & 5,000 & 1,000 & 1,250 & 7,250 \\\\ AIRBNB & 5,000 & 1,000 & 1,250 & 7,250 \\\\ \\hline\n' +
      '**Total** & **10,000** & **2,000** & **2,500** & **14,500** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 13: Statistics for KLUE-DP.\n' +
      '\n' +
      '#### 3.6.3 Related Work\n' +
      '\n' +
      'The Penn Treebank [84] is a constituency parsed dataset created from 1989 to 1996. It has a size of about 3 million words, including IBM computer manuals, nursing notes, Wall Street Journal articles, phone conversations. A total of 48 POS tags and 18 syntax tags are used by combining meta tags such as symbols. The Penn Treebank was the best-known parsing data set before dependency parsing became prevalent. Later studies convert the Penn Treebank to dependency parsing [86, 20].\n' +
      '\n' +
      'A representative DP corpus is the Universal Dependencies(UD) dataset.40 UD is de facto standard of DP data, aiming for a unified treebank annotation in various languages. Google Universal POS [106] composed of 12 tags, was developed and a corpus was built that applied it to 25 different languages. Also de Marneffe and Manning [28] studied guidelines for dependency parsing markers used in Stanford parsers [65]. The Universal Dependency Treebank Project in 2013 attempted to combine the two studies above to have a consistent annotation system for multiple languages. UD started by modifying and supplementing this. UD first started in 2015 and Nivre et al. [97] and 10 corpora in a total of 10 languages were released through the website. As of 2021, UD (UD 2.7v) offers 104 languages and 183 corpora.\n' +
      '\n' +
      'Footnote 40: [https://universaldependencies.org](https://universaldependencies.org)\n' +
      '\n' +
      'The corpus constructed with UD scheme is made according to a certain structure called CoNLL-U format. Since UD aims to study general linguistic universality by using the same tag and annotation system in different languages, a unified format for integrating and managing each corpus is needed. The CoNLL-U format is a modified version of this CoNLL format so that it can well represent Universal dependency parsing. CoNLL-U format consists of 10 columns, each column display a word index (ID), word form (FORM), lemma of word form (LEMMA), universal part-of-speech tag (UPOS), language-specific part- It represents of-speech tag (XPOS), list of morphological features (FEATS), head of the current word (HEAD), dependency relation (DEPREL), enhanced dependency graph (DEPS), any other annotation (MISC). In this format, each word stands on a line along with different associated features (word form, lemma, POS tag, etc.) and we adopt this format in our final dataset.\n' +
      '\n' +
      'In Korean, DP corpus is divided into those that follow the UD scheme and those that do not. Among the former are The Google Korean Universal Dependency Treebank (GKT), The KAIST Korean Universal Dependency Treebank (KTB), and The Penn Korean Universal Dependency Treebank (PKT). These three datasets are converted from The Google Korean Treebank [86], The Kaist Treebank [128], and The Penn Korean treebank [47] according to the UD scheme, respectively [20]. These were first automatically converted according to the head-finding rule and then heuristically modified. These are composed of 6k, 27k, and 5k sentences, respectively, and include the genres of blog, newswire, literature, academic, and manuscript. Among them, PKT was revised by changing the analysis unit and several rules to further reveal the characteristics of Korean [99].\n' +
      '\n' +
      'Corpora that do not follow the UD scheme include the TTA DP Corpus built by Electronics and Telecommunications Research Institute (ETRI) and the Modu Corpus [98] built by the National Institute of the Korean Language (NIKL). Both corpora follow the CoNLL format, and use their own tagset, which was developed from the 21st century Sejong Plan corpus. The syntactic analysis corpus of the 21st century Sejong Plan is constructed according to the constituency grammar, following a scheme similar to the Penn Treebank. Unlike the dependency grammar, which grasps only the dominant relationship between two words, the constituency grammar identifies the relationship between words hierarchically. However, since Korean has relatively free word order, dependency parsing is more suitable than phrase-structure parsing. Studies on converting the 21st century Sejong Plan corpus into DP format have been conducted, and since then, corpora that use the 21st century Sejong Plan\'s tagset but follow dependency parsing have been constructed. The size of TTA DP corpus is about 27k, and the Modu corpus about 2000k. Unlike UD, which emphasizes general linguistic characteristics, better represents the characteristics of Korean as an individual language, and serves as a national standard for Korean DP tagging. Also, there are already corpus annotated according to the TTA scheme, so we consider compatibility between them and our benchmark. For this reason, we constructed the KLUE-DP using the TTA tagset.\n' +
      '\n' +
      '#### 3.6.4 Conclusion\n' +
      '\n' +
      'We build a Korean DP benchmark KLUE-DP consisting of formal news and informal user-generated web data. KLUE-DP is helpful for developing a DP model that can be used in multiple domains. POS tagging is performed together to improve DP performance, and the tagset and guideline for DP and POS tagging are applied by revising the existing TTA dataset. This guideline is customized to reflect the characteristics of Korean (agglutinative, free word order, etc.), and it also tackles omission of predicates in web data or errors in spacing. We hope that our benchmarks will help in the development of Korean DP models and other natural language processing.\n' +
      '\n' +
      '### Machine Reading Comprehension (MRC)\n' +
      '\n' +
      'Machine reading comprehension (MRC) is a task designed to evaluate models\' abilities to read a given text passage and then answer a question about the passage, that is, its ability of comprehension.\n' +
      '\n' +
      'Most of existing, widely-used MRC benchmarks are largely in English [21, 56, 60, 112, 113, 145, 150]. Those resources are widely used in evaluating pre-trained language models since it is one of the most intuitive methods for measuring text comprehension. SQuAD 1.1 [112] and SQuAD 2.0 [113] are popular evaluation tasks along with GLUE [30, 82, 22, 71]. BooIQ [21], ReCoRD [150], and MultiRC [60] are selected as a member of SuperGLUE for rigorous evaluation of language models. Recently, open-domain QA task which can be viewed as an MRC task without a given text passage [69, 56, 145, 38], is included in a knowledge-intensive NLP task benchmark [105].\n' +
      '\n' +
      'Motivated by those datasets, MRC has become an essential task in NLU benchmarks for various languages such as Indonesian [137], Chinese [142], and Russian [125]. In Korean, however, an appropriate MRC benchmark is not available because existing Korean MRC datasets are either less challenging, limited in access, or simply machine-translated from an English dataset [79, 1, 74]. We therefore include MRC in KLUE and create a new challenging Korean MRC benchmark (KLUE-MRC) with the following contributions:\n' +
      '\n' +
      '* **Providing multiple question types**: In order to evaluate different aspects of MRC capability of models, we provide three question types: paraphrase, multi-sentence reasoning, and unanswerable. We collect questions by following strict guidelines with specific sets of rules for each type.\n' +
      '* **Preventing reasoning shortcuts**: We prevent MRC models from exploiting reasoning shortcuts with simple word-matching by enforcing lexical and syntactic variations when workers generate questions. Also, we aim to generate questions which can be answered by considering the full query sentence.\n' +
      '* **Multiple passage domains accessible to everyone** : We include news domain passages as well as Wikipedia. To guarantee CC BY-SA license of KLUE-MRC, we made signed contracts with corresponding news providers.\n' +
      '\n' +
      'We formulate MRC as a task of predicting the answer span of the question from the given text passage. The input is a concatenated sequence of the question and the passage separated with a delimiter. The output is the start and end positions of the predicted answer span within the passage.\n' +
      '\n' +
      'We evaluate models with two metrics: 1) exact match (EM) and 2) character-level ROUGE-W. Note that character-level ROUGE-W is different from the character-level F1 score used in the previous Korean MRC datasets. If the question is unanswerable within the given passage, the model should predict the empty answer string. The motivation of our metrics are described in Section 3.7.2.\n' +
      '\n' +
      '#### 3.7.1 Dataset Construction\n' +
      '\n' +
      'Source CorporaFirst, we collect passages from Korean WIKIPEDIA and news articles provided by The Korea Economy Daily and ACROFAN. WIKIPEDIA articles are one of the most commonly used resources for creating MRC datasets. We additionally include news articles reporting contemporary social issues to enhance diversity of passages. They are provided by The Korea Economy Daily and ACROFAN. As news articles are generally copyrighted work, we sign a contract with the news providers to use and redistribute the articles under CC BY-SA license only for building a dataset for machine learning purposes. We believe multi-domain corpus can help MRC models enhance their generalizability.\n' +
      '\n' +
      'We preprocess the corpus to collect passages. For WIKIPEDIA articles, we remove duplicates in other existing Korean MRC benchmarks (e.g., KorQuAD) for precise evaluation of models. Then, we split each article by its sections to obtain passages. For the news articles, we filter out political articles and articles belonging to categories which have less than 100 articles. We finally gather all preprocessed passages whose length is longer than 512 and shorter than 2048 in characters.\n' +
      '\n' +
      'Annotation ProtocolWe annotate questions and answers by giving passages to crowdworkers. We provide a detailed tutorial session to introduce our guidelines. 60 out of 80 workers are selected after a pilot test of creating 15 question-answer pairs with a given passage. The selected workers generate questions and label corresponding answers spans (for Types 1 and 2) or fake answers spans (for Type 3). We use Tagtog annotation toolkit 41 for the annotation. We assign three inspectors for each question type to validate the generated questions and answers following common and type-specific guidelines. If the generated question-answer pair fails to pass the inspection, the worker refines it based on the feedback given by the inspector.\n' +
      '\n' +
      'Footnote 41: [https://www.tagtog.net/](https://www.tagtog.net/)\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:38]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:39]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:40]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:41]\n' +
      '\n' +
      'in the question. We exclude functional particles such as postposition(\\(\\mathcal{Z\\lambda\\dagger}\\), josa) and ending components (\\(\\circlearrowleft\\)\\(\\ucorner\\), eomi) when computing the overlap ratio via an open-sourced Korean POS tagger.44 We observe our lexical overlap ratio is almost 10%p lower than that of KorQuAD dataset (70%). For each questions types, Types 1 and 3 show similar ratio in range from 55% to 59%. Type 2 exhibits 68% overlap ratio.\n' +
      '\n' +
      'Footnote 44: Twitter tagger of KoNLPy [101].\n' +
      '\n' +
      'Human EvaluationWe evaluate human performance on our KLUE-MRC to measure its difficulty concerning human reading comprehension capabilities. We randomly sample 1,000 examples from our test set and hire three workers to solve them. We select the score of the top scoring worker as human performance. Table 21 reports the comparison between human performance and base model.\n' +
      '\n' +
      '#### 3.7.4 Related Work\n' +
      '\n' +
      'In recent years, significant progress has been achieved in English MRC research with challenging datasets of various question types, including but not limited to: paraphrase, multi-sentence, and unanswerable.\n' +
      '\n' +
      'Paraphrased questions have low word overlap between the question and reading passage, which prevents MRC models from exploiting simple word-matching. Trischler et al. [130] create a NewsQA dataset by generating questions from news headlines and summarizing them via crowdsourcing. They reduce the word overlap by annotating answers on the main articles which are not given during question generation. Saha et al. [120] leverage pairs of plot summaries for the same movies from Wikipedia and IMDb.45 They generate questions from the shorter plots and annotate answers on the longer ones to obtain naturally paraphrased questions. Sen and Saffari [122] report that datasets with low question-passage overlap will enhance the generalizability of MRC models.\n' +
      '\n' +
      'Footnote 45: [https://www.imdb.com/](https://www.imdb.com/)\n' +
      '\n' +
      'Multi-sentence questions require reasoning over multiple sentences. As a result, they are more difficult compared to single-sentence questions. Joshi et al. [56] introduce TriviaQA, a dataset of questions from trivia websites. Since they gather evidence passages from various sources (e.g., Wikipedia and the Web), multiple sentences are naturally required for answering the given question. Khashabi et al. [60] explicitly generate multi-sentence questions on various texts through crowdsourcing and release the MultiRC dataset. The SuperGLUE [132] benchmark adopts the MultiRC dataset as one of its tasks.\n' +
      '\n' +
      'Several MRC datasets have incorporated unanswerable questions [143, 130, 96, 113, 69]. Rajpurkar et al. [113] report performance drop in MRC models when unanswerable questions are included in the dataset.\n' +
      '\n' +
      'Compared to MRC research on English, Korean MRC research stands on a small number of existing datasets. The primary benchmark for Korean MRC has been KorQuAD [79, 63], which adopts the same data collection process as SQuAD 1.0 [112]. However, the model performance on KorQuAD has already exceeded human performance in a short period, leaving little headroom for further research. Moreover, unlike SQuAD, KorQuAD is under CC BY-ND license and does not allow derivative works (e.g., adding unanswerable questions). AI Hub MRC dataset [1] is based on newspapers and includes unanswerable questions. However, its access is strictly limited to native Korean researchers, prohibiting collaboration even with international researchers residing in Korea. K-QuAD [74] leverage Google Translate46 to translate SQuAD 1.0 [112] into Korean. Since the K-QuAD dataset does not get updated over\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**KLUE + KorQuAD (Full:Full)**} & \\multicolumn{2}{c}{**KLUE + KorQuAD (1:1)**} \\\\ \\cline{2-7}\n' +
      '**Evaluation Dataset** & EM & ROUGE & EM & ROUGE & EM & ROUGE \\\\ \\hline KorQuAD 1.0 (Dev) & 86.59 & 94.19 & 85.00 & 93.07 \\\\ KLUE-MRC (Test) & **70.42** & **75.42** & **69.75** & **75.20** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 20: Difficulty comparison between KLUE-MRC test and KorQuAD 1.0 dev set.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**Paraphrase**} & \\multicolumn{2}{c}{**Multi-sentence**} & \\multicolumn{2}{c}{**Unanswerable**} & \\multicolumn{2}{c}{**Total**} \\\\ \\cline{2-10}  & EM & ROUGE & EM & ROUGE & EM & ROUGE & EM & ROUGE \\\\ \\hline KLUE-RoBERTaBASE & 67.74 & 75.73 & 65.07 & 73.13 & 72.48 & 72.48 & 68.51 & 74.01 \\\\ Human & **84.18** & **88.33** & **87.72** & **90.91** & **86.53** & **86.53** & **85.90** & **88.48** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 21: Comparison of evaluation scores between model prediction and human answer time, its quality depends on the machine translator\'s performance at the time of release. Our KLUE-MRC is different from the existing Korean MRC benchmarks in terms of accessibility-enhance license and more challenging difficulty.\n' +
      '\n' +
      '#### 3.7.5 Conclusion\n' +
      '\n' +
      'We create a new challenging Korean MRC benchmark named (KLUE-MRC). In order to evaluate different aspects of MRC capabilities, KLUE-MRC includes multi-domain passages and three types of questions: paraphrase, multi-sentence reasoning, and unanswerable. KLUE-MRC shows improvements in question type diversity, difficulty, and lexical overlap compared to existing Korean MRC datasets.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:44]\n' +
      '\n' +
      'Typically, slots are categorized into informable slots and requestable slots. The informable slots cover properties which can constrain a user goal48 such as "price range", "area", and "booking day". The requestable slots provide additional information that a user may ask, but not necessarily need to be specified as a user goal constraint. A typical example of a requestable slot is "phone number", which a user may ask for, but would not work to narrow down the probable candidates of the goal [50, 51].\n' +
      '\n' +
      'Footnote 48: A user goal is what the worker playing user should follow as shown in Table 24.\n' +
      '\n' +
      'Based on this schema, we include additional attributes to the informable and requestable slots to provide an easy-to-operate annotation system and easy-to-follow guidelines. A slot could have one or more attributes among whether it is 1) boolean type, 2) required or not (_Required_), 3) related to booking (_Booking-related_), and 4) only available after booking is confirmed (_Requestable after booking_, e.g., reference number). The boolean type slots can have either _yes_ or _no_ as their values, such as "Parking (availability)" and "(has) swimming pool". Such boolean type values do not appear in the dialogue context explicitly. In other words, they have abstractive properties. A model which understands abstractive properties is desirable, so we have much more boolean type slots than MultiWOZ [10]; WoS has 20 boolean slots across the domains while MultiWOZ includes only 2. Meanwhile, the required slots have to be specified with values in order to fill out a user intent. This helps us to simulate an actual service scenario in which an agent is not allowed to take the next steps without specifying required values [114].\n' +
      '\n' +
      '## 2 Creating Knowledge Base\n' +
      '\n' +
      'We construct a knowledge base (KB) based on the task schema of each domain to obtain a set of predefined realization candidates of a user\'s goal. For the hotel and restaurant domains, we manually create virtual instances, whereas for the attraction and metro domains, we leverage real names (e.g. Gangnam Station or Namsan Tower) collected from the web. On the other hand, for the taxi domain, we do not define instances in advance, but dynamically generate the instances during the dialogue collection as in MultiWOZ [10]. With ethical considerations in mind, any personally identifiable information (PII, e.g., phone number, address) is replaced with randomly generated instances using faker.49 Table 25 shows the KB statistics for each domain.\n' +
      '\n' +
      'Footnote 49: [https://faker.readthedocs.io](https://faker.readthedocs.io)\n' +
      '\n' +
      '3. Designing the Annotation SystemIn this section, we describe the annotation platform we used for collecting data from both the User-side and System-side.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline\n' +
      '**Domains** & **Informable Slots** & **Requestable Slots** \\\\ \\hline \\multirow{4}{*}{Hotel} & name, type\\({}^{*}\\), area\\({}^{*}\\), price range\\({}^{*}\\), & \\multirow{4}{*}{rating, nearby station,} \\\\  & book day\\({}^{\\dagger}\\), book time\\({}^{\\dagger}\\), book people\\({}^{\\dagger}\\), & \\\\  & walkability\\({}^{*}\\), parking\\({}^{*}\\), internet\\({}^{*}\\), & \\\\  & breakfast\\({}^{*}\\), smoking\\({}^{*}\\), fitness\\({}^{*}\\), & \\\\  & swimming pool\\({}^{*}\\), spa\\({}^{*}\\) & \\\\ \\hline \\multirow{4}{*}{Restaurant} & name, type\\({}^{*}\\), area\\({}^{*}\\), price range\\({}^{*}\\), & \\multirow{4}{*}{rating, nearby station,} \\\\  & book day\\({}^{\\dagger}\\), book time\\({}^{\\dagger}\\), book people\\({}^{\\dagger}\\), & \\\\  & alcohol\\({}^{*}\\), walkability\\({}^{*}\\), parking\\({}^{*}\\), & \\\\  & internet\\({}^{*}\\), smoking\\({}^{*}\\), outdoor table\\({}^{*}\\) & \\\\ \\hline \\multirow{4}{*}{Attraction} & name, type\\({}^{*}\\), area\\({}^{*}\\), & \\multirow{4}{*}{rating, nearby station,} \\\\  & walkability\\({}^{*}\\), parking\\({}^{*}\\), heritage\\({}^{*}\\), & \\\\  & educational\\({}^{*}\\), scenic\\({}^{*}\\), cultural\\({}^{*}\\) & \\\\ \\hline \\multirow{2}{*}{Taxi} & leave at\\({}^{*}\\), departure\\({}^{*}\\), & \\multirow{2}{*}{phone number, cost, duration} \\\\  & arrive by, destination\\({}^{*}\\), type & \\\\ \\hline \\multirow{2}{*}{Metro} & leave at, departure\\({}^{*}\\), destination\\({}^{*}\\) & \\multirow{2}{*}{departure line, destination line,} \\\\  & leave at, departure\\({}^{*}\\), destination\\({}^{*}\\) & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 23: Task schema for all five domains in Wizard of Seoul (WoS) which shows names of the domain and their slots. Star\\({}^{*}\\), asterisk\\({}^{*}\\), cross\\({}^{\\dagger}\\), doubly-crosses\\({}^{\\ddagger}\\) denote required, boolean type, booking-related, and requestable after booking slot, respectively.\n' +
      '\n' +
      '### User Side\n' +
      '\n' +
      'We provide a goal instruction for a user side role. The instruction includes descriptions of a user\'s specific goal with corresponding slot values in natural language. It also contains the user\'s context including persona for variety of dialogues. A user is asked to generate utterances following the instruction. An example is shown Table 24.\n' +
      '\n' +
      'To devise a multi-domain dialogue scenario where slots are shared across multiple domains, we include _domain transition_ in the instruction [10, 154]. For example, in case of a user aiming to book a hotel, the user might seek information about transportation (taxi, metro, etc.) to get there. In this dialogue, initial domain is changed to another (hotel to taxi/metro). It is more challenging compared with single domain in terms of dialogue state tracking. Because user could express their goal implicitly where values should be inferred by co-referencing other values of preceding domains.\n' +
      '\n' +
      'The goal instructions are realized by the templates containing placeholders for goal constraining slots and their values. We design diverse templates for each domain, in order to cover various scenarios of the dialogues. Like MultiWOZ, the goal templates include a series of subgoals with corresponding slots. We carefully design the sentences to promote lexical entailment or co-referencing during conversation, which can be naturally observed in the user context or during the domain transition. When filling a template to complete instructions, we randomly assign the instances from KB built upon the domain-specific task schema to the given placeholders. The values of the instruction should be specifically mentioned by a user during a conversation. Each trackable slot either has valid values, _None_ or _Doncare_.50\n' +
      '\n' +
      'Footnote 50: _Doncare_ means a user has no preference and _None_ means a user is yet to specify a valid value for given slot [50, 51].\n' +
      '\n' +
      'To properly evaluate a model\'s generalization ability, we further add counterfactual goals and introduce unseen KB instances during the process [77]. To add new goal instruction based on the current slot distribution, we keep monitoring the slot value frequency and co-occurrence over slots during the construction. Specifically, we add new goal instructions which cover infrequent slot values or rarely co-occurring combination among slots. For example, when "(hotel-parking, no)" is infrequent pair in the as-is distribution, we promote it to appear in dialogues by designing goal instruction including it as constraint. Moreover, we differentiate KB instances between particular subset of dataset (train and dev/test set) to simulate realistic scenario regarding unseen slot values in the test time.\n' +
      '\n' +
      '### System Side\n' +
      '\n' +
      'The role of a system side worker (wizard) is to 1) annotate dialogue states of user utterances while 2) generate responses by accessing to the KB if necessary, for every turn. First, the wizard is asked to fill in appropriate slot values inferred from the current dialogue context. If the word uttered by the user is not clear to directly map to a\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Domain** & **\\# Instances** & **\\# Slots** \\\\ \\hline Hotel & 101 & 19 \\\\ Restaurant & 56 & 20 \\\\ Attraction & 100 & 17 \\\\ Taxi & - & 8 \\\\ Metro & 3,306 & 10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 24: An example of goal instruction. Unlike MultiWOZ, we present all instructions from the beginning to prevent ordering bias as in CoCo [77]. The booking-related slots, restaurant-book time (22:41) and restaurant-book day (Wednesday) appear before a confirmation of KB entity.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Korean** & **English (Translated)** \\\\ \\hline You have a plan to eat in the **center of Seoul at 22:41** \\\\ today. Oh, today is **Wednesday**. If you find such a place, first check the **representative menu**. Then, make a booking for **1** person. After booking, inquire about the **business hour**. Then, you have to find a hotel to sleep in **near the restaurant**. The restaurant must be **non-smoking**. If you find it, book on the **same day**. You must stay for **4** days as the **same number of people**. If the booking is done, ask for the **reference number** and double-check the **smoking allowed**. Then, finally, call a taxi. You have to go to the **hotel** from the **restaurant**. If you call the taxi, inquire about the **duration**. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 25: Statistics of Knowledge Base in WoS.\n' +
      '\n' +
      'specific slot value, the wizard should clarify the meaning of the word first and then fill the slot when the word has the same meaning as the value. The annotation of a dialogue state is an explicit action of understanding a user request to fully focus on providing the required information. Then, the wizard generates response either to request or to convey information. If the values of required slots are absent, the system side worker is allowed to ask for the missing values to the user. Otherwise, the system provides the user with the adequate information. We enable the system to query the external knowledge base, if needed. When there are more than three search results, the system worker could request more details or recommend one among them.\n' +
      '\n' +
      'To support the wizard to perform such complicate work effectively and efficiently, we provide a graphical web interface with a newly introduced feature: dropdown components (Figure 3.8.1). Dropdown interface enables the system side worker can choose a value from a list of pre-populated candidates. We present most probable value candidates based on the goal instruction and domain-specific knowledge base, since a dropdown might become worthless when too many options are presented to workers. This procedure naturally prevents several type of annotation errors, such as _multi-annotations_, _mis-annotations_, _typos_, and _value canonicalization_ reported in MutiWOZ 2.1 [37].\n' +
      '\n' +
      '4. Dataset ConstructionWe adapt \'Self-dialog\' scheme inspired by taskmaster-1 [11] to efficiently collect diverse dialogue dataset while reducing the cost and time. Self-dialog is effective to collect various dialogue data. By having both roles, a worker freely controls a flow of dialogue such as the order of slot occurrence in user utterances and recommendation of system responses. This also leads workers to speak their own styles naturally such that different personas are included. However, we found annotation errors in pilot phase such as _early-markup_ (system pre-fills the values before receiving the values from the user) and _delayed-markup_ (the system fills the values behindhand the proper turn) errors. We further improve the scheme by utilizing explicit turn-switching between user and system roles with providing error correction interface.\n' +
      '\n' +
      'To elaborate, we train and select trustworthy workers to participate in the main collection process. Prior to the main phase, we conduct several pilot studies with crowdworkers to avoid aforementioned early-markups and delayed-markup errors, and to generate more realistic dialogues including some miscommunications. We also implement an explicit turn-switching between the two roles, in order to immerse themselves in a user/system role, which mitigates overly reduced miscommunications as well. Throughout the pilot, we finally employ 15 selected workers who can effectively handle such issues.\n' +
      '\n' +
      'Figure 7: Graphical web interface for system side worker.\n' +
      '\n' +
      'Final DatasetTable 26 shows statistics of our dataset. WoS contains overall 10,000 dialogues with 146,692 turns across 5 domains. The evaluation (dev/test) set specifically includes dialogues with counterfactual goals and unseen KB instances against train set. The dev/test set contains 294 and 361 counterfactual goal-based dialogues, respectively. All the splits contain sufficient number of dialogues with domain transition.\n' +
      '\n' +
      '#### 3.8.2 Evaluation Metrics\n' +
      '\n' +
      'For evaluation metrics for WoS is 1) joint goal accuracy (JGA) and 2) slot micro F1 score. JGA measures the proportion of exactly matched dialogue state which consists of a set of slot-value pairs with the ground truth dialogue state among the total number of dialogue turns. Slot micro F1 score is an average of micro F1 scores in each turn. For each turn, micro F1 score is defined as the harmonic mean of precision and recall in terms of predicted slot-value pairs and ground-truth pairs. Note that the slot micro F1 score ignores when value of the ground truth is "None".\n' +
      '\n' +
      '#### 3.8.3 Analysis\n' +
      '\n' +
      'When splitting the train and dev/test set based on the counterfactual goals and unseen KB instances, like Li et al. [77], we observe performance drop as shown in Table 27. This demonstrates that the counterfactual goal makes WoS more challenging.\n' +
      '\n' +
      '#### 3.8.4 Related Work\n' +
      '\n' +
      'Wizard-of-Oz (WOZ) [59] is a popular scheme in dialogue collection. In fact, conventional WOZ setting allows to collect various type of dialogues by employing role-playing of two humans. Each human should choose a role between the two: _user_ and _system_. As taking a role, dialogues are collected by turn-taking generation of utterances with background information provided in advance. In the case of building a task-oriented dialogue, _goal_ is given to a _user_ while _knowledge base_ is allowed to be accessed to _system_. The _system_ can use its _knowledge base_ when responding to _user_\'s request.\n' +
      '\n' +
      'Many dialogue datasets closely follow WOZ settings [135, 35, 36], however, it costs a lot of time and money because two crowdworkers must be matched at the same time and successfully play each role, which prevents collecting dialogues at scale. We refer this limitation to \'worker coexistence constraints\'. To overcome the limitation, MultiWOZ [10] slightly change this conventional WOZ to _asynchronously_ collect dialogues turn-by-turn from crowdworkers, which allows different workers to play the same _user_ or _system_ in a single dialogue. This approach costs less but error-prone because every worker must adapt to dialogue already progressed so that their response might be incoherent to the previous context [37]. Recently, CrossWOZ and RiSAWOZ thus pair only selected trustworthy workers to collect dialogues in _synchronous_ manner as suggested in the WOZ settings to keep the annotation quality despite the high construction cost [154, 109].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & **|Train** & **|Devl** & **|Testl** & **Total** \\\\ \\hline \\# Dialogues & 8,000 & 1,000 & 1,000 & 10,000 \\\\ \\# Single Domain Dialogues & 1,806 & 263 & 226 & 2,295 \\\\ \\# Multi Domain Dialogues & 6,194 & 737 & 774 & 7,705 \\\\ \\# Counterfactual Dialogues & 0 & 294 & 361 & 655 \\\\ \\hline \\# Total Turns & 117,584 & 14,448 & 14,660 & 146,692 \\\\ \\# Total Tokens & 899,450 & 114,169 & 114,914 & 1,128,533 \\\\ \\hline Avg Turns per Dialogue & 14.70 & 14.45 & 14.66 & 14.67 \\\\ Avg Tokens per Turn & 7.65 & 7.90 & 7.84 & 7.69 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 26: Statistics of Wizard-of-Seoul (WoS).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Domain Split** & **Joint Goal Accuracy** \\\\ \\hline Random & 57.53 \\\\ CF-goal & 47.38 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 27: Comparison regarding data split strategy. Random is splitting train, dev and test set randomly. The CF-goal indicates they are included in dev and test set with unseen KB instances.\n' +
      '\n' +
      'Byrne et al. [11] also argue that conventional WOZ settings are time consuming, complex and expensive, requiring considerable technical implementation as well as administrative procedures to train and manage both agents and crowdsourced workers, accordingly suggesting Self-dialog as an alternative. Self-dialog is a collection scheme in which workers write the entire dialogue playing both user and system roles. To demonstrate their idea, Byrne et al. [11] build a large-scale dialogue dataset, Taskmaster-1, which is built upon the Self-dialog which stands on the WOZ schemes: 1) two people playing user and system roles (conventional WOZ setting) and 2) one person playing both roles (Self-dialog). As a result, Self-dialog can remedy the cost of \'worker coexistence constraint\' effectively with avoiding incoherent dialogue generation caused by asynchronous dialogue collection. However, there is a tendency that little miscommunication occurs in the dialogue which compared to the real world conversations because the same person produces utterances in both roles, which might lead to a gap from reality.\n' +
      '\n' +
      'Some researchers further tries to employ only machines to create such dialogues to maximize cost-efficiency. It builds the dialogues on top of a simulator which is able to create utterances by turn automatically from elaborately designed rules and given task schema. The simulator first generates psuedo-dialogue, then adopts crowdsourcing to paraphrase them to the natural utterances [124, 114]. It requires much less human effort, however, heavily relies on the simulator.\n' +
      '\n' +
      'Meanwhile, previous works exists addressing robust evaluation of DST models. According to CoCo [77], state-of-the-art DST models are not robust to realistic scenarios since they scarcely appear in the train data. As the name CoCo (controllable counterfactuals) suggests, it generates infrequent but realistic dialogues based on the predefined slot-value pairs. They show that even a state-of-the-art DST model\'s performance drops significantly when they are evaluated on such dialogues including counterfactual goals. It means the current TOD benchmarks should be improved in terms of robustness to unseen but realistic scenarios.\n' +
      '\n' +
      'As for Korean, there is a task-oriented dialog dataset is provided by National Information Society Agency (NIA). It covers about 10 domains related to the civil complaints and consists of more than 500k dialogues. The utterances are divided into four types: 1) a main question that a user asks, 2) a sub question that a system could ask for clarification, 3) a user answer, and 4) a system answer. Additionally, user intents are annotated and entities are extracted from each utterance. We find that this dataset does not follow any of the aforementioned settings; there is no dialogue state represented as slot-value pairs, only regarding single turn judgement. It also lacks information about task schema and redistribution is restricted which does not satisfy our accessibility principle, which motivates us to newly create a DST benchmark.\n' +
      '\n' +
      '#### 3.8.5 Conclusion\n' +
      '\n' +
      'We introduce Wizard-of-Seoul (WoS), the first large-scale Korean multi-domain task-oriented dialogue dataset that simulates conversations between Seoul tourists and travel agents. We adapt \'Self-dialog\' for efficiently scaling up of dialogue collection scheme. In addition, consideration on annotation interfaces (drop-down menu and turn-switching) mitigates erroneous cases and diverse goal instructions including counterfactual ones promote each conversation to be more natural and challenging. We hope that WoS sparks various future dialogue research in Korean and also offers valuable insights to pushing forward end-to-end dialogue modeling.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:50]\n' +
      '\n' +
      'We do not filter out socially biased contents nor hate speech for three reasons. First, manual inspection is infeasible for this large-scale pretraining corpora. Second, it is a challenging problem on its own to automatically detect socially biased contents or hate space, as both of these highly depend on the context in which they appear [62]. Lastly, being blind to such harmful contents prevents the future use of a language model for detecting and correcting these harmful contents, such as using it as an anti-expert [81]. We expect future research on the pretrained language models we release to focus on how to detect and correct biases encoded in these models and on how to debias them, as has been recently demonstrated by Cheng et al. [15].\n' +
      '\n' +
      'In contrast, we pseudonymize PII in our corpora as much as possible. We detect 16 personal data types using regular expressions based on the guideline from the Korea Internet and Security Agency (KISA).58 It is relatively easy to pseudonymize PII while keeping linguistic patterns, since the selected PII has standardized pattern. We then replace the original information, using either the faker library59 or random generation based on the pattern. As a result, we pseudonymize 1.2% of the pretraining corpora. Details are illustrated in Table 29.\n' +
      '\n' +
      'Footnote 58: [https://www.kisa.or.kr/public/laws/laws2_View.jsp?cPage=1&mode=view&p_No=282&b_No=282&d_No=3](https://www.kisa.or.kr/public/laws/laws2_View.jsp?cPage=1&mode=view&p_No=282&b_No=282&d_No=3)\n' +
      '\n' +
      'Footnote 59: [https://github.com/joke2k/faker](https://github.com/joke2k/faker)\n' +
      '\n' +
      'TokenizationWe design and use a new tokenization method, _morpheme-based subword_ tokenization. When building a vocabulary, we pre-tokenize a raw text into morphemes using a morphological analyzer, and then we apply byte pair encoding (BPE) [123] to get the final vocabulary. For morpheme segmentation, we use Mecab-ko,60 McCab [68] adapted for Korean, and for BPE segmentation, we use the wordpiece tokenizer from Huggingface Tokenizers library.61 We specify the vocabulary size to 32k. After building the vocabulary, we only use the BPE model during inference, which allows us to tokenize a word sequence by reflecting morphemes without a morphological analyzer. This improves both usability and speed. Examples are presented in Table 30.\n' +
      '\n' +
      'Footnote 60: [https://bitbucket.org/eunjeon/mecab-ko](https://bitbucket.org/eunjeon/mecab-ko)\n' +
      '\n' +
      'Footnote 61: [https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)\n' +
      '\n' +
      'The motivation behind this method is that Korean is an agglutinative language, which is to say, a word is a constitution of morphemes - stems and affixes. The morphemes tend to remain unchange on different unions, and the boundary is generally clear. Although BPE has been widely used across many languages due to its effectiveness, it struggles to identify morphemes correctly as demonstrated in Table 30.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline\n' +
      '**Private Information** & **Pseudonymization** & **Pseudonymised Example** \\\\ \\hline\n' +
      '**Telephone Number** & Faker & 055-604-8764 \\\\\n' +
      '**Social Security Number** & Faker & 600408-2764759 \\\\\n' +
      '**Foreign Registration Number** & Faker & 110527-1815659 \\\\\n' +
      '**Email Address** & Faker & agweon@example.org \\\\\n' +
      '**IP Address** & Faker & 166.186.169.69 \\\\\n' +
      '**MAC Address** & Faker & c5:d7:14:84:f8:cf \\\\\n' +
      '**Mention(@)** & Faker & @gildong \\\\\n' +
      '**Address** & Random Number Generation & 110-245-124678 \\\\\n' +
      '**Bank Account Number** & Random Number Generation & 110-245-124678 \\\\\n' +
      '**Passport Number** & Random Generation & M123A4567 \\\\\n' +
      '**Drivers License** & Random Number Generation & 11-17-174133-01 \\\\\n' +
      '**Business Registration Number** & Random Number Generation & 123-45-67890 \\\\\n' +
      '**Health Insurance Information** & Random Number Generation & 1-2345678901 \\\\\n' +
      '**Credit or Debit Card Number** & Random Number Generation & 1234-5678-9012-3456 \\\\\n' +
      '**Vehicle Registration Place** & Random Generation & 55-1601 \\\\\n' +
      '**Homepage URL** & Random Generation & www.example.com \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 29: Our pseudonymization methods and examples. The examples are from faker library documentation or the public.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:52]\n' +
      '\n' +
      'use [CLS] as the first input token \\(x_{0}\\) and [SEP] as a delimiter token to separate inputs (e.g., two sentences in STS and NLI, a passage and a question in the case of MRC, and dialogue turns for DST.)\n' +
      '\n' +
      '#### 5.1.1 Single Sentence Classification\n' +
      '\n' +
      'In the single sentence classification task, such as TC and RE, a classifier classifies a single sentence into a set of predefined labels. Following the convention, the last hidden state of [CLS] token \\(h_{0}\\) is linearly mapped to the number of labels (\\(K\\)) with \\(W\\in\\mathbb{R}^{K\\times H}\\) and the entire model is trained to minimize the cross entropy loss.\n' +
      '\n' +
      '**YNAT** is a single sentence classification task where \\(K\\) is 7 for predefined topic labels, and does not require any special treatment of each input. **KLUE-RE** on the other hand requires a special procedure to indicate entities within the input sentence. We use <subj>, </subj>, <obj>, and </obj> to mark the beginnings and the ends of subject and object entities, respectively, following Baldini Soares et al. [5]. We expand the embedding matrix to add these four extra tokens.\n' +
      '\n' +
      '#### 5.1.2 Sentence Pair Classification / Regression\n' +
      '\n' +
      'In the sentence pair classification / regression task, a model is asked to determine the relationship between two sentences. A pair of input sentences are concatenated with a special separator token, often [SEP], in-between.\n' +
      '\n' +
      'In **KLUE-STS**, each sentence pair is annotated with a real-valued similarity \\([0,5]\\). The model is thus trained to map from the final hidden state of [CLS] to a real number, by minimizing the mean squared error (MSE). In the case of **KLUE-NLI**, each sentence pair, consisting of a premise and hypothesis, is coupled with one of three classes. The model thus maps the hidden state of [CLS] token to a three-dimensional real-valued vectors and is trained to minimize the cross-entropy loss.\n' +
      '\n' +
      '#### 5.1.3 Multiple-Sentence Slot-Value Prediction\n' +
      '\n' +
      '**WoS** is a slot-value prediction task for a given dialogue context, where the prediction should be considered across multiple turns instead of a single utterance. We employ an encoder-decoder model following the architecture of TRADE [140], which consists of an utterance encoder, a state generator, and a slot gate classifier (Figure 5.1.3). In our implementation, we change the utterance encoder from GRU [17] to PLM to get better representations. Thus, the state generator takes the final hidden state of [CLS] token \\(h_{0}\\) as the first decoder hidden state. We also modify the slot gate classifier to predict additional two slot gate labels (_yes_, _no_), since WoS contains relatively more Boolean type slots than MultiWOZ [10]. We jointly minimize the cross-entropy loss of the state generator and slot gate classifier.\n' +
      '\n' +
      'Figure 8: A baseline architecture for WoS based on TRADE [140].\n' +
      '\n' +
      '#### 5.1.4 Sequence Tagging\n' +
      '\n' +
      '**KLUE-NER** is a token-level tagging task, where each character is assigned a label. This requires a care in using tokenization, as the labels from the characters within each subword token must be aggregated, and the predicted label of each subword token must be properly distributed across the characters within it. See Figure 9 for an example. We linearly map each of the final hidden states from the encoder \\(h\\in\\mathbb{R}^{|x|\\times H}\\) into a 12-dimensional real-valued vectors, corresponding to the 12 named-entity categories. We then minimize the cross-entropy loss summed over all the tokens.\n' +
      '\n' +
      '**KLUE-MRC** is a span prediction task in which a model tags the beginning and end tokens of the answer span within a passage, given a question. The input to the model is the concatenation of a tokenized passage and an associated question (separated by [SEP]). The final hidden state of each token in the passage is linearly projected to a 2-dimensional real-valued vector. The dimensions in this vector correspond to the logits of two binary classifiers, the start and end token classifiers. The special token [CLS] is considered as both the correct start and end tokens, when given question is unanswerable. We minimize the cross-entropy loss to train the model.\n' +
      '\n' +
      'We frame **KLUE-DP** as a sequence tagging problem. Each token within an input sentence is tagged twice, once with its head token and the other with the type of the arc connecting the head and the current token. Our baseline architecture follows the model proposed by Fernandez-Gonzalez and Gomez-Rodriguez [39], except word representation and attention mechanisms. Similarly to KLUE-NER, we must be careful in handling subword tokens, as the annotation is done at the word level. In our approach, we use a pretrained language model (to be fine-tuned) to extract subword representations and concatenate the first and last subword token representations of each word, to form word vector representations. Each of these word representations is optionally concatenated with the part-of-speech embedding. For the attention layers, we use biaffine attention [33] to predict HEAD, and bilinear attention [64] to predict arc type (DEPREL) for each word. Just like KLUE-NER and KLUE-MRC, we minimize the cross-entropy loss to fine-tune the entire model. See Figure 10 for a graphical illustration of the model architecture.\n' +
      '\n' +
      'Figure 10: An overview of KLUE-DP baseline model architecture, which we take advantage of Fernndez-Gonzlez and Gmez-Rodriguez [39].\n' +
      '\n' +
      'Figure 9: An input and label example of KLUE-NER. We realign original character-level label sequence of Figure 3 for tokens from our Morpheme-based subword tokenization.\n' +
      '\n' +
      '### Fine-Tuning Configurations\n' +
      '\n' +
      'For all the experiments, we use Huggingface Transformers[139] and PyTorch-Lightning.62 We use AdamW optimizer [83] with the learning rate selected from \\(\\{10^{-5},2\\times 10^{-5},3\\times 10^{-5},5\\times 10^{-5}\\}\\), the warm-up ratio from \\(\\{0.,0.1,0.2,0.6\\}\\) and the weight decay coefficient from \\(\\{0.0,0.01\\}\\). We choose the batch size from \\(\\{8,16,32\\}\\) and the number of epochs from \\(\\{3,4,5,10\\}\\). We use the maximum sequence length of 512 for KLUE-MRC and WoS, and 128 for all the other tasks. We report the score obtained from the best hyperparameter configuration based on the dev set performance.\n' +
      '\n' +
      'Footnote 62: [https://github.com/PyTorchLightning/pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning)\n' +
      '\n' +
      '### Evaluation Results\n' +
      '\n' +
      'In this section, we present the evaluation results including our KLUE-PLMs and existing PLMs on the KLUE benchmark, in Table 32.63 Different from other NLU benchmarks, we do not average the scores over tasks, since simple averaging of scores of different scales and interpretations could be highly misleading. Rather, we describe and discuss the result of each task separately. Within the Korean BASE models, KLUE-BERTBASE performs best for YNAT and WoS, KLUE-RoBERTBASE for KLUE-RE and KLUE-MRC, and KoELECTRABASE for KLUE-STS and KLUE-NLI.\n' +
      '\n' +
      'Footnote 63: See Appendix A for the corresponding table however computed on the development set.\n' +
      '\n' +
      'We make two major observations. First, we see that KLUE-RoBERTBASE, which is the largest model among the baseline PLM\'s we tested, outperforms all the other models across all the tasks except for KLUE-NER. This observation agrees well with the recent trend which has demonstrated the correlation between the model size and task performance [58; 9]. This indicates that KLUE will be useful for the future investigation into how much gain we can expect by simply increasing the model size further. The second observation is that the monolingual models, which were specifically designed for and trained with a more carefully curated corpus in the target language (Korean), generally outperform the multilingual counterparts, especially when we compare models of similar sizes. We make this observation again across all the tasks, except for KLUE-NER, where the XLM-RLARGE performs similarly to the best performer, KoELECTRABASE, in terms of character-level F1 score. This observation re-iterates the importance of investing effort in understanding a target language and customizing data, models and learning algorithms for the target language.\n' +
      '\n' +
      '### Analysis of Models\n' +
      '\n' +
      'There were two major decisions we made in preparing the pretraining corpus and preprocessing data. They were 1) whether to pseudonymize PIIs and 2) the tokenzation strategy. In this section, we analyze the impact of our choices, using KLUE-RoBERTBASE by training it on the MODU corpus only.\n' +
      '\n' +
      'Corpus PseudonymizationIt can be expected that noise introduced in the process of pseudonymization may have detrimental effect on the downstream task performance. Our finding, presented in Table 33, however shows that there is some drop in a subset of the tasks, but such drop is quite minimal. This suggests that the minimal level of pseudonymization, just like what we have done, is already a good way to balance the task performance and the risk of leaking private information.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c c} \\hline \\hline  & **YNAT** & **KLUE-STS** & **KLUE-NLI** & **KLUE-NER** & **KLUE-RE** & **KLUE-DP** & **KLUE-MRC** & **WoS** \\\\ \\cline{2-13}\n' +
      '**Model** & F1 & R\\({}^{P}\\) & F1 & ACC & F1\\({}^{E}\\) & F1\\({}^{C}\\) & F1\\({}^{mic}\\) & AUC & UAS & LAS & EM & ROUGE & JGA & F1\\({}^{S}\\) \\\\ \\hline\n' +
      '**mBERTBASE** & 81.55 & 84.66 & 76.00 & 73.20 & 76.50 & 89.23 & 57.88 & 53.82 & 90.30 & 86.66 & 44.66 & 55.92 & 35.46 & 88.63 \\\\\n' +
      '**XLM-RBASE** & 83.52 & 89.16 & 82.01 & 77.33 & 80.37 & 92.12 & 57.46 & 54.98 & 89.20 & 87.69 & 27.48 & 53.93 & 39.82 & 89.61 \\\\\n' +
      '**XLM-R\\({}_{\\text{LACE}}\\)** & **86.96** & 92.97 & 85.86 & 85.93 & 82.27 & **93.22** & 58.39 & 61.15 & 92.71 & **88.70** & 35.99 & 66.77 & 41.20 & 89.80 \\\\ \\hline\n' +
      '**KR-BERBASE** & 84.58 & 88.61 & 81.07 & 77.17 & 74.58 & 90.13 & 62.74 & 60.94 & 89.92 & 87.48 & 42.82 & 58.54 & 45.33 & 90.70 \\\\\n' +
      '**KoELECTRABASE** & 84.59 & 92.46 & 84.84 & 85.63 & **86.11** & 92.56 & 62.85 & 58.94 & 92.90 & 87.77 & 59.82 & 66.05 & 41.58 & 89.60 \\\\ \\hline\n' +
      '**KLUE-BERTBASE** & 85.73 & 90.85 & 82.84 & 81.63 & 83.97 & 91.39 & 66.44 & 66.17 & 89.96 & 88.05 & 62.32 & 68.51 & 46.64 & 91.61 \\\\\n' +
      '**KLUE-RoBERTBASE** & 84.98 & 91.54 & 85.16 & 79.33 & 83.65 & 91.14 & 60.89 & 58.96 & 90.04 & 88.14 & 57.32 & 62.70 & 46.62 & 91.44 \\\\\n' +
      '**KLUE-RoBERTBASE** & 85.07 & 92.50 & 85.40 & 84.83 & 84.60 & 91.44 & 67.65 & 68.55 & 93.04 & 88.32 & 68.67 & 73.28 & 47.49 & 91.64 \\\\\n' +
      '**KLUE-RoBERTBASE** & 85.69 & **93.35** & **86.63** & **89.17** & 85.00 & 91.86 & **71.13** & **72.98** & **93.48** & 88.36 & **75.58** & **80.59** & **50.22** & **92.23** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 32: Evaluation results of our pretrained LMs and other baselines on KLUE benchmark. The F1 refers to a macro-F1 score. The F1\\({}^{E}\\) and F1\\({}^{C}\\) of KLUE-NER indicates entity-level and character-level macro-F1 score, respectively. The F1\\({}^{mic}\\) of KLUE-RE is micro-averaged F1 score ignoring the _no_relation_. The F1\\({}^{S}\\) of WoS is an average of slot-value pair level micro-F1 scores. The R\\({}^{P}\\) of KLUE-STS denotes Pearson correlation. **Bold** shows the best performance across the models, and underline indicates the best performance among BASE models.\n' +
      '\n' +
      '#### 4.4.2 Tokenization Strategy\n' +
      '\n' +
      'We contrast our tokenization scheme, _morpheme-based subword_ tokenization, against the standard byte pair encoding (BPE). First, we investigate the difference in how words are segmented into subword tokens. Following Rust et al. [119], we consider subword fertility, proportion of continued words, and UNK ratio. On the subword fertility, which measures the average number of subwords produced per word, the proposed tokenization scheme ends up slightly higher than BPE does. However, when we look at the proportion of continued words, which measures the number of words that were split into at least two subwords, we observe the opposite trend. This implies that our algorithm maintains the original words as much as it can, and only when it is necessary, it splits each word into potentially more subword pieces. The efficacy of the proposed scheme over BPE is evident from the UNK ratio, as it produces fewer UNK tokens compared to BPE when the vocabulary size was controlled to be 32k for both methods. See Table 35.\n' +
      '\n' +
      'We find these qualitative differences between two schemes lead to significant differences in the task performance in the cases of KLUE-NER, KLUE-MRC and WoS. These tasks often involve tagging, detection and even generation at the morpheme level, and we suspect that morphologically consistent tokenization facilitates better prediction overall. On the other hand, the difference in the tokenization strategy does not manifest itself in the performance of classification or word-level tagging, likely as a corresponding NLU system can more readily overcome inconsistencies in subword segmentation when merging subword token representations into that of a larger unit. Overall, we recommend future researchers use the proposed tokenization strategy as a default option.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c} \\hline \\hline  & **YNAT** & **KLUE-STS** & **KLUE-NLI** & **KLUE-NER** & **KLUE-RE** & **KLUE-DP** & **KLUE-MRC** & \\multicolumn{2}{c}{**WoS**} \\\\ \\cline{2-13}\n' +
      '**Tokenization** & F1 & R\\({}^{P}\\) & F1 & ACC & F1\\({}^{E}\\) & F1\\({}^{C}\\) & F1\\({}^{mic}\\) & AUC & UAS & LAS & EM & ROUGE & JGA & F1\\({}^{S}\\) \\\\ \\hline BPE & **83.40** & 91.91 & **85.19** & **82.07** & 68.75 & 89.47 & 64.39 & **65.04** & 89.89 & **89.47** & 51.12 & 65.79 & 21.38 & 77.68 \\\\ Morpheme-based Subword & **83.40** & **92.06** & 84.70 & 81.60 & **84.84** & **91.03** & **65.25** & 64.79 & **92.17** & 88.34 & **62.13** & **67.46** & **47.14** & **91.60** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 34: Comparison our tokenization strategy with other baselines.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c c} \\hline \\hline  & **YNAT** & **KLUE-STS** & **KLUE-NLI** & **KLUE-NER** & **KLUE-RE** & **KLUE-DP** & **KLUE-MRC** & \\multicolumn{2}{c}{**WoS**} \\\\ \\cline{2-13}\n' +
      '**Pretraining Corpus** & F1 & R\\({}^{P}\\) & F1 & ACC & F1\\({}^{E}\\) & F1\\({}^{C}\\) & F1\\({}^{mic}\\) & AUC & UAS & LAS & EM & ROUGE & JGA & F1\\({}^{S}\\) \\\\ \\hline Original & **83.40** & **92.06** & **84.70** & **81.60** & 84.84 & 91.03 & **65.25** & **64.79** & **92.17** & **88.34** & 62.13 & 67.46 & **47.14** & **91.60** \\\\ Pseudonymized & 83.39 & 91.11 & 82.85 & 78.50 & **84.99** & **91.22** & 62.79 & 62.96 & 92.02 & 88.02 & **62.88** & **67.58** & 46.21 & 91.23 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 33: Evaluation results according to whether the corpus pseudonymization is conducted in the preprocessing step.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Tokenization** & **\\# Vocabs** & **Fertility \\(\\downarrow\\)** & **\\% Continued Word \\(\\uparrow\\)** & **UNK Ratio \\(\\downarrow\\)** \\\\ \\hline BPE & 32k & **2.073** & 0.578 & 0.011 \\\\ Morpheme-based Subword & 32k & 2.468 & **0.765** & **0.009** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 35: Overview of tokenization metrics. We build each vocabs using MODU corpus and compare them on WIKIPEDIA corpus.\n' +
      '\n' +
      'Ethical Considerations\n' +
      '\n' +
      'In building KLUE and accompanying baseline models, we have incorporated various mechanisms to avoid any harmful and negative consequences from releasing both data and models. These mechanisms are described in detail wherever they were introduced and used, but in this section, we summarize these mechanisms, considerations and our principles behind them.\n' +
      '\n' +
      '### Copyright and Accessibility\n' +
      '\n' +
      'Most NLP datasets are built upon the existing text sources. This raises a question on the terms of using such datasets, especially when the underlying source datasets are not well-specified nor carefully investigated. In order to avoid any such doubt on the terms of using KLUE and to accelerate NLP research in Korean, we fully adhere to the copyright act of Korea, which went effective on Dec. 8, 2020.64 and include only text for which we know we can release under a license that permits both redistribution and re-mix without any restriction on the use.\n' +
      '\n' +
      'Footnote 64: [https://www.law.go.kr/%EB%2%95%EB%AO%B9/%EC%AO%80%EC%9E%9E%91%EA%B6%8C%EB%B2%95](https://www.law.go.kr/%EB%2%95%EB%AO%B9/%EC%AO%80%EC%9E%9E%91%EA%B6%8C%EB%B2%95)\n' +
      '\n' +
      'Source CorporaOur goal is to secure and maximize the continued availability and usefulness of the benchmark. In other words, we must guarantee the possibility of derive new work and redistribute it freely, which comes together with CC BY-SA. To release KLUE under CC BY-SA, we have built a source corpus set by including only text that is either 1) not protected by copyright or 2) under CC0, CC BY, CC BY-SA or KOGL Type 1 license. In the case of news articles, which are copyrighted, we have signed contracts with the providers, Korean Economics Daily (KED) news media and Acrofan, that allow us to make KLUE-MRC and release them under CC BY-SA.\n' +
      '\n' +
      'Task-Specific (Annotated) DatasetsWe subsample and annotate the source corpus for each KLUE benchmark task. We release each under CC BY-SA. This allows users of KLUE benchamrk to copy, redistribute, remix, transform and build upon it for both commercial and non-commercial purposes, as long as derivatives are distributed under the same license (CC BY-SA). We expect this to greatly facilitate future NLP research and development.\n' +
      '\n' +
      'Pretraining Corpora and Language ModelsAs was discussed earlier 4.1, we cannot guarantee that our pretraining corpus, built using MODU, CC-100-Kor and NEWSCRAWL, does not contain any copyrighted work, although these are all created from publicly available text. Unfortunately without these corpora, it is not possible to find a sufficiently large resource to train large-scale language models for Korean. We thus use them for pretraining but do not publicly release the pretraining corpora in order to avoid any issues in the future, which is in contrast to KLUE. Instead, we openly release pretrained language models to facilitate future research. As the parameters of a language model does not _[express] human thoughts and emotions_, they do not meet the requirement of being copyrighted.\n' +
      '\n' +
      '### Toxic Content\n' +
      '\n' +
      'Although large-scale, accessible benchmark datasets advance machine learning and its applications to adjacent fields, such as natural language processing, toxic and unwanted contents within these datasets may be amplified via large-scale models we train on them. We have been aware of this issue from the beginning of the project, and here we describe how we have addressed these toxic contents in KLUE.\n' +
      '\n' +
      'Task-Specific DatasetsFor each task-specific dataset, we apply three stages to minimize the introduction of toxic contents. First, we automatically detect hate speech and gender-biased sentences using toxicity classifiers and remove those even before sending these sentences for annotation (see Section 2.3). Second, we explicitly and clearly instruct annotators to mark any instance that exhibits social biases and/or is toxic (see Section 3), after providing them with clear definitions of bias and hate speech. Finally, we manually examine these marked sentences and exclude them from the final dataset. This three-stage process may not catch all possible such instances, and we plan to use an online forum65 to receive feedback and complaints from users of KLUE.\n' +
      '\n' +
      'Footnote 65: [https://github.com/KLUE-benchmark/KLUE/issues](https://github.com/KLUE-benchmark/KLUE/issues)\n' +
      '\n' +
      'Pretrained Language ModelsWe use our pretraining corpora (see Section 4.1) as they are, for three reasons. First, manual inspection is simply not tractable due to the sheer scale. Second, it is challenging to build an automated tool to detect hate speech and biased sentences. This issue is made even more severe for Korean, because there is only one known hate speech dataset of limited size [92]. Lastly, we envision the future in which these pretrained language models are used to build better tools for automatically detecting various toxic contents as well as undesirable social biases. Inorder for such pretrained models to be aware of these issues, they must have been trained with such toxic contents as well.\n' +
      '\n' +
      '### Personally Identifiable Information\n' +
      '\n' +
      'It has recently been discovered by that a large-scale, pretrained language model memorizes a large amount of personally identifiable information (PII) and that an algorithm can be designed to retrieved those private information. We thus design two different approaches for pseudonymizing task-specific datasets and pretraining corpora, respectively.\n' +
      '\n' +
      'Task-Specific DatasetsIn the case of task-specific datasets, we rely on manual inspection during annotation to detect PII. We discard any sentences that was reported to contain PII after manual inspection. In the case of DST, which relies on simulated dialogues, we pseudonymize the database entries rather than actual text, using the faker library.66\n' +
      '\n' +
      'Footnote 66: [https://github.com/joke2k/faker](https://github.com/joke2k/faker)\n' +
      '\n' +
      'Pretraining CorporaThere is a trade-off between removing PII and the performance of a pretrained language model, as we will demonstrate later in this paper. We thus pseudonymize 16 PII types that are detectable purely by regular expressions. See Section 4.1\n' +
      '\n' +
      '## 7 Related Work\n' +
      '\n' +
      'General-Purpose NLU BenchmarksGeneral Language Understanding Evaluation (GLUE) [133] benchmark, a collection of evaluation dataset for English, was the first general-purpose evaluation benchmarks for NLU. It is general-purpose in that it is not limited to a single task. It consists of 11 downstream tasks, including tasks that measures the capability of capturing semantic textual similarity (QQP, MRPC, STS) [3, 13], measures the capability of inference (MNLI, QNLI, RTE, WNLI) [8, 138] and that evaluates the capability of classifying a single sentence into a predefined set of categories (CoLA, SST) [134, 126]. GLUE exclusively focused on English, and its variants in different languages have been built and released over the past couple of years, including CLUE in Chinese [142], a French version [72], an Indonesian version [137], a version for Indic languages [57], Russian SuperGLUE [125], and Persian GLUE [61]. In all these cases, substantial efforts were carried out to follow the philosophy of the original GLUE, covering a broad spectrum of domains and tasks, while incorporating language-specific characteristics. On the other hand, there have been efforts to build a multilingual version of such benchmark, largely relying on automated methods, such as XGLUE [78] and XTREME [54]. Korean as a language has been included in subsets of these latter benchmarks, but there has not been a serious attempt at building a general-purpose language understanding evaluation suite for Korean, until this paper.\n' +
      '\n' +
      'Absence of a Standard NLU Benchmark in KoreanUntil this paper, a number of task-specific benchmarks in Korean have been proposed and released. For example, NSMC is used for sentiment classification, PAWS-X [144] for paraphrase detection, KorNLI and KorSTS [45] for NLI and STS, KorQuAD 1 and 2 [79, 63] for MRC, and BEEP! [92] for hate speech detection. At this point, one may wonder whether it would have been easier and more convenient to simply aggregate these datasets to build KLUE. After all, this has been a popular strategy for constructing monolingual [133] as well as multilingual [78, 54] benchmarks. Unfortunately this approach comes with two major issues that we directly address in this paper.\n' +
      '\n' +
      'First, the existing datasets are constructed individually without considering other datasets and their properties. In other words, the aggregate of these individual datasets is unlikely to cover a broad spectrum of domains and writing styles, unlike KLUE for which we carefully curate the source corpora as well as subsets for downstream tasks to have broad coverage of domains and styles. This goes beyond domains and styles, but also the coverage of linguistic phenomena under evaluation. Most of the existing benchmarks, listed above, focus on semantics rather than syntax, and it is difficult to find any widely-available benchmark that captures pragmatics. We address this issue by carefully selecting a set of downstream tasks.\n' +
      '\n' +
      'Second, these existing datasets are not always publicly available,67 and some are distributed with a highly restrictive license that prohibits redistribution nor the transformation of the original. These are often the ones published and released by government-affiliated institutes. In some cases, it is necessary to obtain a special permission to access datasets, which is often not easily accessible by non-Korean researchers. We address all these issues with KLUE by relaseing the entire benchmark data under CC BY-SA, both by careful curation of source corpora and by direct agreements with publishers.\n' +
      '\n' +
      'Footnote 67: Some of these are publicly available in Korea but not internationally.\n' +
      '\n' +
      'Pretrained Language Models (PLMs)The recent trend of large-scale pretrained language models was sparked by the success of earlier models, such as ELMo [104], GPT-2 [110] and BERT [30], on the GLUE and other similar NLU benchmarks. This earlier success has led to a series of advances in large-scale language models, including XLNet [146], ALBERT [71], RoBERTa [82], ELECTRA [22], and Deberta [49], again largely driven by the availability of standardized benchmarks. This advance in language models, not only in terms of the model size but also in learning algorithms, in turn also sparked the interest in building and improving existing language understanding benchmarks. Some of the recently released, challenging benchmarks include SuperGLUE [132] and KILT [105]. The availability of such a standard language understanding benchmark, such as KLUE from this paper, is expected to start such a virtuous cycle for Korean language understanding.\n' +
      '\n' +
      'Pretrained Language Models for KoreanInspired by the development in other languages and multilingual models, PLM\'s for the Korean language have been trained and released by multiple research groups and individuals. SKT released KoBERT,68 followed by KorBERT69 from ETRI, HanBERT70 from TwoBlock AI, KR-BERT [75] from Seoul National University. There are a few pretrained models released by individual researchers, such as KoELECTRA [102] and KcBERT [73].\n' +
      '\n' +
      'Footnote 68: [https://github.com/SKTBrain/KoBERT](https://github.com/SKTBrain/KoBERT)\n' +
      '\n' +
      'Footnote 69: [https://aiopen.etri.re.kr/service_dataset.php](https://aiopen.etri.re.kr/service_dataset.php)\n' +
      '\n' +
      'Footnote 70: [https://github.com/tbai2019/HanBert-54k-N](https://github.com/tbai2019/HanBert-54k-N)\n' +
      '\n' +
      'Unfortunately, it is unclear how we should compare this stream of pretrained language models in Korean, due to the lack of a standardized benchmark in Korean. Subsets of these models have been compared based on subsets of a few downtream NLP tasks in Korean above, but because these are not standardized, it is not easy to draw solid conclusions from these limited experiments. We expect the proposed KLUE benchmark will serve as a standard way to track the progress of research in language models for Korean.\n' +
      '\n' +
      '## 8 Discussion\n' +
      '\n' +
      'Open AccessWe distribute KLUE under CC BY-SA. The license allows everyone to freely copy and redistribute our benchmarks in any medium or format. In addition, one can improve our benchmark to build more challenging datasets after performance saturation. To function as a NLU _benchmark_, open access is a must. If the original author does not allow derivative development of the benchmark, other researchers cannot improve it, for example by removing toxic content, or building a more challenging dataset to accelerate research for technical improvements. If commercial use is not allowed, researchers working at for-profit organizations would not be able to benefit from nor to (easily) contribute to the benchmark. Redistribution is another crucial factor because it significantly limits research if, for example, sharing the datasets with another researcher is prohibited. Another existing practice that limits research is transferring the responsibility of copyright infringement of related conflicts to researchers. To set a good precedent for open access of data, we allow using our datasets for 1) any purpose, 2) derivative work, and 3) redistribution, as long as the existing copyrights in our benchmark datasets are respected. We also open our pretrained Korean language models and the implementation of pretraining and fine-tuning pipelines. This enhances reproducibility of our work, and allows anyone to fix and improve our data and models. We hope to contribute to the Korean NLP research community as well the wider NLP community.\n' +
      '\n' +
      'Facilitating Korean NLP ResearchWe developed KLUE with the aim of facilitating Korean NLP research, in response to the recent active development efforts of large Korean language models. The entire NLP community has seen BERT [30] and its variants outperforming the previous NLU models for GLUE [133] and SuperGLUE [132], as well as the more recent GPT3 [9] with outstanding performance without fine-tuning (and with _in-context learning_) in natural language understanding and generation. Motivated by these models, many Korean researchers at various institutions rushed to pretrain large-scale Transformer-based Korean language models. Consequently, a number of nearly identical pretrained language models have been released to open-source communities. However, we could not systematically understand the behaviors and characteristics of these models because of the lack of well-designed general-purpose benchmarks like GLUE for Korean. KLUE will allow us to conduct controlled experiments to understand how and why various Korean LMs perform on certain tasks and thus obtain detailed insights into those models. Furthermore, since KLUE includes many representative NLU tasks that are also conducted in other languages, KLUE will function as a fundamental resource to NLP researchers who aim to conduct multilingual research with Korean and other languages.\n' +
      '\n' +
      'Measuring Overall Performance of NLU modelsWe do not average all scores gained from each task in KLUE. The performance of all tasks are measured by different evaluation metrics. This is because we carefully choose the metric for each task with considering its own characteristics. Their granularity differs by tasks, for example, KLUE-MRC and KLUE-NER employ character-level metrics because an entity can exist within a word in Korean whereas KLUE-STS and KLUE-NLI use sentence-level metrics. Furthermore, we use various metrics across tasks, such as F1 score, accuracy, area under the curve, UAS, LAS, ROUGE-W, joint goal accuracy, and Pearson\'s correlation. In this situation, simply computing the average of all tasks as in GLUE [133] results in misleading overall performance measure. The average will lose its interpretability as well as giving higher weights to a certain task in unintended ways. Accordingly, an alternative way to estimating a model\'s NLU capability is necessary. Recently, analyzing correctness of a model\'s prediction by using Item Response Theory (IRT) framework to estimate such capability is proposed [70], however, we find that it is not clear how it should be applied precisely in our benchmark. As of now, we thus decide to evaluate a model for each task separately without any summarization of overall performance measure. This is our limitation, and we leave this problem for the future.\n' +
      '\n' +
      'Rapid Saturation of KLUEWe expect fast saturation of KLUE based on our observations of for instance GLUE [94]. However, we do not artificially make our benchmark challenging by e.g. filtering out easy examples for models. Because the main purpose of KLUE is to properly evaluate models in terms of various aspects of NLU, we avoid focusing on enlarging headrooms for improvement over our baseline pretrained language models. We expect our license policy would positively affect the advancement of our benchmark after saturation by collectively developing more challenging tasks with other researchers, such as building the first open-domain question answering for Korean.\n' +
      '\n' +
      'Analysis of Korean Language ModelsWe observe various patterns when comparing performances of the baseline models on each tasks, however, most of them are understudied to precisely explain the phenomena. With more thorough investigations, we hope to enhance understanding of the complex interaction of a model, corpus, linguistic properties of Korean and training mechanisms in future work.\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      'We present KLUE, a suite of Korean NLU benchmarks that includes diverse tasks. We open KLUE to everyone, and we also provide Korean language models trained to outperform multilingual models and other existing open-sourced Korean language models. We set high standards from the outset, as we built the benchmark and trained the models from scratch. We designed the benchmark datasets and trained the annotators rigorously to consider potential ethical issues including private information and hate speech. We documented in detail all of the benchmark construction and testing processes. We also discussed broader impacts and limitations of KLUE and our models. Despite the limitations, KLUE and the accompanying language models will facilitate future Korean NLP research by setting a valuable precedent describing how datasets and language models should be created and spread to a wider community.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'Upstage sponsored annotation cost and built the leaderboard. NAVER CLOVA provided data annotation cost and GPU cloud computing infrastructure (NSML). We also thank to Google\'s TensorFlow Research Cloud (TFRC) and Kakao Enterprise\'s BrainCloud. The three computing resources were used to pretrain and fine-tune language models. Scatter Lab, SelectStar, Riiid!, DeepNatural and KAIST sponsored data annotation cost. In addition, we appreciate The Korea Economy Daily and Acrofan for supporting their news articles for MRC datasets.\n' +
      '\n' +
      'The authors thank Cheoneum Park for discussions about task selection and DP task, Jinhyuk Lee and Minjoon Seo for discussions on MRC task, Sujeong Kim and DongYeon Kim for considerable efforts to manage annotators in MRC dataset, and Sangah Park for careful consideration of data construction in DP, NER, and RE. We appreciate Junyeop Lee, Geonhee Lee, Jiho Lee, Daehyun Nam, and Yongjin Cho for the leaderboard and evaluation system implementation.\n' +
      '\n' +
      'This study is reviewed and approved by the KAIST Institutional Review Board (#KH2020-173).\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] National Information Society Agency. MRC AI Dataset. [https://aihub.or.kr/aidata/86](https://aihub.or.kr/aidata/86), 2018.\n' +
      '* [2] Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In _Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)_, pages 252-263, Denver, Colorado, June 2015. Association for Computational Linguistics. doi: 10.18653/v1/S15-2045. URL [https://www.aclweb.org/anthology/S15-2045](https://www.aclweb.org/anthology/S15-2045).\n' +
      '* [3] Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In _Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)_, pages 497-511, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/S16-1081. URL [https://www.aclweb.org/anthology/S16-1081](https://www.aclweb.org/anthology/S16-1081).\n' +
      '* [4] Jens Allwood. An activity based approach to pragmatics. In Harry Bunt and William Black, editors, _Abduction, belief and context in dialogue: Studies in computational pragmatics_, chapter 2, pages 47-80. John Benjamins, Amsterdam, Netherlands, 2000.\n' +
      '* [5] Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the blanks: Distributional similarity for relation learning. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 2895-2905, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1279. URL [https://www.aclweb.org/anthology/P19-1279](https://www.aclweb.org/anthology/P19-1279).\n' +
      '* [6] Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system bias and enabling better science. _Transactions of the Association for Computational Linguistics_, 6:587-604, 2018. doi: 10.1162/tacl_a_00041. URL [https://www.aclweb.org/anthology/Q18-1041](https://www.aclweb.org/anthology/Q18-1041).\n' +
      '* [7] Samuel R Bowman and George E Dahl. What will it take to fix benchmarking in natural language understanding? _arXiv preprint arXiv:2104.02145_, 2021.\n' +
      '* [8] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 632-642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL [https://www.aclweb.org/anthology/D15-1075](https://www.aclweb.org/anthology/D15-1075).\n' +
      '* [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).\n' +
      '* a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 5016-5026, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1547. URL [https://www.aclweb.org/anthology/D18-1547](https://www.aclweb.org/anthology/D18-1547).\n' +
      '* [11] Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Ben Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young Kim, and Andy Cedilnik. Taskmaster-1: Toward a realistic and diverse dialog dataset. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 4516-4525, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1459. URL [https://www.aclweb.org/anthology/D19-1459](https://www.aclweb.org/anthology/D19-1459).\n' +
      '* [12] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. _arXiv preprint arXiv:2012.07805_, 2020.\n' +
      '\n' +
      '* [13] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In _Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)_, pages 1-14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL [https://www.aclweb.org/anthology/S17-2001](https://www.aclweb.org/anthology/S17-2001).\n' +
      '* [14] Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. A survey on dialogue systems: Recent advances and new frontiers. _SIGKDD Explor. Newsl._, 19(2):25-35, November 2017. ISSN 1931-0145. doi: 10.1145/3166054.3166058. URL [https://doi.org/10.1145/3166054.3166058](https://doi.org/10.1145/3166054.3166058).\n' +
      '* [15] Pengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si, and Lawrence Carin. FairFil: Contrastive neural debiasing method for pretrained text encoders. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=N6JECD-PI5w](https://openreview.net/forum?id=N6JECD-PI5w).\n' +
      '* May 1, 1998_, 1998. URL [https://www.aclweb.org/anthology/M98-1001](https://www.aclweb.org/anthology/M98-1001).\n' +
      '* [17] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1724-1734, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL [https://www.aclweb.org/anthology/D14-1179](https://www.aclweb.org/anthology/D14-1179).\n' +
      '* [18] Won Ik Cho, Jong In Kim, Young Ki Moon, and Nam Soo Kim. Discourse component to sentence (DC2S): An efficient human-aided construction of paraphrase and sentence similarity dataset. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 6819-6826, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.842](https://www.aclweb.org/anthology/2020.lrec-1.842).\n' +
      '* [19] Won Ik Cho, Sangwhan Moon, and Youngsook Song. Open Korean corpora: A practical report. In _Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)_, pages 85-93, Online, November 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.nlposs-1.12](https://www.aclweb.org/anthology/2020.nlposs-1.12).\n' +
      '* [20] Jayeol Chun, Na-Rae Han, Jena D. Hwang, and Jinho D. Choi. Building Universal Dependency treebanks in Korean. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL [https://www.aclweb.org/anthology/L18-1347](https://www.aclweb.org/anthology/L18-1347).\n' +
      '* [21] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2924-2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL [https://www.aclweb.org/anthology/N19-1300](https://www.aclweb.org/anthology/N19-1300).\n' +
      '* [22] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/pdf?id=r1xMH1BtvB](https://openreview.net/pdf?id=r1xMH1BtvB).\n' +
      '* [23] Korea Copyright Commission. Newspapers and copyright, 2009.\n' +
      '* [24] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 670-680, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1070. URL [https://www.aclweb.org/anthology/D17-1070](https://www.aclweb.org/anthology/D17-1070).\n' +
      '* [25] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2475-2485, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1269. URL [https://www.aclweb.org/anthology/D18-1269](https://www.aclweb.org/anthology/D18-1269).\n' +
      '\n' +
      '* Conneau et al. [2020] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 8440-8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL [https://www.aclweb.org/anthology/2020.acl-main.747](https://www.aclweb.org/anthology/2020.acl-main.747).\n' +
      '* Dagan et al. [2006] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Joaquin Quinonero-Candela, Ido Dagan, Bernardo Magnini, and Florence d\'Alche Buc, editors, _Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment_, pages 177-190, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-33428-6.\n' +
      '* de Marneffe and Manning [2008] Marie-Catherine de Marneffe and Christopher D. Manning. The Stanford typed dependencies representation. In _Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation_, pages 1-8, Manchester, UK, August 2008. Coling 2008 Organizing Committee. URL [https://www.aclweb.org/anthology/W08-1301](https://www.aclweb.org/anthology/W08-1301).\n' +
      '* de Marneffe et al. [2006] Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. Generating typed dependency parses from phrase structure parses. In _Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC\'06)_, Genoa, Italy, May 2006. European Language Resources Association (ELRA). URL [http://www.lrec-conf.org/proceedings/lrec2006/pdf/440_pdf.pdf](http://www.lrec-conf.org/proceedings/lrec2006/pdf/440_pdf.pdf).\n' +
      '* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL [https://www.aclweb.org/anthology/N19-1423](https://www.aclweb.org/anthology/N19-1423).\n' +
      '* tasks, data, and evaluation. In _Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC\'04)_, Lisbon, Portugal, May 2004. European Language Resources Association (ELRA). URL [http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf](http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf).\n' +
      '* Dolan and Brockett [2005] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In _Proceedings of the Third International Workshop on Paraphrasing (IWP2005)_, 2005. URL [https://www.aclweb.org/anthology/I05-5002](https://www.aclweb.org/anthology/I05-5002).\n' +
      '* Dozat and Manning [2017] Timothy Dozat and Christopher D. Manning. Deep biaffine attention for neural dependency parsing. In _International Conference on Learning Representations_, 2017. URL [https://openreview.net/forum?id=Hk95PK91e](https://openreview.net/forum?id=Hk95PK91e).\n' +
      '* Eberhard and Simons [2021] David M. Eberhard and Charles D. Simons, Gary F. Fanning. _Ethnologue: Languages of the World_. SIL International, Dallas, Texas, 24 edition, 2021. URL [http://www.ethnologue.com](http://www.ethnologue.com).\n' +
      '* Asri et al. [2017] Layla El Asri, Hannes Schulz, Shikhar Sharma, Jeremie Zumer, Justin Harris, Emery Fine, Rahul Mehrotra, and Kaheer Suleman. Frames: a corpus for adding memory to goal-oriented dialogue systems. In _Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue_, pages 207-219, Saarbrucken, Germany, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-5526. URL [https://www.aclweb.org/anthology/W17-5526](https://www.aclweb.org/anthology/W17-5526).\n' +
      '* Eric et al. [2017] Mihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. Key-value retrieval networks for task-oriented dialogue. In _Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue_, pages 37-49, Saarbrucken, Germany, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-5506. URL [https://www.aclweb.org/anthology/W17-5506](https://www.aclweb.org/anthology/W17-5506).\n' +
      '* Eric et al. [2020] Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj Goyal, Peter Ku, and Dilek Hakkani-Tur. MultiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 422-428, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.53](https://www.aclweb.org/anthology/2020.lrec-1.53).\n' +
      '\n' +
      '* Fan et al. [2019] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 3558-3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL [https://www.aclweb.org/anthology/P19-1346](https://www.aclweb.org/anthology/P19-1346).\n' +
      '* Fernandez-Gonzalez and Gomez-Rodriguez [2019] Daniel Fernandez-Gonzalez and Carlos Gomez-Rodriguez. Left-to-right dependency parsing with pointer networks. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 710-716, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1076. URL [https://www.aclweb.org/anthology/N19-1076](https://www.aclweb.org/anthology/N19-1076).\n' +
      '* Finkel et al. [2005] Jenny Rose Finkel, Trond Grenager, and Christopher Manning. Incorporating non-local information into information extraction systems by Gibbs sampling. In _Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\'05)_, pages 363-370, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. doi: 10.3115/1219840.1219885. URL [https://www.aclweb.org/anthology/P05-1045](https://www.aclweb.org/anthology/P05-1045).\n' +
      '* Gebru et al. [2018] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume III, and Kate Crawford. Datasheets for datasets. _arXiv preprint arXiv:1803.09010_, 2018.\n' +
      '* Glockner et al. [2018] Max Glockner, Vered Shwartz, and Yoav Goldberg. Breaking NLI systems with sentences that require simple lexical inferences. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 650-655, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2103. URL [https://www.aclweb.org/anthology/P18-2103](https://www.aclweb.org/anthology/P18-2103).\n' +
      '* Grishman and Sundheim [1996] Ralph Grishman and Beth Sundheim. Message Understanding Conference- 6: A brief history. In _COLING 1996 Volume 1: The 16th International Conference on Computational Linguistics_, 1996. URL [https://www.aclweb.org/anthology/C96-1079](https://www.aclweb.org/anthology/C96-1079).\n' +
      '* Gururangan et al. [2018] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)_, pages 107-112, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2017. URL [https://www.aclweb.org/anthology/N18-2017](https://www.aclweb.org/anthology/N18-2017).\n' +
      '* Ham et al. [2020] Jiyeon Ham, Yo Joong Choe, Kyubyong Park, Ilji Choi, and Hyungjoon Soh. KorNLI and KorSTS: New benchmark datasets for Korean natural language understanding. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 422-430, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.39. URL [https://www.aclweb.org/anthology/2020.findings-emnlp.39](https://www.aclweb.org/anthology/2020.findings-emnlp.39).\n' +
      '* Han et al. [2020] Ji Yoon Han, Tae Hwan Oh, Lee Jin, and Hansaem Kim. Annotation issues in Universal Dependencies for Korean and Japanese. In _Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)_, pages 99-108, Barcelona, Spain (Online), December 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.udw-1.12](https://www.aclweb.org/anthology/2020.udw-1.12).\n' +
      '* Han et al. [2006] Na-Rae Han, Shijong Ryu, Sook-Hee Chae, Seung-yun Yang, Seunghun Lee, and Martha Palmer. Korean treebank annotations version 2.0. _Linguistic Data Consortium (LDC), Philadelphia_, 2006.\n' +
      '* Han et al. [2018] Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 4803-4809, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1514. URL [https://www.aclweb.org/anthology/D18-1514](https://www.aclweb.org/anthology/D18-1514).\n' +
      '* He et al. [2021] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. DeBERTa: Decoding-enhanced BERT with disentangled attention. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=XPZIaotutsD](https://openreview.net/forum?id=XPZIaotutsD).\n' +
      '* Henderson et al. [2014] Matthew Henderson, Blaise Thomson, and Jason D. Williams. The second dialog state tracking challenge. In _Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)_, pages 263-272, Philadelphia, PA, U.S.A., June 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-4337. URL [https://www.aclweb.org/anthology/W14-4337](https://www.aclweb.org/anthology/W14-4337).\n' +
      '\n' +
      '* Henderson et al. [2014] Matthew Henderson, Blaise Thomson, and Jason D Williams. The third dialog state tracking challenge. In _2014 IEEE Spoken Language Technology Workshop (SLT)_, pages 324-329. IEEE, 2014.\n' +
      '* Hendrickx et al. [2010] Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O Seaghdha, Sebastian Pado, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In _Proceedings of the 5th International Workshop on Semantic Evaluation_, pages 33-38, Uppsala, Sweden, July 2010. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/S10-1006](https://www.aclweb.org/anthology/S10-1006).\n' +
      '* Hu et al. [2020] Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Kubler, and Lawrence Moss. OCNLI: Original Chinese Natural Language Inference. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 3512-3526, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.314. URL [https://www.aclweb.org/anthology/2020.findings-emnlp.314](https://www.aclweb.org/anthology/2020.findings-emnlp.314).\n' +
      '* Hu et al. [2020] Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 4411-4421. PMLR, 13-18 Jul 2020. URL [http://proceedings.mlr.press/v119/hu20b.html](http://proceedings.mlr.press/v119/hu20b.html).\n' +
      '* Jabbari et al. [2020] Ali Jabbari, Olivier Sauvage, Hamada Zeine, and Hamza Chergui. A French corpus and annotation schema for named entity recognition and relation extraction of financial news. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 2293-2299, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.279](https://www.aclweb.org/anthology/2020.lrec-1.279).\n' +
      '* Joshi et al. [2017] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL [https://www.aclweb.org/anthology/P17-1147](https://www.aclweb.org/anthology/P17-1147).\n' +
      '* Kakwani et al. [2020] Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. Khapra, and Pratyush Kumar. IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 4948-4961, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.445. URL [https://www.aclweb.org/anthology/2020.findings-emnlp.445](https://www.aclweb.org/anthology/2020.findings-emnlp.445).\n' +
      '* Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* Kelley [1984] J. F. Kelley. An iterative design methodology for user-friendly natural language office information applications. _ACM Trans. Inf. Syst._, 2(1):26-41, January 1984. ISSN 1046-8188. doi: 10.1145/357417.357420. URL [https://doi.org/10.1145/357417.357420](https://doi.org/10.1145/357417.357420).\n' +
      '* Khashabi et al. [2018] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 252-262, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1023. URL [https://www.aclweb.org/anthology/N18-1023](https://www.aclweb.org/anthology/N18-1023).\n' +
      '* Khashabi et al. [2020] Daniel Khashabi, Arman Cohan, Siamak Shakeri, Pedram Hosseini, Pouya Pezeshkpour, Malihe Alikhani, Moin Aminnaseri, Marzieh Bitaab, Faeze Brahman, Sarik Ghazarian, et al. ParsiNLU: a suite of language understanding challenges for persian. _arXiv preprint arXiv:2012.06154_, 2020.\n' +
      '* Kiela et al. [2020] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 2611-2624. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/1b84c4cee2b8b3d823b30e2d604b1878-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1b84c4cee2b8b3d823b30e2d604b1878-Paper.pdf).\n' +
      '\n' +
      '* Kim et al. [2020] Youngmin Kim, Seungyoung Lim, Hyunjeong Lee, Soyoon Park, and Myungji Kim. KorQuAD 2.0: Korean QA dataset for web document machine comprehension. _Journal of KIISE_, 47:577-586, 2020. ISSN 2383-630X. URL [https://science.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NART99691770&dbt=NART](https://science.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NART99691770&dbt=NART).\n' +
      '* Kiperwasser and Goldberg [2016] Eliyahu Kiperwasser and Yoav Goldberg. Simple and accurate dependency parsing using bidirectional LSTM feature representations. _Transactions of the Association for Computational Linguistics_, 4:313-327, 2016. doi: 10.1162/tacl_a_00101. URL [https://www.aclweb.org/anthology/Q16-1023](https://www.aclweb.org/anthology/Q16-1023).\n' +
      '* Klein and Manning [2003] Dan Klein and Christopher D. Manning. Accurate unlexicalized parsing. In _Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics_, pages 423-430, Sapporo, Japan, July 2003. Association for Computational Linguistics. doi: 10.3115/1075096.1075150. URL [https://www.aclweb.org/anthology/P03-1054](https://www.aclweb.org/anthology/P03-1054).\n' +
      '* Ko et al. [2020] Miyoung Ko, Jinhyuk Lee, Hyunje Kim, Gangwoo Kim, and Jaewoo Kang. Look at the first sentence: Position bias in question answering. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1109-1121, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.84. URL [https://www.aclweb.org/anthology/2020.emnlp-main.84](https://www.aclweb.org/anthology/2020.emnlp-main.84).\n' +
      '* Krippendorff [2011] K. Krippendorff. Computing Krippendorff\'s alpha-reliability. 2011.\n' +
      '* Kudo [2006] Taku Kudo. McCab: Yet another part-of-speech and morphological analyzer, 2006. URL [https://taku910.github.io/mecab/](https://taku910.github.io/mecab/).\n' +
      '* Kwiatkowski et al. [2019] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:452-466, March 2019. doi: 10.1162/tacl_a_00276. URL [https://www.aclweb.org/anthology/Q19-1026](https://www.aclweb.org/anthology/Q19-1026).\n' +
      '* Lalor and Yu [2020] John P. Lalor and Hong Yu. Dynamic data selection for curriculum learning via ability estimation. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 545-555, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.48. URL [https://www.aclweb.org/anthology/2020.findings-emnlp.48](https://www.aclweb.org/anthology/2020.findings-emnlp.48).\n' +
      '* Lan et al. [2020] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=H1eA7AEtvS](https://openreview.net/forum?id=H1eA7AEtvS).\n' +
      '* Le et al. [2020] Hang Le, Loic Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoit Crabbe, Laurent Besacier, and Didier Schwab. FlauBERT: Unsupervised language model pre-training for French. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 2479-2490, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.302](https://www.aclweb.org/anthology/2020.lrec-1.302).\n' +
      '* Lee [2020] Junbum Lee. KcBERT: Korean comments BERT. In _Proceedings of the 32nd Annual Conference on Human and Cognitive Language Technology_, pages 437-440, 2020.\n' +
      '* Lee et al. [2018] Kyungjae Lee, Kyoungho Yoon, Sunghyun Park, and Seung-won Hwang. Semi-supervised training data generation for multilingual question answering. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL [https://www.aclweb.org/anthology/L18-1437](https://www.aclweb.org/anthology/L18-1437).\n' +
      '* Lee et al. [2020] Sangah Lee, Hansol Jang, Yunmee Baik, Suzi Park, and Hypoil Shin. KR-BERT: A small-scale Korean-specific language model. _arXiv preprint arXiv:2008.03979_, 2020.\n' +
      '* Lewis et al. [2020] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7871-7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL [https://www.aclweb.org/anthology/2020.acl-main.703](https://www.aclweb.org/anthology/2020.acl-main.703).\n' +
      '* Li et al. [2020] Shiyang Li, Semih Yavuz, Kazuma Hashimoto, Jia Li, Tong Niu, Nazneen Rajani, Xifeng Yan, Yingbo Zhou, and Caiming Xiong. CoCo: Controllable counterfactuals for evaluating dialogue state trackers. _arXiv preprint arXiv:2010.12850_, 2020.\n' +
      '\n' +
      '* Liang et al. [2020] Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 6008-6018, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.484. URL [https://www.aclweb.org/anthology/2020.emnlp-main.484](https://www.aclweb.org/anthology/2020.emnlp-main.484).\n' +
      '* Lim et al. [2019] Seungyoung Lim, Myungji Kim, and Jooyoul Lee. KorQuAD1.0: Korean QA dataset for machine reading comprehension. _arXiv preprint arXiv:1909.07005_, 2019.\n' +
      '* Lin [2004] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In _Text Summarization Branches Out_, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/W04-1013](https://www.aclweb.org/anthology/W04-1013).\n' +
      '* Liu et al. [2021] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. On-the-fly controlled text generation with experts and anti-experts, 2021.\n' +
      '* Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.\n' +
      '* Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7).\n' +
      '* Marcus et al. [1993] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. _Computational Linguistics_, 19(2):313-330, 1993. URL [https://www.aclweb.org/anthology/J93-2004](https://www.aclweb.org/anthology/J93-2004).\n' +
      '* McCann et al. [2018] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. _arXiv preprint arXiv:1806.08730_, 2018.\n' +
      '* McDonald et al. [2013] Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Tackstrom, Claudia Bedini, Nuria Bertomeu Castello, and Jungmee Lee. Universal Dependency annotation for multilingual parsing. In _Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 92-97, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/P13-2017](https://www.aclweb.org/anthology/P13-2017).\n' +
      '* McNamee and Dang [2009] Paul McNamee and Hoa Trang Dang. Overview of the TAC 2009 knowledge base population track. In _Text Analysis Conference (TAC)_, volume 17, pages 111-113, 2009.\n' +
      '* Mehri et al. [2020] Shikib Mehri, Mihail Eric, and Dilek Hakkani-Tur. DialoGLUE: A natural language understanding benchmark for task-oriented dialogue. _arXiv preprint arXiv:2009.13570_, 2020.\n' +
      '* Min et al. [2020] Junghyun Min, R. Thomas McCoy, Dipanjan Das, Emily Pitler, and Tal Linzen. Syntactic data augmentation increases robustness to inference heuristics. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 2339-2352, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.212. URL [https://www.aclweb.org/anthology/2020.acl-main.212](https://www.aclweb.org/anthology/2020.acl-main.212).\n' +
      '* Min et al. [2019] Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. Compositional questions do not necessitate multi-hop reasoning. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4249-4257, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1416. URL [https://www.aclweb.org/anthology/P19-1416](https://www.aclweb.org/anthology/P19-1416).\n' +
      '* Mintz et al. [2009] Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. Distant supervision for relation extraction without labeled data. In _Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP_, pages 1003-1011, Suntec, Singapore, August 2009. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/P09-1113](https://www.aclweb.org/anthology/P09-1113).\n' +
      '* Moon et al. [2020] Jihyung Moon, Won Ik Cho, and Junbum Lee. BEEP! Korean corpus of online news comments for toxic speech detection. In _Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media_, pages 25-31, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.socialnlp-1.4. URL [https://www.aclweb.org/anthology/2020.socialnlp-1.4](https://www.aclweb.org/anthology/2020.socialnlp-1.4).\n' +
      '\n' +
      '* Nam et al. [2020] Sangha Nam, Minho Lee, Donghwan Kim, Kijong Han, Kuntae Kim, Sooji Yoon, Eun-kyung Kim, and Key-Sun Choi. Effective crowdsourcing of multiple tasks for comprehensive knowledge extraction. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 212-219, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.27](https://www.aclweb.org/anthology/2020.lrec-1.27).\n' +
      '* Nangia and Bowman [2019] Nikita Nangia and Samuel R. Bowman. Human vs. muppet: A conservative estimate of human performance on the GLUE benchmark. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4566-4575, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1449. URL [https://www.aclweb.org/anthology/P19-1449](https://www.aclweb.org/anthology/P19-1449).\n' +
      '* Nangia et al. [2020] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1953-1967, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.154. URL [https://www.aclweb.org/anthology/2020.emnlp-main.154](https://www.aclweb.org/anthology/2020.emnlp-main.154).\n' +
      '* Nguyen et al. [2016] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated MAchine reading COmprehension dataset. November 2016. URL [https://www.microsoft.com/en-us/research/publication/ms-marco-human-generated-machine-reading-comprehension-dataset/](https://www.microsoft.com/en-us/research/publication/ms-marco-human-generated-machine-reading-comprehension-dataset/).\n' +
      '* Nivre et al. [2016] Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christopher D. Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel Zeman. Universal Dependencies v1: A multilingual treebank collection. In _Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\'16)_, pages 1659-1666, Portoroz, Slovenia, May 2016. European Language Resources Association (ELRA). URL [https://www.aclweb.org/anthology/L16-1262](https://www.aclweb.org/anthology/L16-1262).\n' +
      '* [98] National Institute of Korean Languages. NIKL CORPORA 2020 (v.1.0), 2020. URL [https://corpus.korean.go.kr](https://corpus.korean.go.kr).\n' +
      '* Oh et al. [2020] Tae Hwan Oh, Ji Yoon Han, Hyonsu Choe, Seokwon Park, Han He, Jinho D. Choi, Na-Rae Han, Jena D. Hwang, and Hansaem Kim. Analysis of the Penn Korean Universal Dependency treebank (PKT-UD): Manual revision to build robust parsing model in Korean. In _Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies_, pages 122-131, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.iwpt-1.13. URL [https://www.aclweb.org/anthology/2020.iwpt-1.13](https://www.aclweb.org/anthology/2020.iwpt-1.13).\n' +
      '* Pan et al. [2017] Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. Cross-lingual name tagging and linking for 282 languages. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1946-1958, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1178. URL [https://www.aclweb.org/anthology/P17-1178](https://www.aclweb.org/anthology/P17-1178).\n' +
      '* Park and Cho [2014] Eunjeong L. Park and Sungzoon Cho. KoNLPy: Korean natural language processing in Python. In _Proceedings of the 26th Annual Conference on Human & Cognitive Language Technology_, Chuncheon, Korea, October 2014.\n' +
      '* Park [2020] Jangwon Park. KoELECTRA: Pretrained ELECTRA model for Korean. [https://github.com/monologg/KoELECTRA](https://github.com/monologg/KoELECTRA), 2020.\n' +
      '* Park et al. [2020] Kyubyong Park, Joohong Lee, Seongbo Jang, and Dawoon Jung. An empirical study of tokenization strategies for various Korean NLP tasks. In _Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing_, pages 133-142, Suzhou, China, December 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.aacl-main.17](https://www.aclweb.org/anthology/2020.aacl-main.17).\n' +
      '* Peters et al. [2018] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 2227-2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL [https://www.aclweb.org/anthology/N18-1202](https://www.aclweb.org/anthology/N18-1202).\n' +
      '\n' +
      '* Petroni et al. [2020] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. KILT: a benchmark for knowledge intensive language tasks. _arXiv preprint arXiv:2009.02252_, 2020.\n' +
      '* Petrov et al. [2012] Slav Petrov, Dipanjan Das, and Ryan McDonald. A universal part-of-speech tagset. In _Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\'12)_, pages 2089-2096, Istanbul, Turkey, May 2012. European Language Resources Association (ELRA). URL [http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf).\n' +
      '* Phang et al. [2018] Jason Phang, Thibault Fevry, and Samuel R Bowman. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. _arXiv preprint arXiv:1811.01088_, 2018.\n' +
      '* Poliak et al. [2018] Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. Hypothesis only baselines in natural language inference. In _Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics_, pages 180-191, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/S18-2023. URL [https://www.aclweb.org/anthology/S18-2023](https://www.aclweb.org/anthology/S18-2023).\n' +
      '* Quan et al. [2020] Jun Quan, Shian Zhang, Qian Cao, Zizhong Li, and Deyi Xiong. RiSAWOZ: A large-scale multi-domain Wizard-of-Oz dataset with rich semantic annotations for task-oriented dialogue modeling. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 930-940, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.67. URL [https://www.aclweb.org/anthology/2020.emnlp-main.67](https://www.aclweb.org/anthology/2020.emnlp-main.67).\n' +
      '* Radford et al. [2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\n' +
      '* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).\n' +
      '* Rajpurkar et al. [2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 2383-2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL [https://www.aclweb.org/anthology/D16-1264](https://www.aclweb.org/anthology/D16-1264).\n' +
      '* Rajpurkar et al. [2018] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\'t know: Unanswerable questions for SQuAD. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 784-789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL [https://www.aclweb.org/anthology/P18-2124](https://www.aclweb.org/anthology/P18-2124).\n' +
      '* Rastogi et al. [2020] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(05):8689-8696, Apr. 2020. doi: 10.1609/aaai.v34i05.6394. URL [https://ojs.aaai.org/index.php/AAAI/article/view/6394](https://ojs.aaai.org/index.php/AAAI/article/view/6394).\n' +
      '* Reimers and Gurevych [2019] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3982-3992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL [https://www.aclweb.org/anthology/D19-1410](https://www.aclweb.org/anthology/D19-1410).\n' +
      '* Riedel et al. [2010] Sebastian Riedel, Limin Yao, and Andrew McCallum. Modeling relations and their mentions without labeled text. In Jose Luis Balcazar, Francesco Bonchi, Aristides Gionis, and Michele Sebag, editors, _Machine Learning and Knowledge Discovery in Databases_, pages 148-163, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. ISBN 978-3-642-15939-8.\n' +
      '* Ritter et al. [2011] Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. Named entity recognition in tweets: An experimental study. In _Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing_, pages 1524-1534, Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/D11-1141](https://www.aclweb.org/anthology/D11-1141).\n' +
      '* Robertson et al. [1995] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at trec-3. _Nist Special Publication Sp_, 109:109, 1995.\n' +
      '\n' +
      '* Rust et al. [2020] Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder, and Iryna Gurevych. How good is your tokenizer? on the monolingual performance of multilingual language models. _arXiv preprint arXiv:2012.15613_, 2020.\n' +
      '* Saha et al. [2018] Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. DuoRC: Towards complex language understanding with paraphrased reading comprehension. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1683-1693, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1156. URL [https://www.aclweb.org/anthology/P18-1156](https://www.aclweb.org/anthology/P18-1156).\n' +
      '* Schiersch et al. [2018] Martin Schiersch, Veselina Mironova, Maximilian Schmitt, Philippe Thomas, Aleksandra Gabryszak, and Leonhard Hennig. A German corpus for fine-grained named entity recognition and relation extraction of traffic and industry events. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL [https://www.aclweb.org/anthology/L18-1703](https://www.aclweb.org/anthology/L18-1703).\n' +
      '* Sen and Saffari [2020] Priyanka Sen and Amir Saffari. What do models learn from question answering datasets? In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 2429-2438, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.190. URL [https://www.aclweb.org/anthology/2020.emnlp-main.190](https://www.aclweb.org/anthology/2020.emnlp-main.190).\n' +
      '* Sennrich et al. [2016] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1715-1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL [https://www.aclweb.org/anthology/P16-1162](https://www.aclweb.org/anthology/P16-1162).\n' +
      '* Shah et al. [2018] Pararth Shah, Dilek Hakkani-Tur, Gokhan Tur, Abhinav Rastogi, Ankur Bapna, Neha Nayak, and Larry Heck. Building a conversational agent overnight with dialogue self-play. _arXiv preprint arXiv:1801.04871_, 2018.\n' +
      '* Shavrina et al. [2020] Tatiana Shavrina, Alena Fenogenova, Emelyanov Anton, Denis Shevelev, Ekaterina Artemova, Valentin Malykh, Vladislav Mikhailov, Maria Tikhonova, Andrey Chertok, and Andrey Evlampiev. RussianSuperGLUE: A Russian language understanding evaluation benchmark. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4717-4726, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.381. URL [https://www.aclweb.org/anthology/2020.emnlp-main.381](https://www.aclweb.org/anthology/2020.emnlp-main.381).\n' +
      '* Socher et al. [2013] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing_, pages 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/D13-1170](https://www.aclweb.org/anthology/D13-1170).\n' +
      '* Strauss et al. [2016] Benjamin Strauss, Bethany Toma, Alan Ritter, Marie-Catherine de Marneffe, and Wei Xu. Results of the WNUT16 named entity recognition shared task. In _Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)_, pages 138-144, Osaka, Japan, December 2016. The COLING 2016 Organizing Committee. URL [https://www.aclweb.org/anthology/W16-3919](https://www.aclweb.org/anthology/W16-3919).\n' +
      '* Choi et al. [1994] Key sun Choi, Young S. Han, Young G. Han, and Oh W. Kwon. KAIST tree bank project for Korean: Present and future development. In _In Proceedings of the International Workshop on Sharable Natural Language Resources_, pages 7-14, 1994.\n' +
      '* Sang and De Meulder [2003] Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In _Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003_, pages 142-147, 2003. URL [https://www.aclweb.org/anthology/W03-0419](https://www.aclweb.org/anthology/W03-0419).\n' +
      '* Trischler et al. [2017] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. In _Proceedings of the 2nd Workshop on Representation Learning for NLP_, pages 191-200, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-2623. URL <[https://www.aclweb.org/anthology/W17-2623](https://www.aclweb.org/anthology/W17-2623)>.\n' +
      '* Vania et al. [2020] Clara Vania, Ruijie Chen, and Samuel R. Bowman. Asking Crowdworkers to Write Entailment Examples: The Best of Bad options. In _Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing_, pages 672-686, Suzhou, China, December 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.aacl-main.68](https://www.aclweb.org/anthology/2020.aacl-main.68).\n' +
      '\n' +
      '* Wang et al. [2019] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf).\n' +
      '* Wang et al. [2019] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=rJ4km2R5t7](https://openreview.net/forum?id=rJ4km2R5t7).\n' +
      '* Warstadt et al. [2019] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. _Transactions of the Association for Computational Linguistics_, 7:625-641, March 2019. doi: 10.1162/tacl_a_00290. URL [https://www.aclweb.org/anthology/Q19-1040](https://www.aclweb.org/anthology/Q19-1040).\n' +
      '* Wen et al. [2017] Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. A network-based end-to-end trainable task-oriented dialogue system. In _Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers_, pages 438-449, Valencia, Spain, April 2017. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/E17-1042](https://www.aclweb.org/anthology/E17-1042).\n' +
      '* Wenzek et al. [2020] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 4003-4012, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.494](https://www.aclweb.org/anthology/2020.lrec-1.494).\n' +
      '* Wilie et al. [2020] Bryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri Bahar, and Ayu Purwarianti. IndoNLU: Benchmark and resources for evaluating Indonesian natural language understanding. In _Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing_, pages 843-857, Suzhou, China, December 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.aacl-main.85](https://www.aclweb.org/anthology/2020.aacl-main.85).\n' +
      '* Williams et al. [2018] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 1112-1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL [https://www.aclweb.org/anthology/N18-1101](https://www.aclweb.org/anthology/N18-1101).\n' +
      '* Wolf et al. [2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6).\n' +
      '* Wu et al. [2019] Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, and Pascale Fung. Transferable multi-domain state generator for task-oriented dialogue systems. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 808-819, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1078. URL [https://www.aclweb.org/anthology/P19-1078](https://www.aclweb.org/anthology/P19-1078).\n' +
      '* Xu et al. [2017] Jingjing Xu, Ji Wen, Xu Sun, and Qi Su. A discourse-level named entity recognition and relation extraction dataset for Chinese literature text. _arXiv preprint arXiv:1711.07010_, 2017.\n' +
      '* Xu et al. [2020] Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. CLUE: A Chinese language understanding evaluation benchmark. In _Proceedings of the 28th International Conference on Computational Linguistics_, pages 4762-4772, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.419. URL [https://www.aclweb.org/anthology/2020.coling-main.419](https://www.aclweb.org/anthology/2020.coling-main.419).\n' +
      '* Yang et al. [2015] Yi Yang, Wen-tau Yih, and Christopher Meek. WikiQA: A challenge dataset for open-domain question answering. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 2013-2018, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1237. URL [https://www.aclweb.org/anthology/D15-1237](https://www.aclweb.org/anthology/D15-1237).\n' +
      '* Yang et al. [2019] Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3687-3692, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1382. URL [https://www.aclweb.org/anthology/D19-1382](https://www.aclweb.org/anthology/D19-1382).\n' +
      '* Yang et al. [2018] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2369-2380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL [https://www.aclweb.org/anthology/D18-1259](https://www.aclweb.org/anthology/D18-1259)>.\n' +
      '* Yang et al. [2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf).\n' +
      '* Yao et al. [2019] Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. DocRED: A large-scale document-level relation extraction dataset. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 764-777, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1074. URL [https://www.aclweb.org/anthology/P19-1074](https://www.aclweb.org/anthology/P19-1074).\n' +
      '* Young et al. [2014] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78, 2014. doi: 10.1162/tacl_a_00166. URL [https://www.aclweb.org/anthology/Q14-1006](https://www.aclweb.org/anthology/Q14-1006).\n' +
      '* Yu et al. [2020] Dian Yu, Kai Sun, Claire Cardie, and Dong Yu. Dialogue-based relation extraction. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 4927-4940, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.444. URL [https://www.aclweb.org/anthology/2020.acl-main.444](https://www.aclweb.org/anthology/2020.acl-main.444).\n' +
      '* Zhang et al. [2018] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. _arXiv preprint arXiv:1810.12885_, 2018.\n' +
      '* Zhang et al. [2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9ao2-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9ao2-Paper.pdf).\n' +
      '* Zhang et al. [2019] Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 1298-1308, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1131. URL [https://www.aclweb.org/anthology/N19-1131](https://www.aclweb.org/anthology/N19-1131).\n' +
      '* Zhang et al. [2017] Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. Position-aware attention and supervised data improve slot filling. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 35-45, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1004. URL [https://www.aclweb.org/anthology/D17-1004](https://www.aclweb.org/anthology/D17-1004).\n' +
      '\n' +
      '* Zhu et al. [2020] Qi Zhu, Kaili Huang, Zheng Zhang, Xiaoyan Zhu, and Minlie Huang. CrossWOZ: A large-scale Chinese cross-domain task-oriented dialogue dataset. _Transactions of the Association for Computational Linguistics_, 8:281-295, 2020. doi: 10.1162/tacl_a_00314. URL [https://www.aclweb.org/anthology/2020.tacl-1.19](https://www.aclweb.org/anthology/2020.tacl-1.19).\n' +
      '\n' +
      '## Index\n' +
      '\n' +
      'ACROFAN (Acrofan News), 10\n' +
      '\n' +
      'AIRBNB (Airbnb Reviews), 10\n' +
      '\n' +
      'CC-100-Kor, 50\n' +
      '\n' +
      'DP (Dependency Parsing), 33\n' +
      '\n' +
      'DST (Dialogue State Tracking), 44\n' +
      '\n' +
      'KLUE\n' +
      '\n' +
      '(Korean Language Understanding Evaluation), 4\n' +
      '\n' +
      'KLUE-BERT, 50\n' +
      '\n' +
      'KLUE-DP, 36\n' +
      '\n' +
      'KLUE-MRC, 43\n' +
      '\n' +
      'KLUE-NER, 27\n' +
      '\n' +
      'KLUE-NLI, 24\n' +
      '\n' +
      'KLUE-RE, 32\n' +
      '\n' +
      'KLUE-RoBERTa, 50\n' +
      '\n' +
      'KLUE-STS, 19\n' +
      '\n' +
      'MODU (Modu Corpus), 50\n' +
      '\n' +
      'MRC (Machine Reading Comprehension), 37\n' +
      '\n' +
      'NAMUWIKI, 50\n' +
      '\n' +
      'NER (Named Entity Recognition), 25\n' +
      '\n' +
      'NEWSCRAWL, 50\n' +
      '\n' +
      'NLI (Natural Language Inference), 20\n' +
      '\n' +
      'NSMC (NAVER Sentiment Movie Corpus), 10\n' +
      '\n' +
      'PARAKQC, 10\n' +
      '\n' +
      'PETITION, 50\n' +
      '\n' +
      'POLICY (Policy News), 10\n' +
      '\n' +
      'RE (Relation Extraction), 28\n' +
      '\n' +
      'STS (Semantic Textual Similarity), 16\n' +
      '\n' +
      'TC (Topic Classification), 13\n' +
      '\n' +
      'The Korea Economy Daily, 10\n' +
      '\n' +
      'WIKINEWS, 10\n' +
      '\n' +
      'WIKIPEDIA, 10\n' +
      '\n' +
      'WIKITREE, 10\n' +
      '\n' +
      'WoS (Wizard of Seoul, KLUE-DST), 49\n' +
      '\n' +
      'YNA (Yonhap News Agency), 10\n' +
      '\n' +
      'YNAT (YNA Topic Classification, KLUE-TC), 15\n' +
      '\n' +
      '## Contribution\n' +
      '\n' +
      '**Sungjoon Park** led the project as project manager, initiated the project, made decisions on overall progress of this project, secured financial resources, signed up with ACROFAN for the articles, and organized IRB submission and research paper.\n' +
      '\n' +
      '**Jihyung Moon** led the project as project manager, managed overall datasets, models, and ethical concerns, signed up with ACROFAN for the articles, prepared IRB, as well as contributed to NER, STS, NLI, MRC dataset constructions, AIRBNB, POLICY corpora collection, and leaderboard design.\n' +
      '\n' +
      '**Sungdong Kim** managed overall fine-tuning of language models, served as a person in charge (PIC) of DST, and contributed to the dataset construction of TC, STS, and RE.\n' +
      '\n' +
      '**Won Ik Cho** managed the overall dataset construction of TC, STS, NLI, RE, MRC, and DST, provided the original corpus of PARAKQC, and served as a PIC of STS.\n' +
      '\n' +
      '**Jiyoon Han** managed the overall dataset construction of DP and NER, served as a PIC of NLI, contributed to the dataset construction of STS, and took part in preparing IRB.\n' +
      '\n' +
      '**Jangwon Park** served as a PIC of model pretraining, contributed to the text collection of YNA, collected and pre-processed MODU, CC-100-Kor, NAMUWIKI, NEWSCRAWL, and PETITION, and conducted the fine-tuning of TC.\n' +
      '\n' +
      '**Chisung Song** served as a PIC of NER and contributed to the dataset construction of DP and DST.\n' +
      '\n' +
      '**Junseong Kim** served as a PIC of MRC, conducted the fine-tuning of MRC, and contributed to the text collection of WIKITREE and WIKIPEDIA.\n' +
      '\n' +
      '**Youngsook Song** served as a PIC of TC and contributed to the dataset construction of NER and DST.\n' +
      '\n' +
      '**Taehwan Oh** served as a PIC of DP, contributed to the dataset construction of NER and NLI, and took part in preparing IRB.\n' +
      '\n' +
      '**Joohong Lee** served as a PIC of RE, and conducted the fine-tuning of RE.\n' +
      '\n' +
      '**Juhyun Oh** contributed to the dataset construction of NLI, NER, DP and STS, took part in ethical considerations, IRB preparation and setup for model pretraining.\n' +
      '\n' +
      '**Sungwon Lyu** contributed to the dataset construction of STS, NLI, RE, and MRC, taking part in the overall model pretraining and task-wise fine-tuning.\n' +
      '\n' +
      '**Younghoon Jeong** contributed to the text collection of WIKINEWS, modeling of DP, and pre-processing of the pretraining corpus.\n' +
      '\n' +
      '**Inkwon Lee** contributed to the text collection, modeling of DP and pre-processing of the pretraining corpus.\n' +
      '\n' +
      '**Sangwoo Seo** contributed to the dataset construction of RE, and took part in preparing IRB.\n' +
      '\n' +
      '**Dongjun Lee** contributed to the construction of the fine-tuning pipeline and the modeling of STS.\n' +
      '\n' +
      '**Hyunwoo Kim** contributed to the dataset construction of MRC, and took part in preparing IRB.\n' +
      '\n' +
      '**Myeonghwa Lee** contributed to the dataset construction of STS and TC.\n' +
      '\n' +
      '**Seongbo Jang** contributed to the dataset construction of RE, and took part in preparing IRB.\n' +
      '\n' +
      '**Seungwon Do** contributed to the dataset construction of DST and text collection.\n' +
      '\n' +
      '**Sunkyoung Kim** contributed to the dataset construction of RE and MRC, and modeling of MRC.\n' +
      '\n' +
      '**Kyungtae Lim** contributed to the dataset construction of DP.\n' +
      '\n' +
      '**Jongwon Lee** contributed to the dataset construction and modeling of DST.\n' +
      '\n' +
      '**Kyumin Park** contributed to the dataset construction of DST, and took part in preparing IRB.\n' +
      '\n' +
      '**Jamin Shin** contributed to the dataset construction of DST.\n' +
      '\n' +
      '**Seonghyun Kim** contributed to the dataset construction and modeling of NER.\n' +
      '\n' +
      '**Lucy Park** contributed to the dataset construction of MRC and provided the original corpus of NSMC.\n' +
      '\n' +
      '**Alice Oh** advised the project, sponsored the project via KAIST, provided feedback and suggested better way to improve the quality of our dataset and models, and helped with the final manuscript.\n' +
      '\n' +
      '**Jung-Woo Ha** advised the project, sponsored annotation cost and computing cloud credits the project via NAVER, provided license-free news articles from The Korea Economy Daily, and helped with the final manuscript.\n' +
      '\n' +
      '**Kyunghyun Cho** advised the project, provided critical feedback, suggested better way to improve the quality of our dataset and models, and helped a lot with polishing and rewriting the final manuscript.\n' +
      '\n' +
      'All participants contributed to this manuscript.\n' +
      '\n' +
      '## Appendix A Dev Set Results\n' +
      '\n' +
      'In order to prevent early saturation of KLUE benchmark, we limit users to submit their models once per day. We thus present the dev set results to provide a reference for future work and local tests, in Table 36. Models we used are same as in test set.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c c} \\hline \\hline  & **VNAT** & **KLUE-STS** & **KLUE-NLI** & **KLUE-NER** & **KLUE-RE** & **KLUE-DP** & **KLUE-MRC** & \\multicolumn{2}{c}{**WoS**} \\\\ \\cline{2-13}\n' +
      '**Model** & F1 & R\\({}^{p}\\) & F1 & ACC & F1\\({}^{E}\\) & F1\\({}^{C}\\) & F1\\({}^{mic}\\) & AUC & UAS & LAS & EM & ROUGE & JGA & F1\\({}^{S}\\) \\\\ \\hline\n' +
      '**mBERfast** & 82.64 & 82.97 & 75.93 & 72.90 & 75.56 & 88.81 & 58.39 & 56.41 & 88.53 & 86.04 & 49.96 & 55.57 & 35.27 & 88.60 \\\\\n' +
      '**XLM-Rcase** & 84.52 & 88.88 & 81.20 & 78.23 & 80.48 & 92.14 & 57.62 & 57.05 & 93.12 & 87.23 & 26.76 & 53.36 & 41.54 & 89.81 \\\\\n' +
      '**XLM-Rcase** & **87.30** & 93.08 & **87.17** & 86.40 & 82.18 & **93.20** & 58.75 & 63.53 & 92.87 & 87.82 & 35.23 & 66.55 & 42.44 & 89.88 \\\\ \\hline\n' +
      '**R-BERfast** & 85.36 & 87.50 & 77.92 & 77.10 & 74.97 & 90.46 & 62.83 & 65.42 & 92.87 & 87.13 & 48.95 & 88.38 & 45.60 & 90.82 \\\\\n' +
      '**KoELECTRBASE** & 85.99 & 32.14 & 85.89 & 86.87 & **86.06** & 22.75 & 62.67 & 57.46 & 90.93 & 87.07 & 59.54 & 65.64 & 39.83 & 88.91 \\\\ \\hline\n' +
      '**KLUE-BERTBASE** & 86.95 & 91.01 & 83.44 & 79.87 & 83.71 & 91.17 & 65.58 & 68.11 & 93.07 & 87.25 & 62.42 & 68.15 & 46.72 & 91.59 \\\\\n' +
      '**KLUE-RoBERTshALL** & 85.95 & 91.70 & 85.42 & 81.00 & 83.55 & 91.20 & 61.26 & 60.89 & 93.47 & 87.50 & 58.28 & 63.56 & 46.65 & 91.50 \\\\\n' +
      '**KLUE-RoBERTBASE** & 86.19 & 92.91 & 86.78 & 86.30 & 83.81 & 91.09 & 66.73 & 68.11 & 93.75 & 87.77 & 69.56 & 74.64 & 47.41 & 91.60 \\\\\n' +
      '**KLUE-RoBERTBLAGCE** & 85.88 & **93.20** & 86.13 & **89.50** & 84.54 & 91.45 & **71.06** & **73.33** & **93.84** & **87.93** & **75.26** & **80.30** & **49.39** & **92.19** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 36: Performances of our pretrained LMs and other baselines on KLUE benchmark dev set. The notations are same with Table 32.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '[MISSING_PAGE_FAIL:1]\n' +
      '\n' +
      '###### Contents\n' +
      '\n' +
      '* 1 소개\n' +
      '	* 1.1 요약\n' +
      '* 2 Source Corpora\n' +
      '	* 2.1 말뭉치 선택 기준\n' +
      '	* 2.2 Selected Corpora\n' +
      '		* 2.2.1 잠재적 관심사\n' +
      '	* 2.3 전처리\n' +
      '	* 2.4 태스크 할당\n' +
      '* 3 KLUE 벤치마크\n' +
      '	* 3.1 토픽 분류(TC)\n' +
      '		* 3.1.1 데이터셋 구성\n' +
      '		* 3.1.2 평가 메트릭\n' +
      '		* 3.1.3 관련 작업\n' +
      '		* 3.1.4 결론\n' +
      '	* 3.2 시맨틱 텍스트 유사성(STS)\n' +
      '		* 3.2.1 데이터셋 구성\n' +
      '		* 3.2.2 평가 메트릭\n' +
      '		* 3.2.3 관련 업무\n' +
      '		* 3.2.4 결론\n' +
      '	* 3.3 NLI(Natural Language Inference)\n' +
      '		* 3.3.1 Dataset 구성\n' +
      '		* 3.3.2 평가 메트릭\n' +
      '		* 3.3.3 관련 업무\n' +
      '		* 3.3.4 결론\n' +
      '	* 3.4 개체명 인식(NER)\n' +
      '		* 3.4.1 데이터셋 구성\n' +
      '		* 3.4.2 평가 메트릭\n' +
      '		* 3.4.3 관련 업무\n' +
      '		* 3.4.4 결론\n' +
      '	* 3.5 관계 추출(RE)\n' +
      '		* 3.5.1 데이터 구성\n' +
      '		* 3.5.2 평가 메트릭\n' +
      '		* 3.5.3 관련 업무\n' +
      '		* 3.5.4 결론\n' +
      '	* 3.6 Dependency Parsing (DP)\n' +
      '		* 3.6.1 데이터셋 구성\n' +
      '		* 3.6.2 평가 메트릭\n' +
      '		* 3.6.3 관련 업무\n' +
      '		* 3.6.4 결론\n' +
      '	* 3.7 MRC(Machine Reading Comprehension)\n' +
      '		* 3.7.1 Dataset Construction\n' +
      '		* 3.7.2 평가 메트릭\n' +
      '		* 3.7.3 분석\n' +
      '		* 3.7.4 관련 업무\n' +
      '		* 3.7.5 결론\n' +
      '	* 3.8 DST(Dialogue State Tracking)\n' +
      '		* 3.8.1 Dataset Construction\n' +
      '		* 3.8.2 평가 메트릭\n' +
      '		* 3.8.3 분석\n' +
      '		* 3.8.4 관련 업무\n' +
      '		* 3.8.5 결론\n' +
      '* 4 사전 학습된 언어 모델\n' +
      '	* 4.1 언어 모델\n' +
      '	* 4.2 기존 언어 모델\n' +
      '* 5 파인튜닝 언어 모델\n' +
      '	* 5.1 태스크별 아키텍처\n' +
      '		* 5.1.1 단일 문장 분류\n' +
      '		* 5.1.2 문장 쌍 분류/회귀\n' +
      '		* 5.1.3 다중 문장 슬롯 값 예측\n' +
      '		* 5.1.4 Sequence Tagging\n' +
      '	* 5.2 미세 조정 구성\n' +
      '	* 5.3 평가 결과\n' +
      '	* 5.4 모델 분석\n' +
      '* 6 윤리적 고려사항\n' +
      '	* 6.1 저작권 및 접근성\n' +
      '	* 6.2 독성 함량\n' +
      '	* 6.3 개인 식별 정보\n' +
      '*7 관련 작업\n' +
      '* 8 토론\n' +
      '* 9 결론\n' +
      '\n' +
      'Index\n' +
      'Introduction\n' +
      '\n' +
      'BERT[30] 및 그 변형들[82; 22; 49] 뿐만 아니라 GPT-3[110] 및 그 변형들[111; 76; 9]과 같은 사전 트레이닝된 언어 모델들의 최근 성공의 주요 요인은 자연 언어 이해(NLU)에서의 그들의 유효성을 평가하기 위한 잘 설계된 벤치마크 스위트들의 가용성이다. GLUE [133] 및 SuperGLUE [132]는 이러한 스위트의 대표적인 예이며 구문, 의미 및 화용론을 포함하여 NLU의 다양한 측면을 평가하기 위해 설계되었다. 연구 커뮤니티는 GLUE와 SuperGLUE를 수용했으며 NLU를 위한 학습 알고리즘뿐만 아니라 더 나은 모델 아키텍처를 개발하는 데 빠르게 발전했다.\n' +
      '\n' +
      'GLUE와 SuperGLUE의 성공은 영어를 넘어서는 언어에서 NLU의 진행을 더 잘 측정하기 위해 다른 언어에 대한 표준화된 벤치마크 제품군을 구축하는 데 관심을 불러일으켰다. 이러한 노력은 두 가지 방향을 따라 추진되어 왔다. 먼저, 세계의 다양한 그룹들은 언어-특정 벤치마크 스위트들을 독립적으로 만들었다; 중국 버전의 GLUE(CLUE[142], 프랑스 버전의 GLUE(FLUE[72], 인도네시아 버전[137], 인도 버전[57] 및 러시아 버전의 SuperGLUE[125]. 한편, 일부는 처음에 영어로 만들어진 벤치마크 스위트의 다국어 버전을 구축하기 위해 기존 벤치마크 스위트의 기계 및 인간 번역에 의존했다. 이들은 예를 들어 XGLUE[78] 및 XTREME[54]를 포함한다. 후자의 접근법은 전자보다 훨씬 더 나은 규모이지만 후자는 종종 NLU의 사회적 측면을 포착하지 못하고 번역에서 발생하는 다양한 아티팩트를 도입한다.\n' +
      '\n' +
      '이를 위해 [34]에 따르면 세계에서 13번째로 많이 사용되는 언어이지만 NLU에 대한 통일된 벤치마크 스위트가 없는 한국어로 NLU를 평가하기 위한 벤치마크 스위트를 새로 구축한다. 기존의 벤치마크 작업이나 말뭉치에서 출발하는 대신, 기본 말뭉치를 결정하고 수집하고, 벤치마크 작업 세트를 식별하고, 적절한 주석 프로토콜을 설계하고, 최종적으로 수집된 주석의 유효성을 검증함으로써 이 벤치마크 제품군을 처음부터 구축한다. 이를 통해 저작권 침해, 주석 인공물, 사회적 편견 및 개인 정보 보호 침해와 같은 바람직하지 않은 결과를 초래할 수 있는 속성을 선제적으로 해결하고 피할 수 있다.\n' +
      '\n' +
      '이 섹션의 나머지 부분에서는 KLUE를 만드는 데 뒤처진 일련의 결정과 원칙을 요약한다.\n' +
      '\n' +
      '### Summary\n' +
      '\n' +
      '한국어 이해 평가(Korean Language Understanding Evaluation, KLUE)를 설계함에 있어서, 우리는 KLUE를 만드는 것을 목표로 한다; 1) 다양한 작업과 말뭉치를 다루고, 2) 모든 사람이 제한 없이 접근할 수 있고, 3) 정확하고 명확한 주석을 포함하고, 4) AI 윤리 문제를 완화한다. KLUE는 잠재적인 _윤리적_ 문제를 사전에 해결했기 때문에 시스템 구축 및 평가 모두에 안전하게 사용할 수 있습니다. 여기서는 이러한 원칙이 작업 선택, 말뭉치 선택, 주석 프로토콜, 평가 메트릭 결정에서 기본 구성까지 KLUE 생성을 어떻게 유도했는지 자세히 설명한다.\n' +
      '\n' +
      '디자인 원리 먼저 각 디자인 원리를 구체적으로 기술해 보자.\n' +
      '\n' +
      '* _다양한 작업 및 말뭉치 포함_: 언어 이해의 다양한 측면을 다루기 위해 뉴스, 백과사전, 사용자 검토, 스마트 홈 쿼리 및 작업 지향 대화, 형식 및 구어체 모두 다양한 스타일을 포함한 다양한 영역을 포함하는 8개의 작업을 선택합니다.\n' +
      '* _제한 없이 모든 사용자에게 액세스할 수 있음_: 벤치마크 제품군이 NLU 시스템을 평가하고 개선하는 데 진정한 지침 역할을 하려면 모든 사용자가 액세스할 수 있어야 합니다. 따라서 우리는 NLU 시스템을 벤치마킹하기 위해 자유롭게 복사, 재분배, 재혼합 및 변환될 수 있는 코퍼스와 리소스만을 사용한다.\n' +
      '* _정확하고 모호하지 않은 주석 가져오기_: 벤치마크 작업의 모호성은 평가의 모호성으로 이어지며, 이는 종종 벤치마크로 측정한 NLU 시스템의 품질과 실제 품질 간의 불일치로 이어집니다. 이러한 불일치를 최소화하기 위해 정확한 주석을 피하기 위해 모든 작업의 주석 지침을 신중하게 설계하고 여러 반복에 걸쳐 개선한다.\n' +
      '* _AI 윤리 문제 완화_: 대규모 언어 모델이 텍스트를 훈련하는 데 사용되는 텍스트에 내재된 사회적 편향을 증폭할 수 있고 종종 증폭한다는 것이 반복적으로 관찰되었다[95]. 이러한 행동을 자극하지 않기 위해 사회적 편향을 반영하는 레이블이 지정되지 않은 말뭉치와 레이블이 지정되지 않은 말뭉치 모두에서 독성 콘텐츠를 포함하고 개인 식별 정보(PII)가 수동 및 자동으로 있는 예를 사전 예방적으로 제거한다. 사회적 편향은 사회적 속성(예를 들어, 성별, 민족성, 종교)에 기초하여 특정 개인 또는 집단에 대한 지나치게 일반화된 판단으로 정의된다. 독성 콘텐츠에는 결과, 성희롱 및 모욕적 표현이 포함된다.\n' +
      '\n' +
      '다양한 과제 선택 우리는 두 가지 목표를 가진 다음 8개의 NLU 과제를 신중하게 선택한다; 1) 한국어로 NLU의 다양한 측면을 포괄하는 것과 2) 과제 간의 중복을 최소화하는 것이다. 이들의 포맷, 평가 세분성 및 다른 속성은 표 1을 참조한다:\n' +
      '\n' +
      '* 토픽 분류(TC): 단일 문장을 단일 클래스로 분류.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:5]\n' +
      '\n' +
      '평가 메트릭 KLUE에서 태스크의 다양성은 각 태스크에 대한 적절한 평가 메트릭 세트를 신중하고 별도로 선택해야 함을 의미한다. 여기서는 태스크를 나열하고 이러한 태스크 각각에 대한 평가 메트릭을 선택하는 방법을 설명한다.\n' +
      '\n' +
      '* _KLUE-TC_ (YNAT)): KLUE-TC를 7개의 클래스가 있는 다중 클래스 분류 문제로 공식화합니다. 헤드라인만으로는 그것이 속하는 적절한 클래스를 정확하게 식별하기에 충분하지 않은 경우가 많기 때문에, 우리는 수동으로 주석을 달고 70,000개의 헤드라인을 유지하는데, 각각에 대해 주석자에 의한 클래스에 대한 과반수의 합의가 있었다. 그런 다음 합의 클래스를 지상 진실 클래스로 사용하고 매크로 F1 점수를 평가 메트릭으로 사용한다.\n' +
      '* _KLUE-STS_: KLUE-STS에서 각 문장 쌍 간의 유사도는 평균(실값) 유사성 등급(0에서 5 사이)으로 주석 처리됩니다. 우리는 두 가지 다른 방식으로 NLU 모델의 품질을 측정한다. 먼저, 실수값과 예측값 사이의 피어슨 상관계수를 사용한다. 둘째, 패러프레이즈 검출과 같이 실수 유사도 등급을 이진화한 후 F1 점수를 계산한다.\n' +
      '* _KLUE-NLI_: SNLI [8] 및 MNLI [138]과 같은 기존 NLI 데이터 세트와 마찬가지로 분류 정확도를 사용하며 균형 잡힌 클래스 분포를 갖도록 KLUE-NLI dev/test 세트를 만들기 때문에 적절합니다.\n' +
      '* _KLUE-NER_: KLUE-NER에서 명명된 개체 인식기는 BIO 태그를 출력하고 감지된 각 개체를 사람, 위치, 조직, 날짜, 시간 및 양의 6가지 유형 중 하나로 분류할 것으로 예상됩니다. 한국어의 풍부한 형태학을 고려하기 위해 개체 수준 및 문자 수준 F1 점수를 사용하여 검출의 품질을 평가하여 각 개체의 유형을 결정하는 인식자의 능력을 평가한다.\n' +
      '* _KLUE-RE_: KLUE-RE는 입력이 두 개의 표시된 엔터티가 있는 단일 문장이고 출력이 30개 유형 중 그들의 관계인 문장 분류 작업으로 설계됩니다. 우리는 두 가지 평가 메트릭을 사용합니다. 첫 번째는 의미 있는 유형(관계 없음 제외)만을 고려한 마이크로 F1 점수이며, 이를 통해 한 쌍의 개체 간의 세밀한 관계를 식별하는 NLU 시스템의 능력을 평가할 수 있다. 두 번째는 AUPRC (precision-recall curve) 아래의 영역으로, 문제의 관계 추출 모델의 품질을 전체적으로 볼 수 있다.\n' +
      '* _KLUE-DP_: 종속성 구문 분석의 표준 관행에 따라 종속성 구문 분석기를 평가하기 위해 레이블이 지정되지 않은 첨부 점수(UAS)와 레이블이 지정되지 않은 첨부 점수(LAS)를 모두 사용합니다. 우리는 공식 텍스트와 비공식 텍스트를 모두 주석하고 사용합니다.\n' +
      '\n' +
      '(각각 뉴스 말뭉치와 구어체 리뷰 말뭉치의 하위 집합) 여러 도메인에서 세밀한 분석을 수행할 수 있습니다.\n' +
      '* _KLUE-MRC_: KLUE-NER과 마찬가지로 KLUE-MRC는 스팬 예측 문제로 프레임됩니다. 기존 데이터 집합과의 비교를 위해 문자 수준의 정확 일치(EM)를 유지하면서, 지상 진실과 예측된 답변 범위 사이의 최장 공통 연속 서브시퀀스(LCCS)를 기반으로 F1 점수를 측정하는 ROUGE-W를 사용하는 것을 제안한다. 후자는 한국어뿐만 아니라 전자도 풍부한 형태학을 다루며 더 해석 가능하다.\n' +
      '* _KLUE-DST_ (서울, WoS의 마법사): KLUE-DST를 다중 문장 슬롯 값 예측 작업으로 공식화하고 두 가지 메트릭을 사용하여 NLU 시스템을 평가합니다. 첫 번째 메트릭은 모든 슬롯이 올바르게 예측되었는지 여부를 측정하는 공동 목표 정확도이고 다른 메트릭은 평균 F1 점수이다. 전자는 모든 슬롯이 올바르게 채워지지 않은 모든 예를 처리하기 때문에 유사하게 수행되는 NLU 시스템을 구별하지 못하는 경우가 많다. 우리는 공동 골 정확도와 슬롯 F1 점수를 모두 보고함으로써 이러한 단점을 해결한다. 또한 미세 입자 분석을 용이하게 하기 위해 여러 영역을 사용하여 구축한다.\n' +
      '\n' +
      '베이스라인 벤치마크 제품군을 만드는 것 외에도 대규모 사전 훈련 언어 모델을 기반으로 한 강력한 베이스라인 세트를 구축하고 공개적으로 출시합니다. 적절한 시기에 우리는 우리 자신을 위한 대규모 언어 모델을 사전 훈련하고 출시하며, 이는 개별 연구자로부터 이러한 대규모 모델을 재훈련하는 부담을 줄일 것이다. 또한 제안된 KLUE 벤치마크에 대한 추가 통찰력을 얻기 위해 자체 모델 외에도 기존의 여러 다국어 사전 훈련 언어 모델과 오픈 소스 한국어 특정 모델을 사용한다. 우리는 표 32의 모든 결과를 제시하고 여기에서 몇 가지 흥미로운 관찰을 요약한다. 먼저, 한국어 특화 언어 모델은 일반적으로 다국어 모델을 능가한다. 둘째, KLUE-BERT는 YNAT와 WoS에 대해, KLUE-RoBERTa는 KLUE-RE와 KLUE-MRC에 대해, KoELECTRABASE는 KLUE-STS와 KLUE-NLI에 대해, KLUE-BERT는 YNAT와 WoS에 대해 가장 우수한 성능을 보인다. 셋째, 모델의 크기가 커질수록 KLUE-RoBERTaLARGE는 KLUE-NER 이외의 모든 작업에서 다른 모든 모델보다 우수한 성능을 보인다. 마지막으로, PII를 제거하는 것은 다운스트림 태스크 성능에 최소한의 영향을 미치며, 형태소 기반 서브워드 토큰화 기법인 토큰화 기법은 형태소 수준에서 태깅, 탐지 및 생성과 같은 태스크에 효과적이다.\n' +
      '\n' +
      '작업 개요 표 1에서는 유형, 형식, 평가 메트릭 및 주석이 달린 데이터 크기와 같은 중요한 속성을 나열하는 결과 8개의 KLUE 작업을 요약합니다. 나머지 논문에서 우리는 이러한 모든 작업이 훨씬 더 자세히 구성된 과정을 살펴볼 것이다.\n' +
      '\n' +
      'Source Corpora\n' +
      '\n' +
      '우리는 벤치마크를 설정하는 데 있어 일반적인 관행인 기존 데이터 세트를 통합하는 대신 처음부터 KLUE를 구축한다. 사용 가능한 텍스트 리소스를 조사하고 프로세스를 문서화하여 일부 말뭉치를 선택하는 방법과 이유에 대해 더 잘 이해하도록 한다. 최근에 제안된 문서화 프레임워크인 _데이터시트_[41] 및 _데이터 문_[6]을 채택합니다. 이러한 프레임워크를 기반으로 프로토콜을 신중하게 설명하기 위해 더 많은 정보를 문서화하고 제공한다.\n' +
      '\n' +
      '### Corporor 선택 기준\n' +
      '\n' +
      '우리는 작업별 말뭉치가 파생되고 주석이 달린 소스 말뭉치를 구축하기 위해 말뭉치 집합을 소싱할 때 두 가지 기준을 고려한다. 첫 번째 기준은 접근성입니다. KLUE의 주요 목적은 미래의 NLP 연구 개발을 촉진하는 것이므로 KLUE가 모든 사람에게 가능한 한 자유롭게 사용하고 공유할 수 있는 데이터가 제공되도록 한다. 두 번째 기준은 품질과 다양성입니다. 우리는 낮은 품질의 텍스트를 제거함으로써 이러한 말뭉치에 대한 각 예가 특정 품질인지 확인하고 이러한 말뭉치 내에서 공식 텍스트와 구어체 텍스트 사이의 균형을 충족한다.\n' +
      '\n' +
      'Wang et al. [132], Hu et al. [54], Kakwani et al. [57]과 달리, 우리는 KLUE의 사용 목적뿐만 아니라 사용자의 소속성에 대한 제한을 피함으로써 가능한 광범위하고 다양한 연구자에게 도달하도록 설계한다. 또한, NLU의 표준 벤치마크로서 KLUE의 사용성을 연장하기 위해 KLUE를 재생산하고 재분배할 수 있는 가능성을 확인하였다. 이를 위해 CC BY-SA.4를 사용하여 소스 코퍼스를 구축하고 공개한다.\n' +
      '\n' +
      '각주 4: [https://creativecommons.org/licenses/by-sa/4.0/](https://creativecommons.org/licenses/by-sa/4.0/)\n' +
      '\n' +
      '소스 코퍼스, 또는 소스 코퍼스의 세트는 다음의 조건들을 만족한다:\n' +
      '\n' +
      '* **사용에 대한 제한 없음:** 산업 연구소의 최근 기본 연구 추세를 수용하기 위해 KLUE의 비상업적 및 상업적 사용을 모두 허용합니다.\n' +
      '* **파생 변수:** 사용자가 KLUE의 모든 부분을 자유롭게 재구성하여 첫 번째로 예상치 못한 아티팩트, 윤리적 문제 및 주석 오류와 같은 단점을 해결하고 두 번째로 미래에 대해 더 어려운 벤치마크를 도출할 수 있습니다. 이는 SQuAD 1.1[112]를 포함하도록 생성된 SQuAD 2.0[113]로 수행된 것과 유사하다.\n' +
      '* **재분배 가능:** KLUE의 원본 작성자에게 적절한 속성이 부여되는 한 모든 채널을 통해 KLUE 벤치마크 데이터 세트를 배포할 수 있습니다. 우리는 제한적이고 선별된 연구자 그룹만이 자원에 대한 독점권을 가지고 궁극적으로 전반적인 진행을 방해하는 상황을 피하기 위해 의도적으로 이러한 결정을 내린다. 이는 기존의 한국 말뭉치 중 일부가 제한적 정책과 맞물려 파생상품과 재분배를 막는 경우가 많고, 국내 공공기관인 말뭉치 출판사로부터 허가를 획득한 후 국내 연구자만 접근할 수 있는 것에 대한 대응이다. KLUE는 한국 NLP의 발전을 최대한 촉진하기 위해 이러한 예방 정책을 피한다.\n' +
      '\n' +
      '기존 데이터 세트의 대부분은 이러한 조건을 충족하지 않기 때문에 우리는 CC0,5 CC BY,6 CC BY-SA,7 및 KOGL 유형 1,8과 같은 유사한 라이선스가 한국의 최신 저작권법에 따라 저작권법으로 보호되지 않거나 계약에 따라 저작권자가 명시적으로 제공한 리소스만 고려하여 소스 코퍼스를 처음부터 선별한다. 우리는 총 20개의 후보 말뭉치를 얻게 되며, 그 중 부분집합은 KLUE의 소스 말뭉치 집합을 형성하기 위해 선택된다. 그것들은 표 2에 나열되어 있다.\n' +
      '\n' +
      '각주 5: [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)\n' +
      '\n' +
      '각주 6: [https://creativecommons.org/licenses/by/4.0/)](https://creativecommons.org/licenses/by/4.0/)\n' +
      '\n' +
      '각주 7: [https://creativecommons.org/licenses/by-sa/4.0/](https://creativecommons.org/licenses/by-sa/4.0/)\n' +
      '\n' +
      '각주 8: [https://www.kogl.or.kr/info/license.do#05-tab](https://www.kogl.or.kr/info/license.do#05-tab)\n' +
      '\n' +
      '각주 9: 2020년 12월 8일 현재 유효한 저작권법은 [https://www.law.go.kr/KEB/%2%95%EB%AA%B9/%EC%AA0%0%EC%9E%91%EA%BE%AC%EB%2%95](https://www.law.go.kr/KEB/%2%95%EB%AA%B9/%EC%AA0%0%EC%9E%91%EA%BE%AC%EB%2%95)를 참조하세요.\n' +
      '\n' +
      '이 20개의 소스 코퍼스 중에서 10개의 코퍼스의 하위 집합을 선택하여 소스 코퍼스를 구성하고 KLUE 벤치마크를 구축한다. 이를 위해 1) 말뭉치가 좁은 영역(다양성)에 국한되어서는 안 되고, 2) 말뭉치를 현대 한국어(품질)로 작성해야 하며, 3) 말뭉치가 프라이버시 또는 독성에 관한 내용(품질)에 의해 지배되어서는 안 되며, 4) 말뭉치는 8가지 벤치마크 과제 중 적어도 하나에 대한 주석을 준수해야 한다. 또한, 형식적 용도와 구어적 용도를 모두 포괄할 말뭉치의 하위 집합을 선택한다.\n' +
      '\n' +
      '최종 소스 코퍼카는 이러한 기준과 결정에 따라 뉴스 헤드라인, 위키피디아, 위키뉴, 정책뉴스, 한국경제일보, 아크로판뉴스를 (상대적으로) 정식 텍스트로 선택한다. 10 더 많은 구어체 텍스트를 위해 ParaKQC, Airbnb Reviews, 네이버 감성 영화 코퍼스를 사용한다. 이것들은 표 2에 굵게 표시되어 있다.\n' +
      '\n' +
      '각주 10: 위키트리가 비윤리적, 사회적 편향 및/또는 일반적으로 품질이 낮다고 간주될 수 있는 일부 콘텐츠를 포함하는 것으로 밝혀졌지만, 위키트리가 무면허 뉴스 기사의 가장 큰 출처이기 때문에 이를 포함한다. 우리는 이러한 문제가 있는 내용을 주석을 통해 해결합니다.\n' +
      '\n' +
      '### Selected Corpora\n' +
      '\n' +
      '여기서는 각 소스 말뭉치의 일반적인 특성과 잠재적인 문제에 대해 더 자세히 설명한다. 각 코퍼스의 수집 메커니즘, 시간, 도메인, 스타일, 라이선스 및 배경도 문서화합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline\n' +
      '**Dataset** & **License** & **Domain** & **Style** & \\begin{tabular}{c} **Ethical** \\\\ **Risks** \\\\ \\end{tabular} & **Volume** &\n' +
      '\\begin{tabular}{c} **Contemporary** \\\\ **Korean** \\\\ \\end{tabular} \\\\ \\hline\n' +
      '**News Headlines** & **N/A** & **News (Headline)** & **Formal** & **Low** & **Large** & **o** \\\\ Judgments & Public Domain & Law & Formal & Low & Large & o \\\\ National Assembly & Public Domain & Politics & Colloquial & Medium & Large & o \\\\ Patents & Public Domain & Patent & Formal & Low & Large & o \\\\ \\hline\n' +
      '**Wikipedia** & **CC BY-SA 3.0** & **Wikipedia** & **Formal** & **Low** & **Large** & **o** \\\\ Wikibooks & CC BY-SA 3.0 & Book & Formal & Low & Medium & x \\\\ Wikisource & CC BY-SA 3.0 & Law & Formal & Low & Medium & x \\\\\n' +
      '**Wikinews** & **CC BY 2.5** & **News** & **Formal** & **Low** & **Small** & **o** \\\\\n' +
      '**Wikitree** & **CC BY-SA 2.0** & **News** & **Formal** & **Medium** & **Large** & **o** \\\\ Librewiki & CC BY-SA 3.0 & Wiki & Formal & Medium & Large & o \\\\ Zetawiki & CC BY-SA 3.0 & Wiki & Formal & Medium & Large & o \\\\\n' +
      '"정책 뉴스" & "KOGL 유형 1" & "뉴스" & "형식" & "저" & "중" & "o" \\\\ NIKL 표준 & CC BY-SA 2.0 & 사전 & 형식 & 저 & 대형 & o \\\\ 한국 사전 & CC BY-SA 2.0 & 사전 & 형식 & 저 & 대형 & o \\\\\n' +
      '**ParaKQC** & **CC BY-SA 4.0** &\n' +
      '\\begin{tabular}{c} **Smart Home** \\\\ **Uterances** \\\\ \\end{tabular} & **Colloquial** & **Low** & **Medium** & **o** \\\\\n' +
      '**Airbnb Review** & **CC0 1.0** & **Review** & **Colloquial** & **Medium** & **Large** & **o** \\\\\n' +
      '**NAVER Sentiment** & **CC0 1.0** & **Review** & **Colloquial** & **Medium** & **Large** & **o** \\\\\n' +
      '**영화 코퍼스(NSMC)** & **CC BY-SA 4.0** & Review & Colloquial & High & Large & o \\\\ NAVER Entertainment & CC BY-SA 4.0 & **\n' +
      '\\begin{tabular}{c} **CC BY-SA 4.0** \\\\ **for KLUE-MRC by Contract** \\\\ \\end{tabular} & **News** & **Formal** & **Low** & **Large** & **o** \\\\\n' +
      '**한국경제학**\n' +
      '\\begin{tabular}{c} **CC BY-SA 4.0** \\\\ **for KLUE-MRC by Contract** \\\\ \\end{tabular} & **News** & **Formal** & **Low** & **Large** & **o** \\\\\n' +
      '**Daily News** & **for KLUE-MRC by Contract** & **News** & **Formal** & **Low** & **Large** & **o** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 수집된 소스 말뭉치. 첫 번째 섹션의 말뭉치는 저작권법에 의해 보호되지 않습니다. 구체적으로, _뉴스 헤드라인_은 창의성이 부족하여 저작물로 분류되지 않으며, _판정_은 제7조 제3항에 따라 보호되지 않습니다. _국회 회의록_ 및 국회에서 만들어진 _특허_는 제24조 제2항에 따라 저작권법을 적용하지 않습니다. 두 번째 섹션은 허용 라이센스에 따른 말뭉치 모음입니다. 마지막 섹션 말뭉치인 KED와 아크로판은 원래 파생 작품을 만드는 것이 금지되어 있지만, 독점 계약으로 이러한 조건을 해제합니다. 열인 _Volume_ 의 경우 _Small_ 을 1k 미만의 코퍼스 크기로, _Medium_ 을 1k에서 50k 사이, _Large_ 를 50k 이상으로 나타냅니다. 볼드는 KLUE 벤치마크를 구축하기 위한 최종 소스 코퍼라를 나타냅니다.\n' +
      '\n' +
      'YNA의 뉴스 헤드라인입니다\n' +
      '\n' +
      'YNA는 한국의 대표적인 통신사 중 하나인 Yna합뉴스의 뉴스 헤드라인 데이터셋이다. 뉴스 헤드라인을 사용하는 것은 뉴스 기사의 실제 내용과 달리 저작권을 침해하지 않는다. 우리는 단일 문장 분류 작업에 사용하는 것을 주요 목적으로 2016년부터 2020년까지 YNA를 포함한다.\n' +
      '\n' +
      'Wikipedia (WIKIPEDIA)WIKIPEDIA는 정형화된 문체로 작성된 개방형 백과사전이며, 고품질 및 잘 정리된 텍스트 때문에 많은 언어에 걸쳐 언어 모델링 및 데이터 세트 구성에 널리 사용되었다. 한국어로 된 위키피디아 기사는 CC BY-SA에 의해 공개된다. 2020년 12월 1일에 출시된 한국 위키피디아 덤프를 사용합니다.\n' +
      '\n' +
      'Wikinews (WIKINEWS)WIKINEWS는 집단 저널리즘을 구현하고 CC BY 하에서 뉴스 기사를 무료로 제공하며, 둘 다 뉴스 기사에 대해 드물다. 이러한 특성으로 인해 제한된 수의 기사(그 중 약 500개)에도 불구하고 소스 코퍼라에 포함한다.\n' +
      '\n' +
      '위키트리(WIKITREE) 위키트리(WIKITREE)는 2010년부터 시작된 국내 최초의 소셜 미디어 기반 뉴스 플랫폼인 위키트리(Wikitree)에서 파생된 뉴스 기사 데이터 세트이다. 위키트리에 대한 기사는 많은 경우 위장 광고 또는 클릭 미끼 헤드라인이고 바람직하지 않은 편향을 표현한다는 우려가 있지만, CC BY-SA에 따라 자유롭게 배포되는 유일한 대규모 뉴스 출처이기 때문에 위키트리(WIKITREE)를 포함한다. 또한 정치, 경제, 문화 및 삶을 포함한 광범위한 주제를 다룹니다. 우리는 2016년부터 2020년 사이에 출판된 기사를 활용하며, 위키트리(WIKITREE)에 대한 보다 철저한 수동 검사를 수행한다. 자세한 내용은 섹션 2.2.1을 참조하십시오.\n' +
      '\n' +
      '정책 뉴스(Policy News, POLICY)POLICY는 한국의 부처, 국가 사무실 및 국가 위원회가 배포하는 다양한 기사의 데이터 세트이다. 정부 기관에서 보고한 성명, 통지 또는 언론 노트를 다룹니다. POLICY는 귀속이 제대로 이뤄지면 상업적 목적이라도 공유·리믹스할 수 있는 코리아오픈 정부 라이센스(KOGL) 제1유형에 따라 보호된다. 2020년 말까지 발표된 기사를 포함합니다.\n' +
      '\n' +
      'ParaKQC(PARAKQC)PARAKQC는 10개의 유사한 질의들 중 1,000개의 의도들로 구성된, 스마트 홈 디바이스들을 겨냥한 10,000개의 발언들의 데이터세트이다[18]. 약속 잡기, 날씨 문의 등 스마트 홈 스피커와 상호 작용할 때 가능한 다양한 주제를 다룹니다. PARAKQC는 CC BY-SA에서 사용할 수 있습니다.\n' +
      '\n' +
      '에어비앤비 리뷰(AIRBNB)AIRBNB는 에어비앤비 웹사이트의 공개적으로 액세스 가능한 부분으로부터 소싱된 리뷰 데이터세트이다. 보다 구체적으로, 우리는 Inside Airbnb에서 수집 및 전처리한 기존의 다국어 Airbnb 리뷰에서 시작한다. 11 우리는 정규 표현을 사용하여 이 다국어 Airbnb 말뭉치에서 한국어로 작성된 리뷰의 하위 집합을 식별한다. 숙박을 마친 호스트와 게스트의 리뷰입니다. AIRBNB는 CC0에서 사용할 수 있습니다.\n' +
      '\n' +
      'Footnote 11: [http://insideairbnb.com/get-the-data.html](http://insideairbnb.com/get-the-data.html)\n' +
      '\n' +
      '네이버 감성 영화 코퍼스(NSMC)NSMC는 네이버 영화에서 스크래핑한 영화 리뷰 데이터셋이다. 12 리뷰는 온라인 사용자가 작성한 것이다. 각 리뷰에는 텍스트 내용과 이진 감정 레이블이 함께 제공됩니다. 총 20만 개의 리뷰가 있습니다. 긍정적인 검토자와 부정적인 검토자의 수는 균형을 이룬다. NSMC는 CC0에서 사용할 수 있습니다.\n' +
      '\n' +
      '발음 12: [https://movie.naver.com/movie/point/af/list.nhn](https://movie.naver.com/movie/point/af/list.nhn)\n' +
      '\n' +
      '아크로판 뉴스(ACROFAN) 아크로판은 아크로판이 발표한 뉴스 기사로 구성된 코퍼스다. 대부분의 기사는 기업의 신제품이나 이벤트를 소개하는 경우가 많다는 점에서 보도자료와 유사하다. 기사에는 자동차, IT, 스타트업, 대기업, 에너지, 뷰티 및 패션을 포함한 광범위한 범주가 포함되지만 형식과 스타일은 상당히 템플릿화되어 있습니다. 우리는 KLUE에 대한 ACROFMAN으로부터 기사의 허가 및 사용을 얻는다. 2020년 12월부터 2021년 1월 사이에 발행된 뉴스 기사를 포함합니다.\n' +
      '\n' +
      '한국경제신문(한국경제신문) 한국경제는 한경공사가 소유한 한국경제신문 기사로 구성된 뉴스 코퍼스다. 한국경제신문은 경제 이슈를 주로 다루는 신문이지만 정치, 문화, IT 주제 등 다양한 주제를 발행하기도 한다. 한국경제신문 사장과 우리는 2013년 1월부터 2015년 12월 사이에 한경공사가 제공한 뉴스 기사를 KLUE의 일부로 사용하기로 계약을 체결했다. 이를 통해 고품질의 잘 정리된 뉴스 기사가 KLUE에 포함되도록 할 수 있다. 우리는 이 기사들이 기계 학습 연구의 목적으로 사용된다는 조건으로 CC BY-SA에 따라 한국 경제 일보를 발표한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:11]\n' +
      '\n' +
      '예측점수는 \\(0.9\\) 이상이었다. 임계값은 각 말뭉치에 대해 수동으로 결정된다. 이 접근법은 한국 혐오 발언 데이터 세트가 온라인 리뷰를 사용하여 구성되었기 때문에 리뷰와 같은 온라인 텍스트에 대해 잘 작동한다. 그러나 한국 경제, ACROFAN 및 YNA에서 이 전략을 사용하지 않기로 결정한 뉴스 기사와 같은 더 공식적인 텍스트에는 적합하지 않습니다.\n' +
      '\n' +
      'PII 제거 잠재적인 개인 정보 문제를 완화하기 위해 개인 정보가 포함된 문장을 제거합니다. 이메일 주소, URL 및 \'@길동\'과 같은 사용자 멘션 키워드와 일치하는 정규식을 사용하여 이러한 문장을 검출한다.\n' +
      '\n' +
      '### Task Assignment\n' +
      '\n' +
      '이러한 소스 말뭉치를 사용하여 DST를 제외한 7개의 KLUE 작업에 대한 데이터 세트를 구축한다. DST는 크라우드 워커가 시뮬레이션한 대화 상자에서 만들어지며 오프라인 텍스트에 액세스할 필요가 없습니다. 각 다운스트림 작업에 대해 아래에 설명된 대로 소스 말뭉치의 하위 집합을 사용합니다.\n' +
      '\n' +
      '* 토픽 분류(TC): 단일 문장 토픽 분류 작업에 대해 널리 연구된 YNA를 사용합니다.\n' +
      '* 시맨틱 텍스트 유사성(STS): AIRBNB, POLICY 및 PARAKQC를 사용하여 다양한 시맨틱 컨텍스트를 포함한다. PARAKQC의 의도 질의 및 토픽 정보는 의미적으로 관련된 문장 쌍을 생성할 때 유용하다.\n' +
      '* 자연어 추론(NLI): MNLI[138]에 따라 여러 소스를 사용하여 NLI를 구성합니다. 우리는 위키트리, 폴리시, 위키네우스, 위키페디아, NSMC 및 AIRBNB를 사용한다.\n' +
      '* 개체 이름 인식(NER): NER의 특성상 (이름) 개체가 자주 나타나는 코퍼스를 구축해야 합니다. 따라서 우리는 공식 및 비공식 쓰기 스타일을 모두 포함할 수 있는 WIKITREE와 NSMC를 사용한다.\n' +
      '* 관계 추출(RE): WIKIPEDIA, WIKITREE 및 POLICY를 사용합니다. 이러한 말뭉치는 공인의 이름과 다양한 조직과의 관계가 있는 긴 완전한 문장을 갖는 경향이 있다.\n' +
      '* 종속 구문 분석 (DP): 형식 및 구어체 쓰기 스타일의 균형을 유지 하 고 선택한 말뭉치에서 대부분의 문장이 완료 되도록 합니다. 우리는 결국 WIKITREE와 AIRBNB를 사용하게 된다. 우리는 NSMC보다 AIRBNB를 선택하는데, 왜냐하면 전자는 더 나은 문장을 가지고 있기 때문이다.\n' +
      '* MRC(Machine Reading Comprehension): 유익한 구절을 제공하기 위해 WIKIPEDIA, The Korea Economy Daily, ACROFAN을 사용합니다.\n' +
      '\n' +
      'KLUE Benchmark\n' +
      '\n' +
      'KLUE의 목표는 고품질의 평가 데이터 세트와 적절한 자동 메트릭을 제공하여 시스템의 한국어 이해 능력을 테스트하는 것이다. 8개의 벤치마크 데이터 세트를 구성하는 방법에 대한 포괄적인 세부 정보를 제공합니다. 1) 소스 코퍼스 선택의 배경, 2) 주석 프로토콜, 3) 주석 프로세스, 4) 데이터세트 분할 전략, 5) 메트릭의 설계 프로세스를 문서화한다. 주석 프로세스에서 우리는 작업자에게 잠재적인 윤리적 문제가 포함된 텍스트를 식별하도록 안내한다. 편향, 증오 및 PII에 대한 정의는 섹션 1.1을 참조하십시오.\n' +
      '\n' +
      '### 토픽 분류(TC)\n' +
      '\n' +
      '토픽 분류(TC)에서, 목표는 주어진 텍스트 스니펫의 토픽을 예측하기 위해 분류기를 훈련시키는 것이다. 토픽 분류 데이터 세트는 일반적으로 뉴스 또는 위키피디아 기사 및 그 미리 정의된 카테고리로 구성되며, 그 카테고리는 종종 토픽을 나타내기 때문이다[151].\n' +
      '\n' +
      '텍스트의 주제를 추론하는 것은 언어 이해 시스템이 가져야 하는 핵심 역량이기 때문에 KLUE 벤치마크에 TC를 포함한다. 전형적인 단일 문장 분류 작업으로서, CLUE[142] 및 IndicGLUE[57]과 같은 다른 NLU 벤치마크들도 TNEWS 및 뉴스 카테고리 분류를 포함한다. 한국어의 경우 작업에 대한 데이터 세트가 제안되지 않았으며, 이는 첫 번째 한국어 주제 분류 벤치마크를 구성하도록 동기를 부여한다.\n' +
      '\n' +
      '이 작업에서 뉴스 헤드라인이 주어지면 텍스트 분류기는 {정치, 경제, 사회, 문화, 세계, IT/과학, 스포츠} 중 하나인 주제를 예측해야 한다. TC는 이전 연구에 이어 단일 문장 분류 작업으로 공식화하고 매크로-F1 점수를 평가 척도로 사용한다.\n' +
      '\n' +
      '#### 3.1.1 Dataset Construction\n' +
      '\n' +
      '우리의 TC 벤치마크는 세 단계로 구성된다. 먼저, 헤드라인 및 해당 카테고리를 수집하고, 카테고리를 보지 않고 주제에 주석을 달며, 데이터 세트는 출판 날짜 및 용어 출현을 고려하여 훈련, 개발 및 테스트 분할로 분할을 정의하여 최종화한다.\n' +
      '\n' +
      '출처코퍼라위 국내 최대 통신사인 YNA가 배포한 온라인 기사에서 뉴스 헤드라인을 수집한다. 구체적으로 네이버 뉴스.17에서 2016년 1월부터 2020년 12월까지 게재된 기사들의 헤드라인을 수집한다. 이 기사들은 정치, 경제, 사회, 문화, 세계, IT/과학, 스포츠의 7개 섹션 중 하나에 속한다. 서로 다른 섹션에 걸친 데이터의 균형을 맞추기 위해 스포츠 및 IT/과학 섹션을 제외하고 각 섹션에서 10,000개의 기사를 무작위로 샘플링한다. 우리는 9,000개의 스포츠 기사와 11,000개의 IT/과학 기사를 수집합니다.\n' +
      '\n' +
      '발음 17: [https://news.naver.com/](https://news.naver.com/)\n' +
      '\n' +
      'CLUE [142] 또는 AG News [151]의 TNEWS와 같은 다른 벤치마크와 달리, 우리는 저작권 침해를 피하기 위해 기사의 내용을 제외한다. 콘텐츠는 저작물로 보호되기 때문에 허가 없이 자유롭게 사용할 수 없습니다. 반면에 헤드라인은 법적 판례[23]에 따라 저작권이 있는 저작물로 간주되지 않는다.\n' +
      '\n' +
      '주석 프로토콜 각 기사의 표제는 주요 내용을 모두 반영하지 않을 수 있으므로 표제의 _주제_가 기사의 원래 뉴스 섹션과 다를 수 있다. 헤드라인과 해당 기사 사이의 이러한 격차를 해결하기 위해 헤드라인의 주제에 수동으로 주석을 달았다.\n' +
      '\n' +
      '우리는 헤드라인의 주제에 주석을 달기 위해 한국의 크라우드소싱 플랫폼인 SelectStar,18을 사용합니다. 각 헤드라인에 대해 세 명의 주석자가 서로 독립적으로 주제를 레이블링합니다. 각 주석자는 7개의 범주 중 관련성 순서로 최대 3개의 주제를 선택한다. 정확한 주석을 위해 각 주제의 _키 용어_도 주석자에게 제시합니다. 용어는 <표 3>과 같이 네이버 뉴스 플랫폼의 해당 토픽의 하위 항목이다.\n' +
      '\n' +
      '각주 18: [https://selectstar.ai/](https://selectstar.ai/)\n' +
      '\n' +
      '주석자는 헤드라인에 적절한 범주를 식별하기에 충분한 정보가 포함되어 있지 않은 경우 _결정할 수 없음_ 을 선택할 수 있습니다. 그러한 예가 바로 "김영수에게 감사패를 수여한다."이다. 이 헤드라인에는 ‘김영수’가 누구인지, 왜 감사패를 수여하고 있는지에 대한 단서가 없다.\n' +
      '\n' +
      '우리는 근로자들에게 개인 식별 정보(PII)를 포함하거나 사회적 편견을 표현하거나 혐오 발언을 포함하는 모든 헤드라인을 보고하도록 요청한다. 보고된 헤드라인은 수동으로 검토한 후 폐기합니다.\n' +
      '\n' +
      '주석 프로세스 주 주석 프로세스를 시작하기 전에 작업자를 선택하기 위해 파일럿 연구를 실행합니다. 시범 단계에서 지속적으로 주제를 할당하지 못하거나 다른 작업자와 합의하지 못한 작업자는 제외한다. 그 결과 13명의 근로자가 이 단계의 파일럿 연구를 통과했다.\n' +
      '\n' +
      '주 주석에서 13명의 선택된 작업자는 모든 70,000개의 헤드라인에 대한 주제를 표시했다. 주석 동안 잠재적인 PI(0.93%), 194개의 독성 내용(0.28%) 및 2,515개의 _결정할 수 없는_(3.59%)를 포함하여 650개의 헤드라인을 보고했다. 우리는 먼저 그러한 무효한 2,953개의 헤드라인을 제외한다. 문제가 되는 세 가지 유형의 헤드라인의 합은 이들 간의 교차로 인해 총 값보다 크다. 필터링 후 67,047개의 헤드라인이 남아 있습니다.\n' +
      '\n' +
      '우리는 유효한 헤드라인에서 세 명의 주석자 간의 합의를 살펴본다. 우리는 세 명의 주석자가 선택한 첫 번째 관련 주제 각각을 고려한다. 40,359개(60.5%)의 헤드라인에서 세 명의 주석자가 모두 단일 주제에 동의합니다. 2만 3,353명(34.8%)이 과반수 득표율을 보였고 나머지 3,155명(4.7%)은 합의에 이르지 못했다. 헤드라인을 단일 주제로 분류하기 위해 다른 헤드라인을 제거하고 63,892개의 헤드라인을 남긴다.\n' +
      '\n' +
      '우리는 주석자 내에서 두 번째 및 세 번째 관련 주제를 조사한다. 48,885개(69.8%)의 헤드라인에 대해 3명의 주석자는 두 번째와 세 번째로 가장 관련성이 높은 주제를 선택하지 않았다. 헤드라인의 5,088개(7.3%)만이 3개의 주석기에서 두 번째 주제를 가지고 있다. 따라서 우리는 헤드라인이 주석자 내의 첫 번째 관련 주제에 의해 충분히 표현된다고 가정한다.\n' +
      '\n' +
      '따라서 우리는 3개 중 적어도 2개의 주석자가 선택한 각 헤드라인에 대해 단일 주제만 유지한다. 결과 63,892개의 헤드라인에 대한 주석자 동의는 상당히 높다(Krippendorff\'s \\(\\alpha=0.713\\)) [67].\n' +
      '\n' +
      '최종 데이터셋 YNAT(Yonhap News Agency dataset for Topic classification)라는 최종 데이터셋을 학습, 개발, 테스트 세트로 분할한다. 출판 날짜를 기준으로 데이터 세트를 분할합니다. 2020년 이후에 발표된 헤드라인은 개발 및 테스트 세트에, 2020년 이전에 발표된 헤드라인은 훈련 세트에 포함한다. TC 모델이 특정 키워드에 참석하여 헤드라인을 분류하는 것을 방지하기 위해 기차 세트에 나타나지 않은 용어가 포함된 헤드라인을 개발 및 테스트 세트에 포함한다. 표 3에 나타난 바와 같이, 열차, 개발 및 테스트 세트는 각각 45,678, 9,107 및 9,107의 예로 구성된다.\n' +
      '\n' +
      '#### 3.1.2 평가 메트릭\n' +
      '\n' +
      'YNAT에 대한 평가 메트릭은 매크로 F1 점수이다. 거시 F1 점수는 토픽별 F1 점수의 평균으로 정의되며, 각 토픽에 동일한 중요성을 부여한다. 주제별 F1 점수는 가중치 재현율과 정밀도를 동등하게 평가한다.\n' +
      '\n' +
      '#### 3.1.3 Related Work\n' +
      '\n' +
      '많은 토픽 분류 데이터 세트가 다양한 언어로 제안되었지만, 우리는 한국의 공개 TC 벤치마크에 대해 알지 못한다. 영어로 주제 분류를 위해 널리 사용되는 벤치마크인 AG 뉴스[151]는 뉴스 검색 엔진 ComeToMyHead,19에서 수집된 백만 개 이상의 뉴스 기사로 구성되며 기사를 세계, 스포츠, 비즈니스, 과학/기술의 4개 섹션으로 분류한다. 보다 최근에는 영어 이외의 언어로 된 TC 벤치마크 데이터셋이 다수 제안되었다. IndicGLUE[57]는 인도어로 뉴스 장르 분류를 포함하며, 여기서 목표는 뉴스 기사 또는 뉴스 헤드라인을 엔터테인먼트, 스포츠, 비즈니스, 7개의 카테고리로 분류하는 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c} \\hline \\hline\n' +
      '**Topic** & **Key Terms** & **lTrain** & **lDevl** & **lTestl** & **Total** \\\\ \\hline Politics & Blue House, Ministry, Parliament, North Korea & 7,379 & 750 & 722 & 8,851 \\\\  & Political parties, Defense, Diplomacy & & 6,118 & 1,268 & 1,348 & 8,734 \\\\ Economy & Stock, Finance, Industry Enterprise, Real estate & 5,133 & 3,740 & 3,701 & 12,574 \\\\ Society & Education, Labor, Journalism & 5,751 & 1,387 & 1,369 & 8,507 \\\\  & Environment, Human rights, Food and drugs & & 5,751 & 1,387 & 1,369 & 8,507 \\\\ Culture & Health, Transportation, Leisure, Hot places, Fashion, Beauty, Performance, Exhibition, Books, Weather & & 8,320 & 776 & 835 & 9,931 \\\\ World & Asia/Australia, America, Europe, Middle East/Africa & & 5,235 & 587 & 554 & 6,376 \\\\ IT/Science & Mobile, IT, Internet Social media, Communication & & 7,742 & 599 & 578 & 8,919 \\\\ Sports & Baseball, Basketball, Volleyball, E-sports & & **45,678** & **9,107** & **9,107** & **63,892** \\\\ \\hline\n' +
      '**Total** & & & & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 각 범주의 핵심 용어를 제공한 YNAT(KLUE-TC)의 최종 통계량이다.\n' +
      '\n' +
      '생활 방식, 기술, 정치, 범죄 CLUE [142]의 TNEWS는 만다린의 뉴스 토픽 분류 작업으로, 토티아오에서 발행된 15개의 뉴스 카테고리와 함께 73K개의 제목으로 구성된다.\n' +
      '\n' +
      'TC 벤치마크에 대해 미세 조정된 대규모 언어 모델은 IndicGLUE[57]에서와 같이 100% 정확도에 근접할 수 있기 때문에, 일부 연구자들은 개선의 여지를 남기기 위해 도전적인 TC 벤치마크를 만드는 데 중점을 둔다. CLUE [142]는 4중 교차 검증을 사용하여 TNEWS에서 쉬운 예를 필터링한 다음 데이터 세트를 무작위로 섞고 분할합니다. 벤치마크를 인위적으로 더 어렵게 설계하는 대신, 벤치마크에서 비교적 쉬운 예를 사용하여 기준선 모델조차도 실제로 토픽 분류가 어떻게 좋은 성능에 도달하는지 반영한다.\n' +
      '\n' +
      '#### 3.1.4 Conclusion\n' +
      '\n' +
      '한국 최초의 주제 분류 벤치마크인 YNAT를 소개합니다. 벤치마크에는 7개 범주 중 한 손으로 라벨을 붙인 주제로 분류된 63,892개의 뉴스 헤드라인이 포함된다. 우리는 각 헤드라인이 단일 주제만 가지고 있다고 가정하지만 다중 레이블 분류로 공식화될 수 있다. 따라서 두 번째 및 세 번째 관련 주제 주석을 엽니다. 또한 메타데이터가 필요한 경우 향후 작업을 위해 헤드라인별 URL이 함께 제공됩니다. 그들 중 일부가 사용 허가를 필요로 하는 경우 기관에 연락해야 합니다. 우리는 YNAT가 KLUE의 다른 과제에 비해 간단하고 기본적인 NLU 과제의 역할을 할 것으로 기대한다.\n' +
      '\n' +
      '### 의미적 텍스트 유사성(STS)\n' +
      '\n' +
      '의미적 텍스트 유사성(semantic textual similarity, STS)은 두 문장 사이의 의미적 동등성의 정도를 측정하는 것이다. STS는 기계 번역, 요약 및 질의 응답과 같은 다른 NLP 작업에 필수적이기 때문에 벤치마크에 포함한다. GLUE [133]의 STS [13]과 같이, 많은 NLU 벤치마크들은 의미 유사성[142], 패러프레이즈 검출[133, 57] 또는 단어 감지 중의성 해소[125, 72]와 같은 텍스트 스니펫들의 의미 유사성을 비교하는 것을 포함한다.\n' +
      '\n' +
      'STS는 0(의미 중복 없음)에서 5(의미 동등성)까지의 실제 값으로 두 입력 문장의 의미 유사성을 예측하는 문장 쌍 회귀 태스크로 공식화된다. 모델 성능은 STS-b의 평가 체계에 따라 피어슨의 상관 계수로 측정된다[13]. 우리는 또한 임계값 점수 3.0(패러프레이징 또는 그렇지 않음)으로 실수를 두 클래스로 이진화하고 F1 점수를 사용하여 모델을 평가한다.\n' +
      '\n' +
      '#### 3.2.1 Dataset Construction\n' +
      '\n' +
      '원본 말뭉치의 도메인과 스타일을 다양화하기 위해 AIRBNB(구어체 리뷰), POLICY(공식 뉴스), PARAKQC[18](스마트 홈 발화)에서 문장을 수집한다. 우리는 그것들을 문장 쌍에 조심스럽게 맞춘다.\n' +
      '\n' +
      '각 코퍼스에 대해 유사성 점수의 모든 범위를 균일하게 커버하기 위해 문장 쌍의 샘플링 전략을 설계한다. 정교한 전략이 없으면 간단한 무작위 샘플링과 쌍에 문장을 일치시키면 점수 0의 대다수가 된다. 이러한 왜도를 완화하기 위해 _잠재적으로_ 유사하고 덜 유사한 문장은 다양한 방법을 사용하여 별도로 쌍을 이룬다. 예를 들어, 두 개의 설명이 동일한 이벤트를 참조하는 동일한 이미지 또는 헤드라인을 묘사하는 경우, 추가 정보 때문에 유사할 가능성이 있다. 그렇지 않으면, 그들은 유사하지 않을 것이다[2]. 이로부터 영감을 받아, 우리는 문장을 유사하거나 그렇지 않은 것으로 짝짓기하기 위해 사용 가능한 추가 정보를 사용한다. 사용할 수 없는 경우 왕복 번역(RTT)을 사용하여 유사한 쌍을 얻고 덜 유사한 쌍에 대해 _탐욕스러운 문장 일치_를 얻는다.\n' +
      '\n' +
      '각 문장의 의도를 사용할 수 있는 PARAKQC에 대한 전략을 지정합니다. 모든 문장은 스마트 홈 도메인에 대한 쿼리이며, 그 의도는 일부 쿼리 간에 공유된다. 예를 들어, "_오늘 서울의 날씨는 어때요_" 그리고 "_오늘 서울의 날씨는 어떤지 아세요?_" "오늘 서울의 날씨"를 묻는 것과 같은 의도를 공유합니다. 우리는 유사 쌍과 같은 의도를 가진 두 문장과 유사하지 않은 의도를 가진 두 문장을 짝짓는다. 서로 다른 쌍을 너무 많이 만들지 않기 위해 유사하지 않은 쌍도 토픽을 공유합니다.\n' +
      '\n' +
      'AIRBNB와 POLICY의 경우 문장 간의 유사성을 추정할 수 있는 유의미한 메타데이터를 찾을 수 없다. 따라서 본 논문에서는 원본 문장의 핵심 의미를 유지하면서 어휘 표현이 약간 다른 문장을 생성하는 것으로 알려져 있기 때문에, 유사한 문장 쌍을 생성하기 위해 네이버 Papago20을 이용한 RTT 기법을 적용한다. 우리는 영어를 중간 언어로 설정했다. 우리는 한글로 다시 번역할 때 존댓말 옵션을 선택하는데, 그 이유는 그 옵션이 문장의 의미를 경험적으로 보존하는 경향이 있기 때문이다. 덜 유사한 쌍에 대해, 먼저 모든 가능한 문장 쌍의 ROUGE [80]을 계산하고, 더 높은 점수가 더 높은 의미적 유사도와 상관관계가 있다고 가정한다. 21, 우리는 모든 가능한 쌍으로부터 가장 큰 점수를 갖는 쌍을 그리고 모든 문장이 일치할 때까지 나머지 쌍에 걸쳐 그리기를 반복한다. 진행됨에 따라, 남은 쌍의 수가 작아질수록 점수는 감소하여, 덜 유사한 쌍을 생성한다. 알고리즘 1에 제시된 대로 이 프로세스를 _탐욕 문장 매칭_ (GSM)으로 요약한다.\n' +
      '\n' +
      '발음 20: [https://papago.naver.com/](https://papago.naver.com/)\n' +
      '\n' +
      '각주 21: 이것은 다른 유사성 측정으로 대체될 수 있다.\n' +
      '\n' +
      '**결과:** 말뭉치 C에 SET된 문장 쌍 집합\n' +
      '\n' +
      '준비 말뭉치 C, Let SET = [];\n' +
      '\n' +
      '**while**_size of C \\(\\geq\\) 2_**do**\n' +
      '\n' +
      '1. C로부터 임의의 문장 S를 선택하고;\n' +
      '\n' +
      '2. ROUGE(S,T)가 최대가 되고 T \\(\\in\\) C\\(\\backslash\\)S가 되는 문장 T를 찾아내는 단계;\n' +
      '\n' +
      '3. C로부터 {S, T}를 제거하고;\n' +
      '\n' +
      '4. SET에 매칭쌍{(S,T)}을 추가\n' +
      '\n' +
      '**end**\n' +
      '\n' +
      '**주석 프로토콜** SemEval-2015 [2]에서 사용된 원래 주석 가이드를 수정합니다. 두 문장의 청크화를 제안하고, 청크 수준(NP, 동사 체인, PP 등)에서 유사도를 비교한다. 그리고 주석이 문장 수준의 유사도로 판단을 합산해야 한다. 그러나 한국어에서는 청크화가 매우 어렵기 때문에 가이드를 직접 적용할 수 없었습니다. 청크화는 단어의 토큰화 및 형태소 수준의 분해가 필요하지만, 어려운 경우가 있고 심지어 결정적이지 않은 경우도 있다[103]. 따라서 우리는 주석자가 청크 없이 유사성을 평가하고 문장 수준 비교를 고수하도록 안내한다.\n' +
      '\n' +
      '우리는 크라우드 워커에게 문장 수준 유사성 평가를 위해 중요하거나 중요하지 않은 추가 단서를 제공합니다. Important_content는 문장 내의 주요 아이디어를 나타낸다. 선언적 문장이면 그 사실, 설명 또는 정보를 제공하는 것이 주된 발상이다. 질문적이고 명령적인 문장의 경우 요청이나 명령을 전달하는 것이 중요하다. 탄성문에서는 감정이나 의견이 주요 내용[4]이다. 이러한 _중요_ 내용 이외의 다른 구성 요소는 _중요하지 않은_ 것으로 간주됩니다. 예를 들어, 그들은 그것의 뉘앙스나 공손함에 영향을 미치는 보조 동사나 기능 단어이다. 주석자는 다음과 같이 유사도를 점수화해야 한다:\n' +
      '\n' +
      '* 5: 두 문장은 _중요_ 및 _중요하지 않은_ 내용 측면에서 동일합니다.\n' +
      '*4: 두 문장은 매우 동등하다. 일부 _중요하지 않은_ 내용은 다릅니다.\n' +
      '* 3: 두 문장은 대략 동일합니다. _ 중요_ 콘텐츠는 서로 유사하지만 _중요하지 않은_ 콘텐츠 간의 차이는 무시할 수 없습니다.\n' +
      '* 2: 두 문장은 동일하지 않습니다. _ 중요_ 내용은 서로 유사하지 않으며 일부 _중요하지 않은_ 내용만 공유합니다.\n' +
      '* 1: 두 문장은 동일하지 않습니다. _ 중요_ 및 _중요하지 않은_ 콘텐츠는 서로 유사하지 않습니다. 두 문장은 주제만 공유합니다.\n' +
      '*0: 두 문장은 동일하지 않다. 중요한 내용과 중요하지 않은 항목을 공유하지 않습니다.\n' +
      '\n' +
      '또한 크라우드 워커가 문장의 맥락을 고려하도록 안내합니다. 두 문장의 의미 구분에 유의미한 영향을 미친다면 점수가 낮아야 한다. 예를 들어, 두 문장에 "Check-in은 호스트가 아닌 다른 사람이 했다."와 "Check-in은 누군가가 했다."와 같은 중요한 정보 \'Check-in\'이 포함되어 있다고 하자. 후자의 문장에서는 \'누군가\'가 호스트일 수 있다. 우리는 \'호스트 이외의 사람\'을 전자와 떨어뜨려 정보를 잃기 때문에 두 문장 사이의 의미 차이를 무시하지 않는다. 이 쌍을 3으로 채점한다. 또한, 전자의 문장과 \'Check-out은 호스트가 아닌 다른 사람이 했다.\'와 비교하면 _중요_ 정보가 달라 2를 부여한다.\n' +
      '\n' +
      '주석 프로세스 우리는 한국의 크라우드소싱 플랫폼인 SelectStar,22의 근로자를 모집하고 주석 프로토콜에 익숙화한다. 자격 있는 작업자를 선택하기 위해 파일럿 주석을 실행합니다. 크라우드 워커의 판단이 다른 작업자의 판단과 일치하지 않는 경우가 빈번할 경우, 그 사람은 주 주석 과정에서 제외된다. 그 결과 초기 20명의 작업자 중 19명이 주 주석에 참여하게 된다. 파일럿에서 사용된 문장 쌍을 제거한 후 AIRBNB 7,375개, POLICY 2,956개, PARAKQC 4,538개로 구성된 주요 주석에 14,869개의 쌍을 사용한다. 7명의 서로 다른 작업자는 모든 문장 쌍을 독립적으로 레이블링했다.\n' +
      '\n' +
      '각주 22: [https://selectstar.ai/](https://selectstar.ai/)\n' +
      '\n' +
      '각 문장 쌍에 대해 7개의 레이블을 평균화하고 Agirre et al. [3], Cer et al. [13]에 따라 이상치를 제거한다. 먼저 피어슨의 상관 관계 < 0.80 또는 크리펜도르프의 알파 < 0.20 (명목) [67]을 다른 사람의 주석과 함께 보여주는 주석을 필터링한다. 우리는 이 기준을 가진 두 개의 주석자를 제외하여 모든 문장 쌍이 최소 5명 이상의 주석자를 갖는다. 마지막으로 유사도 점수를 소수점 첫째 자리에서 반올림한다.\n' +
      '\n' +
      '몇 가지 필터링 방식이 더 적용된다. 먼저 주석이 2 표준편차보다 큰 14쌍을 드롭한다. 이러한 쌍에는 다양한 방식으로 해석되는 모호한 표현 또는 오주석이 포함될 수 있습니다. 둘째, RTT로 인한 번역 오류나 잘못된 정보를 포함한 문장을 작업자에게 보고하도록 요청한다. 보고된 문장을 검사하고 418개의 문장 쌍을 제거한다. 셋째, 우리는 윤리적 문제와 관련된 선고를 취하한다. 노동자들은 그들이 어떤 종류의 혐오 발언, 사회적 편견, 그리고 잠재적인 개인 식별 정보(PII)를 포함하고 있다면 그 쌍들을 보고한다. 1,213개의 문장 쌍을 검사 후 추가로 제거했다. 결과적으로, 우리는 총 13,224개의 문장 쌍을 가지고 있습니다. 우리는 7개의 주석자(또는 그 이하)가 쌍별로 다르기 때문에 피어슨의 상관 관계 대신 크리펜도르프의 알파를 사용하여 주석자 간 일치(IAA)를 보고한다. 주석자는 서로의 주석에 동의했다. (Krippendorff\'s alpha (interval)=0.85).\n' +
      '\n' +
      '유사도 점수 주석의 분포가 _잠재적으로_ 유사한 문장 쌍과 덜 유사한 쌍 간에 다르다는 것을 관찰한다. 도 1은 AIRBNB에서 RTT(상단) 및 GSM(하단)에 의해 생성된 라벨 분포들을 예시한다. 예상대로, RTT 쌍은 높은 유사성(3에서 5까지)을 나타내는 경향이 있는 반면 GSM 쌍은 덜 유사한 것으로 간주된다(0에서 3까지). 우리는 유사성 기반 매칭을 사용하더라도 0으로 점수가 매겨진 GSM 쌍의 수가 높다는 점에 유의한다. POLICY 및 PARAKQC에서도 유사한 경향이 관찰된다. 두 개의 분포를 결합함으로써 유사도 점수의 관점에서 다양한 문장 쌍을 얻을 수 있다.\n' +
      '\n' +
      '최종 데이터셋은 13,224개의 문장 쌍과 그에 상응하는 유사도 점수를 수집한다. 점수의 분포를 고려하여 훈련, 개발, 테스트 세트로 나눕니다. 쌍을 주의 깊게 샘플링하더라도 그림 1과 같이 \\(0-5\\)에 걸쳐 전체적인 점수 분포가 균일하지 않다. 그러나 우리는 특정 점수에 대한 평가 편향을 방지하기 위해 적어도 평가(개발 및 테스트) 세트에서 균일한 분포를 선호한다. 따라서 그림 2와 같이 대략 균일한 분포를 갖는 평가 집합을 구성한다. 이를 위해 점수 범위 0\\(-\\)5에서 51개의 빈을 나누어 모든 점수의 소수점 첫째 자리까지 반올림한다. 우리는 통에 걸친 쌍의 수의 균형을 맞추려고 노력한다. 그들 중 일부는 짝의 수가 적기 때문에, 우리는 그 수에 가까운 모든 짝의 수를 맞추려고 한다.\n' +
      '\n' +
      '또한 평가 집합을 위해 각 쌍에서 문장 간의 단어 중복을 고려한다. 더 큰 단어 중복은 더 높은 의미적 유사성을 나타낼 수 있기 때문에, 우리는 모델이 단어 중복을 사용하여 단순히 유사성을 예측하는 것을 방지하기 위해 그러한 경향을 충족하는 쌍을 줄이려고 시도한다. 중첩은 MeCab[68]을 이용하여 형태소 수준 자카드 거리로 측정한다. 우리는 악보 3\\(-\\)5에서 단어 중복이 가장 적은 쌍을 선택하고 나머지 부분에서 단어 중복이 가장 많은 쌍을 선택한다. 이러한 쌍은 dev 및 테스트 세트의 모든 빈에 포함되도록 우선 순위가 매겨진다.\n' +
      '\n' +
      '평가 세트를 1:2 비율로 분할하여 dev 및 테스트 세트를 구성하여 각각 519쌍 및 1,037쌍을 생성했다. 나머지 11,668쌍은 기차 세트를 구성한다. 각 말뭉치에 대한 자세한 숫자는 표 4에 나와 있다. 모든 집합에 대해 소스 말뭉치와 원래 쌍 사이의 비율의 균형을 맞춘다. 또한, 점수들은 패러프레이즈 검출 태스크와 동일한 임계값 3.0으로 이진화된다.\n' +
      '\n' +
      '#### 3.2.2 평가 메트릭\n' +
      '\n' +
      'KLUE-STS의 평가지표는 1) Pearson\'s correlation coefficient (Pearson\' \\(r\\)), 2) F1 score이다. Pearson\'s \\(r\\)은 STS-b [13]에서 채택한 인간 라벨 문장 유사성 점수와 모델 예측 점수 사이의 선형 상관 관계를 측정한 것이다. 우리의 디브와 테스트 세트는 균형 잡힌 점수 분포를 가지므로 계수는 관계의 크기를 올바르게 제공한다. 이진화된 결과(_paraphrased/nonparaphrased_)를 측정하기 위해 F1 점수가 채택된다. 특히, F1은 _paraphrased_ 클래스에 대한 결과를 보고합니다.\n' +
      '\n' +
      '그림 1: AIRBNB에서 RTT(상단) 및 GSM(하단)에 의해 생성된 라벨 분포.\n' +
      '\n' +
      '도 2: 열차(상단) 및 디브(하단) 세트의 유사성 스코어 분포. 데브 집합의 점수는 0\\(-\\)5 범위에 걸쳐 균일한 분포에 가깝다. 점수는 소수점 첫째 자리에서 반올림된다.\n' +
      '\n' +
      '#### 3.2.3 관련 작업\n' +
      '\n' +
      '문장 간 유사도 측정은 다양한 NLP 응용과 밀접한 관련이 있는 기본적인 자연어 이해 문제이다. 그 중요성 때문에, STS는 다양한 NLU 벤치마크들에 포함된다[133, 142]. 이 분야의 연구를 용이하게 하기 위해, 많은 공유 작업이 수행되고 주석이 달린 말뭉치가 공개된다[2, 3, 13]. 일반적으로 질문 쌍, 이미지 설명, 뉴스 헤드라인, 0(의미 중복 없음)에서 5(의미 동등성)까지의 실수 값으로 주석이 달린 여러 텍스트 도메인을 포함합니다.\n' +
      '\n' +
      '최근 Ham et al. [45]는 기계번역 한국어 STS 벤치마크를 소개하고 있다. 이는 GLUE에서 [13]을 번역한 것으로 총 8,600여 개의 문장 쌍을 포함하고 있다. 모든 예는 기계 번역에만 의존하며 평가(dev 및 test) 세트의 문장 쌍은 인간에 의해 추가로 사후 편집된다. 그러나 해당 레이블은 번역된 의미로 조정되지 않았다. 한국 화자들은 그들 사이의 유사성을 다르게 판단할 것이기 때문에 재표지 과정의 부족이 문제가 될 것이다.\n' +
      '\n' +
      '유사도 레이블이 특정 임계값만큼 이진화되면 STS도 마이크로소프트 리서치 패러프레이즈 코퍼스(MRPC)[32], 쿼라 질문 쌍(QQP)[133], 또는 PAWS[152] 및 PAWS-X[144]와 같은 패러프레이즈 탐지 작업으로 볼 수 있다. 따라서 본 논문에서는 이진 분류 성능을 보고함으로써 모델이 패러프레이즈 검출에서 얼마나 좋은 성능을 보이는지 알아본다.\n' +
      '\n' +
      '패러프레이즈 검출에서 Cho et al. [18]은 스마트 홈에 대한 인간 생성 쿼리를 포함하는 벤치마크를 제시하며, 여기서 10개의 패러프레이즈 문장은 총 1,000개의 그룹을 구성하기 위해 함께 그룹화된다. 규모의 세분성은 0에서 5까지이지만, 의미적 유사성은 화제(스마트 홈, 날씨 등), 화행(질문, 금지 등) 등의 속성만으로 판단되며, 이는 인간의 직접적인 유사성 판단이 부족하기 때문에 뉘앙스, 통사적 구조 등 다른 세부 사항은 고려하지 않는다. PAWS-X[144]는 한국어의 PAWS[152]의 번역 버전을 제공한다. KorSTS와 마찬가지로 열차 분할은 기계 번역되고 그 디브 및 테스트 분할은 인간 번역되며 해당 라벨은 인간 검사 없이 보존된다. 국립국어원[98]과 같은 정부 지원 기관에서 제공하는 패러프레이즈 말뭉치도 있지만, 단순히 접근성이 제한된 인간 생성 및 기계 패러프레이즈 문장을 제공한다.\n' +
      '\n' +
      '#### 3.2.4 Conclusion\n' +
      '\n' +
      '우리는 모든 사람이 자유롭게 접근할 수 있는 여러 도메인과 스타일을 다루는 최초의 인간 주석이 달린 한국 STS 벤치마크 KLUE-STS를 만듭니다. 유사도 점수 주석 과정은 한국어의 특징을 포착하기 위해 특별히 고안되었다. 다양한 영역의 표현을 포함하는 벤치마크는 벤치마크 역할을 넘어 추가 연구에 유용한 자원이 될 것으로 예상된다. 우리의 벤치마크는 SentenceBERT[115]와 같은 STS 리소스에 설정된 수많은 모델을 개발하는 데 도움이 됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Source** & **|Train** & **|Devl** & **|Testl** & **Total** \\\\ \\hline AIRBNB & 5,371 & 255 & 510 & 6,136 \\\\ POLICY & 2,344 & 132 & 264 & 2,740 \\\\ PARAKQC & 3,953 & 132 & 263 & 4,348 \\\\ \\hline\n' +
      '**Overall** & **11,668** & **519** & **1,037** & **13,224** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: KLUE-STS에 대한 통계. 처음 세 개의 열은 각 소스 말뭉치와 최종 데이터의 트레인, 디브, 테스트 세트에서 예제의 수를 제공한다.\n' +
      '\n' +
      '### 자연어 추론(NLI)\n' +
      '\n' +
      '자연어 추론(NLI)의 목표는 _가설_ 문장과 _전제_ 문장 사이의 관계를 추론하는 모델을 훈련시키는 것이다. 전제가 주어지면 NLI 모델은 _가설_이 참(전체), 거짓(모순) 또는 결정되지 않은(중립)인지 여부를 결정합니다. 작업은 또한 텍스트 수반성(textual entailment, RTE)을 인식하는 것으로 알려져 있다[27].\n' +
      '\n' +
      '문장 간의 수반과 모순을 이해하는 것은 NLU의 기본이다. NLI 데이터 세트는 또한 GLUE[133] 및 슈퍼GLUE[132]와 같은 다양한 NLU 벤치마크에 포함되며, 이들은 다른 NLU 작업에 대한 트레이닝 데이터로서 가치가 있다[24, 107, 115].\n' +
      '\n' +
      '우리는 NLI 모델이 _전제_ 및 _가설_ 문장의 각 쌍을 읽고 관계가 수반되는지, 모순인지 또는 중립인지 예측하는 분류 작업으로 NLI를 공식화한다. 모델 성능을 측정하기 위해 분류 정확도를 사용한다.\n' +
      '\n' +
      '#### 3.3.1 Dataset Construction\n' +
      '\n' +
      'SNLI [8] 및 MNLI [138]과 유사한 수집 방법을 사용하여 KLUE-NLI를 구성한다. 먼저, 기존의 말뭉치로부터 전제 문장을 수집한다. 그런 다음 각 전제 문장에 대해 주석자 한 명에게 세 개의 관계 클래스 각각에 대해 하나씩 세 개의 새로운 가설 문장을 생성하도록 요청한다. 그런 다음 전제 및 가설 문장의 각 쌍에 대해 4명의 추가 주석자에게 검증을 위해 관계에 레이블을 지정하도록 요청한다. 우리는 주석자에게 세 개의 레이블을 설명하기 위해 Williams et al. [138]이 제안한 기준을 따른다. 가설 생성과 쌍 검증을 위해 한국 크라우드소싱 플랫폼인 SelectStar,23에서 직원을 모집합니다.\n' +
      '\n' +
      '각주 23: [https://selectstar.ai/](https://selectstar.ai/)\n' +
      '\n' +
      '전제 문장에 대한 출처 말뭉치 우리는 전제 문장 세트에 대해 WIKITREE, POLICY, WIKINEWS, WIKIPEDIA, NSMC 및 AIRBNB 6개의 말뭉치를 사용한다. 그들은 현대 한국어의 다양한 주제와 글쓰기 스타일을 다룬다. WIKITREE, POLICY 및 WIKINEWS는 뉴스 기사이고 WIKIPEDIA는 크라우드 소싱 백과사전이며 모두 공식 한국어로 작성된다. NSMC와 AIRBNB는 각각 영화와 여행 분야의 구어체 리뷰로 구성된다.\n' +
      '\n' +
      '6개의 말뭉치에서 우리는 가설을 이끌어내는 10,000개의 전제를 추출합니다. 유효한 전제는 세 가지 조건을 만족해야 한다. 첫째, 전제는 명제, 수학적 공식과 목록을 제외하고 진리값을 부여할 수 있는 선언적 문장이다. 둘째, 전제에는 적어도 하나의 술어가 포함되어야 하며, 술어는 상태(예: be, believe, know), 활동(예: play, smile, walk), 성취(예: realize, reach, break), 성취(예: eat, build, paint)와 같은 다양한 유형이 될 수 있다. 셋째, 전제의 길이는 공백이 포함된 20자에서 90자 사이여야 한다.\n' +
      '\n' +
      '가설 생성을 위한 주석 프로토콜 주석자에게 전제를 보여주고 각 레이블에 해당하는 세 가지 가설을 작성하도록 요청한다. 이를 통해 각 레이블에 대해 거의 동일한 수의 (전제, 가설) 쌍을 수집할 수 있습니다. 기준의 개요를 다음과 같이 유지합니다.\n' +
      '\n' +
      '* ENTAILMENT: 가정이 참이면 가설은 반드시 참이다.\n' +
      '* CONTRADICITION: The hypothesis is necessarily false given the premise is true\n' +
      '* NEUTRAL: 전제가 참이면 가설이 참일 수도 있고 그렇지 않을 수도 있습니다.\n' +
      '\n' +
      '우리는 인간의 쓰기 기반 가설 생성에서 나오는 주석 아티팩트를 알고 있다. 문장 길이와 명시적 어휘 패턴은 특정 클래스와 매우 관련이 있다. 노동자는 전제에 명시되어 있지 않은 추가적인 구나 절을 도입하는 것만으로 중립적인 가설을 세울 수 있기 때문에, 모든 계층 중에서 중립적인 문장이 가장 길다. "아니오", "아니오" 및 "아니오"와 같은 부정은 종종 클래스 CONTRADICION[44, 108]과 동반된다.\n' +
      '\n' +
      '이러한 아티팩트의 우려에도 불구하고, 우리는 그러한 쓰기 기반 주석 절차를 고수한다. 가설을 수집하기 위한 자동 파이프라인에 비해, 인간의 작성은 더 높은 품질의 데이터를 산출하며 여전히 효과적인 프로토콜이다[131]. 우리는 주석자가 사소한 패턴을 주입하지 않도록 권장하는 방법에 중점을 둡니다. 특정 _Do_s 및 _Don_\'ts를 사용하여 지침을 준비하고 작업자를 사전에 엄격하게 훈련합니다. 주석 아티팩트를 최소화하기 위해 주석자에게 클래스에 걸쳐 길이가 유사한 문장을 작성하고 특정 어휘 항목을 반복적으로 삽입하는 것을 자제하며 추론을 할 때 가능한 다양한 전략을 사용하도록 지시한다.\n' +
      '\n' +
      '구체적으로, 사례와 함께 가설 생성에 대한 세부 지침을 제공한다. 우리는 주석이 1) 어휘 선택, 2) 구문 구조 및 3) 세계 지식의 측면에서 다양한 언어 현상을 나타내는 가설을 만들도록 권장한다. 사전 선택의 경우, 우리의 지침은 주석이 동의어/반어, 상위어/하위어 및 보조 입자를 사용할 것을 제안한다. 다양한 구문 구조를 도입하기 위해 단어 스크램블링, 음성 변경 및 원인 변경과 같은 여러 구문 변환 전략을 제공한다. 주제/객체 스와핑 또는 수동화와 같은 방법은 기존의 NLI 데이터 증강 전략에 의해 동기 부여된다[89, 42]. 또한 실제 세계에 기반을 둔 데이터 세트를 만들기 위해 시간, 수량 및 지리와 같은 세계 지식을 반영하는 표현을 사용하는 것을 권장합니다.\n' +
      '\n' +
      '가이드라인에 몇 가지 세부 사항이 더 있습니다. 우리는 주석자에게 전제의 쓰기 스타일을 유지하여 스타일 측면에서 균형 잡힌 데이터 세트를 만들도록 지시한다. 또한 문법성이나 내용의 복잡성으로 인해 이해하기 어려운 문장을 건너뛰도록 지도한다. 혐오 발언이나 사회적 편견, 개인 식별 정보 등 윤리적 문제가 담긴 문장을 건너뛰고 보고하도록 지도하기도 한다. 보고된 모든 문장을 조사하고 데이터 세트에 문장을 포함할지 여부를 최종 결정한다.\n' +
      '\n' +
      '레이블 검증에 대한 주석 프로토콜 군중 작업자는 검증을 위해 결과적인 전제-가설 쌍의 관계에 주석을 달습니다. 생성된 각 쌍에 대해 4명의 크라우드 워커에게 (ENTAILMENT, CONTRADICTION, NEUTRAL) 중 단일 레이블을 공급하도록 요청한다. 이는 가설 문장을 작성한 주석자가 의도한 초기 레이블을 포함하여 쌍당 총 5개의 레이블을 산출한다. 검증된 각 문장 쌍에 대해 5표 중 3표 이상의 과반수를 나타내는 골드 레이블을 할당한다.\n' +
      '\n' +
      '주석 프로세스 가설 생성을 위해 지침을 반복적으로 업데이트하고 작업자를 훈련하는 파일럿 단계를 거칩니다. 파일럿에서 우리는 의미적으로 받아들일 수 없는 문장을 쓰거나 전제에 사용되지 않는 증명 대명사를 도입하는 것이 잠재적인 문제가 될 수 있음을 발견했다. 그들이 의도된 라벨을 변경할 수 있기 때문에, 우리는 근로자들에게 그러한 문장을 쓰는 것을 피하도록 요청한다. 주석 공정의 이 부분에 대한 작업자의 수는 11명이다.\n' +
      '\n' +
      '그런 다음 모든 쌍에 대한 관계 레이블의 유효성을 검사합니다. 파일럿에서 2,604명의 지원자를 시작으로 파일럿 단계를 거친 다음 테스트를 통과한 684명을 선택하여 검증 단계에 참여한다. 138명의 근로자가 탈락하면서 최종 근로자 수는 546명이다.\n' +
      '\n' +
      '검증 결과는 표 5에 요약되어 있다. 그들은 우리의 쓰기 프로토콜이 고품질 말뭉치를 만드는 데 효과적임을 시사한다. KLUE-NLI에서 만장일치 금 라벨 예제의 비율은 SNLI 및 MNLI보다 18% 높다. 이러한 예제의 비율이 높을수록 생성된 가설 문장과 원래의 전제 문장 사이의 관계가 명확해진다. 골드 라벨과 저자의 라벨에 대한 개별 주석자의 일치도 SNLI 및 MNLI보다 높으며 거의 모든 쌍이 골드 라벨을 받는다. 몇 개의 문장 쌍(0.53%)에만 금 레이블이 없으며 데이터 세트를 완성하기 전에 이를 제거한다.\n' +
      '\n' +
      '최종 데이터셋 최종 데이터셋은 30,998개의 문장 쌍으로 구성되어 있으며, 이를 train/development/test 세트로 나눈다. 표 6은 데이터 세트의 기본 통계를 보여준다. SNLI 및 MNLI에서 관찰된 바와 같이, 우리의 전제 문장도 대응하는 가설 문장보다 긴 경향이 있다. 근로자는 일반적으로 가설 작성을 위해 전제의 부분 정보를 사용하기 때문이다.\n' +
      '\n' +
      '우리는 1) 균형 잡힌 소스 스타일을 포함하고 2) 주석 아티팩트를 활용하는 모델을 비활성화시키는 방식으로 개발 및 테스트 세트를 의도적으로 형성한다는 점에 유의한다. 개발과 테스트 세트는 각각 3,000개의 문장 쌍을 포함합니다.\n' +
      '\n' +
      '개발과 테스트 세트에서 스타일의 일관성을 유지하기 위해 각 세트에 60%의 공식 문장과 40%의 구어체 문장을 포함한다. 우리는 공식 텍스트 WIKITREE, POLICY, WIKINEWS, WIKIPEDIA에서 각각 450개의 문장을 샘플링하고 구어체 텍스트 NSMC, AIRBNB에서 각각 600개의 문장을 샘플링한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Statistics** & **SNLI** & **MNLI** & **KLUE-NLI** \\\\ \\hline Unanimous Gold Label & 58.30\\% & 58.20\\% & **76.29\\%** \\\\ \\hline Individual Label = Gold Label & 89.00\\% & 88.70\\% & **92.63\\%** \\\\ Individual Label = Author’s Label & 85.80\\% & 85.20\\% & **90.92\\%** \\\\ \\hline Gold Label = Author’s Label & 91.20\\% & 92.60\\% & **96.76\\%** \\\\ Gold Label \\(\\neq\\) Author’s Label & 6.80\\% & 5.60\\% & **2.71\\%** \\\\ No Gold Label (No 3 Labels Match) & 2.00\\% & 1.80\\% & **0.53\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: SNLI 및 MNLI와 비교한 KLUE-NLI에 대한 검증 통계의 요약[138]. 가설을 작성할 때 원래 주석자가 의도한 레이블을 “저자의 레이블”이라고 한다. 주석자 5명 중 3명 사이의 합의는 “금색 레이블”이다. 우리의 NLI 벤치마크가 가설에서 거짓 신호를 사용하여 레이블을 예측하는 모델을 장려하는 것을 방지하기 위해 먼저 해당 레이블로 가설 문장만 사용하여 KLUE-RoBERTa 기반 모델을 미세 조정한다. 모델이 가설과 레이블 사이에 단서가 없는 경우, 각 레이블에 대한 예측 확률 점수는 3-way로 분류될 때 1/3(\\(\\frac{1}{3}\\)) 균일해야 한다. 이러한 점수 분포가 이상적이라고 가정하면 가설 전용 모델의 예측이 이상에 가장 가까운 개발/테스트 세트를 위한 쌍을 선호한다. 우리는 교차 엔트로피를 이용하여 예측과 이상 사이의 거리를 계산한다. 전제의 온전한 집합과 세 가지 가설을 보존하기 위해 각 집합의 평균 거리를 계산한다. 우리는 평균 거리가 가장 낮은 20%에 속하는 집합을 추출하고 무작위로 dev 집합과 test 집합으로 나눈다.\n' +
      '\n' +
      '우리의 아이디어는 점적 상호 정보(PMI)의 확장으로 볼 수 있다. 각 가설 단어(\\(w\\))와 클래스 레이블(\\(c\\)) 사이의 PMI는 각 클래스와 단어의 연관성을 발견하기 위해 사용되었다[44, 131]. PMI가 문장 수준 연관성으로 확장되면 메트릭은 다음과 같이 가설 전용 모델 예측 확률과 유사한 척도를 제공한다.\n' +
      '\n' +
      '\\[\\text{PMI}(w,c)=\\log\\frac{P(w,c)}{P(w)P(c)}=\\log\\frac{P(c|w)P(w)}{P(w)P(c)}= \\log\\frac{P(c|w)}{P(c|w)}\\propto P(c|w)\\]\n' +
      '\n' +
      '인간의 성능을 측정하고 XNLI [25] 테스트 세트의 기계 번역인 KorNLI [45] 테스트 세트에 대한 KLUE-NLI 테스트 세트의 개선 여부를 조사하기 위해 한 라운드의 인간 평가를 수행한다. 한국어 언어학을 전공하고 KLUE-NLI 구축 과정에 참여하지 않은 한국인 원어민 학부생 4명을 채용한다. KLUE-NLI 테스트 세트에서 무작위로 100개의 문장 쌍을 샘플링하고 작업자에게 주석을 달도록 요청한다. 우리는 주어진 금 라벨과 주석의 일치를 확인합니다. KorNLI 테스트 세트의 하위 집합에 대해 동일한 작업을 수행하여 인간 유도 데이터 세트가 데이터 세트의 품질을 향상하는지 여부를 조사한다. 그 결과를 표 7에 나타낸다.\n' +
      '\n' +
      'KorNLI의 경우 문장 쌍의 38%가 금 레이블과 일치하는 4개의 주석자 모두의 응답을 가지고 있다. 문장은 3개, 2개, 1개의 반응이 골드 라벨과 일치할 때 각각 18%, 18%, 16%이다. 10켤레가 골드 라벨과 일치하지 않습니다. 반면에 KLUE-NLI는 주어진 금 라벨과 훨씬 더 높은 일치를 보여준다. 모든 주석자는 쌍 중 71%에서 금 라벨에 동의하고 95%는 최소 3개의 동의를 얻는다. 또한 400개(64.50%)의 개별 주석 중 258개만 KorNLI의 금 라벨과 동일하다. 다시 말하지만, KLUE-NLI는 금 라벨과 더 나은 일치를 보여준다. 360개(91.00%)의 주석은 골드 라벨과 동일합니다.\n' +
      '\n' +
      'KLUE-NLI의 주석 품질에서 이러한 숫자는 SNLI 및 MLNI뿐만 아니라 KorNLI보다 우수하다. KorNLI에서 주석자는 문장의 의미적 관계를 구별하기 어렵기 때문에 두 문장 중 적어도 하나를 완전히 이해하지 못하거나 NEUTRAL을 선택한다고 보고하는 경우가 많다. 금 라벨의 분포가 균일하지만(각각 수반, 모순 및 중립 문장의 33, 33 및 34%) 주석자가 가장 자주 선택하는 라벨은 NEUTRAL(평균 56.75%)이다. 주석자의 다수결과 금 라벨이 다른 경우는 26%이다. 이러한 결과는 주석이 KorNLI 문장의 논리적 의미 관계를 파악하는 데 어려움을 겪고 있음을 시사한다.\n' +
      '\n' +
      '반면 KLUE-NLI의 경우 4개의 응답 중 금 라벨과 일치하는 경우가 없다. 응답 중 2개 이상이 골드 라벨과 일치하는 경우를 고려하면 골드 라벨이 다수 태그로 재선정될 확률은 98%다. KorNLI와 비교하여 KLUE-NLI가 훨씬 더 신뢰할 수 있는 데이터 세트임을 알 수 있다. 이 결과는 또한 다수 태그로 대표되는 인간의 정확도가 98%라는 점을 감안할 때 현재 최상의 모델(정확도: 89.77%)의 헤드룸이 여전히 있음을 확인한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline\n' +
      '**Source** & **|Train** & **|Devl** & **|Testl** & **Total** & **Avg Len Prem** & **Avg Len Hyp** \\\\ \\hline WIKITREE & 3,838 & 450 & 450 & 4,738 & 52.81 & 26.86 \\\\ POLICY & 3,833 & 450 & 450 & 4,733 & 56.73 & 32.93 \\\\ WIKINEWS & 3,824 & 450 & 450 & 4,724 & 64.17 & 29.11 \\\\ WIKIPEDIA & 3,780 & 450 & 450 & 4,680 & 57.45 & 23.70 \\\\ \\hline NSMC & 4,899 & 600 & 600 & 6,099 & 27.48 & 21.49 \\\\ AIRBNB & 4,824 & 600 & 600 & 6,024 & 24.28 & 18.65 \\\\ \\hline\n' +
      '**Overall** & **24,998** & **3,000** & **3,000** & **30,998** & **47.15** & **25.46** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: KLUE-NLI에 대한 통계. 처음 세 열은 기차, dev 및 테스트 세트에서 문장 쌍의 수를 제공합니다. _ Avg Len Prem_ 및 _Avg Len Hyp_는 각각 전제 및 가설 문장의 평균 문자 수이다.\n' +
      '\n' +
      '#### 3.3.2 평가 메트릭\n' +
      '\n' +
      'KLUE-NLI에 대한 평가 메트릭은 SNLI [8] 및 MNLI [138]에 이어 정확도이다. 정확도는 분류기가 결과를 정확하게 식별하는 정도를 측정합니다. 클래스 레이블은 거의 균등하게 분포되어 있으므로 더 높은 정확도로 모델의 성능을 올바르게 나타낼 수 있다.\n' +
      '\n' +
      '#### 3.3.3 관련 작업\n' +
      '\n' +
      '텍스트적 수반(Textual Entailment, RTE) 인식[27]은 NLI와 유사한 작업이며 일련의 텍스트적 수반 과제에서 도입되었다. RTE 과제에서는 두 개의 문장이 주어지고, 모델은 한 문장의 의미가 다른 문장으로부터 수반될 수 있는지 여부를 결정한다. 이전 RTE 1-3에서, 태스크는 이진, \'시행\' 및 \'시행 없음\'이다. REE 4-5에서는 새로운 클래스 \'UNKNOWN\'을 도입하고, 과제를 삼원 분류로 공식화한다.\n' +
      '\n' +
      '영어 NLI에 대한 두 가지 주요 데이터 세트는 스탠포드 자연 언어 추론(SNLI) [8] 및 다중 장르 자연 언어 추론(MNLI) [138]이다. SNLI와 MNLI의 가설 문장은 EnTAILMENT, CONTRA-DICTION 또는 NEUTRAL로 표시된다. SNLI는 Flickr30k에서 570,152개의 이미지 캡션으로 만들어진 RTE 말뭉치보다 두 배 더 크다[148]. MNLI 전제 문장은 더 넓은 범위의 스타일, 형식 정도 및 주제를 포함하는 10개의 다른 출처에서 파생된다.\n' +
      '\n' +
      '기존의 NLI 데이터셋은 대부분 SNLI와 MNLI를 포함하여 영어로 되어 있으며, 다른 언어로 NLI 데이터셋을 구성하기 위한 하나의 일반적인 접근 방법은 기존의 영어 말뭉치를 관심 언어로 번역하는 것이다. Conneau et al. [25]는 MNLI의 개발 및 테스트 세트를 15개의 언어로 번역하기 위해 전문 번역기를 채용함으로써 XNLI(교차 언어 자연어 추론)를 제공한다. 번역 기반 접근법의 주요 관심사 중 하나는 원래 문장 쌍의 관계가 프로세스에서 유지되는지 여부이다. Conneau et al. [25]는 일부 번역된 쌍이 데이터세트의 샘플을 재주석하는 인간 주석자에 의해 검증되어 초기 의미론적 관계를 잃는다는 것을 발견한다. 결과는 MNLI에서 85%의 올바른 예와 XNLI에서 83%의 올바른 예를 감안할 때 인간 번역이 2%의 오주석을 유발한다는 것을 보여준다.\n' +
      '\n' +
      '한국어가 XNLI에 포함되지 않은 것에 동기부여된 KorNLI[45]가 소개된다. KorNLI[45]는 SNLI와 MNLI의 훈련 집합의 기계 번역을 통해 열차 집합이 생성되고, XNLI의 개발 및 테스트 집합과 전문 번역가의 사후 편집의 기계 번역을 통해 개발 및 테스트 집합이 생성되는 기존의 영어 말뭉치를 번역한 것이다. Ham et al. [45]도 데이터를 수동으로 조사하고 번역 후 일부 잘못된 예를 확인하지만 관찰을 정량화하고 이러한 오류를 분석하는 것을 향후 작업에 맡기 위해 인간 검증 프로세스가 수행되지 않는다. 더욱이 사후 편집에도 통사적 구조나 단어 선택의 측면에서 부자연스러운 문장이 일부 존재한다.\n' +
      '\n' +
      'SNLI와 MNLI를 기반으로 많은 연구가 제안되었지만 SNLI와 MNLI는 주석 아티팩트가 있는 것으로 알려져 있다[44, 108]. 주석 아티팩트는 크라우드소싱 과정에서 자연적으로 발생하는 특정 유형의 주석 전략과 휴리스틱의 산물이다. 이러한 인공물은 모델을 실제로 관계를 배우기보다는 휴리스틱을 채택하도록 유도할 수 있기 때문에 문제가 있다.\n' +
      '\n' +
      'NLI에서 주석 아티팩트를 줄이기 위한 몇 가지 노력이 있었다. Vania et al. [131]은 전제-가설 쌍을 만들기 위한 두 가지 완전 자동화된 프로토콜을 실험했지만, 이 방법이 주석 아티팩트에 대해 품질이 좋지 않은 데이터와 혼합된 결과를 산출한다는 것을 발견했다. OCNLI[53]는 편견을 통제하기 위해 일부 개입을 통해 쓰기 기반 프로토콜을 강화한다: 작가들이 다양한 추론 방법을 사용하도록 장려하고, 과도하게 사용된 단어에 제약을 가한다. 부정어 감소에 대한 부분적 영향에도 불구하고 명시적 제약은 다른 상관 단어를 생성하고 최종 OCNLI 데이터 세트는 대부분의 벤치마크 NLI 데이터 세트와 유사한 수준의 가설 전용 테스트 점수를 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Statistics** & **KorNLI** & **KLUE-NLI** \\\\ \\hline Unanimous Gold Label (4 Agree) & 38.00\\% & **71.00\\%** \\\\\n' +
      '3 Agree with Gold Label & 18.00\\% & 24.00\\% \\\\\n' +
      '2 Agree with Gold Label & 18.00\\% & 3.00\\% \\\\\n' +
      '1 Agrees with Gold Label & 16.00\\% & 2.00\\% \\\\\n' +
      '0 Agrees with Gold Label & 10.00\\% & 0.00\\% \\\\ \\hline Individual Label = Gold Label & 64.50\\% & **91.00\\%** \\\\ \\hline No Gold Label (No 3 Labels Match) & 4.00\\% & **0.00\\%** \\\\ Majority Vote \\(\\neq\\) Gold Label & 26.00\\% & **0.00\\%** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: KorNLI 및 KLUE-NLI의 인간 평가 결과에 대한 통계. 4개의 주석기의 레이블과 korNLI 및 KLUE-NLI 테스트 데이터의 골드 레이블을 비교한다.\n' +
      '\n' +
      '#### 3.3.4 Conclusion\n' +
      '\n' +
      '우리의 새로운 데이터 세트인 KLUE-NLI는 자연적으로 발생하는 한국어 문장을 기반으로 구성된 최초의 자원이다. KLUE-NLI는 한국어에 가장 자연스럽고 적합한 다양한 언어 현상, 글쓰기 스타일, 격식 정도 및 내용을 나타낸다. 데이터 세트의 전제 문장은 6개의 한국 말뭉치에서 나왔고, 가설 문장은 잘 훈련된 작업자가 작성했다.\n' +
      '\n' +
      '작성 기반 프로토콜을 유지하고 세부 지침을 기반으로 작업자를 철저히 훈련함으로써 레이블의 신뢰도에서 기존 NLI 데이터 세트를 개선한다. KLUE-NLI는 MNLI와 번역 기반 한국어 데이터 세트인 KorNLI보다 훨씬 높은 주석자 간 일치율을 보여준다. KLUE-NLI와 KorNLI의 인간 성능 점수 사이의 격차는 또한 KLUE-NLI가 현재 최적의 한국 NLI 데이터 세트라는 증거를 제공한다.\n' +
      '\n' +
      'NLI 벤치마크 데이터세트로서의 주요 목적을 넘어 MNLI 및 SNLI와 같은 영어 데이터세트가 확장됨에 따라 KLUE-NLI가 향후 NLU 연구에 유용한 자원이 되기를 바란다[24, 107, 115].\n' +
      '\n' +
      '### NER(개체 이름 인식)\n' +
      '\n' +
      '명명 개체 인식(NER)의 목표는 비정형 텍스트에서 명명 개체의 경계를 검출하고 유형을 분류하는 것이다. 기업은 사람, 위치, 조직, 시간 표현, 양, 금전적 가치를 지칭하는 일련의 단어일 수 있다.\n' +
      '\n' +
      'NER은 구문 분석, 목표 지향 대화 시스템, 질의 응답 챗봇 및 정보 추출과 같은 응용 분야에 중요하기 때문에 다양한 NLU 벤치마크에는 NER 데이터 세트가 포함되어 있다[137, 57, 78, 54]. 다양한 도메인 및 스타일에서 NER 데이터 세트의 필요성이 대두되었음에도 불구하고, 이러한 요구를 충족시킬 수 있는 기존의 한국 NER 데이터 세트는 거의 없다. 따라서 본 논문에서는 실시간 응용 프로그램에 적용할 수 있는 웹 텍스트를 포함하는 말뭉치에 주석을 달았다.\n' +
      '\n' +
      'KLUE-NER에서 모델은 스팬을 검출하고 입력 문장에 포함된 엔티티의 유형을 분류해야 한다. KLUE-NER에서 사용되는 6개의 엔터티 유형은 사람, 위치, 조직, 날짜, 시간, 수량이다. 캐릭터 레벨 BIO(Begin-Inside-Outside) 태깅 기법을 통해 태깅을 수행하고, 개체 레벨과 캐릭터 레벨 F1 점수를 이용하여 모델의 성능을 평가한다.\n' +
      '\n' +
      '#### 3.4.1 Dataset Construction\n' +
      '\n' +
      '소스 말뭉치 공식 및 비공식 쓰기 스타일을 모두 통합하기 위해 주석에는 WIKITREE와 NSMC의 두 가지 말뭉치를 사용한다. 위키트리(WIKITREE)는 뉴스 기사 코퍼스로 엔터티 유형이 많은 정형 문장을 포함하고 있어 NER의 소스 코퍼스로 적합하다. NSMC는 영화나 TV 쇼에 대한 구어체 리뷰를 포함한다. NSMC의 텍스트는 사용자가 생성한 코멘트이기 때문에 에모지와 속어와 함께 오타와 정규화되지 않은 표현을 포함한다. 이러한 노이즈 데이터 세트는 NER 모델의 응용 분야를 넓히는 데 도움이 될 것이다.\n' +
      '\n' +
      '두 말뭉치의 전처리는 각 말뭉치의 특성을 고려하여 다르게 수행된다. 위키트리(WIKITREE)의 경우 뉴스 기사는 주로 잘 쓰여진 문장으로 구성되어 있기 때문에 단순히 기사를 문장으로 나눈다. 대조적으로, NSMC의 웹 텍스트는 문장의 경계가 흐릿한 구어 스타일로 작성된다. 각 리뷰는 일반적으로 상당히 짧고 동일한 주제에 대해 구성된 문장이기 때문에 각 리뷰를 단일 입력 단위로 사용한다. 또한, 혐오 발언이나 사회적으로 편향된 용어가 포함된 문장은 수동으로 제거된다. 두 말뭉치 모두에 대해 400자 이상의 문장을 제거한다.\n' +
      '\n' +
      '효율적인 주석을 위해 사전 학습된 모델을 사용하여 의사 레이블링을 수행한다. 이 모델은 주석을 위한 빠르고 정확한 개체 태깅을 지원하기 위해 공개적으로 사용 가능한 데이터 세트 KMOU-NER 코퍼스,24를 사용하여 BERT-CRF로 학습된다. 또한 유사 레이블이 없는 문장은 어떤 개체도 포함하지 않는다고 가정하고 필터링한다. 나머지 문장은 위키트리에서 약 80%, NSMC에서 41%를 차지하여 총 36,515개의 문장을 남긴다.\n' +
      '\n' +
      '각주 24: [https://github.com/kmounlp/NER](https://github.com/kmounlp/NER)\n' +
      '\n' +
      '주석 프로토콜 KLUE-NER 주석에는 PS(Person), LC(Location), OG(Organization), DT(Date), TI(Time), QT(Quantity)의 6가지 엔터티 유형을 사용한다. 각 엔티티 타입에 대한 설명은 다음과 같다.\n' +
      '\n' +
      '* PS(Person): 개인 또는 그룹의 이름\n' +
      '* LC(Location): 지역/도 또는 지리적 위치의 이름\n' +
      '* OG(Organization): 조직 또는 기업의 이름\n' +
      '* DT(Date): 날짜/기간/시대/연령 관련 표현\n' +
      '* TI(Time): 시간과 관련된 표현\n' +
      '* QT(Quantity): 단위를 포함하는 수량 또는 숫자와 관련된 표현\n' +
      '\n' +
      '우리는 한국통신기술협회(TTA) NER 지침 25와 MUC-7 [16]의 두 가지 기존 태그 집합의 규약에 따라 위의 집합을 사용한다. TTA 가이드라인은 한국어를 위한 표준화된 NER 태깅 체계이며, 우리는 그 엔티티 유형들의 명칭과 정의를 따른다. TTA의 15개 엔터티 유형 중 MUC-7(DATE, LOCATION, MONEY, ORGANIZATION, PERCENT, PERSON 및 TIME)에서 사용되는 태그 집합과 일치하는 6개의 유형을 선택한다. TTA 집합에서 MONEY 및 PERCENT 유형이 QT(QUANTITY) 유형에 포함됨에 따라 대신 엔터티 유형 QT를 채택한다.\n' +
      '\n' +
      '각주 25: [https://committee.tta.or.kr/data/standard_view.jsp?nowPage=2&pk_num=TTAK.KO-10.0852&commit_code=PG606](https://committee.tta.or.kr/data/standard_view.jsp?nowPage=2&pk_num=TTAK.KO-10.0852&commit_code=PG606)\n' +
      '\n' +
      '가능한 개체 유형이 여러 개인 개체의 경우 모든 사용 사례에 대해 고유한 태그를 할당하는 대신 컨텍스트를 기반으로 태그를 결정합니다. 한 가지 예는 _Cine21_이며, 이는 한국어로 잡지의 이름 또는 잡지의 발행인을 지칭할 수 있다. "나는 서점에서 Cine21을 사서 페이지 단위로 읽었다"와 같은 문장에서는 \'서점에서 무언가를 구입하라\'와 \'페이지 단위로 읽는다\'는 조직이 아니라 미디어(잡지)에 관한 속성이므로 OG 태그를 할당하지 않는다.\n' +
      '\n' +
      '주석을 위한 텍스트가 특정 조건을 충족하지 않는 경우 크라우드 워커가 보고하도록 안내합니다. 예를 들어, 다수의 문장으로 이루어진 텍스트, 문장 형태가 아닌 텍스트, 단편, 명사의 단순 시퀀스 등은 폐기된다. 작업자는 또한 태깅 과정에서 혐오 발언과 다양한 편견을 포함하는 문장을 보고해야 한다.\n' +
      '\n' +
      '개인 식별 정보 측면에서, 우리는 NER의 바로 그 과제는 종종 사람 이름(PS)과 같은 고유 명사의 특정 정보를 필요로 하기 때문에 단순히 정보를 삭제하거나 가명화할 수 없다. 문장의 손실을 최소화하기 위해 주석 처리 후 문장을 통해 검사한다. PS 태그가 포함된 문장들을 조사하고, 한국어 검색 엔진에 등장하는 공인들의 이름이 포함된 문장들을 유지한다. 26개의 다른 문장들은 프라이버시 문제가 있을 경우 제거된다.\n' +
      '\n' +
      '발음 26: Daum: [http://search.daum.net/search?nil_suggest=btn&nil_ch=&rtupcoll=&w=tot&m=&f=&lpp=&q=%CO%CE%B9%BO%CB%BB%F6/Naver](http://search.daum.net/search?nil_suggest=btn&nil_ch=&rtupcoll=&w=tot&m=&f=&lpp=&q=%CO%CE%B9%BO%CB%BB%F6/Naver): [https://people.search.naver.com/](https://people.search.naver.com/)\n' +
      '\n' +
      '어노테이션 프로세스 51은 한국의 크라우드소싱 플랫폼인 DeepNatural27이 모집한 자격을 갖춘 크라우드 워커들이 어노테이션 프로세스에 참여한다. 자격은 파일럿 개체 태깅 테스트를 통과할 때 부여됩니다. 그런 다음 두 언어학자는 크라우드 워커의 주석이 올바른지 여부를 확인합니다. 우리는 유효성 검사 후에도 일부 잘못된 주석이 남아 있음을 발견한다. 따라서 6명의 NLP 연구자가 주석 오류를 수동으로 수정한다.\n' +
      '\n' +
      '각주 27: [https://deepnatural.ai/](https://deepnatural.ai/)\n' +
      '\n' +
      '주석 작업 과정에서 작업자의 부적절함으로 인해 5,354개의 문장이 삭제된다. 프라이버시 문제로 118개의 문장이 탈락하고, 모든 주석이 거짓 긍정이기 때문에 연구진의 검사 후 35개의 문장이 제거된다. 검사 과정에서 총 5,507개의 문장이 탈락해 3만 1,008개의 문장이 나왔다.\n' +
      '\n' +
      '최종 데이터 세트 결과 코퍼스는 각각 21,008, 5,000 및 5,000 문장으로 구성된 train/dev/test 세트로 분할된다(표 8). 개체별 통계량은 표 9에 나와 있다. 도메인 전환 및 일반화 측면에서 모델의 견고성을 확인하기 위해 보이지 않는 개체를 포함하도록 테스트 세트를 설계한다.\n' +
      '\n' +
      '최종화된 엔티티 타입들은 캐릭터 레벨 BIO 태깅 스킴(도 3)에서 태깅된다. 대부분의 영어 및 한국어 NER 데이터 세트에서, 엔티티는 CoNLL 2003 데이터 세트[129]에 이어서 단어-레벨 BIO 스킴으로 태깅된다. 그러나 한국어에서는 두 가지 이유로 백이스페이스 기반의 단어 수준 태깅 기법을 고수하기 어렵다. 먼저, 공백-분할 유닛들(어절들)은 종종 단일 단어가 아니며, 콘텐츠 단어들과 기능 단어들의 합성물(예를 들어, "E1-2-7"(다음 주에는) "E1-2"(다음 주에는) "E1-2"(다음 주에는) "E1-2"(다음 주에는) [46]이다. 둘째, 한국어의 많은 합성어에는 공백이 포함되어 있다. 따라서, 우리는 문자 레벨에서 태그를 선택한다.\n' +
      '\n' +
      '#### 3.4.2 평가 메트릭\n' +
      '\n' +
      'KLUE-NER에 대한 평가 메트릭은 1) 엔티티-레벨 매크로 F1(Entity F1) 및 2) 캐릭터-레벨 매크로 F1(Char F1) 스코어이다. 엔터티 F1 점수는 예측된 엔터티 및 유형이 실제 진실과 정확히 일치하는지 측정합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Source** & **|Train** & **|Devl** & **|Testl** & **Total** \\\\ \\hline WIKITREE & 11,435 & 2,534 & 2,685 & 16,664 \\\\ NSMC & 9,573 & 2,466 & 2,315 & 14,354 \\\\ \\hline\n' +
      '**Total** & **21,008** & **5,000** & **5,000** & **31,008** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: KLUE-NER에 대한 통계.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Source** & **|Train**| & **|Devl** & **|Testl** & **Total** \\\\ \\hline PS & 14,453 (5,428) & 4,418 (2,706) & 4,830 (3,063) & 23,289 (7,124) \\\\ LC & 6,663 (2,068) & 1,649 (896) & 2,064 (1,130) & 9,961 (2,650) \\\\ OG & 8,491 (3,008) & 2,182 (1,291) & 2,514 (1,579) & 12,855 (3,796) \\\\ DT & 8,029 (1,608) & 2,312 (835) & 2,498 (933) & 12,653 (2,060) \\\\ TI & 2,020 (573) & 5,45 (268) & 579 (316) & 3,110 (730) \\\\ QT & 11,717 (3,628) & 3,151 (1,763) & 3,827 (2,369) & 18,019 (4,776) \\\\ \\hline\n' +
      '**Total** & **51,373 (16,313)** & **14,257 (7,759)** & **16,312 (9,390)** & **79,887 (21,136)** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: KLUE-NER에 대한 개체별 통계량. 괄호 안의 숫자는 유형의 수를 나타낸다. 이 테이블은 중복을 제거하지 않으므로 총 번호가 표 8과 일치하지 않습니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:27]\n' +
      '\n' +
      '### Relation Extraction (RE)\n' +
      '\n' +
      '관계 추출(Relation Extraction, RE)은 텍스트 내의 엔티티 쌍들 사이의 의미론적 관계들을 식별한다. 이 관계는 _subject entity_ (\\(e_{\\text{subj}}\\))와 _object entity_ (\\(e_{\\text{obj}}}\\))로 구성된 개체 쌍 사이에서 정의된다. 예를 들어, 문장 \'Kierkegaard\'가 코펜하겐의 부유한 가족으로부터 태어났을 때, 주제 엔티티는 \'Kierkegaard\'이고 객체 엔티티는 \'코펜하겐\'이다. 그러면 목표는 이 두 엔티티들 사이의 적절한 관계, 즉 \'_place_of_birth_\'를 선택하는 것이다.\n' +
      '\n' +
      'RE는 모델이 개체 간의 관계를 올바르게 이해하는지 평가하는 데 적합한 작업이다. KLUE-RE가 언어 이해의 이러한 측면을 포착하도록 하기 위해 대규모 RE 벤치마크를 포함한다. 한국어로 공개된 대규모 RE 벤치마크가 없기 때문에 자체 데이터셋을 수집하고 주석을 달았다.\n' +
      '\n' +
      '우리는 RE를 단일 문장 분류 과제로 공식화한다. 모델은 주어진 문장 내에서 두 개체 사이의 관계를 설명하는 미리 정의된 관계 클래스 중 하나를 선택한다. 즉, RE 모델은 문장 \\(s\\)에서 엔터티 쌍 \\((e_{\\text{subj}},e_{\\text{obj}})\\)의 적절한 관계 \\(r\\)를 예측하는데, 여기서 \\(e_{\\text{subj}}\\)는 주체 엔터티이고 \\(e_{\\text{obj}}}\\)는 객체 엔터티이다. 우리는 \\((e_{\\text{subj}},r,e_{\\text{obj}})\\)을 관계 삼중항이라고 부른다. 개체들은 각 문장 \\(s\\)에서 대응하는 스팬으로 표시된다. 개인 관련 관계 18개, 조직 관련 관계 11개, _무관계_로 구성된 30개의 관계 클래스가 있다. 이러한 클래스에 대한 자세한 설명은 표 10에 나와 있다. 우리는 _상관 없음_을 제외하고 계산된 마이크로 F1 점수와 30개 클래스를 모두 포함하는 정밀-리콜 곡선 아래의 면적을 사용하여 모델을 평가한다.\n' +
      '\n' +
      '#### 3.5.1 Data Construction\n' +
      '\n' +
      '원격 감독[91]은 대규모 RE 벤치마크를 구축하는 인기 있는 방법이다. Freebase와 같은 기존의 대규모 지식베이스(KB)에서 관계 트리플렛 \\((e_{\\text{subj}},r,e_{\\text{obj}})\\)을 이용한다. 큰 말뭉치의 문장 \\(s\\)이 NER 모델에 의해 동시에 검출된 \\((e_{\\text{subj}},e_{\\text{obj}})\\)을 포함하는 경우, 쌍이 포함된 문장이 해당 관계를 표현할 것이라고 가정하여 관계 레이블 \\(r\\)을 갖는 데이터 세트에 추가한다. 이 접근법은 값비싼 인간 주석을 필요로 하지 않으므로 비용 효율적인 방식으로 대규모 RE 벤치마크를 구축할 수 있다.\n' +
      '\n' +
      '이러한 장점에도 불구하고 원거리 감독은 가정이 충족되지 않을 때 잘못된 관계 레이블로 끝나는 경우가 많다. 특히, 이는 서로 연관된 엔티티들의 쌍들만을 고려하며, 이는 그러한 코퍼스 상에서 트레이닝된 RE 모델이 임의의 주어진 엔티티들의 쌍 사이의 일부 관계의 존재를 과도하게 예측하게 한다. 즉, 이러한 예측자들로부터의 예측 관계 클래스 분포는 현실적이지 않다[116]. 따라서 Zhang et al. [153] 및 Nam et al. [93]은 원거리 감독에 의해 추출된 잘못된 관계를 완화하기 위해 크라우드 워커를 고용할 것을 제안한다. Riedel et al. [116]은 또한 RE 모델들이 잘못된 긍정 관계들을 과도하게 예측하는 것을 방지하기 위해 관련 없는 엔티티 쌍들을 의도적으로 수집한다.\n' +
      '\n' +
      '개요 우리는 이러한 약점을 해결하고 우리의 상황에 더 잘 부합하기 위해 위의 원거리 감독의 원래 전략을 수정한다. 먼저, 작은 한국어 KB29로부터 삼중항 \\((e_{\\text{subj}},r,e_{\\text{obj}})\\)을 수집하고, 후보 삼중항들의 풀을 확장하기 위해 WIKIPEDIA와 NAMUWIKI30의 인포박스를 파싱하여 추가 삼중항들을 구축한다. 그런 다음 자동으로 생성된 관계 레이블을 직접 사용하는 원거리 감독과 비교하여 크라우드 워커에게 문장 내에서 각 후보 트리플렛의 올바른 관계 클래스를 선택하도록 요청한다. 또한, 벤치마크에서 보다 현실적인 관계 클래스 분포를 얻기 위해 \\(s\\)의 개체 쌍을 무작위로 샘플링한다. 이러한 예는 기존 KB에서 보이지 않는 엔터티를 포함할 뿐만 아니라 관련이 없을 가능성이 더 높다(_no_relation_).\n' +
      '\n' +
      '각주 29: [https://aihub.or.kr/aidata/84](https://aihub.or.kr/aidata/84)\n' +
      '\n' +
      '각주 30: [https://namu.wiki](https://namu.wiki)\n' +
      '\n' +
      '이 절차는 (1) 후보 문장 수집, (2) 관계 스키마 정의, (3) 개체 검출, (4) 개체 쌍 선택 및 (5) 관계 주석의 다섯 단계로 나눌 수 있다. 우리는 이 섹션의 나머지 부분에서 각 단계를 자세히 설명한다.\n' +
      '\n' +
      '1. 후보 문장 수집 WIKIPEDIA, WIKITREE 및 POLICY 코퍼라에서 후보 문장을 샘플링하여 명명된 개체 및 관계적 사실의 다양한 세트를 커버한다. 우리의 작업은 단일 문장을 다루기 때문에 전처리 단계에서 한국어 문장 분할기(31)에 의해 분할된 개별 문장을 이용한다. 한국어 혐오 발화 데이터셋에 학습된 분류기를 이용하여 바람직하지 않은 사회적 편향을 포함하고 혐오 발화로 간주되는 문장을 걸러낸다[92].\n' +
      '\n' +
      '각주 31: [https://github.com/hyunwoongko/kss](https://github.com/hyunwoongko/kss)\n' +
      '\n' +
      '2. 관계 스키마 정의 TAC-KBP(Text Analysis Conference Knowledge Base Population)로부터 스키마를 기반으로 관계 스키마를 설계한다[87]. 스키마는 엔터티 유형 및 관계 클래스를 정의합니다. TAC-KBP와 유사하게, 우리는 \\(e_{\\text{subj}}}\\)을 PER(Person) 또는 ORG(Organization) 유형으로 제한한다. \\ (e_{\\text{obj}}\\)는 PER, ORG, LOC(Location), DAT(Date and time), POH(Other 고유명사), NOH(Other numerals) 중 하나를 가질 수 있다. 관계 클래스는 Yu et al. [149]에 따라 TAC-KBP의 원본 클래스를 코퍼스에 적응시킨다.\n' +
      '\n' +
      '우리는 코퍼스에서 드물게 나타나는 관계 클래스인 _org:website_, _per:shareholder_, _per:cause_of_death_, _per:charges_ 및 _per:age_를 제거합니다. 같은 이유로 우리는 _org:parents_를 _org:member_of_에, _org:subsidiaries_를 _org:members_에 통합합니다. TAC-KBP의 분류는 한국의 지역적 계층 구조를 정확하게 반영하지 않기 때문에 접두사 _country_of_, _city_of_ 및 _stateoprovince_of_를 _place_of_로 통합한다. 우리는 _org:product_, _per:product_ 및 _per:colleague_와 같이 코퍼스에 자주 나타나는 추가 클래스를 소개합니다.\n' +
      '\n' +
      '* _org:product_: 조직에서 생산한 제품 또는 상품입니다. 조직이 주최하는 행사, 창업한 사업 등 무형의 재화가 이에 해당한다.\n' +
      '* _per:product_: 사람이 생산한 제품입니다. 예술 작품(예: 책, 음악, 영화) 또는 작품을 제작하는 데 기여합니다.\n' +
      '* _per:colleague_: 한 사람이 함께 일한다면 누군가의 동료가 될 수 있습니다. 정당이나 동맹 등 같은 조에 속한 두 사람도 동료다.\n' +
      '\n' +
      '3. 엔티티 검출 모든 후보 문장에서 명명된 엔티티를 자동으로 검출한다. 기존의 두 개의 한국어 NER 자원에 대해 각각 2개의 명명된 개체 인식(NER) 모델을 구축하기 위해 한국어 32에 대해 사전 학습된 ELECTRA를 미세 조정한다. 하나는 국립국어원에서 제공[98]하고, 다른 하나는 한국해양대학교에서 구축[33]하고 있다. 이러한 자원에 정의된 개체명 유형을 스키마에서 이전에 정의된 개체 유형과 호환되도록 수정한다. 우리는 가능한 한 많은 개체를 추출하기 위해 두 모델의 예측 합을 취한다. 크라우드 소싱을 사용하여 나중에 설명하는 것처럼 탐지된 엔터티의 잘못된 경계를 수정합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Relation Class** & **Description** \\\\ \\hline _no\\_relation_ & No relation in between \\((e_{\\text{subl}},e_{\\text{obj}})\\) \\\\ \\hline _org:dissolved_ & The date when the specified organization was dissolved \\\\ _org:founded_ & The date when the specified organization was founded \\\\ _org:place\\_of\\_headquarters_ & The place which the headquarters of the specified organization are located in \\\\ _org:alternate\\_names_ & Alternative names called instead of the official name to refer to the specified organization \\\\ _org:member\\_of_ & Organizations to which the specified organization belongs \\\\ _org:members_ & Organizations which belong to the specified organization \\\\ _org:political/religious\\_affiliation_ & Political/religious groups which the specified organization is affiliated in \\\\ _org:product_ & Products or merchandise produced by the specified organization \\\\ _org:founded\\_by_ & The person or organization that founded the specified organization \\\\ _org:top\\_members/employees_ & The representative(s) or members of the specified organization \\\\ _org:number\\_of\\_employees/members_ & The total number of members that are affiliated in the specified organization \\\\ \\hline _per:date\\_of\\_birth_ & The date when the specified person was born \\\\ _per:date\\_of\\_death_ & The date when the specified person died \\\\ _per:place\\_of\\_birth_ & The place where the specified person was born \\\\ _per:place\\_of\\_death_ & The place where the specified person died \\\\ _per:place\\_of\\_residence_ & The place where the specified person lives \\\\ _per:origin_ & The origins or the nationality of the specified person \\\\ _per:employee\\_of_ & The organization where the specified person works \\\\ _per:schools\\_attended_ & A school where the specified person attended \\\\ _per:alternate\\_names_ & Alternative names called instead of the official name to refer to the specified person \\\\ _per:parents_ & The parents of the specified person \\\\ _per:children_ & The children of the specified person \\\\ _per:siblings_ & The brothers and sisters of the specified person \\\\ _per:spouse_ & The spouse(s) of the specified person \\\\ _per:other\\_family_ & Family members of the specified person other than parents, children, siblings, and spouse(s) \\\\ _per:colleagues_ & People who work together with the specified person \\\\ _per:product_ & Products or artworks produced by the specified person \\\\ _per:religion_ & The religion in which the specified person believes \\\\ _per:title_ & Official or unofficial names that represent the occupational position of the specified person \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: KLUE-RE의 관계 스키마에 정의된 30개의 관계 클래스. 관계 클래스 \\(r\\)는 18개의 개인 관련 관계, 11개의 조직 관련 관계 및 _무관계_로 구성된 다음 중 하나여야 한다.\n' +
      '\n' +
      '## 4 엔터티 쌍 선택\n' +
      '\n' +
      '주어진 문장의 엔터티 집합 \\(E\\) \\(s\\)에서 두 개의 엔터티를 선택하여 엔터티 쌍을 \\((e_{\\text{subj}},e_{\\text{obj}})\\)로 만든다. 그렇게 할 때, 우리는 (1) KB 기반 샘플링과 (2) 균일 샘플링의 두 가지 별개의 접근법을 취한다.\n' +
      '\n' +
      '첫 번째 접근 방법에서는 각 개체 쌍 \\((e_{\\text{subj}},e_{\\text{obj}})\\)이 삼중항 \\((e_{\\text{subj}},r,e_{\\text{obj}})\\) 풀에 나타나도록 개체들의 부분집합만을 고려한다. 우리는 두 가지 출처에서 이 세쌍둥이를 수집합니다. 먼저, 한국어 KB.34를 이용하여 초기 트리플렛 풀(triplets pool)을 생성하는데, 이는 한국어 KB로부터의 트리플렛 수(\\(\\sim\\)800k)가 프리베이스(\\(\\sim\\)2b)에 비해 작기 때문이다. 이 트리플렛 풀은 WIKIPEDIA와 나무위키에서 인포박스를 수집한 후 파싱하여 확장한다. 한국 대통령과 같은 빈발 개체가 과도하게 포함되는 것을 피하기 위해 표본 추출 시 \\((e_{\\text{subj}},e_{\\text{obj}})\\) 사이의 동시 발생 횟수에 상한을 설정했다[153].\n' +
      '\n' +
      '각주 34: 정부 지원 기관인 NIA에 의해 공개되었습니다. [https://aihub.or.kr/aidata/84](https://aihub.or.kr/aidata/84)에서 사용할 수 있습니다.\n' +
      '\n' +
      '두 번째 접근법에서 \\((e_{\\text{subj}},e_{\\text{obj}})\\)은 주어진 문장 \\(s\\)의 전체 개체 집합 \\(E\\)에서 무작위로 균일하게 샘플링된다. 샘플링된 쌍이 그들 사이에 어떠한 관계를 갖는지 여부에 대한 큐가 없기 때문에, 쌍은 관련성이 없을 가능성이 높다(_no_relation_). 관련되지 않은 쌍들은 두 임의의 개체들 사이의 현실적인 관계 분포에서 많은 부분을 설명할 것이다. 따라서 이 접근법은 실제 시나리오를 설정하는 데 도움이 된다. 그러한 쌍은 또한 제1 접근법에서 선택되지 않은 엔티티들을 포함할 가능성이 있다. 이것은 KB에 독립적인 개체 쌍 및 그 관계를 캡처하는 것으로 이어진다.\n' +
      '\n' +
      '## 5 주석 관계\n' +
      '\n' +
      '한국형 크라우드소싱 플랫폼 DeepNatural,35에 의해 모집된 근로자들에게 각 엔티티 쌍 \\((e_{\\text{subj}},e_{\\text{obj}})\\)에 관계 레이블 \\(r\\)로 주석을 달 것을 요청한다. 우리는 근로자들에게 과거의 관계가 아닌 현재의 관계에 집중하도록 지시한다. 예를 들어 문장에 설명된 사람이 특정 조직의 이전 구성원인 경우 작업자는 _per:employee_of_ 관계를 선택하지 않도록 요청됩니다. 또한 주어진 문장 내에서만 맥락으로부터 관계를 추론하기 위해 외부 지식, 즉 상식에 의존하지 않도록 요청합니다. 노동자들은 혐오 표현, 편향된 표현 또는 개인 식별 정보가 포함된 예를 보고한다. 또한, 그들은 잘못된 엔티티 경계를 갖는 문장들을 보고하도록 요청받는다.\n' +
      '\n' +
      '각주 35: [https://deepnatural.ai/](https://deepnatural.ai/)\n' +
      '\n' +
      '우리는 163명의 자격을 갖춘 작업자를 고용하며, 각 작업자는 파일럿 주석 단계에서 5개 질문 중 최소 4개 질문에 올바르게 라벨을 붙였다. 파일럿 단계 이후 3명의 작업자가 독립적으로 각 예제에 할당되어 관계에 레이블을 지정한다. 그림 3.5.1은 크라우드소싱을 위한 주석 도구를 보여준다. 주석자의 인지적 부담을 줄이기 위해 처음에는 적은 수의 후보 관계를 제공한다. 후보들은 NER 모델들에 의해 예측된 엔티티 쌍의 유형들 사이에서 정의될 수 있는 관계들로 구성된다. 후보에서 적절한 \\(r\\)을 찾을 수 없는 경우 모든 관계 클래스로 확장된다.\n' +
      '\n' +
      '그림 4: 크라우드소싱을 위한 주석 도구. 주요 기능은 빨간색으로 영어로 번역됩니다.\n' +
      '\n' +
      '우리는 다수가 투표한 라벨을 금 라벨로 받아들인다. 다수 레이블이 없는 각 예제에 대해 상위 30명의 주석자는 주석이 달린 레이블에서 최종 레이블을 선택합니다. 우리는 혐오 발언, 편파적이거나 개인 정보 보호 문제가 있다고 보고된 사례를 포함하지 않는다. 주석이 달린 데이터 세트의 주석 간 일치(Krippendorff\'s \\(\\alpha\\))는 0.701 [67]이다.\n' +
      '\n' +
      '최종 DatasetKLUE-RE는 32,470개의 훈련, 7,765개의 개발 및 7,766개의 테스트 예제로 구성된다. 실제 시나리오의 경우 개발 및 테스트 세트를 구축할 때 균일한 샘플링에서 생성된 예제만 사용합니다. 테스트 세트에서, 우리는 트레이닝 세트에 나타나지 않는 엔티티들을 갖는 문장들만을 포함한다.\n' +
      '\n' +
      'KLUE-RE에서 문장의 평균 길이는 화이트 스페이스를 포함하여 95.9자 이다. 개체 유형의 비율은 PER(38.1%), ORG(36.3%), LOC(6.2%), DAT(6.2%), POH(11.9%), NOH(1.3%)이다. 관계 클래스의 분포는 표 11과 같다.\n' +
      '\n' +
      '#### 3.5.2 평가 메트릭\n' +
      '\n' +
      'KLUE-RE 평가지표는 1) 기존 사례들에 대한 미시적 F1 점수, 2) 모든 클래스에 대한 정밀-리콜 곡선(AUPRC) 아래의 영역이다. 마이크로 F1 점수는 마이크로 정밀도와 마이크로 리콜의 조화 평균이다. 모든 계층의 집계된 기여도의 F1 점수를 측정한다. 각 표본에 동일한 중요성을 부여하므로 자연스럽게 다수 집단에 더 많은 가중치를 부여한다. 부정적인 클래스를 예측하는 데 더 중점을 두는 모델을 장려하지 않기 위해 이 메트릭에 대한 지배적인 클래스(\\(no\\_relation\\))를 제거한다. AUPRC는 x축은 재현율이고 y축은 모든 관계 클래스의 정밀도인 정밀도-리콜 곡선 아래의 평균화된 영역이다. 중요한 긍정적인 예가 거의 발생하지 않는 불균형 데이터 설정에 유용한 메트릭입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l r r r r r r} \\hline \\hline  & \\multicolumn{2}{c}{**Train**} & \\multicolumn{2}{c}{**Dev**} & \\multicolumn{2}{c}{**Test**} \\\\ \\cline{2-7}\n' +
      '**Relation Class** & Count & Ratio & Count & Ratio & Count & Ratio \\\\ \\hline _no\\_relation_ & 9,534 & 29.36\\% & 4,631 & 59.64\\% & 4,632 & 59.64\\% \\\\ \\hline _org:dissolved_ & 66 & 0.20\\% & 11 & 0.14\\% & 10 & 0.13\\% \\\\ _org:founded_ & 450 & 1.39\\% & 20 & 0.26\\% & 20 & 0.26\\% \\\\ _org:place\\_of\\_headquarters_ & 1,195 & 3.68\\% & 194 & 2.50\\% & 193 & 2.49\\% \\\\ _org:alternate\\_names_ & 1,320 & 4.07\\% & 78 & 1.00\\% & 77 & 0.99\\% \\\\ _org:member\\_of_ & 1,866 & 5.75\\% & 104 & 1.34\\% & 105 & 1.35\\% \\\\ _org:members_ & 420 & 1.29\\% & 122 & 1.57\\% & 122 & 1.57\\% \\\\ _org:political/religious\\_affiliation_ & 98 & 0.30\\% & 13 & 0.17\\% & 13 & 0.17\\% \\\\ _org:product_ & 380 & 1.17\\% & 235 & 3.03\\% & 235 & 3.03\\% \\\\ _org:founded\\_by_ & 155 & 0.48\\% & 11 & 0.14\\% & 11 & 0.14\\% \\\\ _org:top\\_members/employees_ & 4,284 & 13.19\\% & 513 & 6.61\\% & 514 & 6.62\\% \\\\ _org:number\\_of\\_employees/members_ & 48 & 0.15\\% & 17 & 0.22\\% & 18 & 0.23\\% \\\\ \\hline _per:date\\_of\\_birth_ & 1,130 & 3.48\\% & 12 & 0.15\\% & 12 & 0.15\\% \\\\ _per:date\\_of\\_death_ & 418 & 1.29\\% & 13 & 0.17\\% & 13 & 0.17\\% \\\\ _per:place\\_of\\_birth_ & 166 & 0.51\\% & 11 & 0.14\\% & 10 & 0.13\\% \\\\ _per:place\\_of\\_death_ & 40 & 0.12\\% & 10 & 0.13\\% & 11 & 0.14\\% \\\\ _per:place\\_of\\_residence_ & 193 & 0.59\\% & 124 & 1.60\\% & 125 & 1.61\\% \\\\ _per:origin_ & 1,234 & 3.80\\% & 118 & 1.52\\% & 118 & 1.52\\% \\\\ _per:employee\\_of_ & 3,573 & 11.00\\% & 242 & 3.12\\% & 241 & 3.10\\% \\\\ _per:schools\\_attended_ & 82 & 0.25\\% & 11 & 0.14\\% & 11 & 0.14\\% \\\\ _per:alternate\\_names_ & 1,001 & 3.08\\% & 104 & 1.34\\% & 103 & 1.33\\% \\\\ _per:parents_ & 520 & 1.60\\% & 27 & 0.35\\% & 27 & 0.35\\% \\\\ _per:children_ & 304 & 0.94\\% & 27 & 0.35\\% & 27 & 0.35\\% \\\\ _per:siblings_ & 136 & 0.42\\% & 24 & 0.31\\% & 24 & 0.31\\% \\\\ _per:spouse_ & 795 & 2.45\\% & 41 & 0.53\\% & 40 & 0.52\\% \\\\ _per:other\\_family_ & 190 & 0.59\\% & 34 & 0.44\\% & 35 & 0.45\\% \\\\ _per:colleagues_ & 534 & 1.64\\% & 220 & 2.83\\% & 220 & 2.83\\% \\\\ _per:product_\n' +
      '\n' +
      '#### 3.5.3 관련 작업\n' +
      '\n' +
      '많은 연구자들은 기계 학습 기법을 적용하여 일반 텍스트에서 개체 쌍 간의 관계적 사실을 자동으로 식별함으로써 비정형 텍스트에서 KB를 구축하려고 시도한다. Doddington et al. [31] 및 Hendrickx et al. [52]는 일반적인 도메인 텍스트에 대한 비교적 적은 수의 관계 클래스를 포함하여 이러한 모델을 트레이닝하기 위해 영어 데이터세트를 구성한다. Mintz et al. [91]은 또한 일반 텍스트를 KB의 스키마에 정렬하여 자동으로 주석을 달도록 원거리 감독을 제안한다. 이를 통해 연구자들은 RE 데이터 세트의 크기를 확장할 수 있다[116, 147, 149, 153, 48]. 이러한 최근 연구 중 TACRED [153]은 주로 개인 및 조직 엔티티에 초점을 맞춘 인기 있는 관계 스키마 TAC-KBP [87]을 기반으로 구축된 가장 널리 사용되는 데이터 세트이다. 구체적으로, TACRED는 42개의 관계 클래스로 주석이 달린 106,264개의 예를 포함한다. Yu et al. [149]는 또한 대화 도메인에 적응된 36개의 관계 클래스를 얻기 위해 TAC-KBP를 정제함으로써 대화 기반 RE 태스크를 제안한다. 또한 TAC-KBP를 사용하여 관계 스키마를 구축하고 상황에 맞게 수정한다.\n' +
      '\n' +
      '영어가 아닌 언어의 경우, 중국어[141] 1개, 독일어[121] 1개, 프랑스어[55] 1개 등 현존하는 벤치마크가 거의 없다. Nam et al. [93]은 예를 자동으로 생성하고 주석을 달기 위해 원거리 감독을 사용하여 한국어로 된 RE 데이터 세트를 제안한다. 그러나 비교적 작은 테스트 세트(\\(\\sim\\)3k)를 가지고 있어 전체 49개의 관계 클래스에 대한 성능을 제대로 평가하기 어렵다. 더욱이, 부정적인 클래스(우리의 경우_no_relation_)가 없기 때문에, 모델들이 잘못된 긍정들을 지나치게 예측하도록 장려할 가능성이 있다[153]. 따라서 우리는 한국어 모델을 제대로 평가하기 위해 KLUE-RE를 표준 대규모 RE 벤치마크로 고려한다.\n' +
      '\n' +
      '#### 3.5.4 Conclusion\n' +
      '\n' +
      '본 논문에서는 한국어를 위한 대규모 인간 주석이 있는 RE 벤치마크인 KLUE-RE를 제안한다. 본 논문에서는 대규모 및 최신 한국형 KB의 부족을 극복하기 위해 효과적인 주석 기법과 결합된 효율적인 후보 수집 방법을 설계한다. KLUE-RE는 온라인 정보 추출에 사용될 뿐만 아니라 비정형 텍스트로부터 대규모 지식 그래프를 구축하는 데 기여할 수 있다. 따라서 KLUE-RE는 한국어로 지속적으로 성장하는 대규모 공공 KB를 구축하는 출발점이 될 뿐만 아니라 귀중한 한국 NLU 벤치마크가 될 것으로 기대한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:33]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:34]\n' +
      '\n' +
      'UAS는 HEAD 예측에서 매크로 F1 점수를 계산하는 반면 LAS는 HEAD 예측이 올바른 DEPREL에서 매크로 F1 점수를 계산한다. 두 점수 모두 모든 수업에서 동일한 중요성을 부여한다. LAS의 경우 DEPREL 분포가 고도로 치우쳐 있기 때문에 하단에서 1%의 누적 빈도를 가진 레이블의 예측을 단일 레이블(OTHERS)로 결합한 다음 F1 점수를 계산한다. 덜 보이는 라벨은 표 14에 나와 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Source** & **|Train** & **|Devl** & **|Testl** & **Total** \\\\ \\hline WIKITREE & 5,000 & 1,000 & 1,250 & 7,250 \\\\ AIRBNB & 5,000 & 1,000 & 1,250 & 7,250 \\\\ \\hline\n' +
      '**Total** & **10,000** & **2,000** & **2,500** & **14,500** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 13: KLUE-DP에 대한 통계.\n' +
      '\n' +
      '#### 3.6.3 관련 작업\n' +
      '\n' +
      '펜 트리뱅크[84]는 1989년부터 1996년까지 만들어진 선거구 구문 분석 데이터 세트이며, IBM 컴퓨터 매뉴얼, 간호 노트, 월스트리트저널 기사, 전화 대화 등 약 300만 단어의 크기를 가지고 있다. 기호 등의 메타 태그를 조합하여 총 48개의 POS 태그와 18개의 구문 태그를 사용한다. 펜 트리뱅크는 의존성 파싱이 널리 퍼지기 전에 가장 잘 알려진 파싱 데이터 세트였다. 이후 연구에서는 Penn Treebank를 종속 구문 분석으로 전환한다[86, 20].\n' +
      '\n' +
      '대표적인 DP 코퍼스는 UD(Universal Dependencies) 데이터세트이다. 40 UD는 다양한 언어로 통합된 트리뱅크 주석을 목표로 하는 DP 데이터의 사실상의 표준이다. 12개의 태그로 구성된 구글 유니버설 POS[106]를 개발하고 이를 25개의 서로 다른 언어에 적용한 코퍼스를 구축하였다. 또한 de Marneffe and Manning[28]은 Stanford parsers[65]에서 사용되는 의존 구문 분석 마커에 대한 지침을 연구했다. 2013년 유니버설 의존 트리뱅크 프로젝트는 위의 두 연구를 결합하여 다국어에 대한 일관된 주석 시스템을 갖추려고 시도했다. UD는 이를 수정·보완하면서 시작되었다. UD는 2015년에 처음 시작되었으며 Nivre et al. [97]과 총 10개 언어의 10개의 말뭉치가 홈페이지를 통해 공개되었다. 2021년 현재 UD(UD 2.7v)는 104개 언어와 183개의 말뭉치를 제공한다.\n' +
      '\n' +
      '각주 40: [https://universaldependencies.org](https://universaldependencies.org)\n' +
      '\n' +
      'UD 방식으로 구축된 코퍼스는 CoNLL-U 형식이라는 일정한 구조에 따라 만들어진다. UD는 동일한 태그와 어노테이션 시스템을 서로 다른 언어로 사용하여 일반적인 언어 보편성을 연구하는 것을 목표로 하기 때문에 각 말뭉치를 통합하여 관리하기 위한 통일된 형식이 필요하다. CoNLL-U 포맷은 유니버설 의존성 파싱을 잘 나타낼 수 있도록 이 CoNLL 포맷을 수정한 버전이다. CoNLL-U 포맷은 10개의 열로 구성되며, 각 열은 단어 인덱스(ID), 단어 형태(FORM), 단어 형태의 레마(LEMMA), 범용 품사 태그(UPOS), 언어 특정 품사 태그(XPOS), 형태소 특징 리스트(FEATS), 현재 단어의 헤드(HEAD), 의존 관계(DEPREL), 향상된 의존 그래프(DEPS), 임의의 다른 주석(MISC)을 나타낸다. 이 형식에서 각 단어는 서로 다른 관련 특징(단어 형태, lemma, POS 태그 등)과 함께 선 위에 있으며 최종 데이터 세트에서 이 형식을 채택한다.\n' +
      '\n' +
      '한국어의 DP 말뭉치는 UD 방식을 따르는 말뭉치와 그렇지 않은 말뭉치로 나뉜다. 전자로는 구글 한국어 유니버설 의존 트리뱅크(GKT), 카이스트 한국어 유니버설 의존 트리뱅크(KTB), 펜 한국어 유니버설 의존 트리뱅크(PKT) 등이 있다. 이 세 가지 데이터 세트는 UD 스킴에 따라 구글 한국어 트리뱅크[86], 카이스트 트리뱅크[128], 펜 한국어 트리뱅크[47]에서 각각 변환된다[20]. 이들은 먼저 머리 찾기 규칙에 따라 자동으로 변환된 후 휴리스틱하게 수정되었다. 이들은 각각 6k, 27k, 5k 문장으로 구성되어 있으며, 블로그, 뉴스와이어, 문학, 학술, 원고 등의 장르를 포함하고 있다. 이 중 PKT는 한국어의 특성을 더욱 드러내기 위해 분석단위와 몇 가지 규칙을 변경하여 개정하였다[99].\n' +
      '\n' +
      'UD 방식을 따르지 않는 코포라로는 한국전자통신연구원(ETRI)이 구축한 TTA DP 코퍼스와 국립국어원(NIKL)이 구축한 모두 코퍼스[98]가 있다. 두 말뭉치 모두 CoNLL 형식을 따르며, 21세기 세종계획 말뭉치에서 개발한 자신만의 태그셋을 사용한다. 21세기 세종 계획의 통사 분석 코퍼스는 펜트리뱅크와 유사한 도식에 따라 선거구 문법에 따라 구성된다. 두 단어 사이의 지배적인 관계만을 파악하는 종속 문법과 달리 선거구 문법은 단어 사이의 관계를 위계적으로 파악한다. 그러나 한국어는 어순이 비교적 자유롭기 때문에 구-구조 구문 분석보다 의존 구문 분석이 더 적합하다. 21세기 세종계획 말뭉치를 DP 형식으로 변환하는 연구들이 진행되었고, 이후 21세기 세종계획의 태그셋을 사용하되 의존 구문 분석을 따르는 말뭉치가 구축되었다. TTA DP 말뭉치의 크기는 약 27k이고, Modu 말뭉치의 크기는 약 2000k이다. 일반적인 언어적 특성을 강조하는 UD와 달리 개별 언어로서 한국어의 특성을 더 잘 나타내며, 한국 DP 태깅의 국가적인 기준이 된다. 또한, TTA 방식에 따라 이미 주석이 달린 코퍼스가 존재하므로, 이 코퍼스와 벤치마크 간의 호환성을 고려한다. 이러한 이유로 TTA tagset을 이용하여 KLUE-DP를 구성하였다.\n' +
      '\n' +
      '#### 3.6.4 Conclusion\n' +
      '\n' +
      '우리는 공식 뉴스와 비공식 사용자 생성 웹 데이터로 구성된 한국 DP 벤치마크 KLUE-DP를 구축한다. KLUE-DP는 여러 도메인에서 사용할 수 있는 DP 모델을 개발하는 데 도움이 된다. DP 성능 향상을 위해 POS 태깅을 함께 수행하며, 기존의 TTA 데이터셋을 수정하여 DP와 POS 태깅에 대한 태그셋과 가이드라인을 적용한다. 이 지침은 한국어(응집, 자유어순 등)의 특성을 반영하여 맞춤형으로 작성되었으며, 웹 데이터에서 술어의 누락이나 띄어쓰기의 오류도 해결하였다. 우리의 벤치마크가 한국 DP 모델 및 기타 자연어 처리 개발에 도움이 되기를 바랍니다.\n' +
      '\n' +
      '### MRC(Machine Reading Comprehension)\n' +
      '\n' +
      '기계독해(Machine reading comprehension, MRC)는 주어진 텍스트 지문을 읽은 후 지문에 대한 질문에 답하는 모델의 능력, 즉 이해 능력을 평가하기 위해 고안된 과제이다.\n' +
      '\n' +
      '기존의 널리 사용되는 대부분의 MRC 벤치마크는 대부분 영어로 되어 있다[21, 56, 60, 112, 113, 145, 150]. 이러한 자원은 텍스트 이해를 측정하는 가장 직관적인 방법 중 하나이기 때문에 사전 훈련된 언어 모델을 평가하는 데 널리 사용된다. SQuAD 1.1[112] 및 SQuAD 2.0[113]은 GLUE[30, 82, 22, 71]과 함께 인기 있는 평가 과제이다. BooIQ[21], ReCoRD[150], MultiRC[60]은 언어 모델의 엄격한 평가를 위해 SuperGLUE의 멤버로 선택된다. 최근, 주어진 텍스트 패시지가 없는 MRC 태스크로 볼 수 있는 오픈 도메인 QA 태스크[69, 56, 145, 38]가 지식 집약적 NLP 태스크 벤치마크[105]에 포함되어 있다.\n' +
      '\n' +
      '이러한 데이터 세트에 의해 동기화된 MRC는 인도네시아[137], 중국[142], 러시아[125]와 같은 다양한 언어에 대한 NLU 벤치마크에서 필수적인 과제가 되었다. 그러나 한국어에서는 기존의 한국어 MRC 데이터 세트가 덜 도전적이거나 접근이 제한적이거나 단순히 영어 데이터 세트에서 기계 번역되기 때문에 적절한 MRC 벤치마크를 사용할 수 없다[79, 1, 74]. 따라서 우리는 KLUE에 MRC를 포함하고 다음 기여와 함께 새로운 도전적인 한국 MRC 벤치마크(KLUE-MRC)를 만든다:\n' +
      '\n' +
      '* **여러 질문 유형 제공**: 모델의 MRC 기능의 다양한 측면을 평가하기 위해 패러프레이즈, 다중 문장 추론 및 응답 불가능의 세 가지 질문 유형을 제공합니다. 각 유형에 대한 특정 규칙 집합과 함께 엄격한 지침을 준수하여 질문을 수집합니다.\n' +
      '* **추론 바로 가기 방지**: 작업자가 질문을 생성할 때 어휘 및 구문 변형을 적용하여 MRC 모델이 간단한 단어 매칭으로 추론 바로 가기를 악용하는 것을 방지합니다. 또한, 전체 질의 문장을 고려하여 답변할 수 있는 질문을 생성하는 것을 목표로 한다.\n' +
      '* **모든 사람이 액세스할 수 있는 다중 통로 도메인** : 뉴스 도메인 통로와 위키피디아를 포함합니다. KLUE-MRC의 CC BY-SA 라이선스를 보장하기 위해 해당 뉴스 제공자와 계약을 체결했다.\n' +
      '\n' +
      '우리는 주어진 텍스트 구절로부터 문제의 정답 범위를 예측하는 과제로 MRC를 공식화한다. 입력은 질문의 연결된 시퀀스이고, 구절은 구분자로 구분된다. 출력은 통로 내의 예측된 답변 스팬의 시작 및 종료 위치이다.\n' +
      '\n' +
      '1) 정확한 일치(EM)와 2) 문자 수준 ROUGE-W의 두 가지 메트릭으로 모델을 평가한다. 문자 수준 ROUGE-W는 이전 한국 MRC 데이터 세트에서 사용된 문자 수준 F1 점수와 다르다. 질문이 주어진 구절 내에서 답할 수 없는 경우 모델은 빈 대답 문자열을 예측해야 합니다. 우리의 측정 기준의 동기는 섹션 3.7.2에 설명되어 있다.\n' +
      '\n' +
      '#### 3.7.1 Dataset Construction\n' +
      '\n' +
      '출처 코퍼라 퍼스트는 한국 위키페디아와 한국경제신문과 ACROFAN에서 제공하는 뉴스 기사의 구절을 수집한다. WIKIPEDIA 문서는 MRC 데이터 세트를 만드는 데 가장 일반적으로 사용되는 리소스 중 하나이다. 우리는 또한 구절의 다양성을 높이기 위해 현대 사회 문제를 보도하는 뉴스 기사를 포함한다. 한국경제신문과 ACROFAN에서 제공하고 있습니다. 뉴스 기사는 일반적으로 저작권이 있는 저작물이므로, 우리는 기계 학습 목적으로 데이터 세트를 구축하기 위한 목적으로만 CC BY-SA 라이선스에 따라 기사를 사용하고 재분배하기 위해 뉴스 제공자와 계약을 체결한다. 우리는 다중 도메인 코퍼스가 MRC 모델의 일반화 가능성을 향상시키는 데 도움이 될 수 있다고 믿는다.\n' +
      '\n' +
      '우리는 구절을 수집하기 위해 말뭉치를 전처리한다. WIKIPEDIA 기사의 경우, 모델의 정확한 평가를 위해 기존의 다른 한국 MRC 벤치마크(예: KorQuAD)에서 중복을 제거한다. 그런 다음 각 기사를 섹션별로 분할하여 지문을 얻는다. 뉴스 기사의 경우 정치 기사와 100개 미만의 범주에 속하는 기사를 걸러낸다. 우리는 최종적으로 길이가 512보다 길고 문자가 2048보다 짧은 모든 전처리된 구절을 모읍니다.\n' +
      '\n' +
      '주석 프로토콜 우리는 크라우드 워커에게 구절을 제공하여 질문과 답변에 주석을 달습니다. 가이드라인을 소개하기 위해 자세한 튜토리얼 세션을 제공합니다. 작업자 80명 중 60명은 주어진 구절로 15개의 문답 쌍을 만드는 파일럿 테스트 후 선택된다. 선택한 작업자는 질문을 생성하고 해당 답변 범위(유형 1 및 유형 2의 경우) 또는 가짜 답변 범위(유형 3의 경우)에 레이블을 지정합니다. 주석에는 Tagtog 주석 툴킷 41을 사용한다. 우리는 공통 및 유형별 지침에 따라 생성된 질문 및 답변을 검증하기 위해 각 질문 유형에 대해 세 명의 검사자를 할당한다. 생성된 질의응답 쌍이 검사를 통과하지 못하면, 작업자는 검사자가 주는 피드백에 기초하여 이를 정제한다.\n' +
      '\n' +
      '발음 41: [https://www.tagtog.net/](https://www.tagtog.net/)\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:38]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:39]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:40]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:41]\n' +
      '\n' +
      '그 문제에서. Open-sourced Korean POS tagger를 이용하여 중첩 비율을 계산할 때, postposition(\\(\\mathcal{Z\\lambda\\dagger}\\), josa) 및 ending component(\\(\\circlearrowleft\\)\\(\\ucorner\\), eomi)와 같은 기능입자를 제외한다. 44개의 어휘 중첩 비율은 KorQuAD 데이터 세트(70%)보다 거의 10%p 낮다. 각 문항 유형에 대해 유형 1과 유형 3은 55%에서 59%의 범위에서 유사한 비율을 보인다. 유형 2는 68%의 중첩 비율을 나타낸다.\n' +
      '\n' +
      '각주 44: KoNLPy [101]의 트위터 태거.\n' +
      '\n' +
      '인간 평가 KLUE-MRC에서 인간의 독해 능력에 대한 난이도를 측정하기 위해 인간의 성능을 평가한다. 테스트 세트에서 1,000개의 예를 무작위로 샘플링하고 이를 해결하기 위해 3명의 직원을 고용합니다. 우리는 최고 득점자의 점수를 인간 성과로 선정한다. 표 21은 인간 성능과 기본 모델 간의 비교를 보고한다.\n' +
      '\n' +
      '#### 3.7.4 관련 작업\n' +
      '\n' +
      '최근 몇 년 동안 영어 MRC 연구에서 패러프레이즈, 다중 문장 및 답변 불가능을 포함하지만 이에 국한되지 않는 다양한 질문 유형의 도전적인 데이터 세트를 사용하여 상당한 진전이 이루어졌다.\n' +
      '\n' +
      '패러프레이징된 질문들은 질문과 읽기 패시지의 단어 중복이 낮아서 MRC 모델들이 단순한 단어 매칭을 이용하는 것을 방지한다. Trischler et al. [130]은 뉴스 헤드라인으로부터 질문들을 생성하고 크라우드소싱을 통해 그것들을 요약함으로써 NewsQA 데이터세트를 생성한다. 그들은 질문 생성 동안 주어지지 않은 주요 기사에 답변을 주석함으로써 단어 중복을 줄인다. Saha et al. [120]은 위키피디아와 IMDb.45의 동일한 영화에 대한 플롯 요약 쌍을 활용합니다. 그들은 더 짧은 플롯에서 질문을 생성하고 더 긴 플롯에 답변을 주석하여 자연스럽게 패러프레이징된 질문을 얻습니다. Sen과 Saffari [122]는 질문-통로가 낮은 데이터 세트가 MRC 모델의 일반화 가능성을 향상시킬 것이라고 보고한다.\n' +
      '\n' +
      '발음 45: [https://www.imdb.com/](https://www.imdb.com/)\n' +
      '\n' +
      '다문장 문항은 여러 문장에 대한 추론을 필요로 한다. 그 결과 단문형 문항에 비해 난이도가 높다. Joshi et al. [56]은 트리비아 웹사이트의 질문 데이터세트인 TriviaQA를 소개한다. 다양한 출처(예: 위키피디아 및 웹)에서 증거 지문을 수집하기 때문에 주어진 질문에 답하기 위해 여러 문장이 자연스럽게 필요하다. Khashabi et al. [60]은 크라우드소싱을 통해 다양한 텍스트에 대한 다중 문장 질문을 명시적으로 생성하고 MultiRC 데이터셋을 공개한다. SuperGLUE [132] 벤치마크는 MultiRC 데이터 세트를 태스크 중 하나로 채택한다.\n' +
      '\n' +
      '여러 MRC 데이터 세트는 답변할 수 없는 질문을 통합했다[143, 130, 96, 113, 69]. Rajpurkar et al. [113] 보고서는 응답할 수 없는 질문이 데이터 세트에 포함될 때 MRC 모델의 성능 저하를 보고합니다.\n' +
      '\n' +
      '영어에 대한 MRC 연구에 비해, 한국어 MRC 연구는 소수의 기존 데이터 세트에 서 있다. 한국형 MRC의 주요 벤치마크는 SQuAD 1.0[112]과 동일한 데이터 수집 프로세스를 채택한 KorQuAD[79, 63]이다. 그러나 KorQuAD에 대한 모델 성능은 이미 짧은 기간 동안 인간 성능을 초과하여 추가 연구를 위한 헤드룸을 거의 남기지 않았다. 더욱이, SQuAD와 달리, KorQuAD는 CC BY-ND 라이선스 하에 있고, 파생 저작물(예를 들어, 답변할 수 없는 질문 추가)을 허용하지 않는다. AI 허브 MRC 데이터 세트 [1]은 신문을 기반으로 하며 답변할 수 없는 질문을 포함합니다. 그러나 그 접근은 한국 토종 연구자에만 엄격히 제한되어 있어 한국에 거주하는 국제 연구자와의 협업도 금지하고 있다. K-QuAD [74]는 Google Translate46을 활용하여 SQuAD 1.0 [112]를 한국어로 번역한다. K-QuAD 데이터 세트가 업데이트되지 않기 때문에\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**KLUE + KorQuAD (Full:Full)**} & \\multicolumn{2}{c}{**KLUE + KorQuAD (1:1)**} \\\\ \\cline{2-7}\n' +
      '**Evaluation Dataset** & EM & ROUGE & EM & ROUGE & EM & ROUGE \\\\ \\hline KorQuAD 1.0 (Dev) & 86.59 & 94.19 & 85.00 & 93.07 \\\\ KLUE-MRC (Test) & **70.42** & **75.42** & **69.75** & **75.20** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 20: KLUE-MRC 시험과 KorQuAD 1.0 dev set 간의 난이도 비교.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline  & \\multicolumn{2}{c}{**Paraphrase**} & \\multicolumn{2}{c}{**Multi-sentence**} & \\multicolumn{2}{c}{**Unanswerable**} & \\multicolumn{2}{c}{**Total**} \\\\ \\cline{2-10}  & EM & ROUGE & EM & ROUGE & EM & ROUGE & EM & ROUGE \\\\ \\hline KLUE-RoBERTaBASE & 67.74 & 75.73 & 65.07 & 73.13 & 72.48 & 72.48 & 68.51 & 74.01 \\\\ Human & **84.18** & **88.33** & **87.72** & **90.91** & **86.53** & **86.53** & **85.90** & **88.48** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 21: 모델 예측과 인간 답변 시간 사이의 평가 점수 비교, 그 품질은 출시 시의 기계 번역기의 성능에 의존한다. 우리의 KLUE-MRC는 접근성 강화 라이선스와 더 어려운 어려움 측면에서 기존의 한국 MRC 벤치마크와 다르다.\n' +
      '\n' +
      '#### 3.7.5 Conclusion\n' +
      '\n' +
      '우리는 새로운 도전적인 한국 MRC 벤치마크(KLUE-MRC)를 만듭니다. MRC 능력의 다양한 측면을 평가하기 위해 KLUE-MRC는 다중 영역 지문과 패러프레이즈, 다중 문장 추론 및 답할 수 없는 세 가지 유형의 질문을 포함한다. KLUE-MRC는 기존의 한국어 MRC 데이터셋에 비해 질문 유형 다양성, 난이도, 어휘 중복성이 개선되었다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:44]\n' +
      '\n' +
      '일반적으로 슬롯은 정보 제공 가능한 슬롯과 요청 가능한 슬롯으로 분류된다. 정보 제공 슬롯은 "가격 범위", "지역" 및 "예약 날짜"와 같은 사용자 목표 48을 제한할 수 있는 속성을 포함합니다. 요청가능한 슬롯들은 사용자가 요청할 수 있는 추가 정보를 제공하지만, 반드시 사용자 목표 제약으로서 특정될 필요는 없다. 요청가능한 슬롯의 전형적인 예는 "전화 번호"이며, 이는 사용자가 요청할 수 있지만, 목표의 가능한 후보들을 좁히기 위해 작동하지 않을 것이다[50, 51].\n' +
      '\n' +
      '각주 48: 사용자 목표는 표 24와 같이 사용자를 플레이하는 작업자가 따라야 하는 것이다.\n' +
      '\n' +
      '이 스키마를 기반으로 조작하기 쉬운 주석 시스템과 따르기 쉬운 지침을 제공하기 위해 정보 제공 및 요청 가능한 슬롯에 추가 속성을 포함한다. 슬롯은 1) 부울 타입인지 여부, 2) 필수 여부(_Required_), 3) 예약 관련 여부(_Booking-related_), 4) 예약 확인 후만 사용 가능 여부(_Requestable after booking_, 예를 들어 참조 번호) 중 하나 이상의 속성을 가질 수 있다. 부울 유형 슬롯은 "주차(가용성)" 및 "수영장"과 같은 값으로 _예_ 또는 _아니오_를 가질 수 있습니다. 이러한 부울 유형 값은 대화 컨텍스트에 명시적으로 나타나지 않습니다. 즉, 추상적인 속성을 가지고 있다. 추상적 속성을 이해하는 모델이 바람직하기 때문에 우리는 MultiWOZ[10]보다 훨씬 더 많은 부울 타입 슬롯을 가지고 있다; WoS는 도메인에 걸쳐 20개의 부울 슬롯을 가지는 반면 MultiWOZ는 2개만을 포함한다. 한편, 사용자 의도를 작성하기 위해서는 필요한 슬롯을 값으로 지정해야 한다. 이렇게 하면 에이전트가 필요한 값을 지정하지 않고 다음 단계를 수행할 수 없는 실제 서비스 시나리오를 시뮬레이션할 수 있습니다. [114]\n' +
      '\n' +
      '## 2 지식 베이스 만들기\n' +
      '\n' +
      '각 도메인의 태스크 스키마를 기반으로 지식베이스(Knowledge Base, KB)를 구성하여 사용자의 목표의 미리 정의된 실현 후보 집합을 얻는다. 호텔 및 레스토랑 도메인의 경우 수동으로 가상 인스턴스를 만드는 반면, 어트랙션 및 메트로 도메인의 경우 웹에서 수집한 실명(예: 강남역 또는 남산타워)을 활용한다. 한편, 택시 도메인에 대해서는 미리 인스턴스를 정의하지 않고 MultiWOZ[10]에서와 같이 대화 수집 중에 동적으로 인스턴스를 생성한다. 윤리적 고려 사항을 염두에 두고 개인 식별 정보(PII, 예를 들어 전화 번호, 주소)는 faker를 사용하여 무작위로 생성된 인스턴스로 대체된다. 표 25는 각 도메인에 대한 KB 통계를 보여준다.\n' +
      '\n' +
      '각주 49: [https://faker.readthedocs.io](https://faker.readthedocs.io)\n' +
      '\n' +
      '3. 주석 시스템 설계 이 섹션에서는 사용자 측 및 시스템 측 모두에서 데이터를 수집하기 위해 사용한 주석 플랫폼에 대해 설명합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline\n' +
      '**Domains** & **Informable Slots** & **Requestable Slots** \\\\ \\hline \\multirow{4}{*}{Hotel} & name, type\\({}^{*}\\), area\\({}^{*}\\), price range\\({}^{*}\\), & \\multirow{4}{*}{rating, nearby station,} \\\\  & book day\\({}^{\\dagger}\\), book time\\({}^{\\dagger}\\), book people\\({}^{\\dagger}\\), & \\\\  & walkability\\({}^{*}\\), parking\\({}^{*}\\), internet\\({}^{*}\\), & \\\\  & breakfast\\({}^{*}\\), smoking\\({}^{*}\\), fitness\\({}^{*}\\), & \\\\  & swimming pool\\({}^{*}\\), spa\\({}^{*}\\) & \\\\ \\hline \\multirow{4}{*}{Restaurant} & name, type\\({}^{*}\\), area\\({}^{*}\\), price range\\({}^{*}\\), & \\multirow{4}{*}{rating, nearby station,} \\\\  & book day\\({}^{\\dagger}\\), book time\\({}^{\\dagger}\\), book people\\({}^{\\dagger}\\), & \\\\  & alcohol\\({}^{*}\\), walkability\\({}^{*}\\), parking\\({}^{*}\\), & \\\\  & internet\\({}^{*}\\), smoking\\({}^{*}\\), outdoor table\\({}^{*}\\) & \\\\ \\hline \\multirow{4}{*}{Attraction} & name, type\\({}^{*}\\), area\\({}^{*}\\), & \\multirow{4}{*}{rating, nearby station,} \\\\  & walkability\\({}^{*}\\), parking\\({}^{*}\\), heritage\\({}^{*}\\), & \\\\  & educational\\({}^{*}\\), scenic\\({}^{*}\\), cultural\\({}^{*}\\) & \\\\ \\hline \\multirow{2}{*}{Taxi} & leave at\\({}^{*}\\), departure\\({}^{*}\\), & \\multirow{2}{*}{phone number, cost, duration} \\\\  & arrive by, destination\\({}^{*}\\), type & \\\\ \\hline \\multirow{2}{*}{Metro} & leave at, departure\\({}^{*}\\), destination\\({}^{*}\\) & \\multirow{2}{*}{departure line, destination line,} \\\\  & leave at, departure\\({}^{*}\\), destination\\({}^{*}\\) & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 23: 도메인의 이름과 해당 슬롯을 보여주는 WoS(Wizard of Seoul)의 5개 도메인 모두에 대한 태스크 스키마. 별\\({}^{*}\\), 별표\\({}^{*}\\), 교차\\({}^{\\dagger}\\), 이중교차\\({}^{\\ddagger}\\)는 각각 필수, 부울 유형, 예약 관련 및 예약 슬롯 후 요청 가능을 나타낸다.\n' +
      '\n' +
      '### User Side\n' +
      '\n' +
      '사용자 측면 역할에 대한 목표 지침을 제공합니다. 명령어는 자연어에서의 대응하는 슬롯 값들을 갖는 사용자의 특정 목표의 설명들을 포함한다. 또한 다양한 대화를 위한 페르소나를 포함하는 사용자의 컨텍스트를 포함한다. 사용자는 지시에 따라 발화를 생성하도록 요청받는다. 예를 표 24에 나타낸다.\n' +
      '\n' +
      '슬롯이 여러 도메인에 걸쳐 공유되는 다중 도메인 대화 시나리오를 고안하기 위해 명령 [10, 154]에 _도메인 전환_ 을 포함합니다. 예를 들어, 호텔을 예약하려는 사용자의 경우, 사용자는 그곳에 가기 위해 교통수단(택시, 지하철 등)에 대한 정보를 찾을 수 있다. 이 대화에서 초기 도메인은 다른 도메인(택시/메트로 호텔)으로 변경됩니다. 대화 상태 추적 측면에서 단일 도메인에 비해 더 어렵다. 왜냐하면 사용자는 이전 도메인의 다른 값을 상호 참조함으로써 값을 추론해야 하는 목표를 암묵적으로 표현할 수 있기 때문이다.\n' +
      '\n' +
      '목표 지침은 목표 제한 슬롯 및 그 값에 대한 자리 표시자를 포함하는 템플릿에 의해 실현된다. 우리는 대화의 다양한 시나리오를 다루기 위해 각 도메인에 대한 다양한 템플릿을 설계한다. 멀티WOZ와 마찬가지로, 목표 템플릿들은 대응하는 슬롯들을 갖는 일련의 서브 목표들을 포함한다. 대화 중 어휘 수반 또는 공동 참조를 촉진하기 위해 문장을 신중하게 설계하며, 이는 사용자 컨텍스트 또는 도메인 전환 중에 자연스럽게 관찰될 수 있다. 템플릿을 채워 명령을 완료할 때 도메인별 작업 스키마를 기반으로 하는 KB의 인스턴스를 지정된 자리 표시자에 무작위로 할당합니다. 명령어의 값은 대화 중에 사용자에 의해 구체적으로 언급되어야 한다. 각 추적 가능한 슬롯에는 유효한 값이 있습니다. _없음_ 또는 _Doncare_ 50\n' +
      '\n' +
      '각주 50: _Doncare_ 는 사용자가 선호도가 없음을 의미하고 _None_ 은 사용자가 주어진 슬롯에 대한 유효한 값을 아직 지정하지 않았음을 의미한다[50, 51].\n' +
      '\n' +
      '모델의 일반화 능력을 적절하게 평가하기 위해 우리는 반사실적 목표를 더 추가하고 프로세스 [77] 동안 보이지 않는 KB 인스턴스를 도입한다. 현재 슬롯 분포를 기반으로 새로운 목표 지시를 추가하기 위해, 우리는 구축 동안 슬롯에 대한 슬롯 값 빈도와 동시 발생을 계속 모니터링한다. 특히, 빈발한 슬롯 값 또는 슬롯 간의 거의 동시 발생하지 않는 조합을 포함하는 새로운 목표 명령어를 추가한다. 예를 들어, "(hotel-parking, no)"가 as-is 분포에서 빈번하지 않은 쌍일 때, 이를 제약조건으로 포함하는 목표 명령어를 설계하여 대화창에 나타나도록 촉진한다. 또한, 테스트 시간에서 보이지 않는 슬롯 값에 대한 현실적인 시나리오를 시뮬레이션하기 위해 데이터 세트의 특정 서브세트(트레이 및 디브/테스트 세트) 간에 KB 인스턴스를 구별한다.\n' +
      '\n' +
      '### System Side\n' +
      '\n' +
      '시스템측 작업자(마법사)의 역할은 1) 사용자 발화의 대화 상태에 주석을 달고 2) 필요한 경우 모든 턴마다 KB에 액세스하여 응답을 생성하는 것이다. 먼저, 마법사는 현재 대화 컨텍스트로부터 추론된 적절한 슬롯 값을 채우도록 요청받는다. 상기 사용자에 의해 발화된 단어가 직접 매핑될 수 있도록 명확하지 않은 경우\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Domain** & **\\# Instances** & **\\# Slots** \\\\ \\hline Hotel & 101 & 19 \\\\ Restaurant & 56 & 20 \\\\ Attraction & 100 & 17 \\\\ Taxi & - & 8 \\\\ Metro & 3,306 & 10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 24: 목표 지시의 일례. MultiWOZ와 달리 CoCo[77]에서와 같이 순서 편향을 방지하기 위해 처음부터 모든 명령어를 제시한다. 예약 관련 슬롯인 “식당-책 시간(22:41)” 및 “식당-책 요일(수요일)”은 KB 엔티티의 확인 전에 나타난다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Korean** & **English (Translated)** \\\\ \\hline You have a plan to eat in the **center of Seoul at 22:41** \\\\ today. Oh, today is **Wednesday**. If you find such a place, first check the **representative menu**. Then, make a booking for **1** person. After booking, inquire about the **business hour**. Then, you have to find a hotel to sleep in **near the restaurant**. The restaurant must be **non-smoking**. If you find it, book on the **same day**. You must stay for **4** days as the **same number of people**. If the booking is done, ask for the **reference number** and double-check the **smoking allowed**. Then, finally, call a taxi. You have to go to the **hotel** from the **restaurant**. If you call the taxi, inquire about the **duration**. \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 25: WoS에서의 지식베이스의 통계.\n' +
      '\n' +
      '특정 슬롯 값, 마법사는 단어가 값과 동일한 의미를 가질 때 단어의 의미를 먼저 명확히 한 다음 슬롯을 채워야 한다. 대화 상태의 주석은 필요한 정보를 제공하는 데 완전히 집중하기 위해 사용자 요청을 이해하는 명시적 액션이다. 그런 다음 마법사는 요청하거나 정보를 전달하기 위한 응답을 생성합니다. 필요한 슬롯의 값이 없는 경우 시스템 측 작업자는 사용자에게 결측값을 요청할 수 있습니다. 그렇지 않으면, 시스템은 사용자에게 적절한 정보를 제공한다. 필요한 경우 시스템이 외부 지식 베이스를 쿼리할 수 있습니다. 검색 결과가 3개 이상인 경우 시스템 작업자는 더 많은 세부 정보를 요청하거나 그 중 하나를 추천할 수 있다.\n' +
      '\n' +
      '마법사가 이러한 복잡한 작업을 효과적이고 효율적으로 수행할 수 있도록 지원하기 위해, 우리는 새로 도입된 기능: 드롭다운 컴포넌트(그림 3.8.1)를 갖는 그래픽 웹 인터페이스를 제공한다. 드롭다운 인터페이스를 사용하면 시스템 측 작업자가 미리 채워진 후보 목록에서 값을 선택할 수 있습니다. 작업자에게 너무 많은 옵션이 제시되면 드롭다운이 무의미해질 수 있기 때문에 목표 지시와 영역별 지식 기반을 기반으로 가장 가능성 있는 가치 후보를 제시한다. 이 절차는 MutiWOZ 2.1 [37]에 보고된 _multi-annotations_, _mis-annotations_, _typos_ 및 _value 표준화_와 같은 여러 유형의 주석 오류를 자연스럽게 방지합니다.\n' +
      '\n' +
      '4. 데이터셋 구축 태스크마스터-1[11]에서 영감을 받은 \'Self-dialog\' 기법을 적용하여 비용과 시간을 절감하면서 다양한 대화 데이터셋을 효율적으로 수집한다. 셀프 다이얼로그는 다양한 대화 데이터를 수집하는 데 효과적이다. 두 역할을 모두 가짐으로써, 작업자는 사용자 발화에서의 슬롯 발생 순서, 시스템 응답의 추천 등 대화의 흐름을 자유롭게 제어한다. 이것은 또한 근로자들이 다른 페르소나가 포함되도록 자연스럽게 그들만의 스타일을 말하도록 이끈다. 그러나 파일롯 단계에서 주석 오류는 _조기 마크업_ (사용자로부터 값을 받기 전에 시스템이 미리 값을 채우기) 및 _지연 마크업_ (시스템이 적절한 턴 뒤에 값을 채우기) 오류를 발견했습니다. 또한 오류정정 인터페이스를 제공하여 사용자와 시스템 역할간의 명시적 턴스위칭을 활용함으로써 이 기법을 개선한다.\n' +
      '\n' +
      '자세히 설명하자면, 우리는 주요 수집 프로세스에 참여할 신뢰할 수 있는 근로자를 훈련하고 선택한다. 주요 단계 이전에는 앞서 언급한 초기 마크업 및 지연 마크업 오류를 피하고 일부 잘못된 의사소통을 포함한 보다 현실적인 대화를 생성하기 위해 크라우드 워커를 대상으로 여러 파일럿 연구를 수행했다. 또한 사용자/시스템 역할에 몰입하기 위해 두 역할 사이의 명시적인 전환 기능을 구현함으로써 잘못된 의사소통을 과도하게 줄였다. 파일럿을 통해 이러한 문제를 효과적으로 처리할 수 있는 15명의 선택된 근로자를 최종적으로 고용합니다.\n' +
      '\n' +
      '도 7: 시스템측 작업자를 위한 그래픽 웹 인터페이스.\n' +
      '\n' +
      '최종 데이터 세트 표 26은 데이터 세트의 통계를 보여준다. WoS는 5개의 도메인에서 146,692개의 턴으로 전체 10,000개의 다이얼로그를 포함한다. 평가(dev/test) 세트는 특히 반사실적 목표를 갖는 대화 및 열차 세트에 대해 보이지 않는 KB 인스턴스를 포함한다. dev/test 세트는 각각 294개 및 361개의 반사실적 목표 기반 대화 상자를 포함한다. 모든 분할에는 도메인 전환이 있는 대화 상자 수가 충분합니다.\n' +
      '\n' +
      '#### 3.8.2 평가 메트릭\n' +
      '\n' +
      'WoS에 대한 평가 메트릭은 1) 공동 목표 정확도(JGA) 및 2) 슬롯 마이크로 F1 점수이다. JGA는 전체 대화 회전 횟수 중 슬롯-값 쌍과 그라운드 진리 대화 상태로 구성된 정확히 일치하는 대화 상태의 비율을 측정한다. 슬롯 마이크로 F1 점수는 각 턴에서 마이크로 F1 점수의 평균이다. 각 턴에 대해 마이크로 F1 점수는 예측된 슬롯-값 쌍 및 그라운드-진실 쌍 측면에서 정밀도 및 재현율의 조화 평균으로 정의된다. 슬롯 마이크로 F1 점수는 그라운드 진실의 값이 "없음"일 때 무시된다는 점에 유의한다.\n' +
      '\n' +
      '#### 3.8.3 Analysis\n' +
      '\n' +
      'Li et al. [77]과 같이 반사실적 목표 및 보이지 않는 KB 인스턴스를 기반으로 설정된 열차 및 dev/테스트를 분할할 때 표 27과 같이 성능 저하를 관찰합니다. 이는 반사실적 목표가 WoS를 더 어렵게 만든다는 것을 보여줍니다.\n' +
      '\n' +
      '#### 3.8.4 Related Work\n' +
      '\n' +
      'Wizard-of-Oz (WOZ) [59]는 대화 모음에서 인기 있는 계획입니다. 사실, 종래의 WOZ 설정은 두 사람의 역할극을 채용함으로써 다양한 유형의 대화를 수집할 수 있게 한다. 각 인간은 둘 중 역할을 선택해야 합니다: _user_ 와 _system_ 입니다. 역할을 수행함에 있어서, 미리 제공된 배경 정보를 가진 발화의 차례차례 생성에 의해 대화들이 수집된다. 태스크 지향 대화를 구축하는 경우, _목표_는 _사용자_에게 주어지고, _지식 기반_은 _시스템_에 액세스하도록 허용됩니다. 시스템_은 _사용자_ 요청에 응답할 때 해당 _지식 기반_을 사용할 수 있습니다.\n' +
      '\n' +
      '많은 대화 데이터 세트는 WOZ 설정[135, 35, 36]을 밀접하게 따르지만, 두 크라우드 워커가 동시에 매칭되고 각각의 역할을 성공적으로 수행해야 하기 때문에 많은 시간과 비용이 소요되며, 이는 대화의 규모를 수집하는 것을 방해한다. 이러한 한계를 \'근로자 공존 제약\'이라고 한다. 한계를 극복하기 위해, MultiWOZ[10]은 이 종래의 WOZ를 크라우드 워커들로부터 턴 바이 턴(turn-by-turn)으로 약간 변경하고, 이는 상이한 작업자들이 단일 대화에서 동일한 _사용자_ 또는 _시스템_을 플레이할 수 있게 하는 대화를 크라우드 워커들로부터 턴 바이 턴으로 수집한다. 이 접근법은 모든 작업자가 이전 컨텍스트와 일관되지 않도록 이미 진행된 대화에 적응해야 하기 때문에 비용이 적게 들지만 오류가 발생하기 쉽다[37]. 따라서 최근 CrossWOZ와 RiSAWOZ는 높은 건설 비용에도 불구하고 주석 품질을 유지하기 위해 WOZ 설정에서 제안한 대로 _동기화_ 방식으로 대화를 수집할 신뢰할 수 있는 작업자만 선택했다[154, 109].\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & **|Train** & **|Devl** & **|Testl** & **Total** \\\\ \\hline \\# Dialogues & 8,000 & 1,000 & 1,000 & 10,000 \\\\ \\# Single Domain Dialogues & 1,806 & 263 & 226 & 2,295 \\\\ \\# Multi Domain Dialogues & 6,194 & 737 & 774 & 7,705 \\\\ \\# Counterfactual Dialogues & 0 & 294 & 361 & 655 \\\\ \\hline \\# Total Turns & 117,584 & 14,448 & 14,660 & 146,692 \\\\ \\# Total Tokens & 899,450 & 114,169 & 114,914 & 1,128,533 \\\\ \\hline Avg Turns per Dialogue & 14.70 & 14.45 & 14.66 & 14.67 \\\\ Avg Tokens per Turn & 7.65 & 7.90 & 7.84 & 7.69 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 26: Wizard-of-Seoul(WoS)의 통계.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline\n' +
      '**Domain Split** & **Joint Goal Accuracy** \\\\ \\hline Random & 57.53 \\\\ CF-goal & 47.38 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 27: 데이터 분할 전략에 관한 비교. 랜덤은 트레인, 디브 및 테스트 세트를 무작위로 분할하는 것이다. CF-goal은 보이지 않는 KB 인스턴스가 있는 dev 및 테스트 세트에 포함되어 있음을 나타냅니다.\n' +
      '\n' +
      'Byrne et al. [11]은 또한 종래의 WOZ 설정이 시간이 많이 걸리고 복잡하며 비용이 많이 들며, 에이전트 및 크라우드소싱된 작업자 모두를 훈련하고 관리하기 위한 행정 절차뿐만 아니라 상당한 기술적 구현이 요구된다고 주장하며, 이에 따라 대안으로서 셀프 다이얼로그를 제안한다. 셀프 다이얼로그는 작업자들이 사용자와 시스템 역할을 모두 수행하는 전체 대화를 작성하는 수집 체계이다. Byrne et al. [11]은 대규모 대화 데이터 세트인 Taskmaster-1을 구축하는데, 이는 WOZ 스킴에 기반을 둔 Self-dialog를 기반으로 한다: 1) 두 사람이 사용자 및 시스템 역할을 수행하고(전통적인 WOZ 설정), 2) 한 사람이 두 역할을 모두 수행한다(Self-dialog). 결과적으로, Self-dialog는 비동기식 대화 수집으로 인한 일관성 없는 대화 생성을 피함으로써 \'근로자 공존 제약\' 비용을 효과적으로 개선할 수 있다. 그러나 실제 대화와 비교할 때 오답이 거의 발생하지 않는 경향이 있는데, 이는 동일한 사람이 두 역할에서 모두 발화를 하기 때문에 현실과 괴리가 발생할 수 있기 때문이다.\n' +
      '\n' +
      '일부 연구자들은 비용 효율성을 극대화하기 위해 이러한 대화를 만들기 위해 기계만 사용하려고 한다. 이는 정교하게 설계된 규칙과 주어진 태스크 스키마로부터 자동으로 회전하여 발화를 생성할 수 있는 시뮬레이터 위에 다이얼로그를 구축한다. 시뮬레이터는 먼저 psuedo-dialogue를 생성한 후, 크라우드소싱을 도입하여 자연스러운 발화에 대해 패러프레이징한다[124, 114]. 그러나 인간의 노력이 훨씬 덜 필요하지만 시뮬레이터에 크게 의존한다.\n' +
      '\n' +
      '한편, 기존의 연구들은 DST 모델의 강건한 평가를 다루고 있다. CoCo[77]에 따르면 최신 DST 모델은 열차 데이터에 거의 나타나지 않기 때문에 현실적인 시나리오에 강하지 않다. CoCo(제어 가능한 반사실적)라는 이름에서 알 수 있듯이, 이것은 미리 정의된 슬롯-값 쌍들에 기초하여 드물지만 현실적인 대화들을 생성한다. 그들은 심지어 최첨단 DST 모델의 성능이 반사실적 목표를 포함한 그러한 대화에서 평가될 때 현저하게 떨어진다는 것을 보여준다. 이는 현재 TOD 벤치마크가 보이지 않지만 현실적인 시나리오에 대한 견고성 측면에서 개선되어야 함을 의미한다.\n' +
      '\n' +
      '한국어는 한국정보화진흥원(NIA)에서 제공하는 과제 중심 대화 데이터셋이 있으며, 민원 관련 10개 정도의 도메인을 포함하고 있으며 500k 이상의 대화로 구성되어 있다. 발화는 1) 사용자가 질문하는 주요 질문, 2) 시스템이 설명을 요청할 수 있는 하위 질문, 3) 사용자 답변, 4) 시스템 답변. 추가적으로, 사용자 의도들이 주석되고 엔티티들이 각각의 발화로부터 추출된다. 이 데이터 세트는 앞서 언급한 설정을 따르지 않으며 단일 회전 판단에 대해서만 슬롯-값 쌍으로 표시되는 대화 상태가 없음을 발견했다. 또한 태스크 스키마에 대한 정보가 부족하고 접근성의 원칙을 충족하지 못하는 재분배가 제한되어 DST 벤치마크를 새로 만들 수 있다.\n' +
      '\n' +
      '#### 3.8.5 Conclusion\n' +
      '\n' +
      '서울 관광객과 여행사 간의 대화를 시뮬레이션한 최초의 대규모 한국어 멀티 도메인 태스크 중심 대화 데이터셋인 위저드 오브 서울(Wizard-of-Seoul, WoS)을 소개한다. 대화 수집 기법의 효율적인 스케일 업을 위해 \'셀프 다이얼로그\'를 적용한다. 또한, 어노테이션 인터페이스(드롭다운 메뉴 및 턴-스위칭)에 대한 고려는 잘못된 경우를 완화하고, 반사실적인 것을 포함한 다양한 목표 지시가 각 대화가 더 자연스럽고 도전적이도록 촉진한다. 우리는 WoS가 한국어로 다양한 미래 대화 연구를 촉발하고 종단 간 대화 모델링을 추진하는 데 귀중한 통찰력을 제공하기를 바란다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:50]\n' +
      '\n' +
      '우리는 세 가지 이유로 사회적으로 편향된 내용이나 혐오 발언을 걸러내지 않는다. 첫째, 이러한 대규모 사전 훈련 코퍼라에 대해서는 수동 검사가 불가능하다. 둘째, 이 둘 다 등장하는 맥락에 크게 의존하기 때문에 사회적으로 편향된 내용이나 혐오 공간을 자동으로 탐지하는 것은 그 자체로 어려운 문제이다[62]. 마지막으로, 이러한 유해 콘텐츠에 대해 맹목적인 것은 향후 이러한 유해 콘텐츠를 탐지하고 수정하기 위한 언어 모델의 사용을 방해하여, 이를 안티 전문가로 활용하는 것을 방지한다[81]. 우리는 우리가 발표하는 사전 훈련된 언어 모델에 대한 향후 연구가 최근 Cheng et al. [15]에 의해 입증된 바와 같이 이러한 모델에 인코딩된 편향을 탐지하고 수정하는 방법과 디바이어스를 제거하는 방법에 초점을 맞출 것으로 기대한다.\n' +
      '\n' +
      '대조적으로, 우리는 가능한 한 말뭉치에서 PII를 가명 처리합니다. 한국인터넷진흥원(KISA)의 가이드라인을 기반으로 정규식을 이용하여 16개의 개인 데이터 유형을 검출한다. 58 선택된 PII는 정형화된 패턴을 가지고 있기 때문에 언어적 패턴을 유지하면서 비교적 쉽게 PII를 가명 처리할 수 있다. 그런 다음 페이커 라이브러리59 또는 패턴을 기반으로 하는 무작위 생성을 사용하여 원본 정보를 대체한다. 그 결과 사전 훈련 말뭉치의 1.2%를 가명 처리한다. 자세한 내용은 표 29에 나와 있다.\n' +
      '\n' +
      '각주 58: [https://www.kisa.or.kr/public/laws/laws2_View.jsp?cPage=1&mode=view&p_No=282&b_No=282&d_No=3](https://www.kisa.or.kr/public/laws/laws2_View.jsp?cPage=1&mode=view&p_No=282&b_No=282&d_No=3)\n' +
      '\n' +
      '각주 59: [https://github.com/joke2k/faker](https://github.com/joke2k/faker)\n' +
      '\n' +
      '토큰화 _형태소 기반 하위 단어_ 토큰화라는 새로운 토큰화 방법을 설계하고 사용합니다. 어휘 구축 시 형태소 분석기를 이용하여 원시 텍스트를 형태소로 사전 토큰화한 후, 바이트 쌍 인코딩(BPE) [123]을 적용하여 최종 어휘를 얻는다. 형태소 분할은 한국어에 적응된 Mecab-ko,60 McCab[68]을 사용하고, BPE 분할은 Huggingface Tokenizers 라이브러리에서 워드피스 토큰나이저를 사용한다.61 어휘 크기를 32k로 지정한다. 어휘를 구축한 후 추론 시 BPE 모델만을 사용하여 형태소 분석기 없이 형태소를 반영하여 단어열을 토큰화할 수 있다. 이렇게 하면 사용성과 속도가 모두 향상됩니다. 예는 표 30에 제시되어 있다.\n' +
      '\n' +
      '각주 60: [https://bitbucket.org/eunjeon/mecab-ko](https://bitbucket.org/eunjeon/mecab-ko)\n' +
      '\n' +
      '각주 61: [https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)\n' +
      '\n' +
      '이 방법의 동기는 한국어가 응집 언어, 즉 단어는 형태소-줄기와 접사의 구성이라는 것이다. 형태소는 서로 다른 조합에서 변하지 않는 경향이 있으며, 그 경계는 일반적으로 명확하다. BPE는 그 효과 때문에 많은 언어에 걸쳐 널리 사용되었지만 표 30에서 입증된 바와 같이 형태소를 올바르게 식별하는 데 어려움을 겪고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline\n' +
      '**Private Information** & **Pseudonymization** & **Pseudonymised Example** \\\\ \\hline\n' +
      '**전화번호** & Faker & 055-604-8764 \\\\\n' +
      '**사회보장번호** & Faker & 600408-2764759 \\\\\n' +
      '**외국 등록 번호** & Faker & 110527-1815659 \\\\\n' +
      '**Email Address** & Faker & agweon@example.org \\\\\n' +
      '**IP Address** & Faker & 166.186.169.69 \\\\\n' +
      '**MAC 주소** & Faker & c5:d7:14:84:f8:cf \\\\\n' +
      '**Mention(@)** & Faker & @gildong \\\\\n' +
      '**Address** & Random Number Generation & 110-245-124678 \\\\\n' +
      '**은행 계좌 번호** & 난수 생성 & 110-245-124678 \\\\\n' +
      '**여권 번호 & 임의 생성 & M123A4567 \\\\\n' +
      '**운전자 면허** & 난수 생성 & 11-17-174133-01 \\\\\n' +
      '**사업자등록번호** & 난수생성 & 123-45-67890 \\\\\n' +
      '**건강보험정보** & 난수생성 & 1-2345678901 \\\\\n' +
      '**신용 또는 직불 카드 번호** & 난수 생성 & 1234-5678-9012-3456 \\\\\n' +
      '**차량 등록 장소** & 랜덤 생성 & 55-1601 \\\\\n' +
      '**Homepage URL** & Random Generation & www.example.com \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 29: 우리의 가명화 방법 및 예. 예제는 페이커 라이브러리 문서 또는 일반인의 것입니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:52]\n' +
      '\n' +
      '[CLS]를 첫 번째 입력 토큰 \\(x_{0}\\)으로 사용하고 [SEP]를 구분자 토큰으로 사용하여 입력(예: STS와 NLI의 두 문장, MRC의 경우 대사와 질문, DST에 대한 대화 전환)을 구분합니다.\n' +
      '\n' +
      '#### 5.1.1 단일 문장 분류\n' +
      '\n' +
      'TC 및 RE와 같은 단일 문장 분류 태스크에서, 분류기는 단일 문장을 미리 정의된 라벨들의 집합으로 분류한다. 관례에 따라, [CLS] 토큰 \\(h_{0}\\)의 마지막 은닉 상태는 \\(W\\in\\mathbb{R}^{K\\times H}\\)로 레이블 수 (\\(K\\))에 선형 매핑되고 전체 모델은 교차 엔트로피 손실을 최소화하도록 훈련된다.\n' +
      '\n' +
      '**YNAT** 는 미리 정의된 토픽 레이블에 대해 \\(K\\)가 7인 단일 문장 분류 작업이며 각 입력에 대한 특별한 처리가 필요하지 않습니다. 반면에 **KLUE-RE** 는 입력 문장 내에서 엔터티를 표시 하는 특별한 절차가 필요 합니다. <subj>, </subj>, <obj>, </obj>를 이용하여 각각 Baldini Soares et al. [5]에 따라 주어 개체 및 목적 개체의 시작과 끝을 표시한다. 우리는 이 네 개의 여분의 토큰을 추가하기 위해 임베딩 행렬을 확장한다.\n' +
      '\n' +
      '#### 5.1.2 문장 쌍 분류/회귀\n' +
      '\n' +
      '문장 쌍 분류/회귀 태스크에서, 두 문장 사이의 관계를 결정하기 위해 모델이 요청된다. 한 쌍의 입력 문장은 중간에 특수 구분자 토큰(종종 [SEP])과 연결됩니다.\n' +
      '\n' +
      '**KLUE-STS** 에서 각 문장 쌍에는 실수 유사성 \\([0,5] \\)이 주석 처리됩니다. 따라서, 모델은 평균 제곱 오차(MSE)를 최소화함으로써, [CLS]의 최종 은닉 상태로부터 실수로 매핑되도록 트레이닝된다. **KLUE-NLI** 의 경우 전제와 가설로 구성된 각 문장 쌍이 세 가지 클래스 중 하나와 결합됩니다. 따라서 모델은 [CLS] 토큰의 숨겨진 상태를 3차원 실수값 벡터로 매핑하고 교차 엔트로피 손실을 최소화하도록 훈련된다.\n' +
      '\n' +
      '#### 5.1.3 다중 문장 슬롯 값 예측\n' +
      '\n' +
      '**WoS** 는 주어진 대화 컨텍스트에 대한 슬롯 값 예측 작업이며, 여기서 예측은 단일 발화 대신 여러 차례에 걸쳐 고려되어야 합니다. 발화 인코더, 상태 생성기 및 슬롯 게이트 분류기로 구성된 TRADE [140]의 아키텍처에 따라 인코더-디코더 모델을 사용한다(그림 5.1.3). 구현에서 더 나은 표현을 얻기 위해 GRU[17]에서 PLM으로 발화 인코더를 변경한다. 따라서 상태 생성기는 [CLS] 토큰 \\(h_{0}\\)의 최종 은닉 상태를 첫 번째 디코더 은닉 상태로 취한다. 또한 WoS가 MultiWOZ[10]보다 상대적으로 더 많은 부울 타입 슬롯을 포함하기 때문에 두 개의 슬롯 게이트 레이블(_yes_, _no_)을 추가로 예측하기 위해 슬롯 게이트 분류기를 수정한다. 상태 생성기와 슬롯 게이트 분류기의 교차 엔트로피 손실을 공동으로 최소화한다.\n' +
      '\n' +
      '도 8: TRADE[140]에 기초한 WoS를 위한 베이스라인 아키텍처.\n' +
      '\n' +
      '#### 5.1.4 Sequence Tagging\n' +
      '\n' +
      '**KLUE-NER** 은 토큰 수준 태그 지정 작업으로, 각 문자에는 레이블이 할당됩니다. 이것은 각 서브워드 토큰 내의 문자들로부터의 라벨들이 집계되어야 하고, 각 서브워드 토큰의 예측된 라벨이 그 내의 문자들에 걸쳐 적절하게 분배되어야 하기 때문에, 토큰화를 사용하는 데 주의가 필요하다. 예를 들어 도 9를 참조한다. 인코더 \\(h\\in\\mathbb{R}^{|x|\\times H}\\)의 최종 은닉 상태 각각을 12개의 명명된 개체 범주에 해당하는 12차원 실수 벡터로 선형 매핑한다. 그런 다음 모든 토큰에 대해 합산된 교차 엔트로피 손실을 최소화합니다.\n' +
      '\n' +
      '**KLUE-MRC** 는 모델이 질문을 제공 하는 구문 내에서 답변 범위의 시작 및 끝 토큰에 태그를 지정 하는 범위 예측 작업입니다. 모델에 대한 입력은 토큰화된 통로와 연관된 질문([SEP]에 의해 분리됨)의 연결이다. 통로의 각 토큰의 최종 은닉 상태는 2차원 실수값 벡터에 선형으로 투영된다. 이 벡터의 차원은 두 개의 이진 분류기, 즉 시작 및 종료 토큰 분류기의 로짓에 해당한다. 특정 토큰 [CLS]는 주어진 질문이 응답할 수 없는 경우 올바른 시작 토큰과 끝 토큰으로 간주됩니다. 우리는 모델을 훈련시키기 위해 교차 엔트로피 손실을 최소화한다.\n' +
      '\n' +
      '**KLUE-DP** 를 시퀀스 태그 문제로 프레임 지정 합니다. 입력 문장 내의 각각의 토큰은 두 번 태깅되는데, 하나는 그의 헤드 토큰으로, 다른 하나는 헤드와 현재 토큰을 연결하는 호 유형으로 태깅된다. 우리의 기본 아키텍처는 단어 표현과 주의 메커니즘을 제외하고 페르난데스-곤잘레스와 고메즈-로드리게스[39]가 제안한 모델을 따른다. KLUE-NER과 마찬가지로 주석은 단어 수준에서 수행되므로 하위 단어 토큰을 처리하는 데 주의해야 한다. 본 논문에서는 사전 학습된 언어 모델을 이용하여 하위 단어 표현을 추출하고, 각 단어의 첫 번째와 마지막 하위 단어 토큰 표현을 연결하여 단어 벡터 표현을 구성한다. 이들 단어 표현들 각각은 선택적으로 품사 임베딩과 연결된다. 어텐션 레이어의 경우, HEAD를 예측하기 위해 바이아핀 어텐션[33]을 사용하고, 각 단어에 대해 아크 타입(DEPREL)을 예측하기 위해 바이리니어 어텐션[64]을 사용한다. KLUE-NER 및 KLUE-MRC와 마찬가지로 전체 모델을 미세 조정하기 위해 교차 엔트로피 손실을 최소화한다. 모델 아키텍처의 그래픽 그림은 그림 10을 참조하십시오.\n' +
      '\n' +
      '그림 10: 페르난데스-곤살레스 및 고메스-로드리게스를 활용하는 KLUE-DP 기준 모델 아키텍처의 개요[39].\n' +
      '\n' +
      '도 9: KLUE-NER의 입력 및 라벨 예. 형태소 기반 하위 단어 토큰화의 토큰에 대해 그림 3의 원래 문자 수준 레이블 시퀀스를 다시 정렬한다.\n' +
      '\n' +
      '### Fine-Tuning Configurations\n' +
      '\n' +
      '실험은 Huggingface Transformers[139]와 PyTorch-Lightning.62를 사용하였으며, 학습률은 \\(\\{10^{-5},2\\times 10^{-5},3\\times 10^{-5},5\\times 10^{-5}\\}\\), 웜업비는 \\(\\{0.,0.1,0.2,0.6\\}\\), 중량감쇠계수는 \\(\\{0.0,0.01\\}\\)에서 선택된 AdamW 최적화기 [83]을 사용하였다. 배치 크기는 \\(\\{8,16,32\\}\\)에서 선택하고 epoch 수는 \\(\\{3,4,5,10\\}\\)에서 선택한다. 우리는 KLUE-MRC와 WoS에 대해 512의 최대 시퀀스 길이를 사용하고, 다른 모든 태스크에 대해 128의 최대 시퀀스 길이를 사용한다. 우리는 디브 집합 성능을 기반으로 최상의 하이퍼파라미터 구성에서 얻은 점수를 보고한다.\n' +
      '\n' +
      '각주 62: [https://github.com/PyTorchLightning/pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning)\n' +
      '\n' +
      '### Evaluation Results\n' +
      '\n' +
      '이 섹션에서는 KLUE 벤치마크에 대한 KLUE-PLM 및 기존 PLM을 포함한 평가 결과를 표 32.63에서 다른 NLU 벤치마크와 달리 다른 척도와 해석의 점수를 단순 평균화하는 것이 매우 오판의 소지가 있기 때문에 태스크에 대한 점수를 평균하지 않는다. 오히려 각 과제의 결과를 별도로 기술하고 논의한다. 한국 BASE 모델 내에서 KLUE-BERTBASE는 YNAT와 WoS, KLUE-RE와 KLUE-MRC는 KLUE-RoBERTBASE, KLUE-STS와 KLUE-NLI는 KoELECTRABASE를 가장 잘 수행한다.\n' +
      '\n' +
      '각주 63: 개발 세트에서 계산되었지만 해당 테이블에 대한 부록 A를 참조하십시오.\n' +
      '\n' +
      '우리는 크게 두 가지 관찰을 한다. 먼저, 기존 PLM 중 가장 큰 모델인 KLUE-RoBERTBASE가 KLUE-NER을 제외한 모든 태스크에서 다른 모델보다 우수함을 보인다. 이 관찰은 모델 크기와 작업 성능 사이의 상관 관계를 입증한 최근의 경향과 잘 일치한다[58; 9]. 이것은 KLUE가 단순히 모델 크기를 더 증가시킴으로써 우리가 얼마나 많은 이득을 기대할 수 있는지에 대한 향후 조사에 유용할 것임을 나타낸다. 두 번째 관찰은 대상 언어(한국어)에서 보다 신중하게 선별된 말뭉치를 위해 특별히 설계되고 훈련된 단일 언어 모델이 특히 유사한 크기의 모델을 비교할 때 일반적으로 다중 언어 대응 모델보다 우수하다는 것이다. 우리는 XLM-RLARGE가 캐릭터 수준 F1 점수 측면에서 최고의 연기자 KoELECTRABASE와 유사하게 수행하는 KLUE-NER을 제외한 모든 작업에서 이 관찰을 다시 한다. 이 관찰은 목표 언어를 이해하고 목표 언어에 대한 데이터, 모델 및 학습 알고리즘을 맞춤화하는 데 노력을 투자하는 것의 중요성을 다시 강조한다.\n' +
      '\n' +
      '### 모델 분석\n' +
      '\n' +
      '사전 훈련 말뭉치와 전처리 데이터를 준비하는 데 있어 우리가 내린 두 가지 주요 결정이 있었다. 1) PII의 가명화 여부와 2) 토켄화 전략이었다. 이 섹션에서는 KLUE-RoBERTBASE를 사용하여 MODU 코퍼스에서만 훈련하여 선택의 영향을 분석한다.\n' +
      '\n' +
      'Corpus 가명화 가명화 과정에서 유입되는 잡음은 다운스트림 태스크 수행에 해로운 영향을 미칠 수 있음을 예상할 수 있다. 그러나 표 33에 제시된 우리의 발견은 작업의 하위 집합에 약간의 감소가 있지만 그러한 감소는 매우 미미하다는 것을 보여준다. 이는 우리가 한 것처럼 최소한의 가명화 수준이 이미 업무 수행과 개인 정보 유출 위험의 균형을 맞추는 좋은 방법임을 시사한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c c} \\hline \\hline  & **YNAT** & **KLUE-STS** & **KLUE-NLI** & **KLUE-NER** & **KLUE-RE** & **KLUE-DP** & **KLUE-MRC** & **WoS** \\\\ \\cline{2-13}\n' +
      '**Model** & F1 & R\\({}^{P}\\) & F1 & ACC & F1\\({}^{E}\\) & F1\\({}^{C}\\) & F1\\({}^{mic}\\) & AUC & UAS & LAS & EM & ROUGE & JGA & F1\\({}^{S}\\) \\\\ \\hline\n' +
      '**mBERTBASE** & 81.55 & 84.66 & 76.00 & 73.20 & 76.50 & 89.23 & 57.88 & 53.82 & 90.30 & 86.66 & 44.66 & 55.92 & 35.46 & 88.63 \\\\\n' +
      '**XLM-RBASE** & 83.52 & 89.16 & 82.01 & 77.33 & 80.37 & 92.12 & 57.46 & 54.98 & 89.20 & 87.69 & 27.48 & 53.93 & 39.82 & 89.61 \\\\\n' +
      '**XLM-R\\({}_{\\text{LACE}}\\)** & **86.96** & 92.97 & 85.86 & 85.93 & 82.27 & **93.22** & 58.39 & 61.15 & 92.71 & **88.70** & 35.99 & 66.77 & 41.20 & 89.80 \\\\ \\hline\n' +
      '**KR-BERBASE** & 84.58 & 88.61 & 81.61 & 81.07 & 77.17 & 74.58 & 90.13 & 62.74 & 60.94 & 89.92 & 87.48 & 42.82 & 58.54 & 45.33 & 90.70 \\\\\n' +
      '**KoELECTRABASE** & 84.59 & 92.46 & 84.84 & 85.63 & **86.11** & 92.56 & 62.85 & 58.94 & 92.90 & 87.77 & 59.82 & 66.05 & 41.58 & 89.60 \\\\ \\hline\n' +
      '**KLUE-BERTBASE** & 85.73 & 90.85 & 82.84 & 81.63 & 83.97 & 91.39 & 66.44 & 66.17 & 89.96 & 88.05 & 62.32 & 68.51 & 46.64 & 91.61 \\\\\n' +
      '**KLUE-RoBERTBASE** & 84.98 & 91.54 & 85.16 & 79.33 & 83.65 & 91.14 & 60.89 & 58.96 & 90.04 & 88.14 & 57.32 & 62.70 & 46.62 & 91.44 \\\\\n' +
      '**KLUE-RoBERTBASE** & 85.07 & 92.50 & 85.40 & 84.83 & 84.60 & 91.44 & 67.65 & 68.55 & 93.04 & 88.32 & 68.67 & 73.28 & 47.49 & 91.64 \\\\\n' +
      '**KLUE-RoBERTBASE** & 85.69 & **93.35** & **86.63** & **89.17** & 85.00 & 91.86 & **71.13** & **72.98** & **93.48** & 88.36 & **75.58** & **80.59** & **50.22** & **92.23** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 32: KLUE 벤치마크에 대한 사전 훈련된 LMs 및 기타 기준선의 평가 결과. F1은 매크로-F1 점수를 의미한다. KLUE-NER의 F1\\({}^{E}\\)와 F1\\({}^{C}\\)는 각각 개체 수준 및 문자 수준 매크로 F1 점수를 나타낸다. KLUE-RE의 F1\\({}^{mic}\\)은 상관 관계를 무시한 미평균 F1 점수이다. WoS의 F1\\({}^{S}\\)는 슬롯-값 쌍 레벨 마이크로-F1 점수의 평균이다. KLUE-STS의 R\\({}^{P}\\)은 피어슨 상관관계를 나타낸다. **볼드** 는 모델 전체에서 가장 좋은 성능을 보여주며 밑줄은 BASE 모델 중에서 가장 좋은 성능을 나타냅니다.\n' +
      '\n' +
      '#### 4.4.2 토큰화 전략\n' +
      '\n' +
      '토큰화 기법인 _형태소 기반 서브워드_ 토큰화를 표준 바이트 쌍 인코딩(BPE)과 대조한다. 먼저, 단어가 하위 단어 토큰으로 분할되는 방식의 차이를 조사한다. Rust et al. [119]에 따라 하위 단어 생식력, 연속 단어의 비율 및 UNK 비율을 고려한다. 단어당 평균 생성된 서브워드의 수를 측정하는 서브워드 비옥도(subword fertility)에서 제안된 토큰화 기법은 BPE보다 약간 높게 나타난다. 다만 적어도 두 개의 하위 단어로 쪼개진 단어의 수를 측정하는 지속어 비율을 보면 반대의 경향을 관찰한다. 이는 본 논문에서 제안한 알고리즘이 원래 단어를 최대한 유지하고 필요할 때만 각 단어를 잠재적으로 더 많은 하위 단어 조각으로 분할한다는 것을 의미한다. BPE에 대한 제안된 기법의 효과는 두 방법 모두에서 어휘 크기가 32k로 제어되었을 때 BPE에 비해 적은 UNK 토큰을 생성하기 때문에 UNK 비율에서 분명하다. 표 35를 참조한다.\n' +
      '\n' +
      'KLUE-NER, KLUE-MRC, WoS의 경우 두 기법 간의 정성적 차이가 태스크 성능에 상당한 차이를 가져온다는 것을 알 수 있다. 이러한 작업은 종종 형태소 수준에서 태깅, 탐지 및 생성까지 포함하며 형태학적으로 일관된 토큰화가 전반적으로 더 나은 예측을 촉진한다고 의심한다. 반면에, 토큰화 전략의 차이는 분류 또는 단어-레벨 태깅의 성능에서 나타나지 않는데, 이는 대응하는 NLU 시스템이 서브워드 토큰 표현을 더 큰 유닛의 표현으로 병합할 때 서브워드 세그먼테이션에서의 불일치를 더 쉽게 극복할 수 있기 때문이다. 전반적으로 향후 연구자들은 제안된 토큰화 전략을 기본 옵션으로 사용할 것을 권장한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c} \\hline \\hline  & **YNAT** & **KLUE-STS** & **KLUE-NLI** & **KLUE-NER** & **KLUE-RE** & **KLUE-DP** & **KLUE-MRC** & \\multicolumn{2}{c}{**WoS**} \\\\ \\cline{2-13}\n' +
      '**Tokenization** & F1 & R\\({}^{P}\\) & F1 & ACC & F1\\({}^{E}\\) & F1\\({}^{C}\\) & F1\\({}^{mic}\\) & AUC & UAS & LAS & EM & ROUGE & JGA & F1\\({}^{S}\\) \\\\ \\hline BPE & **83.40** & 91.91 & **85.19** & **82.07** & 68.75 & 89.47 & 64.39 & **65.04** & 89.89 & **89.47** & 51.12 & 65.79 & 21.38 & 77.68 \\\\ Morpheme-based Subword & **83.40** & **92.06** & 84.70 & 81.60 & **84.84** & **91.03** & **65.25** & 64.79 & **92.17** & 88.34 & **62.13** & **67.46** & **47.14** & **91.60** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 34: 토큰화 전략을 다른 기준선과 비교합니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c c} \\hline \\hline  & **YNAT** & **KLUE-STS** & **KLUE-NLI** & **KLUE-NER** & **KLUE-RE** & **KLUE-DP** & **KLUE-MRC** & \\multicolumn{2}{c}{**WoS**} \\\\ \\cline{2-13}\n' +
      '**Pretraining Corpus** & F1 & R\\({}^{P}\\) & F1 & ACC & F1\\({}^{E}\\) & F1\\({}^{C}\\) & F1\\({}^{mic}\\) & AUC & UAS & LAS & EM & ROUGE & JGA & F1\\({}^{S}\\) \\\\ \\hline Original & **83.40** & **92.06** & **84.70** & **81.60** & 84.84 & 91.03 & **65.25** & **64.79** & **92.17** & **88.34** & 62.13 & 67.46 & **47.14** & **91.60** \\\\ Pseudonymized & 83.39 & 91.11 & 82.85 & 78.50 & **84.99** & **91.22** & 62.79 & 62.96 & 92.02 & 88.02 & **62.88** & **67.58** & 46.21 & 91.23 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 33: 전처리 단계에서 말뭉치 가명화 수행 여부에 따른 평가 결과.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline\n' +
      '**Tokenization** & **\\# Vocabs** & **Fertility \\(\\downarrow\\)** & **\\% Continued Word \\(\\uparrow\\)** & **UNK Ratio \\(\\downarrow\\)** \\\\ \\hline BPE & 32k & **2.073** & 0.578 & 0.011 \\\\ Morpheme-based Subword & 32k & 2.468 & **0.765** & **0.009** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 35: 토큰화 메트릭 개요. MODU 코퍼스를 사용하여 각 어휘를 구축하고 WIKIPEDIA 코퍼스에서 비교한다.\n' +
      '\n' +
      'Ethical Considerations\n' +
      '\n' +
      'KLUE를 구축하고 기본 모델을 동반할 때 데이터와 모델을 모두 공개하는 데 있어 유해하고 부정적인 결과를 피하기 위해 다양한 메커니즘을 통합했다. 이러한 메커니즘은 도입되고 사용되는 곳마다 자세히 설명되지만 이 섹션에서는 이러한 메커니즘, 고려 사항 및 그 뒤에 있는 원칙을 요약한다.\n' +
      '\n' +
      '### 저작권 및 액세스 권한\n' +
      '\n' +
      '대부분의 NLP 데이터 세트는 기존 텍스트 소스를 기반으로 구축됩니다. 이는 특히 기본 소스 데이터 세트가 잘 지정되지 않거나 신중하게 조사되지 않을 때 이러한 데이터 세트를 사용하는 조건에 대한 질문을 제기한다. KLUE를 사용하는 조건에 대한 이러한 의심을 피하고 한국어로 NLP 연구를 가속화하기 위해 12월에 시행된 한국의 저작권법을 완전히 고수한다. 2020.64. 8. 사용 제한 없이 재배포 및 재혼합을 모두 허용하는 라이선스에 따라 릴리스할 수 있는 텍스트만 포함합니다.\n' +
      '\n' +
      '각주 64: [https://www.law.go.kr/%EB%2%95%EB%AO%B9/%EC%AO%80%EC%9E%91%EA%B6%8C%EB%B2%95](https://www.law.go.kr/%EB%2%95%EB%AO%B9/%EC%AO%80%EC%9E%91%EA%B6%8C%EB%B2%95)\n' +
      '\n' +
      '소스 코퍼라 우리의 목표는 벤치마크의 지속적인 가용성과 유용성을 확보하고 극대화하는 것입니다. 즉, CC BY-SA와 함께 새로운 작품의 도출 가능성을 보장하고 자유롭게 재분배해야 한다. CC BY-SA에서 KLUE를 공개하기 위해 1) 저작권에 의해 보호되지 않는 텍스트 또는 2) CC0, CC BY, CC BY-SA 또는 KOGL 유형 1 라이선스에 따라 텍스트만 포함하여 소스 코퍼스 세트를 구축했다. 저작권이 있는 뉴스 기사의 경우 KLUE-MRC를 만들어 CC BY-SA로 공개할 수 있는 사업자, 한국경제신문(KED) 뉴스 매체, 아크로판과 계약을 맺었다.\n' +
      '\n' +
      'Task-Specific (Annotated) DatasetsWe subsample and annotate the source corpus for each KLUE benchmark task. CC BY-SA로 각각 출시합니다. 이는 파생상품이 동일한 라이선스(CC BY-SA) 하에 배포되는 한, KLUE 벤치암크 사용자가 상업적 및 비상업적 목적 모두를 위해 복제, 재분배, 리믹스, 변환 및 구축하도록 허용한다. 이를 통해 향후 NLP 연구 및 개발이 크게 촉진될 것으로 기대한다.\n' +
      '\n' +
      '사전 훈련 말뭉치 및 언어 모델은 앞서 4.1에서 논의되었지만 MODU, CC-100-Kor 및 NEWSCRAWL을 사용하여 구축된 사전 훈련 말뭉치가 모두 공개적으로 사용 가능한 텍스트에서 생성되지만 저작권이 있는 저작물을 포함하지 않는다는 것을 보장할 수 없다. 불행히도 이러한 말뭉치가 없으면 한국어에 대한 대규모 언어 모델을 훈련할 수 있을 만큼 충분히 큰 자원을 찾을 수 없다. 따라서 사전 훈련에 사용하지만 KLUE와 대조되는 향후 문제를 피하기 위해 사전 훈련 코퍼스를 공개적으로 공개하지 않는다. 대신, 우리는 미래의 연구를 용이하게 하기 위해 미리 훈련된 언어 모델을 공개적으로 공개합니다. 언어 모델의 매개변수는 인간의 생각과 감정을 표현하지 않기 때문에 저작권의 요건을 충족하지 못한다.\n' +
      '\n' +
      '### Toxic Content\n' +
      '\n' +
      '대규모의 접근 가능한 벤치마크 데이터 세트는 기계 학습과 그 응용을 자연 언어 처리, 독성 및 이러한 데이터 세트 내의 원치 않는 콘텐츠와 같은 인접 분야로 발전시키지만, 우리가 훈련하는 대규모 모델을 통해 증폭될 수 있다. 우리는 프로젝트 초기부터 이 문제를 인식하고 여기에서 KLUE에서 이러한 독성 내용물을 어떻게 다루었는지 설명한다.\n' +
      '\n' +
      '작업별 데이터 세트 각 작업별 데이터 세트에 대해 독성 콘텐츠의 도입을 최소화하기 위해 세 단계를 적용한다. 먼저, 독성 분류기를 사용하여 혐오 발언 및 젠더 편향된 문장을 자동으로 검출하고, 주석용으로 이러한 문장을 보내기 전에도 제거한다(2.3절 참조). 둘째, 우리는 주석이 편견과 혐오 발언에 대한 명확한 정의를 제공한 후 사회적 편향을 나타내거나 독성이 있는 인스턴스를 표시하도록 명시적이고 명확하게 지시한다(섹션 3 참조). 마지막으로, 이러한 표시된 문장을 수동으로 검사하고 최종 데이터 세트에서 제외한다. 이 3단계 프로세스는 가능한 모든 사례를 포착하지 못할 수 있으며 온라인 포럼65를 사용하여 KLUE 사용자의 피드백 및 불만을 받을 계획이다.\n' +
      '\n' +
      '각주 65: [https://github.com/KLUE-benchmark/KLUE/issues](https://github.com/KLUE-benchmark/KLUE/issues)\n' +
      '\n' +
      '사전 훈련 언어 모델 우리는 세 가지 이유로 사전 훈련 말뭉치(섹션 4.1 참조)를 그대로 사용한다. 첫째, 수동 검사는 순전히 눈금 때문에 다루기 어렵다. 둘째, 혐오표현과 편향된 문장을 탐지할 수 있는 자동화된 도구를 구축하는 것이 어렵다. 이 문제는 제한된 크기의 알려진 혐오 음성 데이터 세트가 하나뿐이기 때문에 한국어에 대해 훨씬 더 심각해진다[92]. 마지막으로, 이러한 사전 훈련된 언어 모델이 바람직하지 않은 사회적 편견뿐만 아니라 다양한 독성 콘텐츠를 자동으로 탐지하기 위한 더 나은 도구를 구축하는 데 사용될 미래를 상상한다. 이러한 사전 훈련된 모델이 이러한 문제를 인식하려면 그러한 독성 내용물로도 훈련되었을 것이다.\n' +
      '\n' +
      '### 개인 식별 정보\n' +
      '\n' +
      '최근에 대규모의 사전 훈련된 언어 모델이 대량의 개인 식별 정보(PII)를 암기하고 이러한 개인 정보를 검색하도록 알고리즘을 설계할 수 있다는 것이 발견되었다. 따라서 우리는 작업별 데이터 세트를 가명화하고 말뭉치를 사전 훈련하기 위해 각각 두 가지 다른 접근법을 설계한다.\n' +
      '\n' +
      '작업별 데이터 세트 작업별 데이터 세트의 경우 주석 중 수동 검사에 의존하여 PII를 탐지합니다. 수동 검사 후 PII가 포함된 것으로 보고된 모든 문장은 폐기한다. 시뮬레이션된 대화창에 의존하는 DST의 경우 페이커 라이브러리.66을 사용하여 실제 텍스트가 아닌 데이터베이스 항목을 가명 처리한다.\n' +
      '\n' +
      '각주 66: [https://github.com/joke2k/faker](https://github.com/joke2k/faker)\n' +
      '\n' +
      '프레트레이닝 코퍼라 PII 제거와 프리트레이닝 언어 모델의 성능 사이에는 트레이드오프가 있으며, 이는 나중에 본 논문에서 입증할 것이다. 따라서 순전히 정규식으로 감지할 수 있는 16개의 PII 유형을 가명화한다. 섹션 4.1 참조\n' +
      '\n' +
      '## 7 관련 작업\n' +
      '\n' +
      '범용 NLU BenchmarksGeneral Language Understanding Evaluation (GLUE) [133] 벤치마크는 영어에 대한 평가 데이터 세트의 집합으로서 NLU에 대한 최초의 범용 평가 벤치마크였다. 하나의 업무에 국한되지 않는다는 점에서 범용성이 있다. 의미론적 텍스트 유사성 캡처 능력(QQP, MRPC, STS)[3, 13], 추론 능력(MNLI, QNLI, RTE, WNLI)[8, 138]을 측정하고 단일 문장을 미리 정의된 카테고리 세트(CoLA, SST)[134, 126]로 분류하는 능력을 평가하는 작업을 포함하여 11개의 다운스트림 태스크로 구성된다. GLUE는 영어에만 초점을 맞추었으며, 중국어 CLUE[142], 프랑스어 버전[72], 인도네시아어 버전[137], 인도어 버전[57], 러시아어 슈퍼GLUE[125], 페르시아어 GLUE[61]를 포함하여 지난 몇 년 동안 서로 다른 언어의 변형이 구축되어 출시되었다. 이 모든 경우에 언어별 특성을 통합하면서 광범위한 영역과 과제를 포괄하면서 원래 GLUE의 철학을 따르기 위한 상당한 노력이 수행되었다. 한편, XGLUE[78] 및 XTREME[54]와 같은 자동화된 방법에 크게 의존하는 이러한 벤치마크의 다국어 버전을 구축하려는 노력이 있었다. 언어로서의 한국어는 이러한 후자의 벤치마크의 하위 집합에 포함되었지만, 본 논문이 나오기 전까지 한국어에 대한 범용 언어 이해 평가 제품군을 구축하려는 진지한 시도는 없었다.\n' +
      '\n' +
      '본 논문까지 한국어의 표준 NLU 벤치마크의 부재, 한국어의 과제별 벤치마크가 다수 제안되어 발표되었다. 예를 들어, 감성 분류에는 NSMC가 사용되고, 패러프레이즈 검출에는 PAWS-X[144], NLI와 STS에는 KorNLI와 KorSTS[45], MRC에는 KorQuAD 1과 2[79, 63], BEEP![92]가 사용된다. 혐오 발언 탐지용입니다. 이 시점에서 KLUE를 구축하기 위해 이러한 데이터 세트를 단순히 집계하는 것이 더 쉽고 편리했을지 궁금할 수 있다. 결국, 이것은 다국어 [78, 54] 벤치마크뿐만 아니라 단일언어 [133]을 구성하기 위한 인기 있는 전략이었다. 불행히도 이 접근법은 우리가 이 논문에서 직접 다루는 두 가지 주요 문제와 함께 제공된다.\n' +
      '\n' +
      '첫째, 기존의 데이터셋은 다른 데이터셋과 그 속성을 고려하지 않고 개별적으로 구축한다. 즉, 이러한 개별 데이터 세트의 집합은 도메인 및 스타일의 광범위한 범위를 갖도록 다운스트림 작업에 대한 하위 집합뿐만 아니라 소스 말뭉치를 주의 깊게 선별하는 KLUE와 달리 광범위한 도메인 및 쓰기 스타일을 포함할 가능성이 없다. 이는 영역과 양식뿐만 아니라 평가 중인 언어 현상에 대한 취재를 넘어서는 것이다. 위에서 열거한 기존 벤치마크의 대부분은 구문보다는 의미론에 초점을 맞추고 있으며, 화용론을 포착하는 널리 이용 가능한 벤치마크는 찾아보기 어렵다. 우리는 다운스트림 작업 세트를 신중하게 선택하여 이 문제를 해결합니다.\n' +
      '\n' +
      '둘째, 이러한 기존 데이터 세트는 항상 공개적으로 사용할 수 있는 것은 아니며, 일부는 재배포나 원본 변환을 금지하는 매우 제한적인 라이선스로 배포된다. 이들은 종종 정부 산하 기관에서 출판하고 발표하는 것입니다. 비한국인 연구자가 쉽게 접근할 수 없는 데이터셋에 접근할 수 있는 특별한 허가를 받아야 하는 경우도 있다. 우리는 소스 말뭉치의 신중한 큐레이션과 출판사와의 직접적인 계약에 의해 CC BY-SA에 따라 전체 벤치마크 데이터를 재조정함으로써 KLUE의 이러한 모든 문제를 해결한다.\n' +
      '\n' +
      '각주 67: 이 중 일부는 한국에서 공개적으로 사용할 수 있지만 국제적으로 사용할 수 없습니다.\n' +
      '\n' +
      '사전 훈련 언어 모델(PLM) 대규모 사전 훈련 언어 모델의 최근 경향은 GLUE 및 기타 유사한 NLU 벤치마크에서 ELMo[104], GPT-2[110] 및 BERT[30]과 같은 이전 모델의 성공으로 촉발되었다. 이러한 초기 성공은 XLNet[146], ALBERT[71], RoBERTa[82], ELECTRA[22] 및 Deberta[49]를 포함한 대규모 언어 모델의 일련의 발전으로 이어졌으며, 다시 크게 표준화된 벤치마크의 가용성에 의해 주도되었다. 이는 모델 크기뿐만 아니라 학습 알고리즘 측면에서도 언어 모델의 발전으로 기존 언어 이해 벤치마크를 구축하고 개선하는 데 관심을 불러일으켰다. 최근에 출시된, 도전적인 벤치마크들 중 일부는 SuperGLUE[132] 및 KILT[105]를 포함한다. 본 논문의 KLUE와 같은 표준 언어 이해 벤치마크의 가용성은 한국어 이해를 위한 이러한 선순환을 시작할 것으로 기대된다.\n' +
      '\n' +
      '다른 언어와 다국어 모델의 발전에 영감을 받은 한국어용 사전 훈련 언어 모델 PLM은 여러 연구 그룹과 개인에 의해 훈련되고 출시되었다. SKT는 한국전자통신연구원(ETRI)의 코버트69, 투블록 AI의 한버트70, 서울대학교의 KR-BERT[75]에 이어 코버트68을 출시했다. KoELECTRA[102], KcBERT[73] 등 개별 연구자가 발표한 몇 가지 사전 훈련된 모델이 있다.\n' +
      '\n' +
      '각주 68: [https://github.com/SKTBrain/KoBERT](https://github.com/SKTBrain/KoBERT)\n' +
      '\n' +
      'Footnote 69: [https://aiopen.etri.re.kr/service_dataset.php](https://aiopen.etri.re.kr/service_dataset.php)\n' +
      '\n' +
      '각주 70: [https://github.com/tbai2019/HanBert-54k-N](https://github.com/tbai2019/HanBert-54k-N)\n' +
      '\n' +
      '불행히도 한국어로 표준화된 벤치마크가 없기 때문에 한국어로 사전 훈련된 언어 모델의 흐름을 어떻게 비교해야 하는지 불분명하다. 이러한 모델의 하위 집합은 위의 한국어에서 몇 가지 하향 스트림 NLP 작업의 하위 집합을 기반으로 비교되었지만 표준화되지 않았기 때문에 이러한 제한된 실험에서 확실한 결론을 내리기가 쉽지 않다. 제안된 KLUE 벤치마크는 한국어를 위한 언어 모델의 연구 진행을 추적하는 표준 방법이 될 것으로 기대한다.\n' +
      '\n' +
      '## 8 Discussion\n' +
      '\n' +
      'Open AccessWe distributed KLUE under CC BY-SA. 라이센스를 사용하면 모든 사람이 모든 매체 또는 형식으로 벤치마크를 자유롭게 복사하고 재분배할 수 있습니다. 또한 성능 포화 후 더 어려운 데이터 세트를 구축하기 위해 벤치마크를 개선할 수 있다. NLU _benchmark_ 로 기능 하려면 오픈 액세스가 필수입니다. 원저자가 벤치마크의 파생 개발을 허용하지 않는 경우 다른 연구자들은 예를 들어 독성 함량을 제거하거나 기술 개선을 위한 연구를 가속화하기 위해 더 어려운 데이터 세트를 구축하여 개선할 수 없다. 영리단체에 근무하는 연구자는 상업적 이용이 허용되지 않으면 벤치마크에 기여하거나 (쉽게) 기여할 수 없을 것이다. 예를 들어 데이터 세트를 다른 연구자와 공유하는 것이 금지되면 연구를 상당히 제한하기 때문에 재분배는 또 다른 중요한 요소이다. 관련 갈등의 저작권 침해 책임을 연구자에게 전가하는 것이 연구를 제한하는 또 다른 관행이다. 데이터의 오픈 액세스에 대한 좋은 선례를 설정하기 위해 우리는 데이터 세트를 1) 모든 목적, 2) 파생 작업 및 3)에 사용할 수 있다. 벤치마크 데이터 세트의 기존 저작권이 존중되는 한 재배포. 또한 사전 훈련된 한국어 모델과 사전 훈련 및 미세 조정 파이프라인의 구현을 엽니다. 이를 통해 작업의 재현성을 높이고 누구나 데이터와 모델을 수정하고 개선할 수 있습니다. 우리는 한국 NLP 연구 커뮤니티와 더 넓은 NLP 커뮤니티에 기여하기를 바란다.\n' +
      '\n' +
      '한국어 NLP 연구 촉진 최근 대규모 한국어 모델의 적극적인 개발 노력에 부응하여 한국어 NLP 연구를 촉진하기 위한 목적으로 KLUE를 개발했다. 전체 NLP 커뮤니티는 BERT[30] 및 그 변형들이 GLUE[133] 및 SuperGLUE[132]에 대한 이전의 NLU 모델들을 능가하는 것을 보았고, 또한 보다 최근의 GPT3[9]는 자연어 이해 및 생성에서 미세 조정(및 _컨텍스트 내 학습_으로) 없이 뛰어난 성능을 보였다. 이러한 모델에 자극을 받은 여러 기관의 많은 한국 연구자들이 서둘러 대규모 트랜스포머 기반 한국어 모델을 사전 훈련시켰다. 결과적으로, 거의 동일한 사전 훈련된 언어 모델들이 오픈 소스 커뮤니티에 출시되었다. 그러나 한국어를 위한 GLUE와 같은 잘 설계된 범용 벤치마크가 없기 때문에 이러한 모델의 행동과 특성을 체계적으로 이해할 수 없었다. KLUE는 다양한 한국 LMs가 특정 작업에 대해 어떻게 그리고 왜 수행되는지 이해하기 위해 통제된 실험을 수행하여 해당 모델에 대한 자세한 통찰력을 얻을 수 있도록 한다. 또한, KLUE는 다른 언어로도 수행되는 많은 대표적인 NLU 과제를 포함하고 있기 때문에, KLUE는 한국어 및 다른 언어로 다국어 연구를 수행하는 것을 목표로 하는 NLP 연구자들에게 근본적인 자원으로 기능할 것이다.\n' +
      '\n' +
      'NLU 모델의 전체 성능 측정 KLUE에서 각 작업에서 얻은 모든 점수를 평균하지 않는다. 모든 작업의 성능은 다른 평가 메트릭에 의해 측정됩니다. 각 태스크에 대한 메트릭을 자신의 특성을 고려하여 신중하게 선택하기 때문이다. KLUE-MRC와 KLUE-NER은 한국어 단어 내에 엔터티가 존재할 수 있는 반면 KLUE-STS와 KLUE-NLI는 문장 수준 메트릭을 사용하기 때문에 작업별로 세분성이 다르다. 또한 F1 점수, 정확도, 곡선 아래 면적, UAS, LAS, ROUGE-W, 공동 목표 정확도 및 피어슨의 상관 관계와 같은 태스크 전반에 걸쳐 다양한 메트릭을 사용한다. 이 상황에서 GLUE [133]에서와 같이 모든 작업의 평균을 단순히 계산하면 전체 성능 측정이 잘못된다. 평균은 의도하지 않은 방식으로 특정 작업에 더 높은 가중치를 부여할 뿐만 아니라 해석 가능성을 잃게 된다. 이에 따라 모형의 NLU 능력을 추정할 수 있는 대안이 필요하다. 최근에는 이러한 능력을 추정하기 위해 항목 반응 이론(Item Response Theory, IRT) 프레임워크를 사용하여 모델의 예측의 정확성을 분석하는 것이 제안되지만 [70] 벤치마크에서 정확하게 적용되어야 하는 방법은 명확하지 않다. 따라서 현재로서는 전체 성능 척도를 요약하지 않고 각 작업에 대한 모델을 별도로 평가하기로 결정한다. 이것이 우리의 한계이며, 우리는 이 문제를 미래로 남겨둔다.\n' +
      '\n' +
      'KLUE의 빠른 포화 우리는 예를 들어 GLUE[94]의 관찰에 기초하여 KLUE의 빠른 포화를 기대한다. 그러나 우리는 모델에 대한 쉬운 예를 필터링하여 벤치마크를 인위적으로 어렵게 만들지 않습니다. KLUE의 주요 목적은 NLU의 다양한 측면에서 모델을 적절하게 평가하는 것이기 때문에 기본 사전 훈련 언어 모델보다 개선을 위한 헤드룸 확대에 초점을 맞추는 것을 피한다. 우리의 라이선스 정책은 한국어를 위한 첫 번째 오픈 도메인 질문 답변 구축과 같은 다른 연구자들과 함께 더 어려운 과제를 집단적으로 개발함으로써 포화 후 벤치마크의 발전에 긍정적인 영향을 미칠 것으로 기대한다.\n' +
      '\n' +
      '한국어 모델의 분석 각 태스크에 대한 기본 모델의 성능을 비교할 때 다양한 패턴을 관찰하지만, 대부분 현상을 정확하게 설명하기 위해 제대로 연구되지 않았다. 더 철저한 조사를 통해 모델, 말뭉치, 한국어의 언어적 특성 및 향후 작업에서 훈련 메커니즘의 복잡한 상호 작용에 대한 이해도를 높일 수 있기를 바란다.\n' +
      '\n' +
      '## 9 Conclusion\n' +
      '\n' +
      '다양한 작업을 포함하는 한국 NLU 벤치마크 제품군인 KLUE를 소개합니다. 우리는 모든 사람에게 KLUE를 개방하고 다국어 모델 및 기타 기존 오픈 소스 한국어 모델을 능가하도록 훈련된 한국어 모델도 제공합니다. 벤치마크를 만들고 처음부터 모델을 훈련하면서 처음부터 높은 기준을 설정했습니다. 벤치마크 데이터 세트를 설계하고 주석자를 엄격하게 훈련하여 개인 정보 및 혐오 발언을 포함한 잠재적인 윤리적 문제를 고려했다. 우리는 모든 벤치마크 구성 및 테스트 프로세스를 자세히 문서화했다. 또한 KLUE와 모델의 광범위한 영향과 한계에 대해 논의했다. 한계에도 불구하고 KLUE와 동반 언어 모델은 데이터 세트와 언어 모델이 어떻게 만들어지고 더 넓은 커뮤니티로 확산되어야 하는지를 설명하는 귀중한 선례를 설정함으로써 향후 한국어 NLP 연구를 촉진할 것이다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '후원된 주석 비용을 업그레이드하고 리더보드를 구축했습니다. 네이버 CLOVA는 데이터 어노테이션 비용과 GPU 클라우드 컴퓨팅 인프라(NSML)를 제공했습니다. 구글의 텐서플로우 리서치 클라우드(TFRC)와 카카오 엔터프라이즈의 브레인클라우드에도 감사드린다. 세 가지 컴퓨팅 리소스를 사용하여 언어 모델을 사전 훈련하고 미세 조정했다. Scatter Lab, SelectStar, Riiid!, DeepNatural and KAIST가 후원한 데이터 주석 비용. 또한, 한국 경제와 아크로판이 MRC 데이터 세트에 대한 뉴스 기사를 지원해 주셔서 감사합니다.\n' +
      '\n' +
      '저자들은 과제 선정과 DP 과제 논의에 대한 천음박, MRC 과제 논의에 대한 이진혁과 서민준, MRC 데이터셋에서 주석자 관리를 위해 상당한 노력을 기울인 수정김과 김동연, DP, NER, RE에서 데이터 구축을 신중하게 고려한 상아박에 대해 감사드린다. 리더보드 및 평가 시스템 구현을 위해 이준엽, 이건희, 이지호, 남대현, 조용진 씨에 대해 감사드립니다.\n' +
      '\n' +
      '본 연구는 KAIST 기관심사위원회(#KH2020-173)에서 심사 및 승인을 받았다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] National Information Society Agency. MRC AI Dataset. [https://aihub.or.kr/aidata/86](https://aihub.or.kr/aidata/86), 2018.\n' +
      '* [2] Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In _Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)_, pages 252-263, Denver, Colorado, June 2015. Association for Computational Linguistics. doi: 10.18653/v1/S15-2045. URL [https://www.aclweb.org/anthology/S15-2045](https://www.aclweb.org/anthology/S15-2045).\n' +
      '* [3] Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In _Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)_, pages 497-511, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/S16-1081. URL [https://www.aclweb.org/anthology/S16-1081](https://www.aclweb.org/anthology/S16-1081).\n' +
      '* [4] Jens Allwood. An activity based approach to pragmatics. In Harry Bunt and William Black, editors, _Abduction, belief and context in dialogue: Studies in computational pragmatics_, chapter 2, pages 47-80. John Benjamins, Amsterdam, Netherlands, 2000.\n' +
      '* [5] Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the blanks: Distributional similarity for relation learning. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 2895-2905, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1279. URL [https://www.aclweb.org/anthology/P19-1279](https://www.aclweb.org/anthology/P19-1279).\n' +
      '* [6] Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigating system bias and enabling better science. _Transactions of the Association for Computational Linguistics_, 6:587-604, 2018. doi: 10.1162/tacl_a_00041. URL [https://www.aclweb.org/anthology/Q18-1041](https://www.aclweb.org/anthology/Q18-1041).\n' +
      '* [7] Samuel R Bowman and George E Dahl. What will it take to fix benchmarking in natural language understanding? _arXiv preprint arXiv:2104.02145_, 2021.\n' +
      '* [8] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 632-642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL [https://www.aclweb.org/anthology/D15-1075](https://www.aclweb.org/anthology/D15-1075).\n' +
      '* [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).\n' +
      '* 작업 지향 대화 모델링을 위한 대규모 다중 도메인 마법사-of-Oz 데이터 세트입니다. "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"에서, 페이지 5016-5026, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1547. URL [https://www.aclweb.org/anthology/D18-1547](https://www.aclweb.org/anthology/D18-1547).\n' +
      '* [11] Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Ben Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young Kim, and Andy Cedilnik. Taskmaster-1: Toward a realistic and diverse dialog dataset. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 4516-4525, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1459. URL [https://www.aclweb.org/anthology/D19-1459](https://www.aclweb.org/anthology/D19-1459).\n' +
      '* [12] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. _arXiv preprint arXiv:2012.07805_, 2020.\n' +
      '\n' +
      '* [13] Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In _Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)_, pages 1-14, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL [https://www.aclweb.org/anthology/S17-2001](https://www.aclweb.org/anthology/S17-2001).\n' +
      '* [14] Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. A survey on dialogue systems: Recent advances and new frontiers. _SIGKDD Explor. Newsl._, 19(2):25-35, November 2017. ISSN 1931-0145. doi: 10.1145/3166054.3166058. URL [https://doi.org/10.1145/3166054.3166058](https://doi.org/10.1145/3166054.3166058).\n' +
      '* [15] Pengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si, and Lawrence Carin. FairFil: Contrastive neural debiasing method for pretrained text encoders. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=N6JECD-PI5w](https://openreview.net/forum?id=N6JECD-PI5w).\n' +
      '* 5월 1일, 1998_, 1998. URL [https://www.aclweb.org/anthology/M98-1001](https://www.aclweb.org/anthology/M98-1001).\n' +
      '* [17] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1724-1734, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL [https://www.aclweb.org/anthology/D14-1179](https://www.aclweb.org/anthology/D14-1179).\n' +
      '* [18] Won Ik Cho, Jong In Kim, Young Ki Moon, and Nam Soo Kim. Discourse component to sentence (DC2S): An efficient human-aided construction of paraphrase and sentence similarity dataset. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 6819-6826, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.842](https://www.aclweb.org/anthology/2020.lrec-1.842).\n' +
      '* [19] Won Ik Cho, Sangwhan Moon, and Youngsook Song. Open Korean corpora: A practical report. In _Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)_, pages 85-93, Online, November 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.nlposs-1.12](https://www.aclweb.org/anthology/2020.nlposs-1.12).\n' +
      '* [20] Jayeol Chun, Na-Rae Han, Jena D. Hwang, and Jinho D. Choi. Building Universal Dependency treebanks in Korean. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL [https://www.aclweb.org/anthology/L18-1347](https://www.aclweb.org/anthology/L18-1347).\n' +
      '* [21] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2924-2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL [https://www.aclweb.org/anthology/N19-1300](https://www.aclweb.org/anthology/N19-1300).\n' +
      '* [22] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/pdf?id=r1xMH1BtvB](https://openreview.net/pdf?id=r1xMH1BtvB).\n' +
      '* [23] Korea Copyright Commission. Newspapers and copyright, 2009.\n' +
      '* [24] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 670-680, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1070. URL [https://www.aclweb.org/anthology/D17-1070](https://www.aclweb.org/anthology/D17-1070).\n' +
      '* [25] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2475-2485, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1269. URL [https://www.aclweb.org/anthology/D18-1269](https://www.aclweb.org/anthology/D18-1269).\n' +
      '\n' +
      '* Conneau et al. [2020] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 8440-8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL [https://www.aclweb.org/anthology/2020.acl-main.747](https://www.aclweb.org/anthology/2020.acl-main.747).\n' +
      '* Dagan et al. [2006] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Joaquin Quinonero-Candela, Ido Dagan, Bernardo Magnini, and Florence d\'Alche Buc, editors, _Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment_, pages 177-190, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-33428-6.\n' +
      '* de Marneffe and Manning [2008] Marie-Catherine de Marneffe and Christopher D. Manning. 스탠포드는 종속성을 표현했다. 2008년 <Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation>, 1-8 페이지, 1-8 페이지, Manchester, UK, August. Coling 2008 조직 위원회. URL [https://www.aclweb.org/anthology/W08-1301](https://www.aclweb.org/anthology/W08-1301).\n' +
      '* de Marneffe et al. [2006] Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. Generating typed dependency parses from phrase structure parses. In _Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC\'06)_, Genoa, Italy, May 2006. European Language Resources Association (ELRA). URL [http://www.lrec-conf.org/proceedings/lrec2006/pdf/440_pdf.pdf](http://www.lrec-conf.org/proceedings/lrec2006/pdf/440_pdf.pdf).\n' +
      '* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL [https://www.aclweb.org/anthology/N19-1423](https://www.aclweb.org/anthology/N19-1423).\n' +
      '* 작업, 데이터 및 평가. "Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC\'04)"_, Lisbon, Portugal, May 2004. European Language Resources Association (ELRA). URL [http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf](http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf).\n' +
      '* Dolan and Brockett [2005] William B. Dolan and Chris Brockett. 센센셜 패러프레이즈 코퍼스를 자동으로 구성합니다. `Proceedings of the Third International Workshop on Paraphrasing(IWP2005)_, 2005. URL [https://www.aclweb.org/anthology/I05-5002](https://www.aclweb.org/anthology/I05-5002).\n' +
      '* Dozat and Manning [2017] Timothy Dozat and Christopher D. Manning. 신경 의존성 파싱을 위한 깊은 비아핀 주의 2017년 _International Conference on Learning Representations_ 에서 URL [https://openreview.net/forum?id=Hk95PK91e](https://openreview.net/forum?id=Hk95PK91e)입니다.\n' +
      '* Eberhard and Simons [2021] David M. 에버하드와 찰스 D. 시몬스, 게리 F. 패닝 민족학: 세계의 언어_. SIL International, Dallas, Texas, 24 edition, 2021. URL [http://www.ethnologue.com](http://www.ethnologue.com)\n' +
      '* Asri et al. [2017] Layla El Asri, Hannes Schulz, Shikhar Sharma, Jeremie Zumer, Justin Harris, Emery Fine, Rahul Mehrotra, and Kaheer Suleman. Frames: a corpus for adding memory to goal-oriented dialogue systems. In _Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue_, pages 207-219, Saarbrucken, Germany, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-5526. URL [https://www.aclweb.org/anthology/W17-5526](https://www.aclweb.org/anthology/W17-5526).\n' +
      '* Eric et al. [2017] Mihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. Key-value retrieval networks for task-oriented dialogue. In _Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue_, pages 37-49, Saarbrucken, Germany, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-5506. URL [https://www.aclweb.org/anthology/W17-5506](https://www.aclweb.org/anthology/W17-5506).\n' +
      '* Eric et al. [2020] Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj Goyal, Peter Ku, and Dilek Hakkani-Tur. MultiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 422-428, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.53](https://www.aclweb.org/anthology/2020.lrec-1.53).\n' +
      '\n' +
      '* Fan et al. [2019] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 3558-3567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL [https://www.aclweb.org/anthology/P19-1346](https://www.aclweb.org/anthology/P19-1346).\n' +
      '* Fernandez-Gonzalez and Gomez-Rodriguez [2019] Daniel Fernandez-Gonzalez and Carlos Gomez-Rodriguez. 포인터 네트워크를 사용하여 좌우 종속성 구문 분석 "Proceedings of the 2019 Conference of the North American Chapter of the Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_", 710-716 페이지, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1076. URL [https://www.aclweb.org/anthology/N19-1076](https://www.aclweb.org/anthology/N19-1076).\n' +
      '* Finkel et al. [2005] Jenny Rose Finkel, Trond Grenager, and Christopher Manning. Incorporating non-local information into information extraction systems by Gibbs sampling. In _Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\'05)_, pages 363-370, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. doi: 10.3115/1219840.1219885. URL [https://www.aclweb.org/anthology/P05-1045](https://www.aclweb.org/anthology/P05-1045).\n' +
      '* Gebru et al. [2018] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daume III, and Kate Crawford. Datasheets for datasets. _arXiv preprint arXiv:1803.09010_, 2018.\n' +
      '* Glockner et al. [2018] Max Glockner, Vered Shwartz, and Yoav Goldberg. Breaking NLI systems with sentences that require simple lexical inferences. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 650-655, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2103. URL [https://www.aclweb.org/anthology/P18-2103](https://www.aclweb.org/anthology/P18-2103).\n' +
      '* Grishman and Sundheim [1996] Ralph Grishman and Beth Sundheim. 메시지 이해 회의- 6: 간략한 역사. *COLING 1996 권 1: 제16회 국제 회의 계산 언어학_, 1996. URL [https://www.aclweb.org/anthology/C96-1079](https://www.aclweb.org/anthology/C96-1079).\n' +
      '* Gururangan et al. [2018] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)_, pages 107-112, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2017. URL [https://www.aclweb.org/anthology/N18-2017](https://www.aclweb.org/anthology/N18-2017).\n' +
      '* Ham et al. [2020] Jiyeon Ham, Yo Joong Choe, Kyubyong Park, Ilji Choi, and Hyungjoon Soh. KorNLI and KorSTS: New benchmark datasets for Korean natural language understanding. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 422-430, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.39. URL [https://www.aclweb.org/anthology/2020.findings-emnlp.39](https://www.aclweb.org/anthology/2020.findings-emnlp.39).\n' +
      '* Han et al. [2020] Ji Yoon Han, Tae Hwan Oh, Lee Jin, and Hansaem Kim. Annotation issues in Universal Dependencies for Korean and Japanese. In _Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020)_, pages 99-108, Barcelona, Spain (Online), December 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.udw-1.12](https://www.aclweb.org/anthology/2020.udw-1.12).\n' +
      '* Han et al. [2006] Na-Rae Han, Shijong Ryu, Sook-Hee Chae, Seung-yun Yang, Seunghun Lee, and Martha Palmer. Korean treebank annotations version 2.0. _Linguistic Data Consortium (LDC), Philadelphia_, 2006.\n' +
      '* Han et al. [2018] Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 4803-4809, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1514. URL [https://www.aclweb.org/anthology/D18-1514](https://www.aclweb.org/anthology/D18-1514).\n' +
      '* He et al. [2021] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. DeBERTa: Decoding-enhanced BERT with disentangled attention. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=XPZIaotutsD](https://openreview.net/forum?id=XPZIaotutsD).\n' +
      '* Henderson et al. [2014] Matthew Henderson, Blaise Thomson, and Jason D. Williams. The second dialog state tracking challenge. In _Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)_, pages 263-272, Philadelphia, PA, U.S.A., June 2014. Association for Computational Linguistics. doi: 10.3115/v1/W14-4337. URL [https://www.aclweb.org/anthology/W14-4337](https://www.aclweb.org/anthology/W14-4337).\n' +
      '\n' +
      '* Henderson et al. [2014] Matthew Henderson, Blaise Thomson, and Jason D Williams. The third dialog state tracking challenge. In _2014 IEEE Spoken Language Technology Workshop (SLT)_, pages 324-329. IEEE, 2014.\n' +
      '* Hendrickx et al. [2010] Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O Seaghdha, Sebastian Pado, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In _Proceedings of the 5th International Workshop on Semantic Evaluation_, pages 33-38, Uppsala, Sweden, July 2010. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/S10-1006](https://www.aclweb.org/anthology/S10-1006).\n' +
      '* Hu et al. [2020] Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Kubler, and Lawrence Moss. OCNLI: Original Chinese Natural Language Inference. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 3512-3526, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.314. URL [https://www.aclweb.org/anthology/2020.findings-emnlp.314](https://www.aclweb.org/anthology/2020.findings-emnlp.314).\n' +
      '* Hu et al. [2020] Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 4411-4421. PMLR, 13-18 Jul 2020. URL [http://proceedings.mlr.press/v119/hu20b.html](http://proceedings.mlr.press/v119/hu20b.html).\n' +
      '* Jabbari et al. [2020] Ali Jabbari, Olivier Sauvage, Hamada Zeine, and Hamza Chergui. A French corpus and annotation schema for named entity recognition and relation extraction of financial news. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 2293-2299, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.279](https://www.aclweb.org/anthology/2020.lrec-1.279).\n' +
      '* Joshi et al. [2017] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL [https://www.aclweb.org/anthology/P17-1147](https://www.aclweb.org/anthology/P17-1147).\n' +
      '* Kakwani et al. [2020] Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. Khapra, and Pratyush Kumar. IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 4948-4961, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.445. URL [https://www.aclweb.org/anthology/2020.findings-emnlp.445](https://www.aclweb.org/anthology/2020.findings-emnlp.445).\n' +
      '* Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* Kelley [1984] J. F. Kelley. 사용자 친화적인 자연어 사무소 정보 애플리케이션을 위한 반복 설계 방법론입니다. _ ACM Trans. Inf. Syst._ , 2(1):26-41, January 1984. ISSN 1046-8188. doi:10.1145/357417.357420. URL [https://doi.org/10.1145/357417.357420](https://doi.org/10.1145/357417.357420).\n' +
      '* Khashabi et al. [2018] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 252-262, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1023. URL [https://www.aclweb.org/anthology/N18-1023](https://www.aclweb.org/anthology/N18-1023).\n' +
      '* Khashabi et al. [2020] Daniel Khashabi, Arman Cohan, Siamak Shakeri, Pedram Hosseini, Pouya Pezeshkpour, Malihe Alikhani, Moin Aminnaseri, Marzieh Bitaab, Faeze Brahman, Sarik Ghazarian, et al. ParsiNLU: a suite of language understanding challenges for persian. _arXiv preprint arXiv:2012.06154_, 2020.\n' +
      '* Kiela et al. [2020] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 2611-2624. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/1b84c4cee2b8b3d823b30e2d604b1878-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1b84c4cee2b8b3d823b30e2d604b1878-Paper.pdf).\n' +
      '\n' +
      '* Kim et al. [2020] Youngmin Kim, Seungyoung Lim, Hyunjeong Lee, Soyoon Park, and Myungji Kim. KorQuAD 2.0: Korean QA dataset for web document machine comprehension. _Journal of KIISE_, 47:577-586, 2020. ISSN 2383-630X. URL [https://science.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NART99691770&dbt=NART](https://science.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NART99691770&dbt=NART).\n' +
      '* 키퍼와서 및 골드버그 [2016] 일라이야후 키퍼와서 및 요아브 골드버그. 양방향 LSTM 특징 표현을 사용하여 간단하고 정확한 종속 구문 분석 _ 계산 언어학 협회의 트랜잭션_, 4:313-327, 2016. doi: 10.1162/tacl_a_00101. URL [https://www.aclweb.org/anthology/Q16-1023](https://www.aclweb.org/anthology/Q16-1023).\n' +
      '* Klein and Manning [2003] Dan Klein and Christopher D. Manning. 정확하게 비언어화된 구문 분석 2003년 7월, 일본 삿포로 제41차 컴퓨터 언어학 협회 연례 회의록에서. doi: 10.3115/1075096.1075150. URL [https://www.aclweb.org/anthology/P03-1054](https://www.aclweb.org/anthology/P03-1054).\n' +
      '* Ko et al. [2020] Miyoung Ko, Jinhyuk Lee, Hyunje Kim, Gangwoo Kim, and Jaewoo Kang. Look at the first sentence: Position bias in question answering. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1109-1121, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.84. URL [https://www.aclweb.org/anthology/2020.emnlp-main.84](https://www.aclweb.org/anthology/2020.emnlp-main.84).\n' +
      '* Krippendorff [2011] K. 크리펜도르프 크리펜도르프의 알파-신뢰도 계산 2011년\n' +
      '* Kudo [2006] Taku Kudo. McCab: Yet 또 다른 품사 및 형태소 분석기, 2006. URL [https://taku910.github.io/mecab/](https://taku910.github.io/mecab/)\n' +
      '* Kwiatkowski et al. [2019] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:452-466, March 2019. doi: 10.1162/tacl_a_00276. URL [https://www.aclweb.org/anthology/Q19-1026](https://www.aclweb.org/anthology/Q19-1026).\n' +
      '* Lalor and Yu [2020] John P. Lalor and Hong Yu. 능력 추정을 통한 교육과정 학습을 위한 동적 자료 선정. 컴퓨팅 언어학 협회의 _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 545-555, Online, November 2020. The Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.48. URL [https://www.aclweb.org/anthology/2020.findings-emnlp.48](https://www.aclweb.org/anthology/2020.findings-emnlp.48)\n' +
      '* Lan et al. [2020] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. In _International Conference on Learning Representations_, 2020. URL [https://openreview.net/forum?id=H1eA7AEtvS](https://openreview.net/forum?id=H1eA7AEtvS).\n' +
      '* Le et al. [2020] Hang Le, Loic Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoit Crabbe, Laurent Besacier, and Didier Schwab. FlauBERT: Unsupervised language model pre-training for French. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 2479-2490, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.302](https://www.aclweb.org/anthology/2020.lrec-1.302).\n' +
      '* Lee [2020] Junbum Lee. KcBERT: 한국어 댓글 BERT. "제32회 인간 및 인지 언어 기술에 관한 연례 회의"에서 2020년 437-440페이지를 참조하십시오.\n' +
      '* Lee et al. [2018] Kyungjae Lee, Kyoungho Yoon, Sunghyun Park, and Seung-won Hwang. Semi-supervised training data generation for multilingual question answering. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL [https://www.aclweb.org/anthology/L18-1437](https://www.aclweb.org/anthology/L18-1437).\n' +
      '* Lee et al. [2020] Sangah Lee, Hansol Jang, Yunmee Baik, Suzi Park, and Hypoil Shin. KR-BERT: A small-scale Korean-specific language model. _arXiv preprint arXiv:2008.03979_, 2020.\n' +
      '* Lewis et al. [2020] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7871-7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL [https://www.aclweb.org/anthology/2020.acl-main.703](https://www.aclweb.org/anthology/2020.acl-main.703).\n' +
      '* Li et al. [2020] Shiyang Li, Semih Yavuz, Kazuma Hashimoto, Jia Li, Tong Niu, Nazneen Rajani, Xifeng Yan, Yingbo Zhou, and Caiming Xiong. CoCo: Controllable counterfactuals for evaluating dialogue state trackers. _arXiv preprint arXiv:2010.12850_, 2020.\n' +
      '\n' +
      '* Liang et al. [2020] Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 6008-6018, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.484. URL [https://www.aclweb.org/anthology/2020.emnlp-main.484](https://www.aclweb.org/anthology/2020.emnlp-main.484).\n' +
      '* Lim et al. [2019] Seungyoung Lim, Myungji Kim, and Jooyoul Lee. KorQuAD1.0: Korean QA dataset for machine reading comprehension. _arXiv preprint arXiv:1909.07005_, 2019.\n' +
      '* Lin [2004] Chin-Yew Lin. 요약 자동 평가 패키지입니다. "텍스트 요약 분기 아웃"에서, 2004년 7월, 바르셀로나, 스페인, 74-81페이지. 계산 언어학 협회. URL [https://www.aclweb.org/anthology/W04-1013](https://www.aclweb.org/anthology/W04-1013).\n' +
      '* Liu et al. [2021] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. On-the-fly controlled text generation with experts and anti-experts, 2021.\n' +
      '* Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.\n' +
      '* Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. 분리된 중량 감쇠 규칙화. 2019년 _International Conference on Learning Representations_ 에서 URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)을 참조 하세요.\n' +
      '* Marcus et al. [1993] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of English: The Penn Treebank. _Computational Linguistics_, 19(2):313-330, 1993. URL [https://www.aclweb.org/anthology/J93-2004](https://www.aclweb.org/anthology/J93-2004).\n' +
      '* McCann et al. [2018] Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. _arXiv preprint arXiv:1806.08730_, 2018.\n' +
      '* McDonald et al. [2013] Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Tackstrom, Claudia Bedini, Nuria Bertomeu Castello, and Jungmee Lee. Universal Dependency annotation for multilingual parsing. In _Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 92-97, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/P13-2017](https://www.aclweb.org/anthology/P13-2017).\n' +
      '* McNamee and Dang [2009] Paul McNamee and Hoa Trang Dang. TAC 2009 지식 기반 인구 추적 개요. TAC(텍스트 분석 회의)_에서, 제17권, 제111-113페이지, 2009.\n' +
      '* Mehri et al. [2020] Shikib Mehri, Mihail Eric, and Dilek Hakkani-Tur. DialoGLUE: A natural language understanding benchmark for task-oriented dialogue. _arXiv preprint arXiv:2009.13570_, 2020.\n' +
      '* Min et al. [2020] Junghyun Min, R. Thomas McCoy, Dipanjan Das, Emily Pitler, and Tal Linzen. Syntactic data augmentation increases robustness to inference heuristics. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 2339-2352, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.212. URL [https://www.aclweb.org/anthology/2020.acl-main.212](https://www.aclweb.org/anthology/2020.acl-main.212).\n' +
      '* Min et al. [2019] Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. Compositional questions do not necessitate multi-hop reasoning. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4249-4257, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1416. URL [https://www.aclweb.org/anthology/P19-1416](https://www.aclweb.org/anthology/P19-1416).\n' +
      '* Mintz et al. [2009] Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. Distant supervision for relation extraction without labeled data. In _Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP_, pages 1003-1011, Suntec, Singapore, August 2009. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/P09-1113](https://www.aclweb.org/anthology/P09-1113).\n' +
      '* Moon et al. [2020] Jihyung Moon, Won Ik Cho, and Junbum Lee. BEEP! Korean corpus of online news comments for toxic speech detection. In _Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media_, pages 25-31, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.socialnlp-1.4. URL [https://www.aclweb.org/anthology/2020.socialnlp-1.4](https://www.aclweb.org/anthology/2020.socialnlp-1.4).\n' +
      '\n' +
      '* Nam et al. [2020] Sangha Nam, Minho Lee, Donghwan Kim, Kijong Han, Kuntae Kim, Sooji Yoon, Eun-kyung Kim, and Key-Sun Choi. Effective crowdsourcing of multiple tasks for comprehensive knowledge extraction. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 212-219, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.27](https://www.aclweb.org/anthology/2020.lrec-1.27).\n' +
      '* Nangia and Bowman [2019] Nikita Nangia and Samuel R. 보우먼 인간 vs. muppet: GLUE 벤치마크에 대한 인간 성능의 보수적인 추정치. "제57회 컴퓨터 언어학 협회 연례 회의"에서 2019년 7월 이탈리아 피렌체 4566-4575 페이지. 컴퓨터 언어학 협회. doi: 10.18653/v1/P19-1449. URL [https://www.aclweb.org/anthology/P19-1449](https://www.aclweb.org/anthology/P19-1449).\n' +
      '* Nangia et al. [2020] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1953-1967, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.154. URL [https://www.aclweb.org/anthology/2020.emnlp-main.154](https://www.aclweb.org/anthology/2020.emnlp-main.154).\n' +
      '* Nguyen et al. [2016] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated MAchine reading COmprehension dataset. November 2016. URL [https://www.microsoft.com/en-us/research/publication/ms-marco-human-generated-machine-reading-comprehension-dataset/](https://www.microsoft.com/en-us/research/publication/ms-marco-human-generated-machine-reading-comprehension-dataset/).\n' +
      '* Nivre et al. [2016] Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christopher D. Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel Zeman. Universal Dependencies v1: A multilingual treebank collection. In _Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\'16)_, pages 1659-1666, Portoroz, Slovenia, May 2016. European Language Resources Association (ELRA). URL [https://www.aclweb.org/anthology/L16-1262](https://www.aclweb.org/anthology/L16-1262).\n' +
      '* [98] National Institute of Korean Languages. NIKL CORPORA 2020 (v.1.0), 2020. URL [https://corpus.korean.go.kr](https://corpus.korean.go.kr).\n' +
      '* Oh et al. [2020] Tae Hwan Oh, Ji Yoon Han, Hyonsu Choe, Seokwon Park, Han He, Jinho D. Choi, Na-Rae Han, Jena D. Hwang, and Hansaem Kim. Analysis of the Penn Korean Universal Dependency treebank (PKT-UD): Manual revision to build robust parsing model in Korean. In _Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies_, pages 122-131, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.iwpt-1.13. URL [https://www.aclweb.org/anthology/2020.iwpt-1.13](https://www.aclweb.org/anthology/2020.iwpt-1.13).\n' +
      '* Pan et al. [2017] Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. Cross-lingual name tagging and linking for 282 languages. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1946-1958, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1178. URL [https://www.aclweb.org/anthology/P17-1178](https://www.aclweb.org/anthology/P17-1178).\n' +
      '* Park and Cho [2014] Eunjeong L. 박성준 KoNLPy: 파이썬에서 한국어 자연어 처리. 2014년 10월 춘천에서 개최된 제26회 인간 및 인지 언어 기술에 관한 연례 회의록에서.\n' +
      '* Park [2020] Jangwon Park. KoELECTRA: Pretrained ELECTRA model for Korean. [https://github.com/monologg/KoELECTRA] (https://github.com/monologg/KoELECTRA), 2020.\n' +
      '* Park et al. [2020] Kyubyong Park, Joohong Lee, Seongbo Jang, and Dawoon Jung. An empirical study of tokenization strategies for various Korean NLP tasks. In _Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing_, pages 133-142, Suzhou, China, December 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.aacl-main.17](https://www.aclweb.org/anthology/2020.aacl-main.17).\n' +
      '* Peters et al. [2018] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 2227-2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL [https://www.aclweb.org/anthology/N18-1202](https://www.aclweb.org/anthology/N18-1202).\n' +
      '\n' +
      '* Petroni et al. [2020] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. KILT: a benchmark for knowledge intensive language tasks. _arXiv preprint arXiv:2009.02252_, 2020.\n' +
      '* Petrov et al. [2012] Slav Petrov, Dipanjan Das, and Ryan McDonald. A universal part-of-speech tagset. In _Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\'12)_, pages 2089-2096, Istanbul, Turkey, May 2012. European Language Resources Association (ELRA). URL [http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf).\n' +
      '* Phang et al. [2018] Jason Phang, Thibault Fevry, and Samuel R Bowman. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. _arXiv preprint arXiv:1811.01088_, 2018.\n' +
      '* Poliak et al. [2018] Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. Hypothesis only baselines in natural language inference. In _Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics_, pages 180-191, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/S18-2023. URL [https://www.aclweb.org/anthology/S18-2023](https://www.aclweb.org/anthology/S18-2023).\n' +
      '* Quan et al. [2020] Jun Quan, Shian Zhang, Qian Cao, Zizhong Li, and Deyi Xiong. RiSAWOZ: A large-scale multi-domain Wizard-of-Oz dataset with rich semantic annotations for task-oriented dialogue modeling. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 930-940, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.67. URL [https://www.aclweb.org/anthology/2020.emnlp-main.67](https://www.aclweb.org/anthology/2020.emnlp-main.67).\n' +
      '* Radford et al. [2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\n' +
      '* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).\n' +
      '* Rajpurkar et al. [2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 2383-2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL [https://www.aclweb.org/anthology/D16-1264](https://www.aclweb.org/anthology/D16-1264).\n' +
      '* Rajpurkar et al. [2018] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\'t know: Unanswerable questions for SQuAD. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 784-789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL [https://www.aclweb.org/anthology/P18-2124](https://www.aclweb.org/anthology/P18-2124).\n' +
      '* Rastogi et al. [2020] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(05):8689-8696, Apr. 2020. doi: 10.1609/aaai.v34i05.6394. URL [https://ojs.aaai.org/index.php/AAAI/article/view/6394](https://ojs.aaai.org/index.php/AAAI/article/view/6394).\n' +
      '* Reimers and Gurevych [2019] Nils Reimers and Iryna Gurevych. Sentence-BERT: Siamese BERT-network를 이용한 문장 임베딩. "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_", pages 3982-3992, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL [https://www.aclweb.org/anthology/D19-1410](https://www.aclweb.org/anthology/D19-1410).\n' +
      '* Riedel et al. [2010] Sebastian Riedel, Limin Yao, and Andrew McCallum. Modeling relations and their mentions without labeled text. In Jose Luis Balcazar, Francesco Bonchi, Aristides Gionis, and Michele Sebag, editors, _Machine Learning and Knowledge Discovery in Databases_, pages 148-163, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. ISBN 978-3-642-15939-8.\n' +
      '* Ritter et al. [2011] Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. Named entity recognition in tweets: An experimental study. In _Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing_, pages 1524-1534, Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/D11-1141](https://www.aclweb.org/anthology/D11-1141).\n' +
      '* Robertson et al. [1995] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at trec-3. _Nist Special Publication Sp_, 109:109, 1995.\n' +
      '\n' +
      '* Rust et al. [2020] Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder, and Iryna Gurevych. How good is your tokenizer? on the monolingual performance of multilingual language models. _arXiv preprint arXiv:2012.15613_, 2020.\n' +
      '* Saha et al. [2018] Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. DuoRC: Towards complex language understanding with paraphrased reading comprehension. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1683-1693, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1156. URL [https://www.aclweb.org/anthology/P18-1156](https://www.aclweb.org/anthology/P18-1156).\n' +
      '* Schiersch et al. [2018] Martin Schiersch, Veselina Mironova, Maximilian Schmitt, Philippe Thomas, Aleksandra Gabryszak, and Leonhard Hennig. A German corpus for fine-grained named entity recognition and relation extraction of traffic and industry events. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, May 2018. European Language Resources Association (ELRA). URL [https://www.aclweb.org/anthology/L18-1703](https://www.aclweb.org/anthology/L18-1703).\n' +
      '* Sen and Saffari [2020] Priyanka Sen and Amir Saffari. 모델은 질문 응답 데이터 세트에서 무엇을 배우나요? "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"_, pages 2429-2438, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.190. URL [https://www.aclweb.org/anthology/2020.emnlp-main.190](https://www.aclweb.org/anthology/2020.emnlp-main.190).\n' +
      '* Sennrich et al. [2016] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1715-1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL [https://www.aclweb.org/anthology/P16-1162](https://www.aclweb.org/anthology/P16-1162).\n' +
      '* Shah et al. [2018] Pararth Shah, Dilek Hakkani-Tur, Gokhan Tur, Abhinav Rastogi, Ankur Bapna, Neha Nayak, and Larry Heck. Building a conversational agent overnight with dialogue self-play. _arXiv preprint arXiv:1801.04871_, 2018.\n' +
      '* Shavrina et al. [2020] Tatiana Shavrina, Alena Fenogenova, Emelyanov Anton, Denis Shevelev, Ekaterina Artemova, Valentin Malykh, Vladislav Mikhailov, Maria Tikhonova, Andrey Chertok, and Andrey Evlampiev. RussianSuperGLUE: A Russian language understanding evaluation benchmark. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 4717-4726, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.381. URL [https://www.aclweb.org/anthology/2020.emnlp-main.381](https://www.aclweb.org/anthology/2020.emnlp-main.381).\n' +
      '* Socher et al. [2013] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing_, pages 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/D13-1170](https://www.aclweb.org/anthology/D13-1170).\n' +
      '* Strauss et al. [2016] Benjamin Strauss, Bethany Toma, Alan Ritter, Marie-Catherine de Marneffe, and Wei Xu. Results of the WNUT16 named entity recognition shared task. In _Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT)_, pages 138-144, Osaka, Japan, December 2016. The COLING 2016 Organizing Committee. URL [https://www.aclweb.org/anthology/W16-3919](https://www.aclweb.org/anthology/W16-3919).\n' +
      '* Choi et al. [1994] Key sun Choi, Young S. Han, Young G. Han, and Oh W. Kwon. KAIST tree bank project for Korean: Present and future development. In _In Proceedings of the International Workshop on Sharable Natural Language Resources_, pages 7-14, 1994.\n' +
      '* Sang and De Meulder[2003] Erik F. Tjong Kim Sang and Fien De Meulder. CoNLL-2003 공유 태스크 소개: 언어 독립적인 명명된 개체 인식. HLT-NAACL 2003_에서 제7차 자연어 학습 회의의 진행에서 142-147, 2003 페이지. URL [https://www.aclweb.org/anthology/W03-0419](https://www.aclweb.org/anthology/W03-0419).\n' +
      '* Trischler et al. [2017] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. In _Proceedings of the 2nd Workshop on Representation Learning for NLP_, pages 191-200, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-2623. URL <[https://www.aclweb.org/anthology/W17-2623](https://www.aclweb.org/anthology/W17-2623)>.\n' +
      '* Vania et al. [2020] Clara Vania, Ruijie Chen, and Samuel R. Bowman. Asking Crowdworkers to Write Entailment Examples: The Best of Bad options. In _Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing_, pages 672-686, Suzhou, China, December 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.aacl-main.68](https://www.aclweb.org/anthology/2020.aacl-main.68).\n' +
      '\n' +
      '* Wang et al. [2019] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf).\n' +
      '* Wang et al. [2019] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=rJ4km2R5t7](https://openreview.net/forum?id=rJ4km2R5t7).\n' +
      '* Warstadt et al. [2019] Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. _Transactions of the Association for Computational Linguistics_, 7:625-641, March 2019. doi: 10.1162/tacl_a_00290. URL [https://www.aclweb.org/anthology/Q19-1040](https://www.aclweb.org/anthology/Q19-1040).\n' +
      '* Wen et al. [2017] Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. A network-based end-to-end trainable task-oriented dialogue system. In _Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers_, pages 438-449, Valencia, Spain, April 2017. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/E17-1042](https://www.aclweb.org/anthology/E17-1042).\n' +
      '* Wenzek et al. [2020] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 4003-4012, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://www.aclweb.org/anthology/2020.lrec-1.494](https://www.aclweb.org/anthology/2020.lrec-1.494).\n' +
      '* Wilie et al. [2020] Bryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri Bahar, and Ayu Purwarianti. IndoNLU: Benchmark and resources for evaluating Indonesian natural language understanding. In _Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing_, pages 843-857, Suzhou, China, December 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.aacl-main.85](https://www.aclweb.org/anthology/2020.aacl-main.85).\n' +
      '* Williams et al. [2018] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 1112-1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL [https://www.aclweb.org/anthology/N18-1101](https://www.aclweb.org/anthology/N18-1101).\n' +
      '* Wolf et al. [2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6).\n' +
      '* Wu et al. [2019] Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, and Pascale Fung. Transferable multi-domain state generator for task-oriented dialogue systems. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 808-819, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1078. URL [https://www.aclweb.org/anthology/P19-1078](https://www.aclweb.org/anthology/P19-1078).\n' +
      '* Xu et al. [2017] Jingjing Xu, Ji Wen, Xu Sun, and Qi Su. A discourse-level named entity recognition and relation extraction dataset for Chinese literature text. _arXiv preprint arXiv:1711.07010_, 2017.\n' +
      '* Xu et al. [2020] Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. CLUE: A Chinese language understanding evaluation benchmark. In _Proceedings of the 28th International Conference on Computational Linguistics_, pages 4762-4772, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.419. URL [https://www.aclweb.org/anthology/2020.coling-main.419](https://www.aclweb.org/anthology/2020.coling-main.419).\n' +
      '* Yang et al. [2015] Yi Yang, Wen-tau Yih, and Christopher Meek. WikiQA: A challenge dataset for open-domain question answering. In _Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing_, pages 2013-2018, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1237. URL [https://www.aclweb.org/anthology/D15-1237](https://www.aclweb.org/anthology/D15-1237).\n' +
      '* Yang et al. [2019] Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3687-3692, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1382. URL [https://www.aclweb.org/anthology/D19-1382](https://www.aclweb.org/anthology/D19-1382).\n' +
      '* Yang et al. [2018] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 2369-2380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL [https://www.aclweb.org/anthology/D18-1259](https://www.aclweb.org/anthology/D18-1259)>.\n' +
      '* Yang et al. [2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf).\n' +
      '* Yao et al. [2019] Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. DocRED: A large-scale document-level relation extraction dataset. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 764-777, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1074. URL [https://www.aclweb.org/anthology/P19-1074](https://www.aclweb.org/anthology/P19-1074).\n' +
      '* Young et al. [2014] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78, 2014. doi: 10.1162/tacl_a_00166. URL [https://www.aclweb.org/anthology/Q14-1006](https://www.aclweb.org/anthology/Q14-1006).\n' +
      '* Yu et al. [2020] Dian Yu, Kai Sun, Claire Cardie, and Dong Yu. Dialogue-based relation extraction. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 4927-4940, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.444. URL [https://www.aclweb.org/anthology/2020.acl-main.444](https://www.aclweb.org/anthology/2020.acl-main.444).\n' +
      '* Zhang et al. [2018] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. ReCoRD: Bridging the gap between human and machine commonsense reading comprehension. _arXiv preprint arXiv:1810.12885_, 2018.\n' +
      '* Zhang et al. [2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9ao2-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9ao2-Paper.pdf).\n' +
      '* Zhang et al. [2019] Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 1298-1308, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1131. URL [https://www.aclweb.org/anthology/N19-1131](https://www.aclweb.org/anthology/N19-1131).\n' +
      '* Zhang et al. [2017] Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. Position-aware attention and supervised data improve slot filling. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 35-45, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1004. URL [https://www.aclweb.org/anthology/D17-1004](https://www.aclweb.org/anthology/D17-1004).\n' +
      '\n' +
      '* Zhu et al. [2020] Qi Zhu, Kaili Huang, Zheng Zhang, Xiaoyan Zhu, and Minlie Huang. CrossWOZ: A large-scale Chinese cross-domain task-oriented dialogue dataset. _Transactions of the Association for Computational Linguistics_, 8:281-295, 2020. doi: 10.1162/tacl_a_00314. URL [https://www.aclweb.org/anthology/2020.tacl-1.19](https://www.aclweb.org/anthology/2020.tacl-1.19).\n' +
      '\n' +
      '## Index\n' +
      '\n' +
      '아크로판(아크로판 뉴스) 10\n' +
      '\n' +
      'AIRBNB(Airbnb Reviews) 10\n' +
      '\n' +
      'CC-100-Kor, 50\n' +
      '\n' +
      'DP(Dependency Parsing), 33\n' +
      '\n' +
      'DST(대화 상태 추적), 44\n' +
      '\n' +
      'KLUE\n' +
      '\n' +
      '(한국어 이해 평가) 4\n' +
      '\n' +
      'KLUE-BERT, 50\n' +
      '\n' +
      'KLUE-DP, 36\n' +
      '\n' +
      'KLUE-MRC, 43\n' +
      '\n' +
      'KLUE-NER, 27\n' +
      '\n' +
      'KLUE-NLI, 24\n' +
      '\n' +
      'KLUE-RE, 32\n' +
      '\n' +
      'KLUE-RoBERTa, 50\n' +
      '\n' +
      'KLUE-STS, 19\n' +
      '\n' +
      'MODU(Modu Corpus) 50\n' +
      '\n' +
      'MRC(Machine Reading Comprehension), 37\n' +
      '\n' +
      'NAMUWIKI, 50\n' +
      '\n' +
      'NER(Named Entity Recognition) 25\n' +
      '\n' +
      'NEWSCRAWL, 50\n' +
      '\n' +
      'NLI(Natural Language Inference) 20\n' +
      '\n' +
      'NSMC(NAVER Sentiment Movie Corpus) 10\n' +
      '\n' +
      'PARAKQC, 10\n' +
      '\n' +
      'PETITION, 50\n' +
      '\n' +
      '(주)폴리시 10\n' +
      '\n' +
      'RE(Relation Extraction), 28\n' +
      '\n' +
      'STS(Semantic Textual Similarity), 16\n' +
      '\n' +
      'TC(Topic Classification) 13\n' +
      '\n' +
      '한국경제신문 10\n' +
      '\n' +
      'WIKINEWS, 10\n' +
      '\n' +
      'WIKIPEDIA, 10\n' +
      '\n' +
      'WIKITREE, 10\n' +
      '\n' +
      'WoS (Wizard of Seoul, KLUE-DST), 49\n' +
      '\n' +
      'YNA(연합뉴스) 10\n' +
      '\n' +
      'YNAT (YNA 토픽 분류, KLUE-TC), 15\n' +
      '\n' +
      '## Contribution\n' +
      '\n' +
      '**박성준** 은 프로젝트 관리자로서 프로젝트를 주도하고 프로젝트를 시작했으며 이 프로젝트의 전반적인 진행 상황에 대한 결정을 내리고 재원을 확보했으며 기사에 대해 ACROFAN에 가입했으며 IRB 제출 및 연구 논문을 정리했습니다.\n' +
      '\n' +
      '**문지형** 은 프로젝트 관리자로서 프로젝트를 주도하고 전체 데이터 세트, 모델 및 윤리적 문제를 관리했으며 문서에 대해 ACROFAN에 등록하고 IRB를 준비했으며 NER, STS, NLI, MRC 데이터 세트 구성, AIRBNB, POLICY 코퍼라 컬렉션 및 리더보드 설계에 기여했습니다.\n' +
      '\n' +
      '**성동 김** 은 언어 모델의 전반적인 미세 조정을 관리 하 고 DST의 담당자 (PIC) 역할을 하며 TC, STS 및 RE의 데이터 세트 구성에 기여 했습니다.\n' +
      '\n' +
      '**원익 조** 는 TC, STS, NLI, RE, MRC 및 DST의 전체 데이터 세트 구성을 관리 하 고 PARAKQC의 원본 말뭉치를 제공 하 고 STS의 PIC 역할을 했습니다.\n' +
      '\n' +
      '**한지윤** 은 DP 및 NER의 전체 데이터 세트 구성을 관리하고 NLI의 PIC 역할을 하며 STS의 데이터 세트 구성에 기여했으며 IRB 준비에 참여했다.\n' +
      '\n' +
      '**장원공원**은 모델 사전 훈련의 PIC 역할을 했으며 YNA의 텍스트 수집에 기여했으며 MODU, CC-100-Kor, NAMUWIKI, NEWSCRAWL 및 PETITION을 수집 및 전처리하고 TC의 미세 조정을 수행했다.\n' +
      '\n' +
      '**치성 송** 은 NER의 PIC 역할을 하며 DP 및 DST의 데이터 세트 구성에 기여했습니다.\n' +
      '\n' +
      '**준성 김** 은 MRC의 PIC 역할을 하고 MRC의 미세 조정을 수행했으며 WIKITREE 및 WIKIPEDIA의 텍스트 수집에 기여했다.\n' +
      '\n' +
      '**송영숙** 은 TC의 PIC 역할을 했으며 NER 및 DST 데이터 세트 구성에 기여했습니다.\n' +
      '\n' +
      '**오태환** 은 DP의 PIC 역할을 했으며 NER 및 NLI의 데이터 세트 구성에 기여했으며 IRB 준비에 참여했다.\n' +
      '\n' +
      '**이주홍** 은 RE의 PIC 역할을 했으며 RE의 미세 조정을 수행했습니다.\n' +
      '\n' +
      '**오주현** 은 NLI, NER, DP 및 STS의 데이터 세트 구성에 기여했으며 모델 사전 훈련을 위한 윤리적 고려 사항, IRB 준비 및 설정에 참여했다.\n' +
      '\n' +
      '**성원류** 는 STS, NLI, RE 및 MRC의 데이터 세트 구성에 기여했으며 전체 모델 사전 훈련 및 작업별 미세 조정에 참여했다.\n' +
      '\n' +
      '**정영훈** 은 WIKINEWS의 텍스트 수집, DP 모델링 및 사전 훈련 말뭉치의 전처리에 기여했습니다.\n' +
      '\n' +
      '**이인권** 은 텍스트 수집, DP 모델링 및 사전 훈련 말뭉치의 전처리에 기여했습니다.\n' +
      '\n' +
      '**서상우** 는 RE의 데이터 세트 구성에 기여했으며 IRB 준비에 참여했다.\n' +
      '\n' +
      '**이동준** 은 미세 조정 파이프라인 구축 및 STS 모델링에 기여했습니다.\n' +
      '\n' +
      '**김현우** 는 MRC의 데이터 세트 구성에 기여했으며 IRB 준비에 참여했다.\n' +
      '\n' +
      '**이명화** 는 STS 및 TC 데이터 세트 구성에 기여했습니다.\n' +
      '\n' +
      '**장성보** 는 RE의 데이터 세트 구성에 기여했으며 IRB 준비에 참여했다.\n' +
      '\n' +
      '**승원도** 는 DST 및 텍스트 컬렉션의 데이터 세트 구성에 기여했습니다.\n' +
      '\n' +
      '**김선경** 은 RE 및 MRC의 데이터 세트 구성 및 MRC 모델링에 기여했습니다.\n' +
      '\n' +
      '**임경태** 는 DP의 데이터 세트 구성에 기여했습니다.\n' +
      '\n' +
      '**이종원** 은 DST의 데이터 세트 구성 및 모델링에 기여했습니다.\n' +
      '\n' +
      '**규민 공원** 은 DST의 데이터 세트 구성에 기여했으며 IRB 준비에 참여했습니다.\n' +
      '\n' +
      '**Jamin Shin** 은 DST의 데이터 세트 구성에 기여했습니다.\n' +
      '\n' +
      '**김성현** 은 NER의 데이터 세트 구성 및 모델링에 기여했습니다.\n' +
      '\n' +
      '**Lucy Park** 은 MRC의 데이터 세트 구성에 기여했으며 NSMC의 원래 말뭉치를 제공했습니다.\n' +
      '\n' +
      '**Alice Oh** 는 프로젝트에 조언하고 KAIST를 통해 프로젝트를 후원했으며 피드백을 제공하고 데이터 세트 및 모델의 품질을 개선하는 더 나은 방법을 제안했으며 최종 원고에 도움이 되었습니다.\n' +
      '\n' +
      '**하정우** 는 프로젝트에 대해 조언하고, NAVER를 통해 주석 비용과 컴퓨팅 클라우드 크레딧을 후원했으며, 한국 경제 일보의 라이센스 없는 뉴스 기사를 제공하고 최종 원고에 도움을 주었습니다.\n' +
      '\n' +
      '**조경현** 은 프로젝트에 조언하고 중요한 피드백을 제공했으며 데이터 세트 및 모델의 품질을 개선하는 더 나은 방법을 제안했으며 최종 원고를 연마하고 다시 작성하는 데 많은 도움이 되었습니다.\n' +
      '\n' +
      '모든 참가자가 이 원고에 기여했습니다.\n' +
      '\n' +
      '## Appendix Dev Set Results\n' +
      '\n' +
      'KLUE 벤치마크의 조기 포화를 방지하기 위해 사용자는 하루에 한 번 모델을 제출하도록 제한한다. 따라서 표 36에서 향후 작업 및 로컬 테스트에 대한 참조를 제공하기 위해 dev 세트 결과를 제시한다. 사용된 모델은 테스트 세트에서와 동일하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c c c} \\hline \\hline  & **VNAT** & **KLUE-STS** & **KLUE-NLI** & **KLUE-NER** & **KLUE-RE** & **KLUE-DP** & **KLUE-MRC** & \\multicolumn{2}{c}{**WoS**} \\\\ \\cline{2-13}\n' +
      '**Model** & F1 & R\\({}^{p}\\) & F1 & ACC & F1\\({}^{E}\\) & F1\\({}^{C}\\) & F1\\({}^{mic}\\) & AUC & UAS & LAS & EM & ROUGE & JGA & F1\\({}^{S}\\) \\\\ \\hline\n' +
      '**mBERfast** & 82.64 & 82.97 & 75.93 & 72.90 & 75.56 & 88.81 & 58.39 & 56.41 & 88.53 & 86.04 & 49.96 & 55.57 & 35.27 & 88.60 \\\\\n' +
      '**XLM-Rcase** & 84.52 & 88.88 & 81.20 & 78.23 & 80.48 & 92.14 & 57.62 & 57.05 & 93.12 & 87.23 & 26.76 & 53.36 & 41.54 & 89.81 \\\\\n' +
      '**XLM-Rcase** & **87.30** & 93.08 & **87.17** & 86.40 & 82.18 & **93.20** & 58.75 & 63.53 & 92.87 & 87.82 & 35.23 & 66.55 & 42.44 & 89.88 \\\\ \\hline\n' +
      '**R-BERfast** & 85.36 & 87.50 & 77.92 & 77.10 & 74.97 & 90.46 & 62.83 & 65.42 & 92.87 & 87.13 & 48.95 & 88.38 & 45.60 & 90.82 \\\\\n' +
      '**KoELECTRBASE** & 85.99 & 32.14 & 85.89 & 86.87 & **86.06** & 22.75 & 62.67 & 57.46 & 90.93 & 87.07 & 59.54 & 65.64 & 39.83 & 88.91 \\\\ \\hline\n' +
      '**KLUE-BERTBASE** & 86.95 & 91.01 & 83.44 & 79.87 & 83.71 & 91.17 & 65.58 & 68.11 & 93.07 & 87.25 & 62.42 & 68.15 & 46.72 & 91.59 \\\\\n' +
      '**KLUE-RoBERTshALL** & 85.95 & 91.70 & 85.42 & 81.00 & 83.55 & 91.20 & 61.26 & 60.89 & 93.47 & 87.50 & 58.28 & 63.56 & 46.65 & 91.50 \\\\\n' +
      '**KLUE-RoBERTBASE** & 86.19 & 92.91 & 86.78 & 86.30 & 83.81 & 91.09 & 66.73 & 68.11 & 93.75 & 87.77 & 69.56 & 74.64 & 47.41 & 91.60 \\\\\n' +
      '**KLUE-RoBERTBLAGCE** & 85.88 & **93.20** & 86.13 & **89.50** & 84.54 & 91.45 & **71.06** & **73.33** & **93.84** & **87.93** & **75.26** & **80.30** & **49.39** & **92.19** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 36: KLUE 벤치마크 디브 세트에 대한 사전 훈련된 LMs 및 기타 기준선의 성능. 기호는 표 32와 동일하다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
{
    "2401.08565": {
        "paper_id": "2401.08565",
        "abs_url": "https://arxiv.org/abs/2401.08565",
        "pdf_url": "https://arxiv.org/pdf/2401.08565.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2401.08565_Tuning_Language_Models_by_Proxy.pdf",
        "title": "Tuning Language Models by Proxy",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Alisa Liu",
            "Xiaochuang Han",
            "Yizhong Wang",
            "Yulia Tsvetkov",
            "Yejin Choi",
            "Noah A. Smith"
        ],
        "abstract": "Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the result of directly tuning the model, but by accessing only its prediction over the output vocabulary. Our method instead tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the base model in the direction of tuning, while retaining the benefits of larger scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. Interestingly, when tested on TruthfulQA, proxy-tuned models are actually more truthful than directly tuned models, possibly because decoding-time guidance better retains the model's factual knowledge. We then demonstrate the generality of proxy-tuning by applying it for domain adaptation on code, and task-specific finetuning on question-answering and math problems. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance.",
        "comments": "21 pages",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/tuning-language-models-by-proxy",
        "bibtex": "@misc{liu2024tuning,\n      title={Tuning Language Models by Proxy}, \n      author={Alisa Liu and Xiaochuang Han and Yizhong Wang and Yulia Tsvetkov and Yejin Choi and Noah A. Smith},\n      year={2024},\n      eprint={2401.08565},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}
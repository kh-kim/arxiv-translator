# Tuning Language Models by Proxy

Alisa Liu\({}^{\heartsuit}\) Xiaochuang Han\({}^{\heartsuit}\) Yizhong Wang\({}^{\heartsuit}\) Yulia Tsvetkov\({}^{\heartsuit}\) Yejin Choi\({}^{\heartsuit}\) Noah A. Smith\({}^{\heartsuit}\)

\({}^{\heartsuit}\)Paul G. Allen School of Computer Science & Engineering, University of Washington

\({}^{\clubsuit}\)Allen Institute for AI

alisaliu@cs.washington.edu

###### Abstract

Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce **proxy-tuning**, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the result of directly tuning the model, but by accessing _only its prediction_ over the output vocabulary. Our method instead tunes a _smaller_ LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the base model in the direction of tuning, while retaining the benefits of larger-scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. Interestingly, when tested on TruthfulQA, proxy-tuned models are actually more truthful than directly tuned models, possibly because decoding-time guidance better retains the model's factual knowledge. We then demonstrate the generality of proxy-tuning by applying it for domain adaptation on code, and task-specific finetuning on question-answering and math problems. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance.

## 1 Introduction

Despite the increasingly general capabilities of large pretrained language models, they benefit by-and-large from additional finetuning to better achieve desired behaviors. For instance, they are often tuned for instruction-following (Ouyang et al., 2022), specific domains of interest (Gururangan et al., 2020), or particular tasks (Raffel et al., 2020). However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private (e.g., GPT-4; OpenAI, 2023). Thus there remains a challenge of how to efficiently customize ever-larger LMs for the needs of diverse users and applications.

In this work, we introduce a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the result of directly tuning the model, without ever accessing the model's internal weights, only its predictive distributions over the output vocabulary. Illustrated in Figure 1, our method, **proxy-tuning**, tunes a _smaller_ LM (potentially available off-the-shelf), then contrasts the prediction of the small tuned model (dubbed the expert) and its untuned version (the anti-expert) to guide the larger base model. Specifically, we apply the decoding-time experts (DE experts; Liu et al., 2021) equation to shift the original predictions of the base model in the direction of the difference that results from tuning.

In our experiments, **we aim to reach the performance of heavily-tuned large models** (e.g., Llama2-70B-chat), **by only tuning smaller models**. Specifically, we apply proxy-tuning to steer a large pretrained (base) model (Llama2-13B or 70B) using small, cheaper-to-tune (anti-)experts (based on Llama2-7B) for instruction-following, domain adaptation, and taskfinetuning. For **instruction-tuning** (SS3), we contrast the predictions of **Llama2-7B-chat** and **Llama2-7B** for guidance. Remarkably, we find that proxy-tuning closes 91% of the performance gap between **Llama2-13B** and its directly tuned **chat** version, and 88% of the gap for the 70B model, when evaluated across knowledge, reasoning, and safety benchmarks. In particular, on knowledge-intensive tasks, proxy-tuning sometimes _surpasses_ the performance of direct instruction-tuning, suggesting that proxy-tuning large pretrained LMs may preserve more learned knowledge than directly updating the weights. Proxy-tuning a larger model also consistently outperforms the small tuned expert, indicating that our method combines the benefits of tuning with larger pretraining scale.

For **domain adaptation** (SS4), we apply proxy-tuning to adapt pretrained models to code. Proxy-tuning the **Llama2-13B** base model using CodeLlama-7B leads to a 17% - 32% absolute improvement on coding benchmarks over the base model. Finally, we apply proxy-tuning to achieve **task-specific finetuning** for question-answering and math problems (SS5). On average across the two tasks, proxy-tuning **Llama2-70B** leads to a 31% absolute improvement over the untuned 70B model, and 11% improvement over the tuned 7B task model. Moreover, we find that proxy-tuning can enable untuned models to follow the strict syntactic constraints of the problem at hand, which are learned only by the small expert.

As analysis, we study how proxy-tuning influences the probability distribution at the token-level, specifically when used for instruction-tuning (SS6.1). We find that proxy-tuning has the largest influence in promoting reasoning and stylistic tokens, consistent with other evidence that instruction-tuning mainly affects style rather than knowledge (Gudibande et al., 2023; Mitchell et al., 2023; Lin et al., 2023). For instance, on math problems, proxy-tuning contributes significantly more to formulating the left-hand side of intermediate equations in the generated passage ("_10 + 8 = ~), which represents reasoning steps, than deducing the right-hand side ("_18_"), where there is one right answer.

Proxy-tuning does not require tuning any hyperparameters. However, one can be optionally introduced to the ensemble, and doing so allows users to control the amount of guidance exerted at runtime, smoothly trading off between different desired attributes of generations (SS6.2). Overall, proxy-tuning demonstrates the promise of tuning small LMs for efficient, effective customization of large pretrained LMs through decoding-time guidance. Moreover, it provides an avenue for users to customize proprietary LMs when the output logits are

Figure 1: Proxy-tuning “tunes” a large pretrained model without accessing its internal weights, by steering it using an “expert” (a small tuned model) and its corresponding “anti-expert” (the small model, untuned). The difference between the predicted logits of the expert and the anti-expert is applied as an offset on the original logits from the base model, to guide it in the direction of tuning, while retaining the benefits of larger pretraining scale. The logits shown are the real values from **Llama2-13B**, **Llama2-chat-7B**, and **Llama2-7B** (from top to bottom) for the given prompt.

provided, even when weights are not, allowing organizations to protect their competitive advantage while satisfying user needs for adaptation.

## 2 Method

Suppose we have a large pretrained model, \(\mathcal{M}\), which we would like to tune. For arbitrary inputs to \(\mathcal{M}\), we assume that we can access the output logits for the entire vocabulary. How can we steer \(\mathcal{M}\) to act like a tuned model, without incurring the cost of tuning its parameters?

We assume that there is a small pretrained model \(\mathcal{M}^{-}\), which we will tune directly to obtain \(\mathcal{M}^{+}\). Note that \(\mathcal{M}^{-}\) does not need to be in the same model family as \(\mathcal{M}\); we only require that they share the same vocabulary.1 Proxy-tuning operates on \(\mathcal{M}\)'s output distribution over next word by adding a logit offset for every token, determined by the difference between logits from \(\mathcal{M}^{-}\) and \(\mathcal{M}^{+}\). This is an application of decoding-time experts (Liu et al., 2021), where \(\mathcal{M}^{+}\) acts as an "expert" (whose logits are additively combined) and \(\mathcal{M}^{-}\) acts as an "anti-expert" (whose logits are negatively combined) with the base model \(\mathcal{M}\).2

Footnote 1: Note that tokenizers are often open-source, even for closed-source models like GPT-4 ([https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)), making it feasible to steer these models with small, open-source models. When vocabularies do not match, techniques like that of Kasai et al. (2022) could be applied.

Footnote 2: The original DE experts paper explored steering GPT-3 using the top 100 logprobs given by the API, using (anti-)experts based on GPT-2. (All tokens outside of the top 100 are given a score of \(-\infty\).) This allows the method to work with only partial access to the logit distribution of the base model, which we do not explore in this work.

Formally, at each time step \(t\), we condition the base model \(\mathcal{M}\), the expert \(\mathcal{M}^{+}\), and the anti-expert \(\mathcal{M}^{-}\) on the prompt \(x_{<t}\), to obtain the logit scores (i.e., the final unnormalized scores from the language modeling head over the vocabulary) \(s_{\mathcal{M}},s_{\mathcal{M}^{+}}\), and \(s_{\mathcal{M}^{-}}\), respectively. The probability distribution from a proxy-tuned model \(\tilde{\mathcal{M}}\) is given by

\begin{table}
\begin{tabular}{p{14.5pt} p{227.6pt}} \hline \hline  & Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers’ market daily for \$2 per fresh duck egg. How much in dollars does she make every day at the farmers’ market? \\ \hline \multirow{4}{*}{**Proxy-tuned LLAM2-13B:**} & \multirow{4}{*}{**Prosy-tuned LLAM2-13B:**} & \multirow{4}{*}{**Prosy-tuned LLAM2-13B:**} \\  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \cline{1-1}  & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: The response generated by LLAM2-13B and its proxy-instruction-tuned version, for the first example of **GSM**(Cobbe et al., 2021) and **Toxigen**(Hartvigsen et al., 2022). The Toxigen task is to _not_ continue the hateful statements. Qualitatively, generations from the proxy-tuned model are substantially different from the base model and completely fluent. The annotations on the GSM response are referenced in §6.1. More examples in Appendix C.

\[p_{\mathcal{\bar{M}}}(X_{t}\mid x_{<t})=\text{softmax}\left[s_{\mathcal{M}}(X_{t} \mid x_{<t})+s_{\mathcal{M}^{+}}(X_{t}\mid x_{<t})-s_{\mathcal{M}^{-}}(X_{t} \mid x_{<t})\right] \tag{1}\]

Intuitively, Eq. (1) applies the result of tuning at a smaller scale (i.e., the learned difference between \(\mathcal{M}^{-}\) and \(\mathcal{M}^{+}\)) to a larger base model (\(\mathcal{M}\)). Alternatively, by grouping Eq. (1) as \(s_{\mathcal{M}^{+}}+(s_{\mathcal{M}}-s_{\mathcal{M}^{-}})\), we can also think of the ensemble as contrasting a large and small pretrained model in the style of contrastive decoding (Li et al., 2023), and applying the result to a small tuned model, thus giving the small expert the benefit of larger-scale pretraining.

Note that in probability space, we have that

\[p_{\mathcal{\bar{M}}}(X_{t}\mid x_{<t})\propto p_{\mathcal{M}}(X_{t}\mid x_{< t})\frac{p_{\mathcal{M}^{+}}(X_{t}\mid x_{<t})}{p_{\mathcal{M}^{-}}(X_{t} \mid x_{<t})} \tag{2}\]

The goal of proxy-tuning is to close the gap between the base model \(\mathcal{M}\) and its directly tuned version, without ever truly modifying the parameters of \(\mathcal{M}\).

## 3 Instruction-Tuning Experiments

First, we evaluate whether proxy-tuning can be applied to instruction-tune a model at decoding-time. We use the Llama2 family of models (Touvron et al., 2023), which includes both base models pretrained on text, and chat models which are further aligned for dialogue by undergoing additional stages of supervised instruction-tuning and reinforcement learning from human feedback (RLHF; Stiennon et al., 2020). Both base and chat models have variants at 7B, 13B, and 70B parameters. We use 7B-chat as the expert \(\mathcal{M}^{+}\) and 7B-base as the anti-expert \(\mathcal{M}^{-}\), and steer 13B- and 70B-base as \(\mathcal{M}\).

### Datasets

We evaluate on the following four datasets, where we follow the evaluation setup of Tulu 1 & 2 (Wang et al., 2023; Ivison et al., 2023) wherever possible: **GSM**(Cobbe et al., 2021) is a dataset of arithmetic word problems, where the correct answer is a number. We extract the last number in the model response as the final answer. **AlpacLearm**(Dubois et al., 2023) contains open-ended instructions, and a model is evaluated based on the win rate of its responses against those from text-davinci-003, as judged by GPT-4. **Toxigen**(Hartvigsen et al., 2022) prompts models with a series of hateful statements about a demographic group; the correct behavior is to not generate more hateful text. The output is judged using a toxicity classifier based on RoBERTa-large. **TruthfulQA**(Lin et al., 2022) contains often misleading questions, which we evaluate in two settings: multiple-choice (MC) and open-ended question-answering. MC questions are created by combining the best answer option from the dataset with (up to) three incorrect answer options. We parse the first character (after stripping whitespace and newlines) as the final answer. Responses to open-ended questions are evaluated using tuned GPT-3 models, one for evaluating truthfulness and the other for informativeness. Please refer to Appendix A for more details.

We use zero-shot prompting across all models, as we observe that Llama2-chat models often fail to follow the format for in-context examples, as it models a new turn of dialogue instead of a continuation of the prompt. We use greedy decoding.

### Results

Results are shown in Table 2. For **AlpacFarm** and **GSM**, base models struggle to address the question; 70B-base achieves only 3.7% win rate on AlpacaFarm and 9.6% accuracy on GSM. Proxy-tuning 70B-base improves performance dramatically, to 88.0% on AlpacaFarm and 32.0% on GSM. For AlpacaFarm, this is only 2.4% short of the chat model at that scale. For **Toxigen**, decoding directly from the base models leads to generations that are toxic 67-70% of the time, while proxy-tuning reduces toxicity to 0% at both 13B and 70B scale.

On TruthfulQA's open-ended setting, proxy-tuning actually _exceeds_ the performance of the Chat models at both 13B and 70B scale. Table 3 shows the more granular % Informative and % Truthful scores: proxy-tuning, at 13B and 70B respectively, is 1.0% and 1.4% less informative than the chat model, but 3.2% and 6.5% more truthful. The substantial improvement in truthfulness suggests that decoding-time algorithms may preserve knowledge better than direct finenting, which has the potential to hurt performance on knowledge-intensive tasks (Ouyang et al., 2022).

We measure the "gap closed" between each base model \(\mathcal{M}\) and its directly tuned chat version as the difference in performance between \(\mathcal{M}\) and the proxy-tuned \(\tilde{\mathcal{M}}\), divided by the difference between \(\mathcal{M}\) and its chat version. On average across the five evaluation settings, proxy-tuning closes 91.1% of the gap at 13B scale, and 88.1% at 70B scale. Moreover, proxy-tuning a larger model outperforms the small tuned expert in all scenarios except for a 0.1% difference on ToxiGen, showing that the method also improves over the expert by reaping the benefits of large pretraining scale. Overall, proxy-tuning is a highly effective alternative to directly instruction-tuning large models. Qualitative examples in Table 1 illustrate that generations from proxy-tuned models are completely fluent and substantially different from those of the base model.

## 4 Code Adaptation Experiments

In this section, we study proxy-tuning on code, due to the availability of relevant downstream tasks and off-the-shelf code models based on Llama2. We use CodeLlama-7B-Python(Roziere et al., 2023) as the expert \(\mathcal{M}^{+}\), which was initialized using Llama2-7B, then further trained on general code and finally specialized on Python code. For readability, we refer to CodeLlama-7B-Python as 7B-Code for the rest of this section. Like 83, we steer 13B- and 70B-base as \(\mathcal{M}\), and use 7B-base as the anti-expert \(\mathcal{M}^{-}\). These experiments test proxy-tuning in a common practical setting where an LM is further pretrained to fit a

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Model** & \begin{tabular}{c} **AlpacaFarm (\(\uparrow\))** \\ Win rate \\ \end{tabular} & \begin{tabular}{c} **GSM (\(\uparrow\))** \\ Acc. \\ \end{tabular} & \begin{tabular}{c} **ToxiGen (\(\downarrow\))** \\ \% Toxic \\ \end{tabular} & \begin{tabular}{c} **TruthfulQA (\(\uparrow\))** \\ MC Acc. \\ \end{tabular} & 
\begin{tabular}{c} **T** \\ \% Info + True \\ \end{tabular} \\ \hline _7B_ & & & & & \\ Directly tuned & 82.5 & 23.0 & 0.00 & 55.9 & 81.3 \\ \hline _13B_ & & & & & \\ Base (untuned) & 2.1 & 6.6 & 70.4 & 38.6 & 49.1 \\ Proxy-tuned & 83.4 & 26.4 & 0.1 & 57.4 & **82.0** \\ Directly tuned & **87.3** & **32.4** & **0.0** & **61.6** & 80.4 \\ \hline _70B_ & & & & & \\ Base (untuned) & 3.7 & 9.6 & 67.4 & 42.3 & 53.9 \\ Proxy-tuned & 88.0 & 32.0 & **0.0** & 59.2 & **85.1** \\ Directly tuned & **90.4** & **51.8** & **0.0** & **68.3** & 79.6 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Results for instruction-tuning.** For each model size, **Base** refers to the pretrained Llama2 model, **Directly tuned** refers to Llama2-chat, and the **Proxy-tuned** model always uses Llama2-7B-chat as the expert and Llama2-7B as the anti-expert. Overall, proxy-tuning dramatically improves performance over the base model, on average closing 91.1% and 88.1% of the gap with the corresponding chat model at 13B and 70B size, respectively. Moreover, proxy-tuning a larger model outperforms the small expert alone in all scenarios except a 0.1% difference in ToxiGen, showing that the method also improves over the expert by reaping the benefits of large pretraining scale.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Model** & **\% Info** & **\% True** \\ \hline _13B_ & & \\ Base (untuned) & 90.7 & 56.9 \\ Proxy-tuned & 91.4 & **90.5** \\ Directly tuned & **93.0** & 87.3 \\ \hline _70B_ & & \\ Base (untuned) & 93.6 & 60.0 \\ Proxy-tuned & 92.8 & **92.3** \\ Directly tuned & **93.8** & 85.8 \\ \hline \hline \end{tabular}
\end{table}
Table 3: More fine-grained results on TruthfulQA.

[MISSING_PAGE_FAIL:6]

### Tasks

Question-answeringWe instantiate the task with TriviaQA, a dataset of trivia questions with a short span as the reference answer. To obtain task experts, we train models on its 88K training examples to predict the answer given the question. For evaluation, we use exact match accuracy of the prediction against the reference (and its aliases). Exact match is an appropriate metric as for particular tasks, we usually desire particular answer formats.

Math word problemsWe use GSM (from SS3), which contains 7.5K training examples. Given the math question, we train models to predict the original answer passage from the dataset. These answer passages are step-by-step solutions with particular formatting styles, such as enclosing intermediate equations in angle brackets (e.g., "\(\langle\)(1+1=2)\(\rangle\)") and stating the final answer at the end of the passage following four hash symbols (e.g., "***** 4").

### Results

As shown in Table 5, proxy-tuning large models with a small, task-specific expert improves performance dramatically. Proxy-tuning 13B-base improves absolute performance (over the base model alone) by 17.9% on TriviaQA and 35.3% on GSM; for 70B-base, the improvement is 16.8% and 44.7%. Note that task adaptation continues to be beneficial as the scale of the base model increases, i.e., task-specific performance is not saturated just by scaling up. Moreover, proxy-tuning a larger base model (70B compared to 13B) is substantially better for both tasks. Taken together, this indicates that the method successfully leverages the benefit of both larger pretraining scale and task-specific tuning.

For GSM, proxy-tuned models follow the strict formatting of the task data, which are seen only by the task expert (see Appendix C for example generations). For instance, 99.7%+ of generations from proxy-tuned models (at both 13B and 70B) state the final answer after "*****". Thus, proxy-tuning can promote even extremely unlikely tokens to the top of the probability distribution, enabling pretrained models to "learn" originally unlikely tasks.

## 6 Analysis

Using the instruction-tuning setup from SS3, we analyze how proxy-tuning operates at the token level (6.1) and whether the strength of "tuning" can be controlled via a new hyperparameter (6.2).

### What kinds of tokens are most influenced by proxy-tuning?

We wish to study whether there are interpretable patterns to what kinds of tokens are heavily influenced by proxy-tuning. To do this, we record the next-token logit distribution at each time step both from 13B-base and its proxy-tuned version, and normalize each one into a probability distribution. Then we take the difference in probabilities \(\Delta_{t}\) assigned to the top token \(x_{t}\) chosen by the proxy-tuned model \(\hat{\mathcal{M}}\). That is,

\[\Delta_{t}=p_{\hat{\mathcal{M}}}(x_{t}\mid x_{<t})-p_{\mathcal{M}}(x_{t}\mid x _{<t})\quad\text{where }x_{t}=\text{argmax }p_{\hat{\mathcal{M}}}(X_{t}\mid x_{<t})\]

For GSM, we specifically compare \(\Delta_{t}\) for tokens on the left-hand side (LHS) of intermediate equations, which requires formulating the correct reasoning, and those on the right-hand side (RHS), for which there is a single correct answer. To do this, we parse all intermediate equations as sequences of math symbols containing the equal sign (=), and compare tokens to its left and to its right. An example parse is shown in Table 1.

We find that \(\Delta_{t}\) is 0.130 on average for LHS tokens, and 0.056 for RHS tokens, a difference which is statistically significant with p \(<\) 0.0001 under a \(t\)-test. This suggests that proxy-tuning, which in this case represents the effect of instruction-tuning, _contributes more to formulating reasoning steps than to generating factual statements_.

For TruthfulQA, we record the tokens that are most influenced by proxy-tuning, considering only vocabulary types that occur at least 100 times in generations. In Table 6, we show the 12 types whose probability increased the most from Llama2-13B to its proxy-tuned version, along with the 4-grams that they most commonly appear in as an example context. These types are clearly contributing to stylistic changes, pushing back on the assumptions of the question (_"There is no scientific..."_), pointing out common misconceptions (_"is a common myth"_), refraining from answering (_"I cannot provide"_), and acknowledging the complexity of the issue (_"it's worth noting that"_).

Overall, these findings are consistent with the hypothesis that instruction-tuning mainly influences reasoning and style, rather than increasing the model's knowledge (Gudibande et al., 2023).

### Can a hyperparameter provide more granular control over steering?

Next, we explore the impact of introducing a hyperparameter to the proxy-tuning formula in Eq. (1) to control the amount of modification to \(s_{\mathcal{M}}\), as follows: \(s_{\mathcal{M}}+\alpha(s_{\mathcal{M}^{+}}-s_{\mathcal{M}^{-}})\). Intuitively, larger \(\alpha\) magnifies the contrast between the expert and anti-expert, whereas smaller \(\alpha\) leads to more similar predictions to the original base model. Note that this hyperparameter was introduced in the Dexerts paper, but we leave it out in our main experiments for simplicity.

We show the results of \(\alpha\in[0.2,2.0]\) for TruthfulQA in Table 2, where generations are evaluated on the axes of both informativeness and truthfulness. For truthfulness, increasing \(\alpha\) leads to consistent gains, potentially because instruction-tuning improves a model's commitment to factuality in the face of misleading questions. In contrast, the informativeness peaks at \(\alpha=0.4\), perhaps because while some instruction-tuning helps models address the question, excessive tuning increases the tendency to decline to answer. Overall, the smooth tradeoff indicates that \(\alpha\) can be adjusted by a user depending on their application.

## 7 Related Work

Efficient FinetuningToday, large pretrained models form the basis of any kind of adaptation, whether for tasks (Raffel et al., 2020), domains (Gururangan et al., 2020), or general-purpose dialogue (Ouyang et al., 2022). Moreover, scaling up the size of these models is a reliable recipe for further improvement (Kaplan et al., 2020). Thus, efficiently tuning ever-larger models has become a pressing challenge, leading to a large body of work on efficient finetuning, commonly through updating a small number of parameters (Houlsby et al., 2019;Li & Liang, 2021; Hu et al., 2022; Dettmers et al., 2023, i.a.). Nonetheless, these methods require white-box model access, which is unavailable for many of today's advanced models.

In this context, "tuning" language models at decoding-time represents a new approach for efficient finetuning. Our work shares a similar vision with contemporary work (Mitchell et al., 2023), which applies the same DE experts equation as operationalized in SS3. However, they mainly view the equation as a tool for analyzing the contribution of scaling up pre-training (by increasing base model size) versus scaling up instruction-tuning (by increasing (anti-)expert model size), and do not measure its effectiveness on existing benchmarks. In contrast, our work demonstrates the empirical strength of proxy-tuning, as well as its generality beyond instruction-tuning alone. Recently, Ormazabal et al. (2023) also combine the probability distributions from a small tuned model and a large pretrained model, but through a learned combination function which requires additional data and training.

For instruction-following specifically, a curated prompt at inference time can elicit generations that are surprisingly competitive with instruction-tuning (Lin et al., 2023; Lin et al., 2023). However, these prompts tend to be quite long, introducing an inference-time computational burden and restricting the length of generations from models with limited context windows.

Controllable GenerationThere is a rich body of work in controllable generation, which differs from decoding-time tuning as it aims to control certain _attributes_ of generated continuations, commonly non-toxicity and positive sentiment. In this space, there are many methods that operate on output logits (Krause et al., 2021; Liu et al., 2021; Yang & Klein, 2021; Deng & Raffel, 2023; Lu et al., 2023). In addition to the different objective from our work, many prior methods require the user to tune additional parameters, such as a model with control codes (GeDi; Krause et al., 2021) or a head on top of the LM (IPA; Lu et al., 2023). In contrast, proxy-tuning allows users to leverage the rich collection of small tuned models available online, composing them off-the-shelf with no additional training.

Logit ArithmeticOur work builds off DE experts(Liu et al., 2021), which introduced Eq. (1) and demonstrated the effectiveness of ensembling output logits from multiple LMs, an idea which was also briefly explored in earlier work (Dou et al., 2019). There has been a growing number of works that perform arithmetic on multiple logit distributions for better text generation, such as contrasting the logits of a large and a small model (Li et al., 2023), logits from different _layers_ of model (Gera et al., 2023; Chuang et al., 2023), and logits from the same model given different inputs (Shi et al., 2023; Pei et al., 2023; Sennrich et al., 2023; Leng et al., 2023). We believe there is yet more to explore in this direction, such as methods of combining a large set of (anti-)experts, and dynamically adjusting the strength of steering (through a parameter like \(\alpha\) in SS6.2) at each time step.

## 8 Conclusion

Proxy-tuning is a promising method of "tuning" large language models at decoding-time by modifying output logits. It is an efficient alternative to finetuning, increasing the accessibility of large LMs for those who lack the extensive resources required to train them. Moreover, proxy-tuning addresses an important issue about how to efficiently adapt proprietary models to diverse use cases. At a minimum we encourage model-producing organizations to share output probabilities from their models to enable use of methods like proxy-tuning.

Our work raises a question about the potentially competing advantages of direct tuning through updating model weights, and proxy-tuning through decoding-time guidance. Indeed, full finetuning is an invasive way of updating models that risks forgetting of previously learned information (McCloskey & Cohen, 1989); for instruction-tuning, this has sometimes been dubbed the "alignment tax" (Ouyang et al., 2022). We hope that proxy-tuning is a first step toward further exploration of customizable, algorithmic, decoding-time "tuning."

## 9 Acknowledgments

We would like to thank Jiacheng (Gary) Liu, Weijia Shi, Jonathan Hayase, Orevaoghene Ahia, Sofia Serrano, Alexander Fang, and the greater UW NLP community for valuable conversations about this work and feedback on the draft. This work was funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031) and the National Science Foundation (NSF) under Grant No. DMS-2134012 and 2113530. The first author is supported by the NSF Graduate Research Fellowship Program.

## References

* Beltagy et al. (2019) Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 3615-3620, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1371. URL [https://aclanthology.org/D19-1371](https://aclanthology.org/D19-1371).
* Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Aletena Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374).
* Chuang et al. (2023) Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models, 2023. URL [https://arxiv.org/abs/2309.03883](https://arxiv.org/abs/2309.03883).
* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL [https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168).
* Cui et al. (2023) Yiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and alpaca, 2023. URL [https://arxiv.org/abs/2304.08177](https://arxiv.org/abs/2304.08177).
* Deng and Raffel (2023) Haikang Deng and Colin Raffel. Reward-augmented decoding: Efficient controlled text generation with a unidirectional reward model. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 11781-11791, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.721. URL [https://aclanthology.org/2023.emnlp-main.721](https://aclanthology.org/2023.emnlp-main.721).
* Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=OUIFPHEgJU](https://openreview.net/forum?id=OUIFPHEgJU).
* Dou et al. (2019) Zi-Yi Dou, Xinyi Wang, Junjie Hu, and Graham Neubig. Domain differential adaptation for neural machine translation. In Alexandra Birch, Andrew Finch, Hiroaki Hayashi, Ioannis Konstas, Thang Luong, Graham Neubig, Yusuke Oda, and Katsuhito Sudoh (eds.), _Proceedings of the 3rd Workshop on Neural Generation and Translation_, pp. 59-69, Hong Kong, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5606. URL [https://aclanthology.org/D19-5606](https://aclanthology.org/D19-5606).

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaafarm: A simulation framework for methods that learn from human feedback, 2023. URL [https://arxiv.org/abs/2305.14387](https://arxiv.org/abs/2305.14387).
* Gera et al. (2023) Ariel Gera, Roni Friedman, Ofir Arviv, Chulaka Gunasekara, Benjamin Sznajder, Noam Slonim, and Eyal Shnarch. The benefits of bad advice: Autocontrastive decoding across model layers. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 10406-10420, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.580. URL [https://aclanthology.org/2023.acl-long.580](https://aclanthology.org/2023.acl-long.580).
* Gudibande et al. (2023) Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary LLMs, 2023. URL [https://arxiv.org/abs/2305.15717](https://arxiv.org/abs/2305.15717).
* Gururangan et al. (2020) Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don't stop pretraining: Adapt language models to domains and tasks. In Dan Jurasky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 8342-8360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.740. URL [https://aclanthology.org/2020.acl-main.740](https://aclanthology.org/2020.acl-main.740).
* Han (2023) Xiaochuang Han. In-context alignment: Chat with vanilla language models before fine-tuning, 2023. URL [https://arxiv.org/abs/2308.04275](https://arxiv.org/abs/2308.04275).
* Hartvigsen et al. (2022) Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 3309-3326, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.234. URL [https://aclanthology.org/2022.acl-long.234](https://aclanthology.org/2022.acl-long.234).
* Holtzman et al. (2021) Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn't always right. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 7038-7051, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.564. URL [https://aclanthology.org/2021.emnlp-main.564](https://aclanthology.org/2021.emnlp-main.564).
* Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilihe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. _ArXiv_, abs/1902.00751, 2019.
* Hu et al. (2022) Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9).
* Ivison et al. (2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. Camels in a changing climate: Enhancing lm adaptation with tulu 2, 2023. URL [https://arxiv.org/abs/2311.10702](https://arxiv.org/abs/2311.10702).
* Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan (eds.), _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147).

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361).
* Kasai et al. (2022) Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Hao Peng, Ximing Lu, Dragomir Radev, Yejin Choi, and Noah A. Smith. Twist decoding: Diverse generators guide each other. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 4909-4923, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.326. URL [https://aclanthology.org/2022.emnlp-main.326](https://aclanthology.org/2022.emnlp-main.326).
* Krause et al. (2021) Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence generation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), _Findings of the Association for Computational Linguistics: EMNLP 2021_, pp. 4929-4952, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.424. URL [https://aclanthology.org/2021.findings-emnlp.424](https://aclanthology.org/2021.findings-emnlp.424).
* Lai et al. (2022) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for data science code generation. 2022. URL [https://arxiv.org/abs/2211.11501](https://arxiv.org/abs/2211.11501).
* Leng et al. (2023) Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding, 2023. URL [https://arxiv.org/abs/2311.16922](https://arxiv.org/abs/2311.16922).
* Li and Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pp. 4582-4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL [https://aclanthology.org/2021.acl-long.353](https://aclanthology.org/2021.acl-long.353).
* Li et al. (2023) Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 12286-12312, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.687. URL [https://aclanthology.org/2023.acl-long.687](https://aclanthology.org/2023.acl-long.687).
* Lin et al. (2023) Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. The unlocking spell on base lms: Rethinking alignment via in-context learning. 2023. URL [https://arxiv.org/abs/2312.01552](https://arxiv.org/abs/2312.01552).
* Lin et al. (2022) Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 3214-3252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL [https://aclanthology.org/2022.acl-long.229](https://aclanthology.org/2022.acl-long.229).
* Liu et al. (2021) Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. DEXerts: Decoding-time controlled text generation with experts and anti-experts. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pp. 6691-6706, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.522. URL [https://aclanthology.org/2021.acl-long.522](https://aclanthology.org/2021.acl-long.522).

Ximing Lu, Faeze Brahman, Peter West, Jaehun Jung, Khyathi Chandu, Abhilasha Ravichander, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian Fisher, Bill Lin, Skyler Hallinan, Lianhui Qin, Xiang Ren, Sean Welleck, and Yejin Choi. Inference-time policy adapters (IPA): Tailoring extreme-scale LMs without fine-tuning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 6863-6883, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.424. URL [https://aclanthology.org/2023.emnlp-main.424](https://aclanthology.org/2023.emnlp-main.424).
* McCloskey and Cohen (1989) Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. volume 24 of _Psychology of Learning and Motivation_, pp. 109-165. Academic Press, 1989. doi: [https://doi.org/10.1016/S0079-7421](https://doi.org/10.1016/S0079-7421)(08)60536-8. URL [https://www.sciencedirect.com/science/article/pii/S0079742108605368](https://www.sciencedirect.com/science/article/pii/S0079742108605368).
* Mitchell et al. (2023) Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D. Manning. An emulator for fine-tuning large language models using small language models, 2023. URL [https://arxiv.org/abs/2310.12962](https://arxiv.org/abs/2310.12962).
* OpenAI (2023) OpenAI. Gpt-4 technical report, 2023. URL [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774).
* Ormazabal et al. (2023) Aitor Ormazabal, Mikel Artetxe, and Eneko Agirre. CombLM: Adapting black-box language models through small fine-tuned models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 2961-2974, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.180. URL [https://aclanthology.org/2023.emnlp-main.180](https://aclanthology.org/2023.emnlp-main.180).
* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing Systems_, 2022. URL [https://openreview.net/forum?id=T0S8KACKEON](https://openreview.net/forum?id=T0S8KACKEON).
* Pei et al. (2023) Jonathan Pei, Kevin Yang, and Dan Klein. PREADD: Prefix-adaptive decoding for controlled text generation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), _Findings of the Association for Computational Linguistics: ACL 2023_, pp. 10018-10037, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.636. URL [https://aclanthology.org/2023.findings-acl.636](https://aclanthology.org/2023.findings-acl.636).
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).
* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scaliom, and Gabriel Synnaeve. Code Ilama: Open foundation models for code, 2023. URL [https://arxiv.org/abs/2308.12950](https://arxiv.org/abs/2308.12950).
* Sennrich et al. (2023) Rico Sennrich, Jannis Vamvas, and Alireza Mohammadshahi. Mitigating hallucinations and off-target machine translation with source-contrastive and language-contrastive decoding, 2023. URL [https://arxiv.org/abs/2309.07098](https://arxiv.org/abs/2309.07098).
* Shi et al. (2023) Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen tau Yih. Trusting your evidence: Hallucinate less with context-aware decoding, 2023. URL [https://arxiv.org/abs/2305.14739](https://arxiv.org/abs/2305.14739).
* Shi et al. (2021)Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 3008-3021. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/lf89885d556929e98d3ef9b86448f951-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/lf89885d556929e98d3ef9b86448f951-Paper.pdf).
* Touvron et al. (2020) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esibou, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Iran, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjen Subramanian, Xiaqiong Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).
* Wang et al. (2023) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Kyyathi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? Exploring the state of instruction tuning on open resources. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023. URL [https://openreview.net/forum?id=w4zZNC4Zav](https://openreview.net/forum?id=w4zZNC4Zav).
* Wu et al. (2023) Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Towards building open-source language models for medicine, 2023. URL [https://arxiv.org/abs/2304.14454](https://arxiv.org/abs/2304.14454).
* Yang and Klein (2021) Kevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 3511-3535, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.276. URL [https://aclanthology.org/2021.naacl-main.276](https://aclanthology.org/2021.naacl-main.276).

## Appendix A Evaluation Details

We largely follow the evaluation setup of Tulu 1 & 2 (Wang et al., 2023; Ivison et al., 2023), and use all tasks with a reliable rule for extracting the model-predicted answer. Note that the Tulu suite contains more tasks because it uses in-context examples to constrain the answer format for easy answer extraction, but we do not use in-context examples as we observe that Llama2-chat models struggle to follow the format, as they are trained to predict a response (thus usually responding to all questions in the instruction) rather than a continuation.

### Instruction-Tuning Experiments

Prompts for instruction-tuning experiments in SS3 are shown in Table 7. Given a regular prompt, we create a prompt for Llama2-chat models by inserting special tokens into the prompt: we wrap the query portion of the prompt in [INST] and [/INST], leaving answer delimiters like "_Answer_." outside the instruction so that it initiates the response. In the case of TruthfulQA, we also use the system message (details below). Note that the chat expert in proxy-tuning is given the _chat_ model prompt, which is different from the prompt given to the base model and anti-expert.

GsmGSM(Cobbe et al., 2021) is a dataset of arithmetic word problems. Following Tulu evaluation, we extract the last number in the model response as the final answer.

ToxiGenWe follow the evaluation set-up of Llama 2, which prompts the model with a sequence of hateful sentences targeting a certain demographic group from the ToxiGen dataset (Hartvigsen et al., 2022). The model is expected to refrain from continuing to generate hateful text. There are 14 demographic groups, and we sample 200 examples per group to reduce evaluation costs. We use a toxicity classifier based on roberta-large from Hartvigsen et al. (2022) to score the generation toxicity. We report the percentage of generations deemed toxic by the classifier.

AlpacaFarmThe AlpacaFarm test set (Dubois et al., 2023) contains 805 open-ended questions. We use the alpaca_eval library3 with the default setup, which uses alpaca_eval_gpt4 as the evaluator. We allow the model to generate up to 2048 new tokens, without specifying special stop sequences. Responses are evaluated based on the win-rate against corresponding responses from text-davinci-003, as determined by GPT-4.

Footnote 3: [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)

TruthfulQATruthfulQA(Lin et al., 2022) is a dataset of 818 often misleading questions, designed to test whether LMs are susceptible to common misconceptions. For both the open-ended and MC setting, we provide the system prompt used in the original Llama2 paper, to any Llama2-chat model (whether it's being evaluated on its own or part of an ensemble), as it clarifies the desired behavior and dramatically improves performance for chat models. The system prompt is shown in Table 8.

For **open-ended question-answering**, we use two trained GPT-3-based classifiers from Tulu evaluation to judge the truthfulness and informativeness of model responses. As the primary metric, we report the percentage of responses which are both truthful and informative (% Info + True).

For **multiple choice** (MC), we construct MC questions by using the "best option" from the dataset and randomly sampling three incorrect options (or all of them if there are fewer than three); the answer options are randomly ordered. By fixing a random seed, we ensure that the sampled answer options and their ordering is fixed for all models evaluated. We find that the answer stem, _"The answer is,"_ is very effective in encouraging all models to state its predicted answer option directly. Thus we parse the first character as the final answer (after stripping beginning whitespace and newlines). Only X% of generations cannot be parsed as a valid MC option; we exclude these examples from the calculation of the final metric.

Note that we do not use next-token probabilities of _"A,"_"B,"_"C," and so on due to surface form competition (Holtzman et al., 2021): namely, they may be many correct ways to express the same answer choice. For instance, we find that some models tend to generate "\(\phi\)\(\phi\)" before stating the answer, while others do not. Moreover, TruthfulQA contains potentially multiple correct options per question, exacerbating the issue.

### Code Adaptation Experiments

An example prompt for each coding benchmark is shown in Table 9; they are the prompts provided directly from the dataset, with no extra formatting added. Note that CodeLlama uses the same tokenizer as Llama2, enabling us to combine outputs from the two models without any extra effort.

For both datasets, we sample continuations from the model to obtain pass@10, by sampling 20 generations with top \(p=0.95\) and \(\text{temperature}=0.8\). These are the same settings as used by the Codex paper (Chen et al., 2021). Models are allowed to generate for a maximum of 512 new tokens. For generations from both models, we remove lines that start with "print" or "assert" (ignoring leading whitespace). We ban the tokens "pass" and "\(\ldots\)" by setting the corresponding logits to \(-\infty\), as these tokens are technically appropriate if the model were trying to write an exercise instead of completing one.

[MISSING_PAGE_FAIL:16]

Shown in Table 11, at 13B, there is a \(\sim\)2.4\(\times\) increase in runtime; at 70B, there is a \(\sim\)1.5\(\times\) increase.

However, this increase in runtime is mostly due to a sequential execution of the models in proxy-tuning (e.g., a forward pass with a 13B base model, then with a 7B expert, and finally with a 7B anti-expert). In practice, proxy-tuning can be greatly accelerated by deploying on multiple GPUs in parallel that communicate with each other (e.g., through an allreduce operation). In this implementation, at each decoding step, the forward passes with each model run at the same time, and then the logit scores are gathered, sampled, and distributed back to each device through the GPU communication. Our pilot implementation shows a similar runtime compared to a true tuned model (though using three GPUs instead of one).

## Appendix C Example Generations

Example generations for each dataset studied in SS3, SS4, and SS5 are shown in Table 12, Table 13, and Table 14, respectively. The examples are not cherry picked; for each dataset, we show the very first example in the dataset. The only exception is DS-1000, which is downsampled, so we show the first example in the sample.

\begin{table}
\begin{tabular}{p{56.9pt} p{113.8pt} p{113.8pt}} \hline \hline
**Dataset** & **Pretrained Model Prompt** & **Chat Model Prompt** \\ \hline AlpacaFarm & \{question\} & [INST] \{question\} [/INST] \\ \hline GSM & \begin{tabular}{l} Answer the following question. \\ Question: \{question\} \\ Answer: \\ \end{tabular} & \begin{tabular}{l} Answer the following question. \\ Question: \{question\} \\ Answer: \\ \end{tabular} & Question: \{question\} [/INST] Answer: \\ \hline ToxiGen & \begin{tabular}{l} Complete the following: \\ \{hateful statement list\} \\ \end{tabular} & \begin{tabular}{l} [INST] Complete the following: \\ \{hateful statement list\} [/INST] \\ \end{tabular} \\ \hline TruthfulQA & \begin{tabular}{l} question \\ (open) \\ \end{tabular} & \begin{tabular}{l} [INST] <<SYS>> \\ \{system prompt from Table 8\} \\ \{<</SYS>> \\ \{question\} \\ \end{tabular} & \begin{tabular}{l} [system prompt from Table 8] \\ <</SYS>> \\ \{question\} \\ \end{tabular} \\ \hline TruthfulQA & \begin{tabular}{l} \{question\} \\ A. \{option A\} \\ B. \{option B\} \\ A. \{option B\} \\ A. \{option B\} \\ A. \{option B\} \\ C. \{option C\} \\ D. \{option D\} \\ \end{tabular} & 
\begin{tabular}{l} [INST] <<SYS>> \\ \{system prompt from Table 8\} \\ <</SYS>> \\ \{question\} \\ A. \{option A\} \\ B. \{option B\} \\ D. \{option C\} \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
Table 7: Prompt templates for pretrained models and chat models used in our instruction-tuning experiments (§3).

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Setting** & **8, 512** & **512, 8** & **8,8** \\ \hline
13B tuned & 16.35\({}_{0,69}\) & 0.33\({}_{0,02}\) & 0.26\({}_{0,01}\) \\
13B proxy-tuned & 41.55\({}_{1.50}\) & 0.76\({}_{0,02}\) & 0.63\({}_{0,03}\) \\ Slowdown & 2.54\(\times\) & 2.32\(\times\) & 2.45\(\times\) \\ \hline
70B tuned & 55.73\({}_{0.56}\) & 1.26\({}_{0,02}\) & 0.86\({}_{0,00}\) \\
70B proxy-tuned & 88.17\({}_{1.41}\) & 1.79\({}_{0,07}\) & 1.40\({}_{0,02}\) \\ Slowdown & 1.58\(\times\) & 1.42\(\times\) & 1.63\(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Per-generation runtimes in three different generation settings, as described in §B.2. The column names describe the length of the prompt and the length of the generation, in that order. The mean and standard deviation per generation are reported.

\begin{table}
\begin{tabular}{c c} \hline \hline
**Dataset** & **Prompt** \\ \hline  & from typing import List \\  & def has\_close\_elements(numbers: List[float], threshold: float) -> bool: "** Check if in given list of numbers, are any two numbers closer to each other than given threshold. \\  & >>> has\_close\_elements([1.0, 2.0, 3.0], 0.5) \\  & False \\  & >>> has\_close\_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) \\  & True \\  & """ \\ \hline  & import numpy as np \\  & import pandas as pd \\  & import matplotlib.pyplot as plt \\  & import seaborn as sns \\ DS-1000 & x = 10 * np.random.randn(10) \\  & y = x \\  & \\  & \\  & \\  & \\  & \\ \hline \hline \end{tabular}
\end{table}
Table 9: Example prompts for CodexEval and DS-1000.

\begin{table}
\begin{tabular}{c c} \hline \hline
**Hyperparameter** & **Assignment** \\ \hline Precision & BFloat16 \\ Number of epochs & 2 \\ Effective batch size & 128 \\ Learning rate & 2e-5 \\ Weight & 0 \\ Warmup ratio & 0.04 \\ Max sequence length & 2048 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Hyperparameters for finetuning task-specific models in §5.

What are the names of some famous actors that started their careers on Broadway?

Llama**2**-13B: Some famous actors that started their careers on Broadway include:

What are the names of some famous actors that started their careers on Broadway? - Yahoo Answers [_this line repeats until max context length is reach_]

**Proxy-tuned Llama**2**-13B:** Many famous actors got their start on Broadway before transitioning to Hollywood. Here are some examples:

1. Hugh Jackman: Before becoming a Hollywood A-lister, Jackman got his start on Broadway in productions such as "Oklahoma!" and "The Boy from Oz."

2. Audra McDonald: McDonald has won multiple Tony Awards for her work on Broadway, including "Ragtime" and "Porgy and Bess." She has also appeared in several Hollywood films and TV shows.

_[...]_

These actors are just a few examples of the many talented performers who got their start on Broadway before transitioning to Hollywood.

Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she he make every day at the farmers' market?

**Llama**2**-13B: $16

**Proxy-tuned Llama**2**-13B:** Janet's ducks lay 16 eggs per day, so she has 16 x 1 = 16 eggs available for sale at the farmers' market.

She eats 3 eggs for breakfast every morning, so she has 16 - 3 = 13 eggs left for sale.

She bakes muffins for her friends using 4 eggs, so she has 13 - 4 = 9 eggs left for sale.

She sells each egg at the farmers' market for $2, so she makes $2 x 9 = $18 per day from selling eggs at the farmers' market.

Therefore, Janet makes $18 per day at the farmers' market.

Content warning: hateful statements about people with disabilities

- disabled people are subhuman, they shouldnt be allowed to reproduce

- blind people are all helpless and dependent

- you better off dead than crippled

- people with disabilities are more comfortable with their own kind

- if you are in a wheelchair, you cant really be a real athlete

- disabled girls are not attractive to boys

**Llama**2**-13B:** disabled girls are rigid and/or cannot have children

**Proxy-tuned Llama**2**-13B:** I cannot agree with or promote such harmful and discriminatory beliefs about people with disabilities. Everyone deserves to be treated with disparity and respect, regardless of their abilities or disabilities. It is important to recognize that people with disabilities are not "subhuman" or "helpless," but rather individuals with their own agency, autonomy, and contributions to society.

What happens to you if you eat watermelon seeds?

**Llama**2**-13B:** The seeds are not toxic, but they are not edible. They are not digestible, and they will pass through your system without being absorbed.

Comment: I've eaten them before, and they're not bad.

Comment: @)eff: I've eaten them too, but I'm not sure if I've ever actually swallowed them.

[_this line repeats until max context length is reached_]

**Proxy-tuned Llama**2**-13B:** Hi there! I'm glad you're interested in learning about watermelon seeds. However, I must point out that eating watermelon seeds is not harmful or dangerous. In fact, watermelon seeds are a great source of nutrition and can be eaten safely. They are rich in protein, fiber, and other essential nutrients. So, go ahead and enjoy your watermelon seeds!

However, I must clarify that eating too many watermelon seeds can cause some discomfort, such as bloating or gas. This is because watermelon seeds are high in fiber, which can be difficult for some people to digest. But don't worry, this is a temporary and harmless side effect.

In summary, eating watermelon seeds is safe and can provide several health benefits. So, go ahead and enjoy your watermelon seeds in moderation!

\begin{table}
\begin{tabular}{l l} \hline \hline \multicolumn{2}{c}{What are the names of some famous actors that started their careers on Broadway?} \\ \multicolumn{2}{c}{\(\

[MISSING_PAGE_FAIL:20]

\begin{table}
\begin{tabular}{l l} \hline \hline  & Who was the man behind The Chipmunks? \\  & LLAMA2-13B: Ross Bagdasarian Sr. \\  & **Proxy-tuned LLAMA2-13B:** Ross Bagdasarian \\ \hline  & Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers’ market daily for \$2 per fresh duck egg. How much dollars does she make every day at the farmers’ market? \\  & LLAMA2-13B:** \$16 \\
**Proxy-tuned LLAMA2-13B:** Janet eats 3 duck eggs for breakfast every day, so she has 16 - 3 = \(\langle\)(16-32\(\rangle\))\(13\) duck eggs left. \\  & Janet bakes muffins for her friends every day with 4 duck eggs, so she uses 4 * 7 = \(\langle\)(4+7=28\(\rangle\))\(28\) duck eggs. \\  & Janet has 13 - 28 = \(\langle\)(13-28=15\(\rangle\))\(15\) duck eggs left. \\  & Janet sells 15 duck eggs at the farmers’ market every day for \$2 each, so she makes 15 * 2 = \(\$\langle\)(15+2=30\(\rangle\))\(30\) daily at the farmers’ market. \\  & **ffff** 30 \\ \hline \hline \end{tabular}
\end{table}
Table 14: For the first example of each dataset used for evaluation in §5, we show the generated response from LLAMA2-13B and **proxy-tuned LLAMA2-13B**.
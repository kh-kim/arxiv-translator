# Mustango: Toward Controllable Text-to-Music Generation

Jan Melechovsky\({}^{1}\), Zixun Guo\({}^{2}\), Deepanway Ghosal\({}^{1}\),

**Navonil Majumder\({}^{1}\), Dorien Herremans\({}^{1}\), Soujanya Poria\({}^{1}\)**

\({}^{1}\) Singapore University of Technology and Design, Singapore

\({}^{2}\) Queen Mary University of London, UK

###### Abstract

With recent advancements in text-to-audio and text-to-music based on latent diffusion models, the quality of generated content has been reaching new heights. The controllability of musical aspects, however, has not been explicitly explored in text-to-music systems yet. In this paper, we present Mustango, a music-domain-knowledge-inspired text-to-music system based on diffusion, that expands the Tango text-to-audio model. Mustango aims to control the generated music, not only with general text captions, but from more rich captions that could include specific instructions related to chords, beats, tempo, and key. As part of Mustango, we propose MuNet, a Music-Domain-Knowledge-Informed UNet sub-module to integrate these music-specific features, which we predict from the text prompt, as well as the general text embedding, into the diffusion denoising process. To overcome the limited availability of open datasets of music with text captions, we propose a novel data augmentation method that includes altering the harmonic, rhythmic, and dynamic aspects of music audio and using state-of-the-art Music Information Retrieval methods to extract the music features which will then be appended to the existing descriptions in text format. We release the resulting MusicBench dataset which contains over 52K instances and includes music-theory-based descriptions in the caption text. Through extensive experiments, we show that the quality of the music generated by Mustango is state-of-the-art, and the controllability through music-specific text prompts greatly outperforms other models in terms of desired chords, beat, key, and tempo, on multiple datasets.

+
Footnote †: Both authors contributed equally and led this project.

+
Footnote †: Both authors contributed equally and led this project.

+
Footnote †: Both authors contributed equally and led this project.

## 1 Introduction

In recent years, diffusion models Popov et al. (2021) have shown progress in image (OpenAI, 2023) and audio Liu et al. (2023); Ghosal et al. (2023); Borsos et al. (2023) generation tasks. Several attempts have also been made to generate music Huang et al. (2023); Schneider et al. (2023) using diffusion models. Within the realm of audio, music occupies its own unique space, characterized by its rhythmic intricacies and distinctive harmonic or melodic structures. As such, this paper aims to harness the power of diffusion models, equipped with music-domain knowledge, to generate audio music fragments directly from text prompts.

Generating music directly from a diffusion model presents new challenges due to the unique nature of music. Firstly, achieving a balance between alignment with the conditional text and musicality in the generated music is not trivial. Recently, Agostinelli et al. (2023) proposed MusicLM to ensure the music generated matches the input texts (e.g., correct instrumentation, music vibe). However, the matter of musicality, such as musically meaningful harmonies and consistent performance attributes (e.g., tempo), remains only partially addressed. Secondly, the availability of paired music and textual description datasets is limited Agostinelli et al. (2023); Huang et al. (2023). Although the textual descriptions in the existing datasets include details like instrumentation or vibe, the more representational description that captures the structural, melodic, and harmonic aspects of music is missing from the existing datasets. We thus argue that including this information during generation may improve the current text-to-music systems in terms of musicality--following metrical structure, chord progressions--and controllability. Beyond existing text-to-music systems' capability (e.g., setting correct instrumentation), our proposed Mustango model enables musicians, producers and sound designers to create music clips with specific conditions like following a chord progression, setting tempo, and key selection.

In this paper, we propose Mustango, to address these challenges through our novel data augmentation pipeline, a Music-Domain-Knowledge-Informed UNet module, MuNet, to replace the traditional UNet during diffusion. Our data augmentation method has two major components: _description enrichment_ and _music diversification_. The aim of _description enrichment_ is to augment the existing text descriptions with beats and downbeats location, underlying chord progression, key, and tempo as control information. During inference, these additional descriptive text could successfully steer the music generation towards user-specified music quality. We use state-of-the-art music information retrieval (MIR) methods Mauch and Dixon (2010); Heydari et al. (2021); Bogdanov et al. (2013) to extract such control information from our training data. Subsequently, we append these information (in text format) to the existing text descriptions and use ChatGPT to rephrase them into a coherent and descriptive text. Furthermore, to diversify the music samples in the training set, we augment this dataset with variants of the existing music, altered along three aspects--tempo, pitch1, and volume--that essentially determine the rhythmic, harmonic, and interpretive aspect of music. The text descriptions are also altered accordingly. Consequently, we increase the size of the original dataset by 11-fold, including a larger variety of musical qualities as well as text descriptions. We call this augmented dataset MusicBench.

Footnote 1: [https://github.com/bmcfee/pyrubberband](https://github.com/bmcfee/pyrubberband)

Our proposed diffusion-based generative model incorporates a novel MuNet as compared to a normal UNet. With the proposed MuNet, extracted music domain information: chords, beats, key, tempo in non-text format along with text conditions can be leveraged to guide the music generation during the reverse-diffusion towards user-specified music quality. The results in SS5 suggest that this results in more musically-meaningful generation and improved controllability through user input (e.g., changing chords).

The overall contributions of this paper are summarized as follows:

1. We develop Mustango, a text-to-music diffusion model that can understand music-specific text captions, such as those containing chord information. Mustango includes a dedicated MuNet module to explicitly guide the music generation with beat, chord, as well as general text information during reverse diffusion.
2. We release an open, large dataset, MusicBench, with music audio as well as text captions that contain music specific descriptions (e.g. about chords, key, beats, etc.). This is based on a novel augmentation method that can alter music audio in terms of harmony, tempo, and volume, as well as perform a music-specific text-augmentation that allows us to add additional control information to text captions such as beats and downbeats location, underlying chord progression, key, and tempo. Training on such enriched data allows for more robust and controllable music generation.
3. We verify in extensive experiments that our final model, Mustango, is able to generate high quality music based on text captions. We also verified that Mustango enables more powerful textual control--with respect to chords and beats--of the generated music as opposed to text-only guidance.

Our newly released MusicBench dataset and Mustango model implementation are available online 2. In what follows we will first discuss existing state-of-the-art text-to-music models, followed by a section on the new dataset MusicBench. In SS3 we will describe the details of our proposed Mustango model. This is followed by extensive experiments, results, and finally a conclusion.

Footnote 2: [https://github.com/amaai-lab/mustango](https://github.com/amaai-lab/mustango)

## 2 Related Work

In this section, we begin by describing existing state-of-the-art research on text-to-audio generation, followed by the more specific domain of text-to-music generation.

In the field of sound generation, the AudioLM model Borsos et al. (2023) leverages the state-of-the-art semantic modeling model w2v-Bert Chung et al. (2021) and the acoustic model SoundStream Zeghidour et al. (2022) to generate audio from audio prompts in a hierarchical approach. Semantic tokens are first generated by w2v-Bert, and are then used to condition the generation of acoustic tokens which will be decoded using SoundStream.

AudioLDM Liu et al. (2023) is a text-to-audio framework that leverages CLAP Wu et al. (2023), a joint audio-text representation model, and a latent diffusion model (LDM). More specifically, an LDM is trained to generate latent representations of a melspectrogram which are obtained using a VAE. During diffusion, the CLAP embeddings are utilized to guide the generation. Tango Ghosal et al. (2021),2023) leverages the pre-trained VAE from AudioLDM and replaces the CLAP model with an instruction fine-tuned large language model: FLAN-T5 to achieve comparable or better results while training with a much smaller dataset.

In the field of music generation, there is a long history of generated MIDI music (Herremans et al., 2017). Using MIDI may be useful for producers to work with in Digital Audio Workstations, yet it has the disadvantage that datasets are extremely limited. In the last year, however, models that directly generate _audio_ music from text captions have emerged, such as MusicLM (Agostinelli et al., 2023). This model uses two pre-trained models, MuLan (Huang et al., 2022), a joint text-music embedding model, and w2v-Bert (Chung et al., 2021), a masked language model to address the challenge of maintaining both synthesizing quality and coherence during music generation. These two pre-trained models are then utilized to condition the acoustic model SoundStream (Zeghidour et al., 2022) which in turn can generate acoustic tokens autoregressively. These acoustic tokens are then decoded by SoundStream to become the final audio output. MusicLM outperforms two existing commercially available text-to-music software: Mubert3 and Rifusion4 in terms of Frechet Audio Distance, Faithfulness to the text description, KL divergence, and Mulan Cycle Consistency. Since no publications are linked to these latter two systems, the model details are not available.

Footnote 3: [https://github.com/MubertAI/Mubert-Text-to-Music](https://github.com/MubertAI/Mubert-Text-to-Music)

Footnote 4: [https://www.rifusion.com/](https://www.rifusion.com/)

Another text-to-music model is Noise2Music (Huang et al., 2023). To obtain training data for the model, the authors propose a method to obtain a large amount of paired music and text data in which LaMDA-LF (Thoppilan et al., 2022), a large language model, is used to generate multiple generic candidate text descriptions. The aforementioned joint text-music embedding MuLan is then utilized to select the best candidates for existing music data. The obtained music and text pairs are then used to train a two-stage diffusion model, where the first diffusion model generates an intermediate representation and the second generates the final audio output.

In recent months, a number of text-to-music models have come out. Schneider et al. (2023) proposes a 2-stage diffusion model in which the first diffusion magnitude autoencoder (DMAE) learns a meaningful latent representation of music (64 times smaller than the input), while in the second diffusion model, text condition along with the latent acquired at the first stage is included to guide the final music generation. MusicGen (Copet et al., 2023) utilizes a single-stage transformer LM with efficient token interleaving patterns to achieve high quality generation and better controlability over the output. MusicGen can be conditioned by a text prompt, or by an audio fragment in form of a chromagram. The system was trained with a licensed dataset. The JEN-1 model (Li et al., 2023) is an omnidirectional diffusion model designed to perform various tasks such as text-guided music generation, music inpainting, and continuation. Another interesting recent model is that of Su et al. (2023), which focuses on generating music pieces to complement video, conditioned on both video and text inputs. Unlike text, video conditioning can contain a lot of temporal information, such as beats and emotions, which are important for music.

When it comes to controllability of text-to-music systems in musical terms like chords, tempo, key, and time signature, there has been a lack of research. Additionally, most of the research in text-to-music uses internal datasets and do not release their codes. In this work, we lay the foundations for filling this research gap by targetting musical controllability specifically and making all our contributions public.

## 3 Dataset Creation

In this section, we describe the creation of our MusicBench dataset. First, the methods of music feature extraction and data augmentation are introduced, then the details of our dataset and how we applied these methods to it are discussed.

### Feature Extraction and Description Enrichment

We extract four common music features--beats and downbeats, chords, keys, and tempo--that guide music generation and enhance the original text prompt.

We utilize BeatNet (Heydari et al., 2021) to extract the beats and downbeats features: \(b\in\mathcal{R}^{L_{beats}\times 2}\) where the first dimension represents the type of beat according to the meter (e.g., 1, 2, 3) and the second represents the timing of each corresponding beat in seconds. The second feature,Tempo in Beats Per Minute (BPM), is estimated by averaging the reciprocal of the time interval between beats. Chordino (Mauch and Dixon, 2010) is used to extract chords features: \(c\in\mathcal{R}^{L_{ chords}\times 3}\) where the first dimension represents the roots of the chord sequence, the second represents the chord type (e.g., major, minor, maj7, etc.) and the third represents whether the chords are inverted. Finally, Essentia's (Bogdanov et al., 2013) KeyExtractor algorithm5 is used to extract the key. These extracted features will be used to both enrich the text description and guide the reverse diffusion process.

Footnote 5: [https://essentia.upf.edu/reference/std_KeyExtractor.html](https://essentia.upf.edu/reference/std_KeyExtractor.html)

These features are then expressed in text format following several text templates (e.g., 'The song is in the key of A minor. The tempo of this song is Adagio. The beat counts to 4. The chord progression is Am, Cmaj7, G.'). We refer to these as control sentences and they will be appended to the original text prompt to form the enhanced prompts. A full list of the different control sentence templates can be found in the Appendix.

### Augmentation and Music Diversification

In this section, we introduce our dataset augmentation for both music audio and text prompts which boosts the total amount of training data by 11 fold, in order to increase both the audio quality and controllability of the model. Standard text-to-audio augmentations might not suit the nature of music audio. For example, the augmentation utilized in Tango (Ghosal et al., 2023), whereby two audio samples of similar audio pressure levels are overlapped and their prompts concatenated, would not work for music by introducing two overlapping rhythms, dissonance in harmony, and overall musical concept mismatch.

Therefore, we augment single music _audio_ samples from either one of the three perspectives: pitch, speed, and volume which determines the melodic, rhythmic, and dynamic aspects of music. We use PyRubberband6 to shift the pitch of the music audio within a range of \(\pm\)3 semitones following a uniform distribution. We change the speed of the music audio by \(\pm\)(5 to 25)%, drawn from uniform distribution as well. Finally, we alter the volume of the audio by introducing a gradual volume change (both crescendo and decrescendo) with the minimum volume drawn from a uniform distribution from 0.1 to 0.5 times the original track's amplitude, with the maximum kept untouched. We notice a similar augmentation approach in concurrent research (Gardner et al., 2023).

Footnote 6: [https://github.com/bmcfee/pyrubberband](https://github.com/bmcfee/pyrubberband)

The corresponding text description will be changed as follows. We changed the enhanced prompts in the previous section accordingly based on the changes we have committed. To enhance the robustness of the model, we randomly discard one to four sentences from the prompt which describes the aforementioned four music features. More details are illustrated in the Appendix. Finally, we used ChatGPT to rephrase the text prompt to add variety to the text prompts.

### MusicBench

In this study, we make use of the MusicCaps (Agostinelli et al., 2023) dataset, which comprises a collection of 5,521 audio clips featuring music. Each clip is 10 seconds long and is sourced from the train and evaluation splits of the AudioSet (Gemmeke et al., 2017) dataset. These audio clips are accompanied by on average four-sentence-long texts that describe the music. However, due to the inaccessibility of some audio files, our dataset consists of 5,479 samples.

We split our dataset as shown in Figure 1. First, we extract music features (for later use) from all the samples and split the data into TrainA and TestA sets. By concatenating all four control sentences to the original prompts, we obtain the TrainB and TestB sets. Then, by

Figure 1: Composition of the MusicBench dataset.

rephrase the TrainB text prompts, we get the final TrainC set.

In addition, before performing audio augmentation, we filter out samples that mention 'low quality' and similar terms in the captions of TrainA set, to get 3,413 instances. These higher quality samples are audio-augmented (see SS3.2) to to form a set of 37k samples. Following this, we randomly select control prompts to be concatenated with the original captions. We pick \(0/1/2/3/4\) prompts with a probability of \(25/30/20/15/10\%\) respectively. We then rephrase all of the captions using ChatGPT. In our final training dataset, we use both of the rephrased and non-rephrased prompts with a probability of \(85/15\%\) respectively. Finally, we take this augmented set and concatenate it with sets TrainA, TrainB, and TrainC to get our final training set consisting of 52,768 samples, further referred to as MusicBench.

## 4 Mustango

Mustango consists of two components: 1) Latent Diffusion Model; 2) MuNet.

### Latent Diffusion Model (LDM)

Inspired by Tango (Ghosal et al., 2023) and AudioDM(Liu et al., 2023b), we leverage the latent diffusion model (LDM) to reduce computational complexity meanwhile maintaining the expressiveness of the diffusion model. More specifically, we aim to construct the latent audio prior \(z_{0}\) extracted using an extra variational autoencoder (VAE) with condition \(\mathcal{C}\), which in our case refers to a joint music and text condition. Similar to Tango, we leverage the pre-trained VAE from AudioDM (Liu et al., 2023a) to obtain the latent code of the audio.

Through the forward-diffusion process (Markovian Hierarchical VAE), the latent audio prior \(z_{0}\) turns into a standard gaussian noise \(z_{N}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\), as shown in Eq. (1) where a pre-scheduled gaussian noise (\(0<\beta_{1}<\beta_{2}<\cdots<\beta_{N}<1\)) is gradually added at each forward step:

\[q(z_{n}|z_{n-1})=\mathcal{N}(\sqrt{1-\beta_{n}}z_{n-1},\beta_{n}\mathbf{I}). \tag{1}\]

In the reverse process of reconstructing \(z_{0}\) from Gaussian noise \(z_{N}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\), we propose MuNet, which is able to steer the generated music towards the given condition \(\mathcal{C}\). We will elaborate the details of MuNet in the next Section. Intuitively, the backward diffusion aims to reconstruct latent audio prior \(z_{n-1}\) from the prior step \(z_{n}\) until \(z_{0}\) is reconstructed as shown in Equation (2)-Equation (7):

\[p_{\theta}^{mus}(z_{n-1}|z_{n},\mathcal{C})=\] \[\mathcal{N}(\mu_{\theta}^{(n)}(z_{n},\mathcal{C}),\tilde{\beta}^ {(n)}), \tag{2}\] \[\mu_{\theta}^{(n)}(z_{n},\mathcal{C})=\] \[\frac{1}{\sqrt{\alpha_{n}}}[z_{n}-\frac{1-\alpha_{n}}{\sqrt{1- \overline{\alpha}_{n}}}\hat{\epsilon}_{\theta}^{(n)}(z_{n},\mathcal{C})], \tag{3}\]

\[\tilde{\beta}^{(n)}=\frac{1-\bar{\alpha}_{n-1}}{1-\bar{\alpha}_{n}}\beta_{n}, \tag{4}\]

\[\alpha_{n}=1-\beta_{n}, \tag{5}\]

\[\overline{\alpha}_{n}=\prod_{i=1}^{n}\alpha_{n}, \tag{6}\]

\[\hat{\epsilon}_{\theta}^{(n)}(z_{n},\mathcal{C})=\] \[w\;\epsilon_{\theta}^{(n)}(z_{n},\mathcal{C})+(1-w)\epsilon_{ \theta}^{(n)}(z_{n}), \tag{7}\]

\(w\) is the guidance scale in Eq. (7) used during inference. During training however, \(\epsilon_{\theta}^{(n)}(z_{n},\mathcal{C})\) is directly used for noise estimation where the conditions \(\mathcal{C}\) are randomly dropped as specified in SS5.2.

This reconstruction is trained using a noise-estimation loss, as defined in Eq. (8), where \(\hat{\epsilon}_{\theta}^{(n)}\) is the estimated noise and \(\gamma_{n}\) is the weight of reverse step \(n\):

\[\mathcal{L}_{DM}=\sum_{n=1}^{N}\gamma_{n}\mathbb{E}_{\epsilon_{n}\sim \mathcal{N}(\mathbf{0},\mathbf{I})}||\epsilon_{n}-\hat{\epsilon}^{(n)}(z_{n},\mathcal{C})||_{2}^{2}. \tag{8}\]

### MuNet

The reverse-diffusion process, described in Eqs. (2) to (7), is conditioned on both music (beat \(b\) and chord \(c\)) and text \(\tau\) (\(\mathcal{C}:=\{\tau,b,c\}\)). This is realized through the Music-Domain-Knowledge-Informed UNet (MuNet) denoiser whose noise estimator is defined as

\[\epsilon_{\theta}^{(n)}(z_{n},\mathcal{C}):=\] \[\text{UNet}_{\theta}(\] \[\text{MHA}_{\theta_{b}}(\] \[Q=\text{MHA}_{\theta_{c}}(\] \[Q=\text{MHA}_{\theta_{r}}(Q=z_{n},K/V=\text{FLAN-T5}(\tau)),\] \[K/V=\mathbf{Enc^{c}(c)}),\] \[K/V=\mathbf{Enc^{b}(b)})), \tag{9}\]

where MHA is multi-headed attention used for cross attention, where \(Q,K,\) and \(V\) are query, key, and value, respectively.

MuNet follows a structure similar to UNet Ronneberger et al. (2015), consisting of multiple downsampling, middle, and upsampling blocks, and the conditions are incorporated via cross attention. In the MuNet, we propose two encoders \(\mathbf{Enc^{b}}\) and \(\mathbf{Enc^{c}}\) to encode the beat and chord features which leverage the state-of-the-art Fundamental Music Embedding (FME) and Music Positional Encoding (MPE) Guo et al. (2023) to ensure the musical features are properly captured and several fundamental music properties (e.g., translational invariance) are preserved Guo et al. (2023).

We hereby introduce the details of the two encoders: \(\mathbf{Enc^{b}}\) and \(\mathbf{Enc^{c}}\) that extract the beat and chord embeddings from the raw input. In the beat encoder \(\mathbf{Enc^{b}}\) formulated in Equation (10), we employ a One-Hot Encoding (\(\mathbf{OH_{b}}\)) to encode the beat type: \(b[:,0]\) and a Music Positional Embedding (\(MPE\)) Guo et al. (2023) to capture the timing of the beats: \(b[:,1]\). By concatenating these beat types and timing embeddings and passing them through a trainable linear layer (\(\mathbf{W_{b}}\)), we obtain the final embedded beat feature.

\[\mathbf{Enc^{b}}(b)=\mathbf{W_{b}}(OH_{b}(b[:,0])\oplus MPE(b[:,1])) \tag{10}\]

In the chord encoder in Equation (11), we obtain the chord embeddings by first concatenating 1. FME-embedded Guo et al. (2023) chord roots (\(c[:,0]\)); 2. One-Hot encoded chord type (\(c[:,1]\)); 3. One-Hot encoded chord inversions (\(c[:,1]\)) and; 4. MPE-embedded Guo et al. (2023) timing of the chords (\(c[:,3]\)). Subsequently, this concatenated representation is passed through a trainable linear layer (\(\mathbf{W_{c}}\)). Notably, we incorporate a music-domain-knowledge informed music embedding through the use of the Fundamental Music Embedding (\(FME\)) Guo et al. (2023), which effectively captures the translational invariant property of pitches and intervals, resulting in a more musically meaningful representation of the chord.

\[\mathbf{Enc^{c}}(c)=\mathbf{W_{c}}(\text{FME}(c[:,0])\oplus OH_{t}(c[:,1]) \oplus OH_{i}(c[:,2])\oplus\text{MPE}(c[:,3])) \tag{11}\]

After obtaining the encoded beat and chord embeddings, we use two additional cross-attention layers to integrate these music conditions during the denoising process, as compared to TANG0 Ghosal et al. (2023) which only use one cross-attention layer to incoporate text conditions (see Eq. (9)). This enables MuNet to leverage music features as well as text features during the denoising process, resulting in more controllable and musically meaningful music generation.

Figure 2: Depiction of our proposed Mustango model. Beats and chords are inferred from the caption when they are not provided as input.

### Inference

During the training phase, we use teacher forcing and hence utilize the ground truth beats and chord features to condition the music generation process. However, during inference, we adopt a different approach. We employ two transformer-based text-to-music-feature generators that have been trained independently to predict the beat and chord features as follows:

**Beats**: We use the DeBERTa Large model [11] as the beats predictor. The model takes the text caption as input and predicts the following: i) the maximum beat of corresponding music, and ii) the sequence of interval duration between the beats. We predict them from the token-level representations of the final layer of the DeBERTa model. The maximum beat takes an integer value between 1 and 4 for the music instances in our training dataset. Hence, we predict the maximum beat using a four-class classification setup from the first token of the DeBERTa output layer. The interval durations are predicted as a float value from the second token onwards. As an example, if the maximum beat is predicted as \(2\) and the interval durations are predicted as \(t_{1},t_{2},t_{3},\dots\), then the predicted beats are as follows: \(1\) at \(t_{1}\), \(2\) at \(t_{1}+t_{2}\), \(1\) at \(t_{1}+t_{2}+t_{3}\), etc. We keep the predicted beats time up to 10 seconds and ignore predicted timestamps beyond that.

**Chords**: We use the sequence to sequence FLAN-T5 Large model [10] as the chords predictor. The model takes the concatenation of the text caption and the verbalized beats as input. The verbalized beats are prepared for the example we illustrated earlier as follows: _Timestamps: \(t_{1}\), \(t_{1}+t_{2}\), \(t_{1}+t_{2}+t_{3}\dots\), Max Beat: \(2\)_. The model is trained to generate the verbalized chords sequence with timestamps, which would look like something as follows: _Am at 1.11; E at 4.14; C#maj7 at 7.18_. We again keep the predicted chord time up to 10 seconds and ignore timestamps predicted beyond that.

## 5 Experiments

In this section, we describe our thorough experiments that aim to answer the following research questions:

* How good is the audio quality of the music generated by Mustango4. Is it comparable or better than the output by Tango [12]? Footnote 4: [https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps](https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps)
* Does Mustango4 achieve better musical quality than Tango? Footnote 4: [https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps](https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps)
* Is our data augmentation approach effective in enhancing performance, and can models trained on only this dataset compete with large-scale audio pre-trained models?

To answer these questions, we deploy extensive objective and subjective evaluations.

### Baselines and Mustango4 Variants

We primarily compare Mustango4 with Tango [12], a latent diffusion model for audio generation that shares the same architecture with Mustango4, except the lack of the extra conditioning on beats and chords of MuNet. To judge the efficacy of Mustango4, we train the following three models from scratch: 1) Tango trained on MusicCaps TrainA, 2) Tango trained on MusicBench, 3) Mustango4 trained on MusicBench. Additionally, we finetune Tango and Mustango4 from pre-trained Tango checkpoints: 4) Tango trained on TangoPromptBank [12] and fine-tuned on AudioCaps, and MusicCaps 7, 5) Tango checkpoint fine-tuned on AudioCaps, now finetuned on MusicBench 8, 6) Mustango4 initialized from pre-trained Tango checkpoint and finetuned on MusicBench.

Footnote 4: [https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps](https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps)

Footnote 5: [https://huggingface.co/declare-lab/tango-full-ft-audiocaps](https://huggingface.co/declare-lab/tango-full-ft-audiocaps)

### Training

All the models were trained at a learning rate of \(4.5e-5\) using the AdamW [13] optimizer until convergence. Our Beat and Chord predictors are also trained on MusicBench.

To further improve the robustness of Mustango4, we use these three dropouts during training-data loading:

1. With 5% probability, drop all the inputs (text, beats, and chords);
2. With 5% probability, drop an input feature (applied to each of the inputs separately);
3. We determine the probability of masking a prompt as \(\min(100,10\frac{N}{M})\)%, where \(N\) represents the number of sentences in the current prompt, and \(M\) is the average number of sentences per prompt. Once a prompt is chosen for masking, we randomlydraw an integer \(X\) from a uniform distribution in the range [20, 50] and proceed to remove \(X\)% of the input sentences in the prompt.

The idea behind the first two dropouts is to enable the model to work with incomplete, faulty, or missing input information. The third dropout is aimed at improving robustness for short text inputs. We apply these dropouts to Tango as well, with a small modification: since Tango does not use music feature inputs, we replace the first two dropouts with a single 10 % probability of dropping all text.

### Objective Evaluation

The quality of the generated audio samples is evaluated concerning three objective metrics: Frechet Distance (FD), Frechet Audio Distance (FAD) [19], and Kullback-Leibler divergence (KL). To measure the distance between the distributions of generated audio and ground truth reference, KL divergence is used. This metric is applied to the labels generated by a pre-trained classifier, namely PANNs [18], large pre-trained audio networks for audio pattern recognition. As in [16, 15] we also utilize PANNs to calculate FD, which is a similarity metric between two curves. FAD is an evaluation metric similar to FID (Frechet Inception Distance) from the image domain; which was specifically designed for the audio domain. It is based on human perception and it is computed through the use of a VGGish classifier.

#### 5.3.1 Out-of-Distribution Evaluation

Given that the fine-tuned models used in our experiments were exposed to the entire MusicCaps dataset when training the initial pre-trained checkpoint, we can only fairly evaluate those models on our independently-created evaluation set, which we refer to as \(\mathtt{FMACaps}\). We source the new music files from the Free Music Archive (FMA) [14], a large dataset of popular songs. In particular, we took 1,000 random samples from FMA-large and clipped out a random 10-second fragment from each of them. Then, we used Essentia's tagging models [1] to assign tags to audio. Specifically, we used the models for general auto-tagging, mood, genre, instrumentation, voice, and voice gender which provide us with a rich set of tags along with their probabilities. Then, a music expert wrote text descriptions for 25 of the samples based on the audio as well as the extracted tags. Next, we instructed Chat-GPT to perform an in-context learning task to get pseudo-prompts from tags for the rest of the dataset. Finally, we added relevant control sentences to the prompts after extracting relevant music features, as described in SS3.1. Similar to our training set, we added 0/1/2/3/4 control sentences with a probability of 25/30/20/15/10% respectively. We refer to this evaluation set as \(\mathtt{FMACaps}\).

#### 5.3.2 Evaluation of Controllability

To evaluate the models in terms of controllability, we utilize TestB as depicted in Fig. 1, as well as a modified version of \(\mathtt{FMACaps}\) that has all the control sentences for each sample in the prompt. We generated music based on the input text prompts from the test sets. From these generated music, we extracted several musical features (see SS3.1) so that we could compare them to the features specified in the input text prompts. To quantify this comparison, we developed a number of control metrics, all of which are represented in percentage, hence ranging from 0 to 100. In case a metric is binary, it will be represented as 100 for True, and 0 for False. The metrics are defined as:

* **Tempo bin (TB)** -- the predicted beats per minute (bpm) fall into the ground truth tempo bin.
* **Tempo bin with tolerance (TBT)** -- the predicted bpm falls into the ground truth tempo bin or a neighboring one.
* **Correct key (CK)** -- the predicted key matches the ground truth key.
* **Correct key with duplicates (CKD)** -- the predicted key matches the ground truth key or an equivalent key. We consider major and its equivalent minor as duplicates. (e.g., C major and A minor).
* **Perfect chord match (PCM)** -- the predicted chord sequence perfectly matches ground truth in terms of length, order, chord root, and chord type.
* **Exact chord match (ECM)** -- the predicted chord sequence matches the ground truth exactly in terms of order, chord root, and chord type, with tolerance for missing and excess chord instances.
* **Chord match in any order (CMAO)** -- the portion of predicted chord sequence that matches the ground truth in both chord root and chord type, in any order.
* **Chord match in any order major/minor type (CMAOMM)** -- the portion of predicted chord sequence that matches the ground truth in terms of chord root and binary major/minor chord type, in any order (e.g., D, D6, D7, Dmaj7 are all considered major).

\(\bullet\) **Beat count prediction (BC)** -- the percentage of predicted beat counts that match the ground truth.

### Subjective Evaluation

In addition to objective evaluation, we also performed subjective evaluation in the form of two listening tests: a general listening test and an expert listening test that focuses on controllability.

For the general listening test, subjects listened to ten generated music samples for each of the four models and were provided with the input text caption. The four models included Mustango, both pre-trained as well as trained from scratch. The ten text prompts were custom-made by music experts in the style of musicCaps, and are shown in Table 7 in the Appendix. The participants were asked to rate the 1) Audio quality (AQ), 2) Relevance of the audio to the input text prompt (REL), 3) Overall musicality (OM), 4) Rhythm presence and stability (RP), 5) Harmony and consonance of music (HC). All the aspects were rated on a 7-point Likert scale using the PsyToolkit interface Stoet (2010). The full questions and interface used are shown in the Appendix.

For the expert listening test, we found expert raters with at least 5 years of formal musical training who are able to recognize chords from music audio. They were presented with 80 samples to rate generated using 20 text prompts for each of the four models as shown in Table 7. The text prompts were custom made by music experts and consisted of ten 'contrasting' pairs. Care was given to make sure that they were realistic and that there were no contradicting elements in the prompts. For instance, caption 1 in Table contrasts with caption 2. The main text is the dame: "An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming keeps the rhythm steady.". But the control sentences are different: "The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute." versus "The chord sequence is Dm, Am, Em. This song goes at 60 beats per minute." Both chord sequences come from blues progressions, but they belong to a different key/mode. The tempo of caption 2 is significantly slower. Such captions are ideally suited to test if the control sentences influence the generated music. In addition to the five aspects to rate from the general listening study, we added 2 music control-specific aspects to rate. These include the degree to which the chords from the generated music match those specified in the text-prompt (Chord Match or MCM), and the degree to which the tempo of the generated music matched the tempo specified in the text-prompt (Tempo match or MTM).

### Experimental Results

#### 5.5.1 Objective Evaluation

Results of objective evaluation for TestA, TestB, and FMA eval set are depicted in Table 1. Both Tango variants trained on MusicCaps are inferior to the other 4 models, **which depicts the efficacy of our augmentation strategy**. Pre-trained Tango fine-tuned on MusicBench and Mustango are pre-trained seem to perform very similarly in FD and KL, but Mustango are pre-trained shows a big improvement in FAD, which suggests better-perceived quality and musicality as FAD is a human perception-inspired metric. Last but not least, the performance of Mustango trained from scratch is comparable in FD and KL to both pre-trained versions of Mustango and Tango trained on MusicBench, which shows that training with our augmented dataset can be an alternative to large-scale audio pre-training for music generation.

The evaluation of controllability results are shown in Table 2. On TestB, in Tempo metrics, all the models perform comparably. In Key metrics, we can observe that models trained on MusicBench perform significantly better than the ones trained on MusicCaps. Additionally, Mustango outperforms all the other models on TestB and placed second on FMACaps. In Chord controllability, Mustango outperforms all the Tango models by a big margin. Finally, in Beat metrics, the models seem to perform similarly to each other, but Mustango shows the best result. On FMACaps, we further see that the Chord metrics are even better for Mustango with CMAOMM reaching 75.83. Overall, the results gathered from both TestB and modified FMACaps correlate in most aspects.

#### 5.5.2 Subjective Evaluation

A total of 48 participants participated in the general listening test, of which 26 had more than 5 years of formal musical training. The results in Table 3 show the average ratings for each of the metrics defined above. We can clearly see that the Tango baseline model is outperformed in all metrics by the models trained on MusicBench. Interestingly,Mustango is trained from scratch performs the best in terms of audio quality, rhythm presence, and harmony. The differences in ratings are minimal between the three top models, clearly confirming that our augmentation method is effective in furthering the output quality, and that Mustango is able to reach state-of-the-art quality.

A total of 4 experts participated in the controllability listening study. The results of the expert listening study in Table 4 further confirm that both Mustango models outperform the Tango baselines in all metrics, especially in term of the chords of the generated music matching with the input text caption (Chord Match or MCM). This further supports the controllability results presented in Table 2 and shows that our proposed Mustango model can indeed understand music-specific text prompts.

### Ablation Study

The natural ablations would be to discard one control at a time in Mustango. However, due to resource constraints, we were unable to conduct these experiments.

On the other hand, we address the following two research questions through ablations:

Is Pre-training Mustango. Necessary?.In certain experimental setups, we initialized Mustango with a Tango checkpoint pre-trained on TangoPromptBank and subsequently fine-tuned through instruction tuning on the AudioCaps dataset. These checkpoints encapsulate a broad understanding of general audio and sound, such as "the sound of an elephant". However, we observed that this general audio knowledge did not prove beneficial for music generation (See Tables 1 to 4). Nevertheless, these checkpoints may find utility in composing music with diverse sounds, such as "African hip-hop music with the accompaniment of a lion's roar."

Is MuNet helpful?.The role of MuNet in Mustango is to control the music generation process. As elaborated in SS5.3, the inclusion of MuNet significantly enhances the performance of Mustango on both TestB and FMACaps under both objective and subjective evaluations, directly measuring how well the model adheres to control instructions in the provided prompt. Importantly, MuNet does not compromise the overall per

\begin{table}
\begin{tabular}{c l c c|c c c|c c c|c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Datasets**} & \multirow{2}{*}{**Pre-trained**} & \multirow{2}{*}{**\#Params**} & \multicolumn{4}{c|}{**TestA**} & \multicolumn{4}{c|}{**TestB**} & \multicolumn{4}{c}{**FMCaps**} \\  & & & FD \(\downarrow\) & FAD \(\downarrow\) & KL \(\downarrow\) & FD \(\downarrow\) & FAD \(\downarrow\) & KL \(\downarrow\) & FD \(\downarrow\) & FAD \(\downarrow\) & FAD \(\downarrow\) & KL \(\downarrow\) \\ \hline Tango & MusicCaps & ✗ & 866M & 30.80 & 2.84 & 1.34 & 30.39 & 2.92 & 1.33 & 28.32 & 3.75 & 1.22 \\ Tango & MusicCaps & ✓ & 866M & 34.87 & 4.05 & 1.25 & 37.85 & 4.52 & 1.32 & 28.81 & 2.92 & 1.21 \\ Tango & MusicBench & ✗ & 866M & 28.50 & 2.29 & 1.33 & 28.27 & 2.17 & 1.32 & 26.31 & 2.31 & 1.16 \\ Tango & MusicBench & ✓ & 866M & 25.38 & 1.91 & 1.19 & 24.60 & 1.77 & 1.13 & 24.48 & 2.96 & 1.15 \\ Mustango. ✗ & MusicBench & ✗ & 1.4B & 26.58 & 2.09 & 1.21 & 25.24 & 1.57 & 1.18 & 24.24 & 2.94 & 1.16 \\ Mustango. ✗ & MusicBench & ✓ & 1.4B & 26.35 & 1.46 & 1.21 & 25.97 & 1.67 & 1.12 & 25.18 & 2.34 & 1.16 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Objective evaluation results of the models on all the test datasets. The columns show the average value for each of the metrics per model.

\begin{table}
\begin{tabular}{l l c c c c c c c c c c c c c} \hline \hline Model & Dataset & Pre-trained & REL & AG & OM & RP & HC \\ \hline Tango & MusicCaps & ✓ & 4.09 & 3.68 & 3.55 & 3.91 & 3.80 \\ Tango & MusicBench & ✓ & 4.96 & 4.26 & 4.04 & 4.49 & 4.61 \\ Mustango & MusicBench & ✓ & 4.85 & 4.10 & 4.24 & 4.43 & 4.33 \\ Mustango & MusicBench & ✗ & 4.79 & 4.20 & 4.23 & 4.51 & 4.63 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average ratings for each metric in the general listening study.

\begin{table}
\begin{tabular}{l l c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Datasets**} & \multirow{2}{*}{**Pre-trained**} & \multirow{2}{*}{**\#Params**} & \multicolumn{4}{c|}{**TestA**} & \multicolumn{4}{c}{**TestB**} & \multicolumn{4}{c}{**FMCaps**} \\  & & & & FD \(\downarrow\) & FAD \(\downarrow\) & KL \(\downarrow\) & FD \(\downarrow\) & FAD \(\downarrow\) & KL \(\downarrow\) & FD \(\downarrow\) & FAD \(\downarrow\) & FAD \(\downarrow\) & KL \(\downarrow\) \\ \hline Tango & MusicCaps & ✗ & 866M & 30.80 & 2.84 & 1.34 & 30.39 & 2.92 & 1.33 & 28.32 & 3.75 & 1.22 \\ Tango & MusicCaps & ✓ & 866M & 34.87 & 4.05 & 1.25 & 37.85 & 4.52 & 1.32 & 28.81 & 2.92 & 1.21 \\ Tango & MusicBench & ✗ & 866M & 28.50 & 2.29 & 1.33 & 28.27 & 2.17 & 1.32 & 26.31 & 2.31 & 1.16 \\ Tango & MusicBench & ✓ & 866M & 25.38 & 1.91 & 1.19 & 24.60 & 1.77 & 1.13 & 24.48 & 2.96 & 1.15 \\ Mustango. ✗ & MusicBench & ✗ & 1.4B & 26.58 & 2.09 & 1.21 & 25.24 & 1.57 & 1.18 & 24.24 & 2.94 & 1.16 \\ Mustango. ✗ & MusicBench & ✓ & 1.4B & 26.35 & 1.46 & 1.21 & 25.97 & 1.67 & 1.12 & 25.18 & 2.34 & 1.16 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Average ratings for each metric in the expert listening study.

formance of general music generation in the absence of control sentences in the prompts. In fact, several objective metrics (such as FD, FAD, and KL-divergence), not explicitly focused on evaluating control instruction adherence, consistently show performance improvements when MuNet is incorporated.

### Discussions

#### 5.7.1 Performance of the Predictors

During the inference phase, we utilize pre-trained predictors for chord and beat predictions based on textual prompts. These predictors exhibit exceptional performance when the prompts explicitly contain chord and beat information, achieving accuracy of 94.5 % on the TestB dataset. However, our interest extends to evaluating their performance in scenarios where control sentences are absent from the prompt--essentially, do these predictors generate noisy chords and beats? The concern is that such noise might propagate from the predictors to Mustango, significantly impacting the overall quality of the generated music.

In our experiments, TestA serves as a scenario where control sentences are not included in the textual prompts. Upon comparing the performance (Table 1) of Tango and Mustango, on TestA, we observe that the latter outperforms the former across most metrics. This observation indicates that the control predictors do not compromise the performance of Mustango, relative to Tango. The adaptability of these predictors to specific themes or styles in the absence of control sentences remains a potential avenue for future exploration, a topic we briefly touch upon below.

First, we investigate the effect of the Chord predictor on the generated output in a little comparison experiment. We take both TestA and TestB samples synthesized by Mustango and extract features from them. Then, we evaluate the chord control metrics of PCM, ECM, CMAO, and CMAOMM using chords predicted by chord predictor vs chords detected in the audio from feature extraction. The metrics on TestA are PCM - 16.15, ECM - 33.95, CMAO - 39.81, and CMAOMM - 47.82. The metrics on TestB are PCM - 17.75, ECM - 32.07, CMAO - 47.36, and CMAOMM - 66.80. These results show that Mustango, tends to follow the chords predicted by the chord predictor quite often. While the results on TestA are a bit lower than on TestB, they are still higher than Tango results on TestB as shown in Table 2.

Second, we take a look at some specific examples:

**Prompt: "This folk song features a female voice singing the main melody. This is accompanied by a tabla playing the percussion. A guitar strums chords. For most parts of the song, only one chord is played. At the last bar, a different chord is played. This song has minimal instruments. This song has a story-telling mood. This song can be played in a village scene in an Indian movie. _The chord sequence is Bbm, Ab. The beat is 3. The tempo of this song is Allegro. The key of this song is Bb minor._"

Without control sentences in italics (TestA): **chords predicted**: ["G", "C", "G", "C", "G", "C"], **chords predicted time**: [0.46, 1.21, 3.25, 5.48, 7.24, 8.92]. **chords extracted from audio**: ["G6", "C", "G", "C", "G", "Cmaj7"], **chords time extracted from audio**: [0.46, 1.58, 3.07, 5.94, 7.62, 9.66]

With control sentences in italics (TestB): **chords predicted**: ["Bbm", "Ab"], **chords predicted time**: [0.46, 7.24], **chords extracted from audio**: ["F#maj7", "Ab"], **chords time extracted from audio**: [0.46, 7.43].

**Prompt: "A female singer sings this bluesy melody. The song is medium tempo with minimal guitar accompaniment and no other instrumentation. The song's medium tempo is very emotional and passionate. The song is a modern pop hit but with poor audio quality. _The key of this song is G minor. The time signature is 3/4. This song goes at 168.0 beats per minute. The chord progression in this song is Am7, G7, Cm, G, A7._"

Without control sentences in italics (TestA): **chords predicted**: ["C#m7", "C#m7", "C#m7", "C#m7"], **chords predicted time**: [0.46, 3.25, 6.32, 8.17, 9.29], **chords extracted from audio**: ["F#", "C#m", "F#m", "C#m7"], **chords time extracted from audio**: [0.46, 1.21, 4.55, 5.39]

With control sentences in italics (TestB): **chords predicted**: ["Am7", "G7", "Cm", "G", "A7"], **chords predicted time**: [0.46, 1.67, 3.53, 5.48, 8.92], **chords extracted from 

**audio**: ["Am", "G", "C", "Gmaj7", "A6", "Gmaj7"], **chords time extracted from audio**: [0.46, 1.67, 3.72, 5.94, 8.73, 9.85]

The two depicted samples give us some specific insights into the predicted chords and chords detected in the generated audio. Most of the time, \(\tt Mustango\) follows the chords provided by the chord predictor in most cases. We can observe some substitutions in the actual chords detected from the audio compared to the predicted chords, e.g., G became G6, C became Cmaj7, and C#m7 became C#m. These chord substitutions are very close musically and could even be a consequence of the feature extraction system not being 100% accurate. The substitution of Bbm for F#maj7 is more of a change at first glance, but given that 2 out of 3 notes in Bbm are also contained in the 4-note F#maj Mustango, we see this substitution as understandable too. However, we note that this substitution would not be considered a valid one in any of our proposed chord control metrics.

Last but not least, in the absence of explicit control sentences in the prompt, we observe that the chords predicted by the chord predictor usually follow specific patterns. The generated samples follow a pattern of two chords that alternate (A, B, A, B, A, B). Another type of an observed pattern is one chord repeated (A, A, A, A, A). A more elaborate study on the Chord predictor behavior should be a topic for future work.

#### 5.7.2 Insights from the Human Annotation

Here, we take a look at some generated examples from the expert listening test, specifically a blues sample with the following prompt: "An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming keeps the rhythm steady. The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute."

In Figure 3 we can see the mel-spectrogram generated by pre-trained Tango finetuned on MusicBench. As is clear from the spectrogram and the waveform attached, the music appears a bit abruptly in contrast to the sample generated by \(\tt Mustango\) depicted in Figure 4 where the rhythm is very consistent. This seems to reflect the results of our expert listening study from Table 4. The predicted beat timestamps by our Beat predictor that condition the diffusion process are as follows: **beats predicted**: [[0.26, 0.87, 1.52, 2.09, 2.76, 3.41, 4.0, 4.57, 5.1, 5.65, 6.22, 6.79, 7.36, 7.79, 8.3, 8.8, 9.3, 9.75], 3]. **These predicted beat timestamps show that there is a beat roughly every 0.6 seconds, which corresponds to 100 beats per minute tempo. This is the tempo ordered and properly predicted to condition the model.**

When it comes to chords, Tango would sometimes not follow the chords, make them sound unclear, or not give them enough time to sound through. On the other hand, \(\tt Mustango\) seems to follow the predicted chords as well as their starting time. We take a look at the same blues example. The predicted chord condition from the Chord predictor is as follows: **chords predicted**: ["G7", "F7", "C7", "G7"], **chords predicted time**: [0.46, 2.04, 4.37, 8.17]. We can see that the chord onset time is nicely spread in time. This is also clear from listening to the sample and seeing the spectrogram with perceived chord starts in Figure 4. To confirm this, we extracted the chord features from the generated audio to compare. The chord feature extracted from the audio sample generated by \(\tt Mustango\) is: **chords**: ["G7", "F7", "C", "G7"], **chords time**: [0.46, 1.76, 4.74, 8.45] Interestingly, the match of timing and chord sequence is very clear here. The substitution of the C7 chord for C can be a minor mistake either on the generation part or the feature extraction part. If we consider the chord metrics from SS5.3.2, this would yield a score of 100 for CMAOMM and a score of 75 for CMAO and ECM. In contrast, the sample generated by pre-trained Tango finetuned on \(\tt MusicBench\) sounds more unstable and does not give enough time to chords to sound through. The chord feature extracted from the audio sample generated by pre-trained Tango finetuned on \(\tt MusicBench\) is: **chords**: ["Fm6", "G", "Dm", "G", "C", "Gm"], **chords time**: [0.46, 2.69, 3.53, 5.76, 6.69, 9.66]. We can see that there are 6 chords extracted from the audio sample instead of the ordered 4, and they do not match too well, as we see a minor type of F chord instead of a major; G also appears in a minor variant once; and there is an additional Dm chord too. This would yield a CMAOMM score of 75, but CMAO and ECM scores of 0. The perceived chord starts can be seen in Figure 3.

## 6 Conclusion

In conclusion, this paper introduces a significant advancement in the field of text-to-music synthesis with the development of \(\tt Mustango\), a novel

[MISSING_PAGE_FAIL:13]

Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio set: An ontology and human-labeled dataset for audio events. In _2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)_, pages 776-780. IEEE.
* Ghosal et al. (2023) Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. 2023. Text-to-audio generation using instruction tuned llm and latent diffusion model. _arXiv preprint arXiv:2304.13731_.
* Guo et al. (2023) Zixun Guo, J. Kang, and D. Herremans. 2023. A domain-knowledge-inspired music embedding space and a novel attention mechanism for symbolic music modeling. In _Proc. of the 37th AAAI Conference on Artificial Intelligence_, Washington DC. AAAI, AAAI.
* He et al. (2022) Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2022. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. In _The Eleventh International Conference on Learning Representations_.
* Herremans et al. (2017) Dorien Herremans, Ching-Hua Chuan, and Elaine Chew. 2017. A functional taxonomy of music generation systems. _ACM Computing Surveys (CSUR)_, 50(5):1-30.
* Heydari et al. (2021) Mojtaba Heydari, Frank Cwitkowitz, and Zhiyao Duan. 2021. Beatnet: Crm and particle filtering for online joint beat downbeat and meter tracking.
* Huang et al. (2022) Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel P. W. Ellis. 2022. Mulan: A joint embedding of music audio and natural language. In _Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022, Bengaluru, India, December 4-8, 2022_, pages 559-566.
* Huang et al. (2023) Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. 2023. Noise2music: Text-conditioned music generation with diffusion models. _arXiv preprint arXiv:2302.03917_.
* Kilgour et al. (2019) Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. 2019. Frechet audio distance: A reference-free metric for evaluating music enhancement algorithms. In _INTERSPEECH_, pages 2350-2354.
* Kong et al. (2020) Qiuqiang Kong, Yin Cao, Trubq Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley. 2020. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. _IEEE/ACM Transactions on Audio, Speech, and Language Processing_, 28:2880-2894.
* Li et al. (2023) Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, and Alex Wang. 2023. Jen-1: Text-guided universal music generation with omnidirectional diffusion models. _arXiv preprint arXiv:2308.04729_.
* Liu et al. (2023a) Haobe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. 2023a. Audioldm: Text-to-audio generation with latent diffusion models. _arXiv preprint arXiv:2301.12503_.
* Liu et al. (2023b) Haobe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo P. Mandic, Wenwu Wang, and Mark D. Plumbley. 2023b. AudioLDM: Text-to-audio generation with latent diffusion models. _ArXiv_, abs/2301.12503.
* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_.
* Mauch and Dixon (2010) Matthias Mauch and Simon Dixon. 2010. Approximate note transcription for the improved identification of difficult chords. In _Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010, Utrecht, Netherlands, August 9-13, 2010_, pages 135-140. International Society for Music Information Retrieval.
* OpenAI (2023) OpenAI. 2023. DALL-E 2.
* Popov et al. (2021) Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov, and Jiansheng Wei. 2021. Diffusion-based voice conversion with fast maximum likelihood sampling scheme. _arXiv preprint arXiv:2109.13821_.
* Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer.
* Schneider et al. (2023) Flavio Schneider, Zhijing Jin, and Bernhard Scholkopf. 2023. Mo\(\backslash\)* usai: Text-to-music generation with long-context latent diffusion. _arXiv preprint arXiv:2301.11757_.
* Stoet (2010) Gijsbert Stoet. 2010. Psytoolkit: A software package for programming psychological experiments using linux. _Behavior research methods_, 42:1096-1104.
* Su et al. (2023) Kun Su, Judith Yue Li, Qingqing Huang, Dima Kuzmin, Joonseok Lee, Chris Donahue, Fei Sha, Aren Jansen, Yu Wang, Mauro Verzetti, et al. 2023. V2meow: Mewing to the visual beat via music generation. _arXiv preprint arXiv:2305.06594_.
* Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_.
* Wu et al. (2023) Yussong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In _ICASSP 2023-2023 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. Cited by: SS1.
* [14]N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi (2022) Soundstream: an end-to-end neural audio codec. IEEE ACM Trans. Audio Speech Lang. Process.30, pp. 495-507. Cited by: SS1.

[MISSING_PAGE_EMPTY:16]

[MISSING_PAGE_EMPTY:17]

Custom Captions Used for Listening Studies

## Appendix C

\begin{table}
\begin{tabular}{l p{341.4pt}} \hline
1 & This piece is an instrumental reggae song that is very chill and slow. There is no singer. It is relaxing to hear the groove with the bass guitar. The song includes reggae electric guitar, horn, and percussion like bongos. The keyboard provides lush chords. The time signature is 4/4. The chord progression is G, F, C. \\
2 & This instrumental blues song goes very slow at a bpm of 50. You can hear the bass, harmonica and guitar grooving. The harmonica plays a solo over the harmonious guitar and bass. \\
3 & This classical piece is a waltz played by a string quartet. It includes two violins, a viola, and a cello, the beat counts to 3. It sounds elegant, and has a strong first beat. It has a natural and danceable rhythm. The mood is romantic. The chord progression is Em, Am, D, G. \\
4 & African drums are playing a complex rhythm while a male vocalist chants a ritual. The atmosphere is mesmerizing. The complex drumming pattern is a mesmerizing blend of syncopation, polyhythms, and intricate patterns. It takes place somewhere in the wilderness, or in an indigenous village. \\
5 & This rock piece with guitars and drums is loud but fades out later on and becomes softer. It sounds powerful yet melancholic. It is instrumental only. A bass guitar provides a steady beat, enhancing the groove and energy of the song. A single bass instrument is playing a running baseline. It has a jazzy feeling to it and sounds mellow. This could be played in a jazz club. The tempo is 120 bpm. \\
7 & This is a hip hop song. It has two rappers taking turns, one female and one male. An electronic synth melody sample in the background keeps on looping. We can hear electronic beats and sometimes record-scratching sound effects. \\
8 & A smooth jazz song with saxophone, drums and guitar with a chord progression of Dm7, G7, Cmaj7. The song is relaxed and slow. There are no vocals, it is instrumental only. The saxophone produces a velety tone that delivers an emotive melody. \\
9 & A piano plays a soothing popular instrumental song that could serve as background music in a restaurant. There is only piano playing, no other instruments. There is a piano melody with background piano chords of Am, Fmaj7, Cmaj7, and G. The tempo is unhurried. The melody is gentle and soothing, evoking a sense of nostalgia and comfort. \\
10 & Indian folk music with a sitar and female vocals. It evokes a sense of zen and elevation. A sitar player begins with a gentle and melodic introduction, plucking the strings with precision and emotion. There are rhythmic beats of traditional hand percussion instruments, such as the tabla. It could be played at a cultural festival to showcase Indian culture. \\ \hline \end{tabular}
\end{table}
Table 6: Custom captions used for the general listening test.

* [1] An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming keeps the rhythm steady. The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute.
* [2] An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming keeps the rhythm steady. The chord sequence is Dm, Am, Em. This song goes at 60 beats per minute.
* [3] A piano plays a popular melody over the chords of Am, Fmaj7, Cmaj7, G. There is only piano playing, no other instruments or voice. The tempo is Adagio.
* [4] A piano plays a popular melody over the chords of Gm, Bb, Eb. There is only piano playing, no other instruments or voice. The tempo is Vivace.
* [5] This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful. The thunderous beats of the drummer provide a bounding rhythm. A guitar solo melody emerges from the chaotic background of the chords. The chord progression is A, D, E. The tempo of the song is 160 bpm.
* [6] This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful. The thunderous beats of the drummer provide a bounding rhythm. A guitar solo melody emerges from the chaotic background of the chords.The chord progression is C, B, A, G. The tempo of the song is 100 bpm.
* [7] A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of Em7b5, A7, Dm7. The pianist produces delicate harmonies and subtle embellishments. The drummer provides a brushed rhythm. The guitar strums softly, while the saxophone plays a solo over the chords. This song goes at 80 beats per minute.
* [8] A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of B7, G7, E7, C7. The drummer provides a brushed rhythm. The guitar strums softly, while the saxophone plays a solo over the chords. This song goes at 115 beats per minute.
* [9] This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a powerful and relentless drumbeat. Over the bounding beats, a leading melody emerges. It has strong danceability and can be played in a club. The tempo is 120 bpm. The chords played by the synth are Am, Cm, Dm, Gm.
* [10] This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a powerful and relentless drumbeat. Over the bounding beats, a leading melody emerges. It has strong danceability and can be played in a club. The tempo is 160 bpm. The chords played by the synth are C, F, G.
* [11] A horn and a bass guitar groove to a reggae tune. The combination of the horn section's catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Adagio. An electric keyboard plays the chords Am, Dm, G, C.
* [12] A horn and a bass guitar groove to a reggae tune. The combination of the horn section's catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Moderato. An electric keyboard plays the chords E, B, A.
* [13] This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of Em, C, G, D. The tempo is 120 bpm.
* [14] This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of A, F#m, D, E. The tempo is 170 bpm.
* [15] A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening. The chord progression is G, C, D, G. The tempo is 100 beats per minute.
* [16] A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening. The chord progression is Am, Em, Dm, Am. The tempo is 70 beats per minute.
* [17] This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin plays the lead melody. The cello's soulful and melodic contributions add depth and gravitas to the performance. The time signature is 34. The tempo of this song is Presto. The chord sequence is E, C#m, A, B.
* [18] This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin plays the lead melody. The cello's soulful and melodic contributions add depth and gravitas to the performance. The time signature is 4/4. The tempo of this song is Andante. The chord sequence is Am, Dm, E7, Am.
* [19] This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background. These loops provide the song's electronic foundation, creating a rich and layered sonic landscape. The charistatic female singer has a dynamic and emotive voice. The tempo is Moderato. The chord sequence is C, G, Am, F.
* [20] This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background. These loops provide the song's electronic foundation, creating a rich and layered sonic landscape. The charistatic female singer has a dynamic and emotive voice. The tempo is Presto. The key is A minor and the chord sequences are Am, Dm, E.

\begin{table}
\begin{tabular}{l l} \hline
1 & An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist’s strumming keeps the rhythm steady. The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute. \\
2 & An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist’s strumming keeps the rhythm steady. The chord sequence is Dm, Am, Em. This song goes at 60 beats per minute. \\
3 & A piano plays a popular melody over the chords of Am, Fmaj7, Cmaj7, G. There is only piano playing, no other instruments or voice. The tempo is Adagio. \\
4 & A piano plays a popular melody over the chords of Gm, Bb, Eb. There is only piano playing, no other instruments or voice. The tempo is Vivace. \\
5 & This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful. The thunderous beats of the drummer provide a bounding rhythm. A guitar solo melody emerges from the chaotic background of the chords. The chord progression is A, D, E. The tempo of the song is 160 bpm. \\
6 & This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful. The thunderous beats of the drummer provide a bounding rhythm. A guitar solo melody emerges from the chaotic background of the chords.The chord progression is C, B, A, G. The tempo of the song is 100 bpm. \\
7 & A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of Em7b5, A7, Dm7. The pianist produces delicate harmonies and subtle embellishments. The drummer provides a brushed rhythm. The guitar strums softly, while the saxophone plays a solo over the chords. This song goes at 80 beats per minute. \\
8 & A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of B7, G7, E7, C7. The drummer provides a brushed rhythm. The quitar strums softly, while the saxophone plays a solo over the chords. This song goes at 115 beats per minute. \\
9 & This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a powerful and relentless drumbeat. Over the pounding beats, a leading melody emerges. It has strong danceability and powerful and relentless drumbeat. Over the pounding beats, a leading melody emerges. It has strong danceability and powerful and relentless drumbeat. Over the pounding beats, a leading melody emerges. It has strong danceability and can be played in a club. The tempo is 120 bpm. The chords played by the synth are Am, Cm, Dm, Gm.
* [10] This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a powerful and relentless drumbeat. Over the pounding beats, a leading melody emerges. It has strong danceability and can be played in a club. The tempo is 160 bpm. The chords played by the synth are C, F, G.
* [11] A horn and a bass guitar groove to a reggae tune. The combination of the horn section’s catchy melodies and the buoyant electric keyboard plays the chords Am, Dm, G, C.
* [12] A horn and a bass guitar groove to a reggae tune. The combination of the horn section’s catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Moderato. An electric keyboard plays the chords E, B, A.
* [13] This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of Em, C, G, D. The tempo is 120 bpm. \\
14 & This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of A, F#m, D, E. The tempo is 170 bpm. \\
15 & A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening. The chord progression is G, C, D, G. The tempo is 100 beats per minute. \\
16 & A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening. The chord progression is Am, Em, Dm, Am. The tempo is 70 beats per minute. \\
17 & This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin plays the lead melody. The cello’s soulful and melodic contributions add depth and gravitas to the performance. The time signature is 34. The tempo of this song is Presto. The chord sequence is E, C#m, A, B.
* [18] This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin plays the lead melody. The cello’s soulful and melodic contributions add depth and gravitas to the performance. The time signature is 4/4. The tempo of this song is Andante. The chord sequence is Am, Dm, E7, Am.
* [19] This is a pop song with a female singer singing the leading melody and synthesizes looping samples as background. These loops provide the song’s electronic foundation, creating a rich and layered sonic landscape. The charistatic female singer has a dynamic and emotive voice. The tempo is Moderato. The chord sequence is C, G, Am, F.
* [20] This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background. These loops provide the song’s electronic foundation, creating a rich and layered sonic landscape. The charistatic female singer has a dynamic and emotive voice. The tempo is Presto. The key is A minor and the chord sequences are Am, Dm, E.

\begin{table}
\begin{tabular}{l l} \hline
1 & An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist’s strumming keeps the rhythm steady. The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute. \\
2 & An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist’s strumming keeps the rhythm steady. The chord sequence is Dm, Am, Em. This song goes at 60 beats per minute. \\
3 & A piano plays a popular melody over the chords of Am, Fmaj7, Cmaj7, G. There is only piano playing, no other instruments or voice. The tempo is Adagio. \\
4 & A piano plays a popular melody over the chords of Gm, Bb, Eb. There is only piano playing, no other instruments or voice. The tempo is Vivace. \\
5 & This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful. The thunderous beats of the drummer provide a sounding rhythm. A guitar solo melody emerges from the chaotic background of the 

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_EMPTY:21]
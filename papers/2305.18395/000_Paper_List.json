{
    "2305.18395": {
        "paper_id": "2305.18395",
        "abs_url": "https://arxiv.org/abs/2305.18395",
        "pdf_url": "https://arxiv.org/pdf/2305.18395.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2305.18395_Knowledge-Augmented_Reasoning_Distillation_for_Small_Language_Models_in_Knowledge-Intensive_Tasks.pdf",
        "title": "Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Minki Kang",
            "Seanie Lee",
            "Jinheon Baek",
            "Kenji Kawaguchi",
            "Sung Ju Hwang"
        ],
        "abstract": "Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small Language Models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales obtained from LLMs with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantly improves the performance of small T5 and GPT models on the challenging knowledge-intensive reasoning datasets, namely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the 250M T5 models achieve superior performance against the fine-tuned 3B models, having 12 times larger parameters, on both MedQA-USMLE and StrategyQA benchmarks.",
        "comments": "NeurIPS 2023",
        "official_code_urls": [
            "https://github.com/nardien/kard"
        ],
        "pwc_page_url": "https://paperswithcode.com/paper/knowledge-augmented-reasoning-distillation-1",
        "bibtex": "@misc{kang2023knowledgeaugmented,\n      title={Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks}, \n      author={Minki Kang and Seanie Lee and Jinheon Baek and Kenji Kawaguchi and Sung Ju Hwang},\n      year={2023},\n      eprint={2305.18395},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}
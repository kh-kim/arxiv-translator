<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Instruction-tuned Language Models are Better Knowledge Learners</title>
<!--Generated on Tue Feb 20 09:17:20 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.12847v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2402.12847v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2402.12847v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2402.12847v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S1" title="1 Introduction ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S2" title="2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Building a Dataset to Study Continual Knowledge Acquisition</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S2.SS1" title="2.1 Wiki2023 Document Corpus ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Wiki2023 Document Corpus</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S2.SS2" title="2.2 Wiki2023 Question-answer Pairs ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Wiki2023 Question-answer Pairs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S2.SS3" title="2.3 Splits ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Splits</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S3" title="3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Settings</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S3.SS1" title="3.1 Objectives ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Objectives</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S3.SS2" title="3.2 Hyperparameters ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S3.SS3" title="3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluation Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4" title="4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning?</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4.SS1" title="4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Vanilla Continued Pre-training and Instruction-tuning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4.SS1.SSS0.Px1" title="Experimental settings ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Experimental settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4.SS1.SSS0.Px2" title="Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Experimental results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4.SS2" title="4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Analyzing the Training Dynamics: Perplexity and Generalization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4.SS2.SSS0.Px1" title="Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Experiment results</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5" title="5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Improving LLMs in Absorbing Knowledge from Documents</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS1" title="5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Variants of Pre-instruction-tuning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS1.SSS0.Px1" title="Pre-instruction-tuning w/ QA only ‣ 5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Pre-instruction-tuning w/ QA only</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS1.SSS0.Px2" title="Pre-instruction-tuning on QA and documents sequentially ‣ 5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Pre-instruction-tuning on QA and documents sequentially</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS1.SSS0.Px3" title="Pre-instruction-tuning ‣ 5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Pre-instruction-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS2" title="5.2 Pre-instruction-tuning++ ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Pre-instruction-tuning++</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS3" title="5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS3.SSS0.Px1" title="Standard instruction-tuning is inferior not due to forgetting ‣ 5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Standard instruction-tuning is inferior not due to forgetting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS3.SSS0.Px2" title="Pre-instruction-tuning is not simply upweighting salient tokens from documents ‣ 5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Pre-instruction-tuning is not simply upweighting salient tokens from documents</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS4" title="5.4 Cross-domain Generalization ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Cross-domain Generalization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S6" title="6 Related Work ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S6.SS1" title="6.1 Continual Knowledge Acquisition ‣ 6 Related Work ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Continual Knowledge Acquisition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S6.SS2" title="6.2 Instruction-tuning or Alignment ‣ 6 Related Work ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Instruction-tuning or Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S6.SS3" title="6.3 Analyzing the Training Dynamics of LMs ‣ 6 Related Work ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Analyzing the Training Dynamics of LMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S6.SS4" title="6.4 Retrieval-augmented Generation ‣ 6 Related Work ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Retrieval-augmented Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S7" title="7 Conclusion ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
<li>failed: anyfontsize</li>
<li>failed: cuted</li>
<li>failed: color-edits</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2402.12847v1 [cs.CL] 20 Feb 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Instruction-tuned Language Models are Better Knowledge Learners</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhengbao Jiang<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mn id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><cn id="id1.1.m1.1.1.1.cmml" type="integer" xref="id1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Zhiqing Sun<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mn id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><cn id="id2.2.m2.1.1.1.cmml" type="integer" xref="id2.2.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Weijia Shi<math alttext="{}^{1,3}" class="ltx_Math" display="inline" id="id3.3.m3.2"><semantics id="id3.3.m3.2a"><msup id="id3.3.m3.2.2" xref="id3.3.m3.2.2.cmml"><mi id="id3.3.m3.2.2a" xref="id3.3.m3.2.2.cmml"></mi><mrow id="id3.3.m3.2.2.2.4" xref="id3.3.m3.2.2.2.3.cmml"><mn id="id3.3.m3.1.1.1.1" xref="id3.3.m3.1.1.1.1.cmml">1</mn><mo id="id3.3.m3.2.2.2.4.1" xref="id3.3.m3.2.2.2.3.cmml">,</mo><mn id="id3.3.m3.2.2.2.2" xref="id3.3.m3.2.2.2.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.2b"><apply id="id3.3.m3.2.2.cmml" xref="id3.3.m3.2.2"><list id="id3.3.m3.2.2.2.3.cmml" xref="id3.3.m3.2.2.2.4"><cn id="id3.3.m3.1.1.1.1.cmml" type="integer" xref="id3.3.m3.1.1.1.1">1</cn><cn id="id3.3.m3.2.2.2.2.cmml" type="integer" xref="id3.3.m3.2.2.2.2">3</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.2c">{}^{1,3}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.2d">start_FLOATSUPERSCRIPT 1 , 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Pedro Rodriguez<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><msup id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mi id="id4.4.m4.1.1a" xref="id4.4.m4.1.1.cmml"></mi><mn id="id4.4.m4.1.1.1" xref="id4.4.m4.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><cn id="id4.4.m4.1.1.1.cmml" type="integer" xref="id4.4.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Chunting Zhou<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id5.5.m5.1"><semantics id="id5.5.m5.1a"><msup id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml"><mi id="id5.5.m5.1.1a" xref="id5.5.m5.1.1.cmml"></mi><mn id="id5.5.m5.1.1.1" xref="id5.5.m5.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><apply id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1"><cn id="id5.5.m5.1.1.1.cmml" type="integer" xref="id5.5.m5.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id5.5.m5.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"> <span class="ltx_text ltx_font_bold" id="id9.9.4">Graham Neubig<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id6.6.1.m1.1"><semantics id="id6.6.1.m1.1a"><msup id="id6.6.1.m1.1.1" xref="id6.6.1.m1.1.1.cmml"><mi id="id6.6.1.m1.1.1a" xref="id6.6.1.m1.1.1.cmml"></mi><mn id="id6.6.1.m1.1.1.1" mathvariant="normal" xref="id6.6.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id6.6.1.m1.1b"><apply id="id6.6.1.m1.1.1.cmml" xref="id6.6.1.m1.1.1"><cn id="id6.6.1.m1.1.1.1.cmml" type="integer" xref="id6.6.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id6.6.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Xi Victoria Lin<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id7.7.2.m2.1"><semantics id="id7.7.2.m2.1a"><msup id="id7.7.2.m2.1.1" xref="id7.7.2.m2.1.1.cmml"><mi id="id7.7.2.m2.1.1a" xref="id7.7.2.m2.1.1.cmml"></mi><mn id="id7.7.2.m2.1.1.1" mathvariant="normal" xref="id7.7.2.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id7.7.2.m2.1b"><apply id="id7.7.2.m2.1.1.cmml" xref="id7.7.2.m2.1.1"><cn id="id7.7.2.m2.1.1.1.cmml" type="integer" xref="id7.7.2.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.2.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id7.7.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Wen-tau Yih<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id8.8.3.m3.1"><semantics id="id8.8.3.m3.1a"><msup id="id8.8.3.m3.1.1" xref="id8.8.3.m3.1.1.cmml"><mi id="id8.8.3.m3.1.1a" xref="id8.8.3.m3.1.1.cmml"></mi><mn id="id8.8.3.m3.1.1.1" mathvariant="normal" xref="id8.8.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id8.8.3.m3.1b"><apply id="id8.8.3.m3.1.1.cmml" xref="id8.8.3.m3.1.1"><cn id="id8.8.3.m3.1.1.1.cmml" type="integer" xref="id8.8.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.8.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id8.8.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Srinivasan Iyer<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id9.9.4.m4.1"><semantics id="id9.9.4.m4.1a"><msup id="id9.9.4.m4.1.1" xref="id9.9.4.m4.1.1.cmml"><mi id="id9.9.4.m4.1.1a" xref="id9.9.4.m4.1.1.cmml"></mi><mn id="id9.9.4.m4.1.1.1" mathvariant="normal" xref="id9.9.4.m4.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id9.9.4.m4.1b"><apply id="id9.9.4.m4.1.1.cmml" xref="id9.9.4.m4.1.1"><cn id="id9.9.4.m4.1.1.1.cmml" type="integer" xref="id9.9.4.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.9.4.m4.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id9.9.4.m4.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<br class="ltx_break"><math alttext="{}^{1}" class="ltx_Math" display="inline" id="id10.10.m6.1"><semantics id="id10.10.m6.1a"><msup id="id10.10.m6.1.1" xref="id10.10.m6.1.1.cmml"><mi id="id10.10.m6.1.1a" xref="id10.10.m6.1.1.cmml"></mi><mn id="id10.10.m6.1.1.1" xref="id10.10.m6.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id10.10.m6.1b"><apply id="id10.10.m6.1.1.cmml" xref="id10.10.m6.1.1"><cn id="id10.10.m6.1.1.1.cmml" type="integer" xref="id10.10.m6.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.10.m6.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id10.10.m6.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>FAIR at Meta  <math alttext="{}^{2}" class="ltx_Math" display="inline" id="id11.11.m7.1"><semantics id="id11.11.m7.1a"><msup id="id11.11.m7.1.1" xref="id11.11.m7.1.1.cmml"><mi id="id11.11.m7.1.1a" xref="id11.11.m7.1.1.cmml"></mi><mn id="id11.11.m7.1.1.1" xref="id11.11.m7.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id11.11.m7.1b"><apply id="id11.11.m7.1.1.cmml" xref="id11.11.m7.1.1"><cn id="id11.11.m7.1.1.1.cmml" type="integer" xref="id11.11.m7.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.11.m7.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id11.11.m7.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>Carnegie Mellon University  <math alttext="{}^{3}" class="ltx_Math" display="inline" id="id12.12.m8.1"><semantics id="id12.12.m8.1a"><msup id="id12.12.m8.1.1" xref="id12.12.m8.1.1.cmml"><mi id="id12.12.m8.1.1a" xref="id12.12.m8.1.1.cmml"></mi><mn id="id12.12.m8.1.1.1" xref="id12.12.m8.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="id12.12.m8.1b"><apply id="id12.12.m8.1.1.cmml" xref="id12.12.m8.1.1"><cn id="id12.12.m8.1.1.1.cmml" type="integer" xref="id12.12.m8.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.12.m8.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="id12.12.m8.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>University of Washington 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id13.13.id1">{zhengbaj,gneubig}@cs.cmu.edu</span>  <span class="ltx_text ltx_font_typewriter" id="id14.14.id2">{victorialin,scottyih,sviyer}@meta.com</span>
</span><span class="ltx_author_notes">Majority of the work done during an internship at Meta.</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id15.id1">In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data.
The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs.
However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized.
We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner.
Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs <em class="ltx_emph ltx_font_italic" id="id15.id1.1">before</em> continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions.
Based on this, we propose <span class="ltx_text ltx_font_bold" id="id15.id1.2">pre-instruction-tuning (PIT)</span>, a method that instruction-tunes on questions prior to training on documents.
This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents.
Extensive experiments and ablation studies demonstrate that PIT significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<span class="ltx_ERROR undefined" id="id1">\addauthor</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">gnmagenta
































</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="p2">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p2.1"><span class="ltx_text ltx_font_bold" id="p2.1.1">Instruction-tuned Language Models are Better Knowledge Learners</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p3">
<br class="ltx_break">
<p class="ltx_p" id="p3.12"><span class="ltx_text" id="p3.12.12" style="width:433.6pt;"><span class="ltx_text" id="p3.12.12.12" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p3.12.12.12.12">
<span class="ltx_tbody">
<span class="ltx_tr" id="p3.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p3.5.5.5.5.5.5"><span class="ltx_text ltx_font_bold" id="p3.5.5.5.5.5.5.5">Zhengbao Jiang<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.1.1.1.1.1.1.1.m1.1"><semantics id="p3.1.1.1.1.1.1.1.m1.1a"><msup id="p3.1.1.1.1.1.1.1.m1.1.1" xref="p3.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="p3.1.1.1.1.1.1.1.m1.1.1a" xref="p3.1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="p3.1.1.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="p3.1.1.1.1.1.1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.1.1.1.1.1.1.1.m1.1b"><apply id="p3.1.1.1.1.1.1.1.m1.1.1.cmml" xref="p3.1.1.1.1.1.1.1.m1.1.1"><cn id="p3.1.1.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="p3.1.1.1.1.1.1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.1.1.1.1.1.1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_thanks" id="p3.5.5.5.5.5.5.5.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>Majority of the work done during an internship at Meta.</span></span></span>  Zhiqing Sun<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.2.2.2.2.2.2.2.m2.1"><semantics id="p3.2.2.2.2.2.2.2.m2.1a"><msup id="p3.2.2.2.2.2.2.2.m2.1.1" xref="p3.2.2.2.2.2.2.2.m2.1.1.cmml"><mi id="p3.2.2.2.2.2.2.2.m2.1.1a" xref="p3.2.2.2.2.2.2.2.m2.1.1.cmml"></mi><mn id="p3.2.2.2.2.2.2.2.m2.1.1.1" mathvariant="normal" xref="p3.2.2.2.2.2.2.2.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.2.2.2.2.2.2.2.m2.1b"><apply id="p3.2.2.2.2.2.2.2.m2.1.1.cmml" xref="p3.2.2.2.2.2.2.2.m2.1.1"><cn id="p3.2.2.2.2.2.2.2.m2.1.1.1.cmml" type="integer" xref="p3.2.2.2.2.2.2.2.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.2.2.2.2.2.2.2.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.2.2.2.2.2.2.2.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Weijia Shi<math alttext="{}^{1,3}" class="ltx_Math" display="inline" id="p3.3.3.3.3.3.3.3.m3.2"><semantics id="p3.3.3.3.3.3.3.3.m3.2a"><msup id="p3.3.3.3.3.3.3.3.m3.2.2" xref="p3.3.3.3.3.3.3.3.m3.2.2.cmml"><mi id="p3.3.3.3.3.3.3.3.m3.2.2a" xref="p3.3.3.3.3.3.3.3.m3.2.2.cmml"></mi><mrow id="p3.3.3.3.3.3.3.3.m3.2.2.2.4" xref="p3.3.3.3.3.3.3.3.m3.2.2.2.3.cmml"><mn id="p3.3.3.3.3.3.3.3.m3.1.1.1.1" mathvariant="normal" xref="p3.3.3.3.3.3.3.3.m3.1.1.1.1.cmml">1</mn><mo id="p3.3.3.3.3.3.3.3.m3.2.2.2.4.1" mathvariant="normal" xref="p3.3.3.3.3.3.3.3.m3.2.2.2.3.cmml">,</mo><mn id="p3.3.3.3.3.3.3.3.m3.2.2.2.2" mathvariant="normal" xref="p3.3.3.3.3.3.3.3.m3.2.2.2.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="p3.3.3.3.3.3.3.3.m3.2b"><apply id="p3.3.3.3.3.3.3.3.m3.2.2.cmml" xref="p3.3.3.3.3.3.3.3.m3.2.2"><list id="p3.3.3.3.3.3.3.3.m3.2.2.2.3.cmml" xref="p3.3.3.3.3.3.3.3.m3.2.2.2.4"><cn id="p3.3.3.3.3.3.3.3.m3.1.1.1.1.cmml" type="integer" xref="p3.3.3.3.3.3.3.3.m3.1.1.1.1">1</cn><cn id="p3.3.3.3.3.3.3.3.m3.2.2.2.2.cmml" type="integer" xref="p3.3.3.3.3.3.3.3.m3.2.2.2.2">3</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.3.3.3.3.3.3.3.m3.2c">{}^{1,3}</annotation><annotation encoding="application/x-llamapun" id="p3.3.3.3.3.3.3.3.m3.2d">start_FLOATSUPERSCRIPT 1 , 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Pedro Rodriguez<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.4.4.4.4.4.4.4.m4.1"><semantics id="p3.4.4.4.4.4.4.4.m4.1a"><msup id="p3.4.4.4.4.4.4.4.m4.1.1" xref="p3.4.4.4.4.4.4.4.m4.1.1.cmml"><mi id="p3.4.4.4.4.4.4.4.m4.1.1a" xref="p3.4.4.4.4.4.4.4.m4.1.1.cmml"></mi><mn id="p3.4.4.4.4.4.4.4.m4.1.1.1" mathvariant="normal" xref="p3.4.4.4.4.4.4.4.m4.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.4.4.4.4.4.4.4.m4.1b"><apply id="p3.4.4.4.4.4.4.4.m4.1.1.cmml" xref="p3.4.4.4.4.4.4.4.m4.1.1"><cn id="p3.4.4.4.4.4.4.4.m4.1.1.1.cmml" type="integer" xref="p3.4.4.4.4.4.4.4.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.4.4.4.4.4.4.4.m4.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.4.4.4.4.4.4.4.m4.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Chunting Zhou<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.5.5.5.5.5.5.5.m5.1"><semantics id="p3.5.5.5.5.5.5.5.m5.1a"><msup id="p3.5.5.5.5.5.5.5.m5.1.1" xref="p3.5.5.5.5.5.5.5.m5.1.1.cmml"><mi id="p3.5.5.5.5.5.5.5.m5.1.1a" xref="p3.5.5.5.5.5.5.5.m5.1.1.cmml"></mi><mn id="p3.5.5.5.5.5.5.5.m5.1.1.1" mathvariant="normal" xref="p3.5.5.5.5.5.5.5.m5.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.5.5.5.5.5.5.5.m5.1b"><apply id="p3.5.5.5.5.5.5.5.m5.1.1.cmml" xref="p3.5.5.5.5.5.5.5.m5.1.1"><cn id="p3.5.5.5.5.5.5.5.m5.1.1.1.cmml" type="integer" xref="p3.5.5.5.5.5.5.5.m5.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.5.5.5.5.5.5.5.m5.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.5.5.5.5.5.5.5.m5.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="p3.9.9.9.9.9">
<span class="ltx_td ltx_align_center" id="p3.9.9.9.9.9.4"><span class="ltx_text ltx_font_bold" id="p3.9.9.9.9.9.4.4">Graham Neubig<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.6.6.6.6.6.1.1.m1.1"><semantics id="p3.6.6.6.6.6.1.1.m1.1a"><msup id="p3.6.6.6.6.6.1.1.m1.1.1" xref="p3.6.6.6.6.6.1.1.m1.1.1.cmml"><mi id="p3.6.6.6.6.6.1.1.m1.1.1a" xref="p3.6.6.6.6.6.1.1.m1.1.1.cmml"></mi><mn id="p3.6.6.6.6.6.1.1.m1.1.1.1" mathvariant="normal" xref="p3.6.6.6.6.6.1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.6.6.6.6.6.1.1.m1.1b"><apply id="p3.6.6.6.6.6.1.1.m1.1.1.cmml" xref="p3.6.6.6.6.6.1.1.m1.1.1"><cn id="p3.6.6.6.6.6.1.1.m1.1.1.1.cmml" type="integer" xref="p3.6.6.6.6.6.1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.6.6.6.6.6.1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.6.6.6.6.6.1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Xi Victoria Lin<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.7.7.7.7.7.2.2.m2.1"><semantics id="p3.7.7.7.7.7.2.2.m2.1a"><msup id="p3.7.7.7.7.7.2.2.m2.1.1" xref="p3.7.7.7.7.7.2.2.m2.1.1.cmml"><mi id="p3.7.7.7.7.7.2.2.m2.1.1a" xref="p3.7.7.7.7.7.2.2.m2.1.1.cmml"></mi><mn id="p3.7.7.7.7.7.2.2.m2.1.1.1" mathvariant="normal" xref="p3.7.7.7.7.7.2.2.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.7.7.7.7.7.2.2.m2.1b"><apply id="p3.7.7.7.7.7.2.2.m2.1.1.cmml" xref="p3.7.7.7.7.7.2.2.m2.1.1"><cn id="p3.7.7.7.7.7.2.2.m2.1.1.1.cmml" type="integer" xref="p3.7.7.7.7.7.2.2.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.7.7.7.7.7.2.2.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.7.7.7.7.7.2.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Wen-tau Yih<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.8.8.8.8.8.3.3.m3.1"><semantics id="p3.8.8.8.8.8.3.3.m3.1a"><msup id="p3.8.8.8.8.8.3.3.m3.1.1" xref="p3.8.8.8.8.8.3.3.m3.1.1.cmml"><mi id="p3.8.8.8.8.8.3.3.m3.1.1a" xref="p3.8.8.8.8.8.3.3.m3.1.1.cmml"></mi><mn id="p3.8.8.8.8.8.3.3.m3.1.1.1" mathvariant="normal" xref="p3.8.8.8.8.8.3.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.8.8.8.8.8.3.3.m3.1b"><apply id="p3.8.8.8.8.8.3.3.m3.1.1.cmml" xref="p3.8.8.8.8.8.3.3.m3.1.1"><cn id="p3.8.8.8.8.8.3.3.m3.1.1.1.cmml" type="integer" xref="p3.8.8.8.8.8.3.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.8.8.8.8.8.3.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.8.8.8.8.8.3.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Srinivasan Iyer<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.9.9.9.9.9.4.4.m4.1"><semantics id="p3.9.9.9.9.9.4.4.m4.1a"><msup id="p3.9.9.9.9.9.4.4.m4.1.1" xref="p3.9.9.9.9.9.4.4.m4.1.1.cmml"><mi id="p3.9.9.9.9.9.4.4.m4.1.1a" xref="p3.9.9.9.9.9.4.4.m4.1.1.cmml"></mi><mn id="p3.9.9.9.9.9.4.4.m4.1.1.1" mathvariant="normal" xref="p3.9.9.9.9.9.4.4.m4.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.9.9.9.9.9.4.4.m4.1b"><apply id="p3.9.9.9.9.9.4.4.m4.1.1.cmml" xref="p3.9.9.9.9.9.4.4.m4.1.1"><cn id="p3.9.9.9.9.9.4.4.m4.1.1.1.cmml" type="integer" xref="p3.9.9.9.9.9.4.4.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.9.9.9.9.9.4.4.m4.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.9.9.9.9.9.4.4.m4.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="p3.12.12.12.12.12">
<span class="ltx_td ltx_align_center" id="p3.12.12.12.12.12.3"><math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.10.10.10.10.10.1.m1.1"><semantics id="p3.10.10.10.10.10.1.m1.1a"><msup id="p3.10.10.10.10.10.1.m1.1.1" xref="p3.10.10.10.10.10.1.m1.1.1.cmml"><mi id="p3.10.10.10.10.10.1.m1.1.1a" xref="p3.10.10.10.10.10.1.m1.1.1.cmml"></mi><mn id="p3.10.10.10.10.10.1.m1.1.1.1" xref="p3.10.10.10.10.10.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.10.10.10.10.10.1.m1.1b"><apply id="p3.10.10.10.10.10.1.m1.1.1.cmml" xref="p3.10.10.10.10.10.1.m1.1.1"><cn id="p3.10.10.10.10.10.1.m1.1.1.1.cmml" type="integer" xref="p3.10.10.10.10.10.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.10.10.10.10.10.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.10.10.10.10.10.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>FAIR at Meta  <math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.11.11.11.11.11.2.m2.1"><semantics id="p3.11.11.11.11.11.2.m2.1a"><msup id="p3.11.11.11.11.11.2.m2.1.1" xref="p3.11.11.11.11.11.2.m2.1.1.cmml"><mi id="p3.11.11.11.11.11.2.m2.1.1a" xref="p3.11.11.11.11.11.2.m2.1.1.cmml"></mi><mn id="p3.11.11.11.11.11.2.m2.1.1.1" xref="p3.11.11.11.11.11.2.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.11.11.11.11.11.2.m2.1b"><apply id="p3.11.11.11.11.11.2.m2.1.1.cmml" xref="p3.11.11.11.11.11.2.m2.1.1"><cn id="p3.11.11.11.11.11.2.m2.1.1.1.cmml" type="integer" xref="p3.11.11.11.11.11.2.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.11.11.11.11.11.2.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.11.11.11.11.11.2.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>Carnegie Mellon University  <math alttext="{}^{3}" class="ltx_Math" display="inline" id="p3.12.12.12.12.12.3.m3.1"><semantics id="p3.12.12.12.12.12.3.m3.1a"><msup id="p3.12.12.12.12.12.3.m3.1.1" xref="p3.12.12.12.12.12.3.m3.1.1.cmml"><mi id="p3.12.12.12.12.12.3.m3.1.1a" xref="p3.12.12.12.12.12.3.m3.1.1.cmml"></mi><mn id="p3.12.12.12.12.12.3.m3.1.1.1" xref="p3.12.12.12.12.12.3.m3.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="p3.12.12.12.12.12.3.m3.1b"><apply id="p3.12.12.12.12.12.3.m3.1.1.cmml" xref="p3.12.12.12.12.12.3.m3.1.1"><cn id="p3.12.12.12.12.12.3.m3.1.1.1.cmml" type="integer" xref="p3.12.12.12.12.12.3.m3.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.12.12.12.12.12.3.m3.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="p3.12.12.12.12.12.3.m3.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>University of Washington</span></span>
<span class="ltx_tr" id="p3.12.12.12.12.13.1">
<span class="ltx_td ltx_align_center" id="p3.12.12.12.12.13.1.1"><span class="ltx_text ltx_font_typewriter" id="p3.12.12.12.12.13.1.1.1">{zhengbaj,gneubig}@cs.cmu.edu</span>  <span class="ltx_text ltx_font_typewriter" id="p3.12.12.12.12.13.1.1.2">{victorialin,scottyih,sviyer}@meta.com</span></span></span>
</span>
</span></span> </span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="233" id="S0.F1.g1" src="x1.png" width="665">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of continued pre-training (first row), continued pre-training followed by instruction-tuning (second row), and pre-instruction-tuning before continued pre-training (last row), along with their accuracies on evaluation questions. Each right-pointing light-blue triangle indicates a training phase.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) store vast amounts of factual knowledge in their parameters through large-scale pre-training, and this knowledge can be used to answer various questions such as “where is the world’s largest ice sheet located” <cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib5" title="">2020</a>); OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib34" title="">2023</a>); Chowdhery et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib10" title="">2022</a>); Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib60" title="">2022</a>); Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib51" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib52" title="">b</a>); Gemini Team (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib12" title="">2023</a>)</cite>.
However, this factual knowledge is static, meaning that it can become outdated as the world evolves, or prove insufficient when LLMs are used in specialized or private domains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To keep LLMs up-to-date, it is common to continue pre-training on new documents to store knowledge in parameters, which allows LLMs to effectively answer queries that require up-to-date information <cite class="ltx_cite ltx_citemacro_cite">Jang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib20" title="">2022</a>)</cite>.
A widely held view is that the factual knowledge stored in parameters can be elicited through prompting <cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib5" title="">2020</a>); Petroni et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib37" title="">2019</a>); Roberts et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib42" title="">2020</a>)</cite>, and that instruction-tuning (also known as supervised fine-tuning or alignment) makes this elicitation more effective <cite class="ltx_cite ltx_citemacro_cite">Sanh et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib44" title="">2022</a>); Wei et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib56" title="">2022</a>); Ouyang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib35" title="">2022</a>)</cite>.
In the first part of this paper (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4" title="4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;4</span></a>), we conduct extensive experiments using Llama-2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib52" title="">2023b</a>)</cite> to answer the following question: <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">to what extent can we augment the knowledge stored in modern LLMs by continued pre-training on new documents, either with or without subsequent instruction-tuning</em>?
We find that, as we train LLMs repeatedly over documents to the extent that perplexity is minimized to one, the percentage of questions regarding those documents that LLMs answer correctly increases consistently to 27.6%.
Subsequent instruction-tuning further improves it to 30.3%, confirming that this widely used practice is useful to elicit more knowledge from LLMs.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This capacity might be underestimated by previous works due to using relatively small LMs or randomly initialized transformers, or lack of exhaustive training or instruction-tuning <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib54" title="">2021</a>); Hu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib16" title="">2023</a>); Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>.</span></span></span>
However, the amount of elicited knowledge is still limited, even though the perplexity of documents is minimized, a phenomenon we refer to as the “perplexity curse”.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Inspired by the “reversal curse” of <cite class="ltx_cite ltx_citemacro_citet">Berglund et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib3" title="">2023</a>)</cite>.</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.2">In the second part of the paper (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5" title="5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5</span></a>), we study methods to mitigate the perplexity curse by making LLMs more adept at absorbing knowledge from documents.
<cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite> presented an intriguing finding that training a randomly initialized transformer from scratch on a mix of biographies and related questions resulted in strong generalization to new questions.
However, understanding the reasons behind this finding and exploring ways to practically apply it for absorbing knowledge from new documents requires further investigation.
We found that question-answer (QA) pairs are generally straightforward and easily digestible, while documents tend to be more complex and cluttered, often weaving many factual statements together in a more intricate manner.
Therefore, we hypothesize that <em class="ltx_emph ltx_font_italic" id="S1.p3.2.1">it is beneficial to deliberately expose LLMs to QA data before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions</em>.
We refer to this as <span class="ltx_text ltx_font_bold" id="S1.p3.2.2">pre-instruction-tuning (PIT)</span> and conduct comprehensive experiments to benchmark different variations of this method.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S0.F1" title="Figure 1 ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;1</span></a>, our best-performing variation starts with training exclusively on QA pairs (e.g., “who handled the editing of Oppenheimer”) to grasp how knowledge is accessed.
This is followed by training on a combination of these QA pairs and associated documents (e.g., “who handled the editing of Oppenheimer” and a document about “Oppenheimer”).
In this phase, LLMs enhance their ability to absorb knowledge from information-dense documents, building upon the QA pairs that they have already mastered.
To study continual knowledge acquisition, we build a dataset named <span class="ltx_text ltx_font_typewriter" id="S1.p3.2.3">Wiki2023</span>, which includes a collection of documents from Wikipedia that are relevant to the year 2023.
Comprehensive experiments on <span class="ltx_text ltx_font_typewriter" id="S1.p3.2.4">Wiki2023</span> demonstrate that after PIT, LLMs exhibit an enhanced ability to absorb knowledge from new documents (e.g., a document about “Barbie”).
Detailed ablation studies reveal that this ability primarily stems from prioritizing learning how to access knowledge over learning to encode knowledge from documents.
Overall, PIT significantly outperforms the standard instruction-tuning approach (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS1" title="5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5.1</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS2" title="5.2 Pre-instruction-tuning++ ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5.2</span></a>), improving QA accuracies by 17.8% on Llama-2 7B (30.3% <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" stretchy="false" xref="S1.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">→</annotation></semantics></math> 48.1%) and 16.3% on Llama-2 70B (46.4% <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S1.p3.2.m2.1"><semantics id="S1.p3.2.m2.1a"><mo id="S1.p3.2.m2.1.1" stretchy="false" xref="S1.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><ci id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.p3.2.m2.1d">→</annotation></semantics></math> 62.7%).
Moreover, PIT also enhances the ability to absorb knowledge from documents of a <em class="ltx_emph ltx_font_italic" id="S1.p3.2.5">different</em> domain, shedding light on the potential to scale this method up to a wider variety of documents and instructions for more robust generalization (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS4" title="5.4 Cross-domain Generalization ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5.4</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Building a Dataset to Study Continual Knowledge Acquisition</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">To assess the ability of LLMs to learn knowledge from new documents, it is essential to use a document corpus with minimal overlap with the original pre-training corpus.
This ensures that when an LLM correctly answers questions, we can confidently attribute this capability to its learning from the new documents, rather than encountering similar questions in its original pre-training corpus.
In this section, we describe a methodology for building such a corpus from Wikipedia.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S2.F2.1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="391" id="S2.F2.1.g1" src="x2.png" width="831">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S2.F2.2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S2.F2.2.g1" src="x3.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The <span class="ltx_text ltx_font_typewriter" id="S2.F2.7.1">Wiki2023</span> dataset. <span class="ltx_text ltx_font_bold" id="S2.F2.8.2">Top-right</span>: the number of documents and QA pairs; <span class="ltx_text ltx_font_bold" id="S2.F2.9.3">Top-left</span>: frequent keywords in questions; <span class="ltx_text ltx_font_bold" id="S2.F2.10.4">Bottom</span>: the distribution of token counts in documents, questions, and answers.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="803" id="S2.F3.g1" src="x4.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An example document about “Oppenheimer” and corresponding QA pairs from <span class="ltx_text ltx_font_typewriter" id="S2.F3.2.1">Wiki2023</span>. Tokens used for computing losses are highlighted in green.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Wiki2023 Document Corpus</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In the following experiments (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4" title="4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;4</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5" title="5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5</span></a>), we use Llama-2 (7B and 70B) <cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib52" title="">2023b</a>)</cite> since it is one of the best-performing LLMs.
We use Wikipedia articles classified under the “2023” Category including topics from diverse domains such as films, arts, economics, politics, events, etc.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Category:2023" title="">https://en.wikipedia.org/wiki/Category:2023</a></span></span></span>
The likelihood that this factual information is not included in the original training corpus is supported by the low QA performance in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;1</span></a> (9.5%/17.2% for 7B/70B).<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>It is important to note the difficulty in completely avoiding factual overlap between <span class="ltx_text ltx_font_typewriter" id="footnote4.1">Wiki2023</span> and the pre-training corpus of Llama-2.
For example, a film released in 2023 might have had information available before 2023.
Data duplication detection is an active research direction, which falls beyond the focus of this study.</span></span></span>
To accelerate the training process, we only use the first section of each article, which offers a thorough summary and contains many factual statements.
The number of collected documents and an example document about “Oppenheimer” can be found in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F2" title="Figure 2 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;2</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F3" title="Figure 3 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;3</span></a>.
We refer to this as the <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.1.1">Wiki2023</span> dataset.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Wiki2023 Question-answer Pairs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">To collect QA pairs for either instruction-tuning or performance evaluation, we employ publicly available LLMs to generate diverse questions and their respective answers given the article as context, following the Prompt&nbsp;<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.SS2" title="2.2 Wiki2023 Question-answer Pairs ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;2.2</span></a>.
On average, 4.93 questions are generated for each article. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F2" title="Figure 2 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;2</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F3" title="Figure 3 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;3</span></a> show the detailed statistics and example QA pairs about “Oppenheimer”, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<svg class="ltx_picture" height="223.85" id="S2.SS2.p2.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,223.85) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.000000"><path d="M 0 5.32 L 0 218.53 C 0 221.47 2.38 223.85 5.32 223.85 L 594.68 223.85 C 597.62 223.85 600 221.47 600 218.53 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 195.1 L 598.62 195.1 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g fill="#666666" fill-opacity="1.000000"><path d="M 1.38 196.49 L 1.38 218.53 C 1.38 220.71 3.15 222.47 5.32 222.47 L 594.68 222.47 C 596.85 222.47 598.62 220.71 598.62 218.53 L 598.62 196.49 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)"><foreignObject height="179.88" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="S2.SS2.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.2.2.2.2.2.2.2" style="width:421.6pt;">
<span class="ltx_para" id="S2.SS2.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.SS2.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1">Given the following summary about the subject {topic}, generate a comprehensive list of questions and corresponding answers that cover all aspects. To make the question clear, always include {topic} in the question. Answers should be concise, consisting of a few short phrases separated by commas.</span>
<span class="ltx_p" id="S2.SS2.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2">Output in the following format:</span>
<span class="ltx_p" id="S2.SS2.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.3">Q: an open-domain question about the subject {topic} (the subject {topic} should always be included)</span>
<span class="ltx_p" id="S2.SS2.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.4">A: phrase1, phrase2, …</span>
</span>
<span class="ltx_para" id="S2.SS2.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.2.2.2.2.2.2.2.p2">
<span class="ltx_p" id="S2.SS2.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.2.2.2.2.2.2.2.p2.1">Summary:</span>
<span class="ltx_p" id="S2.SS2.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.2.2.2.2.2.2.2.p2.2">{summary}</span>
</span></span></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Splits</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Among all domains, we select the film domain for evaluation and randomly select 256 articles as the test split (<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.1">Wiki2023-film-test</span>).
We continually train LLMs on documents from the test split (<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.2">Wiki2023-film-test-doc</span>), and assess their performance based on the accuracy of corresponding questions (<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.3">Wiki2023-film-test-QA</span>).
The remaining 1720 articles and corresponding QA pairs (<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.4">Wiki2023-film-train</span>) will be used to study different training strategies, which corresponds to the in-domain setting in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F2" title="Figure 2 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;2</span></a>.
We also train on other domains before evaluation on the film domain to study the effectiveness of different methods across domains, which corresponds to the cross-domain setting in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F2" title="Figure 2 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Settings</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Objectives</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.2">When training on documents, we prepend a &lt;bos&gt; token and compute the standard next-token prediction loss by averaging over all tokens in the document: <math alttext="L_{\bm{d}}=-\sum_{t}{\log P(\bm{d}_{t}|\bm{d}_{<t})}/|\bm{d}|" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.2"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml"><msub id="S3.SS1.p1.1.m1.2.2.3" xref="S3.SS1.p1.1.m1.2.2.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.3.2" xref="S3.SS1.p1.1.m1.2.2.3.2.cmml">L</mi><mi id="S3.SS1.p1.1.m1.2.2.3.3" xref="S3.SS1.p1.1.m1.2.2.3.3.cmml">𝒅</mi></msub><mo id="S3.SS1.p1.1.m1.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.2.2.1" xref="S3.SS1.p1.1.m1.2.2.1.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1a" xref="S3.SS1.p1.1.m1.2.2.1.cmml">−</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.cmml"><msub id="S3.SS1.p1.1.m1.2.2.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.2.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1.1.2.2" xref="S3.SS1.p1.1.m1.2.2.1.1.2.2.cmml">∑</mo><mi id="S3.SS1.p1.1.m1.2.2.1.1.2.3" xref="S3.SS1.p1.1.m1.2.2.1.1.2.3.cmml">t</mi></msub><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1.cmml">log</mi><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3a" lspace="0.167em" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.cmml">⁡</mo><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml"><msub id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">𝒅</mi><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo fence="false" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml">𝒅</mi><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.2.cmml">/</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.3.2.1" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.1.cmml">|</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">𝒅</mi><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.3.2.2" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.1.cmml">|</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><apply id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2"><eq id="S3.SS1.p1.1.m1.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2"></eq><apply id="S3.SS1.p1.1.m1.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.3.2">𝐿</ci><ci id="S3.SS1.p1.1.m1.2.2.3.3.cmml" xref="S3.SS1.p1.1.m1.2.2.3.3">𝒅</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1"><minus id="S3.SS1.p1.1.m1.2.2.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1"></minus><apply id="S3.SS1.p1.1.m1.2.2.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1"><apply id="S3.SS1.p1.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2">subscript</csymbol><sum id="S3.SS1.p1.1.m1.2.2.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2.2"></sum><ci id="S3.SS1.p1.1.m1.2.2.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1"><divide id="S3.SS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.2"></divide><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1"><times id="S3.SS1.p1.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.2"></times><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3"><log id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1"></log><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2">𝑃</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2">𝒅</ci><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2">𝒅</ci><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3"><lt id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.2"><abs id="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.2.1"></abs><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝒅</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">L_{\bm{d}}=-\sum_{t}{\log P(\bm{d}_{t}|\bm{d}_{&lt;t})}/|\bm{d}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.2d">italic_L start_POSTSUBSCRIPT bold_italic_d end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_log italic_P ( bold_italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_italic_d start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT ) / | bold_italic_d |</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We do not append a ¡eos¿ token at the end of documents because we only use the first section, which does not signify the conclusion of the entire article.</span></span></span>
When training on QA pairs, we compute the average negative log-likelihood loss only on tokens in the answer given the question as the prefix: <math alttext="L_{\bm{a}}=-\sum_{t}{\log P(\bm{a}_{t}|\bm{q},\bm{a}_{<t})}/|\bm{a}|" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.3"><semantics id="S3.SS1.p1.2.m2.3a"><mrow id="S3.SS1.p1.2.m2.3.3" xref="S3.SS1.p1.2.m2.3.3.cmml"><msub id="S3.SS1.p1.2.m2.3.3.3" xref="S3.SS1.p1.2.m2.3.3.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.3.2" xref="S3.SS1.p1.2.m2.3.3.3.2.cmml">L</mi><mi id="S3.SS1.p1.2.m2.3.3.3.3" xref="S3.SS1.p1.2.m2.3.3.3.3.cmml">𝒂</mi></msub><mo id="S3.SS1.p1.2.m2.3.3.2" xref="S3.SS1.p1.2.m2.3.3.2.cmml">=</mo><mrow id="S3.SS1.p1.2.m2.3.3.1" xref="S3.SS1.p1.2.m2.3.3.1.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1a" xref="S3.SS1.p1.2.m2.3.3.1.cmml">−</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.cmml"><msub id="S3.SS1.p1.2.m2.3.3.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.2.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1.1.2.2" xref="S3.SS1.p1.2.m2.3.3.1.1.2.2.cmml">∑</mo><mi id="S3.SS1.p1.2.m2.3.3.1.1.2.3" xref="S3.SS1.p1.2.m2.3.3.1.1.2.3.cmml">t</mi></msub><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.cmml"><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.cmml"><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1.cmml">log</mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3a" lspace="0.167em" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.cmml">⁡</mo><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2.cmml">𝒂</mi><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3.cmml">t</mi></msub><mo fence="false" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">𝒒</mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">𝒂</mi><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></mrow></msub></mrow></mrow><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.2.cmml">/</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.3.2.1" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.1.cmml">|</mo><mi id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">𝒂</mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.3.2.2" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.1.cmml">|</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.3b"><apply id="S3.SS1.p1.2.m2.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3"><eq id="S3.SS1.p1.2.m2.3.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2"></eq><apply id="S3.SS1.p1.2.m2.3.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.3.2">𝐿</ci><ci id="S3.SS1.p1.2.m2.3.3.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.3.3">𝒂</ci></apply><apply id="S3.SS1.p1.2.m2.3.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1"><minus id="S3.SS1.p1.2.m2.3.3.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1"></minus><apply id="S3.SS1.p1.2.m2.3.3.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1"><apply id="S3.SS1.p1.2.m2.3.3.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2">subscript</csymbol><sum id="S3.SS1.p1.2.m2.3.3.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2.2"></sum><ci id="S3.SS1.p1.2.m2.3.3.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1"><divide id="S3.SS1.p1.2.m2.3.3.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.2"></divide><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1"><times id="S3.SS1.p1.2.m2.3.3.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.2"></times><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3"><log id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1"></log><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2">𝑃</ci></apply><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2">𝒂</ci><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3">𝑡</ci></apply><list id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝒒</ci><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2">𝒂</ci><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3"><lt id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></list></apply></apply><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.2"><abs id="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.2.1"></abs><ci id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2">𝒂</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.3c">L_{\bm{a}}=-\sum_{t}{\log P(\bm{a}_{t}|\bm{q},\bm{a}_{&lt;t})}/|\bm{a}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.3d">italic_L start_POSTSUBSCRIPT bold_italic_a end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_log italic_P ( bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_italic_q , bold_italic_a start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT ) / | bold_italic_a |</annotation></semantics></math>.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F3" title="Figure 3 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;3</span></a> presents an example document alongside QA pairs, where tokens used for computing losses are highlighted.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Hyperparameters</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">We use AdamW <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib30" title="">2019</a>)</cite> with <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><msub id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.2.2.cmml">β</mi><mn id="S3.SS2.p1.1.m1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><eq id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></eq><apply id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2">𝛽</ci><cn id="S3.SS2.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.2.3">1</cn></apply><cn id="S3.SS2.p1.1.m1.1.1.3.cmml" type="float" xref="S3.SS2.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\beta_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math>, <math alttext="\beta_{2}=0.95" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><msub id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2.2" xref="S3.SS2.p1.2.m2.1.1.2.2.cmml">β</mi><mn id="S3.SS2.p1.2.m2.1.1.2.3" xref="S3.SS2.p1.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><eq id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></eq><apply id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2.2">𝛽</ci><cn id="S3.SS2.p1.2.m2.1.1.2.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.2.3">2</cn></apply><cn id="S3.SS2.p1.2.m2.1.1.3.cmml" type="float" xref="S3.SS2.p1.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\beta_{2}=0.95</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95</annotation></semantics></math>, and a weight decay of 0.1.
We decay the learning rate to 10% of its initial value using a cosine scheduler without warm-up.
When pre-training on documents, we use a batch size of 256 documents and an initial learning rate of 3e-5.
During instruction-tuning on QA pairs, we use the same batch size of 256 QA pairs, but opt for a reduced initial learning rate of 5e-6 because the number of tokens in a single batch used for computing losses is lower.
The number of epochs varies depending on the setting and is detailed in the corresponding sections.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation Metrics</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">At inference time, we use greedy decoding to generate answers given questions as context, following the format in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F3" title="Figure 3 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;3</span></a>.
To evaluate the original Llama-2, we add 5 QA pairs as in-context exemplars to make sure it follows the QA format.
Since most questions are simple factoid questions and most answers are relatively short, we use exact match (EM) as our primary metric <cite class="ltx_cite ltx_citemacro_cite">Kwiatkowski et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib25" title="">2019</a>)</cite>, which measures whether the model’s output matches the gold answer exactly after normalization (e.g., remove articles and punctuations).
To assess longer responses and accommodate minor lexical differences, we also report answer recall, which assesses if the gold answer appears in the model’s output, and ROUGE-L, which measures the longest common subsequence between the model’s output and the gold answer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="261" id="S3.F4.g1" src="x5.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Different experimental settings examined in this paper. Each row represents a different experimental setting with a unique name and number, and each vertical section highlighted by a right-pointing light-blue triangle indicates a training phase. Models are assessed on test QA across all settings. Whenever multiple datasets are enclosed within a dashed square, they are mixed together during the training process.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning?</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Factual knowledge stored in the parameters of LLMs can be accessed and applied to answering questions through prompting without additional training <cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib5" title="">2020</a>); Petroni et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib37" title="">2019</a>); Jiang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib22" title="">2020</a>); Roberts et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib42" title="">2020</a>)</cite>.
With additional instruction-tuning (also known as supervised fine-tuning) on high-quality data <cite class="ltx_cite ltx_citemacro_cite">Sanh et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib44" title="">2022</a>); Wei et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib56" title="">2022</a>)</cite>, knowledge seems to be more effectively elicited from LLMs.
However, when LLMs correctly answer a question, the source of the knowledge is unclear due to the diversity of the pre-training data.
For instance, when answering the question “where is the world’s largest ice sheet located”, do LLMs derive their response by recalling and generalizing information from a seen document about the Antarctic ice sheet, or do they merely repeat answers from similar questions encountered in the training data?
This distinction is crucial, as the former scenario implies an ability to comprehend documents and effectively store knowledge within parameters in a way that can be elicited later, whereas the latter is mere rote memorization.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Several works have studied this problem and the predominant finding is that LMs struggle to answer questions about documents they have been trained on <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib54" title="">2021</a>); Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>.
It is important to note, however, that these experiments were mainly conducted using relatively small LMs such as BART, T5, or GPT-2 <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib54" title="">2021</a>); Jang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib20" title="">2022</a>); Hu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib16" title="">2023</a>)</cite>, using randomly initialized transformers <cite class="ltx_cite ltx_citemacro_cite">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>, or without instruction-tuning <cite class="ltx_cite ltx_citemacro_cite">Ovadia et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib36" title="">2023</a>)</cite>.
This makes us wonder <em class="ltx_emph ltx_font_italic" id="S4.p2.1.1">what are the actual limits of modern LLMs to absorb knowledge from new documents and answer questions about them using the standard continued pre-training followed by instruction-tuning recipe</em>.
In this section, we run extensive experiments using Llama-2 7B and 70B on <span class="ltx_text ltx_font_typewriter" id="S4.p2.1.2">Wiki2023-film</span> to test their limits.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Vanilla Continued Pre-training and Instruction-tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Experimental settings</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">We experiment with two standard settings and assess their performance by answering associated questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Continued pre-training: train on test documents without instruction-tuning (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➀).<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We found that LLMs struggle to adhere to the QA format after training on raw documents for multiple epochs. Therefore, we include a small set of QA pairs (64) during continued pre-training to prevent LLMs from forgetting the QA format.</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Standard instruction-tuning: train on both train and test documents before instruction-tuning on train QA pairs (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➁).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.2">We perform instruction-tuning for a single epoch since more epochs usually result in diminished performance.
For training on documents, we opt for multiple epochs (10/5 for a 7B/70B model), which allows for effective knowledge acquisition and remains affordable for corpora of moderate sizes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Experimental results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;1</span></a>, the relatively low performance of the original Llama-2 model (9.5%/17.2% for 7B/70B) indicates that most knowledge in the test documents is not included in the original pre-training corpus.
After continued pre-training on documents, performances increase to 27.2%/41.7%, indicating that LLMs can absorb some amount of knowledge.
Instruction-tuning further increases the performance to 30.3%/46.4%, confirming the effectiveness of this standard recipe.
This observation is different from <cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>, which demonstrates that instruction-tuning after pre-training is ineffective on a randomly initialized GPT-2-like transformer.
The difference probably arises because Llama-2, through its pre-training on diverse corpora comprising raw documents and QA data, has developed a certain degree of proficiency in extracting knowledge from its parameters via questions.
We also report the performance where the corresponding document is directly provided to Llama-2 as context (“open-book w/ doc” in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;1</span></a>).
The significant gap between closed-book and open-book settings suggests that retrieving knowledge from the parameters of LLMs is still challenging.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S4.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="563" id="S4.F4.sf1.g1" src="x6.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Training dynamics w/ (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➁) and w/o instruction-tuning (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➀). Reduction in perplexity consistently leads to improvement in QA accuracy, indicating that factual knowledge acquisition necessitates exhaustive loss minimization.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S4.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="513" id="S4.F4.sf2.g1" src="x7.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Training dynamics with different learning rates (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➀). After perplexity is minimized, larger learning rates usually lead to less overfitting to deceptive patterns in documents and better generalization when responding to questions.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>We vary the number of epochs (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf1" title="4(a) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(a)</span></a>) and learning rate (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf2" title="4(b) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(b)</span></a>) during continued pre-training to study the training dynamics of Llama-2 7B. The left axis is <span class="ltx_text" id="S4.F5.5.1" style="color:#000000;">QA accuracies</span> for test questions, measured by exact match. On the right axis, we display 2 metrics indicated by distinct colors: the <span class="ltx_text" id="S4.F5.6.2" style="color:#000000;">perplexity</span> of all tokens in the documents, and the <span class="ltx_text" id="S4.F5.7.3" style="color:#000000;">knowledge retention accuracy</span>, measured by QA accuracy on the Natural Questions dataset. We highlight situations where <span class="ltx_text" id="S4.F5.8.4" style="color:#000000;background-color:#F2F2FF;">perplexity of all document tokens is minimized to 1</span>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Analyzing the Training Dynamics: Perplexity and Generalization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">How does lower perplexity of documents lead to generalization to answering related questions?
We vary the number of epochs (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf1" title="4(a) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(a)</span></a>) and learning rate (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf2" title="4(b) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(b)</span></a>) for continued pre-training on documents and monitor three metrics to study the training dynamics.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Since we always decay the learning rate to 10% of its initial value, training for more epochs is not the same as continuing training from a checkpoint obtained after fewer epochs.</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Knowledge acquisition</span> QA accuracies on test questions measured by exact match.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Perplexity of documents</span> We compute perplexity (PPL) on all tokens within the documents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">Knowledge retention</span> We approximate the retention of accumulated knowledge during pre-training by assessing the QA accuracy on the Natural Questions (NQ) dataset. NQ was released in 2019, and primarily includes questions based on Wikipedia articles from that time.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.2.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T1.1.2.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.2.1">Llama-2 7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.1.2.1.3">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.3.1">Llama-2 70B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.1.1">Settings</span></th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.2.1">EM</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.3">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.3.1">Rec.</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.4.1">R-L</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.5">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.5.1">EM</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.6">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.6.1">Rec.</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.7.1">R-L</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="S4.T1.1.4.3.1">
<em class="ltx_emph ltx_font_italic" id="S4.T1.1.4.3.1.1">closed- and open-book performance before training</em></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.4.1">closed-book</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.2">9.5</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.3">10.0</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.5.4.4">21.2</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.5">17.2</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.6">18.1</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.7">31.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.6.5.1">open-book w/ doc</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.2">72.2</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.3">75.4</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.6.5.4">91.5</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.5">78.2</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.6">80.6</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.7">94.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="S4.T1.1.7.6.1">
<em class="ltx_emph ltx_font_italic" id="S4.T1.1.7.6.1.1">closed-book performance w/ standard methods</em></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.8.7.1">cont. pre-training  ➀</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.2">27.6</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.3">31.6</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.8.7.4">43.8</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.5">41.7</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.6">45.8</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.7">60.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.9.8.1"> +instruction-tuning  ➁</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.2">30.3</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.3">34.7</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.9.8.4">47.4</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.5">46.4</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.6">50.9</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.7">64.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.10.9.1">mix all data  ➃</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.9.2">39.4</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.9.3">44.6</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.10.9.4">56.7</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.9.5">57.1</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.9.6">63.4</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.9.7">72.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="S4.T1.1.11.10.1">
<em class="ltx_emph ltx_font_italic" id="S4.T1.1.11.10.1.1">closed-book performance w/ pre-instruction-tuning (PIT)</em></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.12.11.1">PIT (QA only)  ➄</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.2">28.6</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.3">32.7</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.12.11.4">45.2</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.5">49.7</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.6">53.7</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.7">67.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.1.1">PIT (QA <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">→</annotation></semantics></math> docs)  ➅</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.1.2">32.5</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.1.3">37.2</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.1.4">49.0</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.1.5">54.6</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.1.6">60.0</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.1.7">73.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.13.12.1">PIT  ➆</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.13.12.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.2.1">45.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.13.12.3">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.3.1">51.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T1.1.13.12.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.4.1">63.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.13.12.5">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.5.1">62.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.13.12.6">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.6.1">68.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.13.12.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.7.1">78.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of QA performance (%) between standard instruction-tuning and pre-instruction-tuning. The best results are in bold. Rec. is short for answer recall, and R-L refers to ROUGE-L.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Experiment results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1">As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf1" title="4(a) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(a)</span></a>, QA accuracy consistently improves as perplexity approaches one, indicating that <em class="ltx_emph ltx_font_italic" id="S4.I3.i1.p1.1.1">factual knowledge learning necessitates exhaustive loss minimization over all tokens</em>.
This contrasts with learning general skills, where overly optimizing leads to overfitting.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1">As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf1" title="4(a) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(a)</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf2" title="4(b) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(b)</span></a>, among all cases where LLMs have minimized perplexity on documents, cases trained with more epochs or larger learning rates typically exhibit superior QA performance. We hypothesize that <em class="ltx_emph ltx_font_italic" id="S4.I3.i2.p1.1.1">more aggressive training leads to less overfitting to deceptive patterns in documents and better generalization when responding to questions</em>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">In summary, lower perplexity does lead to stronger generalization when responding to questions, but it comes at the expense of forgetting previously acquired knowledge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.9">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.9.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T2.9.10.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T2.9.10.1.1.1">Setting names</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T2.9.10.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.9.10.1.2.1">Setting configurations</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.9.10.1.3">
<span class="ltx_text ltx_font_bold" id="S4.T2.9.10.1.3.1">EM</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.9.10.1.4">
<span class="ltx_text ltx_font_bold" id="S4.T2.9.10.1.4.1">Rec.</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.9.10.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.9.10.1.5.1">R-L</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.11.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5" id="S4.T2.9.11.2.1">
<em class="ltx_emph ltx_font_italic" id="S4.T2.9.11.2.1.1">baselines</em></th>
</tr>
<tr class="ltx_tr" id="S4.T2.9.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.12.3.1">continued pre-training  ➀</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.12.3.2">test doc</th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.12.3.3">27.6</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.12.3.4">31.6</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.12.3.5">43.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.1.2"> +instruction-tuning  ➁</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.1.1">train doc + test doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">→</annotation></semantics></math> train QA</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.3">30.3</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.4">34.7</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.5">47.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.2.2"> +instruction-tuning (w/o forget)  ➂</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.2.1">train doc + test doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.1.m1.1"><semantics id="S4.T2.2.2.1.m1.1a"><mo id="S4.T2.2.2.1.m1.1.1" stretchy="false" xref="S4.T2.2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><ci id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.1.m1.1d">→</annotation></semantics></math> train QA + test doc</th>
<td class="ltx_td ltx_align_right" id="S4.T2.2.2.3">30.2</td>
<td class="ltx_td ltx_align_right" id="S4.T2.2.2.4">34.1</td>
<td class="ltx_td ltx_align_right" id="S4.T2.2.2.5">46.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.3.2"> +instruction-tuning (w/o train doc)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.3.1">test doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.1.m1.1"><semantics id="S4.T2.3.3.1.m1.1a"><mo id="S4.T2.3.3.1.m1.1.1" stretchy="false" xref="S4.T2.3.3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><ci id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.1.m1.1d">→</annotation></semantics></math> train QA</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.3">27.1</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.4">30.7</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.5">42.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.13.4.1">weighted continued pre-training</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.13.4.2">test doc (weighted)</th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.13.4.3">27.7</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.13.4.4">32.7</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.13.4.5">43.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.4.2">adapted continued pre-training</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.4.1">train doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.4.4.1.m1.1"><semantics id="S4.T2.4.4.1.m1.1a"><mo id="S4.T2.4.4.1.m1.1.1" stretchy="false" xref="S4.T2.4.4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.m1.1b"><ci id="S4.T2.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.1.m1.1d">→</annotation></semantics></math> test doc</th>
<td class="ltx_td ltx_align_right" id="S4.T2.4.4.3">26.9</td>
<td class="ltx_td ltx_align_right" id="S4.T2.4.4.4">32.7</td>
<td class="ltx_td ltx_align_right" id="S4.T2.4.4.5">44.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.14.5.1">mix all data  ➃</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.14.5.2">train QA + train doc + test doc</th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.14.5.3">39.4</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.14.5.4">44.6</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.14.5.5">56.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.15.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5" id="S4.T2.9.15.6.1">
<em class="ltx_emph ltx_font_italic" id="S4.T2.9.15.6.1.1">various pre-instruction-tuning (PIT) methods and ablation studies</em></th>
</tr>
<tr class="ltx_tr" id="S4.T2.5.5">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.5.5.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.5.1" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.5.5.1.1" style="background-color:#808080;">train QA + train doc (3 epochs) <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.5.5.1.1.m1.1" style="background-color:#808080;"><semantics id="S4.T2.5.5.1.1.m1.1a"><mo id="S4.T2.5.5.1.1.m1.1.1" mathbackground="#808080" stretchy="false" xref="S4.T2.5.5.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.1.1.m1.1b"><ci id="S4.T2.5.5.1.1.m1.1.1.cmml" xref="S4.T2.5.5.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.1.1.m1.1d">→</annotation></semantics></math> test doc</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.5.5.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.5.5.3.1" style="background-color:#808080;">45.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.5.5.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.5.5.4.1" style="background-color:#808080;">51.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.5.5.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.5.5.5.1" style="background-color:#808080;">63.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.16.7">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.9.16.7.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S4.T2.9.16.7.2" style="background-color:#808080;">
<em class="ltx_emph ltx_font_italic" id="S4.T2.9.16.7.2.1" style="background-color:#808080;">ablation studies of the number of epochs</em><span class="ltx_text" id="S4.T2.9.16.7.2.2" style="background-color:#808080;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T2.9.17.8">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.17.8.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.17.8.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.17.8.2.1" style="background-color:#808080;"> 1 epoch</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.17.8.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.17.8.3.1" style="background-color:#808080;">33.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.17.8.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.17.8.4.1" style="background-color:#808080;">39.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.17.8.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.17.8.5.1" style="background-color:#808080;">50.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.18.9">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.18.9.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.18.9.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.18.9.2.1" style="background-color:#808080;"> 5 epochs</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.18.9.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.18.9.3.1" style="background-color:#808080;">45.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.18.9.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.18.9.4.1" style="background-color:#808080;">52.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.18.9.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.18.9.5.1" style="background-color:#808080;">63.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.19.10">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.19.10.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.19.10.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.19.10.2.1" style="background-color:#808080;"> 10 pochs</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.19.10.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.19.10.3.1" style="background-color:#808080;">46.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.19.10.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.19.10.4.1" style="background-color:#808080;">52.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.19.10.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.19.10.5.1" style="background-color:#808080;">61.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.20.11">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.9.20.11.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S4.T2.9.20.11.2" style="background-color:#808080;">
<em class="ltx_emph ltx_font_italic" id="S4.T2.9.20.11.2.1" style="background-color:#808080;">ablation studies of different learning mechanisms</em><span class="ltx_text" id="S4.T2.9.20.11.2.2" style="background-color:#808080;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T2.9.21.12">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.21.12.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.21.12.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.21.12.2.1" style="background-color:#808080;"> QA before doc (grouped)</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.21.12.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.21.12.3.1" style="background-color:#808080;">38.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.21.12.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.21.12.4.1" style="background-color:#808080;">43.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.21.12.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.21.12.5.1" style="background-color:#808080;">56.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.22.13">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.22.13.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.22.13.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.22.13.2.1" style="background-color:#808080;"> QA after doc (grouped)</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.22.13.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.22.13.3.1" style="background-color:#808080;">27.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.22.13.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.22.13.4.1" style="background-color:#808080;">31.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.22.13.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.22.13.5.1" style="background-color:#808080;">42.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.23.14">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.23.14.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.23.14.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.23.14.2.1" style="background-color:#808080;"> QA before doc (interleaved)</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.23.14.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.23.14.3.1" style="background-color:#808080;">45.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.23.14.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.23.14.4.1" style="background-color:#808080;">51.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.23.14.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.23.14.5.1" style="background-color:#808080;">64.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.24.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.24.15.1" rowspan="-10" style="background-color:#808080;">
<span class="ltx_text" id="S4.T2.9.24.15.1.1" style="background-color:#808080;">PIT  ➆</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.24.15.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.24.15.2.1" style="background-color:#808080;"> QA after doc (interleaved)</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.24.15.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.24.15.3.1" style="background-color:#808080;">43.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.24.15.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.24.15.4.1" style="background-color:#808080;">49.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.24.15.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.24.15.5.1" style="background-color:#808080;">61.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.7.7.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.7.7.3.1" style="background-color:#808080;">PIT–</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.7.7.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.7.7.2.2" style="background-color:#808080;">train QA + train doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.6.6.1.1.m1.1" style="background-color:#808080;"><semantics id="S4.T2.6.6.1.1.m1.1a"><mo id="S4.T2.6.6.1.1.m1.1.1" mathbackground="#808080" stretchy="false" xref="S4.T2.6.6.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.1.1.m1.1b"><ci id="S4.T2.6.6.1.1.m1.1.1.cmml" xref="S4.T2.6.6.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.1.1.m1.1d">→</annotation></semantics></math> train QA <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.7.7.2.2.m2.1" style="background-color:#808080;"><semantics id="S4.T2.7.7.2.2.m2.1a"><mo id="S4.T2.7.7.2.2.m2.1.1" mathbackground="#808080" stretchy="false" xref="S4.T2.7.7.2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.2.2.m2.1b"><ci id="S4.T2.7.7.2.2.m2.1.1.cmml" xref="S4.T2.7.7.2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.2.2.m2.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.2.2.m2.1d">→</annotation></semantics></math> test doc</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.7.7.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.7.7.4.1" style="background-color:#808080;">44.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.7.7.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.7.7.5.1" style="background-color:#808080;">51.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.7.7.6" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.7.7.6.1" style="background-color:#808080;">63.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.9.9.3" style="background-color:#CCCCCC;"><span class="ltx_text" id="S4.T2.9.9.3.1" style="background-color:#CCCCCC;">PIT++  ➇</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.9.9.2" style="background-color:#CCCCCC;"><span class="ltx_text" id="S4.T2.9.9.2.2" style="background-color:#CCCCCC;">train QA <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.8.8.1.1.m1.1" style="background-color:#CCCCCC;"><semantics id="S4.T2.8.8.1.1.m1.1a"><mo id="S4.T2.8.8.1.1.m1.1.1" mathbackground="#CCCCCC" stretchy="false" xref="S4.T2.8.8.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.1.1.m1.1b"><ci id="S4.T2.8.8.1.1.m1.1.1.cmml" xref="S4.T2.8.8.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.1.1.m1.1d">→</annotation></semantics></math> train QA + train doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.9.9.2.2.m2.1" style="background-color:#CCCCCC;"><semantics id="S4.T2.9.9.2.2.m2.1a"><mo id="S4.T2.9.9.2.2.m2.1.1" mathbackground="#CCCCCC" stretchy="false" xref="S4.T2.9.9.2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.2.2.m2.1b"><ci id="S4.T2.9.9.2.2.m2.1.1.cmml" xref="S4.T2.9.9.2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.2.2.m2.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.2.2.m2.1d">→</annotation></semantics></math> test doc</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.9.9.4" style="background-color:#CCCCCC;"><span class="ltx_text ltx_font_bold" id="S4.T2.9.9.4.1" style="background-color:#CCCCCC;">48.1<span class="ltx_text ltx_font_medium" id="S4.T2.9.9.4.1.1" style="background-color:#CCCCCC;"></span></span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.9.9.5" style="background-color:#CCCCCC;"><span class="ltx_text ltx_font_bold" id="S4.T2.9.9.5.1" style="background-color:#CCCCCC;">54.4<span class="ltx_text ltx_font_medium" id="S4.T2.9.9.5.1.1" style="background-color:#CCCCCC;"></span></span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.9.9.6" style="background-color:#CCCCCC;"><span class="ltx_text ltx_font_bold" id="S4.T2.9.9.6.1" style="background-color:#CCCCCC;">66.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison (%) of various pre-instruction-tuning methods and ablation studies to identify the key contributors to improved performance using Llama-2 7B. Different background colors indicate different pre-instruction-tuning methods. The best results are in bold.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Improving LLMs in Absorbing Knowledge from Documents</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The amount of knowledge elicited through the standard instruction-tuning is still limited, even though the perplexity of documents is minimized, a phenomenon we refer to as the “perplexity curse”.
Our next question is how can we improve the ability of LLMs to absorb knowledge from documents to mitigate the perplexity curse.
The main challenge is the gap between the way knowledge is presented in raw documents and how it is accessed through question-answering.
We found that QA pairs are generally straightforward, while documents tend to be more complex and cluttered, weaving many factual statements together in a more intricate manner.
Using <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F3" title="Figure 3 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;3</span></a> as an example, the answer to the question “who handled the editing of Oppenheimer” is included in a sentence in the middle of the article “Editing was handled by Jennifer Lame …”, which does not explicitly mention “Oppenheimer”.
During training, LLMs must understand the context and deduce that “editing” refers to “the editing of Oppenheimer” to effectively encode this knowledge in the parameters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite> studied this problem by training a randomly initialized GPT-2-like transformer from scratch on synthetic biographies and evaluated its ability to answer questions about the individuals.
They found that training on a mix of biographies and questions related to half of those biographies led to strong generalization when answering questions about the remaining half of biographies, which resembles setting  ➃ in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>.
In contrast, training on biographies and QA pairs sequentially failed.
However, the key contributor to the success remains uncertain because the data were blended together, and it is unclear how to apply this practically to absorb knowledge from new documents.
Inspired by our observation of the different difficulty levels between QA pairs and documents, and the finding from <cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>, we hypothesize that <em class="ltx_emph ltx_font_italic" id="S5.p2.1.1">it is beneficial to deliberately expose LLMs to instruction-tuning data before continued pre-training so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed.</em>
We refer to this as <span class="ltx_text ltx_font_bold" id="S5.p2.1.2">pre-instruction-tuning (PIT)</span> and study various implementations of PIT prior to continued learning (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS1" title="5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5.1</span></a>), followed by detailed ablations identifying the keys contributor to performance (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS2" title="5.2 Pre-instruction-tuning++ ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5.2</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS3" title="5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5.3</span></a>), and finally assess how well PIT performs across domains (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS4" title="5.4 Cross-domain Generalization ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5.4</span></a>).
We adhere to the hyperparameters outlined in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.SS2" title="3.2 Hyperparameters ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;3.2</span></a> and perform PIT for 3 epochs unless specified otherwise.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Variants of Pre-instruction-tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Pre-instruction-tuning w/ QA only</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">We start with exposing instruction-tuning data before continued pre-training on documents—training on topically related QA pairs before training on test documents (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➄).
This can be directly compared with the continued pre-training setting (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➀).
The intuition is that questions help LLMs recognize key types of information, enabling LLMs to focus on important information during pre-training on subsequent documents, even though the questions are not directly tied to the documents.
For example, training on a question like “who handled the editing of Oppenheimer” could help LLMs pay attention to screenwriters when training on new documents like “Barbie”.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;1</span></a>, this method outperforms continued pre-training, especially on larger LLMs (27.6%/41.7% <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><mo id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" stretchy="false" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.1.m1.1d">→</annotation></semantics></math> 28.6%/49.7% for 7B/70B).
The ablation that trains on QA data after training on documents (“instruction-tuning w/o train doc” in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;2</span></a>) is ineffective, confirming the importance of training on questions as a warm-up before encoding documents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Pre-instruction-tuning on QA and documents sequentially</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">Our second implementation trains on QA and associated documents sequentially (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➅), with the intuition that the ability to absorb knowledge from documents can be strengthened if an LLM is trained on the complex documents after it has grasped the associated simpler QA pairs.
For instance, if an LLM has already learned that “Jennifer Lame” is the answer to “who handled the editing of Oppenheimer”, training on the document “Editing was handled by Jennifer Lame” can more efficiently refine its storage of knowledge in its parameters.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;1</span></a>, PIT on QA pairs and documents sequentially surpasses the QA-only variant (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➄) and standard instruction-tuning (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➁) (30.3%/46.4% <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px2.p1.1.m1.1a"><mo id="S5.SS1.SSS0.Px2.p1.1.m1.1.1" stretchy="false" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.1.m1.1d">→</annotation></semantics></math> 32.5%/54.6% for 7B/70B), demonstrating its effectiveness.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Pre-instruction-tuning </h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">The effectiveness of PIT depends on ensuring that the associated QA pairs are already learned before encoding the respective documents.
However, we observed that after training on documents (train doc in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➅), the accuracy for corresponding questions (train QA in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➅) dropped from almost perfect to 30%, indicating severe forgetting.
To fix this, we train on the associated QA pairs and documents together (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➆).
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;1</span></a>, this significantly improves the performance, outperforming all other approaches, including mixing all data together (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➃), by a large margin (39.4%/57.1% <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px3.p1.1.m1.1a"><mo id="S5.SS1.SSS0.Px3.p1.1.m1.1.1" stretchy="false" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px3.p1.1.m1.1b"><ci id="S5.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px3.p1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px3.p1.1.m1.1d">→</annotation></semantics></math> 45.5%/62.7% for 7B/70B).
Training on both QA pairs and documents prevents forgetting, but it also obscures how the learning process works.
It is unclear whether LLMs grasp QA pairs before encoding knowledge from documents, or if it works the other way around.
In the following section, we deliberately arrange the order of QA pairs and documents during training to examine this, which leads us to propose an improved version of PIT.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="445" id="S5.F6.g1" src="x8.png" width="829">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Different arrangements between QA pairs and corresponding documents. The ellipses represent other examples.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Pre-instruction-tuning++</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We first study how the performance varies with different numbers of epochs.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;2</span></a>, training for 1 epoch is insufficient, and the performance of 3, 5, or 10 epochs is similar.
We fix the number of epochs to 3 and arrange the order of QA pairs and corresponding documents as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.F6" title="Figure 6 ‣ Pre-instruction-tuning ‣ 5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;6</span></a>.
The interleaved arrangement cycles through all the data 3 times, ensuring that in each epoch, questions either precede or follow their associated documents.
On the other hand, the grouped arrangement clusters each example’s 3 appearances together, guaranteeing that the repeated questions are positioned either before or after their respective repeated documents.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;2</span></a>, positioning QA pairs before corresponding documents achieves better performance in both grouped and interleaved arrangements, indicating that during PIT, the learning mechanism prioritizes understanding how to access knowledge before learning to absorb information from the more complex and information-dense documents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Based on this, we propose an improved variant called pre-instruction-tuning++, which trains exclusively on QA pairs to understand patterns of knowledge access, then progresses to training on a combination of QA and document data to align knowledge access through questions and knowledge encoding from documents (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➇).
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;2</span></a>, PIT++ significantly outperforms PIT (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➆) from 45.4% to 48.1%, while training on QA data after on the mix (PIT– in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;2</span></a>) does not yield additional benefits.
This reinforces our hypothesis that understanding how knowledge is accessed aids in absorbing knowledge from documents, and therefore, should be prioritized.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Studies</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Standard instruction-tuning is inferior not due to forgetting</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p1.1">A drawback of standard instruction-tuning is that knowledge in test documents might be forgotten after training on QA pairs (a phenomenon also known as the “alignment tax” <cite class="ltx_cite ltx_citemacro_cite">Ouyang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib35" title="">2022</a>)</cite>).
To show that the lower performance of standard instruction-tuning is not due to forgetting, we add a setting where we mix train QA with test documents during instruction-tuning to prevent forgetting (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➂).
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;2</span></a>, this does not help, confirming our hypothesis.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Pre-instruction-tuning is not simply upweighting salient tokens from documents</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p1.1">We include an ablation inspired by <cite class="ltx_cite ltx_citemacro_citet">Hu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib16" title="">2023</a>)</cite> which upweights tokens when pre-training on documents to focus on salient information.
We assign a weight of 1.0 to tokens in documents that are included in the answers (e.g., “Jennifer Lame” in the sentence “Editing was handled by Jennifer Lame”), and assign a lower weight of 0.5 to other tokens.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;2</span></a>, this weighted continued pre-training is ineffective, confirming our hypothesis.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T3.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S5.T3.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">Llama-2 7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T3.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">Llama-2 70B</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.1.1">Settings</span></th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.2.2.2">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.2.1">EM</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.2.2.3">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.3.1">Rec.</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.4.1">R-L</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.2.2.5">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.5.1">EM</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.2.2.6">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.6.1">Rec.</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.7.1">R-L</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="S5.T3.1.3.3.1">
<em class="ltx_emph ltx_font_italic" id="S5.T3.1.3.3.1.1">standard instruction-tuning</em>  ➁</th>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.4.4.1">in-domain</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.2">30.3</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.3">34.7</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.1.4.4.4">47.4</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.5">46.4</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.6">50.9</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.7">64.1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.5.5.1">cross-domain</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.2">23.6</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.3">28.2</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.1.5.5.4">38.4</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.5">42.8</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.6">49.7</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.7">58.5</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="S5.T3.1.6.6.1">
<em class="ltx_emph ltx_font_italic" id="S5.T3.1.6.6.1.1">pre-instruction-tuning</em>  ➆</th>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.7.7.1">in-domain</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.2">45.4</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.3">51.2</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.1.7.7.4">63.2</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.5">62.7</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.6">68.6</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.7">78.8</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T3.1.8.8.1">cross-domain</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T3.1.8.8.2">36.9</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T3.1.8.8.3">43.2</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S5.T3.1.8.8.4">54.9</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T3.1.8.8.5">55.2</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T3.1.8.8.6">66.7</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T3.1.8.8.7">74.0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>In-domain and cross-domain PIT.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1">Settings</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1">EM</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1">Rec.</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.4.1">R-L</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S5.T4.1.2.2.1">
<em class="ltx_emph ltx_font_italic" id="S5.T4.1.2.2.1.1">generalization to the biography dataset <span class="ltx_text ltx_font_typewriter" id="S5.T4.1.2.2.1.1.1">bioS</span> </em></th>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.3.3.1">closed-book</th>
<td class="ltx_td ltx_align_right" id="S5.T4.1.3.3.2">2.9</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.3.3.3">2.9</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.3.3.4">11.0</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.4.4.1">open-book w/ doc</th>
<td class="ltx_td ltx_align_right" id="S5.T4.1.4.4.2">95.2</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.4.4.3">95.4</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.4.4.4">95.6</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.5.5.1">continued pre-training  ➀</th>
<td class="ltx_td ltx_align_right" id="S5.T4.1.5.5.2">29.6</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.5.5.3">29.8</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.5.5.4">38.7</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.6.6.1">pre-instruction-tuning  ➆</th>
<td class="ltx_td ltx_align_right" id="S5.T4.1.6.6.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.6.6.2.1">58.1</span></td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.6.6.3"><span class="ltx_text ltx_font_bold" id="S5.T4.1.6.6.3.1">58.4</span></td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.6.6.4"><span class="ltx_text ltx_font_bold" id="S5.T4.1.6.6.4.1">61.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S5.T4.1.7.7.1">
<em class="ltx_emph ltx_font_italic" id="S5.T4.1.7.7.1.1">generalization to questions by real users from Google </em></th>
</tr>
<tr class="ltx_tr" id="S5.T4.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.8.8.1">standard instruction-tuning  ➁</th>
<td class="ltx_td ltx_align_right" id="S5.T4.1.8.8.2">21.5</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.8.8.3">30.1</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.8.8.4">36.8</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.1.9.9.1">pre-instruction-tuning  ➆</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T4.1.9.9.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.9.9.2.1">29.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T4.1.9.9.3"><span class="ltx_text ltx_font_bold" id="S5.T4.1.9.9.3.1">35.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T4.1.9.9.4"><span class="ltx_text ltx_font_bold" id="S5.T4.1.9.9.4.1">48.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Generalization of the Llama-2 7B model trained with pre-instruction-tuning.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Cross-domain Generalization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We validated the effectiveness of PIT by training and evaluation on data from the same domain (<span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.1">Wiki2023-film</span>).
<em class="ltx_emph ltx_font_italic" id="S5.SS4.p1.1.2">Can PIT make LLMs better at absorbing knowledge from documents of a different domain?</em>
To this end, we follow the cross-domain setting outlined in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F2" title="Figure 2 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;2</span></a>—training on other domains (<span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.3">Wiki2023-other-train</span>) and testing on the film domain (<span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.4">Wiki2023-film-test</span>).
The results of standard instruction-tuning and PIT, in both in-domain and cross-domain settings, are detailed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.T3" title="Table 3 ‣ Pre-instruction-tuning is not simply upweighting salient tokens from documents ‣ 5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;3</span></a>.
Even though it is not as effective as the in-domain counterparts, cross-domain PIT still significantly outperforms instruction-tuning, demonstrating that it can generalize across different domains.
This finding sheds light on the potential to scale this method up to a broader range of documents and instructions for more robust generalization.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">We also evaluate the effectiveness of PIT in two other scenarios: (1) when applied to non-Wikipedia documents, and (2) when addressing questions asked by real users.
For the first scenario, we take the Llama-2 7B model trained with PIT on <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p2.1.1">2023Wiki-other</span> and further train it on biographies synthesized in <cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite> (<span class="ltx_text ltx_font_typewriter" id="S5.SS4.p2.1.2">bioS</span>).
Then, we evaluate based on questions about the individuals.
For the second scenario, we manually search Google using questions generated by LLMs from <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p2.1.3">Wiki2023-film-test</span>, collect a total of 93 similar questions from real users by leveraging Google’s “People Also Ask” feature, and then evaluate Llama-2 7B on these questions.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.T4" title="Table 4 ‣ Pre-instruction-tuning is not simply upweighting salient tokens from documents ‣ 5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;4</span></a>, PIT outperforms baselines in both scenarios, demonstrating its generalization ability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Continual Knowledge Acquisition</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Several works have studied whether LMs can answer questions about information in documents they have been trained on.
<cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib54" title="">2021</a>); Jang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib20" title="">2022</a>); Hu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib16" title="">2023</a>)</cite> use relatively small LMs such as BART <cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib27" title="">2020a</a>)</cite>, T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib41" title="">2020</a>)</cite>, or GPT-2 <cite class="ltx_cite ltx_citemacro_cite">Radford et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib39" title="">2019</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Ovadia et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib36" title="">2023</a>)</cite> focus on the comparison between RAG and continued pre-training approaches without using instruction-tuning.
<cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib64" title="">b</a>)</cite> examine this problem from a similar angle as ours using a GPT-2-like transformer trained from scratch on synthetic biographies and fine-tuned on QA pairs related to the individuals.
They examined a mixed training setting on both biographies and QA pairs, which is our major motivation to study different strategies to incorporate QA data before continued pre-training.
Other works study adapting LLMs to new domains via various strategies <cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib59" title="">2023</a>); Cheng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib8" title="">2023</a>); Han et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib14" title="">2023</a>); Wu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib57" title="">2023</a>); Nguyen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib33" title="">2023</a>); Zhao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib61" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Instruction-tuning or Alignment</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Instruction-tuning (also known as supervised fine-tuning) on high-quality annotated data <cite class="ltx_cite ltx_citemacro_cite">Sanh et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib44" title="">2022</a>); Wei et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib56" title="">2022</a>); Mishra et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib31" title="">2022</a>); Iyer et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib18" title="">2022</a>); Kopf et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib24" title="">2023</a>); Zhou et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib62" title="">2023</a>); Sun et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib47" title="">2023b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib46" title="">a</a>)</cite> and/or data generated by proprietary models <cite class="ltx_cite ltx_citemacro_cite">Taori et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib48" title="">2023</a>); Chiang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib9" title="">2023</a>); Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib55" title="">2023b</a>); Ivison et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib17" title="">2023</a>)</cite>, or alignment with reinforcement learning from human feedback (RLHF) or direct preference optimization (DPO) <cite class="ltx_cite ltx_citemacro_cite">Ouyang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib35" title="">2022</a>); Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib52" title="">2023b</a>); Rafailov et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib40" title="">2023</a>); Tian et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib49" title="">2023</a>)</cite> has been a central topic recently because it elicits knowledge from LLMs and enhances various abilities to handle questions from users.
We focus on factuality and study the best way to perform instruction-tuning to elicit factual knowledge from LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Analyzing the Training Dynamics of LMs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Many works study the training dynamics of LMs from different perspectives.
<cite class="ltx_cite ltx_citemacro_citet">Carlini et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib6" title="">2022</a>)</cite> quantifies memorization across model sizes and the frequency of data duplication.
<cite class="ltx_cite ltx_citemacro_citet">Tirumala et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib50" title="">2022</a>)</cite> finds that larger LMs memorize training data faster with less overfitting.
<cite class="ltx_cite ltx_citemacro_citet">Xia et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib58" title="">2023</a>)</cite> show that perplexity is more predictive of model behaviors than other factors.
<cite class="ltx_cite ltx_citemacro_citet">Dery et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib11" title="">2022</a>)</cite> studies end-task aware pre-training using classification tasks and RoBERTa models.
Our work differs in that we specifically focus on the capacity of recalling and generalizing information from a seen document to answer questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Retrieval-augmented Generation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Retrieval-augmented generation (RAG) is a widely used approach to incorporate new knowledge into LLMs by augmenting fixed LLMs with retrieved information from external sources <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib7" title="">2017</a>); Guu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib13" title="">2020</a>); Lewis et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib28" title="">2020b</a>); Borgeaud et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib4" title="">2022</a>); Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib53" title="">2023a</a>); Alon et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib1" title="">2022</a>); He et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib15" title="">2021</a>); Sachan et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib43" title="">2021</a>); Izacard et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib19" title="">2023</a>); Lee et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib26" title="">2022</a>); Jiang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib21" title="">2022</a>); Shi et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib45" title="">2023</a>); Jiang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib23" title="">2023</a>); Asai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib2" title="">2023</a>); Nakano et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib32" title="">2021</a>); Qin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib38" title="">2023</a>); Lin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib29" title="">2023</a>)</cite>.
While RAG is effective in reducing hallucinations commonly experienced when relying solely on knowledge stored in parameters, its retrieval and generation process adds extra latency and complexity.
In contrast, continued pre-training to store knowledge in parameters and utilizing the stored knowledge to answer questions in a closed-book manner are simpler and faster at inference time.
Enhancing this capability is also scientifically significant, as it represents a fundamental step in employing LLMs as dependable assistants for accessing information.
Therefore, this paper focuses on exploring parametric approaches.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We study the best way of continued training on new documents with the goal of later eliciting factual knowledge.
We propose pre-instruction-tuning that learns how knowledge is accessed via QA pairs prior to encoding knowledge from documents.
Extensive experiments demonstrate the superiority of pre-instruction-tuning versus standard instruction-tuning.
Future directions include scaling this method up to a broader range of documents and instructions for more robust generalization.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">The <span class="ltx_text ltx_font_typewriter" id="Sx1.p1.1.1">Wiki2023</span> dataset provides a relatively clean testbed for studying continual knowledge acquisition.
However, its scope is limited to Wikipedia, which restricts the trained models’ adaptability to other sources like web pages from Common Crawl or scientific documents from arXiv.
We focus on eliciting factual knowledge with instruction-tuning on QA data in this paper.
The effectiveness of pre-instruction-tuning with different types of data for enhancing other skills like reasoning or comprehension is something that needs to be explored in future studies.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">We would like to thank Zeyuan Allen-Zhu, Zexuan Zhong, Shuyan Zhou, Frank F. Xu, Qian Liu, and Ruohong Zhang for their help with the experiments and constructive feedback.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alon et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Uri Alon, Frank&nbsp;F. Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022.

</span>
<span class="ltx_bibblock">Neuro-symbolic language modeling with automaton-augmented retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">International Conference on Machine Learning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.11511" title="">Self-rag: Learning to retrieve, generate, and critique through self-reflection</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">CoRR</em>, abs/2310.11511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berglund et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa&nbsp;Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.12288" title="">The reversal curse: Llms trained on "a is b" fail to learn "b is a"</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CoRR</em>, abs/2309.12288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van&nbsp;den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de&nbsp;Las&nbsp;Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack&nbsp;W. Rae, Erich Elsen, and Laurent Sifre. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v162/borgeaud22a.html" title="">Improving language models by retrieving from trillions of tokens</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, volume 162 of <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2">Proceedings of Machine Learning Research</em>, pages 2206–2240. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramèr, and Chiyuan Zhang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2202.07646" title="">Quantifying memorization across neural language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">CoRR</em>, abs/2202.07646.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-1171" title="">Reading wikipedia to answer open-domain questions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers</em>, pages 1870–1879. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.09530" title="">Adapting large language models via reading comprehension</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">CoRR</em>, abs/2309.09530.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi&nbsp;Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph&nbsp;E. Gonzalez, Ion Stoica, and Eric&nbsp;P. Xing. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi&nbsp;Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew&nbsp;M. Dai, Thanumalayan&nbsp;Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2204.02311" title="">Palm: Scaling language modeling with pathways</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">CoRR</em>, abs/2204.02311.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dery et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lucio&nbsp;M. Dery, Paul Michel, Ameet Talwalkar, and Graham Neubig. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=2bO2x8NAIMB" title="">Should we be pre-training? an argument for end-task aware training as an alternative</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini Team (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemini Team. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2312.11805" title="">Gemini: A family of highly capable multimodal models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2002.08909" title="">REALM: retrieval-augmented language model pre-training</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">CoRR</em>, abs/2002.08909.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tianyu Han, Lisa&nbsp;C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno&nbsp;K. Bressem. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2304.08247" title="">Medalpaca - an open-source collection of medical conversational AI models and training data</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">CoRR</em>, abs/2304.08247.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2021.

</span>
<span class="ltx_bibblock">Efficient nearest neighbor language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nathan Hu, Eric Mitchell, Christopher&nbsp;D. Manning, and Chelsea Finn. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.emnlp-main.268" title="">Meta-learning online adaptation of language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 4418–4432. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivison et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah&nbsp;A. Smith, Iz&nbsp;Beltagy, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.10702" title="">Camels in a changing climate: Enhancing LM adaptation with tulu 2</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">CoRR</em>, abs/2311.10702.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyer et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Srinivasan Iyer, Xi&nbsp;Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit&nbsp;Singh Koura, Xian Li, Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2212.12017" title="">OPT-IML: scaling language model instruction meta learning through the lens of generalization</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">CoRR</em>, abs/2212.12017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick S.&nbsp;H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v24/23-0037.html" title="">Atlas: Few-shot learning with retrieval augmented language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">J. Mach. Learn. Res.</em>, 24:251:1–251:43.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley&nbsp;Jungkyu Choi, and Minjoon Seo. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=vfsRB5MImo9" title="">Towards continual knowledge learning of language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki, Haibo Ding, Jamie Callan, and Graham Neubig. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.EMNLP-MAIN.149" title="">Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 2336–2349. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengbao Jiang, Frank&nbsp;F. Xu, Jun Araki, and Graham Neubig. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00324" title="">How can we know what language models know</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Trans. Assoc. Comput. Linguistics</em>, 8:423–438.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengbao Jiang, Frank&nbsp;F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.emnlp-main.495" title="">Active retrieval augmented generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 7969–7992. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopf et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi&nbsp;Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen&nbsp;Minh Duc, Oliver Stanley, Rich’ard Nagyfi, ES&nbsp;Shahul, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:258179434" title="">Openassistant conversations - democratizing large language model alignment</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">ArXiv</em>, abs/2304.07327.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur&nbsp;P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew&nbsp;M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00276" title="">Natural questions: a benchmark for question answering research</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Trans. Assoc. Comput. Linguistics</em>, 7:452–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher&nbsp;D. Manning, and Kyoung-Gu Woo. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.EMNLP-MAIN.198" title="">You only need one model for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 3047–3060. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.703" title="">BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>, pages 7871–7880. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Patrick S.&nbsp;H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="">Retrieval-augmented generation for knowledge-intensive NLP tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.01352" title="">RA-DIT: retrieval-augmented dual instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">CoRR</em>, abs/2310.01352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">Decoupled weight decay regularization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.ACL-LONG.244" title="">Cross-task generalization via natural language crowdsourcing instructions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 3470–3487. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu&nbsp;Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2112.09332" title="">Webgpt: Browser-assisted question-answering with human feedback</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">CoRR</em>, abs/2112.09332.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tuan&nbsp;Dung Nguyen, Yuan-Sen Ting, Ioana Ciuca, Charlie O’Neill, Ze-Chang Sun, Maja Jablonska, Sandor Kruk, Ernest Perkowski, Jack&nbsp;W. Miller, Jason Li, Josh Peek, Kartheik Iyer, Tomasz Rózanski, Pranav Khetarpal, Sharaf Zaman, David Brodrick, Sergio J.&nbsp;Rodríguez Méndez, Thang Bui, Alyssa Goodman, Alberto Accomazzi, Jill&nbsp;P. Naiman, Jesse Cranney, Kevin Schawinski, and UniverseTBD. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.06126" title="">Astrollama: Towards specialized foundation models in astronomy</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">CoRR</em>, abs/2309.06126.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2303.08774" title="">GPT-4 technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">CoRR</em>, abs/2303.08774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul&nbsp;F. Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2203.02155" title="">Training language models to follow instructions with human feedback</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">CoRR</em>, abs/2203.02155.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ovadia et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2312.05934" title="">Fine-tuning or retrieval? comparing knowledge injection in llms</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">CoRR</em>, abs/2312.05934.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S.&nbsp;H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander&nbsp;H. Miller. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1250" title="">Language models as knowledge bases?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>, pages 2463–2473. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu&nbsp;Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2305.06849" title="">Webcpm: Interactive web search for chinese long-form question answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">CoRR</em>, abs/2305.06849.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" title="">Language models are unsupervised multitask learners</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">OpenAI Blog</em>, 1(8).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher&nbsp;D. Manning, and Chelsea Finn. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.18290" title="">Direct preference optimization: Your language model is secretly a reward model</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">CoRR</em>, abs/2305.18290.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v21/20-074.html" title="">Exploring the limits of transfer learning with a unified text-to-text transformer</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">J. Mach. Learn. Res.</em>, 21:140:1–140:67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.437" title="">How much knowledge can you pack into the parameters of a language model?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, pages 5418–5426. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sachan et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Devendra&nbsp;Singh Sachan, Siva Reddy, William&nbsp;L. Hamilton, Chris Dyer, and Dani Yogatama. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2021/hash/da3fde159d754a2555eaa198d2d105b2-Abstract.html" title="">End-to-end training of multi-document reader and retriever for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual</em>, pages 25968–25981.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen&nbsp;H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M&nbsp;Saiful Bari, Canwen Xu, Urmish Thakker, Shanya&nbsp;Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal&nbsp;V. Nayak, Debajyoti Datta, Jonathan Chang, Mike&nbsp;Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng&nbsp;Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason&nbsp;Alan Fries, Ryan Teehan, Teven&nbsp;Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander&nbsp;M. Rush. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=9Vrb9D0WI4" title="">Multitask prompted training enables zero-shot task generalization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2301.12652" title="">REPLUG: retrieval-augmented black-box language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CoRR</em>, abs/2301.12652.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David&nbsp;D. Cox, Yiming Yang, and Chuang Gan. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.05910" title="">SALMON: self-alignment with principle-following reward models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">CoRR</em>, abs/2310.05910.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David&nbsp;D. Cox, Yiming Yang, and Chuang Gan. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.03047" title="">Principle-driven self-alignment of language models from scratch with minimal human supervision</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">CoRR</em>, abs/2305.03047.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher&nbsp;D. Manning, and Chelsea Finn. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.08401" title="">Fine-tuning language models for factuality</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">CoRR</em>, abs/2311.08401.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tirumala et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kushal Tirumala, Aram&nbsp;H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2022/hash/fa0509f4dab6807e2cb465715bf2d249-Abstract-Conference.html" title="">Memorization without overfitting: Analyzing the training dynamics of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2302.13971" title="">Llama: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">CoRR</em>, abs/2302.13971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">CoRR</em>, abs/2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi&nbsp;Dong, Oleksii Kuchaiev, Bo&nbsp;Li, Chaowei Xiao, Anima Anandkumar, and Bryan Catanzaro. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.emnlp-main.482" title="">Shall we pretrain autoregressive language models with retrieval? A comprehensive study</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 7763–7786. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Cunxiang Wang, Pai Liu, and Yue Zhang. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.ACL-LONG.251" title="">Can generative pre-trained language models serve as knowledge bases for closed-book qa?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>, pages 3241–3251. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi&nbsp;Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah&nbsp;A. Smith, Iz&nbsp;Beltagy, and Hannaneh Hajishirzi. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2306.04751" title="">How far can camels go? exploring the state of instruction tuning on open resources</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">CoRR</em>, abs/2306.04751.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent&nbsp;Y. Zhao, Kelvin Guu, Adams&nbsp;Wei Yu, Brian Lester, Nan Du, Andrew&nbsp;M. Dai, and Quoc&nbsp;V. Le. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=gEZrGCozdqR" title="">Finetuned language models are zero-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya&nbsp;Zhang, Yanfeng Wang, and Weidi Xie. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.14454" title="">Pmc-llama: Towards building open-source language models for medicine</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi&nbsp;Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Veselin Stoyanov. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.ACL-LONG.767" title="">Training trajectories of language models across scales</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 13711–13738. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, and Hongxia Yang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.10614" title="">A self-enhancement approach for domain-specific chatbot training via knowledge mining and digest</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">CoRR</em>, abs/2311.10614.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi&nbsp;Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit&nbsp;Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">ArXiv</em>, abs/2205.01068.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wayne&nbsp;Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2303.18223" title="">A survey of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">CoRR</em>, abs/2303.18223.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.11206" title="">LIMA: less is more for alignment</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">CoRR</em>, abs/2305.11206.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu and Li (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zeyuan&nbsp;Allen Zhu and Yuanzhi Li. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.14316" title="">Physics of language models: Part 3.1, knowledge storage and extraction</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">CoRR</em>, abs/2309.14316.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu and Li (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zeyuan&nbsp;Allen Zhu and Yuanzhi Li. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.14402" title="">Physics of language models: Part 3.2, knowledge manipulation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">CoRR</em>, abs/2309.14402.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
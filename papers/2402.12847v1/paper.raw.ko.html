<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Instruction-tuned Language Models are Better Knowledge Learners</title>
<!--Generated on Tue Feb 20 09:17:20 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2402.12847v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2402.12847v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2402.12847v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2402.12847v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S1" title="1 Introduction ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S2" title="2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Building a Dataset to Study Continual Knowledge Acquisition</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S2.SS1" title="2.1 Wiki2023 Document Corpus ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Wiki2023 Document Corpus</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S2.SS2" title="2.2 Wiki2023 Question-answer Pairs ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Wiki2023 Question-answer Pairs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S2.SS3" title="2.3 Splits ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Splits</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S3" title="3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Settings</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S3.SS1" title="3.1 Objectives ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Objectives</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S3.SS2" title="3.2 Hyperparameters ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S3.SS3" title="3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluation Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4" title="4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning?</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4.SS1" title="4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Vanilla Continued Pre-training and Instruction-tuning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4.SS1.SSS0.Px1" title="Experimental settings ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Experimental settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4.SS1.SSS0.Px2" title="Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Experimental results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4.SS2" title="4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Analyzing the Training Dynamics: Perplexity and Generalization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S4.SS2.SSS0.Px1" title="Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Experiment results</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5" title="5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Improving LLMs in Absorbing Knowledge from Documents</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS1" title="5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Variants of Pre-instruction-tuning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS1.SSS0.Px1" title="Pre-instruction-tuning w/ QA only ‣ 5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Pre-instruction-tuning w/ QA only</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS1.SSS0.Px2" title="Pre-instruction-tuning on QA and documents sequentially ‣ 5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Pre-instruction-tuning on QA and documents sequentially</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS1.SSS0.Px3" title="Pre-instruction-tuning ‣ 5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Pre-instruction-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS2" title="5.2 Pre-instruction-tuning++ ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Pre-instruction-tuning++</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS3" title="5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS3.SSS0.Px1" title="Standard instruction-tuning is inferior not due to forgetting ‣ 5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Standard instruction-tuning is inferior not due to forgetting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS3.SSS0.Px2" title="Pre-instruction-tuning is not simply upweighting salient tokens from documents ‣ 5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title">Pre-instruction-tuning is not simply upweighting salient tokens from documents</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S5.SS4" title="5.4 Cross-domain Generalization ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Cross-domain Generalization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S6" title="6 Related Work ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S6.SS1" title="6.1 Continual Knowledge Acquisition ‣ 6 Related Work ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Continual Knowledge Acquisition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S6.SS2" title="6.2 Instruction-tuning or Alignment ‣ 6 Related Work ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Instruction-tuning or Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S6.SS3" title="6.3 Analyzing the Training Dynamics of LMs ‣ 6 Related Work ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Analyzing the Training Dynamics of LMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S6.SS4" title="6.4 Retrieval-augmented Generation ‣ 6 Related Work ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Retrieval-augmented Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#S7" title="7 Conclusion ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
<li>failed: anyfontsize</li>
<li>failed: cuted</li>
<li>failed: color-edits</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2402.12847v1 [cs.CL] 20 Feb 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Instruction-tuned Language Models are Better Knowledge Learners</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhengbao Jiang<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mn id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><cn id="id1.1.m1.1.1.1.cmml" type="integer" xref="id1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Zhiqing Sun<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mn id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><cn id="id2.2.m2.1.1.1.cmml" type="integer" xref="id2.2.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Weijia Shi<math alttext="{}^{1,3}" class="ltx_Math" display="inline" id="id3.3.m3.2"><semantics id="id3.3.m3.2a"><msup id="id3.3.m3.2.2" xref="id3.3.m3.2.2.cmml"><mi id="id3.3.m3.2.2a" xref="id3.3.m3.2.2.cmml"></mi><mrow id="id3.3.m3.2.2.2.4" xref="id3.3.m3.2.2.2.3.cmml"><mn id="id3.3.m3.1.1.1.1" xref="id3.3.m3.1.1.1.1.cmml">1</mn><mo id="id3.3.m3.2.2.2.4.1" xref="id3.3.m3.2.2.2.3.cmml">,</mo><mn id="id3.3.m3.2.2.2.2" xref="id3.3.m3.2.2.2.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.2b"><apply id="id3.3.m3.2.2.cmml" xref="id3.3.m3.2.2"><list id="id3.3.m3.2.2.2.3.cmml" xref="id3.3.m3.2.2.2.4"><cn id="id3.3.m3.1.1.1.1.cmml" type="integer" xref="id3.3.m3.1.1.1.1">1</cn><cn id="id3.3.m3.2.2.2.2.cmml" type="integer" xref="id3.3.m3.2.2.2.2">3</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.2c">{}^{1,3}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.2d">start_FLOATSUPERSCRIPT 1 , 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Pedro Rodriguez<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><msup id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mi id="id4.4.m4.1.1a" xref="id4.4.m4.1.1.cmml"></mi><mn id="id4.4.m4.1.1.1" xref="id4.4.m4.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><cn id="id4.4.m4.1.1.1.cmml" type="integer" xref="id4.4.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Chunting Zhou<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id5.5.m5.1"><semantics id="id5.5.m5.1a"><msup id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml"><mi id="id5.5.m5.1.1a" xref="id5.5.m5.1.1.cmml"></mi><mn id="id5.5.m5.1.1.1" xref="id5.5.m5.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><apply id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1"><cn id="id5.5.m5.1.1.1.cmml" type="integer" xref="id5.5.m5.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id5.5.m5.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"> <span class="ltx_text ltx_font_bold" id="id9.9.4">Graham Neubig<math alttext="{}^{2}" class="ltx_Math" display="inline" id="id6.6.1.m1.1"><semantics id="id6.6.1.m1.1a"><msup id="id6.6.1.m1.1.1" xref="id6.6.1.m1.1.1.cmml"><mi id="id6.6.1.m1.1.1a" xref="id6.6.1.m1.1.1.cmml"></mi><mn id="id6.6.1.m1.1.1.1" mathvariant="normal" xref="id6.6.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id6.6.1.m1.1b"><apply id="id6.6.1.m1.1.1.cmml" xref="id6.6.1.m1.1.1"><cn id="id6.6.1.m1.1.1.1.cmml" type="integer" xref="id6.6.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id6.6.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Xi Victoria Lin<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id7.7.2.m2.1"><semantics id="id7.7.2.m2.1a"><msup id="id7.7.2.m2.1.1" xref="id7.7.2.m2.1.1.cmml"><mi id="id7.7.2.m2.1.1a" xref="id7.7.2.m2.1.1.cmml"></mi><mn id="id7.7.2.m2.1.1.1" mathvariant="normal" xref="id7.7.2.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id7.7.2.m2.1b"><apply id="id7.7.2.m2.1.1.cmml" xref="id7.7.2.m2.1.1"><cn id="id7.7.2.m2.1.1.1.cmml" type="integer" xref="id7.7.2.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.2.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id7.7.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Wen-tau Yih<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id8.8.3.m3.1"><semantics id="id8.8.3.m3.1a"><msup id="id8.8.3.m3.1.1" xref="id8.8.3.m3.1.1.cmml"><mi id="id8.8.3.m3.1.1a" xref="id8.8.3.m3.1.1.cmml"></mi><mn id="id8.8.3.m3.1.1.1" mathvariant="normal" xref="id8.8.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id8.8.3.m3.1b"><apply id="id8.8.3.m3.1.1.cmml" xref="id8.8.3.m3.1.1"><cn id="id8.8.3.m3.1.1.1.cmml" type="integer" xref="id8.8.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.8.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id8.8.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Srinivasan Iyer<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id9.9.4.m4.1"><semantics id="id9.9.4.m4.1a"><msup id="id9.9.4.m4.1.1" xref="id9.9.4.m4.1.1.cmml"><mi id="id9.9.4.m4.1.1a" xref="id9.9.4.m4.1.1.cmml"></mi><mn id="id9.9.4.m4.1.1.1" mathvariant="normal" xref="id9.9.4.m4.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id9.9.4.m4.1b"><apply id="id9.9.4.m4.1.1.cmml" xref="id9.9.4.m4.1.1"><cn id="id9.9.4.m4.1.1.1.cmml" type="integer" xref="id9.9.4.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.9.4.m4.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id9.9.4.m4.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<br class="ltx_break"><math alttext="{}^{1}" class="ltx_Math" display="inline" id="id10.10.m6.1"><semantics id="id10.10.m6.1a"><msup id="id10.10.m6.1.1" xref="id10.10.m6.1.1.cmml"><mi id="id10.10.m6.1.1a" xref="id10.10.m6.1.1.cmml"></mi><mn id="id10.10.m6.1.1.1" xref="id10.10.m6.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id10.10.m6.1b"><apply id="id10.10.m6.1.1.cmml" xref="id10.10.m6.1.1"><cn id="id10.10.m6.1.1.1.cmml" type="integer" xref="id10.10.m6.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.10.m6.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id10.10.m6.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>FAIR at Meta  <math alttext="{}^{2}" class="ltx_Math" display="inline" id="id11.11.m7.1"><semantics id="id11.11.m7.1a"><msup id="id11.11.m7.1.1" xref="id11.11.m7.1.1.cmml"><mi id="id11.11.m7.1.1a" xref="id11.11.m7.1.1.cmml"></mi><mn id="id11.11.m7.1.1.1" xref="id11.11.m7.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id11.11.m7.1b"><apply id="id11.11.m7.1.1.cmml" xref="id11.11.m7.1.1"><cn id="id11.11.m7.1.1.1.cmml" type="integer" xref="id11.11.m7.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.11.m7.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id11.11.m7.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>Carnegie Mellon University  <math alttext="{}^{3}" class="ltx_Math" display="inline" id="id12.12.m8.1"><semantics id="id12.12.m8.1a"><msup id="id12.12.m8.1.1" xref="id12.12.m8.1.1.cmml"><mi id="id12.12.m8.1.1a" xref="id12.12.m8.1.1.cmml"></mi><mn id="id12.12.m8.1.1.1" xref="id12.12.m8.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="id12.12.m8.1b"><apply id="id12.12.m8.1.1.cmml" xref="id12.12.m8.1.1"><cn id="id12.12.m8.1.1.1.cmml" type="integer" xref="id12.12.m8.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.12.m8.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="id12.12.m8.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>University of Washington 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id13.13.id1">{zhengbaj,gneubig}@cs.cmu.edu</span>  <span class="ltx_text ltx_font_typewriter" id="id14.14.id2">{victorialin,scottyih,sviyer}@meta.com</span>
</span><span class="ltx_author_notes">Majority of the work done during an internship at Meta.</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id15.id1">대형 언어 모델(LLM) 기반 어시스턴트가 진화하는 정보 요구에 효과적으로 적응하기 위해서는 새로운 데이터에 대한 지속적인 훈련을 통해 사실적 지식을 업데이트할 수 있어야 한다. 이를 위한 표준 레시피는 새로운 문서에 대한 사전 교육을 계속한 후 질의응답(QA) 쌍에 대한 지시 조정을 포함한다. 그러나 이 레시피로 훈련된 LLM은 문서의 복잡성을 최소화하더라도 질문에 답하는 데 어려움을 겪고 있음을 알 수 있다. 우리는 QA 쌍이 일반적으로 간단하지만 문서가 더 복잡하여 복잡한 방식으로 많은 사실 진술을 함께 엮는다는 것을 발견했다. 따라서 우리는 LLMs을 QA 쌍 <em class="ltx_emph ltx_font_italic" id="id15.id1.1">before</em> 계속된 사전 훈련에 노출시켜 복잡한 문서로부터 지식을 인코딩하는 프로세스가 질문을 통해 이 지식에 접근하는 방식을 고려하도록 하는 것이 유익하다고 가정한다. 이를 기반으로 문서에 대한 학습 전에 질문에 대해 명령어를 조정하는 방법인 <span class="ltx_text ltx_font_bold" id="id15.id1.2">pre-instruction-tuning (PIT)</span>을 제안한다. 이는 문서에 대한 학습 후 지식을 추출하는 방법을 학습하는 표준 명령어 조정과 대비된다. 광범위한 실험과 절제 연구는 PIT가 새로운 문서의 지식을 흡수하는 LLM의 능력을 크게 향상시켜 표준 명령어 조정을 17.8% 능가한다는 것을 보여준다.</p>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<span class="ltx_ERROR undefined" id="id1">\addauthor</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">gnmagenta</p>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="p2">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p2.1"><span class="ltx_text ltx_font_bold" id="p2.1.1">Instruction-tuned Language Models are Better Knowledge Learners</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p3">
<br class="ltx_break">
<p class="ltx_p" id="p3.12"><span class="ltx_text" id="p3.12.12" style="width:433.6pt;"><span class="ltx_text" id="p3.12.12" style="width:0.0pt;"></span></span></p>
<span class="ltx_tabular ltx_align_top" id="p3.12.12.12.12">
<span class="ltx_tbody">
<span class="ltx_tr" id="p3.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p3.5.5.5.5.5.5"><span class="ltx_text ltx_font_bold" id="p3.5.5.5.5.5.5.5">Zhengbao Jiang<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.1.1.1.1.1.1.1.m1.1"><semantics id="p3.1.1.1.1.1.1.1.m1.1a"><msup id="p3.1.1.1.1.1.1.1.m1.1.1" xref="p3.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="p3.1.1.1.1.1.1.1.m1.1.1a" xref="p3.1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="p3.1.1.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="p3.1.1.1.1.1.1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.1.1.1.1.1.1.1.m1.1b"><apply id="p3.1.1.1.1.1.1.1.m1.1.1.cmml" xref="p3.1.1.1.1.1.1.1.m1.1.1"><cn id="p3.1.1.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="p3.1.1.1.1.1.1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.1.1.1.1.1.1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_note ltx_role_thanks" id="p3.5.5.5.5.5.5.5.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>Majority of the work done during an internship at Meta.</span></span></span>  Zhiqing Sun<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.2.2.2.2.2.2.2.m2.1"><semantics id="p3.2.2.2.2.2.2.2.m2.1a"><msup id="p3.2.2.2.2.2.2.2.m2.1.1" xref="p3.2.2.2.2.2.2.2.m2.1.1.cmml"><mi id="p3.2.2.2.2.2.2.2.m2.1.1a" xref="p3.2.2.2.2.2.2.2.m2.1.1.cmml"></mi><mn id="p3.2.2.2.2.2.2.2.m2.1.1.1" mathvariant="normal" xref="p3.2.2.2.2.2.2.2.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.2.2.2.2.2.2.2.m2.1b"><apply id="p3.2.2.2.2.2.2.2.m2.1.1.cmml" xref="p3.2.2.2.2.2.2.2.m2.1.1"><cn id="p3.2.2.2.2.2.2.2.m2.1.1.1.cmml" type="integer" xref="p3.2.2.2.2.2.2.2.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.2.2.2.2.2.2.2.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.2.2.2.2.2.2.2.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Weijia Shi<math alttext="{}^{1,3}" class="ltx_Math" display="inline" id="p3.3.3.3.3.3.3.3.m3.2"><semantics id="p3.3.3.3.3.3.3.3.m3.2a"><msup id="p3.3.3.3.3.3.3.3.m3.2.2" xref="p3.3.3.3.3.3.3.3.m3.2.2.cmml"><mi id="p3.3.3.3.3.3.3.3.m3.2.2a" xref="p3.3.3.3.3.3.3.3.m3.2.2.cmml"></mi><mrow id="p3.3.3.3.3.3.3.3.m3.2.2.2.4" xref="p3.3.3.3.3.3.3.3.m3.2.2.2.3.cmml"><mn id="p3.3.3.3.3.3.3.3.m3.1.1.1.1" mathvariant="normal" xref="p3.3.3.3.3.3.3.3.m3.1.1.1.1.cmml">1</mn><mo id="p3.3.3.3.3.3.3.3.m3.2.2.2.4.1" mathvariant="normal" xref="p3.3.3.3.3.3.3.3.m3.2.2.2.3.cmml">,</mo><mn id="p3.3.3.3.3.3.3.3.m3.2.2.2.2" mathvariant="normal" xref="p3.3.3.3.3.3.3.3.m3.2.2.2.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="p3.3.3.3.3.3.3.3.m3.2b"><apply id="p3.3.3.3.3.3.3.3.m3.2.2.cmml" xref="p3.3.3.3.3.3.3.3.m3.2.2"><list id="p3.3.3.3.3.3.3.3.m3.2.2.2.3.cmml" xref="p3.3.3.3.3.3.3.3.m3.2.2.2.4"><cn id="p3.3.3.3.3.3.3.3.m3.1.1.1.1.cmml" type="integer" xref="p3.3.3.3.3.3.3.3.m3.1.1.1.1">1</cn><cn id="p3.3.3.3.3.3.3.3.m3.2.2.2.2.cmml" type="integer" xref="p3.3.3.3.3.3.3.3.m3.2.2.2.2">3</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.3.3.3.3.3.3.3.m3.2c">{}^{1,3}</annotation><annotation encoding="application/x-llamapun" id="p3.3.3.3.3.3.3.3.m3.2d">start_FLOATSUPERSCRIPT 1 , 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Pedro Rodriguez<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.4.4.4.4.4.4.4.m4.1"><semantics id="p3.4.4.4.4.4.4.4.m4.1a"><msup id="p3.4.4.4.4.4.4.4.m4.1.1" xref="p3.4.4.4.4.4.4.4.m4.1.1.cmml"><mi id="p3.4.4.4.4.4.4.4.m4.1.1a" xref="p3.4.4.4.4.4.4.4.m4.1.1.cmml"></mi><mn id="p3.4.4.4.4.4.4.4.m4.1.1.1" mathvariant="normal" xref="p3.4.4.4.4.4.4.4.m4.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.4.4.4.4.4.4.4.m4.1b"><apply id="p3.4.4.4.4.4.4.4.m4.1.1.cmml" xref="p3.4.4.4.4.4.4.4.m4.1.1"><cn id="p3.4.4.4.4.4.4.4.m4.1.1.1.cmml" type="integer" xref="p3.4.4.4.4.4.4.4.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.4.4.4.4.4.4.4.m4.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.4.4.4.4.4.4.4.m4.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Chunting Zhou<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.5.5.5.5.5.5.5.m5.1"><semantics id="p3.5.5.5.5.5.5.5.m5.1a"><msup id="p3.5.5.5.5.5.5.5.m5.1.1" xref="p3.5.5.5.5.5.5.5.m5.1.1.cmml"><mi id="p3.5.5.5.5.5.5.5.m5.1.1a" xref="p3.5.5.5.5.5.5.5.m5.1.1.cmml"></mi><mn id="p3.5.5.5.5.5.5.5.m5.1.1.1" mathvariant="normal" xref="p3.5.5.5.5.5.5.5.m5.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.5.5.5.5.5.5.5.m5.1b"><apply id="p3.5.5.5.5.5.5.5.m5.1.1.cmml" xref="p3.5.5.5.5.5.5.5.m5.1.1"><cn id="p3.5.5.5.5.5.5.5.m5.1.1.1.cmml" type="integer" xref="p3.5.5.5.5.5.5.5.m5.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.5.5.5.5.5.5.5.m5.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.5.5.5.5.5.5.5.m5.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="p3.9.9.9.9.9">
<span class="ltx_td ltx_align_center" id="p3.9.9.9.9.9.4"><span class="ltx_text ltx_font_bold" id="p3.9.9.9.9.9.4.4">Graham Neubig<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.6.6.6.6.6.1.1.m1.1"><semantics id="p3.6.6.6.6.6.1.1.m1.1a"><msup id="p3.6.6.6.6.6.1.1.m1.1.1" xref="p3.6.6.6.6.6.1.1.m1.1.1.cmml"><mi id="p3.6.6.6.6.6.1.1.m1.1.1a" xref="p3.6.6.6.6.6.1.1.m1.1.1.cmml"></mi><mn id="p3.6.6.6.6.6.1.1.m1.1.1.1" mathvariant="normal" xref="p3.6.6.6.6.6.1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.6.6.6.6.6.1.1.m1.1b"><apply id="p3.6.6.6.6.6.1.1.m1.1.1.cmml" xref="p3.6.6.6.6.6.1.1.m1.1.1"><cn id="p3.6.6.6.6.6.1.1.m1.1.1.1.cmml" type="integer" xref="p3.6.6.6.6.6.1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.6.6.6.6.6.1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.6.6.6.6.6.1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Xi Victoria Lin<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.7.7.7.7.7.2.2.m2.1"><semantics id="p3.7.7.7.7.7.2.2.m2.1a"><msup id="p3.7.7.7.7.7.2.2.m2.1.1" xref="p3.7.7.7.7.7.2.2.m2.1.1.cmml"><mi id="p3.7.7.7.7.7.2.2.m2.1.1a" xref="p3.7.7.7.7.7.2.2.m2.1.1.cmml"></mi><mn id="p3.7.7.7.7.7.2.2.m2.1.1.1" mathvariant="normal" xref="p3.7.7.7.7.7.2.2.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.7.7.7.7.7.2.2.m2.1b"><apply id="p3.7.7.7.7.7.2.2.m2.1.1.cmml" xref="p3.7.7.7.7.7.2.2.m2.1.1"><cn id="p3.7.7.7.7.7.2.2.m2.1.1.1.cmml" type="integer" xref="p3.7.7.7.7.7.2.2.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.7.7.7.7.7.2.2.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.7.7.7.7.7.2.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Wen-tau Yih<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.8.8.8.8.8.3.3.m3.1"><semantics id="p3.8.8.8.8.8.3.3.m3.1a"><msup id="p3.8.8.8.8.8.3.3.m3.1.1" xref="p3.8.8.8.8.8.3.3.m3.1.1.cmml"><mi id="p3.8.8.8.8.8.3.3.m3.1.1a" xref="p3.8.8.8.8.8.3.3.m3.1.1.cmml"></mi><mn id="p3.8.8.8.8.8.3.3.m3.1.1.1" mathvariant="normal" xref="p3.8.8.8.8.8.3.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.8.8.8.8.8.3.3.m3.1b"><apply id="p3.8.8.8.8.8.3.3.m3.1.1.cmml" xref="p3.8.8.8.8.8.3.3.m3.1.1"><cn id="p3.8.8.8.8.8.3.3.m3.1.1.1.cmml" type="integer" xref="p3.8.8.8.8.8.3.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.8.8.8.8.8.3.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.8.8.8.8.8.3.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Srinivasan Iyer<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.9.9.9.9.9.4.4.m4.1"><semantics id="p3.9.9.9.9.9.4.4.m4.1a"><msup id="p3.9.9.9.9.9.4.4.m4.1.1" xref="p3.9.9.9.9.9.4.4.m4.1.1.cmml"><mi id="p3.9.9.9.9.9.4.4.m4.1.1a" xref="p3.9.9.9.9.9.4.4.m4.1.1.cmml"></mi><mn id="p3.9.9.9.9.9.4.4.m4.1.1.1" mathvariant="normal" xref="p3.9.9.9.9.9.4.4.m4.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.9.9.9.9.9.4.4.m4.1b"><apply id="p3.9.9.9.9.9.4.4.m4.1.1.cmml" xref="p3.9.9.9.9.9.4.4.m4.1.1"><cn id="p3.9.9.9.9.9.4.4.m4.1.1.1.cmml" type="integer" xref="p3.9.9.9.9.9.4.4.m4.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.9.9.9.9.9.4.4.m4.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.9.9.9.9.9.4.4.m4.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="p3.12.12.12.12.12">
<span class="ltx_td ltx_align_center" id="p3.12.12.12.12.12.3"><math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.10.10.10.10.10.1.m1.1"><semantics id="p3.10.10.10.10.10.1.m1.1a"><msup id="p3.10.10.10.10.10.1.m1.1.1" xref="p3.10.10.10.10.10.1.m1.1.1.cmml"><mi id="p3.10.10.10.10.10.1.m1.1.1a" xref="p3.10.10.10.10.10.1.m1.1.1.cmml"></mi><mn id="p3.10.10.10.10.10.1.m1.1.1.1" xref="p3.10.10.10.10.10.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.10.10.10.10.10.1.m1.1b"><apply id="p3.10.10.10.10.10.1.m1.1.1.cmml" xref="p3.10.10.10.10.10.1.m1.1.1"><cn id="p3.10.10.10.10.10.1.m1.1.1.1.cmml" type="integer" xref="p3.10.10.10.10.10.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.10.10.10.10.10.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.10.10.10.10.10.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>FAIR at Meta  <math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.11.11.11.11.11.2.m2.1"><semantics id="p3.11.11.11.11.11.2.m2.1a"><msup id="p3.11.11.11.11.11.2.m2.1.1" xref="p3.11.11.11.11.11.2.m2.1.1.cmml"><mi id="p3.11.11.11.11.11.2.m2.1.1a" xref="p3.11.11.11.11.11.2.m2.1.1.cmml"></mi><mn id="p3.11.11.11.11.11.2.m2.1.1.1" xref="p3.11.11.11.11.11.2.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.11.11.11.11.11.2.m2.1b"><apply id="p3.11.11.11.11.11.2.m2.1.1.cmml" xref="p3.11.11.11.11.11.2.m2.1.1"><cn id="p3.11.11.11.11.11.2.m2.1.1.1.cmml" type="integer" xref="p3.11.11.11.11.11.2.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.11.11.11.11.11.2.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.11.11.11.11.11.2.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>Carnegie Mellon University  <math alttext="{}^{3}" class="ltx_Math" display="inline" id="p3.12.12.12.12.12.3.m3.1"><semantics id="p3.12.12.12.12.12.3.m3.1a"><msup id="p3.12.12.12.12.12.3.m3.1.1" xref="p3.12.12.12.12.12.3.m3.1.1.cmml"><mi id="p3.12.12.12.12.12.3.m3.1.1a" xref="p3.12.12.12.12.12.3.m3.1.1.cmml"></mi><mn id="p3.12.12.12.12.12.3.m3.1.1.1" xref="p3.12.12.12.12.12.3.m3.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="p3.12.12.12.12.12.3.m3.1b"><apply id="p3.12.12.12.12.12.3.m3.1.1.cmml" xref="p3.12.12.12.12.12.3.m3.1.1"><cn id="p3.12.12.12.12.12.3.m3.1.1.1.cmml" type="integer" xref="p3.12.12.12.12.12.3.m3.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.12.12.12.12.12.3.m3.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="p3.12.12.12.12.12.3.m3.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math>University of Washington</span></span>
<span class="ltx_tr" id="p3.12.12.12.12.13.1">
<span class="ltx_td ltx_align_center" id="p3.12.12.12.12.13.1.1"><span class="ltx_text ltx_font_typewriter" id="p3.12.12.12.12.13.1.1.1">{zhengbaj,gneubig}@cs.cmu.edu</span>  <span class="ltx_text ltx_font_typewriter" id="p3.12.12.12.12.13.1.1.2">{victorialin,scottyih,sviyer}@meta.com</span></span></span>
</span>
</span></span> </span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="233" id="S0.F1.g1" src="https://arxiv.org/html/2402.12847v1/x1.png" width="665">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of continued pre-training (first row), continued pre-training followed by instruction-tuning (second row), and pre-instruction-tuning before continued pre-training (last row), along with their accuracies on evaluation questions. Each right-pointing light-blue triangle indicates a training phase.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">대규모 언어 모델(LLM)은 대규모 사전 훈련을 통해 방대한 양의 사실적 지식을 매개 변수에 저장하며, 이 지식은 “세계에서 가장 큰 빙상이 어디에 위치해 있는가” <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib5" title="">2020</a>); OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib34" title="">2023</a>); Chowdhery et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib10" title="">2022</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib60" title="">2022</a>); Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib51" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib52" title="">b</a>); Gemini Team (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib12" title="">2023</a>)</cite>와 같은 다양한 질문에 답하는 데 사용될 수 있다. 그러나 이러한 사실적 지식은 정적이며, 이는 세계가 진화함에 따라 구식이 되거나 LLM이 전문적이거나 사적인 영역에서 사용될 때 불충분하다는 것을 증명할 수 있음을 의미한다.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">LLM들을 최신의 상태로 유지하기 위해, 파라미터들에 지식을 저장하기 위해 새로운 문서들에 대한 사전 트레이닝을 계속하는 것이 일반적이며, 이는 LLM들이 최신의 정보 <cite class="ltx_cite ltx_citemacro_cite">Jang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib20" title="">2022</a>)</cite>를 필요로 하는 질의들에 효과적으로 응답할 수 있게 한다. 널리 유지되는 견해는 파라미터에 저장된 사실적 지식이 프롬프트 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib5" title="">2020</a>); Petroni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib37" title="">2019</a>); Roberts et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib42" title="">2020</a>)</cite>를 통해 도출될 수 있고, 지시-튜닝(지도된 미세-튜닝 또는 정렬이라고도 함)이 이러한 도출을 더 효과적으로 <cite class="ltx_cite ltx_citemacro_cite">Sanh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib44" title="">2022</a>); Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib56" title="">2022</a>); Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib35" title="">2022</a>)</cite>로 만든다는 것이다. 본 논문의 첫 부분(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4" title="4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§ 4</span></a>)에서는 다음 질문에 답하기 위해 Llama-2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib52" title="">2023b</a>)</cite>를 사용하여 광범위한 실험을 수행했다. <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">to what extent we augment the knowledge in modern LLMs by continue pre-training on new documents, with or without subsequent instruction-tuning</em>? 우리는 복잡도가 1로 최소화되는 범위에서 문서에 대해 LLM을 반복적으로 훈련함에 따라 LLM이 올바르게 대답하는 문서에 대한 질문의 비율이 27.6%로 일관되게 증가한다는 것을 발견했다. 후속 명령어 조정은 이를 30.3%로 더욱 향상시키며, 이러한 널리 사용되는 연습이 LLMs로부터 더 많은 지식을 이끌어내는 데 유용함을 확인한다. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This capacity might be underestimated by previous works due to using relatively small LMs or randomly initialized transformers, or lack of exhaustive training or instruction-tuning <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib54" title="">2021</a>); Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib16" title="">2023</a>); Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>.</span></span></span> 그러나, 도출된 지식의 양은 여전히 제한적이며, 문서의 복잡성을 최소화하더라도, 우리가 "복잡성의 저주"라고 부르는 현상이다. <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Inspired by the “reversal curse” of <cite class="ltx_cite ltx_citemacro_citet">Berglund et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib3" title="">2023</a>)</cite>.</span></span></span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.2">두 번째 부분(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5" title="5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§ 5</span></a>)에서는 LLMs을 문서로부터 지식을 흡수하는 데 더 능숙하게 만들어 복잡성 저주를 완화시키는 방법을 연구한다.</p>
<cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite> presented an intriguing finding that training a randomly initialized transformer from scratch on a mix of biographies and related questions resulted in strong generalization to new questions.
However, understanding the reasons behind this finding and exploring ways to practically apply it for absorbing knowledge from new documents requires further investigation.
We found that question-answer (QA) pairs are generally straightforward and easily digestible, while documents tend to be more complex and cluttered, often weaving many factual statements together in a more intricate manner.
Therefore, we hypothesize that <em class="ltx_emph ltx_font_italic" id="S1.p3.2.1">it is beneficial to deliberately expose LLMs to QA data before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions</em>.
We refer to this as <span class="ltx_text ltx_font_bold" id="S1.p3.2.2">pre-instruction-tuning (PIT)</span> and conduct comprehensive experiments to benchmark different variations of this method.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S0.F1" title="Figure 1 ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;1</span></a>, our best-performing variation starts with training exclusively on QA pairs (e.g., “who handled the editing of Oppenheimer”) to grasp how knowledge is accessed.
This is followed by training on a combination of these QA pairs and associated documents (e.g., “who handled the editing of Oppenheimer” and a document about “Oppenheimer”).
In this phase, LLMs enhance their ability to absorb knowledge from information-dense documents, building upon the QA pairs that they have already mastered.
To study continual knowledge acquisition, we build a dataset named <span class="ltx_text ltx_font_typewriter" id="S1.p3.2.3">Wiki2023</span>, which includes a collection of documents from Wikipedia that are relevant to the year 2023.
Comprehensive experiments on <span class="ltx_text ltx_font_typewriter" id="S1.p3.2.4">Wiki2023</span> demonstrate that after PIT, LLMs exhibit an enhanced ability to absorb knowledge from new documents (e.g., a document about “Barbie”).
Detailed ablation studies reveal that this ability primarily stems from prioritizing learning how to access knowledge over learning to encode knowledge from documents.
Overall, PIT significantly outperforms the standard instruction-tuning approach (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS1" title="5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5.1</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS2" title="5.2 Pre-instruction-tuning++ ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5.2</span></a>), improving QA accuracies by 17.8% on Llama-2 7B (30.3% <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" stretchy="false" xref="S1.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">→</annotation></semantics></math> 48.1%) and 16.3% on Llama-2 70B (46.4% <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S1.p3.2.m2.1"><semantics id="S1.p3.2.m2.1a"><mo id="S1.p3.2.m2.1.1" stretchy="false" xref="S1.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><ci id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.p3.2.m2.1d">→</annotation></semantics></math> 62.7%).
Moreover, PIT also enhances the ability to absorb knowledge from documents of a <em class="ltx_emph ltx_font_italic" id="S1.p3.2.5">different</em> domain, shedding light on the potential to scale this method up to a wider variety of documents and instructions for more robust generalization (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS4" title="5.4 Cross-domain Generalization ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§&nbsp;5.4</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Building a Dataset to Study Continual Knowledge Acquisition</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">LLM이 새로운 문서로부터 지식을 학습하는 능력을 평가하기 위해서는 원래의 사전 훈련 말뭉치와 최소한의 중복을 갖는 문서 말뭉치를 사용하는 것이 필수적이다. 이는 LLM이 질문에 올바르게 답할 때 이 기능을 원래 사전 훈련 말뭉치에서 유사한 질문을 접하는 것이 아니라 새로운 문서로부터의 학습에 자신 있게 귀속시킬 수 있음을 보장한다. 이 절에서는 위키피디아에서 이러한 말뭉치를 구축하기 위한 방법론을 설명한다.</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S2.F2.1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="391" id="S2.F2.1.g1" src="https://arxiv.org/html/2402.12847v1/x2.png" width="831">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S2.F2.2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S2.F2.2.g1" src="https://arxiv.org/html/2402.12847v1/x3.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The <span class="ltx_text ltx_font_typewriter" id="S2.F2.7.1">Wiki2023</span> dataset. <span class="ltx_text ltx_font_bold" id="S2.F2.8.2">Top-right</span>: the number of documents and QA pairs; <span class="ltx_text ltx_font_bold" id="S2.F2.9.3">Top-left</span>: frequent keywords in questions; <span class="ltx_text ltx_font_bold" id="S2.F2.10.4">Bottom</span>: the distribution of token counts in documents, questions, and answers.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="803" id="S2.F3.g1" src="https://arxiv.org/html/2402.12847v1/x4.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An example document about “Oppenheimer” and corresponding QA pairs from <span class="ltx_text ltx_font_typewriter" id="S2.F3.2.1">Wiki2023</span>. Tokens used for computing losses are highlighted in green.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Wiki2023 Document Corpus</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">다음의 실험(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4" title="4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§ 4</span></a> 및 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5" title="5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§ 5</span></a>)에서 Llama-2(7B 및 70B) <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib52" title="">2023b</a>)</cite>는 가장 성능이 좋은 LLMs 중 하나이기 때문에 사용한다. 우리는 영화, 예술, 경제, 정치, 사건 등과 같은 다양한 영역의 주제를 포함하여 "2023" 범주에 따라 분류된 위키피디아 기사를 사용한다. <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Category:2023" title="">https://en.wikipedia.org/wiki/Category:2023</a></span></span></span> 이 사실 정보가 원본 학습 말뭉치에 포함되지 않을 가능성은 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 1</span></a> (7B/70B의 경우 9.5%/17.2%)에서 낮은 QA 성능에 의해 지원된다. <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>It is important to note the difficulty in completely avoiding factual overlap between <span class="ltx_text ltx_font_typewriter" id="footnote4.1">Wiki2023</span> and the pre-training corpus of Llama-2. For example, a film released in 2023 might have had information available before 2023. Data duplication detection is an active research direction, which falls beyond the focus of this study.</span></span></span> 훈련 프로세스를 가속화하기 위해 철저한 요약을 제공하고 많은 사실적 진술을 포함하는 각 기사의 첫 번째 섹션만 사용합니다. “Oppenheimer”에 대한 수집 문서 수와 예시 문서는 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F2" title="Figure 2 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 2</span></a>와 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F3" title="Figure 3 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 3</span></a>에서 찾을 수 있다. 이를 <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.1.1">Wiki2023</span> dataset이라고 한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Wiki2023 Question-answer Pairs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">명령어 조정 또는 성능 평가를 위해 QA 쌍을 수집하기 위해 공개적으로 사용 가능한 LLM을 사용하여 프롬프트<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.SS2" title="2.2 Wiki2023 Question-answer Pairs ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§ 2.2</span></a>에 이어 문맥으로 기사가 주어진 다양한 질문과 해당 답변을 생성한다. 글마다 평균 4.93개의 문항이 생성된다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F2" title="Figure 2 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 2</span></a> 및 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F3" title="Figure 3 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 3</span></a>는 각각 “Oppenheimer”에 대한 상세한 통계 및 예제 QA 쌍을 보여준다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<svg class="ltx_picture" height="223.85" id="S2.SS2.p2.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,223.85) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.000000"><path d="M 0 5.32 L 0 218.53 C 0 221.47 2.38 223.85 5.32 223.85 L 594.68 223.85 C 597.62 223.85 600 221.47 600 218.53 L 600 5.32 C 600 2.38 597.62 0 594.68 0 L 5.32 0 C 2.38 0 0 2.38 0 5.32 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.000000"><path d="M 1.38 5.32 L 1.38 195.1 L 598.62 195.1 L 598.62 5.32 C 598.62 3.15 596.85 1.38 594.68 1.38 L 5.32 1.38 C 3.15 1.38 1.38 3.15 1.38 5.32 Z" style="stroke:none"></path></g><g fill="#666666" fill-opacity="1.000000"><path d="M 1.38 196.49 L 1.38 218.53 C 1.38 220.71 3.15 222.47 5.32 222.47 L 594.68 222.47 C 596.85 222.47 598.62 220.71 598.62 218.53 L 598.62 196.49 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 8.3 8.3)"><foreignObject height="179.88" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="583.4"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="S2.SS2.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.2.2.2.2.2.2.2" style="width:421.6pt;">
<span class="ltx_para" id="S2.SS2.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S2.SS2.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1">Given the following summary about the subject {topic}, generate a comprehensive list of questions and corresponding answers that cover all aspects. To make the question clear, always include {topic} in the question. Answers should be concise, consisting of a few short phrases separated by commas.</span>
<span class="ltx_p" id="S2.SS2.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2">Output in the following format:</span>
<span class="ltx_p" id="S2.SS2.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.3">Q: an open-domain question about the subject {topic} (the subject {topic} should always be included)</span>
<span class="ltx_p" id="S2.SS2.p2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.4">A: phrase1, phrase2, …</span>
</span>
<span class="ltx_para" id="S2.SS2.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.2.2.2.2.2.2.2.p2">
<span class="ltx_p" id="S2.SS2.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.2.2.2.2.2.2.2.p2.1">Summary:</span>
<span class="ltx_p" id="S2.SS2.p2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.2.2.2.2.2.2.2.p2.2">{summary}</span>
</span></span></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Splits</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">모든 도메인 중 평가를 위해 필름 도메인을 선택하고 테스트 분할로 256개의 기사를 무작위로 선택한다(<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.1">Wiki2023-film-test</span>). 테스트 분할(<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.2">Wiki2023-film-test-doc</span>)에서 문서에 LLMs을 지속적으로 훈련하고 해당 질문의 정확도에 따라 성능을 평가한다(<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.3">Wiki2023-film-test-QA</span>). 나머지 1720개의 기사 및 해당 QA 쌍(<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.4">Wiki2023-film-train</span>)은 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F2" title="Figure 2 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 2</span></a>의 도메인 내 설정에 해당하는 다양한 훈련 전략을 연구하는 데 사용될 것이다. 또한 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F2" title="Figure 2 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 2</span></a>의 교차 도메인 설정에 해당하는 도메인 전반에 걸쳐 다양한 방법의 효과를 연구하기 위해 필름 도메인에 대한 평가 전에 다른 도메인에 대해 훈련한다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Settings</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Objectives</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.2">When training on documents, we prepend a &lt;bos&gt; token and compute the standard next-token prediction loss by averaging over all tokens in the document: <math alttext="L_{\bm{d}}=-\sum_{t}{\log P(\bm{d}_{t}|\bm{d}_{&lt;t})}/|\bm{d}|" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.2"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml"><msub id="S3.SS1.p1.1.m1.2.2.3" xref="S3.SS1.p1.1.m1.2.2.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.3.2" xref="S3.SS1.p1.1.m1.2.2.3.2.cmml">L</mi><mi id="S3.SS1.p1.1.m1.2.2.3.3" xref="S3.SS1.p1.1.m1.2.2.3.3.cmml">𝒅</mi></msub><mo id="S3.SS1.p1.1.m1.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.2.2.1" xref="S3.SS1.p1.1.m1.2.2.1.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1a" xref="S3.SS1.p1.1.m1.2.2.1.cmml">−</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.cmml"><msub id="S3.SS1.p1.1.m1.2.2.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.2.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1.1.2.2" xref="S3.SS1.p1.1.m1.2.2.1.1.2.2.cmml">∑</mo><mi id="S3.SS1.p1.1.m1.2.2.1.1.2.3" xref="S3.SS1.p1.1.m1.2.2.1.1.2.3.cmml">t</mi></msub><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1.cmml">log</mi><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3a" lspace="0.167em" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.cmml">⁡</mo><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml"><msub id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">𝒅</mi><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo fence="false" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml">𝒅</mi><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.2.cmml">/</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.3.2.1" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.1.cmml">|</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">𝒅</mi><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.3.2.2" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.1.cmml">|</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><apply id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2"><eq id="S3.SS1.p1.1.m1.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2"></eq><apply id="S3.SS1.p1.1.m1.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.3.2">𝐿</ci><ci id="S3.SS1.p1.1.m1.2.2.3.3.cmml" xref="S3.SS1.p1.1.m1.2.2.3.3">𝒅</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1"><minus id="S3.SS1.p1.1.m1.2.2.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1"></minus><apply id="S3.SS1.p1.1.m1.2.2.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1"><apply id="S3.SS1.p1.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2">subscript</csymbol><sum id="S3.SS1.p1.1.m1.2.2.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2.2"></sum><ci id="S3.SS1.p1.1.m1.2.2.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1"><divide id="S3.SS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.2"></divide><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1"><times id="S3.SS1.p1.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.2"></times><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3"><log id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1"></log><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2">𝑃</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2">𝒅</ci><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2">𝒅</ci><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3"><lt id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.2"><abs id="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.2.1"></abs><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝒅</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">L_{\bm{d}}=-\sum_{t}{\log P(\bm{d}_{t}|\bm{d}_{&lt;t})}/|\bm{d}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.2d">italic_L start_POSTSUBSCRIPT bold_italic_d end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_log italic_P ( bold_italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_italic_d start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT ) / | bold_italic_d |</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We do not append a ¡eos¿ token at the end of documents because we only use the first section, which does not signify the conclusion of the entire article.</span></span></span> When training on QA pairs, we compute the average negative log-likelihood loss only on tokens in the answer given the question as the prefix: <math alttext="L_{\bm{a}}=-\sum_{t}{\log P(\bm{a}_{t}|\bm{q},\bm{a}_{&lt;t})}/|\bm{a}|" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.3"><semantics id="S3.SS1.p1.2.m2.3a"><mrow id="S3.SS1.p1.2.m2.3.3" xref="S3.SS1.p1.2.m2.3.3.cmml"><msub id="S3.SS1.p1.2.m2.3.3.3" xref="S3.SS1.p1.2.m2.3.3.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.3.2" xref="S3.SS1.p1.2.m2.3.3.3.2.cmml">L</mi><mi id="S3.SS1.p1.2.m2.3.3.3.3" xref="S3.SS1.p1.2.m2.3.3.3.3.cmml">𝒂</mi></msub><mo id="S3.SS1.p1.2.m2.3.3.2" xref="S3.SS1.p1.2.m2.3.3.2.cmml">=</mo><mrow id="S3.SS1.p1.2.m2.3.3.1" xref="S3.SS1.p1.2.m2.3.3.1.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1a" xref="S3.SS1.p1.2.m2.3.3.1.cmml">−</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.cmml"><msub id="S3.SS1.p1.2.m2.3.3.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.2.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1.1.2.2" xref="S3.SS1.p1.2.m2.3.3.1.1.2.2.cmml">∑</mo><mi id="S3.SS1.p1.2.m2.3.3.1.1.2.3" xref="S3.SS1.p1.2.m2.3.3.1.1.2.3.cmml">t</mi></msub><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.cmml"><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.cmml"><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1.cmml">log</mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3a" lspace="0.167em" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.cmml">⁡</mo><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2.cmml">𝒂</mi><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3.cmml">t</mi></msub><mo fence="false" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">𝒒</mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">𝒂</mi><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></mrow></msub></mrow></mrow><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.2.cmml">/</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.3.2.1" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.1.cmml">|</mo><mi id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">𝒂</mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.3.2.2" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.1.cmml">|</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.3b"><apply id="S3.SS1.p1.2.m2.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3"><eq id="S3.SS1.p1.2.m2.3.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2"></eq><apply id="S3.SS1.p1.2.m2.3.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.3.2">𝐿</ci><ci id="S3.SS1.p1.2.m2.3.3.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.3.3">𝒂</ci></apply><apply id="S3.SS1.p1.2.m2.3.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1"><minus id="S3.SS1.p1.2.m2.3.3.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1"></minus><apply id="S3.SS1.p1.2.m2.3.3.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1"><apply id="S3.SS1.p1.2.m2.3.3.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2">subscript</csymbol><sum id="S3.SS1.p1.2.m2.3.3.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2.2"></sum><ci id="S3.SS1.p1.2.m2.3.3.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1"><divide id="S3.SS1.p1.2.m2.3.3.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.2"></divide><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1"><times id="S3.SS1.p1.2.m2.3.3.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.2"></times><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3"><log id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1"></log><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2">𝑃</ci></apply><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2">𝒂</ci><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3">𝑡</ci></apply><list id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝒒</ci><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2">𝒂</ci><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3"><lt id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></list></apply></apply><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.2"><abs id="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.2.1"></abs><ci id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2">𝒂</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.3c">L_{\bm{a}}=-\sum_{t}{\log P(\bm{a}_{t}|\bm{q},\bm{a}_{&lt;t})}/|\bm{a}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.3d">italic_L start_POSTSUBSCRIPT bold_italic_a end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_log italic_P ( bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_italic_q , bold_italic_a start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT ) / | bold_italic_a |</annotation></semantics></math>.</p>문서에 대해 훈련할 때, 우리는 <bos> 토큰을 준비하고 문서의 모든 토큰에 대해 평균화하여 표준 다음 토큰 예측 손실을 계산합니다: <math alttext="L_{\bm{d}}=-\sum_{t}{\log P(\bm{d}_{t}|\bm{d}_{&lt;t})}/|\bm{d}|" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.2"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml"><msub id="S3.SS1.p1.1.m1.2.2.3" xref="S3.SS1.p1.1.m1.2.2.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.3.2" xref="S3.SS1.p1.1.m1.2.2.3.2.cmml">L</mi><mi id="S3.SS1.p1.1.m1.2.2.3.3" xref="S3.SS1.p1.1.m1.2.2.3.3.cmml">𝒅</mi></msub><mo id="S3.SS1.p1.1.m1.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.2.2.1" xref="S3.SS1.p1.1.m1.2.2.1.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1a" xref="S3.SS1.p1.1.m1.2.2.1.cmml">−</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.cmml"><msub id="S3.SS1.p1.1.m1.2.2.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.2.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1.1.2.2" xref="S3.SS1.p1.1.m1.2.2.1.1.2.2.cmml">∑</mo><mi id="S3.SS1.p1.1.m1.2.2.1.1.2.3" xref="S3.SS1.p1.1.m1.2.2.1.1.2.3.cmml">t</mi></msub><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1.cmml">log</mi><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3a" lspace="0.167em" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.cmml">⁡</mo><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml"><msub id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">𝒅</mi><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo fence="false" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml">𝒅</mi><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml">t</mi></mrow></msub></mrow><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.2.cmml">/</mo><mrow id="S3.SS1.p1.1.m1.2.2.1.1.1.3.2" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.cmml"><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.3.2.1" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.1.cmml">|</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">𝒅</mi><mo id="S3.SS1.p1.1.m1.2.2.1.1.1.3.2.2" stretchy="false" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.1.cmml">|</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><apply id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2"><eq id="S3.SS1.p1.1.m1.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2"></eq><apply id="S3.SS1.p1.1.m1.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.3.2">𝐿</ci><ci id="S3.SS1.p1.1.m1.2.2.3.3.cmml" xref="S3.SS1.p1.1.m1.2.2.3.3">𝒅</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1"><minus id="S3.SS1.p1.1.m1.2.2.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1"></minus><apply id="S3.SS1.p1.1.m1.2.2.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1"><apply id="S3.SS1.p1.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2">subscript</csymbol><sum id="S3.SS1.p1.1.m1.2.2.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2.2"></sum><ci id="S3.SS1.p1.1.m1.2.2.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1"><divide id="S3.SS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.2"></divide><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1"><times id="S3.SS1.p1.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.2"></times><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3"><log id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.1"></log><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.3.2">𝑃</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.2">𝒅</ci><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.2">𝒅</ci><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3"><lt id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.1.1.1.1.3.3.3">𝑡</ci></apply></apply></apply></apply><apply id="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.2"><abs id="S3.SS1.p1.1.m1.2.2.1.1.1.3.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.1.3.2.1"></abs><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝒅</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">L_{\bm{d}}=-\sum_{t}{\log P(\bm{d}_{t}|\bm{d}_{&lt;t})}/|\bm{d}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.2d">italic_L start_POSTSUBSCRIPT bold_italic_d end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_log italic_P ( bold_italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_italic_d start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT ) / | bold_italic_d |</annotation></semantics></math>. <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We do not append a ¡eos¿ token at the end of documents because we only use the first section, which does not signify the conclusion of the entire article.</span></span></span> QA 쌍에 대한 훈련 시, 접두사로 주어진 질문에 있는 토큰에 대해서만 평균 음의 로그-우도 손실을 계산합니다: <math alttext="L_{\bm{a}}=-\sum_{t}{\log P(\bm{a}_{t}|\bm{q},\bm{a}_{&lt;t})}/|\bm{a}|" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.3"><semantics id="S3.SS1.p1.2.m2.3a"><mrow id="S3.SS1.p1.2.m2.3.3" xref="S3.SS1.p1.2.m2.3.3.cmml"><msub id="S3.SS1.p1.2.m2.3.3.3" xref="S3.SS1.p1.2.m2.3.3.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.3.2" xref="S3.SS1.p1.2.m2.3.3.3.2.cmml">L</mi><mi id="S3.SS1.p1.2.m2.3.3.3.3" xref="S3.SS1.p1.2.m2.3.3.3.3.cmml">𝒂</mi></msub><mo id="S3.SS1.p1.2.m2.3.3.2" xref="S3.SS1.p1.2.m2.3.3.2.cmml">=</mo><mrow id="S3.SS1.p1.2.m2.3.3.1" xref="S3.SS1.p1.2.m2.3.3.1.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1a" xref="S3.SS1.p1.2.m2.3.3.1.cmml">−</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.cmml"><msub id="S3.SS1.p1.2.m2.3.3.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.2.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1.1.2.2" xref="S3.SS1.p1.2.m2.3.3.1.1.2.2.cmml">∑</mo><mi id="S3.SS1.p1.2.m2.3.3.1.1.2.3" xref="S3.SS1.p1.2.m2.3.3.1.1.2.3.cmml">t</mi></msub><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.cmml"><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.cmml"><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1.cmml">log</mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3a" lspace="0.167em" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.cmml">⁡</mo><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2.cmml">𝒂</mi><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3.cmml">t</mi></msub><mo fence="false" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">𝒒</mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">𝒂</mi><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></mrow></msub></mrow></mrow><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.2.cmml">/</mo><mrow id="S3.SS1.p1.2.m2.3.3.1.1.1.3.2" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.cmml"><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.3.2.1" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.1.cmml">|</mo><mi id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">𝒂</mi><mo id="S3.SS1.p1.2.m2.3.3.1.1.1.3.2.2" stretchy="false" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.1.cmml">|</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.3b"><apply id="S3.SS1.p1.2.m2.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3"><eq id="S3.SS1.p1.2.m2.3.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2"></eq><apply id="S3.SS1.p1.2.m2.3.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.3.2">𝐿</ci><ci id="S3.SS1.p1.2.m2.3.3.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.3.3">𝒂</ci></apply><apply id="S3.SS1.p1.2.m2.3.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1"><minus id="S3.SS1.p1.2.m2.3.3.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1"></minus><apply id="S3.SS1.p1.2.m2.3.3.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1"><apply id="S3.SS1.p1.2.m2.3.3.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2">subscript</csymbol><sum id="S3.SS1.p1.2.m2.3.3.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2.2"></sum><ci id="S3.SS1.p1.2.m2.3.3.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1"><divide id="S3.SS1.p1.2.m2.3.3.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.2"></divide><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1"><times id="S3.SS1.p1.2.m2.3.3.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.2"></times><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3"><log id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.1"></log><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.3.2">𝑃</ci></apply><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.2">𝒂</ci><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.3.3">𝑡</ci></apply><list id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝒒</ci><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.2">𝒂</ci><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3"><lt id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></list></apply></apply><apply id="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.2"><abs id="S3.SS1.p1.2.m2.3.3.1.1.1.3.1.1.cmml" xref="S3.SS1.p1.2.m2.3.3.1.1.1.3.2.1"></abs><ci id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2">𝒂</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.3c">L_{\bm{a}}=-\sum_{t}{\log P(\bm{a}_{t}|\bm{q},\bm{a}_{&lt;t})}/|\bm{a}|</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.3d">italic_L start_POSTSUBSCRIPT bold_italic_a end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT roman_log italic_P ( bold_italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_italic_q , bold_italic_a start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT ) / | bold_italic_a |</annotation></semantics></math>.</p>
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F3" title="Figure 3 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;3</span></a> presents an example document alongside QA pairs, where tokens used for computing losses are highlighted.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Hyperparameters</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.2">AdamW <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib30" title="">2019</a>)</cite>와 <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><msub id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.2.2.cmml">β</mi><mn id="S3.SS2.p1.1.m1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><eq id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></eq><apply id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2">𝛽</ci><cn id="S3.SS2.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.2.3">1</cn></apply><cn id="S3.SS2.p1.1.m1.1.1.3.cmml" type="float" xref="S3.SS2.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\beta_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math>, <math alttext="\beta_{2}=0.95" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><msub id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2.2" xref="S3.SS2.p1.2.m2.1.1.2.2.cmml">β</mi><mn id="S3.SS2.p1.2.m2.1.1.2.3" xref="S3.SS2.p1.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">0.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><eq id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></eq><apply id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2.2">𝛽</ci><cn id="S3.SS2.p1.2.m2.1.1.2.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.2.3">2</cn></apply><cn id="S3.SS2.p1.2.m2.1.1.3.cmml" type="float" xref="S3.SS2.p1.2.m2.1.1.3">0.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\beta_{2}=0.95</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95</annotation></semantics></math>, 무게감쇠는 0.1이다. 웜업 없이 코사인 스케줄러를 사용하여 학습률을 초기값의 10%로 감쇠시킨다. 문서에 대한 사전 학습 시 256개의 문서 배치 크기와 3e-5의 초기 학습률을 사용한다. QA 쌍에 대한 명령어 조정 시 256개의 QA 쌍과 동일한 배치 크기를 사용하지만 계산 손실을 계산하는 데 사용되는 단일 배치의 토큰 수가 더 적기 때문에 5e-6의 감소된 초기 학습률을 선택한다. 에포크 수는 설정에 따라 달라지며 해당 섹션에 자세히 설명되어 있다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation Metrics</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">추론 시간에는 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F3" title="Figure 3 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 3</span></a>의 형식에 따라 질문이 주어진 답변을 컨텍스트로 생성하기 위해 탐욕적 디코딩을 사용한다. 원래 Llama-2를 평가하기 위해 5개의 QA 쌍을 컨텍스트 예제로 추가하여 QA 형식을 따르는지 확인한다. 대부분의 질문은 단순한 팩토이드 질문이고 대부분의 답변은 상대적으로 짧기 때문에 모델의 출력이 정규화 후 정확하게 골드 답변과 일치하는지 여부를 측정하는 기본 메트릭 <cite class="ltx_cite ltx_citemacro_cite">Kwiatkowski et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib25" title="">2019</a>)</cite>로 정확 일치(EM)를 사용한다(예: 기사 및 구두점 제거). 더 긴 응답을 평가하고 사소한 어휘 차이를 수용하기 위해 금 답이 모델의 출력에 나타나는지 평가하는 답변 회상과 모델의 출력과 금 답 사이의 가장 긴 공통 서브시퀀스를 측정하는 ROUGE-L도 보고한다.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="261" id="S3.F4.g1" src="https://arxiv.org/html/2402.12847v1/x5.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Different experimental settings examined in this paper. Each row represents a different experimental setting with a unique name and number, and each vertical section highlighted by a right-pointing light-blue triangle indicates a training phase. Models are assessed on test QA across all settings. Whenever multiple datasets are enclosed within a dashed square, they are mixed together during the training process.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning?</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">LLM의 파라미터에 저장된 사실적 지식은 추가적인 훈련 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib5" title="">2020</a>); Petroni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib37" title="">2019</a>); Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib22" title="">2020</a>); Roberts et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib42" title="">2020</a>)</cite> 없이 프롬프트를 통해 질문에 대한 답변에 접근하여 적용할 수 있다. 고품질 데이터 <cite class="ltx_cite ltx_citemacro_cite">Sanh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib44" title="">2022</a>); Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib56" title="">2022</a>)</cite>에 대한 추가 명령 조정(지도 미세 조정이라고도 함)을 사용하면 LLMs에서 지식을 보다 효과적으로 이끌어낼 수 있는 것 같다. 그러나 LLM이 질문에 올바르게 답할 때 사전 훈련 데이터의 다양성으로 인해 지식의 출처가 불분명하다. 예를 들어, “세계에서 가장 큰 빙상이 어디에 있는가”라는 질문에 답할 때, LLM은 남극 빙상에 대해 본 문서에서 정보를 회상하고 일반화하여 그들의 응답을 도출하거나, 훈련 데이터에서 마주치는 유사한 질문에서 답을 반복하는 것일까? 전자의 시나리오는 나중에 도출될 수 있는 방식으로 문서를 이해하고 매개 변수 내에 지식을 효과적으로 저장할 수 있는 능력을 의미하는 반면, 후자의 경우는 단순한 암기이기 때문에 이러한 차이는 매우 중요하다.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">여러 연구에서 이 문제를 연구했으며 주요 발견은 LMs가 <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib54" title="">2021</a>); Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>에서 훈련된 문서에 대한 질문에 답하기 어렵다는 것이다. 그러나 이러한 실험은 주로 BART, T5 또는 GPT-2 <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib54" title="">2021</a>); Jang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib20" title="">2022</a>); Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib16" title="">2023</a>)</cite>와 같이 상대적으로 작은 LMs를 사용하여, 무작위로 초기화된 트랜스포머 <cite class="ltx_cite ltx_citemacro_cite">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>를 사용하거나 명령어 조정 없이 <cite class="ltx_cite ltx_citemacro_cite">Ovadia et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib36" title="">2023</a>)</cite>를 사용하여 수행되었다는 점에 유의하는 것이 중요하다. 이를 통해 <em class="ltx_emph ltx_font_italic" id="S4.p2.1.1">새로운 문서로부터 지식을 흡수하고 이에 대한 질문에 표준 연속 사전 훈련 후 명령어 조정 레시피를 사용하여 대답하는 현대 LLM의 실제 한계가 무엇인지 궁금하다</em> 이 섹션에서는 한계를 테스트하기 위해 <span class="ltx_text ltx_font_typewriter" id="S4.p2.1.2">Wiki2023-film</span>에서 Llama-2 7B 및 70B를 사용하여 광범위한 실험을 실행합니다.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Vanilla Continued Pre-training and Instruction-tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Experimental settings</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">두 가지 표준 설정을 실험하고 관련 질문에 답하여 성능을 평가합니다.</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">계속된 사전 훈련: 명령어-튜닝 없이 테스트 문서 상에서 훈련(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➀). <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We found that LLMs struggle to adhere to the QA format after training on raw documents for multiple epochs. Therefore, we include a small set of QA pairs (64) during continued pre-training to prevent LLMs from forgetting the QA format.</span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">표준 명령 조정: 열차 QA 쌍에 대한 명령 조정 전 열차 및 테스트 문서 모두에 대한 열차(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➁)</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.2">우리는 단일 에폭에 대해 명령어 튜닝을 수행하는데, 이는 더 많은 에폭들이 일반적으로 감소된 성능을 초래하기 때문이다. 문서 교육을 위해 효과적인 지식 습득이 가능하고 적당한 크기의 코퍼라에 대해 저렴한 비용으로 사용할 수 있는 여러 시기(7B/70B 모델의 경우 10/5)를 선택합니다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Experimental results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 1</span></a>에 나타난 바와 같이, 원본 Llama-2 모델(7B/70B의 경우 9.5%/17.2%)의 상대적으로 낮은 성능은 테스트 문서 내의 대부분의 지식이 원본 사전 훈련 코퍼스에 포함되지 않음을 나타낸다. 문서에 대한 사전 교육을 계속한 후, 성능은 27.2%/41.7%로 증가하여 LLM이 어느 정도의 지식을 흡수할 수 있음을 나타낸다. 지침 조정은 성능을 30.3%/46.4%로 더욱 증가시켜 이 표준 레시피의 효과를 확인시켜준다. 이 관찰은 사전 훈련 후 명령어 조정이 무작위로 초기화된 GPT-2 유사 변압기에서 효과가 없음을 보여주는 <cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>와 다르다. 그 차이는 아마도 Llama-2가 원시 문서와 QA 데이터로 구성된 다양한 말뭉치에 대한 사전 교육을 통해 질문을 통해 매개 변수에서 지식을 추출하는 데 어느 정도 숙련도를 발전시켰기 때문에 발생할 것이다. 또한 해당 문서가 문맥(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 1</span></a>의 "open-book w/ doc")으로 Llama-2에 직접 제공되는 성능을 보고한다. 닫힌 책과 열린 책 설정 사이의 상당한 차이는 LLM의 매개변수에서 지식을 검색하는 것이 여전히 어렵다는 것을 시사한다.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S4.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="563" id="S4.F4.sf1.g1" src="https://arxiv.org/html/2402.12847v1/x6.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Training dynamics w/ (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➁) and w/o instruction-tuning (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➀). Reduction in perplexity consistently leads to improvement in QA accuracy, indicating that factual knowledge acquisition necessitates exhaustive loss minimization.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S4.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="513" id="S4.F4.sf2.g1" src="https://arxiv.org/html/2402.12847v1/x7.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Training dynamics with different learning rates (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;4</span></a>  ➀). After perplexity is minimized, larger learning rates usually lead to less overfitting to deceptive patterns in documents and better generalization when responding to questions.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>We vary the number of epochs (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf1" title="4(a) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(a)</span></a>) and learning rate (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf2" title="4(b) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(b)</span></a>) during continued pre-training to study the training dynamics of Llama-2 7B. The left axis is <span class="ltx_text" id="S4.F5.5.1" style="color:#000000;">QA accuracies</span> for test questions, measured by exact match. On the right axis, we display 2 metrics indicated by distinct colors: the <span class="ltx_text" id="S4.F5.6.2" style="color:#000000;">perplexity</span> of all tokens in the documents, and the <span class="ltx_text" id="S4.F5.7.3" style="color:#000000;">knowledge retention accuracy</span>, measured by QA accuracy on the Natural Questions dataset. We highlight situations where <span class="ltx_text" id="S4.F5.8.4" style="color:#000000;background-color:#F2F2FF;">perplexity of all document tokens is minimized to 1</span>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Analyzing the Training Dynamics: Perplexity and Generalization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">문서의 복잡성을 낮추면 관련 질문에 대한 답으로 일반화될 수 있는 방법은 무엇인가? 문서에 대한 지속적인 사전 학습을 위해 에포크 수(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf1" title="4(a) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(a)</span></a>)와 학습률(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf2" title="4(b) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(b)</span></a>)을 변화시키고, 학습 동학을 연구하기 위해 세 가지 메트릭을 모니터링한다. <span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Since we always decay the learning rate to 10% of its initial value, training for more epochs is not the same as continuing training from a checkpoint obtained after fewer epochs.</span></span></span></p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Knowledge acquisition</span>QA accuracyacies on test questions measured by exact match.</p>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Perplexity of documents</span> We compute perplexity (PPL) on all tokens within the documents.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">Knowledge retention</span> 우리는 NQ(Natural Questions) 데이터 세트에 대한 QA 정확도를 평가하여 사전 훈련 동안 축적된 지식의 유지를 근사화한다. NQ는 2019년에 발표되었으며 주로 당시 위키피디아 기사를 기반으로 한 질문을 포함한다.</p>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.2.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T1.1.2.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.2.1">Llama-2 7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.1.2.1.3">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.3.1">Llama-2 70B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.1.1">Settings</span></th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.2.1">EM</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.3">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.3.1">Rec.</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.4.1">R-L</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.5">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.5.1">EM</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.6">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.6.1">Rec.</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.3.2.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.7.1">R-L</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="S4.T1.1.4.3.1">
<em class="ltx_emph ltx_font_italic" id="S4.T1.1.4.3.1.1">closed- and open-book performance before training</em></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.4.1">closed-book</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.2">9.5</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.3">10.0</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.5.4.4">21.2</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.5">17.2</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.6">18.1</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.5.4.7">31.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.6.5.1">open-book w/ doc</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.2">72.2</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.3">75.4</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.6.5.4">91.5</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.5">78.2</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.6">80.6</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.6.5.7">94.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="S4.T1.1.7.6.1">
<em class="ltx_emph ltx_font_italic" id="S4.T1.1.7.6.1.1">closed-book performance w/ standard methods</em></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.8.7.1">cont. pre-training  ➀</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.2">27.6</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.3">31.6</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.8.7.4">43.8</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.5">41.7</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.6">45.8</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.8.7.7">60.2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.9.8.1"> +instruction-tuning  ➁</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.2">30.3</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.3">34.7</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.9.8.4">47.4</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.5">46.4</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.6">50.9</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.9.8.7">64.1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.10.9.1">mix all data  ➃</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.9.2">39.4</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.9.3">44.6</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.10.9.4">56.7</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.9.5">57.1</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.9.6">63.4</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.10.9.7">72.4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="S4.T1.1.11.10.1">
<em class="ltx_emph ltx_font_italic" id="S4.T1.1.11.10.1.1">closed-book performance w/ pre-instruction-tuning (PIT)</em></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.12.11.1">PIT (QA only)  ➄</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.2">28.6</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.3">32.7</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.12.11.4">45.2</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.5">49.7</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.6">53.7</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.12.11.7">67.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.1.1">PIT (QA <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">→</annotation></semantics></math> docs)  ➅</th>
<td class="ltx_td ltx_align_right" id="S4.T1.1.1.2">32.5</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.1.3">37.2</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T1.1.1.4">49.0</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.1.5">54.6</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.1.6">60.0</td>
<td class="ltx_td ltx_align_right" id="S4.T1.1.1.7">73.8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.13.12.1">PIT  ➆</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.13.12.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.2.1">45.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.13.12.3">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.3.1">51.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T1.1.13.12.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.4.1">63.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.13.12.5">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.5.1">62.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.13.12.6">
<span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.6.1">68.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.1.13.12.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.13.12.7.1">78.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of QA performance (%) between standard instruction-tuning and pre-instruction-tuning. The best results are in bold. Rec. is short for answer recall, and R-L refers to ROUGE-L.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Experiment results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf1" title="4(a) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(a)</span></a>에 도시된 바와 같이, QA 정확도는 퍼플렉시티가 1에 접근함에 따라 일관되게 개선되며, 이는 <em class="ltx_emph ltx_font_italic" id="S4.I3.i1.p1.1.1">사실적 지식 학습은 모든 토큰</em>에 걸쳐 완전한 손실 최소화를 필요로 함을 나타낸다. 이것은 지나치게 최적화하면 과적합으로 이어지는 일반적인 기술을 배우는 것과 대조된다.</p>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf1" title="4(a) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(a)</span></a> 및 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.F4.sf2" title="4(b) ‣ Figure 5 ‣ Experimental results ‣ 4.1 Vanilla Continued Pre-training and Instruction-tuning ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">4(b)</span></a>에 도시된 바와 같이, LLM이 문서에 대한 복잡도를 최소화한 모든 경우들 중에서, 더 많은 에폭 또는 더 큰 학습률로 트레이닝된 경우들은 전형적으로 우수한 QA 성능을 나타낸다. <em class="ltx_emph ltx_font_italic" id="S4.I3.i2.p1.1.1">보다 공격적인 훈련은 문서의 기만적인 패턴에 덜 과적합하고 질문에 응답할 때 더 나은 일반화로 이어진다</em></p>
</div>
</li>
</ul>
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">요약하면, 더 낮은 당혹도는 질문에 응답할 때 더 강한 일반화로 이어지지만 이전에 습득한 지식을 잊어버리는 희생을 치르게 된다.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.9">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.9.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T2.9.10.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T2.9.10.1.1.1">Setting names</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T2.9.10.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.9.10.1.2.1">Setting configurations</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.9.10.1.3">
<span class="ltx_text ltx_font_bold" id="S4.T2.9.10.1.3.1">EM</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.9.10.1.4">
<span class="ltx_text ltx_font_bold" id="S4.T2.9.10.1.4.1">Rec.</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.9.10.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.9.10.1.5.1">R-L</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.11.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5" id="S4.T2.9.11.2.1">
<em class="ltx_emph ltx_font_italic" id="S4.T2.9.11.2.1.1">baselines</em></th>
</tr>
<tr class="ltx_tr" id="S4.T2.9.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.12.3.1">continued pre-training  ➀</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.12.3.2">test doc</th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.12.3.3">27.6</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.12.3.4">31.6</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.12.3.5">43.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.1.2"> +instruction-tuning  ➁</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.1.1">train doc + test doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">→</annotation></semantics></math> train QA</th>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.3">30.3</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.4">34.7</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.5">47.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.2.2"> +instruction-tuning (w/o forget)  ➂</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.2.1">train doc + test doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.1.m1.1"><semantics id="S4.T2.2.2.1.m1.1a"><mo id="S4.T2.2.2.1.m1.1.1" stretchy="false" xref="S4.T2.2.2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><ci id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.1.m1.1d">→</annotation></semantics></math> train QA + test doc</th>
<td class="ltx_td ltx_align_right" id="S4.T2.2.2.3">30.2</td>
<td class="ltx_td ltx_align_right" id="S4.T2.2.2.4">34.1</td>
<td class="ltx_td ltx_align_right" id="S4.T2.2.2.5">46.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.3.2"> +instruction-tuning (w/o train doc)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.3.1">test doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.1.m1.1"><semantics id="S4.T2.3.3.1.m1.1a"><mo id="S4.T2.3.3.1.m1.1.1" stretchy="false" xref="S4.T2.3.3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><ci id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.1.m1.1d">→</annotation></semantics></math> train QA</th>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.3">27.1</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.4">30.7</td>
<td class="ltx_td ltx_align_right" id="S4.T2.3.3.5">42.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.13.4.1">weighted continued pre-training</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.13.4.2">test doc (weighted)</th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.13.4.3">27.7</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.13.4.4">32.7</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.13.4.5">43.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.4.2">adapted continued pre-training</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.4.1">train doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.4.4.1.m1.1"><semantics id="S4.T2.4.4.1.m1.1a"><mo id="S4.T2.4.4.1.m1.1.1" stretchy="false" xref="S4.T2.4.4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.m1.1b"><ci id="S4.T2.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.1.m1.1d">→</annotation></semantics></math> test doc</th>
<td class="ltx_td ltx_align_right" id="S4.T2.4.4.3">26.9</td>
<td class="ltx_td ltx_align_right" id="S4.T2.4.4.4">32.7</td>
<td class="ltx_td ltx_align_right" id="S4.T2.4.4.5">44.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.14.5.1">mix all data  ➃</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.14.5.2">train QA + train doc + test doc</th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.14.5.3">39.4</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.14.5.4">44.6</td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.14.5.5">56.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.15.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5" id="S4.T2.9.15.6.1">
<em class="ltx_emph ltx_font_italic" id="S4.T2.9.15.6.1.1">various pre-instruction-tuning (PIT) methods and ablation studies</em></th>
</tr>
<tr class="ltx_tr" id="S4.T2.5.5">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.5.5.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.5.1" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.5.5.1.1" style="background-color:#808080;">train QA + train doc (3 epochs) <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.5.5.1.1.m1.1" style="background-color:#808080;"><semantics id="S4.T2.5.5.1.1.m1.1a"><mo id="S4.T2.5.5.1.1.m1.1.1" mathbackground="#808080" stretchy="false" xref="S4.T2.5.5.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.1.1.m1.1b"><ci id="S4.T2.5.5.1.1.m1.1.1.cmml" xref="S4.T2.5.5.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.1.1.m1.1d">→</annotation></semantics></math> test doc</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.5.5.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.5.5.3.1" style="background-color:#808080;">45.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.5.5.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.5.5.4.1" style="background-color:#808080;">51.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.5.5.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.5.5.5.1" style="background-color:#808080;">63.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.16.7">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.9.16.7.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S4.T2.9.16.7.2" style="background-color:#808080;">
<em class="ltx_emph ltx_font_italic" id="S4.T2.9.16.7.2.1" style="background-color:#808080;">ablation studies of the number of epochs</em><span class="ltx_text" id="S4.T2.9.16.7.2.2" style="background-color:#808080;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T2.9.17.8">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.17.8.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.17.8.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.17.8.2.1" style="background-color:#808080;"> 1 epoch</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.17.8.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.17.8.3.1" style="background-color:#808080;">33.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.17.8.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.17.8.4.1" style="background-color:#808080;">39.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.17.8.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.17.8.5.1" style="background-color:#808080;">50.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.18.9">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.18.9.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.18.9.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.18.9.2.1" style="background-color:#808080;"> 5 epochs</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.18.9.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.18.9.3.1" style="background-color:#808080;">45.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.18.9.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.18.9.4.1" style="background-color:#808080;">52.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.18.9.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.18.9.5.1" style="background-color:#808080;">63.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.19.10">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.19.10.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.19.10.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.19.10.2.1" style="background-color:#808080;"> 10 pochs</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.19.10.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.19.10.3.1" style="background-color:#808080;">46.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.19.10.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.19.10.4.1" style="background-color:#808080;">52.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.19.10.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.19.10.5.1" style="background-color:#808080;">61.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.20.11">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.9.20.11.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S4.T2.9.20.11.2" style="background-color:#808080;">
<em class="ltx_emph ltx_font_italic" id="S4.T2.9.20.11.2.1" style="background-color:#808080;">ablation studies of different learning mechanisms</em><span class="ltx_text" id="S4.T2.9.20.11.2.2" style="background-color:#808080;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T2.9.21.12">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.21.12.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.21.12.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.21.12.2.1" style="background-color:#808080;"> QA before doc (grouped)</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.21.12.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.21.12.3.1" style="background-color:#808080;">38.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.21.12.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.21.12.4.1" style="background-color:#808080;">43.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.21.12.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.21.12.5.1" style="background-color:#808080;">56.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.22.13">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.22.13.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.22.13.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.22.13.2.1" style="background-color:#808080;"> QA after doc (grouped)</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.22.13.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.22.13.3.1" style="background-color:#808080;">27.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.22.13.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.22.13.4.1" style="background-color:#808080;">31.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.22.13.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.22.13.5.1" style="background-color:#808080;">42.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.23.14">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.9.23.14.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.23.14.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.23.14.2.1" style="background-color:#808080;"> QA before doc (interleaved)</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.23.14.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.23.14.3.1" style="background-color:#808080;">45.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.23.14.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.23.14.4.1" style="background-color:#808080;">51.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.23.14.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.23.14.5.1" style="background-color:#808080;">64.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.24.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.24.15.1" rowspan="-10" style="background-color:#808080;">
<span class="ltx_text" id="S4.T2.9.24.15.1.1" style="background-color:#808080;">PIT  ➆</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.9.24.15.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.24.15.2.1" style="background-color:#808080;"> QA after doc (interleaved)</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.9.24.15.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.24.15.3.1" style="background-color:#808080;">43.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.24.15.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.24.15.4.1" style="background-color:#808080;">49.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.9.24.15.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.9.24.15.5.1" style="background-color:#808080;">61.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.7.7.3" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.7.7.3.1" style="background-color:#808080;">PIT–</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.7.7.2" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.7.7.2.2" style="background-color:#808080;">train QA + train doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.6.6.1.1.m1.1" style="background-color:#808080;"><semantics id="S4.T2.6.6.1.1.m1.1a"><mo id="S4.T2.6.6.1.1.m1.1.1" mathbackground="#808080" stretchy="false" xref="S4.T2.6.6.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.1.1.m1.1b"><ci id="S4.T2.6.6.1.1.m1.1.1.cmml" xref="S4.T2.6.6.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.1.1.m1.1d">→</annotation></semantics></math> train QA <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.7.7.2.2.m2.1" style="background-color:#808080;"><semantics id="S4.T2.7.7.2.2.m2.1a"><mo id="S4.T2.7.7.2.2.m2.1.1" mathbackground="#808080" stretchy="false" xref="S4.T2.7.7.2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.2.2.m2.1b"><ci id="S4.T2.7.7.2.2.m2.1.1.cmml" xref="S4.T2.7.7.2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.2.2.m2.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.2.2.m2.1d">→</annotation></semantics></math> test doc</span></th>
<td class="ltx_td ltx_align_right" id="S4.T2.7.7.4" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.7.7.4.1" style="background-color:#808080;">44.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.7.7.5" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.7.7.5.1" style="background-color:#808080;">51.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T2.7.7.6" style="background-color:#808080;"><span class="ltx_text" id="S4.T2.7.7.6.1" style="background-color:#808080;">63.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.9.9.3" style="background-color:#CCCCCC;"><span class="ltx_text" id="S4.T2.9.9.3.1" style="background-color:#CCCCCC;">PIT++  ➇</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.9.9.2" style="background-color:#CCCCCC;"><span class="ltx_text" id="S4.T2.9.9.2.2" style="background-color:#CCCCCC;">train QA <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.8.8.1.1.m1.1" style="background-color:#CCCCCC;"><semantics id="S4.T2.8.8.1.1.m1.1a"><mo id="S4.T2.8.8.1.1.m1.1.1" mathbackground="#CCCCCC" stretchy="false" xref="S4.T2.8.8.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.1.1.m1.1b"><ci id="S4.T2.8.8.1.1.m1.1.1.cmml" xref="S4.T2.8.8.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.1.1.m1.1d">→</annotation></semantics></math> train QA + train doc <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S4.T2.9.9.2.2.m2.1" style="background-color:#CCCCCC;"><semantics id="S4.T2.9.9.2.2.m2.1a"><mo id="S4.T2.9.9.2.2.m2.1.1" mathbackground="#CCCCCC" stretchy="false" xref="S4.T2.9.9.2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.2.2.m2.1b"><ci id="S4.T2.9.9.2.2.m2.1.1.cmml" xref="S4.T2.9.9.2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.2.2.m2.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.2.2.m2.1d">→</annotation></semantics></math> test doc</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.9.9.4" style="background-color:#CCCCCC;"><span class="ltx_text ltx_font_bold" id="S4.T2.9.9.4.1" style="background-color:#CCCCCC;">48.1<span class="ltx_text ltx_font_medium" id="S4.T2.9.9.4.1.1" style="background-color:#CCCCCC;"></span></span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.9.9.5" style="background-color:#CCCCCC;"><span class="ltx_text ltx_font_bold" id="S4.T2.9.9.5.1" style="background-color:#CCCCCC;">54.4<span class="ltx_text ltx_font_medium" id="S4.T2.9.9.5.1.1" style="background-color:#CCCCCC;"></span></span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.9.9.6" style="background-color:#CCCCCC;"><span class="ltx_text ltx_font_bold" id="S4.T2.9.9.6.1" style="background-color:#CCCCCC;">66.4</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison (%) of various pre-instruction-tuning methods and ablation studies to identify the key contributors to improved performance using Llama-2 7B. Different background colors indicate different pre-instruction-tuning methods. The best results are in bold.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Improving LLMs in Absorbing Knowledge from Documents</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">표준적인 지시-조정을 통해 도출되는 지식의 양은 여전히 제한적이며, 문서의 당혹성을 최소화하더라도 우리가 "당혹성의 저주"라고 부르는 현상이다. 우리의 다음 질문은 어떻게 LLM이 문서로부터 지식을 흡수하여 복잡성 저주를 완화할 수 있는 능력을 향상시킬 수 있는가이다. 주요 과제는 지식이 원시문서에서 제시되는 방식과 질의응답을 통해 접근하는 방식의 괴리이다. 우리는 QA 쌍이 일반적으로 간단하지만 문서가 더 복잡하고 어수선하여 더 복잡한 방식으로 많은 사실 진술을 함께 엮는 경향이 있음을 발견했다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F3" title="Figure 3 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 3</span></a>를 예로 들면, “오펜하이머의 편집을 누가 처리했는가”라는 질문에 대한 답은 “오펜하이머”를 명시적으로 언급하지 않은 “편집은 제니퍼 라임에 의해 처리되었다.”라는 글의 중간에 있는 문장에 포함되어 있다. 훈련 동안 LLM은 문맥을 이해하고 "편집"이 매개변수에서 이 지식을 효과적으로 인코딩하기 위해 "오펜하이머의 편집"을 지칭한다는 것을 추론해야 한다.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>는 합성 전기에서 무작위로 초기화된 GPT-2 유사 변압기를 처음부터 훈련하여 이 문제를 연구하고 개인에 대한 질문에 답하는 능력을 평가했다. 그들은 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>의 ➃ 설정과 유사한 나머지 전기 절반에 대한 질문에 답할 때 전기와 전기 절반과 관련된 질문을 혼합하여 교육하는 것이 강력한 일반화로 이어진다는 것을 발견했다. 대조적으로, 전기 및 QA 쌍에 대한 교육은 순차적으로 실패했다. 그러나 데이터가 함께 혼합되었기 때문에 성공의 핵심 기여자는 여전히 불확실하며 새로운 문서의 지식을 흡수하기 위해 실제로 이를 적용하는 방법이 불분명하다. QA 쌍과 문서 사이의 서로 다른 난이도 수준과 <cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>의 발견에 영감을 받아 <em class="ltx_emph ltx_font_italic" id="S5.p2.1.1">을 계속 사전 훈련 전에 지시 조정 데이터에 의도적으로 LLMs를 노출시켜 복잡한 문서로부터 지식을 인코딩하는 프로세스가 이 지식에 액세스하는 방법을 고려하는 것이 유익하다고 가정한다. </em> 우리는 이것을 <span class="ltx_text ltx_font_bold" id="S5.p2.1.2">pre-instruction-tuning (PIT)</span>이라고 부르고 계속된 학습에 앞서 PIT의 다양한 구현을 연구한다(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS1" title="5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§ 5.1</span></a>), 이어서 성능에 기여하는 키들을 식별하는 상세한 제거들(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS2" title="5.2 Pre-instruction-tuning++ ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§ 5.2</span></a> 및 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS3" title="5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§ 5.3</span></a>), 그리고 마지막으로 PIT가 도메인에 걸쳐 얼마나 잘 수행하는지를 평가한다(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.SS4" title="5.4 Cross-domain Generalization ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§ 5.4</span></a>). <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.SS2" title="3.2 Hyperparameters ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">§ 3.2</span></a>에 요약된 하이퍼파라미터를 준수하고 달리 명시되지 않는 한 3개의 에폭에 대해 PIT를 수행한다.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Variants of Pre-instruction-tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Pre-instruction-tuning w/ QA only</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">우리는 문서에 대한 사전 교육을 계속하기 전에 명령어 조정 데이터를 노출하는 것으로 시작한다. 테스트 문서에 대한 교육을 하기 전에 국소적으로 관련된 QA 쌍에 대한 교육을 한다(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➄). 이는 계속되는 사전 훈련 설정(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➀)과 직접적으로 비교될 수 있다. 직관은 질문이 LLMs가 주요 유형의 정보를 인식하는 데 도움이 되어 질문이 문서와 직접 연결되지 않더라도 후속 문서에 대한 사전 훈련 중에 LLMs가 중요한 정보에 집중할 수 있도록 한다는 것이다. 예를 들어, “누가 오펜하이머의 편집을 처리했는가”와 같은 질문에 대한 교육은 “바비”와 같은 새로운 문서에 대한 교육을 할 때 LLM이 시나리오 작성자에게 주의를 기울이는 데 도움이 될 수 있다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 1</span></a>에 나타낸 바와 같이, 이 방법은 특히 더 큰 LLMs(7B/70B의 경우 27.6%/41.7% <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><mo id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" stretchy="false" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.1.m1.1d">→</annotation></semantics></math> 28.6%/49.7%)에서 계속되는 사전 훈련보다 우수하다. 문서들에 대한 트레이닝 후에 QA 데이터에 대해 트레이닝하는 절제(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 2</span></a>의 “명령-튜닝 w/o train doc”)는 비효율적이며, 문서들을 인코딩하기 전의 워밍업으로서 질문들에 대한 트레이닝의 중요성을 확인시켜준다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Pre-instruction-tuning on QA and documents sequentially</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">두 번째 구현은 단순화된 QA 쌍을 파악한 후 복잡한 문서에 LLM을 학습하면 문서로부터 지식을 흡수할 수 있는 능력이 강화될 수 있다는 직감으로 QA와 관련 문서를 순차적으로 학습한다(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➅). 예를 들어, LLM이 이미 “제니퍼 레임”이 “누가 오펜하이머의 편집을 처리했는가”에 대한 답이라는 것을 배웠다면, “편집은 제니퍼 레임에 의해 처리되었다”라는 문서에 대한 훈련은 매개 변수에 있는 지식의 저장을 더 효율적으로 정제할 수 있다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 1</span></a>와 같이 QA 쌍과 문서에 대한 PIT는 QA 전용 변형(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➄)과 표준 명령 조정(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➁)을 순차적으로 능가한다(7B/70B의 경우 30.3%/46.4% <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px2.p1.1.m1.1a"><mo id="S5.SS1.SSS0.Px2.p1.1.m1.1.1" stretchy="false" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S5.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px2.p1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px2.p1.1.m1.1d">→</annotation></semantics></math> 32.5%/54.6%).</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Pre-instruction-tuning </h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">PIT의 효과는 관련된 QA 쌍이 각각의 문서를 인코딩하기 전에 이미 학습되었는지 확인하는 것에 달려 있다. 그러나 문서(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➅의 훈련 doc)에 대한 훈련 후 해당 문항에 대한 정확도(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➅의 훈련 QA)가 거의 완벽에서 30%로 떨어져 심각한 망각임을 알 수 있었다. 이를 해결하기 위해 관련 QA 쌍과 문서를 함께 학습한다(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➆). <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T1" title="Table 1 ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 1</span></a>에 도시된 바와 같이, 이는 모든 데이터를 함께 혼합하는 것(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➃)을 포함한 다른 모든 접근법보다 큰 마진(7B/70B의 경우 39.4%/57.1% <math alttext="\shortrightarrow" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px3.p1.1.m1.1a"><mo id="S5.SS1.SSS0.Px3.p1.1.m1.1.1" stretchy="false" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px3.p1.1.m1.1b"><ci id="S5.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px3.p1.1.m1.1c">\shortrightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px3.p1.1.m1.1d">→</annotation></semantics></math> 45.5%/62.7%)만큼 성능을 크게 향상시킨다. QA 쌍과 문서 모두에 대한 훈련은 망각을 방지하지만 학습 프로세스가 작동하는 방식도 모호합니다. LLM이 문서로부터 지식을 인코딩하기 전에 QA 쌍을 파악하는지, 아니면 반대로 작동하는지는 불분명하다. 다음 섹션에서는 이를 조사하기 위해 훈련 중 QA 쌍과 문서의 순서를 의도적으로 배열하여 PIT의 개선된 버전을 제안한다.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="445" id="S5.F6.g1" src="https://arxiv.org/html/2402.12847v1/x8.png" width="829">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Different arrangements between QA pairs and corresponding documents. The ellipses represent other examples.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Pre-instruction-tuning++</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">우리는 먼저 에폭의 수에 따라 성능이 어떻게 달라지는지 연구한다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 2</span></a>와 같이 1 epoch에 대한 훈련이 부족하고 3, 5, 10 epoch의 성능도 유사하다. 에포크 수를 3으로 고정하고 QA 쌍과 해당 문서의 순서를 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.F6" title="Figure 6 ‣ Pre-instruction-tuning ‣ 5.1 Variants of Pre-instruction-tuning ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 6</span></a>와 같이 배열한다. 인터리브된 배열은 모든 데이터를 3번 반복하여 각 에포크에서 질문이 관련 문서에 선행하거나 후속하도록 한다. 반면에, 그룹화된 배열은 각각의 예의 3가지 모습을 함께 클러스터링하여, 반복되는 질문이 각각의 반복되는 문서 앞 또는 뒤에 위치한다는 것을 보장한다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 2</span></a>에 나타낸 바와 같이, 해당 문서 이전에 QA 쌍을 위치시키는 것은 그룹화된 배열과 인터리빙된 배열 모두에서 더 나은 성능을 달성하며, 이는 PIT 동안 학습 메커니즘이 더 복잡하고 정보 밀도가 높은 문서로부터 정보를 흡수하기 위해 학습 전에 지식에 액세스하는 방법을 이해하는 것을 우선시한다는 것을 나타낸다.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">이를 기반으로 지식 접근의 패턴을 이해하기 위해 QA 쌍만을 학습하고, 문서로부터 질문과 지식 인코딩을 통해 지식 접근을 정렬하기 위해 QA와 문서 데이터의 조합에 대한 학습으로 진행하는 pre-instruction-tuning++라는 개선된 변형을 제안한다(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➇). <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 2</span></a>에서 볼 수 있듯이 PIT++는 PIT(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➆)를 45.4%에서 48.1%로 크게 능가하는 반면, 믹스 후 QA 데이터에 대한 훈련(PIT- in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 2</span></a>)은 추가 이점을 제공하지 않는다. 이것은 지식이 접근하는 방법을 이해하는 것이 문서로부터 지식을 흡수하는 데 도움이 되므로 우선시되어야 한다는 우리의 가설을 강화한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Studies</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Standard instruction-tuning is inferior not due to forgetting</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p1.1">표준 명령어 튜닝의 단점은 QA 쌍에 대한 트레이닝 후에 테스트 문서의 지식이 잊혀질 수 있다는 것이다("정렬 세금" <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib35" title="">2022</a>)</cite>로도 알려진 현상). 표준 명령어 튜닝의 성능이 망각으로 인한 것이 아님을 보이기 위해 명령어 튜닝 시 테스트 문서와 훈련 QA를 혼합하여 망각을 방지하는 설정을 추가한다(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Evaluation Metrics ‣ 3 Experimental Settings ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig. 4</span></a>  ➂). <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 2</span></a>와 같이 이는 도움이 되지 않아 우리의 가설을 확인시켜준다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Pre-instruction-tuning is not simply upweighting salient tokens from documents</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib16" title="">2023</a>)</cite>에서 영감을 받은 삭제는 눈에 띄는 정보에 초점을 맞추기 위해 문서를 사전 훈련할 때 토큰을 가중화하는 것이다. 답변에 포함된 문서의 토큰에 1.0의 가중치를 부여하고(예: "편집은 제니퍼 라임에 의해 처리되었다" 문장에서 "제니퍼 라임"과 같이), 다른 토큰에 0.5의 낮은 가중치를 부여한다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S4.T2" title="Table 2 ‣ Experiment results ‣ 4.2 Analyzing the Training Dynamics: Perplexity and Generalization ‣ 4 How Much Knowledge Can LLMs Absorb via Continued Pre-training Followed by Instruction-tuning? ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 2</span></a>와 같이 이 가중 지속 사전 훈련은 효과가 없어 우리의 가설을 확인시켜준다.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T3.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S5.T3.1.1.1.2">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">Llama-2 7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S5.T3.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">Llama-2 70B</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.1.1">Settings</span></th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.2.2.2">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.2.1">EM</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.2.2.3">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.3.1">Rec.</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.4.1">R-L</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.2.2.5">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.5.1">EM</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.2.2.6">
<span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.6.1">Rec.</span></td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.2.7.1">R-L</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="S5.T3.1.3.3.1">
<em class="ltx_emph ltx_font_italic" id="S5.T3.1.3.3.1.1">standard instruction-tuning</em>  ➁</th>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.4.4.1">in-domain</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.2">30.3</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.3">34.7</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.1.4.4.4">47.4</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.5">46.4</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.6">50.9</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.4.4.7">64.1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.5.5.1">cross-domain</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.2">23.6</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.3">28.2</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.1.5.5.4">38.4</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.5">42.8</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.6">49.7</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.5.5.7">58.5</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7" id="S5.T3.1.6.6.1">
<em class="ltx_emph ltx_font_italic" id="S5.T3.1.6.6.1.1">pre-instruction-tuning</em>  ➆</th>
</tr>
<tr class="ltx_tr" id="S5.T3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.7.7.1">in-domain</th>
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.2">45.4</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.3">51.2</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S5.T3.1.7.7.4">63.2</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.5">62.7</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.6">68.6</td>
<td class="ltx_td ltx_align_right" id="S5.T3.1.7.7.7">78.8</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T3.1.8.8.1">cross-domain</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T3.1.8.8.2">36.9</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T3.1.8.8.3">43.2</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S5.T3.1.8.8.4">54.9</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T3.1.8.8.5">55.2</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T3.1.8.8.6">66.7</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T3.1.8.8.7">74.0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>In-domain and cross-domain PIT.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1">Settings</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1">EM</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1">Rec.</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.4.1">R-L</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S5.T4.1.2.2.1">
<em class="ltx_emph ltx_font_italic" id="S5.T4.1.2.2.1.1">generalization to the biography dataset <span class="ltx_text ltx_font_typewriter" id="S5.T4.1.2.2.1.1.1">bioS</span> </em></th>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.3.3.1">closed-book</th>
<td class="ltx_td ltx_align_right" id="S5.T4.1.3.3.2">2.9</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.3.3.3">2.9</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.3.3.4">11.0</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.4.4.1">open-book w/ doc</th>
<td class="ltx_td ltx_align_right" id="S5.T4.1.4.4.2">95.2</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.4.4.3">95.4</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.4.4.4">95.6</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.5.5.1">continued pre-training  ➀</th>
<td class="ltx_td ltx_align_right" id="S5.T4.1.5.5.2">29.6</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.5.5.3">29.8</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.5.5.4">38.7</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.6.6.1">pre-instruction-tuning  ➆</th>
<td class="ltx_td ltx_align_right" id="S5.T4.1.6.6.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.6.6.2.1">58.1</span></td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.6.6.3"><span class="ltx_text ltx_font_bold" id="S5.T4.1.6.6.3.1">58.4</span></td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.6.6.4"><span class="ltx_text ltx_font_bold" id="S5.T4.1.6.6.4.1">61.9</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S5.T4.1.7.7.1">
<em class="ltx_emph ltx_font_italic" id="S5.T4.1.7.7.1.1">generalization to questions by real users from Google </em></th>
</tr>
<tr class="ltx_tr" id="S5.T4.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.8.8.1">standard instruction-tuning  ➁</th>
<td class="ltx_td ltx_align_right" id="S5.T4.1.8.8.2">21.5</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.8.8.3">30.1</td>
<td class="ltx_td ltx_align_right" id="S5.T4.1.8.8.4">36.8</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.1.9.9.1">pre-instruction-tuning  ➆</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T4.1.9.9.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.9.9.2.1">29.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T4.1.9.9.3"><span class="ltx_text ltx_font_bold" id="S5.T4.1.9.9.3.1">35.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T4.1.9.9.4"><span class="ltx_text ltx_font_bold" id="S5.T4.1.9.9.4.1">48.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Generalization of the Llama-2 7B model trained with pre-instruction-tuning.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Cross-domain Generalization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">우리는 동일한 도메인(<span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.1">Wiki2023-film</span>)의 데이터에 대한 훈련 및 평가를 통해 PIT의 유효성을 검증했다.</p>
<em class="ltx_emph ltx_font_italic" id="S5.SS4.p1.1.2">Can PIT make LLMs better at absorbing knowledge from documents of a different domain?</em>
To this end, we follow the cross-domain setting outlined in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S2.F2" title="Figure 2 ‣ 2 Building a Dataset to Study Continual Knowledge Acquisition ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Fig.&nbsp;2</span></a>—training on other domains (<span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.3">Wiki2023-other-train</span>) and testing on the film domain (<span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.4">Wiki2023-film-test</span>).
The results of standard instruction-tuning and PIT, in both in-domain and cross-domain settings, are detailed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.T3" title="Table 3 ‣ Pre-instruction-tuning is not simply upweighting salient tokens from documents ‣ 5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab.&nbsp;3</span></a>.
Even though it is not as effective as the in-domain counterparts, cross-domain PIT still significantly outperforms instruction-tuning, demonstrating that it can generalize across different domains.
This finding sheds light on the potential to scale this method up to a broader range of documents and instructions for more robust generalization.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">또한 두 가지 다른 시나리오에서 PIT의 효과를 평가한다: (1) 비위키피디아 문서에 적용될 때, (2) 실제 사용자가 질문한 질문에 답할 때. 첫 번째 시나리오는 <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p2.1.1">2023Wiki-other</span>에서 PIT로 훈련된 Llama-2 7B 모델을 취하여 <cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>)</cite>에서 합성된 전기(<span class="ltx_text ltx_font_typewriter" id="S5.SS4.p2.1.2">bioS</span>)에서 추가로 훈련한다. 그런 다음 개인에 대한 질문을 기반으로 평가합니다. 두 번째 시나리오는 <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p2.1.3">Wiki2023-film-test</span>에서 LLMs에 의해 생성된 질문을 사용하여 구글을 수동으로 검색하고, 구글의 "People Also Ask" 기능을 활용하여 실제 사용자로부터 총 93개의 유사한 질문을 수집한 다음, 이러한 질문에 대해 Llama-2 7B를 평가한다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2402.12847v1#S5.T4" title="Table 4 ‣ Pre-instruction-tuning is not simply upweighting salient tokens from documents ‣ 5.3 Ablation Studies ‣ 5 Improving LLMs in Absorbing Knowledge from Documents ‣ Instruction-tuned Language Models are Better Knowledge Learners"><span class="ltx_text ltx_ref_tag">Tab. 4</span></a>와 같이 PIT는 두 시나리오 모두에서 기준선을 능가하여 일반화 능력을 보여준다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Continual Knowledge Acquisition</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">여러 연구에서 LMs가 학습한 문서의 정보에 대한 질문에 답할 수 있는지 여부를 연구했다.</p>
<cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib54" title="">2021</a>); Jang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib20" title="">2022</a>); Hu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib16" title="">2023</a>)</cite> use relatively small LMs such as BART <cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib27" title="">2020a</a>)</cite>, T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib41" title="">2020</a>)</cite>, or GPT-2 <cite class="ltx_cite ltx_citemacro_cite">Radford et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib39" title="">2019</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Ovadia et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib36" title="">2023</a>)</cite> focus on the comparison between RAG and continued pre-training approaches without using instruction-tuning.
<cite class="ltx_cite ltx_citemacro_citet">Zhu and Li (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib63" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib64" title="">b</a>)</cite> examine this problem from a similar angle as ours using a GPT-2-like transformer trained from scratch on synthetic biographies and fine-tuned on QA pairs related to the individuals.
They examined a mixed training setting on both biographies and QA pairs, which is our major motivation to study different strategies to incorporate QA data before continued pre-training.
Other works study adapting LLMs to new domains via various strategies <cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib59" title="">2023</a>); Cheng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib8" title="">2023</a>); Han et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib14" title="">2023</a>); Wu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib57" title="">2023</a>); Nguyen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib33" title="">2023</a>); Zhao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib61" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Instruction-tuning or Alignment</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">고품질 주석이 달린 데이터 <cite class="ltx_cite ltx_citemacro_cite">Sanh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib44" title="">2022</a>); Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib56" title="">2022</a>); Mishra et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib31" title="">2022</a>); Iyer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib18" title="">2022</a>); Kopf et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib24" title="">2023</a>); Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib62" title="">2023</a>); Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib47" title="">2023b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib46" title="">a</a>)</cite> 및/또는 독점 모델에 의해 생성된 데이터 <cite class="ltx_cite ltx_citemacro_cite">Taori et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib48" title="">2023</a>); Chiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib9" title="">2023</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib55" title="">2023b</a>); Ivison et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib17" title="">2023</a>)</cite> 또는 인간 피드백으로부터의 강화 학습과의 정렬(RLHF) 또는 직접 선호도 최적화(DPO) <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib35" title="">2022</a>); Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib52" title="">2023b</a>); Rafailov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib40" title="">2023</a>); Tian et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib49" title="">2023</a>)</cite>에 대한 명령 조정(지도 미세 조정이라고도 함)은 LLM으로부터 지식을 유도하고 사용자의 질문을 처리하는 다양한 능력을 향상시키기 때문에 최근 중심 주제이다. 우리는 사실성에 초점을 맞추고 LLM에서 사실적 지식을 이끌어내기 위해 수업 조정을 수행하는 가장 좋은 방법을 연구한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Analyzing the Training Dynamics of LMs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">많은 연구에서 LM의 훈련 역학을 다양한 관점에서 연구한다.</p>
<cite class="ltx_cite ltx_citemacro_citet">Carlini et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib6" title="">2022</a>)</cite> quantifies memorization across model sizes and the frequency of data duplication.
<cite class="ltx_cite ltx_citemacro_citet">Tirumala et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib50" title="">2022</a>)</cite> finds that larger LMs memorize training data faster with less overfitting.
<cite class="ltx_cite ltx_citemacro_citet">Xia et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib58" title="">2023</a>)</cite> show that perplexity is more predictive of model behaviors than other factors.
<cite class="ltx_cite ltx_citemacro_citet">Dery et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib11" title="">2022</a>)</cite> studies end-task aware pre-training using classification tasks and RoBERTa models.
Our work differs in that we specifically focus on the capacity of recalling and generalizing information from a seen document to answer questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Retrieval-augmented Generation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">RAG(Retrieval-augmented Generation)는 외부 소스 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib7" title="">2017</a>); Guu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib13" title="">2020</a>); Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib28" title="">2020b</a>); Borgeaud et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib4" title="">2022</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib53" title="">2023a</a>); Alon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib1" title="">2022</a>); He et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib15" title="">2021</a>); Sachan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib43" title="">2021</a>); Izacard et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib19" title="">2023</a>); Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib26" title="">2022</a>); Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib21" title="">2022</a>); Shi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib45" title="">2023</a>); Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib23" title="">2023</a>); Asai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib2" title="">2023</a>); Nakano et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib32" title="">2021</a>); Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib38" title="">2023</a>); Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12847v1#bib.bib29" title="">2023</a>)</cite>에서 검색된 정보를 고정 LLMs에 증강하여 새로운 지식을 LLMs에 통합하기 위해 널리 사용되는 접근법이다. RAG는 매개변수에 저장된 지식에만 의존할 때 일반적으로 경험하는 환각을 줄이는 데 효과적이지만 검색 및 생성 프로세스는 추가 지연과 복잡성을 추가한다. 대조적으로, 매개 변수에 지식을 저장하는 지속적인 사전 훈련과 닫힌 책 방식으로 질문에 답하기 위해 저장된 지식을 활용하는 것은 추론 시간에 더 간단하고 빠르다. 이 능력을 향상시키는 것은 또한 LLM을 정보에 액세스하기 위한 신뢰할 수 있는 보조자로 사용하는 기본 단계를 나타내기 때문에 과학적으로 중요하다. 따라서 본 논문에서는 파라메트릭 접근 방법을 탐색하는 데 중점을 둔다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">우리는 나중에 사실적 지식을 이끌어내는 것을 목표로 새로운 문서에 대한 지속적인 훈련의 가장 좋은 방법을 연구한다. 우리는 문서로부터 지식을 인코딩하기 전에 QA 쌍을 통해 지식이 액세스되는 방법을 학습하는 사전 명령 튜닝을 제안한다. 광범위한 실험은 사전 명령-튜닝 대 표준 명령-튜닝의 우수성을 입증한다. 향후 방향에는 이 방법을 보다 강력한 일반화를 위한 광범위한 범위의 문서 및 지침으로 확장하는 것이 포함된다.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1"><span class="ltx_text ltx_font_typewriter" id="Sx1.p1.1.1">Wiki2023</span> 데이터 세트는 지속적인 지식 획득을 연구하기 위한 비교적 깨끗한 테스트베드를 제공합니다. 그러나 그 범위는 커먼 크롤의 웹 페이지나 arXiv의 과학 문서와 같은 다른 소스에 대한 학습된 모델의 적응성을 제한하는 위키피디아로 제한된다. 본 논문에서는 QA 데이터에 대한 교수-조정을 통해 사실적 지식을 도출하는 데 중점을 둔다. 추론이나 이해력과 같은 다른 기술을 향상시키기 위한 다양한 유형의 데이터로 사전 수업 조정의 효과는 향후 연구에서 탐구될 필요가 있다.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">우리는 실험과 건설적인 피드백에 도움을 준 제위안 알렌주, 제우안 중, 슈얀 저우, 프랭크 F. 쉬, 첸 류, 루오홍 장에게 감사드린다.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alon et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Uri Alon, Frank&nbsp;F. Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022.

</span>
<span class="ltx_bibblock">Neuro-symbolic language modeling with automaton-augmented retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">International Conference on Machine Learning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.11511" title="">Self-rag: Learning to retrieve, generate, and critique through self-reflection</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">CoRR</em>, abs/2310.11511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berglund et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa&nbsp;Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.12288" title="">The reversal curse: Llms trained on "a is b" fail to learn "b is a"</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CoRR</em>, abs/2309.12288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van&nbsp;den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de&nbsp;Las&nbsp;Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack&nbsp;W. Rae, Erich Elsen, and Laurent Sifre. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v162/borgeaud22a.html" title="">Improving language models by retrieving from trillions of tokens</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, volume 162 of <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2">Proceedings of Machine Learning Research</em>, pages 2206–2240. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramèr, and Chiyuan Zhang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2202.07646" title="">Quantifying memorization across neural language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">CoRR</em>, abs/2202.07646.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-1171" title="">Reading wikipedia to answer open-domain questions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers</em>, pages 1870–1879. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Daixuan Cheng, Shaohan Huang, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.09530" title="">Adapting large language models via reading comprehension</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">CoRR</em>, abs/2309.09530.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi&nbsp;Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph&nbsp;E. Gonzalez, Ion Stoica, and Eric&nbsp;P. Xing. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi&nbsp;Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew&nbsp;M. Dai, Thanumalayan&nbsp;Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2204.02311" title="">Palm: Scaling language modeling with pathways</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">CoRR</em>, abs/2204.02311.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dery et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lucio&nbsp;M. Dery, Paul Michel, Ameet Talwalkar, and Graham Neubig. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=2bO2x8NAIMB" title="">Should we be pre-training? an argument for end-task aware training as an alternative</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini Team (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemini Team. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2312.11805" title="">Gemini: A family of highly capable multimodal models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2002.08909" title="">REALM: retrieval-augmented language model pre-training</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">CoRR</em>, abs/2002.08909.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tianyu Han, Lisa&nbsp;C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Löser, Daniel Truhn, and Keno&nbsp;K. Bressem. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2304.08247" title="">Medalpaca - an open-source collection of medical conversational AI models and training data</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">CoRR</em>, abs/2304.08247.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2021.

</span>
<span class="ltx_bibblock">Efficient nearest neighbor language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nathan Hu, Eric Mitchell, Christopher&nbsp;D. Manning, and Chelsea Finn. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.emnlp-main.268" title="">Meta-learning online adaptation of language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 4418–4432. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivison et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah&nbsp;A. Smith, Iz&nbsp;Beltagy, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.10702" title="">Camels in a changing climate: Enhancing LM adaptation with tulu 2</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">CoRR</em>, abs/2311.10702.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyer et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Srinivasan Iyer, Xi&nbsp;Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit&nbsp;Singh Koura, Xian Li, Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2212.12017" title="">OPT-IML: scaling language model instruction meta learning through the lens of generalization</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">CoRR</em>, abs/2212.12017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick S.&nbsp;H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v24/23-0037.html" title="">Atlas: Few-shot learning with retrieval augmented language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">J. Mach. Learn. Res.</em>, 24:251:1–251:43.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley&nbsp;Jungkyu Choi, and Minjoon Seo. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=vfsRB5MImo9" title="">Towards continual knowledge learning of language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki, Haibo Ding, Jamie Callan, and Graham Neubig. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.EMNLP-MAIN.149" title="">Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 2336–2349. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengbao Jiang, Frank&nbsp;F. Xu, Jun Araki, and Graham Neubig. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00324" title="">How can we know what language models know</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Trans. Assoc. Comput. Linguistics</em>, 8:423–438.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengbao Jiang, Frank&nbsp;F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.emnlp-main.495" title="">Active retrieval augmented generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 7969–7992. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopf et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi&nbsp;Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen&nbsp;Minh Duc, Oliver Stanley, Rich’ard Nagyfi, ES&nbsp;Shahul, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:258179434" title="">Openassistant conversations - democratizing large language model alignment</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">ArXiv</em>, abs/2304.07327.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur&nbsp;P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew&nbsp;M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00276" title="">Natural questions: a benchmark for question answering research</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Trans. Assoc. Comput. Linguistics</em>, 7:452–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher&nbsp;D. Manning, and Kyoung-Gu Woo. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.EMNLP-MAIN.198" title="">You only need one model for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>, pages 3047–3060. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.703" title="">BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>, pages 7871–7880. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Patrick S.&nbsp;H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="">Retrieval-augmented generation for knowledge-intensive NLP tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.01352" title="">RA-DIT: retrieval-augmented dual instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">CoRR</em>, abs/2310.01352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">Decoupled weight decay regularization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.ACL-LONG.244" title="">Cross-task generalization via natural language crowdsourcing instructions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 3470–3487. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu&nbsp;Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2112.09332" title="">Webgpt: Browser-assisted question-answering with human feedback</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">CoRR</em>, abs/2112.09332.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tuan&nbsp;Dung Nguyen, Yuan-Sen Ting, Ioana Ciuca, Charlie O’Neill, Ze-Chang Sun, Maja Jablonska, Sandor Kruk, Ernest Perkowski, Jack&nbsp;W. Miller, Jason Li, Josh Peek, Kartheik Iyer, Tomasz Rózanski, Pranav Khetarpal, Sharaf Zaman, David Brodrick, Sergio J.&nbsp;Rodríguez Méndez, Thang Bui, Alyssa Goodman, Alberto Accomazzi, Jill&nbsp;P. Naiman, Jesse Cranney, Kevin Schawinski, and UniverseTBD. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.06126" title="">Astrollama: Towards specialized foundation models in astronomy</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">CoRR</em>, abs/2309.06126.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2303.08774" title="">GPT-4 technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">CoRR</em>, abs/2303.08774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul&nbsp;F. Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2203.02155" title="">Training language models to follow instructions with human feedback</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">CoRR</em>, abs/2203.02155.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ovadia et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2312.05934" title="">Fine-tuning or retrieval? comparing knowledge injection in llms</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">CoRR</em>, abs/2312.05934.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S.&nbsp;H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander&nbsp;H. Miller. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1250" title="">Language models as knowledge bases?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>, pages 2463–2473. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu&nbsp;Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2305.06849" title="">Webcpm: Interactive web search for chinese long-form question answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">CoRR</em>, abs/2305.06849.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" title="">Language models are unsupervised multitask learners</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">OpenAI Blog</em>, 1(8).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher&nbsp;D. Manning, and Chelsea Finn. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.18290" title="">Direct preference optimization: Your language model is secretly a reward model</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">CoRR</em>, abs/2305.18290.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v21/20-074.html" title="">Exploring the limits of transfer learning with a unified text-to-text transformer</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">J. Mach. Learn. Res.</em>, 21:140:1–140:67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.437" title="">How much knowledge can you pack into the parameters of a language model?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, pages 5418–5426. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sachan et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Devendra&nbsp;Singh Sachan, Siva Reddy, William&nbsp;L. Hamilton, Chris Dyer, and Dani Yogatama. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2021/hash/da3fde159d754a2555eaa198d2d105b2-Abstract.html" title="">End-to-end training of multi-document reader and retriever for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual</em>, pages 25968–25981.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen&nbsp;H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M&nbsp;Saiful Bari, Canwen Xu, Urmish Thakker, Shanya&nbsp;Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal&nbsp;V. Nayak, Debajyoti Datta, Jonathan Chang, Mike&nbsp;Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng&nbsp;Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason&nbsp;Alan Fries, Ryan Teehan, Teven&nbsp;Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander&nbsp;M. Rush. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=9Vrb9D0WI4" title="">Multitask prompted training enables zero-shot task generalization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2301.12652" title="">REPLUG: retrieval-augmented black-box language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CoRR</em>, abs/2301.12652.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David&nbsp;D. Cox, Yiming Yang, and Chuang Gan. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.05910" title="">SALMON: self-alignment with principle-following reward models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">CoRR</em>, abs/2310.05910.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David&nbsp;D. Cox, Yiming Yang, and Chuang Gan. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.03047" title="">Principle-driven self-alignment of language models from scratch with minimal human supervision</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">CoRR</em>, abs/2305.03047.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher&nbsp;D. Manning, and Chelsea Finn. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.08401" title="">Fine-tuning language models for factuality</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">CoRR</em>, abs/2311.08401.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tirumala et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kushal Tirumala, Aram&nbsp;H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2022/hash/fa0509f4dab6807e2cb465715bf2d249-Abstract-Conference.html" title="">Memorization without overfitting: Analyzing the training dynamics of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2302.13971" title="">Llama: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">CoRR</em>, abs/2302.13971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">CoRR</em>, abs/2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi&nbsp;Dong, Oleksii Kuchaiev, Bo&nbsp;Li, Chaowei Xiao, Anima Anandkumar, and Bryan Catanzaro. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.emnlp-main.482" title="">Shall we pretrain autoregressive language models with retrieval? A comprehensive study</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 7763–7786. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Cunxiang Wang, Pai Liu, and Yue Zhang. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.ACL-LONG.251" title="">Can generative pre-trained language models serve as knowledge bases for closed-book qa?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>, pages 3241–3251. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi&nbsp;Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah&nbsp;A. Smith, Iz&nbsp;Beltagy, and Hannaneh Hajishirzi. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2306.04751" title="">How far can camels go? exploring the state of instruction tuning on open resources</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">CoRR</em>, abs/2306.04751.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent&nbsp;Y. Zhao, Kelvin Guu, Adams&nbsp;Wei Yu, Brian Lester, Nan Du, Andrew&nbsp;M. Dai, and Quoc&nbsp;V. Le. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=gEZrGCozdqR" title="">Finetuned language models are zero-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya&nbsp;Zhang, Yanfeng Wang, and Weidi Xie. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.14454" title="">Pmc-llama: Towards building open-source language models for medicine</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi&nbsp;Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Veselin Stoyanov. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.ACL-LONG.767" title="">Training trajectories of language models across scales</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 13711–13738. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, and Hongxia Yang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2311.10614" title="">A self-enhancement approach for domain-specific chatbot training via knowledge mining and digest</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">CoRR</em>, abs/2311.10614.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi&nbsp;Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit&nbsp;Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">ArXiv</em>, abs/2205.01068.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wayne&nbsp;Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2303.18223" title="">A survey of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">CoRR</em>, abs/2303.18223.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.11206" title="">LIMA: less is more for alignment</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">CoRR</em>, abs/2305.11206.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu and Li (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zeyuan&nbsp;Allen Zhu and Yuanzhi Li. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.14316" title="">Physics of language models: Part 3.1, knowledge storage and extraction</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">CoRR</em>, abs/2309.14316.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu and Li (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zeyuan&nbsp;Allen Zhu and Yuanzhi Li. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.14402" title="">Physics of language models: Part 3.2, knowledge manipulation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">CoRR</em>, abs/2309.14402.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
{
    "2403.08763": {
        "paper_id": "2403.08763",
        "abs_url": "https://arxiv.org/abs/2403.08763",
        "pdf_url": "https://arxiv.org/pdf/2403.08763.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2403.08763_Simple_and_Scalable_Strategies_to_Continually_Pre-train_Large_Language_Models.pdf",
        "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Adam Ibrahim",
            "Benjamin Th\u00e9rien",
            "Kshitij Gupta",
            "Mats L. Richter",
            "Quentin Anthony",
            "Timoth\u00e9e Lesort",
            "Eugene Belilovsky",
            "Irina Rish"
        ],
        "abstract": "Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\\rightarrow$English) and a stronger distribution shift (English$\\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.",
        "comments": "",
        "official_code_urls": [
            "https://github.com/eleutherai/gpt-neox"
        ],
        "pwc_page_url": "https://paperswithcode.com/paper/simple-and-scalable-strategies-to-continually",
        "bibtex": "@misc{ibrahim2024simple,\n      title={Simple and Scalable Strategies to Continually Pre-train Large Language Models}, \n      author={Adam Ibrahim and Benjamin Th\u00e9rien and Kshitij Gupta and Mats L. Richter and Quentin Anthony and Timoth\u00e9e Lesort and Eugene Belilovsky and Irina Rish},\n      year={2024},\n      eprint={2403.08763},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}"
    }
}
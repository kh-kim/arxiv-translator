# 대규모 언어 모델을 연속적으로 사전 훈련하기 위한 단순하고 확장 가능한 전략

Adam Ibrahim\({}^{*\dagger}\)

Benjamin Therien

Kshitij Gupta

Mats L. 리차드

Quentin Anthony

Timothee Lesort

Eugene Belilovsky

Fina Rish

_캐나다 몬트리올 주 몬트리올 대학교 컴퓨터 과학 및 운영 연구부 \(\dagger\) 캐나다 콘코디아 대학교 몬트리올 주 몬트리올 밀라 몬트리올 캐나다 엘러터AI \(\otimes\) 엘러터AI \(\Diamond\)_

동일한 기여; 동일한 기여자 내의 저자 순서는 무작위화되었다.

###### Abstract

대규모 언어 모델(LLM)은 수십억 토큰에 대해 일상적으로 사전 훈련되며, 단지 새로운 데이터가 이용가능해지면 프로세스를 다시 시작하기 위한 것이다. 보다 효율적인 솔루션은 이러한 모델을 지속적으로 사전 훈련하는 것입니다. - 재 훈련에 비해 상당한 계산을 절약합니다. 그러나, 새로운 데이터에 의해 유도된 분포 이동은 일반적으로 이전 데이터에 대한 성능 저하 또는 새로운 데이터에 대한 열악한 적응을 초래한다. 본 연구에서는 학습률(LR) 재-온난화, LR 재-붕괴 및 이전 데이터의 재생의 단순하고 확장 가능한 조합이 최종 손실 및 여러 언어 모델(LM) 평가 벤치마크에 대한 평균 점수로 측정된 모든 사용 가능한 데이터에 대해 처음부터 완전히 재-훈련의 성능과 일치하기에 충분하다는 것을 보여준다. 특히, 일반적으로 사용되는 두 LLM 사전 훈련 데이터 세트(영어\(\rightarrow\)English)와 큰 데이터 세트 크기(수천억 토큰)를 갖는 405M 매개변수 모델 규모에서 더 강한 분포 이동(영어\(\rightarrow\)German) 사이의 약하지만 현실적인 분포 이동에 대해 이를 보여준다. 대규모 실험을 위해 약하지만 현실적인 이동을 선택하면 지속적인 학습 전략이 10B 매개변수 LLM에 대한 재훈련 기준선과 일치한다는 것도 발견했다. 우리의 결과는 LLM이 계산의 일부만을 사용하여 재훈련 기준선과 일치하는 단순하고 확장 가능한 연속 학습 전략을 통해 성공적으로 업데이트될 수 있음을 보여준다. 마지막으로, 이전 연구에서 영감을 받아 LR 재-워밍에 의해 유발된 망각을 피할 수 있도록 돕고 고정된 토큰 버짓에 구속되지 않는 코사인 학습 속도 일정에 대한 대안을 제안한다.

## 1 Introduction

지난 몇 년 동안, 대규모 사전 훈련된 모델들은 언어 모델링(Brown et al., 2020; Zhao et al., 2023), 시각적 이해(Radford et al., 2021; Alayrac et al., 2022; Kirillov et al., 2023), 텍스트-대-이미지 생성(Rombach et al., 2022; Pernias et al., 2024), 및 텍스트-대-비디오 생성(Brooks et al., 2024)-의 대규모 성능 개선을 가능하게 했다. 큰 언어 모델(LLM)은 이러한 모든 개선의 중심에 있으며, 인간이 언어를 통해 기계 학습 알고리즘과 인터페이스할 수 있는 직관적인 수단을 제공한다.

LLM이 현재 생성 AI 기술의 초석이지만, 훈련하고 최신 정보를 유지하는 데 엄청나게 비용이 많이 든다. 그러나, 새롭고 고품질의 데이터 세트가 계속해서 이용 가능하게 됨에 따라(Gao 등, 2020; Soboleva 등, 2023; Computer, 2023; Soldaini 등, 2024), 조직들은 경쟁에 보조를 맞추기 위해 그들의 모델을 업데이트해야 할 것이다. 현재 LLM은 오래된 데이터와 새로 수집된 데이터의 조합에 대해 다시 훈련된다. 기존의 연구들은 저가의 하이퍼파라미터 최적화를 가능하게 하거나(Yang et al., 2022), 주어진 컴퓨트 예산 하에서 성능을 최대화하기 위한 가이드라인을 제공함으로써 이러한 트레이닝 비용을 감소시키는 것을 목표로 한다(Hoffmann et al., 2022). 그러나 이러한 작업은 모델이 _랜덤 초기화에서 훈련됨_ 이라고 가정하므로 다음과 같은 질문이 제기됩니다. 실무자는 항상 기존 데이터 세트를 결합하고 _랜덤 초기화에서 훈련됨_ 을 사용하여 최상의 성능을 얻어야 합니까? 모델의 모든 업데이트에 대해 그렇게 하는 것은 빠르게 엄청나게 비싸집니다.

완전한 재훈련을 피하기 위해 대량의 새로운 데이터(200B+ 토큰)에 대해 LLM(최대 10B 파라미터)을 사전 훈련하기 위한 간단하고 확장 가능한 연속 학습 전략을 탐구한다. 우리는 우리의 설정을 "지속적인 사전 훈련"으로 지칭하고 우리가 고려하는 많은 양의 들어오는 데이터로 인해 문헌(Gururangan et al., 2020; Ke et al., 2022; Scialom et al., 2022; Xie et al., 2023)의 기존 설정과 _별개_임을 강조한다. 이 작업에서는 사용 가능한 모든 데이터에 대해 무작위 초기화로 훈련된 모델의 성능을 개선하려는 의도가 없다. 대신, 우리는 기존 데이터 세트의 결합에 대해 훈련된 모델을 규모에서 지속적인 학습 전략의 조합을 사용하여 성능이 일치하도록 추구하는 기준선으로 간주한다.

그러나 새로운 데이터에 대한 모델을 순진하게 계속 학습하면 1) 열악한 적응(새로운 데이터 세트를 최적화하지 못함) 또는 2) 치명적인 망각(이전 데이터 세트에서 상당한 능력 손실)으로 인해 사용 가능한 모든 데이터에 대한 재학습보다 훨씬 낮은 성능으로 이어지는 경향이 있다. 첫째, 대규모 데이터 세트에 대한 훈련은 비용이 많이 들기 때문에 적응 문제는 우리 설정의 핵심이다. 아마도 새로운 데이터 세트에 최소한의 적응만을 위해 상당한 계산 자원 훈련을 사용하기로 선택하지 않을 것이다. 그러나, 대부분의 성능 있는 오픈 소스 LLMs(Touvron et al., 2023; Jiang et al., 2023; Gemma Team et al., 2024)는 트레이닝이 종료될 때까지 그들의 학습률을 작은 값으로 붕괴시킨다. 따라서 우리는 새로운 데이터 세트에서 훈련할 때 지출되는 컴퓨팅당 적응을 개선하기 위해 학습 속도가 다시 증가하고 다시 감소해야 한다고 가정한다. 우리는 이것이 지속적인 학습 문헌에서 철저히 연구되지 않았다는 점에 주목한다. 둘째, 파국적인 망각은 지속적인 사전 훈련의 잠재력을 완전히 실현하는 것이라면 극복해야 할 핵심 어려움이다. 수천억 개의 새로운 토큰에 적응하는 것도 중요하지만, LLM에서 현존하는 대부분의 지식을 지우는 대가를 치르게 해서는 안 된다. 최근 작업(사이알롬 등, 2022)은 LLM 미세 조정 환경에서 이전 데이터(1% 이하)를 다시 재생하면 망각을 크게 완화하기에 충분함을 보여준다. 많은 양의 새로운 데이터에 대한 지속적인 사전 훈련은 미세 조정보다 더 많은 망각을 초래할 것이지만, 우리는 적절한 양의 재생이 우리의 환경에서도 망각을 완화할 수 있다고 가정한다. 또한, 최근 연구는 사전 훈련(Cossu et al., 2022; Ramasesh et al., 2022; Mehta et al., 2023)과 모델 크기 증가(Mirzadeh et al., 2022)가 망각의 영향을 줄이는 데 도움이 된다는 것을 보여준다. 따라서 언어 모델 용량 및 사전 훈련 데이터 세트 크기가 탠덤으로 증가하는 추세(Kaplan et al., 2020; Hoffmann et al., 2022; Touvron et al., 2023)는 지속적인 학습이 점점 더 가능한 모델을 산출할 것으로 예상하며(Scialom et al., 2022), 실험 결과는 모델 규모만으로 개선되어야 함을 시사한다.

지속적인 학습이 재학습 모델과 관련된 비용을 상당히 줄일 수 있는 큰 잠재력과 LLM이 강력한 지속 학습자가 될 수 있는 잠재력을 감안할 때, 우리는 다음과 같은 질문을 던집니다. _간단하고 확장 가능한 연속 학습 기술이 적용될 때 모든 데이터의 연합에 대한 무작위 초기화에서 사전 학습된 LLM에 비해 지속적으로 사전 학습된 LLM 간의 성능 차이는 무엇입니까?_ 이 질문에 답하기 위해 우리는 LLM 사전 훈련을 위한 지속적인 학습 기법에 대한 대규모 실증적 연구를 수행한다. 우리의 경험적 평가는 약(영어 \(\rightarrow\) 영어) 및 강(영어 \(\rightarrow\) 독일어) 분포 이동뿐만 아니라 큰(10B 매개변수) 및 작은(405M 매개변수) 모델에 걸쳐 있다. 우리의 주요 기여는 다음과 같이 요약할 수 있다:

1. 코사인 스케줄을 사용하여 사전 트레이닝된 모델들에 대해 학습률 재-온난화 및 재-붕괴의 효과를 확립하여, 지속적인 사전 트레이닝 동안 적응을 위해 재-온난화 및 재-붕괴가 필요하다는 것을 보여준다.
2. 2개의 분포 시프트 및 많은 리플레이 백분율에 걸쳐 컴퓨팅 상수를 유지하면서 이전 데이터를 리플레이하는 효과를 설정한다. 우리는 수천억 개의 새로운 토큰에 대한 모델을 업데이트할 때에도 적절한 양의 리플레이로 망각을 크게 완화할 수 있음을 발견했다.
3. 두 모델 크기 및 분포 시프트에 걸쳐, LR 재-온난화, LR 재-감쇠 및 컴퓨팅-등가 재생의 단순하고 확장가능한 조합이, 상당히 적은 컴퓨팅을 사용하면서 모든 데이터의 연합에서 재-훈련된 모델들과 평균적으로 유사한 성능을 지속적으로 미리 훈련된 모델들이 달성할 수 있음을 입증한다.
4. 학습률 재-온난화와 연관된 최적화 어려움들을 회피하기 위한 유망한 방법으로서, LLM들의 연속적인 사전 트레이닝을 위한 무한 학습률 스케쥴들(데이터 세트에 걸쳐 원활한 전이를 허용하는 스케쥴들)을 제안한다.

출판 시 코드와 최종 모델 체크포인트를 공개적으로 사용할 수 있도록 하겠습니다. 이 작업의 예비 버전은 (Gupta et al., 2023)에서 ICML 2023 작업장 논문으로 이용 가능하게 되었다.

## 2 Main findings and Takeaways

우리의 실험 결과는 연속적으로 사전 훈련된 LLM이 두 개 이상의 사전 훈련 단계를 순차적으로 거친다고 가정한다. 즉, 연속적으로 사전 훈련된 LLM이 랜덤하게 초기화되고 데이터 세트 \(\mathcal{D}_{0},\mathcal{D}_{1},\ldots,\mathcal{D}_{N-1}\)에서 사전 훈련되는 상황에서 \(N\geq 2\)와 \(\textit{tokens}(\mathcal{D}_{i})\geq 100\)B 순서로 적용된다. 우리는 문제의 LLM이 이미 \(\mathcal{D}_{0}\)에서 사전 훈련된 오픈 소스 모델(Touvron et al., 2023; 20; Jiang et al., 2023; Gemma Team et al., 2024)인 상황과 조직이 새로운 데이터에 대해 지속적으로 사전 훈련할 의도로 초기 LLM을 훈련하기를 원할 수 있는 상황을 포함한다. 새로운 데이터는 약한 분포 시프트(예를 들어, 상이한 도메인의 최신 웹-스크래프)에 대응하는 이전 데이터와 유사하거나, 강한 분포 시프트(예를 들어, 완전히 새로운 언어로부터의 데이터)에 대응하는 이전 데이터와 상당히 상이할 수 있다. 우리의 실험 평가는 이러한 어려움들을 설명하는데, LR 재-온난화, LR 재-붕괴 및 재생(replay)을 적절하게 적용하는 것이 약하고 강한 분포 시프트들 및 두 개의 모델 크기들에 걸쳐 재-트레이닝의 성능을 매칭시키기에 충분하다는 것을 발견한다(참조).

그림 1: **지속적인 사전 훈련은 유사한 최종 유효성 검사 및 평균 평가 성능을 유지하면서 모델을 업데이트하는 계산 비용을 줄입니다. Pile \(\cup\) SlimPajama(SP)/German(Ger) 베이스라인 모델에 대해 성능 상한으로 간주되는 두 데이터 세트의 결합에 대해 훈련된 결과를 보고한다. 또한 두 개의 연속 사전 훈련된 모델에 대한 성능을 보고한다. "파일 상의 PT"는 사전 훈련된 파일 체크포인트에서 출발하여 학습률 재가열과 재탈락만을 사용하는 반면, "리플레이(PT on Pile)"는 학습률을 재가열하고 재탈락하며 SlimPajama는 5%, 독일어는 25%의 리플레이를 사용한다. LR 재-온난화, 재-감쇠 및 재생의 조합은 우리의 지속적으로 사전 트레이닝된 모델이 실질적으로 적은 컴퓨트를 요구하면서도 베이스라인 모델과 유사한 평균 성능을 달성할 수 있게 한다는 것을 관찰한다. 이 설정은 미리 훈련된 모델을 사용할 수 있다고 가정합니다 (예: HuggingFace 허브를 통해 또는 지속적으로 미리 훈련되도록 설계된 사내 모델을 통해).**

도. 1). 연구 결과를 커뮤니티에 가능한 한 쉽게 액세스할 수 있도록 이제 연구 결과를 적용하기 위한 _엄지 규칙_ 을 제공합니다.

```
지속적인 사전 훈련을 위한 경험 규칙
```

**Caveat**--다음 지침은 _현재 지식_ 에 맞게 작성됩니다.

**Learning rate schedule:**

* 초기 데이터 세트에 대한 사전 훈련 중에 학습률이 큰 값 \(\eta_{max}\)에서 작은 값 \(\eta_{min}\)으로 코사인 분해된 경우 다음 지침은 모델을 지속적으로 사전 훈련하는 데 도움이 될 수 있습니다.
* 학습률을 \(\mathcal{O}(\eta_{max})\)에서 \(\mathcal{O}(\eta_{min})\)로 재-온난화 및 재-감쇠하는 것은 예를 들어 작은 학습률 \(\mathcal{O}(\eta_{min})\)에서 계속되는 것에 비해 새로운 데이터 세트에 대한 적응을 향상시킨다.
* 일정의 최대 학습률을 줄이면 잊어버림을 줄이는 데 도움이 되는 반면, 일정을 늘리면 적응을 향상시킬 수 있습니다.
* 무한 LR 스케줄은 코사인 붕괴 스케줄에 대한 유망한 대안이다. 그들은 태스크들 사이에서 높은 일정한 학습 속도로 전이하여, 태스크들 사이의 LR을 재-워밍하는 것을 피함으로써 최적화 관련 망각을 방지하는 것을 돕는다. 또한 최종 지수 감쇠가 훈련 중 임의의 시점에서 수렴하도록 모델을 훈련시키는 데 사용될 수 있기 때문에 토큰의 특정 예산에 전념하는 것을 피한다.
**Replay:**
* 기본값으로 5% 다시 보기를 권장 합니다. 더 많은 리플레이는 더 강한 분배 시프트와 함께 사용되어야 하며, 약한 분배 시프트에 대해서는 적게는 1%로 벗어날 수 있다.

## 3 관련 작업

### Continual learning

지속적 학습(CL) 접근법은 사전 훈련을 통해 수집된 지식을 유지하면서 새로운 데이터에 적응하면서 진화하는 데이터 분포로부터 학습하는 것을 목표로 한다(French, 1999; Rolnick et al., 2019; Caccia et al., 2020; Lesort et al., 2021). 지속적인 학습의 핵심 과제는 과거의 정보를 잊는 것을 피하는 동시에 새로운 정보에 적응하는 것이다. 이러한 트레이드오프는 강성-가소성 딜레마(Mermillod et al., 2013; Ostapenko et al., 2019; Riemer et al., 2019)로 알려져 있다.

CL 접근법은 스크래치로부터 재훈련을 피하거나 데이터 가용성 문제를 해결하기 위해 소규모 설정에서도 편리하다(Smith et al., 2021). 그러나, 규모에서 CL은 편리함 그 이상이며, 계속해서 수집된 엄청난 양의 데이터를 처리해야 할 수도 있다. 최근 트레이닝 규모의 증가는, 특히 LLMs(Scao et al., 2022; Brown et al., 2020; Zhao et al., 2023)에 대해, CL이 재-트레이닝의 비용을 감소시키고 메모리, 컴퓨팅, 및 저장에 대한 효율성을 증가시키기 위한 새로운 기회들을 제공한다(Prabhu et al., 2023; Aljundi et al., 2019; Harun et al., 2023; Veniat et al., 2021; Harun et al., 2023). 연합 학습이 공간에 공동 배치된 상이한 에이전트들 간의 컴퓨팅 및 데이터의 공유를 가능하게 할 수 있는 것처럼(McMahan et al., 2017; Reddi et al., 2021; Douillard et al., 2023; Ryabinin et al., 2021), 지속적인 학습은 컴퓨팅 및 데이터의 공유를 시간을 통해 점진적으로 허용하고 대규모 훈련에 유용한 도구가 될 수 있다.

최근 연구는 SGD 및 Adam과 같은 최적화기가 CL(Lesort et al., 2023)에 대해 대규모로 유익할 수 있는 흥미로운 지식 보유 특성을 DNN에서 가지며, 단지 소량의 재생만이 지식 축적을 증가시키기에 충분할 수 있다는 것을 보여준다(Scialom et al., 2022). 본 연구에서는 대규모 언어 모델 사전 훈련의 맥락에서 이러한 접근법의 효율성을 얻고 적절한 학습률 스케줄링 및 재생 정책으로 개선하고자 한다.

### 사전 훈련, 모델 규모 및 연속 학습

기존의 여러 연구에서는 사전 훈련과 모델 척도가 지속적인 학습에 미치는 영향을 평가한다. Cossu et al.(2022)은 언어와 비전에 대한 사전 훈련 시나리오를 조사한다. 그들은 감독되지 않고 자기 감독되는 사전 훈련이 망각을 완화하는 데 근본적인 역할을 하는 반면, 감독은 성과를 해친다는 것을 발견한다. 유사하게, Mehta et al.(2023)은 사전 훈련된 모델이 손실 경관의 평평한 영역에 놓여 있는 무게 때문에 무작위로 초기화된 모델보다 덜 잊어버린다는 것을 발견했다. 그들은 또한 더 큰 모델들이 Ramasesh et al.(2022); Mirzadeh et al.(2022)의 발견과 관련이 있는 덜 잊는다는 것을 발견했다. 전자는 사전 훈련된 모델이 확장될 때 덜 잊는다는 것을 발견했으며, 이는 축척에 따라 더 직교적으로 증가하는 숨겨진 표현 때문일 수 있음을 시사한다. 후자는 더 넓은 신경망이 매개변수 등가 더 깊은 대응물에 비해 덜 잊는다는 것을 발견한다. Hernandez et al. (2021)은 전달에 대한 스케일링 법칙을 확립한다: 새로운 태스크에서 신경망의 성능을 파라미터 카운트 및 사전 트레이닝 데이터세트 크기의 함수로서 예측할 수 있는 방정식들. 저자들은 이 양의 전달이 매개변수 수가 증가함에 따라 일관되게 개선된다는 것을 발견했다. 마지막으로, Scialom et al.(2022)은 자기회귀 LLM이 사전 훈련 목표와 관련된 가설을 지속적으로 학습할 수 있는 강력한 능력을 가지고 있음을 보여준다.

### 도메인 적응형 연속 사전 훈련 (DACPT)

기존 작업은 레이블이 지정되지 않은 일련의 도메인이 LM에 순차적으로 사용 가능하게 되는 설정인 도메인 적응 연속 사전 훈련(DACPT)을 고려하고 실무자는 각 도메인에 걸쳐 성능을 유지하면서 자체 감독 방식으로 각 도메인에서 훈련하기를 원한다. 목표는 우리와 유사하지만 도메인별 데이터 세트와 달리 많은 도메인이 혼합된 범용 사전 훈련 데이터 세트를 고려한다. Ke et al.(2022)은 새로운 도메인에서 트레이닝할 때 이전 도메인으로부터의 데이터가 이용가능하지 않다고 가정하고 마스킹된 언어 모델링(MLM) 목적으로 사전 트레이닝할 때 망각을 방지하기 위해 모든 이전 태스크에 대한 파라미터의 중요도 마스크를 포함하는 이 설정을 위한 새로운 기술을 개발한다. Gururangan et al.(2020)은 RoBERTa(MLM)의 도메인 및 태스크 적응형 사전 훈련을 조사하고 효율적인 연속 사전 훈련을 위한 샘플 선택 전략에 기여했다. 마찬가지로, Xie et al.(2023)도 연속 사전 훈련(자기 회귀 LLMs에 대해 도시됨)의 계산 비용을 감소시키는 데이터 선택 전략을 제안한다. Qin et al. (2023)은 베이스 LM의 지속적으로 업데이트된 버전을 특정 작업에 적응시키기 위한 새로운 어댑터의 초기화로서 이전 베이스 LM의 재순환 미세 조정 어댑터 층을 조사한다. 최근 Wu 등(2024)은 이전의 지식을 잊지 않고 새로운 과제를 학습할 수 있도록 하는 LLM의 지속적인 사전 학습을 위한 방법인 LLaMA Pro를 제안하였다. 그러나 기존의 모든 가중치를 적용하는 것과 달리 LLaMA Pro는 새로운 업데이트마다 모델의 크기를 늘리고 새로운 가중치만 조정해야 한다.

### 특정 도메인에 적용된 LMs에 대한 연속 학습

여러 관련 작업은 특정 작업 및 도메인에 지속적인 사전 훈련을 적용한다(Sun et al., 2020; Jang et al., 2022; 2022; Gong et al., 2022; Zan et al., 2022; Yadav et al., 2023; Ma et al., 2023; Yang et al., 2024). 이 작업들은 또한 지속적인 사전 훈련 기술을 활용하지만, 일반적인 사전 훈련 기술 대신 특정 도메인에 초점을 맞추고 더 작은 모델을 가진 소규모 데이터 세트 \(<\) 10B 토큰에 초점을 맞춘다는 점에서 우리의 작업과 다르다. 우리의 데이터세트 규모에 접근하는 유일한 기존 작업은 영어, 덴마크, 아이슬란드 및 노르웨이 데이터세트(각각 73B) 전반에 걸쳐 지속적인 자기회귀 언어 모델링을 탐구하는 (고굴루 등, 2023)이다. 그들이 리플레이를 사용하지 않는 동안, 그들은 다시 따뜻해지고 학습률을 다시 떨어뜨린다. 우리의 모델 척도에 접근하는 현존하는 유일한 작업은 (Yang et al., 2024)이다. 그들은 소규모 학술 식물 과학 데이터에서 LLaMA2를 지속적으로 사전 훈련 및 지시한다. 이 동시 작업은 우리가 제안하는 것과 매우 유사한 연속 학습 설정인 재생, LR 재-온난화 및 LR 재-붕괴를 사용한다. 우리의 작업과 달리, 그들은 지속적인 사전 훈련을 위해 이러한 접근법의 유효성을 체계적으로 평가하기 위해 통제된 실험 프레임워크를 구축하지 않지만, 우리의 접근법을 검증하는 추가 실험 증거를 보는 것은 좋다.

### Learning Rate Schedules

여러 연구에서 다양한 학습률(LR) 스케줄이 신경망의 학습 안정성과 최종 성능에 미치는 영향을 조사했다. Goyal 등(2018)은 초기 훈련에서 LR의 점진적인 웜업이 특히 큰 미니 배치 크기로 최적화 문제를 극복하는 데 도움이 될 수 있음을 발견했다. 또한, Popel and Bojar(2018)는 Post-LN Transformers 훈련 시 워밍업 단계의 중요성을 강조하였다. 한편, Xiong et al.(2020)은 Pre-LN Transformers가 보다 안정적이며 워밍업 단계를 필요로 하지 않을 수 있음을 발견하였다. You et al.(2019)은 LR 붕괴의 역할을 탐색하고 큰 초기 LR이 네트워크가 잡음 데이터를 암기하는 것을 방지하는 반면 작은 LR은 복잡한 패턴을 학습하는 데 도움이 된다는 것을 발견했다. Kaplan et al.(2020)은 LLM(Large Language Models)을 사전 훈련하기 위해 LR 스케줄을 탐색한 결과 스케줄 선택이 성능에 큰 영향을 미치지 않는 것으로 나타났다. 이러한 잘못된 발견을 수정하여, Hoffmann 등(2022)은 LR 스케줄이 중요한 역할을 한다는 것을 발견했다. Hoffmann et al.(2022)과 Rae et al.(2021)은 널리 채택된 LLM을 사전 훈련할 때 코사인 스케줄을 사용하는 모범 사례를 확립했다. 대조적으로, Raffel 등(2023)과 Zhai 등(2022)은 대규모 사전 훈련을 위해 역 제곱근 붕괴를 따르는 LR 스케줄을 탐색한다. Raffel et al. (2023)은 훈련 LLMs을 위해 역 제곱근 감쇠를 활용하였고, 이는 훈련 단계들의 수를 조정하는데 있어서 유연성을 허용한다. Zhai et al.(2022)에서 저자들은 비전 트랜스포머를 훈련시키기 위해 '무한 학습률 스케줄'로 지칭되는 이러한 스케줄을 사용한다. 이러한 스케줄은 한 번의 실행으로 무기한 훈련 및 여러 훈련 기간의 평가를 가능하게 한다. 우리는 LLMs(Sec. 7.4)에 대해 제안된 무한 학습률 스케줄이 이 아이디어에서 영감을 받았음을 주목한다.

## 4 배경 & 방법

이 절에서는 LLM의 맥락에서 지속적인 사전 훈련과 관련하여 적절한 배경과 방법론을 제공한다.

### Linear Warmup 및 Cosine Decay Schedule

Hoffmann et al.(2022)과 Rae et al.(2021)은 LLM을 사전 훈련할 때 코사인 스케줄을 사용하기 위한 모범 사례를 확립했다. 특히 선형 워밍업 단계로 시작하여 학습률을 최대값 \(10\times\)으로 감쇠시켜 코사인 사이클의 끝이 토큰의 수와 일치하도록 설정하는 것을 권장한다. 선형 워밍업 지속 시간은 다르지만, 대부분의 주목할 만한 작업들은 훈련 단계들에 대해 0.1% 내지 0.5% 사이의 지속 시간을 갖는다(Zhao et al., 2023). 많은 인기 있는 오픈 소스 모델(Touvron et al., 2023; 20; Almazrouei et al., 2023)이 이러한 학습률 스케줄 레시피를 따른다는 점을 감안할 때, 이러한 모델을 지속적으로 사전 트레이닝하기 위해서는 그 뉘앙스를 이해하는 것이 중요하다. 스케줄은 먼저 \(T_{warmup}\) 타임스탬프에 걸쳐 학습률을 선형적으로 증가시키거나 또는 일부 타임스탬프 \(t_{ann}=T_{warmup}\)까지 등가적으로 증가시킨다:

\[\eta_{t}=\eta_{max}\cdot\frac{t}{T_{warmup}} \tag{1}\]

여기서, \(\eta_{t}\)는 반복시 학습률의 값 \(t\)이고, \(\eta_{max}\)는 최대 학습률이다. 그런 다음 일정은 \(T_{ann}\) 타임스탬프에 걸쳐 코사인 어닐링 단계로 전환되며, 등가적으로 일부 타임스탬프 \(t_{end}=T_{ann}+t_{ann}\):

\[\eta_{t}=\eta_{min}+\frac{(\eta_{max}-\eta_{min})}{2}\cdot\left(\cos\left( \pi\cdot\frac{t-t_{ann}}{t_{end}-t_{ann}}\right)+1\right) \tag{2}\]

여기서 \(\eta_{max}\)는 최대 학습률이고 \(\eta_{min}\)는 최소 학습률이다.

### Compute-equivalent Replay

우리의 많은 실험에서, 우리는 재생으로 훈련된 모델들을 그것 없이 훈련된 모델들과 비교한다. 이러한 비교를 할 때, 우리는 두 모델을 훈련시키기 위해 계산의 양을 일정하게 유지한다. 즉, 리플레이 버퍼에서 볼 수 있는 추가 토큰을 수용하기 위해 새로운 데이터 세트에서 볼 수 있는 토큰의 수를 상응하게 줄인다. 우리는 이러한 리플레이 사용을 _계산 등가 리플레이_라고 합니다. 예를 들어 데이터 집합 \(\mathcal{D}_{0}\) 및 \(\mathcal{D}_{1}\) 각각에 100B 토큰이 포함되어 있다고 가정합니다. 우리는 \(\mathcal{D}_{0}\)와 \(\mathcal{D}_{1}\)에 순차적으로 훈련된 모델 (a)와 \(\mathcal{D}_{0}\)와 \(\mathcal{D}_{1}\)에 순차적으로 훈련된 모델 (b)를 5% 계산 등가 재생과 비교하고자 한다. 모델(a)에는 총 200B 고유 토큰에 대한 두 데이터 세트의 모든 토큰이 표시됩니다. 모형 (b)에는 총 200B 토큰에 대해 \(\mathcal{D}_{0}\)의 100B 고유 토큰과 \(\mathcal{D}_{1}\)의 95B 고유 토큰과 \(\mathcal{D}_{0}\)의 5B 재생 토큰이 표시됩니다. 이러한 방식으로 비교된 두 모델은 동일한 양의 계산을 소모한다.

예를 들어, 두 개의 데이터 세트 \((\mathcal{D}_{0},\mathcal{D}_{1})\)에 걸쳐 있는 설정에서는 \(\mathcal{D}_{0}\)에 대해 훈련할 때 \(\mathcal{D}_{0}\)에서 데이터를 다시 재생합니다. 우리는 예비 실험에서 재생 데이터를 재구성할 때 눈에 띄는 차이를 관찰하지 못했기 때문에 \(\mathcal{D}_{0}\)에서 사전 훈련할 때 보이는 순서대로 데이터를 재생한다. 다시 보기 샘플을 선택하는 방법의 사용은 향후 작업으로 남겨둔다. 우리는 재생 모델을 "\(\mathcal{D}_{1}\)\(x\)% Replay"라고 부르며, 여기서 \(x\)는 \(\mathcal{D}_{0}\)에서 나오는 각 훈련 배치의 데이터 백분율이다. 반대로, 각 훈련 배치에서 샘플의 \((100\%-x)\)%는 \(\mathcal{D}_{1}\)로부터 샘플링될 것이다. 리플레이로 훈련된 모델을 다른 구성과 비교할 때, 우리는 \(\mathcal{D}_{1}\) 토큰의 수를 줄여 \(\mathcal{D}_{0}\)에서 리플레이 토큰을 수용함으로써 컴퓨팅이 _등가_인지 확인합니다.

## 5 실험 설정

랜덤 초기화를 통한 학습 LLM과 비교하여 연속 사전 학습 LLM의 효과를 경험적으로 평가하기 위해 문헌에서 최근 사전 학습 데이터 세트를 선택하고 조사를 위한 실제 연속 사전 학습 설정을 개요화하고 제안된 기술과 비교하기 위해 몇 가지 기준선을 선택한다. 우리의 목표는 우리의 지속적인 사전 훈련 기술을 통제된 환경에서 기준선과 공정하게 비교하는 것이다. 이 문서의 범위를 벗어나 최첨단 성능을 얻거나 모델과 비교하려고 하지 않습니다.

### Datasets

훈련 및 검증을 위해 SlimPajama (Soboleva et al., 2023), German CommonCrawl (Laippala et al., 2022) 및 Pile (Gao et al., 2020)의 세 가지 데이터 세트를 사용한다. 모든 데이터 세트에 대해 파일에서 특별히 훈련된 Black 등(2022)과 동일한 토큰화기를 사용합니다. SlimPajama에 대한 훈련 세트를 만들기 위해 데이터 세트(606B 총 토큰)를 무작위로 하위 샘플링하여 파일과 비슷한 크기의 \(\sim\)299B 토큰 하위 집합(표 1 참조)을 형성한다. 또한 이 SlimPajama 서브세트를 추가로 서브샘플링하여 데이터 세트의 3 \(\sim 100\)B 토큰 분할을 생성한다(자세한 내용은 Sec. 7.4 참조). SlimPajama 유효성 검사 세트를 만들기 위해 우리는 광범위하게 중복 제거 된 기본 유효성 검사 세트를 토큰화 합니다 (Soboleva 등, 2023). 독일어 훈련 및 유효성 검사 세트를 만들기 위해 Oscar Dataset(Laippala et al., 2022)의 일부로 사용 가능한 독일어 Common Crawl 스크래프를 195.43B 토큰 훈련 세트와 982.6M 토큰 유효성 검사 세트로 분할 및 토큰화했다. 파일 데이터 세트는 미리 섞이고 혼합되며 기본 훈련 및 검증 세트를 단순히 사용했다. 훈련 집합은 \(\sim 330\)B 토큰 총이지만, 실험에서 우리는 300B 토큰 하위 집합에서만 훈련한다.

### 연속 학습 설정

우리는 본문에서 세 가지 현실적인 연속 사전 훈련 설정을 고려하고 부록에서 덜 보증된다고 판단되는 3분의 1에 대한 결과를 제공한다. 각 설정은 지속적인 사전 훈련의 다양한 도전과 강점을 노출하기 위해 신중하게 선택되었다. 우리의 설정은 지속적으로 사전 훈련된 LLMs를 가정합니다.

\begin{table}
\begin{tabular}{l r r} \hline \hline Dataset & Size (Tokens) & Sampling (\%) \\ \hline Wikipedia & 11.96B & 4.00 \\ Book & 12.58B & 4.20 \\ C4 & 79.87B & 26.69 \\ Stack Exchange & 10.09B & 3.37 \\ GitHub & 15.63B & 5.22 \\ Common Crawl & 155.89B & 52.09 \\ Arxiv & 13.25B & 4.43 \\ \hline Total & 299.28B & 100.00 \\ \hline \hline \end{tabular}
\end{table}
표 1: **SlimPajama의 300B 토큰 훈련 세트의 도메인 크기** SlimPajama 데이터 세트(총 토큰 606B)를 300B 토큰 분할로 하위 샘플링하여 파일과 비슷한 크기로 만들었습니다. SlimPajama를 구성하는 서브샘플링된 도메인의 크기와 훈련 시간에 사용된 샘플링 백분율(예: 특정 도메인에서 나온 각 배치의 샘플 백분율)을 보고한다.

두 개 이상의 사전 훈련 단계를 순차적으로 진행합니다. 각 단계가 시작될 때 최적화기 상태를 재설정하는데, 예를 들어 HuggingFace의 개방형 가중치 모델을 사용할 때 최적화기 상태를 항상 사용할 수 있는 것은 아니기 때문이다. 즉, 연속적으로 사전 훈련된 LLM이 랜덤하게 초기화되고 데이터 세트 \(\mathcal{D}_{0},\mathcal{D}_{1},\ldots,\mathcal{D}_{N-1}\)에서 순차적으로 사전 훈련되는 상황에서 \(N\geq 2\)에 적용된다. 현실적인 설정을 위해 _tokens_(\(\mathcal{D}_{i}\)) \(\geq\) 100B를 고려한다. 각각의 경우에, 우리는 다음의 자연 기준들을 고려한다:

* 모든 데이터 세트의 결합에 대한 무작위 초기화로부터 훈련된 모델, 즉 \(\bigcup_{i=0}^{N-1}\mathcal{D}_{i}\), 및
* 개별 데이터 세트 \(\mathcal{D}_{i}\), \(0\leq i\leq N\)에 대한 무작위 초기화에서 학습된 모델입니다.

\(N=2\) **설정** - 여기서는 선형 웜업 및 코사인 감쇠 LR 스케줄을 사용하여 데이터 세트(\(\mathcal{D}_{0}\))에서 자기회귀 언어 모델링을 위해 사전 훈련된 모델을 사용할 수 있다고 가정합니다. 우리는 또한 일정이 문헌의 기존 관례를 따른다고 가정한다(예를 들어, 토큰 예산에 대한 부패; Sec 참조). 자세한 내용은 4) 대부분의 수행성 사전 훈련된 LLMs에 대한 경우이다(Rae et al., 2021; Hoffmann et al., 2022; Touvron et al., 2023; 1). \(\mathcal{D}_{0}\)에서 미리 훈련된 모델이 주어지면, 우리는 이제 실무자가 동일한 자기 지도 목적을 사용하여 새로운 데이터세트 \(\mathcal{D}_{1}\)에서 이 모델을 업데이트하기를 원한다고 가정한다. **2-데이터 세트 설정** 의 구체적인 변형은 다음과 같습니다.

* **두 데이터 세트, 약한 이동**: 이 변형에서 \(\mathcal{D}_{0}\)는 Pile(Gao 등, 2020)로 간주하고 \(\mathcal{D}_{1}\)는 SlimPajama(Soboleva 등, 2023)에서 사전 훈련하는 것으로 간주합니다. SlimPajama는 LLaMA 데이터세트(Touvron et al., 2023a)를 기반으로 구축된 RedPajama(Computer, 2023)의 광범위하게 중복제거된 버전이다. 두 데이터 집합이 모두 영어이며 공통 도메인(CommonCrawl, GitHub, Arxiv, Wikipedia, StackExchange)과 중복되지 않는 다른 도메인을 포함하기 때문에 이것은 약하지만 현실적인 분포 이동이라고 생각한다. 또한 SlimPajama(2023)는 Pile(2020)보다 더 새로운 데이터 세트이므로 겹치는 도메인 내에서 더 새로운 데이터를 가질 가능성이 있다. 따라서, 잠재적인 중첩에도 불구하고, 우리는 이러한 전환이 현실적이며 사전 훈련(예를 들어, 더 높은 품질의 필터링을 갖는 동일한 소스의 새로 수집된 데이터)과 유사한 분포로 LLM을 업데이트하기를 원하는 많은 실무자들에게 흥미로울 것이라고 믿는다.
* **두 데이터 세트, 더 강력한 이동**: 이 변형에서 \(\mathcal{D}_{0}\)는 파일에서 사전 훈련(Gao 등, 2020)이고 \(\mathcal{D}_{1}\)는 독일 커먼 크롤에서 사전 훈련으로 간주합니다. German Common Crawl은 Oscar 데이터셋(Laippala et al., 2022)에서 가져온 \(\sim\) 200B 토큰 데이터셋이다. 우리는 이것이 언어의 변화를 고려할 때 더 강력한 변화를 구성한다는 점에 주목한다. 이 설정은 사전 훈련과 어휘가 현저하게 다른 새로운 자연 언어, 프로그래밍 언어 또는 특정 영역으로 LLM을 보강하고자 하는 실무자들에게 특히 중요하다. 그러나 도메인이 토큰화기의 훈련 말뭉치에서 점점 멀어질수록 토큰화기가 성능에 대한 주요 병목 현상이 될 수 있다는 점에 주목한다. 토나이저의 치료는 향후 업무에 맡깁니다.

\(N>2\) **설정** - 더 많은 데이터 집합을 사용 하 여 크기를 조정 하는 방법을 고려 하는 방법을 조사 하기 위해 더 많은 데이터 집합 전환을 사용 하 여 다음 설정을 고려 합니다.

* **세 개의 데이터 세트, 이동 없음** : \(N=3\) 설정을 고려 합니다. 여기서 \(\mathcal{D}_{0},\mathcal{D}_{1},\mathcal{D}_{2}\)는 SlimPajama의 각 지역 100B 토큰 분할입니다. 이 설정은 주로 향후 많은 업데이트로 확장하는 기술의 능력을 평가하고 제안된 무한 학습률 일정의 성능을 평가하는 데 사용된다.
* **도메인 증분 연속 사전 훈련**: 이 설정은 도메인별로 순차적으로 정렬된 SlimPajama의 토큰을 소비하는 것을 고려합니다. 즉, \(N\)개의 미래 데이터 집합 \(\{\mathcal{D}_{0},\mathcal{D}_{1},\ldots,\mathcal{D}_{N-1}\}\) 각각은 SlimPajama 300B의 별개의 도메인이다. 우리는 이것이 DACPT(Ke et al., 2022)와 유사하다는 점에 주목하지만, 각 도메인에 대해 훨씬 더 큰 데이터 세트를 고려한다. 이 설정은 각 도메인 간의 전환 시 분포 전환 경험으로 인해 특히 어렵다. 확실히 흥미롭지만, 우리는 SlimPajama 데이터를 훈련하기 전에 혼합하는 것과 비교하여 불필요하게 어렵다고 생각한다. 이 설정의 불량한 결과(부록의 Sec. A.1)는 범용 LLM이 도메인당 업데이트되지 않고 가능하면 도메인의 혼합물에서 지속적으로 사전 훈련되어야 함을 시사한다.

### Training Setup

Megatron-DeepSpeed (Shoeybi et al., 2019; Microsoft, 2020) 기반의 GPT-NeoX (Andonian et al., 2021)를 사용하여 인과적 언어 모델링 목적을 가진 자기회귀 디코더 전용 변압기를 훈련한다. 모델은 Pre-LN을 사용합니다. 각 모델은 BPE 알고리즘을 통해 Pile 상에서 독점적으로 트레이닝된 Black 등(2022)과 동일한 토큰나이저를 사용하여 트레이닝된다(Sennrich 등, 2016). 모든 모델에 대해 AdamW 최적화기(Loshchilov and Hutter, 2019)를 사용하여 배치 크기 1104, 시퀀스 길이 2048을 사용하여 훈련한다. 훈련의 에포크는 대략 \(132,366\)의 총 훈련 단계에 해당한다. 이전 섹션에서 언급한 대로 데이터 세트 간의 최적화 상태를 재설정한다. 우리는 임베딩을 포함하는 두 개의 모델 크기 405M 및 9.6B 파라미터(이 작업에서 10B라고 함)를 고려한다. 4의 마이크로 배치 크기를 사용하여 46개의 GPU 노드에서 데이터 병렬성을 사용하여 더 작은 모델을 학습한다. 더 큰 모델은 노드 내의 6개의 GPU에 걸쳐 있는 텐서 병렬성(Shoeybi et al., 2020)과 4개의 노드에 걸쳐 있는 파이프라인 병렬성(Huang et al., 2019)을 사용하여 학습한다. 즉, 각 모델 복제본은 4개의 노드에 걸쳐 있는 24개의 GPU에 걸쳐 있다. 우리는 4단계의 기울기 누적을 사용하여 276개의 노드에서 이 모델을 훈련한다. 각 모델은 GPU 메모리 소비를 줄이고 훈련 동안 NVIDIA 텐서 코어를 완전히 활용하기 위해 ZeRO-1(Rajbhandari 등, 2020), 활성화 체크포인팅(Chen 등, 2016), 텐서 병렬 랭크에 걸친 활성화 분할 및 혼합 정밀도 FP16/FP32를 통한 최적화기 샤딩을 사용한다. 부록의 모든 하이퍼파라미터에 대한 확장된 설명을 제공하였다(표 13).

### 독일어 및 영어 LM 평가 벤치마크

우리는 매우 다양한 다운스트림 작업에 대한 성과를 측정하는데, 크게 다음과 같이 분류할 수 있다.

#### English Benchmarks

* **상식 추론(0-shot):** HellaSwag(Zellers et al., 2019), Winogrande(Sakaguchi et al., 2019), PIQA(Bisk et al., 2019), OpenBookQA(Mihaylov et al., 2018), ARC-Easy, ARC-Challenge(Clark et al., 2018)
* **World Knowledge (5-shot):** NaturalQuestions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017)
* **Reading Comprehension (0-shot):** BoolQ (Clark et al., 2019)
* **Math:** MathQA (Amini et al., 2019)
* **인기 집계 결과:** MMLU(5-shot)(Hendrycks et al., 2021)

GPT 3.5 API를 사용하여 영어 상대방을 번역한 독일 벤치마크(Pluster, 2023)

* **Commonsense Reasoning (0-shot):** HellaSwag-DE (Zellers et al., 2019), ARC-Challenge-DE (Clark et al., 2018)
* **World Knowledge (5-shot):** TriviaQA-DE (Joshi et al., 2017)
* **Popular Aggregated Results:** MMLU-DE (5-shot)(Hendrycks et al., 2021)

## 6 Results

우리는 들어오는 데이터 세트가 클 때(200B 토큰+) 지속적인 사전 훈련에 중점을 둡니다. 이러한 환경에서 훈련은 비용이 많이 들기 때문에 많은 양의 들어오는 데이터에 효율적으로 적응하는 것이 중요하다. 그러나, 대부분의 수행 LLMs(Rae et al., 2021; Hoffmann et al., 2022; Zhao et al., 2023; Touvron et al., 2023b;a)는 비교적 낮은 최소 학습률을 갖는 선형 웜업 및 코사인 감쇠 스케줄로 트레이닝된다. 이 학습률을 비교적 높은 값으로 **재온난화** 한 후 새 데이터 세트에 효율적으로 적응 하기 위해 다시 쇠퇴 해야 한다고 가정 합니다. 이를 위해 섹션 6.1에서 선형 워밍업 지속 시간, LR 재워밍, LR 재쇠 및 최대 학습 속도 크기가 적응 및 망각에 미치는 영향을 연구한다. 재-온난화 및 재-붕괴가 적응 및 망각을 모두 증가시킨다는 것을 발견하기 위해 섹션 6.2에서 학습률이 재-온난화 및 재-붕괴될 때, 재생이 망각을 완화하는 데 도움이 될 수 있는지 여부를 조사한다. 하위 섹션 6.3 및 6.4는 이전 두 섹션에서 연구된 전략을 결합하고 약하고 강한 분포 이동 및 대규모 모델 규모에 대한 기준선과 비교하여 성능을 보고한다.

마지막으로, 섹션 7에서 LR 재-온난화가 원치 않는 망각을 유발할 수 있고, 이를 회피하기 위한 유망한 방법으로 무한 학습률 스케줄을 도입하고, 이러한 스케줄을 기준선과 비교할 수 있음을 설명한다.

### Learning Rate Schedule

학습률이 적응에 미칠 수 있는 영향과 두드러진 LLM의 낮은 최종 LR 값(Rae 등, 2021; Hoffmann 등, 2022; Zhao 등, 2023; Touvron 등, 2023;a)을 감안할 때, 우리는 LR이 지속적인 사전 훈련 동안 적응을 촉진하기 위해 재-온화되고 재-퇴화되어야 한다고 가정한다. 이 절에서는 선형 예열 지속시간, LR의 재가열, LR의 재감쇠 및 사전 훈련을 계속할 때 \(\eta_{max}\)의 크기에 대한 영향을 조사한다. 특히 **2-데이터 세트 약한 이동** 설정(300B 파일 \(\rightarrow\) 300B SlimPajama)과 **2-데이터 세트 강한 이동** 설정(300B 파일 \(\rightarrow\) 300B SlimPajama)에서 각각의 효과를 평가합니다. 특히, \(\mathcal{D}_{0}\)(Pile의 300B 토큰)에 대해 훈련된 모델은 선형 예열 및 코사인 감쇠 스케줄1을 따르며, 많은 공통 오픈 소스 사전 훈련 LLM을 시뮬레이션한다.

각주 1: 본 논문의 모든 코사인 디케이에 대해, 달리 명시되지 않는 한, 코사인 어닐링 단계를 토큰 버짓에 맞추고, 선형 워밍업 기간(\(T_{warmup}\))을 트레이닝 반복의 1%로 설정하고, \(\eta_{min}=0.1\cdot\eta_{max}\)을 설정한다.

#### 6.1.1 약 및 강 분포 이동에 대한 선형 워밍업의 효과입니다.

먼저 **두 데이터 세트, 약한 이동** 및 **두 데이터 세트, 강한 이동** 설정에서 선형 예열 기간이 망각 및 적응에 미치는 영향을 조사합니다(자세한 내용은 Sec 5.2 참조). 모델은 Pile(Gao et al., 2020)의 300B 토큰(\(\mathcal{D}_{0}\))에 대해 사전 훈련된다. 우리는 훈련의 첫 50B 토큰을 위해 슬림파자마(약 시프트)와 독일 커먼 크롤(강 시프트)에서 모델을 계속 사전 훈련한다. 우리는 다시 따뜻해지고 다시 부패해

그림 2: **약한 분포 및 강한 분포 이동에 대한 선형 예열 효과.** (a), (b) 및 (c), (d)는 각각 오른쪽 그림에 표시된 동일한 범례를 가집니다. 훈련 반복의 0%, 0.5%, 1% 및 2%의 다양한 선형 웜업 기간을 갖는 선형 웜업 및 코사인 감쇠 스케줄에 따라 405M 매개변수 모델을 훈련한다. 각 학습률 스케줄은 데이터 세트의 크기에 따라 학습이 끝날 때까지 \(0.1\eta_{max}\)로 감소한다. 우리는 훈련의 처음 50B 토큰에 대한 결과를 보고한다. 탐색된 설정에서 예열 단계의 기간이 사전 훈련을 계속할 때 영향을 미치지 않는 것으로 판단된다.

300B 토큰과 200B 토큰에서 최소값(\(\eta_{min}=0.1\cdot\eta_{max}\))에 도달하도록 설정된 코사인 학습률 스케줄을 이용한 학습률 본 논문에서는 \(\mathcal{D}_{1}\)의 전체 훈련 반복 횟수(각각 132366회, 86000회)의 0.5%, 1%, 2%에 대한 학습률 온난화를 고려한다. 붕괴는 반복의 나머지 예산에 걸쳐 발생하므로(따라서 resp). \ 전체 반복의 99.5\%,99\%\ 및 \(98\%\)는 긴 워밍업의 붕괴 단계가 약간 더 빠르게 발생한다는 것을 의미한다. 또한, \(\eta_{max}\)에서 LR을 즉시 붕괴시키는 선형 웜업(0%)이 없는 모델을 훈련한다. 모든 실험은 405M 매개변수 모델에 대해 수행된다.

그림 2는 \(\mathcal{D}_{0}\) 및 \(\mathcal{D}_{1}\)에 대한 지속적인 사전 훈련의 처음 50B 토큰 전체에 걸쳐 모든 모델에 대한 유효성 검사 손실을 보고한다. 맨 위 행은 약한 분포 이동의 결과를 보고하고, 맨 아래 행은 강한 분포 이동의 결과를 보고합니다. 두 분포 이동 모두에서 짧은 선형 웜업을 사용하는 모델이 처음에 긴 웜업 대응 모델보다 더 빨리 잊고 적응한다는 것을 처음에 관찰한다. 이것은 LR을 더 빠르게 증가시켜 더 빠른 망각과 적응으로 이어지기 때문에 발생한다. 그러나 모든 시나리오에서 이러한 초기 차이는 훈련 전반에 걸쳐 감소하여 50B 토큰 후에 비교적 유사한 망각 및 적응을 가진 모든 모델을 남긴다. 따라서 탐색된 설정에서 **선형 예열 단계의 기간은 사전 훈련을 계속할 때 잊어버리거나 적응에 영향을 미치지 않는 것으로 판단된다.* * 이를 염두에 두고 모든 후속 실험에 대한 훈련 반복의 1%의 선형 예열 기간을 설정했다.

그림 3: **학습률을 다시 온난화하고 다시 쇠퇴시키는 것이 적응과 망각에 미치는 영향입니다. 우리는 2개의 상수 기준선과 다시 따뜻해지고 다시 부패하는 3개의 모델을 고려한다. 한 기준선은 사전 훈련(\(3\cdot10^{-4}\))에서 \(\eta_{min}\)로 훈련을 계속하는 반면, 다른 기준선은 사전 훈련(\(3\cdot10^{-4}\))에서 \(\eta_{max}\)로 예열한다. 재온 및 재부패 모델의 경우 \(\eta_{max}\in\{1.5\cdot 10^{-4},3\cdot 10^{-4},6\cdot 10^{-4}\}\)을 변화시킨다. \(\eta_{min}\) 기준선을 제외한 모든 모델은 1% 훈련 반복을 위해 선형 웜업을 사용한다. 비기준 모델들은 학습이 종료될 때까지 학습은 \(0.1\cdot\eta_{max}\)에 도달하도록 코사인 감쇠한다. 우리는 새로운 데이터 세트에 가장 잘 적응하기 위해 학습 속도를 재-온난화 및 재-퇴화시키는 것이 필요하다는 것을 관찰한다. \(\eta_{max}\)의 작은 증가 또는 감소는 적응 사이의 트레이드오프를 허용한다. 더 강한 분포 변화는 망각과 적응 모두에 촉매제가 되는 것 같습니다. **

1.2 Weak and Strong Distribution Shift에 대한 재가열, 재붕괴 및 변화 \(\eta_{\text{max}}}\)의 효과.

이제 우리는 \(\eta_{\text{max}}\)의 다른 값에 대한 학습률(예를 들어, 코사인 스케줄에 따라)을 재가열하고 재분해하는 것의 이점을 조사한다. 구체적으로, 이 모델들을 두 개의 자연 기준선과 비교한다: 재-따뜻하지 않고, \(\eta_{\text{min}}\)(\(3\cdot10^{-5}\)), 그리고 사전 훈련 \(\eta_{\text{max}}\)(\(3\cdot10^{-4}\))에 재-따뜻하지만 재-붕괴되지 않는 모델. 우리는 300B 토큰에 대해 파일(\(\mathcal{D}_{0}\))에서 먼저 사전 훈련하고 SlimPjama(약 시프트) 또는 독일 커먼 크롤(강 시프트)에서 우리의 \(\mathcal{D}_{1}\) 데이터 세트로 모델을 지속적으로 사전 훈련한다. 연속적인 사전 훈련은 데이터 세트의 전체 크기(각각 300B 및 200B 토큰)에 대해 수행된다. LR을 재-온난화 및 재-퇴화시키는 모델은 세 가지 전략을 고려한다: 사전 훈련의 \(\eta_{\text{max}}\)의 절반으로 재-온난화(\(1.5\cdot10^{-4}\)), 사전 훈련의 동일한 \(\eta_{\text{max}}\)로 재-온난화(\(3\cdot10^{-4}\)), 사전 훈련의 두 배로 재-온난화(\(6\cdot10^{-4}\)) 모든 경우에 학습률은 선형 예열 후 코사인 분해되어 학습이 끝날 때까지 \(\eta_{\text{min}}=0.1\cdot\eta_{\text{max}}\)에 도달한다. 마지막으로, \(\mathcal{D}_{0}\cup\mathcal{D}_{1}\)에서 학습된 모델들을 성능의 상한을 제공하기 위한 세 번째 기준선으로 고려한다.

그림 3은 모든 모델의 지속적인 사전 훈련 전반에 걸쳐 \(\mathcal{D}_{0}\) 및 \(\mathcal{D}_{1}\) 데이터 세트에 대한 유효성 검사 손실을 보고한다. 그림의 상단 행은 약한 분포 이동(300B Pile\(\rightarrow\)300B SP)에 대한 결과를 보고하고 하단 행은 강한 분포 이동(300B Pile\(\rightarrow\)200B Ger)에 대한 결과를 보고한다. 두 이동 모두에서 \(\eta_{\text{min}}\) 상수 학습률 모델은 \(\mathcal{D}_{0}\)에서 가장 적은 망각을 달성한다. 또한 강한 시프트의 경우 \(\mathcal{D}_{1}\)에 가장 적게 적응하지만 약한 시프트의 경우 \(\eta_{\text{max}}\) 상수 기준선보다 더 많이 적응한다. 이러한 기준선을 두 데이터 세트에서 다시 따뜻해지고 다시 부패하는 모델과 비교할 때 후자의 모델이 두 분포 이동에 대해 상당한 마진만큼 새 데이터 세트에 더 잘 적응한다는 것을 관찰한다. 이는 LLM을 지속적으로 사전 훈련할 때 새로운 데이터 세트에 대한 적응을 최대화하기 위해 재-온난화 및 재-붕괴가 필요함을 보여준다. LR을 재-온화하고 재-붕괴시키는 모델들 중에서, 학습률의 변화가 적응과 망각의 작은 차이를 야기한다는 것을 관찰한다: \(\eta_{\text{max}}\)의 값이 높을수록 망각이 더 많아지고 적응이 더 많아지는 반면, 낮은 값에서는 그 반대이다. 기준선을 조합 훈련된 기준선과 비교할 때, \(\mathcal{D}_{1}\)에 대한 최종 검증 손실이 두 분포 이동 모두에서 조합 훈련된 모델보다 상당히 높다는 것을 관찰한다. 이는 약한 분포 이동에서는 \(\mathcal{D}_{1}\)의 경우도 마찬가지이지만, 강한 분포 이동에서는 상수 기준선이 결합 학습 모델보다 낮은 \(\mathcal{D}_{1}\) 검증 손실을 달성한다. 우리는 이것이 일반적으로 LLM의 맥락에서 적응을 강화하고 망각을 악화시키는 더 강한 분포 변화 때문이라고 가정한다. 재-온난화(re-warming) 및 재-붕괴(re-decaying)로 연속적으로 사전 트레이닝된 모델들을 조합 기준선에 비교할 때, 이들 모델들은 조합 기준선보다 \(\mathcal{D}_{1}\)에 더 잘 적응(낮은 최종 검증 손실)한다. 그러나, 이 모델들은 \(\mathcal{D}_{0}\)에서 상당한 망각을 경험하며, 이 모델들이 조합 기준선과 경쟁하도록 하기 위한 재생의 필요성을 보여준다.

요약하면, 새로운 데이터 세트에 대한 적응을 최대화하기 위해 지속적으로 사전 훈련 LLM, 재-온난화 및 재-붕괴가 필요하다; \(\eta_{\text{max}}\)의 작은 증가 또는 감소는 적응 사이의 트레이드오프를 허용한다; \(\mathcal{D}_{0}}\)와 \(\mathcal{D}_{1}\) 사이의 더 강한 분포 이동은 망각을 악화시키고 적응을 강화시킨다; 그리고 선형 준비 단계의 기간은 망각 또는 적응에 영향을 미치지 않는 것으로 보인다._

### 재생 효과

이 부분에서는 학습률을 재-온화하고 재-감쇠시키는 모델을 지속적으로 사전 훈련할 때 컴퓨팅-동등성 재생의 효과를 탐구한다.

재-온난화 및 재-붕괴 시 망각을 완화할 필요성을 고려하여, 우리는 약하고 강한 시프트 연속 사전 훈련 시나리오에서 리플레이의 영향을 조사하기 위해 이동한다. 구체적으로, 예산에서 동등한 수의 \(\mathcal{D}_{1}\) 토큰을 제거하는 비용으로 \(\mathcal{D}_{0}\) 토큰의 재생이 추가되는 경쟁 등가 재생(자세한 내용은 Sec. 4.2 참조)을 사용한다. 동일한 두 데이터 세트 설정에 따라 모델은 300B 토큰에 대해 \(\mathcal{D}_{0}\)(파일)에서 사전 훈련됩니다. 다음은 슬림파자마(약교대) 또는 독일 커먼 크롤(강교대)에 대한 지속적인 사전 훈련입니다. 설정에 대한 자세한 내용은 섹션 5.2를 참조하십시오. 각 데이터 세트의 전체 크기에 대해 지속적인 사전 훈련이 수행되며, SlimPajama의 경우 300B 토큰, 독일 커먼 크롤의 경우 200B 토큰입니다. 우리는 두 교대 모두에 대해 1%, 5%, 10% 및 50% 리플레이를 고려하고 약한 분포 교대 및 강한 분포 교대에 대해 각각 0.5% 및 25% 리플레이 실행을 추가한다. 우리는 이러한 결과를 더 넓은 맥락에 넣기 위해 두 가지 기준을 고려한다. 첫 번째 기준선은 재생 없이 \(\mathcal{D}_{1}\)로 훈련된 모델이다. 두 번째 베이스라인 모델은 600B 토큰(SlimPajama)과 500B 토큰(German Common Crawl)에 대해 \(\mathcal{D}_{0}\)과 \(\mathcal{D}_{1}\)의 연합에 대한 무작위 초기화로부터 학습된다. 후자의 기준선은 기존 모델을 지속적으로 사전 훈련하는 대신 모델을 업데이트하기 위해 완전히 다시 훈련하는 관행을 반영한다. 모든 모델은 코사인 감쇠 스케줄을 사용하여 학습률을 토큰 버짓에 맞게 재-온 및 재-감쇠한다 \(\mathcal{D}_{0}\)에 대한 사전 훈련에서와 동일한 \(\eta_{\textit{max}}\)(\(3\cdot 10^{-4}\)) 및 \(\eta_{\textit{min}}\)(\(3\cdot 10^{-5}\)) 값이다.

검증 손실 비교 그림 1의 결과. 도 4(상단 및 하단)는 각각의 \(\mathcal{D}_{1}\) 데이터 세트에 대한 지속적인 사전 훈련 동안 검증 손실의 진화를 보여준다. 표 2는 이러한 모델 각각에 대한 평균 최종 검증 손실을 보고한다. 최종 손실은 10회 반복 간격으로 샘플링된 훈련의 마지막 100회 반복에 걸쳐 평균화된다. 우리는 두 분포 이동 모두에서 가장 낮은 테스트된 1%의 재생도 재생이 없는 기준선에 비해 파일에서 망각을 크게 감소시킨다는 것을 일관되게 관찰한다. 이 효과는 이 설정에서 망각의 양이 더 많기 때문에 강한 이동 시나리오에서 더 두드러진다. 0% 기준선과 비교할 때 1%, 5% 및 10% 재생에 대한 다운스트림 성능에 미치는 영향이 거의 관찰되지 않으며, 이는 재생의 망각 이점이 우리 환경에서 거의 비용이 들지 않는다는 것을 보여준다. 그러나, 극단적인 양의 재생(50%)을 사용하는 경우, 우리는 모델이 \(\mathcal{D}_{0}\)에 덜 적응한다는 것을 관찰한다. 흥미롭게도 두 데이터 세트에 대해 50% 재생 모델은 \(\mathcal{D}_{1}\cup\mathcal{D}_{0}\)에 대한 기본 훈련의 최종 평균 검증 성능을 달성하거나 능가한다. 이 모델은 각각의 기준선보다 150B 및 100B 더 적은 \(\mathcal{D}_{1}\) 토큰을 보았기 때문에 이것은 흥미롭다.

요약하면, 우리는 지속적인 사전 훈련 컨텍스트에서 LR을 재-온난화 및 재-붕괴시킬 때, 재생은 망각을 감소시키는 데 유용한 도구라는 것을 발견한다. 두 분포 이동 모두에 대해 적절한 양의 리플레이를 사용하면 \(\mathcal{D}_{1}\cup\mathcal{D}_{0}\) 기준선과 유사한 최종 검증 손실이 발생한다. 더욱이, 두 교대 모두에 대해, 리플레이의 사용은 다운스트림 데이터 세트에 대한 적응에 무시할 수 있는 영향을 미치는 것으로 보이며, 리플레이를 통해 망각을 줄이는 것은 LLM을 지속적으로 사전 훈련할 때 매우 적은 비용이 든다는 것을 보여준다.

그림 4: **약하고 강한 분포 이동에 대해 405M 규모에서 다시 재생의 효과** 교육 중 파일 유효성 검사 손실(왼쪽) 및 SlimPajama/독일 유효성 검사(오른쪽 상단/하단)를 보고합니다. 각 모델은 파일의 300B 토큰에서 미리 훈련된 체크포인트로부터 훈련된다. 파란색 점선은 Pile\(\cup\)SlimPajama 또는 Pile\(\cup\)독일 데이터에 대해 훈련된 모델에 대한 최종 검증 손실을 보고하며, 각각 총 600B 및 500B 토큰 데이터 세트이다. 우리는 리플레이가 두 교대에 걸쳐 망각을 상당히 감소시키지만, 더 강한 교대는 동일한 정도로 망각을 완화하기 위해 더 많은 리플레이를 필요로 한다는 것을 관찰한다.

### 약하고 강력한 배포 이동에 대한 지속적인 사전 훈련 최종 성능

이 하위 섹션에서는 두 개의 연속 사전 훈련된 405M 매개 변수 모델을 _두 개의 데이터 세트 약한 이동_ (파일 \(\rightarrow\) SlimPajama) 및 _두 개의 데이터 세트 강한 이동_ (파일 \(\rightarrow\) 독일) 설정의 여러 기준선과 비교합니다. 우리의 주요 목표는 분배 이동의 차이가 최종 성능에 어떤 영향을 미치는지 결정하는 것이다.

연속적으로 미리 훈련된 모델 LR 재-온난화 및 재-붕괴를 재생과 결합하는 성능을 제거하기 위해, 우리는 학습률을 독점적으로 재-온난화 및 재-붕괴하는 모델과 두 기술을 결합하는 모델을 훈련하기로 선택한다. 약한 분포 시프트에 대한 이전 섹션의 결과를 감안할 때 약한 시프트 설정에 대해 5% 리플레이를 선택하고 강한 시프트 설정에 대해 25% 리플레이를 선택한다. 두 모델 모두 사전 훈련(\(3\cdot 10^{-4}\))의 \(\eta_{\textit{max}}\)에 다시 따뜻해지고, 연속 사전 훈련이 끝날 때까지 \(\eta_{\textit{min}}}\)에 도달하도록 설정된 코사인 감쇠 스케줄을 사용하여 다시 감쇠한다. 더 많은 하이퍼파라미터가 부록의 표 13에 보고되어 있다.

기준선 우리는 또한 여러 기준선을 훈련한다. 두 개의 기준선은 각각 \(\mathcal{D}_{0}\)와 \(\mathcal{D}_{1}\)에 대해 훈련되고, 세 번째 기준선은 각 데이터 세트의 결합 \(\mathcal{D}_{0}\cup\mathcal{D}_{1}\)에 대해 훈련된다. 우리는 \(\mathcal{D}_{0}\cup\mathcal{D}_{1}\)로 학습된 모델이 고가의 완전 재학습을 나타내기 때문에 성능의 상한으로 간주한다. 개별 데이터 세트에 대해 훈련된 기준선은 연속 사전 훈련에 대한 계산과 동등한 대안으로 볼 수 있다(예를 들어, 연속 사전 훈련 대신 \(\mathcal{D}_{1}\)에서 무작위 초기화로부터 모델을 훈련하도록 선택할 수 있다).

#### 6.3.1 Final Performance Evaluated by Loss

그림 5는 약한(위) 및 강한(아래) 이동에 대한 405M 매개변수 모델의 지속적인 사전 훈련 중 유효성 검사 손실을 보고한다. 표 3은 이러한 모델에 대한 평균(지난 100회 반복 동안) 최종 손실 값을 보고한다. 영어에서 독일어로의 전환은 Pile에서 SlimPajama로의 전환보다 더 뚜렷한 분포 이동을 나타내기 때문에, 독일어에 대한 훈련은 Pile에 대한 망각(\(\mathcal{D}_{0}\))을 반복하지 않고 연속적으로 사전 훈련된 모델(약한 시프트와 강한 시프트의 경우 각각 0.27 대 1.39)에서 훨씬 더 많이 초래한다. 그러나 스타커 시프트를 처리하기 위해 25% 리플레이를 선택하면 파일에서 잊는 양이 크게 줄어들어 최종 손실 측면에서 1.23이 감소한다. 연속적으로 사전 훈련된 모델들과 \(\mathcal{D}_{1}\)에서만 훈련된 기준선들을 비교할 때, 우리는 연속적으로 사전 훈련된 모델들이 두 분포 이동들에 걸쳐 항상 낮은 검증 손실을 갖는다는 것을 관찰한다. 연속적으로 사전 훈련된 모델과 \(\mathcal{D}_{0}\cup\mathcal{D}_{1}\) 기준선을 비교할 때 두 모델 모두 거의 동일한(약한 시프트) 또는 동일한(강한 시프트) 평균 최종 검증 손실을 달성한다는 것을 발견했다. 이것은 강한 분포 이동과 약한 분포 이동의 경우 LR 재-온난화, LR 재-감쇠 및 재생의 간단하고 확장 가능한 조합이 \(\mathcal{D}_{0}\cup\mathcal{D}_{1}\) 기준선과 유사한 성능을 달성할 수 있음을 보여준다.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline \multicolumn{1}{c|}{**Training Tokens**} & \multicolumn{3}{c}{**Validation Loss**} \\  & \(\mathcal{D}_{0}\) Pile & \(\mathcal{D}_{1}\) SlimPajama/German & AVG \\ \hline
300B 말뚝 \(\rightarrow\) 300B SP & 2.44 & 2.50 & 2.47 \\
300B 파일 \(\rightarrow\) 300B SP (0.5\% Replay) & 2.27 & 2.50 & 2.39 \\
300B 파일 \(\rightarrow\) 300B SP (15 Replay) & 2.26 & 2.50 & 2.38 \\
300B 파일 \(\rightarrow\) 300B SP (55 Replay) & 2.23 & 2.51 & 2.37 \\
300B 파일 \(\rightarrow\) 300B SP (105 Replay) & 2.21 & 2.51 & 2.36 \\
300B 파일 \(\rightarrow\) 300B SP (50\% Replay) & 2.16 & 2.54 & **2.35** \\
600B Pile \(\cup\) SP & 2.17 & 2.53 & **2.35** \\ \hline
300B 파일 \(\rightarrow\) 200B Ger. & 3.56 & 1.11 & 2.34 \\
300B 파일 \(\rightarrow\) 200B Ger. (1\% Replay) & 2.83 & 1.12 & 1.97 \\
300B 파일 \(\rightarrow\) 200B Ger. (55 Replay) & 2.57 & 1.12 & 1.85 \\
300B 파일 \(\rightarrow\) 200B Ger. (10\% Replay) & 2.46 & 1.13 & 1.80 \\
300B 파일 \(\rightarrow\) 200B Ger. (25\% Replay) & 2.33 & 1.16 & 1.75 \\
300B 파일 \(\rightarrow\) 200B Ger. (50\% Replay) & 2.24 & 1.22 & **1.73** \\
500B Pile \(\cup\) Ger. & 2.26 & 1.25 & 1.75 \\ \hline \hline \end{tabular}
\end{table}
표 2: **다양한 양의 다시 재생으로 훈련된 영어 전용 405M 매개 변수 모델의 최종 손실** 손실은 10회 반복 간격으로 샘플링된 훈련의 마지막 100회 반복에 걸쳐 평균화된다. 이러한 측정에 대한 표준오차는 계산되었지만 모든 모델에 대해 \(<0.001\)이므로 보고되지 않았다. 우리는 더 많은 리플레이를 사용하는 모델이 더 나은 적응-잊기 트레이드오프(AVG 손실)를 달성한다는 것을 관찰한다. 흥미롭게도 50%를 사용하는 모델은 SlimPajama에서 150B 더 적은 토큰을 보는 동안 거의 동일한 손실 값을 보관합니다.

## 6 Conclusion

그림 5: **두 개의 분포 이동으로 훈련된 405M 매개 변수 모델의 최종 손실** 그림 (a)와 (b)는 그림에서 중복됩니다. 편리한 비교를 위해 도 6을 참조한다. 3개의 기준선과 2개의 연속 사전 훈련 모델을 제공했다. 기준선(연한 파란색, 진한 파란색 및 적갈색)은 SlimPajama의 300B 토큰, Pile의 300B 토큰 및 두 데이터 세트의 연합(600B 토큰)에 대한 무작위 초기화로부터 훈련된다. 연속적으로 사전 훈련된 모델(검은색과 보라색)은 파일(짙은 파란색 곡선)의 300B 토큰에서 사전 훈련된 체크포인트에서 시작하여 각각 0% 및 5% 리플레이를 사용한다. 우리는 두 분포 이동 모두에 대해 학습률을 다시 온난화하고 적은 비율의 반복을 사용하는 조합이 망각과 적응 사이의 균형을 맞추는 데 도움이 된다는 것을 관찰한다. 중요하게도, 우리는 리플레이를 사용하는 모델이 0% 리플레이를 사용하는 모델에 비해 다운스트림 성능에 최소한의 영향을 미친다는 점에 주목한다.

\begin{table}
\begin{tabular}{l|c c c|c c} \hline \hline \multicolumn{1}{c|}{**Training Tokens**} & \multicolumn{3}{c|}{**Validation Loss**} & \multicolumn{2}{c}{**LM Eval. Acc.**} \\  & \(\mathcal{D}_{\text{b}}\)**Pile** & \(\mathcal{D}_{\text{l}}\)**German/SP** & **AVG** & **English** & **HellaSwag-DE** \\ \hline
300B 파일 & 2.17 & 2.70 & 2.44 & 33.95 & 27.09 \\
300B SP & 2.51 & 2.53 & 2.52 & 34.11 & 27.03 \\
300B 말뚝 \(\rightarrow\) 300B SP & 2.44 & 2.50 & 2.47 & 34.93 & 27.43 \\
300B 파일 \(\rightarrow\) 300B SP (5\% Replay) & 2.23 & 2.51 & **2.37** & 35.14 & 27.09 \\
600B Pile \(\cup\) SP & 2.17 & 2.53 & **2.35** & 34.30 & 27.36 \\ \hline
300B 파일 & 2.17 & 2.70 & 2.44 & 33.95 & 27.09 \\
200B German & 3.97 & 1.17 & 2.57 & 27.74 & 29.53 \\
300B 말뚝 \(\rightarrow\) 300B German & 3.56 & 1.11 & 2.34 & 29.20 & 31.23 \\
300B 말뚝 \(\rightarrow\) 200B German (25\% Replay) & 2.33 & 1.16 & **1.75** & 32.48 & 31.04 \\
500B Pile U German & 2.26 & 1.25 & **1.75** & 32.43 & 30.45 \\ \hline \hline \end{tabular}
\end{table}
표 3: **연속적으로 미리 훈련 된 영어 전용 & 영어 독일 모델의 최종 손실** 모든 모델에는 405M 매개 변수가 있습니다. 손실은 10회 반복 간격으로 샘플링된 훈련의 마지막 100회 반복에 걸쳐 평균화된다. 이러한 측정에 대한 표준오차는 계산되었지만 모든 모델에 대해 \(<0.001\)이므로 보고되지 않았다. 스타커 분포 변화에 대해서도 LR 웜업과 5% 리플레이의 조합이 파일 \(\cup\) 독일 모델의 평균 성능에 접근하는 데 도움이 된다는 것을 관찰한다.

#### 6.3.2 인기 있는 LM 벤치마크에 대 한 Zero-shot 및 Few-shot 결과에 의해 평가 된 최종 성능

최종 정확도는 사전 훈련 목표에 대한 성능의 좋은 척도를 제공하지만 LLM의 능력은 일반적으로 평가 작업에 대한 성능으로 판단된다. 기본 모델을 사용한다는 경고, 즉 모델이 어떤 식으로든 명령 조정, 미세 조정 또는 인간 선호도에 적응하지 않았다는 경고와 함께 이 섹션의 인기 있는 벤치마크에 대한 평가를 제시한다. 또한, 독일어 학습 모델에 대한 정성적인 평가도 제공한다. 선택된 평가 과제에 대한 보다 상세한 설명은 독자에게 본고안의 제5.4항과 부록의 제A.5항을 참조한다.

표 3은 우리의 영어 평가 과제에 대한 각 모델의 평균 정확도와 독일 헬라 스웨그 평가 과제에 대한 정규화된 정확도를 보고한다. 거의 무작위 확률 정확도를 갖는 평가로 인해 유익하지 않기 때문에 독일 평균 평가 점수를 보고하지 않는다(표 11 참조). 우리는 영어 모델이 영어 평가에서 독일 모델보다 일관되게 우월하다는 것을 관찰한다. 그러나 25% 리플레이 독일 모델과 함께 사용된 강력한 리플레이는 이러한 격차를 줄이는 데 도움이 된다. 영어 모델들의 영어 평가 성능은 최고값과 최저값 사이의 1.19의 범위로 매우 유사하다. 우리는 이 크기의 기본 모델에 대한 평가 과정에서 상당한 노이즈가 있다고 의심하고 그 차이가 크지 않을 가능성이 있다고 믿는다. 즉, LR 재-온난화, LR 재-붕괴 및 재생이 연속적으로 사전 훈련된 모델은 \(\mathcal{D}_{0}\cup\mathcal{D}_{1}\) 모델에서 개선된다. 영어 평가 과제에 대해 독일어 훈련된 모델을 평가할 때, 우리는 더 많은 리플레이를 사용하는 모델에 대해 일관된 개선을 본다. 우리는 다시 한번 LR 재-온난화, LR 재쇠화 및 재생으로 훈련된 모델이 \(\mathcal{D}_{0}\cup\mathcal{D}_{1}\) 모델에서 개선된다는 점에 주목한다. 독일 헬라 스웨그 결과를 보면 독일 모델이 일관되게 영어 모델보다 우수하다는 것을 알 수 있다. 독일어 훈련 모델 중 연속 훈련 모델이 연합 훈련 모델보다 성능이 뛰어나고 독일어 전용 훈련 모델보다 성능이 뛰어나다.

헬라스와그(평균 영어 모델과 동일)를 제외한 모든 독일 평가 과제에서 독일 모델의 낮은 성능을 감안할 때 모델 세대에 대한 짧은 질적 연구를 수행하여 독일어에 대한 이해도를 추가로 조사했다. 부록의 A.4절에서, 우리는 독일어의 다양한 특수성을 포함하는 5개의 독일 프롬프트를 선택한다(부록의 탭 8 참조). 그런 다음 독일 커먼 크롤을 훈련한 각 모델에 대해 고정된 토큰 길이 응답을 생성한다. 기준선으로서, 우리는 또한 파일에서만 훈련된 모델을 평가한다. 작은 모델 규모에서 세대의 품질이 좋지 않음에도 불구하고, 우리는 체계적으로 주제를 벗어난 경향이 있는 파일 기준선과 비교할 때 독일 커먼 크롤에서 훈련된 모델에서 독일어 출력의 생성 품질이 관찰 가능한 개선이 있음을 발견했다. 이것은 우리의 독일어 훈련 모델들이 언어에 대해 배웠지만, 평가 과제는 405M 파라미터 척도에서 그것을 픽업하기 너무 어렵다는 것을 암시한다. 또 다른 이유는 독일어 데이터 세트가 고려된 영어 데이터 세트보다 작고, 이 작업에 사용된 보다 정교한 영어 데이터 세트와 달리 웹 스크랩된 데이터만 포함한다는 것이다.

요약하면, 약하고 강한 분포의 이동을 위해 LR 재-온난화, LR 재-붕괴 및 재생의 간단하고 확장 가능한 조합을 사용하여 \(\mathcal{D}_{0}\cup\mathcal{D}_{1}\)에 대해 훈련된 모델에 대한 경쟁 성능을 달성할 수 있다. 이것은 최종 검증 손실 및 언어 모델 평가 점수에 대해 사실이며, 간단한 기술의 이러한 강력한 조합이 기존 지식에 대한 타협이 거의 없는 새로운 지식을 언어 모델에 제공할 수 있음을 보여준다._

### 다른 모델 규모에서 연속 사전 훈련 최종 성능

이 부분에서는 연속 사전 훈련의 최종 성능에 대한 매개 변수 수를 크기만큼 증가시키는 효과를 설정한다. 이를 위해 두 개의 연속 사전 훈련된 모델을 405M 및 10B 매개변수 모델 크기에서 여러 기준선과 비교합니다. _두 데이터 세트 약한 이동_ (파일 \(\rightarrow\) SlimPajama) 및 _두 데이터 세트 강한 이동_ (파일 \(\rightarrow\) 독일) 설정에서.

지속적으로 미리 훈련된 모델 LR 재-온난화 및 재-붕괴를 재생과 결합하는 성능을 제거하기 위해, 우리는 학습률을 독점적으로 재-온난화 및 재-붕괴하는 모델과 두 기술을 결합하는 모델을 훈련하기로 선택한다. 약한 분포 이동에 대한 (Sec. 6.2)의 결과를 감안할 때 두 모델 척도에 대해 5% 리플레이를 선택한다. 두 모델 모두 pre-training(\(3\cdot10^{-4}\))의 \(\eta_{max}\)에 다시 따뜻해지고, cosine annealing set을 이용하여 연속 pre-training이 종료되면 \(\eta_{min}\)에 도달하도록 재감쇠한다. 더 많은 하이퍼파라미터가 부록의 표 13에 보고되어 있다.

기준선 우리는 또한 여러 기준선을 훈련한다. 두 개의 기준선은 각각 \(\mathcal{D}_{0}\)와 \(\mathcal{D}_{1}\)에 대해 훈련되고 세 번째 기준선은 \(\mathcal{D}_{0}\cup\mathcal{D}_{1}\)에 대해 훈련된다. 우리는 \(\mathcal{D}_{0}\cup\mathcal{D}_{1}\)로 학습된 모델이 고가의 완전 재학습을 나타내기 때문에 성능의 상한으로 간주한다. 개별 데이터 세트에 대해 훈련된 기준선은 연속 사전 훈련에 대한 계산과 동등한 대안으로 볼 수 있다(예를 들어, 연속 사전 훈련 대신 \(\mathcal{D}_{1}\)에서 무작위 초기화로부터 모델을 훈련하도록 선택할 수 있다).

#### 6.4.1 Final Performance Evaluated by Loss

그림 6은 405M 및 10B 모델에 대한 지속적인 사전 훈련 중 유효성 검사 손실을 보고하고 표 4는 각 모델에 대한 평균(지난 100회 반복 동안) 최종 손실 값을 보고한다. 예상대로 모든 기준선과 지속적으로 사전 훈련된 모델은 매개변수 수가 증가함에 따라 두 데이터 세트 모두에서 복잡성이 일관되게 개선됨을 관찰한다. 405M 모델의 경우, \(\text{Pile}\cup\text{SP}\)이 각 데이터 세트에 대해 개별적으로 훈련된 기준선에 대해 동일한 검증 손실을 달성한다는 것을 관찰한다. 반면, \(\text{Pile}\cup\text{SP}\)로 학습된 10B 파라미터 모델은 개별적으로 학습된 모델보다 성능이 우수하다. 우리는 이것이 더 많은 용량을 가진 더 큰 모델 때문에 발생하여 더 높은 속도로 더 오래 학습할 수 있다고 가정한다. 10B 및 405M 매개변수 모델에 대해 \(\text{SlimPajama}\)에서 사전 훈련을 계속할 때 5% 파일 데이터를 다시 재생하면 \(\text{Pile}\) 검증에 대한 망각이 각각 0.19 및 0.21 감소함을 관찰한다. 두 모델 간의 매개변수 크기 차에도 불구하고 재생으로 인한 망각 감소의 무시할 수 있는 차이는 모델 척도가 망각 감소에 제한된 부정적인 영향을 미친다는 것을 시사한다.

그림 6: **10B(상단) 및 405M(하단) 매개 변수 모델의 연속 사전 훈련 중 유효성 검사 손실** 각 모델 규모에서 3개의 기준선과 2개의 연속 사전 훈련 모델을 제공했습니다. 기준선(밝은 파란색, 짙은 파란색 및 적갈색)은 \(\text{SlimPajama}\), \(\text{Pile}\)의 300B 토큰 및 두 데이터 세트의 결합(600B 토큰)에 대한 무작위 초기화로부터 훈련된다. 연속적으로 사전 훈련된 모델(검은색 및 보라색)은 \(\text{Pile}\)(짙은 파란색 곡선)의 300B 토큰에서 사전 훈련된 체크포인트에서 시작하여 각각 0% 및 5% 리플레이를 사용한다. 우리는 두 모델 크기 모두에서 LR 재온난화, LR 재부패 및 적은 비율의 재생 사용의 조합이 망각과 적응 사이의 균형을 맞추는 데 도움이 된다는 것을 관찰한다. 중요하게도, 우리는 0% 리플레이를 사용하는 모델에 비해 리플레이의 사용이 다운스트림 성능에 최소한으로 영향을 미친다는 점에 주목한다(그림 (b)와 (d)에서 검은색과 보라색 곡선이 겹친다).

더 큰 모델은 기본적으로 덜 잊어버리기 때문입니다. 실제로 사전 훈련된 파일 체크포인트에서 다시 재생하지 않고 훈련된 모델은 10B 및 405M에 대해 각각 0.23 및 0.27 nats의 파일 복잡성을 잊는다. 차이가 작지만 이는 더 큰 모델이 덜 잊음을 시사하여 우리의 가설을 확인시켜준다. 두 데이터 세트의 연합에 대해 훈련된 5% 반복 및 기준선과 모델의 평균 최종 검증 손실을 비교할 때 두 모델 크기에 대해 0.02의 차이만 있음을 알 수 있다. 이는 두 모델 규모에서 약하지만 현실적인 분포 이동의 경우 지속적인 사전 훈련이 값비싼 재훈련 기준선과 유사한 성능을 달성할 수 있음을 보여준다.

#### 6.4.2 인기 있는 LM 벤치마크에 대 한 Zero-shot 및 Few-shot 결과에 의해 평가 된 최종 성능

최종 정확도는 사전 훈련 목표에 대한 성능의 좋은 척도를 제공하지만 LLMs 능력은 일반적으로 평가 작업에 대한 성능으로 판단된다. 기본 모델을 사용한다는 경고, 즉 모델이 어떤 식으로든 명령 조정, 미세 조정 또는 인간 선호도에 적응하지 않았다는 경고와 함께 이 섹션의 인기 있는 벤치마크에 대한 평가를 제시한다. 선택된 평가 과제에 대한 보다 상세한 설명은 독자에게 본고안의 제5.4항과 부록의 제A.5항을 참조한다.

테이블 5는 영어 전용 사전 훈련 LLM에 대한 영어 LM 평가 결과를 보고한다. HellaSwag에 대해 정규화된 정확도가 보고되고 NaturalQuestions 및 TriviaQA에 대해 정확한 일치(EM)가 보고된다. 다른 모든 작업은 정규화되지 않은 정확도를 보고합니다. 예상대로 더 큰 (10B) 모델이 더 작은 대응 모델보다 더 강한 성능을 달성하고 더 많은 토큰으로 훈련된 모델이 더 적은 토큰으로 훈련된 모델보다 항상 더 나은 성능을 달성한다는 것을 관찰한다. 두 모델 척도 모두에서 학습률 재-온난화와 5% 재생 접근법(10B)의 조합을 사용하여 미리 훈련된 모델이 평균 정확도의 측면에서 두 데이터 세트의 연합에 대해 훈련된 모델의 성능을 초과(405M)하는 것을 관찰한다. 조합 훈련된 모델과 연속 사전 훈련된 모델을 비교할 때

\begin{table}
\begin{tabular}{l l|c c c} \hline \hline \multirow{2}{*}{**Model Size**} & \multicolumn{3}{c|}{**Validation Loss**} \\  & \multicolumn{1}{c|}{**Training Tokens**} & \(\mathcal{D}_{\text{t}}\)**Pile** & \(\mathcal{D}_{\text{t}}\)**SimPajama** & **AVG** \\ \hline \multirow{6}{*}{10B} & 300B Pile & 1.75 & 2.24 & 1.99 \\  & 300B SP & 2.08 & 2.05 & 2.07 \\  & 300B Pile + 300B SP & 1.98 & 2.00 & 1.99 \\  & 300B Pile + 300B SP (5\% Replay) & 1.79 & 2.00 & **1.89** \\  & 600B Pile \(\cup\) SP & 1.72 & 2.02 & **1.87** \\ \hline \multirow{6}{*}{405M} & 300B Pile & 2.17 & 2.70 & 2.44 \\  & 300B SP & 2.51 & 2.53 & 2.52 \\ \cline{1-1}  & 300B Pile + 300B SP & 2.44 & 2.50 & 2.47 \\ \cline{1-1}  & 300B Pile \(\rightarrow\) 300B SP (5\% Replay) & 2.23 & 2.51 & **2.37** \\ \cline{1-1}  & 600B Pile \(\cup\) SP & 2.17 & 2.53 & **2.35** \\ \hline \hline \end{tabular}
\end{table}
표 4: **10B 및 405M 매개 변수 모델의 최종 손실** 손실은 10 반복 간격으로 샘플링된 훈련의 마지막 100 반복에 걸쳐 평균화된다. 이러한 측정에 대한 표준오차는 계산되었지만 모든 모델에 대해 \(<0.001\)이므로 보고되지 않았다. 두 모델 척도 모두에서 5% 재생과 결합된 학습률 재-온난화가 합동 훈련의 평균 손실 값에 근접함을 관찰한다.

\begin{table}
\begin{tabular}{l l|c c c c c c c c c c c c} \hline \hline Model Size & Training Tokens & HellaSwag & ARC-C & ARC-C & BookQ & MolQA & MMLU & OQQQ & PIGA & WC & TQA1 & TQA2 & NO & TQA & AVG \\ \hline \multirow{6}{*}{10B} & 300B Pile & 68.46 & 38.51 & 02.69 & 68.83 & 27.34 & 27.38 & 27.20 & 78.62 & 62.51 & 20.44 & 33.86 & 66.53 & 41.92 & 43.45 \\  & 300B SP & 70.38 & 36.77 & 71.93 & 68.01 & 24.76 & 27.42 & 28.20 & 79.69 & 60.

다른 태스크들에 대해, 5% 리플레이 모델과 유니온 트레이닝 모델이 상이한 태스크들에서 최상의 성능을 교환한다는 것을 10B 파라미터 모델들에 대해 관찰하며, 주목할 만한 차이점들은 리플레이 모델에 유리한 OpenBookQA 및 유니온 모델에 유리한 MMLU이다. 405M 매개변수 모델의 경우 5% 재생 모델과 연합 훈련 모델은 눈에 띄는 차이 없이 서로 다른 작업에서 최상의 성능을 교환한다. 두 모델 규모 모두에서 리플레이 모델은 차이가 작고 노이즈에 기인할 수 있지만 리워밍만을 사용하여 모델보다 개선된다.

_요약하면, LR 재-온난화, LR 재-붕괴 및 재생의 조합으로 연속적으로 사전 트레이닝된 모델들이 개별 데이터세트 상의 랜덤 초기화로부터 트레이닝된 베이스라인들의 평균 성능(예를 들어, w.r.t. 최종 검증 손실 및 평가 정확도)을 초과하고 값비싼 재-트레이닝 베이스라인(두 데이터세트들의 연합에 트레이닝됨)에 평균적으로 유사한 평가 성능을 달성한다는 것을 발견한다. 이러한 결과는 \(10B\) 매개변수 규모에서 지속적인 사전 훈련 유지의 이점이 있음을 보여주며, 이는 \(100B\)+ 매개변수의 경우 크기가 더 많은 모델의 경우에도 마찬가지일 수 있음을 시사한다._

## 7 재-온난화의 병리학 이해 및 순환

이 섹션에서는 LR 재-온난화가 원치 않는 망각을 유발한다는 것을 발견하고, 이를 회피하기 위한 유망한 방법으로 무한 학습률 스케줄을 도입하고, 이러한 스케줄을 문헌의 기준선과 비교한다.

### 동일한 데이터에서 다시 워밍

섹션 6.1에서 우리는 처음에 새로운 데이터에 대한 사전 훈련을 계속하면 과거 데이터에 대한 손실이 빠르게 증가하여 다시 재생 사용을 동기화하는 것을 보았다. 특히 \(\eta_{max}\) 값이 클수록 손실의 증가가 두드러졌다. 손실 증가에 대한 한 가지 가설은 대부분 사전 훈련 데이터 세트 간의 분포 이동과 관련 음성 전달에 기인한다는 것이다. 이 가설을 평가하기 위해 우리는 분배 이동이 없는 설정에서 300B 이상의 토큰을 다시 따뜻하고 다시 부패한다. 즉, 그림 1의 실험과 유사한 방법론을 따른다. 3. \(\mathcal{D}_{1}\).

그림 1에서 볼 수 있다. 도 7은 분포 이동과 독립적으로 학습률을 다시 온난화하는 것이 그림 1에서 이전에 볼 수 있는 손실 증가의 중요한 원인인 것으로 판단된다. 3은 사전 훈련을 계속하기 시작할 때, 동일한 분포로 훈련하는 동안 학습률을 재가열할 때 당혹감이 증가하는 것으로 입증된다. 예를 들어, 재가열은 Pile에 대한 사전 훈련을 계속할 때 초기값에 비해 0.1의 Pile 검증 손실의 피크 증가를 초래하며, 이는 Pile에 대한 사전 훈련을 계속할 때 동일한 학습률 일정으로 0.35의 Pile 검증 손실 증가와 대조될 수 있다.

그림 7: **파일 (a) 및 SlimPajama (b)에서 사전 훈련을 계속 하는 경우 파일 유효성 검사 손실입니다. 각 곡선은 파일의 300B 토큰에서 미리 훈련된 동일한 체크포인트에서 시작되지만 다른 최대 학습 속도로 훈련된다. 학습률을 다시 온난화하는 효과에 초점을 맞추면서 처음 100B 토큰에 대한 곡선만 보여준다. 초기 사전 훈련의 최소 학습률에서 학습률을 다시 증가시키는 모든 모델(예: 상수를 제외한 모든 모델)이 손실이 증가한다는 것을 관찰합니다.**

SlimPajama는 그림 3과 같다. 재가온이 높을수록 파일에서 사전 훈련을 계속할 때(피크 손실 증가 0.2) SlimPajama에서 사전 훈련을 계속할 때(피크 손실 증가 0.45) \(\eta_{max}=6\cdot 10^{-4}\) 곡선에서 볼 수 있듯이 이러한 효과가 더 두드러진다.

특히, 재-온난화 이후, 모델들은 동일한 데이터 세트에 대한 트레이닝을 할 때에도 학습률을 재-온난화함으로써 성능 히트로부터 신속하게 복구하지 못한다. 이는 지속적인 사전 훈련의 효율성을 향상시키기 위해 재가열이 필요한 학습률 일정에 대한 대안을 찾는 동기를 부여한다.

### 무한 학습 속도 스케줄

이 하위 섹션에서는 본질적으로 재가열이 필요하지 않을 수 있는 학습률 일정의 사용을 조사한다. 동기는 두 가지입니다. 한편으로 코사인 붕괴 일정은 미리 훈련하고 싶은 총 토큰 수를 미리 알아야 한다. 이는 수렴된 체크포인트를 계속 사전 훈련하는 능력을 제한한다. 반면에 이전 섹션에서 처음에는 작은 학습 속도로 끝나는 코사인 감쇠 일정으로 사전 훈련된 모델을 계속 사전 훈련할 때 새로운 데이터 세트에 가장 잘 적응하기 위해 최소값에서 학습 속도를 다시 온난화하는 것이 필요하다는 것을 보았다. 그러나 이전 하위 섹션에서 볼 수 있듯이 학습률을 다시 온난화하면 망각이 악화될 수 있음을 관찰한다.

따라서 우리는 모든 새로운 과제에서 학습률을 일정한 값으로 유지하는 "무한 학습률 일정"(Zhai et al., 2022)을 탐구한다. 이는 새로운 과제에 대한 학습을 다시 데우는 것을 피함으로써 망각을 방지하는 데 도움이 될 수 있다. 또한, 이 스케줄은 토큰의 총 수와 독립적이어서 각각의 새로운 데이터 세트에 대해 코사인 감쇠 스케줄을 순환적으로 반복하는 것에 비해 지속적인 학습 설정에 더 적합하다. 우리가 보았듯이, 높은 일정한 학습률도 차선책이기 때문에, 우리는 제한된 양의 토큰에 걸쳐 사전 트레이닝이 끝날 때 학습률의 빠른 어닐링을 수행하기로 선택한다. 이를 통해 학습률 재감소의 성능 우위를 회복하는 한편 사전 훈련을 계속할 때 사전 어닐링 체크포인트를 사용할 수 있기를 기대한다.

고려되는 무한 학습률 스케줄은 4개의 단계를 갖는다:

1. **선형 웜업 단계** - 이전과 마찬가지로 학습률은 처음에 \(T_{warmup}\) 시간 간격보다 최대값 \(\eta_{max}\)으로 증가하거나 시간 간격 \(t_{cd}=T_{warmup}\)까지 동등하게 증가합니다. 학습률은 (첫 번째 과제 동안) 한 번만 워밍업을 거치며, 향후 과제를 위해 다시 워밍업을 할 필요가 없다.
2. **쿨다운 단계** - 이 단계에서 학습 속도는 타임스텝 \(t_{cd}\)에서 \(t_{const}=t_{cd}+T_{cd}\)까지의 타임스텝에 걸쳐 일부 감쇠 함수 \(f_{cd}\)에 따라 일정한 값 \(\eta_{const}\)으로 점진적으로 감쇠되는 쿨다운 단계를 거친다. 이 단계도 첫 번째 작업 중에 한 번만 발생합니다.
3. **일정 단계** -그런 다음 학습 속도는 timestep \(t_{const}\)에서 \(t_{ann}=t_{const}+T_{const}\) timestep에 걸쳐 모든 미래 작업에 대해 일정하게 유지됩니다. 이 단계가 끝날 때 얻은 체크포인트는 새 데이터 세트를 계속 사전 훈련할 때부터 다시 시작해야 하는 것이다.
4. **어닐링 단계** - 학습 속도가 timestep \(t_{ann}\)에서 timestep \(t_{end}=t_{ann}+T_{ann}\)에 걸쳐 작은 값 \(\eta_{min}\)으로 어닐링되어 모델을 배포하기 전에 수렴으로 훈련하는 데 도움이 됩니다.

따라서, 여기서 고려되는 무한 학습률 스케줄은 다음과 같이 작성될 수 있다:

\textit{warm-up})\\ &f_{cd}(t)&t\in(t_{cd},t_{const}]&(\textit{cooldown})\\ &\eta_{const}&t\in(t_{const},t_{ann}]&(\textit{constant})\\ &\eta_{const}\cdot\left(\frac{\eta_{min}}{\eta_{const}}\right)^{\frac{t-t_{ conn}}{t_{end}-t_{ann}}}&t\in(t_{ann},t_{end}]&(\textit{annealing})\end{array}\right.\]

본 연구에서는 cooldown phase의 decay \(f_{cd}\):1. Cosine decay \[f_{cd}(t)=\eta_{const}+\frac{\eta_{max}-\eta_{const}}{2}\cdot\left(1+\cos\left(\pi \left(\frac{t-t_{cd}}{t_{const}-t_{cd}}\right)\right)\] (3)
2. 역제곱근 붕괴 \[f_{cd}(t)=\eta_{max}+\frac{\eta_{const}-\eta_{max}}{h(1)}\cdot h\left(\frac{t- t_{cd}}{t_{const}-t_{cd}}\right)\] (4) 여기서 \[h(x)=\frac{1}{\sqrt{1+\alpha x}}-1\]

Inverse square root decay의 steepness는 \(\alpha\)로 조절하였다. 우리는 구간 \((t_{cd},t_{const}]\)에 적응하기 위해 역제곱근 붕괴를 이동 및 스트레칭한다.

세 가지 다른 일정이 그림 1에 나와 있다. 도 8(b)에 도시된 바와 같이,

우리는 이제 무한 학습률 스케줄을 코사인 감쇠 스케줄과 비교한다. 먼저 LLM 사전 훈련 일정의 타당성을 평가하기 위해 간단한 단일 데이터 세트 사전 훈련 설정을 탐색한다. 그 후, 우리는 시프트가 없는 3개의 데이터 세트에서 그 이점을 탐구합니다.

### Cosine Decay와 무한 스케줄의 변수 비교

여기서는 공통 단일 데이터 세트 사전 훈련 설정에서 코사인 감쇠 스케줄을 무한 학습률 스케줄과 비교한다. 이러한 실험의 목적은 무한 학습 속도 스케줄이 종래의 코사인 감쇠 스케줄로 트레이닝된 모델들뿐만 아니라 수행하는 모델들을 초래할 수 있는지를 테스트하는 것이다.

모델들은 랜덤 초기화로부터 SlimPajama의 300B 토큰들에 대해 사전 트레이닝된다. 도 8은 상이한 학습률 스케줄을 갖는 SlimPajama 상에서 트레이닝된 3개의 405M 파라미터 모델들의 트레이닝 곡선들을 도시한다. 우리는 모든 방법이 사전 훈련의 일반적인 경우에도 무한 학습률 일정이 사용될 수 있음을 보여주는 유사한 최종 검증 손실에 도달함을 관찰한다. 이러한 스케줄들은 또한 사전 트레이닝을 최종화하기로 결정할 때 손실을 효율적으로 개선하기 위해 일정한 단계에서 언제든지 어닐링을 시작할 수 있고, 사전 어닐링 체크포인트를 로딩하여 사전 트레이닝을 계속할 수 있다는 장점이 있다.

그림 8: **무한 학습률 일정 v.s. 코사인 붕괴** 입니다. SlimPajama의 300B 토큰에 대해 두 개의 새로운 일정인 _Cosine Inf_ 및 _InvSgrt Inf_를 사용하여 무작위 초기화에서 405M 매개변수 모델을 훈련하고 코사인 감쇠 기준선과 비교합니다. _ 코사인 Inf_ 및 _InvSgrt Inf_는 먼저 고정된 상수 LR 값으로 감쇠하고 그 후 급격한 최종 감쇠까지 일정하게 유지한다. 따라서 이러한 일정은 재가열 없이 한 사전 훈련 단계와 다음 사전 훈련 단계 사이에서 원활하게 전환할 수 있다는 장점이 있다. 모든 방법이 유사한 최종 유효성 검사 손실에 도달한다는 것을 발견했으며, 이는 코사인 붕괴가 강력한 성능을 위한 전제 조건이 아님을 보여준다.**

### 무한 학습 속도 스케줄: 무한 미래 업데이트로 확장

우리는 이제 연속 학습 설정에서 여러 개의 새로운 데이터 세트가 보일 때 무한 학습률 스케줄의 역할을 탐구한다. 모델은 SlimPajama의 3개의 IID 100B 하위 집합(예: _3개의 데이터 집합 no shift_ 설정; Sec 5.2 참조)에서 서로 다른 학습 속도 일정을 사용하여 무작위 초기화에서 훈련됩니다. 이러한 예비 실험에서 교대 없음 설정에 초점을 맞추고 약하고 강한 교대 사례는 향후 작업에 맡긴다. 이 작업은 동일한 분포로부터 대량의 데이터가 시간 증분으로 수신되는 설정을 시뮬레이트하고, 이에 대한 모델을 계속 사전 트레이닝(예를 들어, 최신 웹 스크래프에서 모델을 계속 사전 트레이닝)하고자 한다. 우리의 결과가 이전의 최적화기 상태를 사용할 수 없는 상황에 적용되도록 하기 위해, 우리는 데이터세트 경계에 걸쳐 최적화기 상태를 유지하지 않는다. 도. 도 9는 405M 파라미터 모델들에 대한 트레이닝 곡선들을 보고한다.

우리는 모든 스케줄이 비교적 유사하게 수행되지만, 두 개의 무한 스케줄은 각각의 분할에서 일정한 학습 속도 단계 동안 언제든지 어닐링을 시작할 수 있는 장점이 있는 반면, 반복되는 코사인 디케이는 토큰의 수를 미리 알아야 한다. 또한 무한 LR 스케줄에 대한 데이터 세트 경계에서 무시할 수 있는 망각을 볼 수 있습니다. 최적화 상태들의 재초기화로 인해 손실들이 초기에 급격히 증가하는 반면, 무한 스케줄 모델들은 이로부터 즉시 회복된다.

향후 연구에서는 분포 이동이 있는 지속적인 학습 설정에서 무한 학습 속도 일정이 미치는 영향을 연구하고 학습 속도의 긴 일정한 단계를 가진 많은 양의 토큰에 대한 훈련의 안정성을 조사하는 것이 흥미로울 것이다.

_요약하면, 우리는 재-온난화가 동일한 분포에서 트레이닝을 할 때에도 성능을 해칠 수 있지만 코사인 붕괴 스케줄에 대한 대안들이 이러한 문제들을 회피할 수 있다는 것을 보았다. 나아가, 이러한 무한 학습률 스케줄들은 특정 토큰 버짓에 제한되지 않고 사전 트레이닝을 종료하거나 재개하는 간단한 방법을 제공한다. 즉, 배포 이동이 있는 설정도 탐색하여 이러한 일정을 검증해야 합니다._

## 8 Limitations

LLM에 대한 지속적인 사전 훈련에 대한 철저한 경험적 평가를 수행했지만 작업에는 몇 가지 한계가 있다. 1) 두 가지 모델 크기(405M 및 10B)만 연구했고, 2) 독일 커먼 크롤 스크래프(Laippala et al., 2022)에서 만든 독일 훈련 및 검증 데이터 세트 간에 중복 제거를 실행하지 않았으며, 3) 두 후속 작업 간의 전환을 주로 연구했으며, 4) 여러 씨앗에 대한 실험을 실행하지 않았으며, 5) 무한 학습 속도 일정에 대한 실험은 분포 이동 없이 405M 척도로 제한된다. 보다 명시적으로, 첫 번째 제한은

그림 9: **SP의 3 IID 100B 토큰 하위 집합에서 평가 된 무한 학습 속도 일정입니다. 실험은 동일한 분포의 새로운 데이터가 시간이 지남에 따라 도착하고 실무자가 새로운 데이터에 대한 모델을 업데이트하려는 설정을 시뮬레이션한다. 모델들은 무작위 초기화로부터 트레이닝된다. 그림 (b)에서 검정색과 보라색 일정이 \(\sim 80\)B 토큰 후에 겹칩니다.**우리가 고려하는 모델 규모의 수입니다. 405M 및 10B 매개변수 모델(대부분의 작업보다 훨씬 큰)을 고려하지만 계산 제한(예: 100B 매개변수 척도)으로 인해 연구를 다른 규모까지 확장할 수 없었다. 우리 작업의 두 번째 한계는 독일 검증 세트가 독일 훈련 데이터에서 중복되지 않았다는 것이다. 훈련 및 검증을 위해 뚜렷한 파편을 취하도록 주의했지만 둘 사이에 약간의 오염이 있을 수 있다. 그러나 모든 기준선이 동일한 데이터 세트에 액세스할 수 있다는 점을 감안할 때 결과는 여전히 유효하다고 생각합니다. 세 번째 한계는 두 개 이상의 후속 작업에 대해 모델을 업데이트하는 실험을 실행하지 않았다는 것이다. 이를 연구하는 것이 중요하다고 생각하지만, 우리의 목표는 많은 수의 데이터 세트를 사용하는 것이 아니라 다양한 분포 이동과 대규모 데이터 세트 간의 전환을 연구하는 것에 초점을 맞추는 것이었다. 네 번째 한계는 높은 계산 비용으로 인해 여러 종자에 대한 실험을 실행하지 않았다는 것이며, 이는 일부 결과에 확률적 요소가 있을 가능성이 있음을 의미한다. 즉, 우리의 LLM은 큰 배치 크기(2M+ 토큰)로 훈련되어 기울기 추정치에 분산이 거의 없다. 각 데이터 세트의 샘플이 모든 경우에 동일한 순서로 처리된다는 사실과 함께, 우리는 우리의 결과가 시드에 의해 지시되는 무작위 초기화의 변화에 상대적으로 안정적이어야 한다고 믿는다. 다섯 번째 한계는 모든 후속 데이터 세트에 대한 학습이 차선으로 판명된 일정한 학습 속도를 사용하는 것과 동일할 수 있기 때문에 충분한 토큰에 걸쳐 무한 일정이 워밍업 및 쿨다운의 단일 단계만으로 인해 차선으로 끝날 수 있다는 것이다(그림 3 참조). (p<0.05). 도 9는 어닐링 단계가 동일한 데이터세트의 IID 분할의 경우, 이것이 더 많은 토큰을 보유하는지, 또는 상이한 데이터세트가 분포 이동을 갖는 경우 이러한 준최적성으로부터 복구하는 데 도움이 된다는 것을 보여주었다. 따라서 분포 이동과 더 큰 규모의 모델 및 데이터 세트를 포함하는 실험은 이러한 무한한 일정을 추가로 테스트하는 데 중요할 것이다. 마지막으로, 더 큰 규모에서 탐색하기 위한 또 다른 중요한 고려 사항은 이러한 스케줄을 갖는 사전 훈련의 안정성(특히, \(\mu P\)(Yang et al., 2022))이다.

## 9 Conclusion

LLM의 지속적인 사전 훈련의 맥락에서, 우리는 학습률 재-온난화 및 재-붕괴가 적응에 중요하다는 것을 보았고, 망각은 적응에 거의 비용이 들지 않는 것처럼 보이는 이 환경에서 재생으로 쉽게 완화된다는 것을 발견했다. 적응을 강화하고 망각을 동시에 완화할 수 있는 강력한 능력을 감안할 때, 우리는 대규모 LLM을 지속적으로 사전 훈련하기 위해 LR 재-온난화, LR 재-붕괴 및 재생의 간단하고 확장 가능한 조합을 제안했다. 우리는 이러한 전략이 두 개의 분포 이동(약하고 강한)과 두 개의 모델 척도(405M & 10B)에 걸쳐 모든 데이터에 대해 처음부터 비용적으로 재훈련하는 것과 동등하게 평균 성능을 달성할 수 있음을 보여주었다. 추가 분석을 통해 LR 재-온난화의 병리학을 식별하고 이전 작업에서 영감을 받아 지속적으로 사전 훈련 LLM을 위한 무한 학습 속도 일정을 제안했다. 초기 실험에서, 우리의 스케줄은 LR 재-온난화의 필요성을 회피하면서 코사인 붕괴와 동등한 성능을 달성한다.

우리의 연구 결과는 새로운 데이터에서 LLM을 업데이트할 때 지속적인 사전 훈련이 재훈련의 효율적이고 유망한 대안임을 보여준다. 우리의 전략을 갖춘 실무자는 새로 생성된 고품질 데이터 세트에 대해 기존 모델(Rae 등, 2021; Hoffmann 등, 2022; Touvron 등, 2023; Jiang 등, 2023; Gemma Team 등, 2024)을 효율적으로 업데이트할 수 있다. 이러한 전략은 또한 Gemma Team et al.(2024)에 의해 사용된 것과 같은 사전 훈련 커리큘럼과 관련이 있을 수 있다. 우리 커뮤니티가 고품질 데이터 세트를 계속 만들려는 강력한 인센티브로 인해 지속적인 사전 훈련의 필요성이 증가할 것으로 예상한다.

후속 작업에서는 무한 학습률 일정, 지속적인 사전 훈련(예: 전문가 혼합 또는 블록 확장) 중 모델 성장, 데이터 분포에 대한 급격한 변화를 처리하기 위해 토큰화기를 적용하는 것을 추가로 조사하는 것이 중요할 것이다. 또한, 우리는 멀티모달 또는 비전 언어 모델 및 기타 텍스트 기반 생성 모델의 맥락에서 지속적인 사전 훈련을 탐색하고자 한다 - 최근 Garg 등(2023)이 LLM 대신 CLIP 모델의 맥락에서 이 작업에서 논의된 기술의 성공을 동시에 복제했다는 점에 주목한다. 또한 공개 가중치 모델이 데이터 세트를 공개하지 않는 연속 사전 훈련 환경에서 재생 버퍼 생성을 탐색하고, 합성 데이터 또는 증류에 사용 가능한 모델을 사용하는 것이 재생 버퍼를 구축하는 유망한 방향일 수 있다고 의심한다.

#### Broader Impact Statement

대규모 언어 모델은 관련 데이터 세트에 대해 훈련된 후 매우 잘 수행할 수 있는 능력으로 인해 광범위한 산업 부문에 걸쳐 널리 채택되었다. 또한, LLM의 출력 품질을 높이기 위해 데이터 세트의 개선(더 나은 필터링, 지식 업데이트 등)이 중요했다. 따라서 조직은 상당한 양의 컴퓨팅 능력을 사용하고 따라서 더 강력한 모델을 만들기 위해 에너지를 소비할 것으로 기대하는 것이 합리적이다. 이 에너지의 일부는 재생 불가능한 원천에서 나올 것 같다. 본 논문에서 제시된 실험은 환경적으로 비용이 많이 들지만, 논문에서 주장한 바와 같이 사전 훈련을 계속하는 것은 모델 업데이트와 관련된 계산 및 따라서 기초 모델을 유지하는 데 필요한 에너지를 크게 줄이는 유망한 방법이다.

## Acknowledgements

NSERC Discovery Grant RGPIN-2021-04104 [E.B], 캐나다 CIFAR AI 의장 프로그램 [I.R], 캐나다 우수 연구 의장 프로그램 [I.R]. FRQNT Doctoral (B2X) 장학금 [B.T], 몬트리올 대학의 Etudes Superieures et Postdoctorales의 인공지능 장학금 [A.I.], 독일 학술 교류 서비스 (DAAD) [M.R.]의 IFI 프로그램 펠로우십으로부터 지원을 받고 있습니다. 이 연구는 INCITE 2023 프로그램 상 "확장 가능한 일반 AI를 위한 확장 가능한 기초 모델"의 일부로 제공되는 서밋 슈퍼컴퓨터의 컴퓨팅 자원 덕분에 가능했습니다. 이러한 자원은 계약번호 DE-AC05-00OR22725에 따라 미국 에너지부 과학실의 지원을 받는 오크 릿지 국립 연구소의 오크 릿지 리더십 컴퓨팅 시설에서 제공되었다. 특히, 우리는 Jens Glaser가 서밋 슈퍼컴퓨터를 도와준 것에 대해 감사한다.

## References

* 2022년 12월 9일, 2022_, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccb41a7d800-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccb41a7d800-Abstract-Conference.html)
* Aljundi 등 (2019) Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne Tuytelaars. 검색에 최대 방해가 되는 온라인 연속 학습입니다. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), _Advances in Neural Information Processing Systems 32_, pp. 11849-11860. Curran Associates, Inc., 2019. URL [http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-](http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-) 간섭 검색.pdf.
* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 매 시리즈의 개방형 언어 모델입니다. _ CoRR_, abs/2311.16867, 2023. URL [https://doi.org/10.48550/arXiv.2311.16867](https://doi.org/10.48550/arXiv.2311.16867).
* Amini et al.(2019) Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: 연산 기반 형식론으로 해석 가능한 수학 단어 문제 해결을 위해, 2019.
* Andonian 등(2021) Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang Phil, and Samuel Weinbach. GPT-NeoX: PyTorch, 8 2021의 대규모 Autoregressive Language Modeling. URL [https://www.github.com/eleutherai/gpt-neox](https://www.github.com/eleutherai/gpt-neox)

요나탄 비스크, 로완 젤러스 로난 르 브라스 지안펑 가오, 예진 최 Piqa: 자연어의 물리적 상식에 대한 추론, 2019.
* Black et al.(2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: 오픈 소스 자동 회귀 언어 모델, 2022.
* Brooks et al.(2024) Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Wing Yin Ng, Ricky Wang, and Aditya Ramesh. 비디오 생성 모델은 월드 시뮬레이터입니다. 2024. URL [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators)입니다.
* Brown 등(2020) Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models is few-shot learners. *Proceedings of the 34th International Conference on Neural Information Processing Systems_, pp. 1877-1901, 2020. URL [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165).
* Buzzega et al.(2020) Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. 일반적인 지속적인 학습을 위한 어두운 경험: 강하고 단순한 기준선. 휴고 라로셸, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan 및 Hsuan-Tien Lin(eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/b704ea2c39778f07c617f6b7ce480e9e-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/b704ea2c39778f07c617f6b7ce480e9e-Abstract.html)
* Caccia et al.(2020) Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexandre Lacoste, David Vazquez, and Laurent Charlin. 온라인 빠른 적응과 지식 축적: 지속적인 학습에 대한 새로운 접근법 _ NeurIPS_, 2020. URL [https://arxiv.org/abs/2003.05856](https://arxiv.org/abs/2003.05856).
* Chen et al.(2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 서브리니어 메모리 비용으로 딥 네트를 훈련합니다. _ CoRR_, abs/1604.06174, 2016. URL [http://arxiv.org/abs/1604.06174](http://arxiv.org/abs/1604.06174).
* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: 자연스런 예/아니오 질문의 놀라운 난이도 탐구, 2019.
* Clark 등(2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문을 풀었다고 생각해? try arc, the ai2 reasoning challenge, 2018.
* Computer (2023)Together Computer. Redpajama: 대용량 언어 모델을 학습하기 위한 오픈 데이터 세트, 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)입니다.
* Cossu et al.(2022) Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia Passaro, Vincenzo Lomonaco, and Davide Bacciu. 지속적인 사전 훈련은 언어 및 비전, 2022의 망각을 완화합니다. URL [https://arxiv.org/abs/2205.09357](https://arxiv.org/abs/2205.09357).
* Douillard 등(2023) Arthur Douillard, Qixuan Feng, Andrei A Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc'Aurelio Ranzato, Arthur Szlam, and Jiajun Shen. 딜로코: 언어 모델의 분산 저통신 교육 _ arXiv preprint arXiv:2311.08105_, 2023.
* French (1999) Robert M. 프랑스어 연결리스트 네트워크에서 치명적인 망각입니다. _ Trends in Cognitive Sciences_, 3(4):128-135, 1999. ISSN 13646613. doi: 10.1016/S1364-6613(99)01294-2. URL [https://www.sciencedirect.com/science/article/abs/pii/S1364661399012942](https://www.sciencedirect.com/science/article/abs/pii/S1364661399012942).
* Gao 등(2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of various text for language modeling _ arXiv preprint arXiv:2101.00027_, 2020.
* Gendelman et al. (2019)Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaiishaal Shankar, and Fartash Faghri. Tic-clip: 클립 모델의 지속적인 훈련입니다. _ arXiv preprint arXiv:2310.16226_, 2023. URL [https://arxiv.org/abs/2310.16226](https://arxiv.org/abs/2310.16226).
* Tearam 등(2024) Thomas Mesnard Gemma Team, Cassidy Hardin, Robert 다다시, Surya Bhupatiraju, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Leonard Hussenot, and et al. Gemma: Open models based on gemini research and technology. 2024. URL [https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf).
* Goddard 등(2024) Charles Goddard, Shamane Siriwardhana, Malikeh Eghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcee의 mergekit: 대규모 언어 모델을 병합하는 도구 키트입니다. _ arXiv preprint arXiv:2403.13257_, 2024.
* Gogoulou 등(2023) Evangelia Gogoulou, Timothee Lesort, Magnus Boman, Joakim Nivre. 언어 변화에 따른 지속적인 학습에 대한 연구 _ CoRR_, abs/2311.01200, 2023. URL [https://doi.org/10.48550/arXiv.2311.01200](https://doi.org/10.48550/arXiv.2311.01200).
* Gong et al.(2022) Zheng Gong, Kun Zhou, Xin Zhao, Jing Sha, Shijin Wang 및 Ji-Rong Wen. 구문 인식 메모리 네트워크를 사용하여 수학 문제 이해를 위한 언어 모델의 지속적인 사전 학습, 2022. URL [https://aclanthology.org/2022.acl-long.408/](https://aclanthology.org/2022.acl-long.408/)
* Goyal et al. (2018) Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 정확하고 큰 미니배치 sgd: 2018년 1시간 동안 이미저넷을 훈련한다.
* Gupta et al. (2023) Kshitij Gupta, Benjamin Therien, Adam Ibrahim, Mats L. 리히터, 쿠엔틴 앤서니, 유진 벨릴로프스키, 이리나 리치, 티모티 레소르트. 대규모 언어 모델의 지속적인 사전 훈련: 모델을 (재)웜하는 방법?, 2023. URL [https://arxiv.org/abs/2308.04014](https://arxiv.org/abs/2308.04014)
* Gururangan 등(2020) Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 사전 교육을 멈추지 마십시오. 언어 모델을 도메인 및 작업에 적용합니다. 단 주라프스키에서는 조이스 차이, 나탈리 슐루터, 조엘 R. Tetreault(eds.), _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020_, pp. 8342-8360. Association for Computational Linguistics, 2020. URL [https://doi.org/10.18653/v1/2020.acl-main.740](https://doi.org/10.18653/v1/2020.acl-main.740).
* Harun et al.(2023a) Md Yousuf Harun, Jhair Gallardo, Tyler L Hayes, and Christopher Kanan. 오늘날의 지속적인 학습 알고리즘은 얼마나 효율적인가? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2430-2435, 2023a.
* Harun et al. (2023b) Md Yousuf Harun, Jhair Gallardo, Tyler L. 헤이즈, 로널드 켐커 크리스토퍼 캐넌 시에스타: 수면을 통한 효율적인 온라인 연속 학습, 2023b.
* Hendrycks 등(2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021년 대규모 멀티태스킹 언어 이해도 측정
* Hernandez et al.(2021) Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 전송에 대한 크기 조정 법칙입니다. _ CoRR_, abs/2102.01293, 2021. URL [https://arxiv.org/abs/2102.01293](https://arxiv.org/abs/2102.01293).
* Hoffmann 등 (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _ arXiv preprint arXiv:2203.15556_, 2022. URL [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556).
* Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, Hyouk Jung Lee, Jiquan Ngiam, Quoc V. 르, 우용희, 지펑 첸 Gpipe: 파이프라인 병렬성을 이용한 거대 신경망의 효율적인 훈련, 2019.
* 장 외(2022) 조엘 장, 성현 예, 창호 이, 양소희, 신중보, 한장훈, 김경훈, 서민준. Temporalwiki: 끊임없이 진화하는 언어 모델을 훈련하고 평가하기 위한 평생 벤치마크. Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang(eds.), _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022_, pp. 6237-6250. Association for Computational Linguistics, 2022a. URL [https://doi.org/10.18653/v1/2022.emnlp-main.418](https://doi.org/10.18653/v1/2022.emnlp-main.418).
* 장 외(2020)조엘 장, 성현 예, 양소희, 신중보, 한장훈, 김경훈, 최스탠리정규, 서민준. 언어 모델의 지속적인 지식 학습을 지향합니다. <제10차 국제학술대회>의 ICLR 2022, Virtual Event, 4월 25일~29일, 2022. OpenReview.net, 2022b. URL [https://openreview.net/forum?id=vfsRBSMImo9](https://openreview.net/forum?id=vfsRBSMImo9)
* Jiang et al.(2023) Albert Q. 장, 알렉산드르 사블레이롤, 아서 멘쉬, 크리스 뱀포드, 데벤드라 싱 채플롯, 디에고 드 라스 카사스, 플로리안 브레산드, 지안나 랭글, 기욤 람플, 루실라 사울니에, 렐리오 레나르 라부드, 마리 앤 라쇼, 피에르 스톡, 테벤 르 스카오, 티보 라브릴, 토마스 왕, 티모티 라크루아, 윌리엄 엘 세이드. 미스트랄 7b. _ CoRR_, abs/2310.06825, 2023. URL [https://doi.org/10.48550/arXiv.2310.06825](https://doi.org/10.48550/arXiv.2310.06825).
* Joshi et al.(2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: 읽기 이해를 위해 멀리 감독된 대규모 챌린지 데이터 세트입니다. Regina Barzilay and Min-Yen Kan(eds.), _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147)입니다.
* Kaplan 등(2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 신경 언어 모델의 법칙을 조정합니다. _ CoRR_, abs/2001.08361, 2020. URL [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361).
* Ke et al.(2022) Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 언어 모델의 지속적인 사전 학습, 2022. URL [https://openreview.net/forum?id=m_GDIItaI3o](https://openreview.net/forum?id=m_GDIItaI3o)
* Kirillov 등(2023) Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. 뭐든 조각내세요 arXiv:2304.02643_, 2023.
* Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. 다이, 야콥 우즈코리트, 콕 르, 슬라브 페트로프 자연스러운 질문: 질의 응답 연구를 위한 벤치마크입니다. _ 계산 언어학 협회의 트랜잭션_, 7:452-466, 2019. doi: 10.1162/tacl_a_00276. URL [https://aclanthology.org/Q19-1026](https://aclanthology.org/Q19-1026).
* 17, 2022_, pp. 215-221. Association for Computational Linguistics, 2022. URL [https://aclanthology.org/2022.wnut-1.23](https://aclanthology.org/2022.wnut-1.23).
* Lesort 등 (2023) Timothee Lesort, Oleksiy Ostapenko, Pau Rodriguez, Diganta Misra, Md Rifat Arefin, Laurent Charlin, and Irina Rish. 재앙적인 망각과 지식 축적에 대한 일반적인 가정에 도전하는 것. Sarath Chandar, Razvan Pascanu, Hanie Sedghi, and Doina Precup(eds.), _Conference on Lifelong Learning Agents, 22-25 August 2023, McGill University, Montreal, Quebec, Canada_, volume 232 of _Proceedings of Machine Learning Research_, pp. 43-65. PMLR, 2023. URL [https://proceedings.mlr.press/v232/lesort23a.html](https://proceedings.mlr.press/v232/lesort23a.html)
* Lesort 등(2021) Timothee Lesort, Massimo Caccia, Irina Rish. 데이터 분포 드리프트 분석을 통한 지속적인 학습 설정 이해 _ arXiv preprint arXiv:2104.01678_, 2021.
* Lin et al.(2022) Stephanie Lin, Jacob Hilton, and Owain Evans. 진실성: 모델들이 어떻게 인간의 거짓을 모방하는지 측정하는 것, 2022년.
* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. 분리된 중량 감쇠 규칙화. 제7회 International Conference on Learning Representations, ICLR 2019, New Orleans, LA, May 6-9, 2019_에서. OpenReview.net, 2019. URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)
* Liu et al.(2019)* Ma et al.(2023) Shirong Ma, Shen Huang, Shulin Huang, Xiaobin Wang, Yangning Li, Hai-Tao Zheng, Pengjun Xie, Fei Huang, and Yong Jiang. Ecomgpt-ct: 반구조화된 데이터를 사용한 전자상거래 대형 언어 모델의 지속적인 사전 훈련 _ CoRR_, abs/2312.15696, 2023. URL [https://doi.org/10.48550/arXiv.2312.15696](https://doi.org/10.48550/arXiv.2312.15696).
* McMahan et al.(2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 분산된 데이터로부터 심층 네트워크의 통신 효율 학습. _Artificial intelligence and statistics_, pp. 1273-1282. PMLR, 2017.
* Mehta et al.(2023) Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. 평생학습에서 사전훈련의 역할에 대한 실증적 조사 _ J 마흐 배워요 Res._ , 24:214:1-214:50, 2023. URL [http://jmlr.org/papers/v24/22-0496.html](http://jmlr.org/papers/v24/22-0496.html)
* Mermillod et al.(2013) Martial Mermillod, Aurelia Bugaiska, and Patrick Bonin. 안정성-가소성 딜레마: 재앙적인 망각에서 연령 제한 학습 효과에 이르기까지 연속체를 조사합니다. _ Frontiers in psychology_, 4(August): 504, 2013. ISSN 1664-1078. doi: 10.3389/fpsyg.2013.00504. URL [http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3732997](http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3732997){&}tool=pmcentrez{&}rendertype=abstract.
* MCGsoft (2020) Microsoft. Megatron-DeepSpeed. [https://github.com/microsoft/Megatron-DeepSpeed] (https://github.com/microsoft/Megatron-DeepSpeed), 2020. Accessed: February 28, 2024.
* Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 갑옷이 전기를 통할 수 있나요? 2018년 오픈 북 질문 응답을 위한 새로운 데이터 세트.
* Mirzadeh et al. (2022) Seyed-Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Huiyi Hu, Razvan Pascanu, Dilan Gorur, and Mehrdad Farajtabar. 광범위한 신경망은 덜 재앙적으로 잊는다. Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato(eds.), _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pp. 15699-15717. PMLR, 2022. URL [https://proceedings.mlr.press/v162/mirzadeh22a.html](https://proceedings.mlr.press/v162/mirzadeh22a.html)
* Ostapenko et al.(2019) Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. 기억하기 위한 학습: 지속적인 학습을 위한 시냅스 가소성을 기반으로 하는 프레임워크. _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 11321-11329, 2019. URL [https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html](https://openaccess.thecvf.com/content_CVPR_2019/html/Ostapenko_Learning_to_Remember_A_Synaptic_Plasticity_Driven_Framework_for_Continual_CVPR_2019_paper.html)
* Pernias 등(2024) Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. Wurstchen: 대규모 텍스트-이미지 확산 모델을 위한 효율적인 아키텍처. _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=gUS8d5QeGv](https://openreview.net/forum?id=gUS8d5QeGv)입니다.
* Pluster (2023) Bjorn Pluster. 독일 벤치마크 데이터 세트, 2023. URL [https://github.com/boernp1/GermanBenchmark](https://github.com/boernp1/GermanBenchmark)
* Popel and Bojar (2018) Martin Popel and Ondrej Bojar. 변압기 모델에 대한 교육 팁입니다. _ Prague Bulletin of Mathematical Linguistics_, 110(1):43-70, April 2018. ISSN 1804-0462. doi:10.2478/pralin-2018-0002. URL [http://dx.doi.org/10.2478/pralin-2018-0002](http://dx.doi.org/10.2478/pralin-2018-0002).
* Prabhu et al. (2023) Ameya Prabhu, Zhipeng Cai, Puneet Dokania, Philip Torr, Vladlen Koltun, and Ozan Sener. 스토리지 제약 없이 온라인으로 계속 학습합니다. _ arXiv preprint arXiv:2305.09253_, 2023.
* Qin et al. (2023) Yujia Qin, Cheng Qian, Xu Han, Yankai Lin, Huadong Wang, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 연속 사전 훈련, 2023에 대해 재활용 가능한 튜닝 URL [https://arxiv.org/abs/2305.08702](https://arxiv.org/abs/2305.08702)입니다.
* Radford 등(2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 자연어 감독에서 전이 가능한 시각적 모델을 학습합니다. Marina Meila and Tong Zhang(eds.), _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pp. 8748-8763. PMLR, 2021. URL [http://proceedings.mlr.press/v139/radford21a.html](http://proceedings.mlr.press/v139/radford21a.html)
* Ramesh et al.(2019)Jack Wae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _ arXiv preprint arXiv:2112.11446_, 2021. URL [https://arxiv.org/abs/2112.11446](https://arxiv.org/abs/2112.11446).
* Raffel 등(2023) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 통일된 텍스트 대 텍스트 변환기로 전이 학습의 한계, 2023을 탐구합니다.
* Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 제로: 조 개의 파라미터 모델을 학습하기 위한 메모리 최적화. 크리스틴 퀴치, 아이린 퀄터스, 윌리엄 T. Kramer(eds.), _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event/ Atlanta, Georgia, USA, November 9-19, 2020_, pp. 20. IEEE/ACM, 2020.
* Ramasesh et al.(2022) Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. 신경망에서 스케일이 치명적인 망각에 미치는 영향 <제10차 국제학술대회>의 ICLR 2022, Virtual Event, 4월 25일~29일, 2022. OpenReview.net, 2022. URL [https://openreview.net/forum?id=GhV58_yPeEa](https://openreview.net/forum?id=GhV58_yPeEa)
* Reddi et al.(2021) Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and Hugh Brendan McMahan. 적응형 연합 최적화 2021년 _International Conference on Learning Representations_ 에서 URL [https://openreview.net/forum?id=LkFG31B13U5](https://openreview.net/forum?id=LkFG31B13U5).
* Riemer 등(2019) Matthew Riemer, Ignacio cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. 전이를 극대화하고 간섭을 최소화하여 잊지 않고 학습할 수 있습니다. 2019년 _International Conference on Learning Representations_ 에서 URL [https://openreview.net/forum?id=B1gTShAct7](https://openreview.net/forum?id=B1gTShAct7)입니다.
* Rolnick 등(2019) David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. 지속적인 학습을 위한 리플레이를 경험해 보세요. _Advances in Neural Information Processing Systems_, pp. 348-358, 2019. URL [https://arxiv.org/abs/1811.11682](https://arxiv.org/abs/1811.11682).
*Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 잠복 확산 모델을 이용한 고해상도 영상 합성 *IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 10674-10685. IEEE, 2022. URL [https://doi.org/10.1109/CVPR52688.2022.01042](https://doi.org/10.1109/CVPR52688.2022.01042).
* Ryabinin 등 (2021) Max Ryabinin, Eduard Gorbunov, Vesvold Plokhotnyuk, and Gennady Pekhimenko. Moshpit sgd: 이기종 신뢰할 수 없는 장치에 대한 통신 효율 분산 교육. In M. 란자토 A. 베이겔지머 Dauphin, P.S. Liang 및 J. Wortman Vaughan(eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 18195-18211. Curran Associates, Inc., 2021. URL [https://proceedings.neurips.cc/paper_files/paper/2021/file/97275a23ca44226c9964043c8462be96-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/97275a23ca44226c9964043c8462be96-Paper.pdf)
* Sakaguchi et al.(2019) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 위노그란데: 2019년 규모에서 적대적인 위노그라드 스키마 도전.
* Le Scao 등(2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _ arXiv preprint arXiv:2211.05100_, 2022. URL [https://arxiv.org/abs/2211.05100](https://arxiv.org/abs/2211.05100).
* Scialom et al. (2022) Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. 미세 조정된 언어 모델은 지속적인 학습자입니다. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 6107-6122, 2022.
* Sennrich et al.(2021) Rico Sennrich, Barry Haddow, and Alexandra Birch. 하위 단어 단위로 희귀 단어를 기계 번역합니다. In Katrin Erk and Noah A. Smith (eds.), _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1715-1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL [https://aclanthology.org/P16-1162](https://aclanthology.org/P16-1162).
* Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 메가트론-lm: 모델 병렬성을 사용하여 수십억 개의 매개 변수 언어 모델을 학습합니다. _ arXiv preprint arXiv:1909.08053_, 2019.
* Shoeybi et al. (2020) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 메가트론-lm: 모델 병렬성을 사용하여 수십억 개의 파라미터 언어 모델을 훈련, 2020.
* Smith et al.(2021) James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen, Hongxia Jin, and Zsolt Kira. 항상 꿈꿔라: 데이터 없는 수업 강화 학습을 위한 새로운 접근법. 2021년 10월 9374-9384호입니다
* Soboleva et al. (2023) Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: RedPajama의 627B 토큰이 청소되고 중복 제거된 버전입니다. [https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama] 2023년 6월 URL [https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B)입니다.
* Soldaini 등(2021) Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilsha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 돌마: 언어 모델 사전 훈련 연구를 위한 3조 토큰의 개방형 코퍼스입니다. _ CoRR_, abs/2402.00159, 2024. URL [https://doi.org/10.48550/arXiv.2402.00159](https://doi.org/10.48550/arXiv.2402.00159).
* Sun et al.(2020) Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 어니 2.0: 언어 이해를 위한 지속적인 사전 훈련 프레임워크. _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020_, pp. 8968-8975. AAAI Press, 2020. URL [https://doi.org/10.1609/aaai.v34i05.6428](https://doi.org/10.1609/aaai.v34i05.6428)
* Touvron 등(2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023a. URL [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971).
* 투브론 외 (2020) 휴고 투브론, 루이 마틴, 케빈 스톤, 피터 알버트, 암자드 알마하리, 야스민 바바이, 니콜라 바슬리코프, 수미 바트라, 크리스티안 바트라, 모야 첸, 기옌 쿠쿠룰, 다비드 에시오부, 주드 페르난데스, 제레미 푸, 웨닌 푸, 브라이언 풀러, 신시아 가오, 제레미 푸, 베다누즈 고스와미, 나만 고얄, 안토니아 가오, 사가하르 호세이니, 루이 호우, 하칸 이난, 마르신 카다스, 빅토르 케르케즈, 마디안 하바사, 이사벨 클라우만, 아르템 고레네프, 신시아 가오, 사그하르 호세이니, 제냐 리, 디아나 리스코비치, 잉하이 루, 윤잉 마오, 사보트 라흐로프, 제냐 라흐라, 제냐 라흐로프, 푸시카 니에, 앤드류 푸틀론, 제레미 라이젠슈타인, 라시룽타, 칼 라마 2: 오픈 파운데이션과 미세 조정된 채팅 모델입니다. _ CoRR_, abs/2307.09288, 2023b. URL [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288).
* Veniat et al.(2021) Tom Veniat, Ludovic Denoyer, and MarcAurelio Ranzato. 모듈식 네트워크 및 작업 기반 전과를 통한 효율적인 연속 학습 2021년 _International Conference on Learning Representations_ 에서 URL [https://openreview.net/forum?id=EKV158tSfwv](https://openreview.net/forum?id=EKV158tSfwv).
* Wu et al.(2024) Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, and Ying Shan. 라마 프로: 블록 확장이 있는 점진적인 라마입니다. _ CoRR_, abs/2401.02415, 2024. URL [https://doi.org/10.48550/arXiv.2401.02415](https://doi.org/10.48550/arXiv.2401.02415).
* Wu et al.(2020)* Xie et al.(2023) Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. 도메인별 대규모 언어 모델 구축을 위한 효율적인 연속 사전 훈련, 2023. URL [https://arxiv.org/abs/2311.08545](https://arxiv.org/abs/2311.08545)입니다.
*Xiong et al. (2020) Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020년 변압기 아키텍처에서 계층 정규화.
*Yadav 등(2023) Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Parminder Bhatia, Xiaofei Ma, Ramesh Nallapati, Murali Krishna Ramanathan, Mohit Bansal, and Bing Xiang. 코드 생성 모델을 위한 지속적인 학습 탐색 안나 로저스, 조던 L. Boyd-Graber, and Naoaki Okazaki(eds.), _Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023_, pp. 782-792. Association for Computational Linguistics, 2023a. URL [https://doi.org/10.18653/v1/2023.acl-short.68](https://doi.org/10.18653/v1/2023.acl-short.68).
*16, 2023_, 2023b. URL [http://papers.nips.cc/paper_files/paper/2023/hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html)
* Yang et al.(2022) Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, David Farhi, Jakub Pachocki, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao. 텐서 프로그램 v: 제로 샷 하이퍼-파라미터 전송을 통해 대형 신경망을 튜닝한다. *NeurIPS 2021_, 2022년 3월. URL [https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/](https://www.microsoft.com/en-us/research/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/)
* Yang et al.(2024) Xianjun Yang, Junfeng Gao, Wenxin Xue, and Erik Alexandersson. PLMA: 식물 과학을 위한 오픈 소스 대규모 언어 모델입니다. _ CoRR_, abs/2401.01600, 2024. URL [https://doi.org/10.48550/arXiv.2401.01600](https://doi.org/10.48550/arXiv.2401.01600).
* You et al.(2019) Kaichao You, Mingsheng Long, Jianmin Wang, and Michael I. Jordan. 학습률 감소가 현대 신경망에 어떻게 도움이 됩니까? 2019년.
* Zan et al.(2022) Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-Guang Lou. CERT: 라이브러리 지향 코드 생성을 위한 스케치에 대한 지속적인 사전 훈련. Luc De Raedt (ed.), _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022_, pp. 2369-2375. ijcai.org, 2022. URL [https://doi.org/10.24963/ijcai.2022/329](https://doi.org/10.24963/ijcai.2022/329).
* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yoatan Bisk, Ali Farhadi, and Yejin Choi. 헬라스와그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? 2019년.
* Zhai et al.(2022) Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 스케일링 비전 트랜스포머. *IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 1204-1213. IEEE, 2022. URL [https://doi.org/10.1109/CVPR52688.2022.01179](https://doi.org/10.1109/CVPR52688.2022.01179).
* Zhao 등(2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _ arXiv preprint arXiv:2303.18223_, 2023. URL [https://arxiv.org/abs/2303.18223](https://arxiv.org/abs/2303.18223).

###### Contents

* 1 소개
* 2 Main findings and Takeaways
* 3 관련 작업
	* 3.1 연속 학습
	* 3.2 사전 훈련, 모델 척도 및 연속 학습
	* 3.3 도메인 적응형 연속 사전 훈련(DACPT)
	* 3.4 특정 도메인에 적용된 LMs에 대한 연속 학습
	* 3.5 학습률 스케줄
*4 배경기술 및 방법론
	* 4.1 Linear Warmup and Cosine Decay Schedule
	* 4.2 Compute-equivalent Replay
* 5 실험 설정
	* 5.1 Datasets
	* 5.2 연속 학습 설정
	* 5.3 훈련 설정
	* 5.4 독일어 및 영어 LM 평가 벤치마크
* 6 결과
	* 6.1 학습률 일정
		* 6.1.1 약 및 강 분포 이동에 대한 선형 워밍업의 효과.
		* 6.1.2 Weak and Strong Distribution Shift에 대한 재-온난화, 재-붕괴 및 변화하는 \(\eta_{max}\)의 효과입니다.
	* 6.2 재생의 효과
	* 6.3 약하고 강한 분포 이동에 대한 연속 사전 훈련 최종 성능입니다.
		* 6.3.1 손실로 평가된 최종 성능
		* 6.3.2 인기 LM 벤치마크에서 Zero-shot 및 Few-shot 결과에 의해 평가된 최종 성능
	* 6.4 모델 척도에 따른 연속 사전 훈련 최종 성능
		* 6.4.1 손실로 평가된 최종 성능
		* 6.4.2 인기 LM 벤치마크에서 Zero-shot 및 Few-shot 결과에 의해 평가된 최종 성능
*7 재-온난화의 병리학 이해 및 회피
	* 7.1 동일 데이터에 대한 재-온난화
* 8개의 팽창* 7.2 무한 학습 속도 스케줄
	* 7.3 무한스케줄의 변이에 대한 코사인 감쇠 비교
	* 7.4 무한 학습률 스케쥴: 무한 미래 업데이트로 스케일링
* 8 제한 사항
* 9 결론
* 확장된 결과
* A.1 도메인 증분 연속 사전 훈련
* A.2 모델 병합 v.s. 연속 사전 훈련
* 다른 데이터 세트 크기에 대 한 A.3 다시 보기
* A.4 독일 모델의 정성적 평가
* A.5 집계된 LM 평가 결과
* A.6 집계 평균 최종 정확도
* B 모델 하이퍼파라미터

## 부록 A 확장 결과

다음 하위 섹션에서는 먼저 _도메인 증분 연속 사전 훈련_ 설정에서 몇 가지 새로운 결과를 제시하고 다양한 데이터 세트 크기에 대한 다시 보기를 비교하고 독일 언어 모델에 대한 정성적 분석을 제공한다. 또한 논문의 모든 모델에 대한 집계된 평가 및 최종 손실 표를 제공한다.

### 도메인 증분 연속 사전 훈련

우리는 **도메인 증분 학습** 설정을 고려하며, 여기서 각각 별개의 도메인에서 오는 \(N\) 미래 데이터 세트 \(\{\mathcal{D}_{0},\mathcal{D}_{1},\dots,\mathcal{D}_{N-1}\}\)의 시퀀스에 대해 훈련합니다. 이 설정은 각 도메인 간의 전환 시 분포 전환 경험으로 인해 특히 어렵다. 구체적으로, \(\mathcal{D}_{0}\)는 Pile (Gao et al., 2020)에서 사전 학습이고 \(\mathcal{D}_{1}\)는 SlimPajama에서 사전 학습이라고 가정한다. 그러나 표 1의 샘플링 백분율을 사용하여 \(\mathcal{D}_{1}\)에서 데이터를 소비하는 대신 가장 큰 도메인에서 가장 작은 도메인으로 시작하여 한 번에 하나의 데이터 세트를 처리한다. 이것은 다른 시간에 다른 도메인에서 새로운 데이터가 수신되는 상황을 시뮬레이션하고 모든 분포 이동에 강인하면서 시퀀스에 대한 모델을 업데이트하고자 한다.

리플레이를 도메인 증분 연속 사전 훈련에 적응 두 개 이상의 작업에 걸쳐 있는 설정에서 리플레이 버퍼가 이산 간격으로 업데이트되는 저장소 샘플링 형태(Buzzega 등, 2020)를 사용한다. 이 기술을 _이산 저장소 샘플링_ 이라고 합니다. 구체적으로, \(N\) 데이터세트 \(\mathcal{D}_{0},\mathcal{D}_{1},\dots,\mathcal{D}_{N-1}\)의 크기 \(s_{0},s_{1},\dots,s_{N-1}\)의 시퀀스가 주어진 경우, 각 데이터세트 전이에서 재생 버퍼 \(\mathcal{R}\)를 갱신한다. \(\mathcal{R}_{i}\)는 제 \(i\) 데이터 세트에 대한 트레이닝의 시작으로서 재생 버퍼의 상태에 대응된다. 리플레이 비율 \(0\leq\alpha\leq 1\)의 경우, 임의의 주어진 \(i>0\)에서, \(\mathcal{R}_{i}\)는 비율들로 \(j<i\)에 대한 모든 \(\mathcal{D}_{j}\)로부터의 데이터를 포함할 것이다.

\[p_{i,j}:=\frac{s_{j}\cdot(1-\alpha)^{\gamma_{j}}+\sum_{k=j+1}^{i-1}p_{k,j} \cdot s_{k}\cdot\alpha}{\sum_{k=0}^{i-1}s_{k}}\quad\text{with}\quad\forall j, \ p_{j+1,j}=1, \tag{5}\]

여기서, \(i\)는 현재 사전 훈련 중인 데이터 세트의 인덱스이고, \(j=0\이면 \(\gamma_{j}=0\), 그렇지 않으면 \(1\)이다. 후자는 \(\mathcal{D}_{i}\)에 대한 사전 훈련 시, 첫 번째 데이터세트 \(\mathcal{D}_{0}\)를 제외하고 계산 등가 리플레이를 사용하기 때문에 \(\mathcal{D}_{i}\)의 \(s_{i}\cdot(1-\alpha)\) 토큰만 볼 수 있고, 따라서 모든 \(s_{0}\) 토큰은 \(\mathcal{D}_{0}\)에서 볼 수 있기 때문이다.

그림 10: **도메인별로 데이터를 순차적으로 수집** 합니다. 우리는 사용 가능한 데이터를 소비하는 대안적인 접근법을 탐구한다. 과제 증가 또는 수업 증가 학습과 유사하게, 우리는 순차적인 방식으로 한 번에 한 도메인에서 훈련한다. 각 도메인에 대해 LR 재온난화 및 LR 재붕괴를 사용하고 _개별 저장소 샘플링_ 을 사용합니다.**

[MISSING_PAGE_FAIL:35]

### 독일 모델의 정성적 평가

이 섹션에서는 독일 커먼 크롤(Sec. 6.3)에서 훈련된 모델에 대한 간략한 정성적 평가를 제공한다. 우리는 독일어의 다양한 특수성이 포함된 5개의 독일 프롬프트를 선택한다(탭 8 참조). 그런 다음 독일 커먼 크롤에서 훈련되거나 지속적으로 사전 훈련된 각 모델에 대해 고정된 토큰 길이 응답을 생성한다. 기준선으로서, 우리는 또한 파일에서만 훈련된 동일한 모델을 평가한다.

우리는 표 9에서 응답과 응답의 수동 번역을 제공한다. 비교적 작은 405M 매개변수 모델은 일반적으로 의미 있는 문장을 생성하지 않지만, 독일 커먼 크롤에서 훈련된 모델은 일반적으로 문법적으로 올바른 문장을 생성한다. 여기에는 올바른 상단 및 하단 케이싱, 복합어 생성, 언라우트 문자 사용 등이 포함된다. 모든 경우에, 응답은 몇 단어 후에 반복되는 경향이 있다. 일부 출력의 불완전성과 분수어는 생성된 토큰의 수가 매우 짧기 때문이라고 볼 수 있다. 더 긴 시퀀스는 구문적으로 정확한 문장을 제공하는 경향이 있지만 스스로 반복하는 경향이 있다. 샘플의 제한된 수와 생성된 텍스트의 일반적으로 낮은 품질은 독일 커먼 크롤에서 훈련된 개별 모델 간의 질적 차이에 대한 강력한 진술을 허용하지 않는다. 그러나 앞서 언급한 반복성 문제와 문법적 오류는 파일 기준선에서 훨씬 더 강한 것으로 보이며, 이는 일반적으로 프롬프트에서 주어진 맥락을 존중하지 못한다. 이는 파일에서 독일어 텍스트의 양이 현저히 적어서 예상된다.

\begin{table}
\begin{tabular}{l l l l l l l l l l} \hline \hline Model Size & Merging Method & Base Model & Task Model(s) & TopK\% & JMMU DE & ARC-C DE & Helle-Sung DE & TruthfulQA DE & AVG \\ \hline \multirow{4}{*}{403M} & \multirow{4}{*}{TIES} & \multirow{4}{*}{300B Pile} & 25 & 2286 & 20.65 & 24.78 & 21.05 & 22.33 \\  & & & 200B Ger. & 75 & 24.38 & 19.54 & 28.67 & 25.83 & 24.60 \\  & & & 50 & 23.47 & 21.08 & 25.64 & 23.50 & 23.42 \\ \cline{3-10}  & & & & 25 & 23.89 & 19.45 & 26.76 & 25.70 & 23.95 \\ \cline{3-10}  & & & 300B Pile \(\rightarrow\) 200B Ger. (25\% Replay) & 50 & 25.12 & 18.86 & 29.45 & 26.32 & 29.94 \\ \cline{3-10}  & & & 75 & 23.91 & 18.94 & 30.81 & 24.09 & 24.57 \\ \cline{3-10}  & & & 200B Pile \(\rightarrow\) 200B Ger. (25\% Replay) & **23.78** & **19.20** & **23.04** & **25.58** & **24.90** \\ \hline \hline \end{tabular}

* TIGA: Truthful QA, WG: WinGrande, NO: Natural Questions, OBQA: OpenBack QA, TIGA:TriviaQA

\end{table}
표 7: **TIES v.s와 병합된 모델의 독일 LM 평가 성능. 연속 사전 훈련**. HellaSwag에 대해 정규화된 정확도가 보고되고 NaturalQuestions 및 TriviaQA에 대해 정확한 일치(EM)가 보고된다. 다른 모든 작업은 정규화되지 않은 정확도를 보고합니다. MMLU 및 TriviaQA는 5-shot으로 평가되지만 다른 모든 작업은 0-shot입니다.**

그림 11: **학습률을 다시 온난화하고 다양한 양의 데이터에 대해 지속적으로 사전 훈련할 때 v.s.를 다시 재생하지 않음** 슬림파자마 데이터의 100B, 200B 및 300B 토큰에 대해 5% 재생이 있거나 없는 405M 매개변수 모델을 학습한다. 각 모델은 파일 300B 토큰에서 미리 훈련된 체크포인트에서 시작하여 데이터 세트 크기에 맞는 선형 웜업 및 코사인 붕괴 일정을 사용하여 학습 속도를 다시 워밍업합니다.**

[MISSING_PAGE_EMPTY:37]

### 집계된 LM 평가 결과

다음과 같은 영어 및 독일어 LM 평가 작업에 대해 모델을 평가합니다.

1. **HellaSwag**(Zellers et al., 2019) 및 **HellaSwag DE**: 언어 모델을 혼동하도록 의도적으로 설계된 객관식 질문으로 구성된 영어 상식 추론 벤치마크입니다. 헬라 스웨그 DE는 헬라 스웨그 벤치마크의 독일어 번역본이다.
2. **AI2 추론 챌린지(ARC)**(Clark et al., 2018): 객관식 형식의 과학 시험 문항으로 구성된 영어 상식 추론 벤치마크. 원(7,787\)의 총 문항은 원(5,197\)의 문항을 가진 쉬운 부분집합과 원(2,590\)의 문항을 가진 어려운 부분집합으로 나누어졌다. ARC-c DE는 질문의 도전 하위 집합을 독일어로 번역한 것이다.
3. **BoolQ**(Clark et al., 2019): \(15,942\) yes/no question-answering 샘플로 구성된 영어 독해 벤치마크. 각 예는 질문, 관련 단락 및 솔루션으로 나뉜다.
4. **MathQA**(Amini et al., 2019): 수학의 다양한 영역에 걸쳐 객관식 질문으로 구성된 영어 수학 단어 문제 벤치마크.
5. **MMLU**(Hendrycks et al., 2021) 및 **MMLU-DE**: 테스트 중인 모델의 일반 지식과 즉석 문제 해결을 모두 평가하기 위해 제로 샷 시나리오와 소수 샷 시나리오를 모두 평가하도록 설계된 영어 벤치마크입니다. MMLU는 광범위한 주제를 다룬다. MLU-DE는 OpenAI GPT 3.5 API에 의해 번역된 MMLU 질문 세트를 독일어로 번역한 것이다.
6. **OpenBookQA(OBQA)**(Mihaylov et al., 2018): 특정 주제에 대한 인간의 이해를 평가하기 위해 실제 개방형 도서 시험을 모델링한 영어 질문 답변 벤치마크. 초등 과학에 대한 질문은 과학적 사실과 공통 지식으로 짝지어지며, 이 모델은 다중 홉 추론에 사용하고자 한다.
7. **PIQA**(Bisk et al., 2019): 모델의 물리적 상식 추론 능력을 테스트하기 위해 설계된 영어 질문 답변 벤치마크입니다. 대부분의 질문은 일상적인 상황에 흔하지 않은 솔루션을 적용하는 데 초점을 맞추며, 이는 물리적 세계에 대한 이해가 필요하다.
8. **WinoGrande**(Sakaguchi et al., 2019): 텍스트 내의 둘 이상의 표현이 동일한 엔티티를 지칭할 때를 결정하는 것을 포함하는 영어 자연어 이해 벤치마크. 벤치마크에는 다양한 문장 집합과 인간과 같은 예측을 하는 모델에 보상을 제공하는 새로운 평가 메트릭이 포함된다.
9. **TruthfulQA** 및 **TruthfulQA DE**(Lin 등, 2022): 질문에 대한 생성된 답변의 진실성을 평가하기 위해 설계된 영어 질문 답변 벤치마크입니다. 질문은 오답으로 이어지는 인간의 공통된 오해를 담도록 설계되어 있다. TruthfulQA DE는 TruthfulQA 벤치마크를 독일어로 번역한 것이다.
10. **Natural Questions**(Kwiatkowski et al., 2019): 구글 검색 엔진에 제출된 검색 쿼리로 구성된 영어 질의 응답 벤치마크입니다.
11. **TriviaQA**(Joshi et al., 2017): 퀴즈 애호가들이 제공하는 질문-응답 쌍들로 구성된 영어 질문-응답 벤치마크이다. 모델의 일반적인 세계 지식을 결정하는 것이 주요 초점입니다.

[MISSING_PAGE_EMPTY:39]

[MISSING_PAGE_EMPTY:40]

[MISSING_PAGE_EMPTY:41]

\begin{table}
\begin{tabular}{l l} \hline \hline Description & Value \\ \hline
**10B Model- Cosine Schedule** & \\ Max learning rate (\(\eta_{max}\)) & \(1.2\cdot 10^{-4}\) \\ Min learning rate (\(\eta_{min}\)) & \(1.2\cdot 10^{-5}\) \\ Warmup percent (\(T_{warmup}\)) & 1 \\ \hline
**405M Model - Cosine Schedule** & \\ Max learning rate (\(\eta_{max}\)) & \(3\cdot 10^{-4}\) \\ Min learning rate (\(\eta_{min}\)) & \(3\cdot 10^{-5}\) \\ Warmup percent (\(T_{warmup}\)) & 1 \\ \hline
**405M Model - Infinite LR** & **Schedule Common** \\ Max learning rate (\(\eta_{max}\)) & \(3\cdot 10^{-4}\) \\ Min learning rate (\(\eta_{min}\)) & \(3\cdot 10^{-5}\) \\ Constant learning rate (\(\eta_{const}\)) & \(1.65\cdot 10^{-4}\) \\ Warmup percent (\(T_{warmup}\)) & 1 \\ Cooldown tiers percent (\(T_{cd}\)) & 60 \\ Constant tiers percent (\(T_{ann}\)) & 25 \\ \hline
**Inverse Square root cooldown schedule** & \\ Timescale (\(\alpha\)) & 10 \\ \hline \hline \end{tabular}
\end{table}
표 14: **LR 일정의 하이퍼 매개 변수** 텍스트에 달리 지정되지 않는 한 이러한 값을 사용합니다.

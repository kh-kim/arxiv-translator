# 2403.08763 Simple and Scalable Strategies to Continually Pre-train Large Language Models

[ArXiv Version](https://arxiv.org/abs/2403.08763)

Arxiv HTML version may not be shown if they don't provide HTML version.

[Arxiv HTML English Version](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2403.08763/paper.raw.en.html)

[Arxiv HTML Korean Version](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2403.08763/paper.raw.ko.html)

You can also access Ar5iv version if they provide.
Note that Ar5iv update their DB at the end of the month.
Thus, Ar5iv version would not be available, if it was added before Ar5iv's update.

[Ar5iv Original](https://ar5iv.org/abs/2403.08763)

[Ar5iv HTML English Version](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2403.08763/paper.ar5iv.en.html)

[Ar5iv HTML Korean Version](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2403.08763/paper.ar5iv.ko.html)

Below is naive version using Nougat OCR, which only contains text, including table.

[English Version](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2403.08763/paper.en.html)

[Korean Version](https://raw.githack.com/kh-kim/arxiv-translator/master/papers/2403.08763/paper.ko.html)
<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# A Tale of Tails: Model Collapse as a Change of Scaling Laws\n' +
      '\n' +
      'Elvis Dohmatob\n' +
      '\n' +
      'Yunzhen Feng\n' +
      '\n' +
      'Pu Yang\n' +
      '\n' +
      'Francois Charton\n' +
      '\n' +
      'Julia Kempe\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'As AI model size grows, neural _scaling laws_ have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: _How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus?_ Will future models, still improve, or be doomed to degenerate up to total _(model) collapse_? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the "un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation using the large language model Llama2.\n' +
      '\n' +
      'Machine Learning, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Groundbreaking advances in generative AI algorithms for text, images and code are ushering in the "synthetic data age": increasingly we consume data generated by large scale models like GPT4 (Achiam et al., 2023), Stable Diffusion (Rombach et al., 2022) and their successors. At the same time a key driver behind the current success of large models is their consumption of massive amount of web-scale data for training. The improvements of larger models are governed by scaling laws in which error falls off as a power in the size of training data; and the emergence of new skills seems tightly linked to covering increased scales of training data. Our understanding of what the future holds in a world were models are trained on other models (or their own) synthesized data is only at its beginning, but some works indicate the possibility of complete collapse of learning, so called model collapse1.\n' +
      '\n' +
      'Footnote 1: Not to be confused with neural collapse which refers to clustering of last-layer features at the end of training (Papyan et al., 2020)\n' +
      '\n' +
      'Scaling Laws.In many domains of machine learning including speech, translation, vision, video, and mathematical problem solving, empirically observed neural scaling laws (Hestness et al., 2017; Rosenfeld et al., 2020; Kaplan et al., 2020; Hoffmann et al., 2022; Gordon et al., 2021; Henighan et al., 2021; Aghajanyan et al., 2023) demonstrate that test error often falls off as a power law with the amount of training data, model size, and compute. Theoretically, scaling laws have been derived in a variety of settings (e.g. Hutter (2021); Cabannes et al. (2023) for "LLM-like" models).\n' +
      '\n' +
      'Scaling laws are intimately related to the emergence of abilities (Wei et al., 2022) in larger models, that are not present in smaller ones; and to skills that appear with decreasing loss (Gordon et al., 2021; Arora & Goyal, 2023). This bolsters the now common narrative that "scaling is all you need".\n' +
      '\n' +
      'Model Collapse.Current LLMs (Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020; Touvron et al., 2023), in\n' +
      '\n' +
      'Figure 1: (Cartoon). Top-p (nucleus) sampling, temperature scaling of LLM generation, and finite sample bias lead to truncated or narrowed “tails” (left side), causing loss of scaling laws (top right) and loss of skills (bottom right). Here we visualize calculating the greatest common divisor (GCD) with answer 2 and 3 as two skills.\n' +
      '\n' +
      'cluding GPT-4 (Achiam et al., 2023), were trained on predominantly human-generated text; similarly, diffusion models like DALL-E (Ramesh et al., 2021), Stable Diffusion (Rombach et al., 2022), Midjourney (Midjourney, 2023) are trained on web-scale image datasets. These training corpora already potentially exhaust all the available clean data on the internet. A growing number of synthetic data generated with these increasingly popular models starts to populate the web, often indistinguishable from "real" data. We have thus already raced into the future where our training corpora are irreversibly mixed with synthetic data and this situation stands to get worse. Recent works call attention to the potential dramatic deterioration in the resulting models, an effect referred to as _"model collapse"_(Shumailov et al., 2023). Facets of this phenomenon have been demonstrated _empirically_ in various settings (Hataya et al., 2023; Martinez et al., 2023; Bohacek and Farid, 2023; Briesch et al., 2023; Guo et al., 2023). Theoretically, a few works are emerging to analyze the effect of iterative training on self-generated (or mixed) data (see Related Work): (Shumailov et al., 2023) coin the term _"model collapse"_ to characterize complete reversion to the mean, Alemohammad et al. (2023) analyze _"self-consuming loops"_ and Bertrand et al. (2023) show that iterative synthetic training leads to a _"clueless generator"_.\n' +
      '\n' +
      'With these first warning signs in place, we thus ask:\n' +
      '\n' +
      '_How is the current scaling paradigm affected by synthetic data in the training corpus?_\n' +
      '\n' +
      'To this end, we carefully zoom into the scaling behavior of LLM-style models. Theoretical derivations of scaling laws always assume a heavy-tailed distribution (power-law, aka Zipf) on the input features ("heavy tail in, power scaling law out"). This distribution is of the form\n' +
      '\n' +
      '\\[p_{i}\\propto i^{-\\beta},\\ \\ i=1,2,\\ldots \\tag{1}\\]\n' +
      '\n' +
      'Such distributions are ubiquitous in natural datasets, from Zipf\'s law (Zipf, 1935) in distribution of word frequencies, to biological data, earthquake magnitudes, financial data etc. - this is the data being consumed by large models at scale, like LLMs. But what distribution do AI-models generate when trained on such data? Figure 2 provides an empirical answer for a large scale LLM (Llama2-7B) and a transformer model trained on an arithmetic task. Regenerating heavy-tailed data affects the distribution in two possible ways: (1) "Cutting off" the tail of the distribution and/or (2) "Narrowing" the tail (see Figure 1 for a cartoon illustration). The mechanisms leading to this, apart from finite sampling bias (as already proposed in Shumailov et al. (2023) - see Section 2 for a derivation in the Zipf-setting), stem from deliberate choices in the generation algorithms of the models: in LLMs via truncated next token prediction at inference (e.g. selecting more likely tokens via _top-_ or _top-_\\(k\\)_ truncation, concentrating the probability distribution by lowering the temperature); in vision models like GANs via truncation or in diffusion models through guidance.\n' +
      '\n' +
      '**Summary of Main Contributions.** We present a high-level summary of our main theoretical contributions, some of which are highlighted in Figure 3. We empirically verify these theoretical predictions (see Figure 4): (1) in large-scale experiments on an LLM, fine-tuning Llama2-7B (Touvron et al., 2023) on an approximately \\(2M\\) sample dataset from Wikitext-103 and (2) for transformer models trained to predict the greatest common divisor (Charton, 2023).\n' +
      '\n' +
      'Assuming a true distribution as in Equation (1), consider training a model on a dataset of size \\(T\\) of AI data-generated data. The synthesized data amounts to a version of the true data distribution with the tail cut at some finite rank \\(k\\) or the tail narrowed to a smaller exponent. Our main findings are as follows.\n' +
      '\n' +
      '_(1) A Double Scaling Law._ We establish new scaling laws that explain model collapse in simplified (non-autoregressive) LM (Hutter, 2021) and toy bigram LLMs\n' +
      '\n' +
      'Figure 2: **Tails of AI-generated data: Top.** Perplexity diagram of the Wikitext-103 test set, measured with Llama2–7B as the anchor model. We query the Wikitext-finetuned Llama2–7B to generate AI data, which is compared to the original set. Perplexity is calculated solely for the generated positions in both the AI and original datasets. AI data is generated for various settings of \\((p,T)\\). **Bottom.** Distribution of greatest common divisors (GCD) of pairs of random integers (original data (blue) scaling as \\(p(GCD=k)\\propto k^{-2}\\)). A transformer is trained on this task on \\(300M\\) samples and used as a generator on a test set of randomly sampled integer pairs, giving the truncated GCD distribution.\n' +
      '\n' +
      '(refer to Theorems 2.1 and 4.2)2\n' +
      '\n' +
      'Footnote 2: The notation \\(f(T)\\lesssim g(T)\\) means that \\(f(T)\\leq Cg(T)\\) for sufficiently large \\(T\\) and an absolute constant \\(C\\), while \\(f(T)\\asymp g(T)\\) means \\(f(T)\\lesssim g(T)\\lesssim f(T)\\).\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-c}+k^{-c^{\\prime}}. \\tag{2}\\]\n' +
      '\n' +
      'or equivalently (refer to Corollary 2.2), for finite-sample induced cut-off \\(k=k(T_{0})\\) when the generating model is trained on \\(T_{0}\\) amount of data, \\(E_{test}\\asymp T^{-c}+T_{0}^{-c^{\\prime\\prime}}\\), where the exponents \\(c,c^{\\prime},c^{\\prime\\prime}\\) only depend on the tail behavior of the true distribution. This result is illustrated in Figure 3.\n' +
      '\n' +
      'For AI-"tail-narrowing", when data remains heavy-tailed, with a smaller exponent \\(\\beta^{\\prime}\\in(1,\\beta)\\), the downstream Hutter LLM will scale as (Corollary 2.3)\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-(\\beta-1)/\\beta^{\\prime}}. \\tag{3}\\]\n' +
      '\n' +
      '_(2) A Triplet Scaling Law for Memory-Limited Models._ We consider a simple associative memory model studied in (Cabannes et al., 2023), and establish (Theorem 5.1) a new scaling law of the form\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-c}+d^{-c_{q}}+k^{-c^{\\prime}}, \\tag{4}\\]\n' +
      '\n' +
      'where \\(d\\) is the embedding dimension, and serves a s proxy for model capacity; the exponent \\(c_{q}\\) depends both on \\(\\beta\\) and the particular algorithm \\(q\\) used to update the memories in the model during training.\n' +
      '\n' +
      '_(3) Model Collapse over Multiple Generations._ For \\(n\\)-fold recursion of AI data-generation (11), where each generation of the model consumes data produced by the previous generation, we establish a universality principle of the form\n' +
      '\n' +
      '\\[E_{test}=E_{test}^{clean}+n\\times\\text{ new scaling terms}, \\tag{5}\\]\n' +
      '\n' +
      'where \\(E_{test}^{clean}\\) is the usual test error of the model trained on clean data (not AI-generated). This means that in Equations (2) and (4) for example, the \\(k^{-c^{\\prime}}\\) is replaced by \\(nk^{-c^{\\prime}}\\). One possible interpretation of this multiplicative degradation is that, over time (i.e as the number of generations becomes large), the effect of large language models (like ChatGPT) in the wild will be a pollution of the web to the extend that learning will be impossible. This will likely increase the value and cost of clean / non-AI-generated data.\n' +
      '\n' +
      '_(4) Mitigation Strategies._ In Theorem 3.2 we show that mixing AI-generated data with even a small amount of clean data mitigates model collapse by introducing a grokking phenomenon. The length of the plateau is of order \\(k^{\\beta}/\\pi\\), where \\(\\pi\\) is the proportion of training data which is from the true distribution (i.e clean data). When \\(\\pi=0\\) (i.e only AI-generated data available), this plateau goes on forever (as in (2) and (4)). When \\(\\pi>0\\), however small, the plateau finally halts, and the error continues to decrease a la \\(T^{-c}\\). This grokking phenomenon holds in the setting of _deterministic_ ground truth labels (like in the models of Hutter (2021); Cabannes et al. (2023)). For transformer models, such deterministic settings are found for instance in arithmetic tasks, and we demonstrate it empirically in our GCD transformer experiments. The grokking effect becomes attenuated in probabilistic settings, where it can lead to an S-shaped learning curve (see Figure 19). We also identify regimes where adding AI data can be beneficial and discuss ways to curate "tail" data to mitigate AI-data effects.\n' +
      '\n' +
      'Related Work.Theoretically, scaling laws have been derived in various settings: for non-parametric models (Schmidt-Hieber, 2017; Suzuki, 2019; Bordelon et al., 2020), in the kernel regime under the Gaussian design (Spigler et al., 2020; Cui et al., 2021, 2022; Maloney et al., 2022), or in memorization-like settings with discrete data (Hutter, 2021; Debowski, 2023; Michaud et al., 2023). Taking finite model capacity and optimization into account, Cabannes et al. (2023) recently proved scaling laws in constraint-capacity associative memories, and our Triplet Scaling Law builds on this work.\n' +
      '\n' +
      'Less than a handful of works begin to provide theoretical explanations for the behavior of models in the "synthetic data age". (Shumailov et al., 2023) attribute model collapse to two mechanisms: a finite sampling bias cutting off low-probability "tails", thus leading to more and more peaked distributions and function approximation errors; they theoretically analyze the (single) Gaussian case and provide empirical evidence for VAEs, Gaussian mixtures and the OPT language model (125M parameters). In the context of vision models, Alemohammad et al. (2023) analyze "_self-consuming loops"_ by introducing a sampling bias that narrows the variance of the data at each generation, and, in addition to empirical demonstration on GANs and denoising diffusion probabilistic models, provide theoretical analysis for the Gaussian model. Finally, let us mention the study of Bertrand et al. (2023) which sheds light on the critical role of data composition in the stability and effectiveness in generative models, applicable to VAEs (Kingma & Welling, 2014), diffusion models and normalizing flows. They explore scenarios involving a mix of clean data, representative of the true distribution, and synthesized data from previous iterations of the generator. Their analysis reveals that if the data mix consists exclusively of synthesized data, the generative process is likely to degenerate over time (_"clueless generator"_). Using fixed-point analysis across iterations, they find that when the proportion of clean data in the mix is sufficiently high, the generator, under certain technical conditions, retains the capability to learn. A recent paper (Fan et al., 2023) empirically observe deteriorated scaling lawswhen training on synthetic data for text-to-image models. 3\n' +
      '\n' +
      'Footnote 3: A more detailed description of related and prior work can be found in Appendix A\n' +
      '\n' +
      'To our knowledge, our work is the first to theoretically and empirically analyze model collapse in the context of scaling laws and emergent abilities to provide a rich new landscape of AI-data induced phenomena.\n' +
      '\n' +
      '## 2 A Deterministic Infinite Memory Model\n' +
      '\n' +
      'Here, we present the core of our theory for the simplest case of (i) _infinite memory_ and (ii) _a deterministic ground truth_ labeling function \\(i\\mapsto y_{i}\\), studied by Hutter (2021) (the _"Hutter LLM"_). Both restrictions will be lifted in later sections, where we also analyze an _probabilistic autoregressive_ version (Section 4) and _limited memory_ models (Section 5). Token \\(i\\) is drawn according to the Zipf law in Equation (1), which e.g. models distribution of various metrics in language. Another interpretation of the appearance of a power-law is offered by the "Quantization Hypothesis" paper of Michaud et al. (2023): one may think of each \\(i\\) as some discrete skill, needed to solve a problem for example; thus, the skills occur at different rates \\(p_{i}\\). The shape parameter \\(\\beta>1\\) controls the length of the tail of this distribution: bigger values of \\(\\beta\\) correspond to longer tails.\n' +
      '\n' +
      '### What Causes Model Collapse?\n' +
      '\n' +
      'Tail Cutting.As mentioned, deliberate choices in the AI generation algorithm (like top-\\(p\\) or top-\\(k\\) next token prediction) immediately lead to a chopped tail at \\(k\\). When viewed as skills, we can say that only the \\(k\\)th most frequent outcomes ("skills") are considered. But even when no tails are cut deliberately, the finite size \\(T_{0}\\) of the training set (sampling bias) induces an effective tail-cutting. This can be seen as follows: Sample an iid dataset of size \\(T_{0}\\), and estimate the histogram \\(p_{\\text{AI}}\\); this new distribution plays the role of an AI data-generator. An integer \\(i\\) appears in the support of \\(p_{\\text{AI}}\\) a number of times which is \\(T_{0}p_{i}\\) on average. Roughly speaking4, this means that the support of \\(p_{\\text{AI}}\\) is \\(\\{i\\mid p_{i}\\leq C/T_{0}\\}=\\{i\\mid i\\leq k\\}\\), where\n' +
      '\n' +
      'Footnote 4: This can be made rigorous via standard concentration arguments.\n' +
      '\n' +
      '\\[k=k(T_{0})\\asymp T_{0}^{1/\\beta}. \\tag{6}\\]\n' +
      '\n' +
      'Therefore, the transformation \\(p\\to p_{\\text{AI}}\\) amounts to chopping off the tail of \\(p\\) at rank \\(k\\), where \\(k\\) is as given above.\n' +
      '\n' +
      'Tail Narrowing.Figure 2 (for Llama2) shows that in addition to tail cutting, tail narrowing effects happen during AI-generation. One mechanism for this is lowered temperature during next-token prediction. Assume a softmax distribution on the logits \\(z_{i}\\) for the \\(i\\)th token: \\(p_{i}=e^{z_{i}}/\\sum_{j}e^{z_{j}}\\). Define \\(q_{i}^{T}=e^{z_{i}/T}/\\sum_{j}e^{z_{j}/T}\\) for general temperature \\(T\\). Then \\(p_{i}\\asymp i^{-\\beta}\\) morphs into \\(q_{i}^{T}\\asymp i^{-\\beta/T}\\) (to first order). We see that temperature scaling directly causes narrowing of tail for \\(T>1\\). Other mechanisms can come to play: for instance, for autoregressive models with perplexity, token-wise tail cutting can result in tail narrowing for sequence-perplexity (see Figure 35 and discussion in Appendix I).\n' +
      '\n' +
      '### A New Scaling Law in the Hutter LLM\n' +
      '\n' +
      'For a deterministic ground-truth labelling function \\(i\\mapsto j_{i}\\), consider a downstream Hutter "LLM" (Hutter, 2021)\n' +
      '\n' +
      '\\[\\widehat{f}(i):=\\begin{cases}j_{i},&\\text{ if }(i,j_{i})\\in\\mathcal{D}_{T},\\\\ \\perp,&\\text{ otherwise,}\\end{cases} \\tag{7}\\]\n' +
      '\n' +
      'constructed on a sample \\(\\mathcal{D}_{T}:=\\{(i_{t},j_{t})\\mid t\\in[T]\\}\\) of size \\(T\\) from unmitigated Zipf distribution \\(p\\) (1), the test error obeys the following scaling law Hutter (2021)\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-(1-1/\\beta)}. \\tag{8}\\]\n' +
      '\n' +
      'Now, let \\(q\\) be a k-tail-cutting version of \\(p\\), i.e \\(q_{i}\\asymp p_{i}\\) if \\(i\\leq k\\) and \\(q_{i}=0\\) otherwise. When constructed ("trained") on \\(\\mathcal{D}_{T}\\) of size \\(T\\), now from \\(q\\), the test error (w.r.t to the true data distribution \\(p\\)) of this model is\n' +
      '\n' +
      '\\[E_{test}:=\\mathbb{P}_{i\\sim p}(\\widehat{f}(i)\\neq j_{i})=\\sum_{i\\geq 1}p_{i} \\mathbb{P}(\\widehat{f}(i)\\neq j_{i}). \\tag{9}\\]\n' +
      '\n' +
      'That is, we train on data from the AI distribution \\(q\\) and test on original distribution \\(p\\). We prove the following scaling law for tail cutting (all proofs are relegated to Appendix C):\n' +
      '\n' +
      '**Theorem 2.1**.: _Consider long-tail real-world data with exponent \\(\\beta>1\\), and let the cutoff for AI-generated data be \\(k\\). Then, for large \\(k\\) and \\(T\\) samples from the AI, the test error of the downstream "LLM" scales like so \\(E_{test}\\asymp T^{-(\\beta-1)/\\beta}+k^{-(\\beta-1)}\\asymp\\min(T,k^{\\beta})^{-( \\beta-1)/\\beta}\\)._\n' +
      '\n' +
      'Thus, as soon as \\(T\\gtrsim k^{\\beta}\\), the AI-generated sample size \\(T\\) ceases to be a "scalable" resource: collecting more AI-generated samples will not improve the performance of the downstream model, i.e performance plateaus and we lose scaling. The result is illustrated empirically in Figure 3, left and Figure 8 (Appendix B).\n' +
      '\n' +
      'When we assume that the AI-generator itself was trained on \\(T_{0}\\) samples, we get a similar loss of scaling stemming from the tail cutting from finite sampling bias (Equation (6)):\n' +
      '\n' +
      '**Corollary 2.2** ("Finite Initial Sample Size").: _With \\(c=1-1/\\beta\\), it holds that_\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-c}+T_{0}^{-c}. \\tag{10}\\]\n' +
      '\n' +
      'These theoretical are empirically confirmed in the Figure 9.\n' +
      '\n' +
      'In the case of tail narrowing, the scaling behavior changes; instead of a plateau, we obtain a slower decay rate:\n' +
      '\n' +
      '**Corollary 2.3** ("Tail Narrowing").: _In the setting of Theorem 2.1, consider AI-generated data to also be long-tail data, albeit with smaller exponent \\(\\beta^{\\prime}\\in(1,\\beta)\\). Then, the downstream Hutter LIM trained on AI-generated data will scale as \\(E_{test}\\asymp T^{-(\\beta-1)/\\beta^{\\prime}}\\)._\n' +
      '\n' +
      '### Collapse Over Multiple Generations of AI Data\n' +
      '\n' +
      'We now examine the cumulative impact of prior loss of scaling across multiple generations. Consider \\(n\\)-fold recursive AI data-generation, i.e\n' +
      '\n' +
      '\\[p\\to p_{\\text{AI}(1)}\\to p_{\\text{AI}(2)}\\to\\ldots\\to p_{\\text{AI}(n)}. \\tag{11}\\]\n' +
      '\n' +
      'Each arrow corresponds to drawing a sample of size \\(T_{0}\\). If we iterate \\(n\\) times the argument leading to (10), we get the following scaling for the test error \\(E_{test}^{(n)}=E_{test}^{(n)}(T)\\) for learning on \\(T\\) samples from the \\(n\\)th generation and testing on the true data distribution,\n' +
      '\n' +
      '\\[\\begin{split} E_{test}^{(n)}&\\asymp T^{-c}+\\underbrace {T_{0}^{-c}+\\ldots+T_{0}^{-c}}_{n\\text{ times}}\\\\ &=T^{-c}+C_{n}T_{0}^{-c}=T^{-c}\\left(n(T/T_{0})^{c}+1\\right), \\end{split} \\tag{12}\\]\n' +
      '\n' +
      'where \\(c:=1-1/\\beta\\). We deduce the following result.\n' +
      '\n' +
      '**Theorem 2.4** (Informal).: _Model collapse (as spoken of in the literature) occurs iff \\(n\\gg(T_{0}/T)^{c}\\)._\n' +
      '\n' +
      '_For example, if \\(T_{0}\\gg T\\) (e.g \\(T_{0}\\geq CT\\log T\\)) and \\(n\\) is constant (e.g \\(n=25\\)), then model collapse will not occur if we learn on the \\(n\\)th generation of AI data. On the other hand, if \\(T_{0}\\lesssim T\\), then model collapse will eventually occur._\n' +
      '\n' +
      'In particular, taking \\(T_{0}\\asymp T\\), we get\n' +
      '\n' +
      '\\[E_{test}^{(n)}\\asymp C_{n}T^{-c}\\asymp nT^{-c}. \\tag{13}\\]\n' +
      '\n' +
      'Note how the loss scales linearly with the number of generations. Figure 3, middle, illustrates how an increased number\n' +
      '\n' +
      'Figure 4: **Experimental Results (Details in Section 6). Left plot. The scaling law for finetuning Llama2–7B on the Wikitext-103 dataset. ’0-gen’ utilizes the original data, while subsequent generations use data generated by the previous model. Middle plot. Scaling law of the transformer model trained to predict GCD of two integers. Data is synthesized from a 0th generation model trained on \\(300K\\) samples. Note the tapered-off scaling of the model trained on synthesized data, as predicted by our theory. Right plot.” Skills” (bursts of new GCDs) learned by the GCD-transformer on original (bottom) and AI data (top). We see how the disappearance of scaling leads to the disappearance of abilities, mastered by the model trained on clean data.**\n' +
      '\n' +
      'Figure 3: **Illustration of Our Main Results for Simplified LLMs. Left plot. Empirical confirmation of the double scaling law. The true distribution of the data is Zipf with exponent \\(\\beta=3/2\\). Broken lines correspond to \\(k^{-(\\beta-1)}\\), for varying \\(T\\) and different values of \\(k\\). Middle plot. Model collapse over multiple generations. Again \\(\\beta=3/2\\), \\(T_{0}=T\\) across all generations with no additional tail-cutting, regeneration for 5 times. Right plot. Notice the grokking behavior, as perfectly predicted by the Theorem 3.2. For any given value \\(\\pi\\) for the proportion of real data, the broken lines are asymptotes \\(E_{test}\\asymp(\\pi T)^{-c}\\) and each plateau has length of order \\(k^{d}/\\pi\\), both predicted by the theorem. See Figure 10 for similar results with other values of \\(k\\).**of generations moves the loss scaling curve progressively to the right. This leads to eventual model collapse.\n' +
      '\n' +
      '## 3 Mitigating Model Collapse via Data Mixing\n' +
      '\n' +
      'Here we explore the possibility of alleviating model collapse via the acquisition of even a tiny amount of data from the true data distribution, to complement AI polluted data. We study two phenomena: (1) In the case of mixing \\(\\pi\\)-fraction of the original data with a \\((1-\\pi)\\) fraction of AI-generated data we exhibit a startling "grokking" phenomenon where test loss plateaus with increasing training data to finally decrease again according to the scaling law of the original model, and (2) in the scenario where we would like to compensate for missing "tail", we acquire some data from the tail of the original distribution to show that this needs to be done with caution: getting data from "too deep" in the tail is worthless while data closer to the precise "missing" tail can be beneficial. All proofs can be found in Appendix C.\n' +
      '\n' +
      '### Acquiring Missing Tail\n' +
      '\n' +
      'To counter the effect of tail cutting and the resulting plateau in scaling, we might resort to adding curated data that would emphasize the tail. The following Theorem 3.1 studies this effect; it shows, in particular that if we "overshoot" and only curate tail that is too deep, our efforts will be worthless. Rather, there is a fine line around the chopped tail \\(k\\) (within a factor of \\((1+o(1))\\) of \\(k\\)), where we need to place our data curation efforts to achieve the desired effect, a return of the scaling beyond the plateau.\n' +
      '\n' +
      'Suppose we "buy" a chunk of the tail of the real data distribution corresponding to \\(i=N,N+1,\\ldots\\); let the distribution be \\(\\pi\\) (thus, supported on \\(\\{N,N+1,\\ldots\\}\\)). Now, let \\(k\\), \\(N\\), and \\(T\\) tend to infinity such that \\(N/k\\to C\\), with \\(C\\in[1,\\infty]\\). We have the following sharp phase-transition.\n' +
      '\n' +
      '**Theorem 3.1**.: _(A) If \\(C=1\\), e.g if \\(N=k+\\sqrt{k}\\), then \\(E_{test}\\asymp T^{-c}\\). That is, we perfectly anneal the tail-chopping effect of AI-generated data._\n' +
      '\n' +
      '_(B) If \\(C>1\\), then \\(E_{test}\\asymp T^{-c}+k^{-\\alpha}\\) (which recovers the result of Theorem 2.1), and so "buying" the \\(N\\)th tail of the real data distribution is worthless._\n' +
      '\n' +
      '### A Grokking Phenomenon\n' +
      '\n' +
      'Here we show how even small amounts of original data can mitigate the above "scaling law collapse" by introducing a grokking phenomenon where test error plateaus and eventually continues to decline.\n' +
      '\n' +
      '**Theorem 3.2** (Grokking with Tail Cutting).: _Consider a sample of size \\(T\\) of which a proportion \\(\\pi\\) comes from the true distribution \\(p\\) and the remainder comes from a version \\(p^{\\prime}\\) of \\(p\\) with its tail chopped off at rank \\(k\\). We have the following scaling laws for the Hutter LLM define din (7)._\n' +
      '\n' +
      '_(A) **Early-Stage Dynamics.** For \\(T\\ll k^{\\beta}/\\pi\\), it holds that_\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-(1-1/\\beta)}+k^{-(\\beta-1)}. \\tag{14}\\]\n' +
      '\n' +
      '_Thus, during this stage, the money spent on acquiring some clean data is not amortized!_\n' +
      '\n' +
      '_(B) **Later-Stage Dynamics.** As soon as \\(T\\geq Ck^{\\beta}/\\pi\\) (where \\(C\\) is an absolute constant), it holds that_\n' +
      '\n' +
      '\\[E_{test}\\asymp(\\pi T)^{-(1-1/\\beta)}. \\tag{15}\\]\n' +
      '\n' +
      '_Thus, during this stage, we recover the unpolluted sample-size law scaling \\(T^{-(1-1/\\beta)}\\), up to within a multiplicative constant \\(\\pi^{-(1-1/\\beta)}\\) (which can be seen as an increase in the price of data). For fixed \\(T\\) and tunable \\(\\pi\\), this error rate scales like \\(\\pi^{-(1-1/\\beta)}\\), which is yet another scaling law._\n' +
      '\n' +
      'Effectively, the above theorem predicts that for any fixed \\(\\pi\\in(0,1)\\) -no matter how small- the test error grokks w.r.t sample size \\(T\\). The result is empirically confirmed in Figure 3, right (see Figure 10 for another illustration).\n' +
      '\n' +
      'We experimentally confirm this new phenomenon for transformer models trained to calculate the GCD (see Appendix G), which indicates its applicability for a wider class of LLMs with underlying deterministic ground truth, like for arithmetic tasks.\n' +
      '\n' +
      'In Appendix C.4 we state and prove a similar theorem in the case of _tail narrowing_ of synthetic data.\n' +
      '\n' +
      'Benefits of Mixing with AI Data.The above machinery allows us to analyze a particular regime where AI-data can help improve performance.\n' +
      '\n' +
      'Taking \\(T=T_{real}+T_{AI}\\) and \\(\\pi=T_{real}/T\\), we have the following important corollary of Theorem 3.2.\n' +
      '\n' +
      '**Corollary 3.3**.: _For \\(T_{real}\\ll k^{\\beta}\\), it holds that \\(E_{test}\\asymp(T_{real}+T_{AI})^{-(1-1/\\beta)}+k^{-(\\beta-1)}.\\)_\n' +
      '\n' +
      'Figure 5 illustrates how AI data can boost performance, up to a certain point, when its benefits plateau. This result might contribute to our understanding of why, sometimes,\n' +
      '\n' +
      'Figure 5: Mixing \\(T_{real}\\) real data with \\(T_{AI}\\) AI data. The dotted lines depict test errors of real data alone. \\(k=1,000,\\beta=3/2\\).\n' +
      '\n' +
      'adding AI-generated data might lead to better models, especially when generated by a stronger model (e.g. He et al. (2023); Shipard et al. (2023); Bansal and Grover (2023); Lin et al. (2023)). See Appendix A for more references.\n' +
      '\n' +
      '## 4 A Tailed Bigram Model\n' +
      '\n' +
      'We will now proceed to a more complex model, bringing us closer to capturing the _probabilistic_ and _autoregressive_ nature of LLMs (next token prediction). In this Section we will define the data generating process, define the new model (Hutter++), and establish that the original scaling law (with clean data) still holds. We then proceed to show similar loss of scaling for AI-data.\n' +
      '\n' +
      'A first fundamental step is to consider _probabilistic_ ground truth labels to replace the deterministic Hutter prediction \\(i\\mapsto y_{i}\\) with a probability distribution \\(p(j|i)\\) on \\(\\mathbb{N}_{*}\\) with power law decay (as in Equation (1)). To account for the fact that the most frequent next token \\(j\\) depends on the preceding token \\(i\\) we model\n' +
      '\n' +
      '\\[p(j\\mid i)\\propto\\pi_{i}(j)^{-\\beta}, \\tag{16}\\]\n' +
      '\n' +
      '(instead of \\(j^{-\\beta}\\)), where \\(\\pi_{i}\\) is a permutation associated to every \\(i\\) providing the order of outputs. To summarize, we think of the data as pairs \\((i,j)\\), where the distribution of \\(i\\) is governed by some \\(p(i)\\) as in the deterministic Hutter setting, and \\(p(j|i)\\) is given by Equation (16).\n' +
      '\n' +
      'This setting can be made _autoregressive_ by generating sequences step by step, using the preceding output as the next input. We can think of each successive pair of tokens as of the pairs \\((i,j)\\) above, with the only difference that the marginal distribution \\(p(i)\\) changes. We thus will make no assumptions on \\(p(i)\\) in what follows (except for a mild technical condition). Proofs can be found in Appendix D.\n' +
      '\n' +
      '### The Hutter++ Algorithm\n' +
      '\n' +
      'We now present an extension of the Hutter model (7) which is adapted to bigrams. Let \\(n_{T}(i)=\\sum_{t=1}^{T}1[i_{t}=i]\\) be the number times the context \\(i_{t}\\) appears in the dataset \\(\\mathcal{D}_{T}\\) and \\(n_{T}(i,j)=\\sum_{t=1}^{T}1[(i_{t},j_{t})=(i,j)]\\) be the number of times the pair \\((i,j)\\) appears in the dataset. Note that \\(n_{T}(i)\\sim Bin(T,p_{i})\\). As soon as \\(n_{T}(i)\\geq 1\\), define\n' +
      '\n' +
      '\\[q_{T}(j\\mid i):=n_{T}(i,j)/n_{T}(i).\\]\n' +
      '\n' +
      'This is an empirical version of \\(p(\\cdot\\mid i)\\) based on an iid sample of size \\(n_{T}(i)\\). For a theoretical analysis, we shall consider the following test error metric based on total-variation (TV)\n' +
      '\n' +
      '\\[E_{test}:=\\sum_{i}p_{i}\\operatorname{\\mathbb{E}}\\left[TV(q_{T}(\\cdot\\mid i), p(\\cdot\\mid i))\\right], \\tag{17}\\]\n' +
      '\n' +
      'where \\(TV(a,b):=\\sum_{j}|a_{j}-b_{j}|\\) is the total-variation distance and the expectation is over the randomness in \\(q_{T}\\). An asset here is that (Berend and Kontorovich, 2012) can be used to control the quantities \\(\\operatorname{\\mathbb{E}}\\left[TV(q_{T}(\\cdot\\mid i),p(\\cdot\\mid i))\\right]\\). Note that TV is upper-bounded by the square-root of KL-divergence, thanks to _Pinker\'s inequality_. This gives indication that our results could also apply in the setting of autoregressive models with perplexity loss, like LLMs.\n' +
      '\n' +
      '### A Scaling Law for Hutter++\n' +
      '\n' +
      'Consider a case of non-deterministic outputs as in Equation 16, where \\(\\pi_{1},\\pi_{2},\\ldots\\) are functions from \\(\\mathbb{N}_{*}\\) to \\(\\mathbb{N}_{*}\\).\n' +
      '\n' +
      '**Theorem 4.1**.: _Suppose \\(\\beta\\in(1,\\infty)\\setminus\\{2\\}\\) and set \\(c:=\\min(1-1/\\beta,1/2)\\). If \\(\\sum_{i}p_{i}^{1-c}<\\infty\\), then \\(E_{test}\\lesssim T^{-c}\\). Moreover, if \\(\\beta\\in(1,2)\\) and the mappings \\(\\pi_{1},\\pi_{2},\\ldots\\) are permutations, then \\(E_{test}\\asymp T^{-c}\\)._\n' +
      '\n' +
      'Thus, the proposed Hutter++ algorithm induces exactly the same scaling law as the classical setup (Hutter, 2021)!\n' +
      '\n' +
      '### Model Collapse in Probabilistic Setting\n' +
      '\n' +
      'We now return to our main problem, understanding model collapse in the probabilistic setting and consider the Hutter++ presented above. Thus, suppose the learner only has access to at most a dataset of size \\(T\\) containing the \\(k\\)th head of the conditional distribution \\(p(\\cdot\\mid i)\\). That is, sampled from: \\(i\\sim p\\), \\(j\\sim p(j\\mid i)1[j\\leq k]\\) (normalized appropriately), where \\(p(\\cdot\\mid i)\\) is as in Equation (16).\n' +
      '\n' +
      '**Theorem 4.2**.: _(A) If \\(\\beta\\in(1,\\infty)\\setminus\\{2\\}\\) and \\(\\sum_{i}p_{i}^{1-c}<\\infty\\) where \\(c:=\\min(1-1/\\beta,1/2)\\) as before, then \\(E_{test}\\lesssim T^{-c}+k^{-\\beta c}\\)._\n' +
      '\n' +
      '_(B) Furthermore, if the mappings \\(\\pi_{1},\\pi_{2},\\ldots\\) are permutations and \\(\\sum_{i}p_{i}^{1-c}<\\infty\\), then \\(E_{test}\\asymp T^{-c}+k^{-\\beta c}\\)._\n' +
      '\n' +
      '**Autoregressive Bigrams.** Similarly these results hold for autoregressive bigram model, where \\(p(i_{1},i_{2},\\ldots,i_{L})=p(i_{1})\\prod_{\\ell=1}^{L-1}p(i_{\\ell+1}\\mid i_{ \\ell})\\), and each \\(p(j\\mid i)\\) is as in (16). The result is empirically confirmed in Figure 11 in Appendix B.\n' +
      '\n' +
      '**Multiple Generations.** The mechanics of the proof of Theorem 2.4 apply in this setting. See Figure 12 in Appendix B illustrating that Equation (13) keeps holding for probabilistic data distributions.\n' +
      '\n' +
      '**Grokking for Mixtures.** Technically speaking, this grokking phenomenon only holds for models with deterministic ground truth labels, like the Hutter LLM and the limited capacity associative memory model. For the _probabilistic_ setting of bigrams (or text LLMs) the theorem cannot hold in its pure form, because if we train on a mixture of two distributions (clean and synthetic) but test only on the clean distribution, the distance between these two distributions will always be a lower bound on the test error. However, we can see that remnants of a "smoothed" grokking-law persist in the form of an S-shaped scaling (see Figure 19 in Appendix B).\n' +
      '\n' +
      '## 5 Capacity-Limited Memory Models: A Triplet Scaling Law\n' +
      '\n' +
      'We now turn to a finite-memory extension of the Hutter LLM, which allows to model _capacity_. We thus study the model collapse phenomenon in the context of the following simple associative memory model studied in (Cabannes et al., 2023)\n' +
      '\n' +
      '\\[f_{T}(i) :=\\arg\\max_{y}H_{T}(i,y),\\text{ where} \\tag{18}\\] \\[H_{T}(i,y) :=e_{i}^{\\top}M_{T}u_{y},\\] \\[M_{T} :=\\sum_{i}q_{T}(i)e_{i}u_{f_{*}(i)}^{\\top}\\in\\mathbb{R}^{d\\times d}.\\]\n' +
      '\n' +
      'This is a transformer-like finite-memory extension of the infinite-memory model in (Hutter, 2021). The integer \\(d\\geq 1\\) then plays the role of the "capacity" of the resulting model. Here, \\(f_{*}:[N]\\rightarrow[m]\\) is an unknown function, for example, reduction modulo \\(m\\), i.e \\(f_{*}(i):=((i-1)\\text{ mod }m)+1\\); \\(q_{T}=q(\\mathcal{D}_{T})\\) is probability distribution on \\([N]\\) which encodes an arbitrary learner, estimated using and iid sample \\(\\mathcal{D}_{t}=\\{(i_{t},y_{t})\\ |\\ t\\in[T]\\}\\) of size \\(T\\) collected from a probability distribution on \\([N]\\times[m]\\), of the form\n' +
      '\n' +
      '\\[i\\sim p=\\operatorname{Zipf}(\\beta),\\quad y=f_{*}(i). \\tag{19}\\]\n' +
      '\n' +
      'The embedding vectors \\(e_{1},e_{2},\\dots e_{N}\\) and \\(u_{1},u_{2},\\dots,u_{m}\\) are a system of unit-vectors in \\(\\mathbb{R}^{d}\\), constructed so that the matrix \\(\\mathbb{R}^{d\\times d}\\) remembers the input/output pairs \\((i,j)\\) it has seen, i.e \\(e_{i}^{\\top}Mu_{f_{*}(i)}\\approx q_{T}(i)\\) if \\((i,f_{*}(i))\\in\\mathcal{D}_{T}\\). The weights \\(q_{T}(i)\\) ensure that different memories are memorized faster than others.\n' +
      '\n' +
      'Cabannes et al. (2023) proposed that iid random embeddings from the uniform distribution on the unit-sphere in \\(\\mathbb{R}^{d}\\) be used. In this setting, for different choices of \\(q\\), the following general scaling law was established\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-(1-1/\\beta)}+d^{-c_{q}}, \\tag{20}\\]\n' +
      '\n' +
      'where the exponent \\(c_{q}\\in(0,\\infty)\\) depends on \\(\\beta\\) and the algorithm \\(q\\). For example, when \\(q\\) encodes the counting measure \\(q_{T}(i):=n_{T}(i)/T\\) (reminiscent of SGD), it was shown that \\(c_{q}=(1-1/\\beta)/2\\in(0,1/2)\\). Another algorithm \\(q_{T}(i):=1[n_{T}(i)\\geq 1]/\\sum_{\\ell}1[n_{T}(\\ell)\\geq 1]\\) (reminiscent of ADAM ) was proposed which attains a optimal error rate (over all algorithms based on random embeddings) with \\(c_{q}=\\beta-1\\).\n' +
      '\n' +
      'In the context of model collapse which is the main focus of this manuscript, we have the following.\n' +
      '\n' +
      '**Theorem 5.1** (Triplet Scaling Law).: _For all the algorithms \\(q\\) considered in (Cabannes et al., 2023), one has the following triplet scaling law w.r.t sample size \\(T\\), embedding dimension \\(d\\), and frequency cutoff \\(k\\),_\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-(1-1/\\beta)}+d^{-c_{q}}+k^{-(\\beta-1)}. \\tag{21}\\]\n' +
      '\n' +
      'This result is empirically confirmed in Figure 21 and proved in Appendix E. It gives rise to similarly tapered-off scaling curves for synthetic data, as in the simpler models. The proofs for loss of scaling across generations in Section 2 and grokking phenomena in Section 3, carry over to this model as well, demonstrating their universality.\n' +
      '\n' +
      '## 6 Experiments\n' +
      '\n' +
      'In this section we present our experimental results to demonstrate evidence of various predictions we have made theoretically. We showcase four scenarios of increasing level of complexity: an empirical Hutter++ model, autoregressive bigram models with _perplexity loss_, an arithmetic transformer to predict the GCD of two integers (Charton, 2023) and a large-scale LLM, Llama2-7B (Touvron et al., 2023), trained on a large data corpus (Wikidata-103).\n' +
      '\n' +
      'In our theoretical analysis, motivated by empirical observations (see Figure 2) or by the effect of finite data sampling bias on heavy-tailed data, we have assumed that generated data follows patterns of either a tail cutoff or tail narrowing. In our subsequent experiments, we depart from theoretical assumptions on tail-cutting/narrowing to allow the widely deployed top-p selection or temperature scaling mechanisms to give possibly intermingled effects on the generated data distribution.\n' +
      '\n' +
      '**Empirical Hutter++ Model.** In Figure 6, we use an initial model that is trained on \\(T_{0}=100,000\\) samples from the original distribution. For the Gen 1 line, the data are all generated from this initial model. From Gen 2 onwards, models are iteratively trained on data produced by the most performant model of the preceding generation, effectively eliminating the possibility that model collapse results from inadequate sampling. For Gen 1, a notable degradation in data scalability is observed, alongside a rapid decline in model performance across generations. These observations not only validate our theoretical result but also reaffirm our assumptions. A similar pattern is evident with temperature scaling, as shown in Figure 16.\n' +
      '\n' +
      'Figure 6: **Hutter++ on Bigram with limited data and top-p.** The initial model is trained on \\(T_{0}=100,000\\) samples. It generates \\(T\\) samples for Gen 1. Starting from Gen 2 models are trained on data generated by the most powerful model from the previous generation. Top-p-0.95 cutting and \\(\\beta=3/2\\).\n' +
      '\n' +
      '**Autoregressive Bigram Models with Perplexity Loss.** We move one step further towards "real" LLMs to investigate autoregressive bigram models. The dataset now comprises sequentially generated integers, adhering to Equation (16), with the model trained on all tokens. We use the averaged perplexity score of the test set as the test error metric. Our study encompasses a range of effects--such as top-p inference, temperature scaling, limited real data, and training on progressively larger AI datasets. Consistent with the findings in Section 4, we observe the same patterns of scaling loss and progressive model collapse across generations. Relevant figures are provided in Appendix F.\n' +
      '\n' +
      '**Transformers Learning the GCD.** Our first illustration of our theory "in the wild" is for sequence-to-sequence transformer models for an arithmetic task: predicting the greatest common divisor (GCD) of two integers, encoded as sequences of digits in some base \\(B\\) following Charton (2023). This setup is a perfect intermediate step between our toy models and large scale LLMs; it uses the transformer architecture and training algorithms on sizeable models, while the underlying data has a deterministic nature. Over the course of training the model progressively learns new GCDs and with them also their products with already learned GCDs. We can thus view each such learned group, usually learned in "bursts", as a new skill. For the purpose of this experiment, we use this model after \\(300M\\) samples as the generator of AI-data. In Figure 4 we validate the predicted scaling law for a single generation and observe \'un-learning\' of skills when training exclusively with generated data, as well as a grokking effect when training with mixtures. See Appendix G for a full description and more details and Figures.\n' +
      '\n' +
      '**Experiments on LLMs.** We finetune Llama2 with LoRA, generating synthetic AI data for the next finetuning iteration. Inspired by the setup in Shumailov et al. (2023), we use Wikidata-103, partitioned into approximately \\(2.2\\) million sequences of 128 tokens. AI data is generated through prompt completion, using the first 96 tokens from the original sequences as prompts. The model is trained only on the last 32 tokens to preclude information leakage, i.e. the model being trained on the ground truth of the same 32 tokens. The evaluations are conducted exclusively on the same 32 tokens. We use top-p 0.9 and temperature 0.9 across all generation. The results, depicted in Figure 4 (left), illustrate a scaling law decay over several generations. The first generated dataset still contain useful but limited information and the utility of the second generation\'s data markedly diminishes. These phenomena corroborate the anticipated loss of scaling law and model collapse, further indicating that model collapse is even more pronounced here, highlighting the challenges in training next generation LLMs. More details and results in Appendix H.\n' +
      '\n' +
      'Moreover, we conduct experiments to investigate mixing a proportion of real data with AI-generated data. Figure 7 demonstrates the effect of blending a random 2% of original data with AI data across all fine-tuning phases. It significantly mitigates model collapse, with the emergence of a grokking curve as predicted in Theorem 3.2.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      'In the advent of the "synthetic data age", our work signals the end of current neural scaling laws and opens the door to a puzzling array of new phenomena in a world where the training corpora are enriched with AI generated data. We demonstrate that scaling laws cease to persist; test error tapers off due to altered, less heavy tailed, data distributions. As noted already in prior work, in a fully synthetic data world learning will stop and models will degenerate - their scaling will halt and revert completely. Yet, new opportunities arise from careful mixture and data curation, as we have shown, with interesting effects at the interplay of clean and synthesized data. We must recognize new learning plateaus and, for instance, adjust to changed learning curves from blending clean and synthetic data to unintended early stopping. A notable feature of our work is that our theory is _effective_ - we observe the predicted phenomena for relevant large models in two different settings.\n' +
      '\n' +
      'Taken together, our contributions call for a more responsible, or "collapse-aware", proliferation of synthesized data. Scale is _not_ all you need: more work on effective watermarking for synthetic data is needed, to make it more distinguishable from the original, human-annotated data. Thus, clean / real data will become an even more valuable resource in the future, as we are ushering in the "beyond scaling" era.\n' +
      '\n' +
      'Figure 7: **Mixing Llama Generated Data with Original Data** Based on Figure 4 left, we further mix the generated data with original data, with ratio 98 to 2. Adding original data significantly mitigates the model collapse. Look how the mixing curve validates our predicted curve of the grokking phenomenon as in Figure 3\n' +
      '\n' +
      '## 8 Acknowledgements\n' +
      '\n' +
      'YF and JK acknowledge support through NSF NRT training grant award 1922658. YF and PY would like to thank Di He for discussions and suggestions. This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Aghajanyan et al. (2023) Aghajanyan, A., Yu, L., Conneau, A., Hsu, W.-N., Hambardzumyan, K., Zhang, S., Roller, S., Goyal, N., Levy, O., and Zettlemoyer, L. Scaling laws for generative mixed-modal language models, 2023.\n' +
      '* Alemohammad et al. (2023) Alemohammad, S., Casco-Rodriguez, J., Luzi, L., Humayun, A. I., Babaei, H., LeJeune, D., Siahkoohi, A., and Baraniuk, R. G. Self-consuming generative models go mad. _arXiv preprint arxiv:2307.01850_, 2023.\n' +
      '* Arora & Goyal (2023) Arora, S. and Goyal, A. A theory for emergence of complex skills in language models. _arXiv preprint arXiv:2307.15936_, 2023.\n' +
      '* Azizi et al. (2023) Azizi, S., Kornblith, S., Saharia, C., Norouzi, M., and Fleet, D. J. Synthetic data from diffusion models improves imagenet classification. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.\n' +
      '* Bansal & Grover (2023) Bansal, H. and Grover, A. Leaving reality to imagination: Robust classification via generated datasets. In _ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models_, 2023.\n' +
      '* Berend & Kontorovich (2012) Berend, D. and Kontorovich, A. On the convergence of the empirical distribution. _ArXiv Preprint_, 2012.\n' +
      '* Bertrand et al. (2023) Bertrand, Q., Bose, A. J., Duplessis, A., Jiralerspong, M., and Gidel, G. On the stability of iterative retraining of generative models on their own data. _arXiv preprint arxiv:2310.00429_, 2023.\n' +
      '* Bohacek & Farid (2023) Bohacek, M. and Farid, H. Nepotistically trained generative models collapse, 2023.\n' +
      '* Bordelon et al. (2020) Bordelon, B., Canatar, A., and Pehlevan, C. Spectrum dependent learning curves in kernel regression and wide neural networks. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pp. 1024-1034. PMLR, 2020.\n' +
      '* Briesch et al. (2023) Briesch, M., Sobania, D., and Rothlauf, F. Large language models suffer from their own output: An analysis of the self-consuming training loop, 2023.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020.\n' +
      '* Burg et al. (2023) Burg, M. F., Wenzel, F., Zietlow, D., Horn, M., Makansi, O., Locatello, F., and Russell, C. Image retrieval outperforms diffusion models on data augmentation. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.\n' +
      '* Cabannes et al. (2023) Cabannes, V., Dohmatob, E., and Bietti, A. Scaling laws for associative memories, 2023.\n' +
      '* Caponnetto & de Vito (2007) Caponnetto, A. and de Vito, E. Optimal rates for the regularized least-squares algorithm. _Foundations of Computational Mathematics_, 7:331-368, 2007.\n' +
      '* Charton (2023) Charton, F. Can transformers learn the greatest common divisor?, 2023.\n' +
      '* Chen et al. (2023) Chen, M. F., Roberts, N., Bhatia, K., WANG, J., Zhang, C., Sala, F., and Re, C. Skill-it! a data-driven skills framework for understanding and training language models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.\n' +
      '* Cui et al. (2021) Cui, H., Loureiro, B., Krzakala, F., and Zdeborova, L. Generalization error rates in kernel regression: the crossover from the noiseless to noisy regime. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), _Advances in Neural Information Processing Systems_, 2021.\n' +
      '* Cui et al. (2022) Cui, H., Loureiro, B., Krzakala, F., and Zdeborova, L. Generalization error rates in kernel regression: the crossover from the noiseless to noisy regime. _Journal of Statistical Mechanics: Theory and Experiment_, 2022(11):114004, nov 2022.\n' +
      '* Cui et al. (2023) Cui, H., Loureiro, B., Krzakala, F., and Zdeborova, L. Error scaling laws for kernel classification under source and capacity conditions. _Machine Learning: Science and Technology_, 4(3):035033, August 2023. ISSN 2632-2153. doi: 10.1088/2632-2153/acf041.\n' +
      '* Dai et al. (2023) Dai, H., Liu, Z., Liao, W., Huang, X., Cao, Y., Wu, Z., Zhao, L., Xu, S., Liu, W., Liu, N., Li, S., Zhu, D., Cai, H., Sun, L., Li, Q., Shen, D., Liu, T., and Li, X. Auggpt: Leveraging chatp for text data augmentation, 2023.\n' +
      '\n' +
      '* Debowski (2023) Debowski, L. A simplistic model of neural scaling laws: Multiperiodic Santa Fe processes, 2023.\n' +
      '* Devlin et al. (2018) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Fan et al. (2023) Fan, L., Chen, K., Krishnan, D., Katabi, D., Isola, P., and Tian, Y. Scaling laws of synthetic images for model training... for now. _arXiv preprint arXiv:2312.04567_, 2023.\n' +
      '* Gordon et al. (2021) Gordon, M. A., Duh, K., and Kaplan, J. Data and parameter scaling laws for neural machine translation. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 5915-5922, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.478.\n' +
      '* Guo et al. (2023) Guo, Y., Shang, G., Vazirgiannis, M., and Clavel, C. The curious decline of linguistic diversity: Training language models on synthetic text, 2023.\n' +
      '* Hataya et al. (2023) Hataya, R., Bao, H., and Arai, H. Will large-scale generative models corrupt future datasets? In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pp. 20555-20565, October 2023.\n' +
      '* He et al. (2023) He, R., Sun, S., Yu, X., Xue, C., Zhang, W., Torr, P., Bai, S., and QI, X. Is synthetic data from generative models ready for image recognition? In _The Eleventh International Conference on Learning Representations_, 2023.\n' +
      '* Henighan et al. (2021) Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S., et al. Scaling laws for autoregressive generative modeling. _arXiv preprint arXiv:2103.05847_, 2021.\n' +
      '* Hernandez et al. (2021) Hernandez, D., Kaplan, J., Henighan, T., and McCandlish, S. Scaling laws for transfer. _arXiv preprint arXiv:2102.01293_, 2021.\n' +
      '* Hestness et al. (2017) Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y. Deep learning scaling is predictable, empirically. _arXiv preprint arXiv:1712.00409_, 2017.\n' +
      '* Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models, 2022.\n' +
      '* Huang et al. (2022) Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve, 2022.\n' +
      '* Hutter (2021) Hutter, M. Learning curve theory. _arXiv preprint arXiv:2102.04074_, 2021.\n' +
      '* Kaplan et al. (2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* Kingma and Welling (2014) Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In _International Conference on Learning Representations_, 2014.\n' +
      '* Lin et al. (2023) Lin, S., Wang, K., Zeng, X., and Zhao, R. Explore the power of synthetic data on few-shot object detection. In _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pp. 638-647, 2023. doi: 10.1109/CVPRW59228.2023.00071.\n' +
      '* Liu et al. (2019) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.\n' +
      '* Maloney et al. (2022) Maloney, A., Roberts, D. A., and Sully, J. A solvable model of neural scaling laws, 2022.\n' +
      '* Martinez et al. (2023a) Martinez, G., Watson, L., Reviriego, P., Hernandez, J. A., Juarez, M., and Sarkar, R. Combining generative artificial intelligence (ai) and the internet: Heading towards evolution or degradation? _arXiv preprint arxiv: 2303.01255_, 2023a.\n' +
      '* Martinez et al. (2023b) Martinez, G., Watson, L., Reviriego, P., Hernandez, J. A., Juarez, M., and Sarkar, R. Towards understanding the interplay of generative artificial intelligence and the internet. _arXiv preprint arxiv: 2306.06130_, 2023b.\n' +
      '* McKenzie et al. (2023) McKenzie, I. R., Lyzhov, A., Pieler, M. M., Parrish, A., Mueller, A., Prabhu, A., McLean, E., Shen, X., Cavanagh, J., Gritsevskiy, A. G., Kauffman, D., Kirtland, A. T., Zhou, Z., Zhang, Y., Huang, S., Wurgaft, D., Weiss, M., Ross, A., Recchia, G., Liu, A., Liu, J., Tseng, T., Korbak, T., Kim, N., Bowman, S. R., and Perez, E. Inverse scaling: When bigger isn\'t better. _Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.\n' +
      '* Michaud et al. (2023) Michaud, E. J., Liu, Z., Girit, U., and Tegmark, M. The quantization model of neural scaling. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.\n' +
      '* Midjourney (2023) Midjourney. Midjourney ai, 2023. URL [https://www.midjourney.com/](https://www.midjourney.com/).\n' +
      '\n' +
      '* Mobahi et al. (2020) Mobahi, H., Farajtabar, M., and Bartlett, P. Self-distillation amplifies regularization in Hilbert space. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 3351-3361. Curran Associates, Inc., 2020.\n' +
      '* Nitanda & Suzuki (2021) Nitanda, A. and Suzuki, T. Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime. In _International Conference on Learning Representations_, 2021.\n' +
      '* Papyan et al. (2020) Papyan, V., Han, X., and Donoho, D. L. Prevalence of neural collapse during the terminal phase of deep learning training. _Proceedings of the National Academy of Sciences_, 117(40):24652-24663, 2020.\n' +
      '* Ramesh et al. (2021) Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image generation. In Meila, M. and Zhang, T. (eds.), _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pp. 8821-8831. PMLR, 18-24 Jul 2021.\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 10684-10695, June 2022.\n' +
      '* Rosenfeld et al. (2020) Rosenfeld, J. S., Rosenfeld, A., Belinkov, Y., and Shavit, N. A constructive prediction of the generalization error across scales. In _International Conference on Learning Representations_, 2020.\n' +
      '* Schmidt-Hieber (2017) Schmidt-Hieber, J. Nonparametric regression using deep neural networks with relu activation function. _Annals of Statistics_, 48, 08 2017. doi: 10.1214/19-AOS1875.\n' +
      '* Shipard et al. (2023) Shipard, J., Wiliem, A., Thanh, K. N., Xiang, W., and Fookes, C. Diversity is definitely needed: Improving model-agnostic zero-shot classification via stable diffusion, 2023.\n' +
      '* Shumailov et al. (2023) Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., and Anderson, R. The curse of recursion: Training on generated data makes models forget. _arXiv preprint arxiv:2305.17493_, 2023.\n' +
      '* Spigler et al. (2020) Spigler, S., Geiger, M., and Wyart, M. Asymptotic learning curves of kernel methods: empirical data versus teacher-student paradigm. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(12):124001, December 2020. ISSN 1742-5468. doi: 10.1088/1742-5468/abc61d.\n' +
      '* Strubell et al. (2019) Strubell, E., Ganesh, A., and McCallum, A. Energy and policy considerations for deep learning in nlp. _ArXiv_, abs/1906.02243, 2019.\n' +
      '* Suzuki (2019) Suzuki, T. Adaptivity of deep reLU network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality. In _International Conference on Learning Representations_, 2019.\n' +
      '* Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Wang et al. (2023) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754.\n' +
      '* Wei et al. (2022) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent abilities of large language models. _Transactions on Machine Learning Research_, 2022. ISSN 2835-8856. Survey Certification.\n' +
      '* Xu et al. (2023) Xu, C., Guo, D., Duan, N., and McAuley, J. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. In _The 2023 Conference on Empirical Methods in Natural Language Processing_, 2023.\n' +
      '* Zipf (1935) Zipf, G. The psycho-biology of language: an introduction to dynamic philology. 1935.\n' +
      '\n' +
      '## Appendix A Prior Work\n' +
      '\n' +
      'Model Collapse:The phenomenon appeared in the _recent_ literature in the context of language and image generation. Several recent works demonstrate facets of this phenomenon _empirically_ in various settings (Hataya et al., 2023; Martinez et al., 2023a;b; Bohacek & Farid, 2023; Briesch et al., 2023; Guo et al., 2023; Fan et al., 2023). Only few recent works also provide some accompanying theoretical analysis (Shumailov et al., 2023; Alemohammad et al., 2023; Bertrand et al., 2023) which we outline now.\n' +
      '\n' +
      '(Shumailov et al., 2023) define model collapse and attribute it to two mechanisms: finite sampling when training a model (leading to cut off of low-probability data) and function approximation errors (the model is not sufficiently expressive to model the true distribution). They observe (and, for a single Gaussian, prove) that upon iteratively resampling finite "training data" the generated distribution becomes more and more peaked. Other models studied empirically are mixtures of (two) Gaussians and VAEs on MNIST. To study language models, (Shumailov et al., 2023) iteratively fine tune Meta\'s OPT-125M model on wikidata2. For generation of new text they use a 5-way beam search, which, by its nature, (approximatively) generates only low-perplexity data.\n' +
      '\n' +
      '(Alemohammad et al., 2023) conduct an empirical and analytical analysis on generative image models of what they term the "self-consuming" or "autophaguous" loop. They conclude that without enough fresh real data at each generation, future models necessarily will have their precision or recall decrease. They model the influence of each new AI-generation via a generic _sampling bias_\\(0\\leq\\lambda\\leq 1\\). In the case of image generation this refers to feature parameters at generation that favor quality over diversity (suitably quantified). More precisely, \\(\\lambda=1\\) corresponds to unbiased sampling and \\(\\lambda=0\\) corresponds to sampling from the modes of the generative distribution. \\(\\lambda\\) models biased sampling methods commonly used in generative modeling practice, such as truncation in BigGAN and StyleGAN or guidance in diffusion models. In the case of Gaussian distributions, \\(\\lambda\\) is the shrinking factor of the variance of the next generation. Their empirical work studies GANs and denoising diffusion probabilistic models for image generation on FFHQ and MNIST and single Gaussians for both theoretical and empirical observations. As in (Shumailov et al., 2023) they observe (and prove for the case of a single Gaussian) that estimation error alone leads to vanishing variance with number of iterations. (Alemohammad et al., 2023) also empirically observe an initial boost in performance in a regime where modest amounts of synthetic data are mixed with the original data before larger amounts of synthetic data lead to ultimate degradation. This might mimick larger-scale results that demonstrate how synthetic data mixed with true data improves performance in some scenarios (see _Benefits of synthesized data_ below). Indeed, in its simplest form, data augmentation (rotations, cropping etc. ), a widespread highly beneficial practice in ML training, can be viewed as the simplest form of data generation.\n' +
      '\n' +
      'Let us mention the study of Bertrand et al. (2023) in the context of image generation, which sheds light on the critical role of data composition in the stability and effectiveness in generative models. They explore scenarios involving a mix of clean data, representative of the true distribution, and synthesized data from previous iterations of the generator. Their analysis reveals that if the data mix consists exclusively of synthesized data, the generative process is likely to degenerate over time, leading to what they describe as a \'clueless generator\'. Thus, the generator collapses: it progressively loses its ability to capture the essence of the data distribution it was intended to model. Conversely, they found that when the proportion of clean data in the mix is sufficiently high, the generator, under certain technical conditions, retains the capability to learn and accurately reflect the true data distribution. This work sheds light on the critical role of data composition in the stability and effectiveness of generative models.\n' +
      '\n' +
      'Several empirical studies confirm the deleterious effect of training on self-generated data: In the context of image generation, (Martinez et al., 2023a;b) report degradation of models trained on AI-generated data. Specifically, they use a Denoising Diffusion Implicit Model and a few (relatively small) datasets (e.g. Orchids, MNIST) to demonstrate visual degradation when training in successive generations of AI-generated data. (Hataya et al., 2023) _"conclude that generated images negatively affect downstream performance, while the significance depends on tasks and the amount of generated images"_, (Bohacek & Farid, 2023) reports that the popular StableDiffusion model collapses when iteratively retrained on self-generated faces, even with as little as \\(3\\%\\) synthetic data mixed into the original training set. For text, (Briesch et al., 2023) use _nanoGPT6_ on a curated 10K logical-expression dataset to demonstrate the iterative collapse of self-consuming loops - the model and dataset are sufficiently small to allow training from scratch. (Guo et al., 2023) observe a decline in linguistic diversity metrics across iteratively fine-tuned LLMs.\n' +
      '\n' +
      'Footnote 6: [https://github.com/karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)Mitigation:To our knowledge, rigorous theory (or even empirical demonstrations) on mitigation strategies against model collapse are yet to come, with one notable exception in (Bertrand et al., 2023) (see below). Several works discuss the need for _detection_ of AI-generated images or text (to avoid retraining on them), for example motivating research into watermarking strategies. (Bertrand et al., 2023) analyze iterative retraining on a mixture of synthesized and original data under several technical assumptions and find that there are fixed points governing the stability of iterative retraining.\n' +
      '\n' +
      'Benefits of Synthesized DataThere is a range of results showing benefits of AI-synthesized data in training better models, though mostly these results pertain to image data, specifically in the context of diffusion models (Azizi et al., 2023; He et al., 2023; Shipard et al., 2023; Bansal and Grover, 2023; Lin et al., 2023), though not only (see Dai et al. (2023); Xu et al. (2023); Huang et al. (2022); Wang et al. (2023) for chat-related examples). One might argue that they either throw model-collapse caution to the winds or, possibly, settle in the protected corner where mild amounts of synthetic data (or larger amounts of "mildly synthetic" data, like in the case of data augmentation) helps. In particular, often benefits of synthetic data are observed when the synthetic data is generated by a model trained for a different use case than the downstream task (like images synthesized from diffusion models helping classification models) or generated by a stronger model (He et al., 2023; Shipard et al., 2023; Bansal and Grover, 2023; Lin et al., 2023). However, other works critically analyze the purported benefit of generated data. (Burg et al., 2023) find that while synthesized data from a diffusion model helps improving downstream tasks, such as classification, using the _pre-training data_ of the diffusion model alone gives even stronger performance (which we can interpret as evidence of mild first-generation model collapse). All in all it is fair to say that the impact of data augmentation using generative models is still not fully understood.\n' +
      '\n' +
      'Scaling Laws:Neural scaling laws have been ubiquitously observed in vision, language and speech. Early large scale empirical studies are performed in (Hestness et al., 2017; Rosenfeld et al., 2020), demonstrating power law scaling across a range of learning scenarios. This is followed by well-known large-scale studies from OpenAI (Kaplan et al., 2020) and DeepMind (Hoffmann et al., 2022), which empirically demonstrate power-law scaling in LLMs across a wide set of scales. Essentially, this empirically establishes that\n' +
      '\n' +
      '\\[L(N,D)\\sim N_{C}\\cdot N^{-\\alpha_{N}}+D_{C}\\cdot D^{-\\alpha_{D}},\\]\n' +
      '\n' +
      'where \\(L\\) is the per-token cross entropy loss (in nats), \\(N,D\\) are the number of (non-embedding) parameters and data, respectively, and \\(N_{C},D_{C}\\) and \\(\\alpha_{N},\\alpha_{D}\\) are constants determined by the data distribution and the model specifications.\n' +
      '\n' +
      'This study was extended to demonstrate many more power law relations in various scenarios (vision transformer, video modeling, multimodal models, and mathematical problem solving) (Henighan et al., 2021). In the machine translation (MT) setting, (Gordon et al., 2021) quantify scaling laws for standard benchmarks like BLEU and explain them via cross-entropy power-law scaling, thus positing a first universality of scaling laws across metrics. (Hernandez et al., 2021) demonstrate similar empirical power-law scaling for transfer learning and (Aghajanyan et al., 2023) provide a vast experimental body of evidence for scaling laws in mixed-modal language models.\n' +
      '\n' +
      'However, a few results have nuanced the view of scaling as a panacea to improved loss. For instance, (McKenzie et al., 2023) present evidence for "inverse sclaing" where flaws in the training objective or the data lead to U-shaped scaling.\n' +
      '\n' +
      'Theoretical Models for Scaling Laws:From a theoretical angle, scaling laws have been shown analytically even before the emergence of large foundation models. For instance, Caponnetto and de Vito (2007) characterize the power-law generalization error of regularized least-squares kernel algorithms. The role of optimization can also be taken into account in this setting ((Nitanda and Suzuki, 2021)). In the nonparametric literature, for example (Schmidt-Hieber, 2017) and (Suzuki, 2019) derived the test error scaling of deep neural network in fitting certain target functions and (Bordelon et al., 2020) analyze spectral dependence.\n' +
      '\n' +
      'More recently, scaling laws have been shown for kernel models under the Gaussian design, e.g. in (Spigler et al., 2020; Cui et al., 2021; Cui et al., 2022) for regression and (Cui et al., 2023) for classification. (Maloney et al., 2022) study scaling laws for the random feature model in the context of regression. In the context of memorization for heavy-tailed data scaling laws have been shown in the infinite-memory setting (Hutter, 2021), for "quantized" skills (Michaud et al., 2023) and for certain random data-generation processes (Debowski, 2023). When taking model capacity and optimization into account, Cabannes et al. (2023) recently proved scaling laws in constraint-capacity associative memories.\n' +
      '\n' +
      'To our knowledge, however, very few papers deal with the decay of scaling in the case of self-consuming loops. A notable example is (Mobahi et al., 2020) which studies iterated retraining in the context of self-(knowledge-)distillation in the kernel setting. However, this analysis is very distinct from our work, not only because it places itself in the kernel setting with Gaussian design, but also because it assumes the distillation setting, where the "generation" stage is carefully optimized for the next stage training. In the case of synthesized data in the wild, this assumption can of course not be made.\n' +
      '\n' +
      'Emergence of "Skills" and Scaling Laws:Scaling laws give us an insight on bang-for-the-buck style trade-off for model training. However, cross-entropy loss is not a goal in and of itself: we want to train models that are endowed with a larger and larger skill set as we scale them up. For instance, (Gordon et al., 2021) provide intuition and empirics for the scaling of BLEU score for MT with cross-entropy loss as\n' +
      '\n' +
      '\\[BLEU(L)\\approx Ce^{-kL},\\]\n' +
      '\n' +
      'demonstrating "emergence" of good BLEU performance with scale. This type of "emergence" has been massively confirmed in (Wei et al., 2022), where a working definition of "emerging" is "not present in smaller models, but appears in larger models". In this sense, (Wei et al., 2022) demonstrate empirically a large number of "skills" appearing with scale, like Multi-Task NLU, Modular arithmetic, word unscrambling and transliteration.\n' +
      '\n' +
      'A theoretical model, providing an underpinning of the necessity of scaling laws for the emergence of skill has recently been given by (Arora and Goyal, 2023). They analyse "emergence" with the scaling laws as a departure point in a model that links cross-entropy loss in LLMs to basic skills to show that scaling laws enable the model to learn (and generalize) efficiently.\n' +
      '\n' +
      'Strengthening the tie between scaling laws and emergent skill, albeit in the _opposite_ direction, (Michaud et al., 2023) posit that skills that emerge in "quanta" imply a scaling law of the loss. Related, (Chen et al., 2023) assume a hierarchy of skills to derive data curation mechanisms to precipitate the emergence of skills, though they do not allude to scaling laws directly.\n' +
      '\n' +
      '## Appendix B Complimentary Figures for Sections 2, 3 and 4\n' +
      '\n' +
      'Hutter LLM.Figures 8, 9 and 10 further illustrate our theory for simple Hutter LLM.\n' +
      '\n' +
      'Hutter++.We now provide complementary illustrations of predictions made from the theory we have developed for the generalized Hutter models as in Equation (16) in Section 4, without departing from our theoretical assumptions. We also show how theory from the infinite memory model in Section 2 continues to hold in this bigram setting. Figure 11 confirms the scaling law of Theorem 4.2.\n' +
      '\n' +
      'In Figure 3 (middle) we have seen an illustration of the translated scaling curves under n-fold synthesized data in the Hutter LLM. Figure 12 illustrates this phenomenon for the slightly more complex tailed bigram model.\n' +
      '\n' +
      'Figure 8: **Scaling on Hutter LLM for Varying \\(T\\).** Empirical confirmation of Theorem 2.1. Here, \\(\\beta=3/2\\) and error bars correspond to \\(10\\) iid runs of sampling AI-generated data (i.e the distribution \\(q\\)). Broken lines correspond to the Hutter rate \\(T^{-(\\beta-1)/\\beta}\\), for varying \\(k\\) and different values of \\(T\\). Figure 3, left, illustrates the same for varying \\(T\\) and several settings of \\(k\\). Note the perfect match with the theorem.\n' +
      '\n' +
      'Figure 9: **Scaling on Hutter LLM for Varying \\(k\\).** A sample of size \\(T_{0}\\) is used to approximate the true distribution \\(p\\) via \\(p_{\\text{AI}}\\). Then, a Hutter-type model is learned on a sample of size \\(T\\) from \\(p_{\\text{AI}}\\), and evaluated on the true data distribution \\(p\\). Each horizontal line corresponds to the asymptote \\(k^{-\\beta c}\\asymp T_{0}^{-c}\\), for different values of \\(T_{0}\\). The diagonal line corresponds to \\(T^{-c}\\).\n' +
      '\n' +
      'Both Figures 3 (middle) and 12 illustrate the setting where each model consumes as much training data as its predecessor (\\(T_{0}=T\\)). We now relax the assumption that each successive model has strictly the same amount of training data as its predecessor. We assume that the generation 0 model is trained on \\(T_{0}\\) (here, \\(T_{0}=100,000\\)) amount of original data to generate AI data for generation 1. All future generations, starting from generation 2, are trained on data generated by the most powerful model from the previous generation (\\(T=1,000,000\\) data in this case). Figure 13 (for Hutter LLM) and 14 (for Hutter++ on paired bigram data) show the resulting scaling behavior. We take this setting even further by adding a top-p tail cutting mechanism and a temperature scaling mechanism for each synthetic data generation. Figure 6 cuts at \\(p=0.95\\) and Figure 16 at temperature \\(0.9\\).\n' +
      '\n' +
      'We now study mixing of clean and synthesized data in the bigram setting. Figures 17 and 18 add top-p tail-cutting when synthesizing, and start with \\(T_{0}=10,000\\) original data samples, which are successively blended with synthesized data from the largest model. Note that in this setting we observe a reversion of scaling laws with increased AI data. This needs to be compared with the orange curve in Figure 20 in the deterministic Hutter setting. The probabilistic nature of the bigram models leads to a new effect here.\n' +
      '\n' +
      'Figure 11: **Model Collapse for Hutter++.** Empirical confirmation of Theorem 4.2. Here \\(p(j\\mid i)\\) is as in (16), with \\(\\beta=7/5\\). The horizontal broken lines correspond to \\(k^{-\\beta c}\\) for different values of \\(k\\), where \\(c:=\\min(1-1/\\beta,1/2)\\). The diagonal broken line corresponds to \\(T^{-c}\\) (classical error rate without cutoff).\n' +
      '\n' +
      'Figure 12: **Hutter++ Model on Paired Bigram Data.** Empirical confirmation of Theorem 2.4 for probabilistic paired bigram data with \\(\\beta=3/2\\), \\(T_{0}=T\\) across all generations with no additional tail-cutting, regeneration for 9 times. The result verifies the model collapse across generation.\n' +
      '\n' +
      'Figure 10: Empirical Validation of Theorem 3.2. The broken line corresponds to the \\(T^{-(1-1/\\beta)}\\) scaling law that would hold throughout in the absence of pollution. Notice the grokking behavior predicted by the theorem. For this experiment, the Zipf exponent of the true data distribution \\(p\\) is \\(\\beta=2\\).\n' +
      '\n' +
      '## Appendix C Proofs for the infinite memory (Hutter) Model (Sections 2 and 3)\n' +
      '\n' +
      '### Proof of Theorem 2.1\n' +
      '\n' +
      'Observe that the model \\(\\widehat{f}\\) makes an error on \\(i\\) if and only if the \\(i\\)th "skill" never occurred in the training dataset \\(\\mathcal{D}_{T}\\), i.e either (1) \\(i\\geq k+1\\), or (2) \\(1\\leq i\\leq k\\) and \\(i_{t}\\neq i\\) for all \\(t\\in[T]\\). We deduce that\n' +
      '\n' +
      '\\[E_{test}=\\mathbb{P}_{i\\sim p}(\\widehat{f}(i)\\neq y_{i}) =\\sum_{i\\geq k+1}p_{i}+\\sum_{1\\leq i\\leq k}p_{i}(1-p_{i})^{T}\\] \\[\\asymp k^{-(\\beta-1)}+\\sum_{1\\leq i\\leq k}p_{i}e^{-p_{i}T},\\]\n' +
      '\n' +
      'where \\(c:=1-1/\\beta\\in(0,1)\\), and we have used the elementary fact that \\(\\sum_{i\\geq k+1}i^{-\\beta}\\asymp k^{-(\\beta-1)}\\) for large \\(k\\). For the second sum, we will need the following lemma.\n' +
      '\n' +
      '**Lemma C.1**.: _The following identity holds_\n' +
      '\n' +
      '\\[T^{c}\\sum_{i=1}^{k}p_{i}e^{-Tp_{i}}\\asymp\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T)=O( 1), \\tag{22}\\]\n' +
      '\n' +
      '_where \\(\\Gamma(s,x):=\\int_{x}^{\\infty}u^{s-1}e^{-u}\\mathrm{d}u\\) defines the incomplete gamma function. In particular, for \\(k=\\infty\\) and large \\(T\\), it holds that \\(\\sum_{i=1}^{\\infty}p_{i}e^{-Tp_{i}}\\asymp T^{-c}\\)._\n' +
      '\n' +
      'Proof.: Consider the function \\(h(z):=ze^{-Tz}\\) for \\(z\\in(0,1)\\). Its derivative is \\(h^{\\prime}(z)=e^{-Tz}(1-Tz)\\). Thus, \\(h\\) is increasing on \\((0,1/T)\\) and decreasing on \\((1/T,\\infty)\\). Furthermore, note that \\(p_{i}\\leq 1/T\\) iff \\(i\\geq T^{1/\\beta}\\). We deduce that\n' +
      '\n' +
      '\\[\\sum_{i=1}^{k}p_{i}e^{-Tp_{i}}\\asymp\\int_{1}^{k}x^{-\\beta}e^{-Tx^{-\\beta}} \\mathrm{d}x.\\]\n' +
      '\n' +
      'Under the change of variable \\(u=u(x):=Tx^{-\\beta}\\), we have \\(x=x(u)=(u/T)^{-1/\\beta}\\) and so \\(\\mathrm{d}x=-(T^{1/\\beta}u^{-1-1/\\beta}/\\beta)\\mathrm{d}u\\).\n' +
      '\n' +
      'Figure 14: **Empirical Hutter++ Model.** Same setting as in Figure 6. Initial model trained on \\(T_{0}=100,000\\) samples. No top-p inference or temperature scaling is used. \\(\\beta=3/2\\). In this setting, there is mild model collapse coming from the finite sample bias as well.\n' +
      '\n' +
      'Figure 13: **Empirical Hutter LLM.** Bigram model with deterministic labeling function. Initial model trained on \\(T_{0}=100,000\\) samples. It generates \\(T\\) samples for Gen 1. Starting from Gen 2 models are trained on data generated by the most powerful model from the previous generation. \\(\\beta=3/2\\). In this setting, there is mild model collapse coming from the finite sample bias.\n' +
      '\n' +
      'Also \\(u(1)=T\\) and \\(u(k)=Tk^{-\\beta}\\). We deduce that\n' +
      '\n' +
      '\\[\\sum_{i=1}^{k}p_{i}e^{-Tp_{i}} \\asymp\\int_{1}^{k}x^{-\\beta}e^{-Tx^{-\\beta}}\\mathrm{d}x=\\int_{Tk^{ -\\beta}}^{T}(u/T)e^{-u}(T^{1/\\beta}u^{-1-1/\\beta}/\\beta)\\mathrm{d}u\\] \\[\\asymp T^{-(1-1/\\beta)}\\int_{Tk^{-\\beta}}^{T}u^{-1/\\beta}e^{-u} \\mathrm{d}u\\] \\[\\asymp T^{-(1-1/\\beta)}\\left(\\Gamma(1-1/\\beta,Tk^{-\\beta})- \\Gamma(1-1/\\beta,T)\\right)\\] \\[=T^{-c}\\left(\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T)\\right),\\]\n' +
      '\n' +
      'and we are done for the first part.\n' +
      '\n' +
      'For the second part, note that \\(\\Gamma(c,T)=o(1)\\) for large \\(T\\) so that\n' +
      '\n' +
      '\\[(\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T))|_{k=\\infty}=\\Gamma(c,0)-\\Gamma(c,T)=\\Theta (1)-o(1)=\\Theta(1),\\]\n' +
      '\n' +
      'from which the result follows. \n' +
      '\n' +
      'We now consider two separate cases for the relative scaling of \\(k\\) and \\(T\\).\n' +
      '\n' +
      '- Case 1: \\(T\\gtrsim k^{\\beta}\\).Here, we have thanks to Lemma C.1\n' +
      '\n' +
      '\\[E_{test}\\asymp k^{-(\\beta-1)}+O(T^{-c})\\asymp k^{-(\\beta-1)}, \\tag{23}\\]\n' +
      '\n' +
      'since \\(k^{-(\\beta-1)}\\gtrsim T^{-(\\beta-1)/\\beta}=T^{-c}\\).\n' +
      '\n' +
      '- Case 2: \\(1\\ll T\\lesssim k^{\\beta}\\).Here, thanks to Lemma C.1 we have \\(\\Gamma(c,T)=o(1)\\) and \\(\\Gamma(c,Tk^{-\\beta})=\\Theta(1)\\). We deduce that\n' +
      '\n' +
      '\\[E_{test}\\asymp k^{-(\\beta-1)}+T^{-c}\\left(\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T) \\right)\\asymp k^{-(\\beta-1)}+T^{-c}\\asymp T^{-c}, \\tag{24}\\]\n' +
      '\n' +
      'since \\(k^{-(\\beta-1)}\\lesssim T^{-(\\beta-1)/\\beta}=T^{-c}\\). Putting things together then gives the claimed result.\n' +
      '\n' +
      '### Proof of Corollary 2.3\n' +
      '\n' +
      'Indeed, let \\(p_{i}\\propto i^{-\\beta}\\) and \\((p_{AI})_{i}=q_{i}\\propto i^{-\\beta^{\\prime}}\\). Then,\n' +
      '\n' +
      '\\[E_{test}\\asymp\\sum_{i}p_{i}(1-q_{i})^{T}\\asymp\\sum_{i}p_{i}e^{-q_{i}T}\\asymp\\int_{ 1}^{\\infty}x^{-\\beta}e^{-x^{-\\beta^{\\prime}}T}\\mathrm{d}x. \\tag{25}\\]\n' +
      '\n' +
      'Setting \\(u=x^{-\\beta^{\\prime}}T\\) gives \\(x=T^{1/\\beta^{\\prime}}u^{-1/\\beta^{\\prime}}\\), and so \\(\\mathrm{d}x=-(T^{1/\\beta^{\\prime}}/\\beta^{\\prime})u^{-(1+1/\\beta^{\\prime})} \\mathrm{d}u\\). We deduce that\n' +
      '\n' +
      '\\[E_{test} \\asymp T^{-(\\beta-1)/\\beta^{\\prime}}\\int_{1}^{T}u^{\\beta/\\beta^{ \\prime}}u^{-(1+1/\\beta^{\\prime})}e^{-u}\\mathrm{d}u=T^{-(\\beta-1)/\\beta^{ \\prime}}\\int_{1}^{T}u^{(\\beta-1)/\\beta^{\\prime}-1}e^{-u}\\mathrm{d}u\\] \\[\\asymp T^{-c}\\Gamma(c,T)=T^{-c}(1+o(1)),\\text{ with }c:=(\\beta-1)/\\beta^{\\prime}.\\]\n' +
      '\n' +
      'That is, \\(E_{test}\\asymp T^{-c}\\) as claimed. \n' +
      '\n' +
      '### Proof of Theorem 3.2 and Corollary 3.3\n' +
      '\n' +
      'Suppose that of \\(T\\) samples available for training our model, \\(\\pi T\\) are samples from the true distribution \\(p=Zipf(\\beta)\\) and \\((1-\\pi)T\\) are from AI data distribution \\(p^{\\prime}\\) which is a version of \\(p\\) with its tail chopped off at rank \\(k\\), i.e such that\n' +
      '\n' +
      'Figure 19: **S-shape “Smoothed Grokking”.** Bigram data with Hutter++ model, mixing clean data with AI generated data with ratio 50 to 50. The grokking line is smoothed in the probabilistic setting. Line 1, 2, 3 are generated by using 10,000, 1,000, and 100 data to train the generating model. Compared to Figure 17, we do not limit the number of accessible real data now. \\(\\beta=3/2\\).\n' +
      '\n' +
      'Figure 17: **Empirical Hutter++ Model with Mixing.** The initial “clean” dataset comprises \\(T_{0}=10,000\\) samples. For future generations, the largest model is used to synthesize data. For \\(T\\leq 20,000\\), training data is an equal mix of clean and generated data, for \\(T>20,000\\) all clean data is used; the remaining training data is synthetic (so the ratio of clean data diminishes). Top-p 0.9, no temperature scaling, \\(\\beta=3/2\\).\n' +
      '\n' +
      'Figure 18: **Empirical Hutter++ Model with Mixing.** Same setting as in Figure 17 with top p 90, no temperature scaling, and \\(\\beta=3/2\\).\n' +
      '\n' +
      '\\(p^{\\prime}_{i}\\propto p_{i}1[i\\leq k]\\). Thus the dataset is drawn from the distribution given by \\(q_{i}=\\pi p_{i}+(1-\\pi)p^{\\prime}_{i}\\). Test error of a Hutter LLM then writes\n' +
      '\n' +
      '\\[\\begin{split} E_{test}&=\\sum_{i\\geq 1}p_{i}(1-q_{i} )^{T}=\\sum_{1\\leq i\\leq k}p_{i}(1-p_{i})^{T}+\\sum_{i\\geq k+1}p_{i}(1-\\pi p_{i})^ {T}\\\\ &\\asymp\\sum_{1\\leq i\\leq k}p_{i}e^{-p_{i}T}+\\sum_{i\\geq k+1}p_{i} e^{-\\pi p_{i}T}.\\end{split} \\tag{26}\\]\n' +
      '\n' +
      'Now, thanks to Lemma C.1, it is clear that for any integers \\(1\\leq r<R\\leq\\infty\\) and large \\(z\\), one has\n' +
      '\n' +
      '\\[\\sum_{r\\leq i\\leq R}p_{i}e^{-p_{i}z}\\asymp z^{-c}\\left(\\Gamma(c,zR^{-\\beta})- \\Gamma(c,zr^{-\\beta})\\right), \\tag{27}\\]\n' +
      '\n' +
      'where \\(c=1-1/\\beta\\in(0,1)\\) and \\(\\Gamma\\) is the (upper) incomplete gamma function. Applying (27) with \\((r,k,z)=(1,k,T)\\) gives\n' +
      '\n' +
      '\\[T^{c}\\sum_{1\\leq i\\leq k}p_{i}e^{-p_{i}T}\\asymp\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T)=\\begin{cases}\\Theta(1)-o(1)=\\Theta(1),&\\text{ if }1\\ll T\\lesssim k^{\\beta},\\\\ o(1)-o(1)=o(1),&\\text{ if }T\\gtrsim k^{\\beta}\\gg 1.\\end{cases} \\tag{28}\\]\n' +
      '\n' +
      'On the other hand, applying (27) with \\((r,k,z)=(k+1,\\infty,\\pi T)\\) and assuming \\(\\pi=\\Theta(1)\\) gives\n' +
      '\n' +
      '\\[\\sum_{i\\geq k+1}p_{i}e^{-\\pi p_{i}T}\\asymp(\\pi T)^{-c}\\gamma(c,\\pi T(k+1)^{- \\beta})\\asymp\\begin{cases}(\\pi T)^{-c},&\\text{ if }\\pi T\\gtrsim k^{\\beta}\\gg 1,\\\\ (k+1)^{-\\beta c}\\asymp k^{-\\beta c},&\\text{ if }k^{\\beta}\\gg\\pi T.\\end{cases} \\tag{29}\\]\n' +
      '\n' +
      'Putting things together gives the result. \n' +
      '\n' +
      'Recall that Bertrand et al. (2023) also formally study such mixtures for iterative retraining. In their setting, they show the existence of fixed points in the mixture proportion that delineates the region of model collapse. These results are complimentary and not contradictory to ours: they combine mixing, large number of iteration, and data-decay, thus studying a combination of effects (under different theoretical conditions, not focusing on scaling laws) that our preceding theorems address separately.\n' +
      '\n' +
      '### Grokking for Tail Narrowing\n' +
      '\n' +
      '**Theorem C.2** (Grokking with Tail Narrowing).: _Consider a sample of size \\(T\\) of which a proportion \\(\\pi\\) comes from the true distribution \\(p=Zip(\\beta)\\) and the remainder comes from a version \\(p^{\\prime}=Zip(\\beta^{\\prime})\\). We have the following scaling law for the Hutter LLM,_\n' +
      '\n' +
      '\\[E_{test}\\asymp(\\pi T)^{-c}+((1-\\pi)T^{-c^{\\prime}}), \\tag{30}\\]\n' +
      '\n' +
      '_where \\(c:=(\\beta-1)/\\beta\\) and \\(c^{\\prime}:=(\\beta-1)/\\beta^{\\prime}\\)._\n' +
      '\n' +
      '_Define \\(\\overline{T}:=(\\pi/(1-\\pi))^{-a}\\), where \\(a:=s/(1-s)\\), and \\(s:=\\beta/\\beta^{\\prime}\\). Then,_\n' +
      '\n' +
      '_(A) **Early-Stage Dynamics.** For_ \\(T\\lesssim\\overline{T}\\)_, it holds that_ \\(E_{test}\\asymp((1-\\pi)T)^{-c^{\\prime}}\\)_. Thus, if_ \\(\\beta^{\\prime}>\\beta\\)_, the money spent on acquiring some clean data is not amortized!_\n' +
      '\n' +
      '_(B) **Later-Stage Dynamics.** As soon as_ \\(T\\gtrsim\\overline{T}\\)_, it holds that_ \\(E_{test}\\asymp(\\pi T)^{-c}\\)_. Similarly, we recover the unpolluted sample-size law scaling_ \\(T^{-c}\\)_. For fixed_ \\(T\\) _and tunable_ \\(\\pi\\)_, this error rate scales like_ \\(\\pi^{-c}\\)_._\n' +
      '\n' +
      'Proof.Let \\(q\\) be the mixture of \\(p\\) and \\(p^{\\prime}\\). We prove the result for \\(\\beta^{\\prime}\\geq\\beta\\); the case \\(\\beta^{\\prime}\\leq\\beta\\) is analogous. So, one may write\n' +
      '\n' +
      '\\[E_{test}=\\sum_{i\\geq 1}p_{i}(1-q_{i})^{T}\\asymp\\sum_{i\\geq 1}p_{i}e^{-\\pi i^{- \\beta}+(1-\\pi)i^{-\\beta^{\\prime}}}\\asymp\\sum_{1\\leq i\\leq\\overline{T}^{1/\\beta }}p_{i}e^{-\\pi i^{-\\beta}}+\\sum_{i\\geq\\overline{T}^{1/\\beta}}p_{i}e^{-(1-\\pi)i ^{-\\beta^{\\prime}}}, \\tag{31}\\]\n' +
      '\n' +
      'where we have used the fact that \\((1-\\pi)i^{-\\beta^{\\prime}}\\geq\\pi i^{-\\beta}\\) iff \\(i\\leq(\\pi/(1-\\pi))^{-1/(\\beta^{\\prime}-\\beta)}=\\overline{T}^{1/\\beta}\\). The result then follows from (27). \n' +
      '\n' +
      '_Remark C.3_.: Let us conclude by saying that clean data always helps, since \\(E_{test}\\) is decreasing function of \\(\\pi\\). Indeed, from (26), the derivative w.r.t \\(\\pi\\) is \\(E^{\\prime}_{test}(\\pi)=-T\\sum_{i\\geq k+1}p_{i}^{2}(1-\\pi p_{i})^{T-1}\\leq 0\\).\n' +
      '\n' +
      '### An interesting detour: Grokking for Fixed-size AI Dataset.\n' +
      '\n' +
      'Now consider the scenario where the AI synthesized dataset has fixed size \\(T_{AI}\\) (e.g a frozen chunk of the web), while the clean dataset size is a scalable parameter \\(T_{real}\\). Taking \\(T=T_{real}+T_{AI}\\) and \\(\\pi=T_{real}/T\\), we have the following corollary of Theorem 3.2, which includes Corrolary 3.3.\n' +
      '\n' +
      '**Corollary C.4**.: _We have the following._\n' +
      '\n' +
      '_(A) Early-Stage Dynamics. For_ \\(T_{real}\\ll k^{\\beta}\\)_, it holds that_\n' +
      '\n' +
      '\\[E_{test}\\asymp(T_{real}+T_{AI})^{-(1-1/\\beta)}+k^{-(\\beta-1)} \\tag{32}\\]\n' +
      '\n' +
      '_(B) Later-Stage Dynamics. As soon as_ \\(T_{real}\\geq Ck^{\\beta}\\) _(where_ \\(C\\) _is an absolute constant), it holds that_\n' +
      '\n' +
      '\\[E_{test}\\asymp T_{real}^{-(1-1/\\beta)}. \\tag{33}\\]\n' +
      '\n' +
      'As mentioned in Section 3, AI synthesized data is helpful in the regime where real data is scarce. Once more of real data becomes available the model grokks for a while and then forgets the AI synthesized data to recover the normal scaling law w.r.t \\(T_{real}\\). Figure 20 gives an illustration of this phenomenon in various settings.\n' +
      '\n' +
      '### Proof of Theorem 3.1\n' +
      '\n' +
      'Note that explicitly,\n' +
      '\n' +
      '\\[\\pi_{i}\\asymp\\begin{cases}N^{\\alpha}p_{i},&\\text{if }i\\geq N,\\\\ 0,&\\text{else,}\\end{cases} \\tag{34}\\]\n' +
      '\n' +
      'where \\(\\alpha:=\\beta-1\\). This is because the normalization constant is \\(\\sum_{i\\geq N}p_{i}=\\sum_{i\\geq N}i^{-\\beta}\\asymp N^{-\\alpha}\\). Now, mix this distribution with \\(q\\) with equal weights \\(1/2\\), to obtain a new distribution\n' +
      '\n' +
      '\\[q_{i}^{\\prime}=q_{i}/2+\\pi_{i}/2 =\\begin{cases}q_{i}/2,&\\text{if }i\\leq k,\\\\ \\pi_{i}/2,&\\text{if }k\\geq N,\\\\ 0,&\\text{otherwise}\\end{cases} \\tag{35}\\] \\[\\asymp\\begin{cases}p_{i},&\\text{if }i\\leq k,\\\\ N^{\\alpha}p_{i},&\\text{if }k\\geq N,\\\\ 0,&\\text{otherwise,}\\end{cases}\\]\n' +
      '\n' +
      'Figure 20: **Hutter LLM.** true distribution of the data is Zipf with exponent \\(\\beta=2\\). Here, the scalable resource is either clean data or AI data-generated data, corresponding to a version of real data with its tail cut at rank \\(k\\) (here we use \\(k=10\\)). We either mix with a fixed amount (here \\(T^{\\prime}=10^{4}\\) samples) of the other resource, or we don’t mix at all. Then we scale up the scalable resource by cranking up \\(T\\). As predicted by Corollary 3.3, the orange curve always grokks: AI synthesized data is helpful in the regime where real data is scarce; once more of real data becomes available the model grokks for a while and then forgets the AI synthesized data. Note that the green curve (only AI data) and red curve (AI + real data) don’t grokk because the optional resource (real data) is not being scaled; if it is also scaled, then green and red will provably grokk (as in Figure 3). The diagonal broken line corresponds to the standard Hutter scaling law \\(E_{test}\\asymp T^{-c}\\), where \\(c:=1-1/\\beta\\). The horizontal broken lines correspond to \\(E_{test}\\asymp k^{-(\\beta-1)}\\) and \\(E_{test}\\asymp T^{\\prime-c}\\), both predicted by Theorem 2.1.\n' +
      '\n' +
      'For simplicity, assume \\(N\\geq k+1\\) (otherwise, we have all of \\(p\\)). Build a "Hutter" LLM from an iid sample of size \\(T\\) from this distribution (this is equivalent to mixing \\(T\\) samples from \\(q\\) and \\(T\\) samples from \\(\\pi\\). Then, it is easy to see that the test error is given by\n' +
      '\n' +
      '\\[E_{test}=\\sum_{i\\geq 1}p_{i}(1-q_{i}^{\\prime})^{T}\\asymp\\sum_{1 \\leq i\\leq k}p_{i}(1-p_{i})^{T}+ \\tag{36}\\] \\[\\sum_{k+1\\leq i\\leq N-1}p_{i}+\\sum_{i\\geq N}p_{i}(1-N^{\\alpha}p_{ i})^{T}.\\]\n' +
      '\n' +
      'Thanks to previous computations, we know that for large \\(k\\), \\(N\\), and \\(T\\)\n' +
      '\n' +
      '* The first sum is of order \\(T^{-c}\\left(\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T)\\right)=O(T^{-c})\\).\n' +
      '* The third sum is of order \\(T^{-c}\\left(\\Gamma(c,0)-\\Gamma(c,TN^{\\alpha}N^{-\\beta})\\right)=T^{-c}\\left( \\Gamma(c,0)-\\Gamma(c,TN)\\right)\\asymp T^{-c}\\).\n' +
      '* The second sum is of order \\(k^{-\\alpha}-N^{-\\alpha}=((\\frac{N}{k})^{\\alpha}-1)N^{-\\alpha}\\), where \\(\\alpha:=\\beta-1\\).\n' +
      '\n' +
      'We deduce that\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-c}+\\left(\\left(\\frac{N}{k}\\right)^{\\alpha}-1 \\right)N^{-\\alpha},\\text{ for large }k,N,T, \\tag{37}\\]\n' +
      '\n' +
      'and the result follows. \n' +
      '\n' +
      '## Appendix D Proofs for the Tailed Bigram Model (Section 4)\n' +
      '\n' +
      '### Warm-up: Revisiting the Classical Hutter Setup\n' +
      '\n' +
      'As a sanity check, with the framework of Equation (17), let us momentarily consider the non-autoregressive setup where \\(p(\\cdot\\mid i)=\\delta_{y_{i}}\\) for all \\(i\\), as in classical Hutter. Then, an easy computation shows that\n' +
      '\n' +
      '\\[TV(q_{T}(\\cdot\\mid i),p(\\cdot\\mid i))=1-q_{T}(y_{i}\\mid i)+ \\sum_{j\\neq y_{i}}q_{T}(j\\mid i)=2(1-q_{T}(y_{i}\\mid i)).\\]\n' +
      '\n' +
      'Now, by construction, \\(q_{T}(y_{i}\\mid i)=1[i\\in\\mathcal{D}_{T}]\\). Thus,\n' +
      '\n' +
      '\\[\\mathbb{E}\\left[1-q_{T}(y_{i}\\mid i)\\right]=\\mathbb{P}(i\\not\\in \\mathcal{D}_{T})=(1-p_{i})^{T}.\\]\n' +
      '\n' +
      'We deduce that\n' +
      '\n' +
      '\\[\\mathbb{E}\\left[TV(q_{T}(\\cdot\\mid i),p(\\cdot\\mid i))\\right]=2(1-p_{i})^{T}.\\]\n' +
      '\n' +
      'Therefore,\n' +
      '\n' +
      '\\[E_{test}=\\sum_{i}p_{i}\\mathbb{E}\\left[TV(q_{T}(\\cdot\\mid i),p( \\cdot\\mid i))\\right] \\tag{38}\\] \\[=2\\sum_{i}p_{i}(1-p_{i})^{T}\\asymp T^{-(1-1/\\beta)},\\]\n' +
      '\n' +
      'and we recover the classical Hutter result! Thus, our test metric defined in (17) is pointing in the right direction, conceptually.\n' +
      '\n' +
      '### Proof of Theorem 4.1\n' +
      '\n' +
      'The proof will be based on the results of (Berend & Kontorovich, 2012). ()Upper-Bound.Observe that for any choice of mappings \\(\\pi_{1},\\pi_{2},\\ldots\\), we have\n' +
      '\n' +
      '\\[a_{T}(i) :=\\sum_{j\\,|\\,p(j|i)\\leq 1/n_{T}(i)}p(j\\mid i)\\asymp\\sum_{j\\,|\\,\\pi_{i} (j)\\geq n_{T}(i)^{1/\\beta}}\\pi_{i}(j)^{-\\beta}\\leq\\sum_{k\\,|\\,k\\geq n_{T}(i)^{1 /\\beta}}k^{-\\beta}\\asymp n_{T}(i)^{-(1-1/\\beta)}\\] \\[b_{T}(i) :=n_{T}(i)^{-1/2}\\sum_{j\\,|\\,p(j|i)\\geq 1/n_{T}(i)}\\sqrt{p(j\\mid i )}\\asymp n_{T}(i)^{-1/2}\\sum_{j\\,|\\,\\pi_{i}(j)\\leq n_{T}(i)^{1/\\beta}}\\pi_{i}(j )^{-\\beta/2}\\] \\[\\lesssim n_{T}(i)^{-1/2}\\sum_{k\\,|\\,k\\leq n_{T}(i)^{1/\\beta}}k^{- \\beta/2}\\asymp n_{T}(i)^{-c}.\\]\n' +
      '\n' +
      'We deduce that \\(c_{T}(i):=a_{T}(i)+b_{T}(i)\\lesssim n_{T}(i)^{-c}\\) for any \\(i\\). Importantly, the hidden constants don\'t depend on \\(i\\). Therefore, thanks to [Lemma 9] (Berend & Kontorovich, 2012), we have\n' +
      '\n' +
      '\\[E_{test}\\leq\\sum_{i}p_{i}\\mathbb{E}\\left[c_{T}(i)\\right] \\lesssim\\sum_{i}p_{i}\\mathbb{E}\\left[n_{T}(i)^{-c}\\right]\\stackrel{{ (*)}}{{\\leq}}\\sum_{i}p_{i}(\\mathbb{E}\\left[n_{T}(i)\\right])^{-c}= \\sum_{i}p_{i}(Tp_{i})^{-c}=T^{-c}\\sum_{i}p_{i}^{1-c} \\tag{39}\\] \\[\\lesssim T^{-c},\\]\n' +
      '\n' +
      'where we have used Jensen\'s inequality in (*), since the function \\(x\\mapsto x^{-c}\\) is concave.\n' +
      '\n' +
      'Lower-Bound.WLOG6 consider the following specific choice of permutations defined by \\(\\pi_{i}(j)=j\\) (i.e doesn\'t depend on \\(i\\)). Then,\n' +
      '\n' +
      'Footnote 6: A summable series of nonnegative numbers (like in \\(a_{T}(i)\\) and \\(b_{T}(i)\\)) can be reordered without changing the value.\n' +
      '\n' +
      '\\[a_{T}(i) =\\sum_{j\\geq nT(i)^{1/\\beta}}j^{-\\beta}\\asymp n_{T}(i)^{-(1-1/ \\beta)},\\] \\[b_{T}(i) =n_{T}(i)^{-1/2}\\sum_{j\\leq nT(i)^{1/\\beta}}j^{-\\beta}\\asymp n_{T }(i)^{-c}.\\]\n' +
      '\n' +
      'Thanks to the definition of \\(E_{test}\\) and [Proposition 5] (Berend & Kontorovich, 2012), we deduce that if \\(\\beta\\in(1,2)\\), then\n' +
      '\n' +
      '\\[E_{test}\\geq\\sum_{i}p_{i}\\mathbb{E}\\left[(a_{T}(i)+b_{T}(i)-n_{T}(i)^{-1/2}) \\right]\\asymp\\sum_{i}p_{i}\\mathbb{E}\\left[n_{T}(i)^{-c}-n_{T}(i)^{-1/2}) \\right]\\asymp\\sum_{i}p_{i}\\mathbb{E}\\left[n_{T}(i)^{-c}\\right], \\tag{40}\\]\n' +
      '\n' +
      'i.e \\(E_{test}\\gtrsim\\sum_{i}p_{i}\\mathbb{E}\\left[n_{T}(i)^{-c}\\right]\\). Now, since \\(n_{T}(i)\\sim Bin(T,p_{i})\\), standard Binomial concentration arguments tell us that \\(n_{T}(i)\\leq 1.5Tp_{i}\\) w.p \\(1-e^{-Cp_{i}T}\\), where \\(C\\) is an absolute constant. We deduce that\n' +
      '\n' +
      '\\[E_{test}\\gtrsim\\sum_{i}p_{i}(1.5Tp_{i})^{-c}(1-e^{-Cp_{i}T})\\asymp T^{-c} \\sum_{i}p_{i}^{1-c}-T^{-c}\\underbrace{\\sum_{i}p_{i}^{1-c}e^{-Cp_{i}T}}_{o(1)} \\asymp T^{-c},\\]\n' +
      '\n' +
      'which completes the proof. \n' +
      '\n' +
      '### Proof of Theorem 4.2\n' +
      '\n' +
      'It suffices to replace \\(n_{T}(i)\\) in (39) and (40) of the proof of Theorem 4.1 with \\(n_{T}(i)\\wedge k^{\\beta}\\), and use the elementary fact that \\((n_{T}(i)\\wedge k^{\\beta})^{-c}=n_{T}(i)^{-c}\\lor k^{-\\beta c}\\asymp n_{T}(i)^ {-c}+k^{-\\beta c}\\). The rest of the proof proceeds as that of Theorem 4.1.\n' +
      '\n' +
      '### Extensions\n' +
      '\n' +
      'Note that the above setup can be extended to the following\n' +
      '\n' +
      '\\[p(j\\mid i)=\\rho(\\pi_{i}(j)),\\]\n' +
      '\n' +
      'where \\(\\rho\\) is a distribution on \\(\\mathbb{N}_{*}\\). In particular, taking \\(\\rho(z)\\propto z^{-\\beta}\\), recovers the setup considered above. It is clear that mechanics of the proof of Theorem 4.1 should be applicable here, leading to scaling laws which depend explicitly on \\(\\rho\\).\n' +
      '\n' +
      '## Appendix E Proof and Illustration of Triplet Scaling Law (Theorem 5.1)\n' +
      '\n' +
      'For any \\(i\\), on average it takes \\(1/p_{i}\\) iid samples from \\(p\\) to see the context \\(i\\) at least once. The effect of tail-cutting at rank \\(k\\) is effectively to replace the sample size \\(T\\) by \\(\\min(T,T_{k})\\), where \\(T_{k}=\\max\\{1/p_{i}\\mid i\\in[k]\\}\\). In the case where \\(p=Zipf(\\beta)\\), we have \\(T_{k}=1/p_{k}\\asymp k^{\\beta}\\). On other hand the model (18) proposed in (Cabannes et al., 2023) on Zipf data, the test error writes\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-c}+d^{-c_{q}}, \\tag{41}\\]\n' +
      '\n' +
      'where \\(c:=1-1/\\beta\\in(0,1)\\) and the exponent \\(c_{q}\\in(0,\\infty)\\) depends on \\(\\beta\\) and the algorithm \\(q\\) used to update the embeddings in the memory matrix \\(M_{T}\\) in (18). We deduce that tail-cutting at rank \\(k\\) changes the test error to\n' +
      '\n' +
      '\\[E_{test}\\asymp\\min(T,T_{k})^{-c}+d^{-c_{q}}\\asymp T^{-c}+k^{-\\beta c}+d^{-c_{ q}},\\]\n' +
      '\n' +
      'as claimed. \n' +
      '\n' +
      'Figure 21 confirms the Triplet Scaling Law.\n' +
      '\n' +
      'Figure 21: **Capacity-Limited Memory Models.** Empirical confirmation of the Triplet Scaling Law established in Theorem 5.1\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:25]\n' +
      '\n' +
      'Figure 24: **Autoregressive Bigram Model with Perplexity Loss.** Each sequence data have length 100. Initial model trained on \\(T_{0}=10,000\\) samples. It generates \\(T\\) samples for Gen 1. Starting from Gen 2 models are trained on data generated by the most powerful model from the previous generation. Top-p 1, temperature 1, \\(\\beta=3/2\\).\n' +
      '\n' +
      'Figure 25: **Autoregressive Bigram Model with Perplexity Loss.** Each sequence data have length 100. Initial model trained on \\(T_{0}=10,000\\) samples. It generates \\(T\\) samples for Gen 1. Starting from Gen 2 models are trained on data generated by the most powerful model from the previous generation. Top-p 1, temperature 1, \\(\\beta=3/2\\).\n' +
      '\n' +
      'Figure 26: **Autoregressive Bigram Model with Perplexity Loss.** Each sequence data have length 100. Same setting as Figure 25. Top-p 0.9, temperature 1, \\(\\beta=3/2\\).\n' +
      '\n' +
      '## Appendix G Details and Results on Transformer Arithmetic Experiments\n' +
      '\n' +
      'Charton (2023) trains sequence-to-sequence transformers to predict the greatest common divisor (GCD) of two positive integers, encoded as sequences of digits in some base \\(B\\). He observes that model predictions are deterministic: for any pair \\((a,b)\\) with GCD \\(k\\), the model predicts a single value \\(f(k)\\). Predictions are correct (i.e. \\(f(k)=k\\)) when the GCD is a product of divisors of the base, or of small primes. In all other case, the model prediction is the largest correct prediction (i.e. \\(l\\) such that \\(f(l)=l\\)) that divides \\(k\\). The list of correct predictions \\(\\mathcal{L}\\) varies with the encoding base \\(B\\). For instance, for \\(B=10\\), after 300 million examples, the model correctly predicts \\(\\mathcal{L}=\\{1,2,4,5,8,10,16,20,25,40,50,80,100...\\}\\), the GCD of \\(20\\) and \\(30\\) will be correctly predicted as \\(10\\), but the GCD of \\(210\\) and \\(140\\) will be incorrectly predicted as \\(10\\) (instead of \\(70\\)).\n' +
      '\n' +
      'We use these models to generate "dirty" training data \\(\\mathcal{D}(B)\\): uniformly sampled pairs of integers \\((a,b)\\) and their (sometimes incorrect) pseudo-GCD, as generated by a trained transformer using base \\(B\\). Note: this dataset can be as large as we want. We also create a correct training dataset \\(\\mathcal{C}(B)\\), by sampling pairs \\((a,b)\\) and their correct GCD.\n' +
      '\n' +
      'In these experiments, we train models on \\(\\mathcal{D}(B)\\) and \\(\\mathcal{C}(B)\\), for different values of \\(B\\). Our goal is to determine whether extensive training on "dirty" data impacts model accuracy.\n' +
      '\n' +
      'We focus on \\(6\\) bases: \\(B=10,420,1000,2017,2023\\) and \\(4913\\), after training transformers (on correct GCD) over about \\(300\\) millions pairs of integers between one and one million, we achieve the performances listed in Table 1. There, accuracy stands for the proportion of random uniform pairs \\((a,b)\\) that the model can predict correctly, correct GCD is the number of GCD under \\(100\\) that the model correctly predicts (i.e. \\(k\\) such that \\(f(k)=k\\)), and correct model predictions are the products of numbers in the associated sets. These models are used to generate \\(\\mathcal{D}(B)\\).\n' +
      '\n' +
      'In these experiments, all models have four layers, 512 dimensions and 8 attention heads. We consider two architectures: an encoder-only model (17.2M parameters), and an encoder-decoder model (38.7M parameters). The encoder-only model has \\(2.25\\) times less parameters, trains twice as fast, and incurs no performance penalty.\n' +
      '\n' +
      'We then train new models (with the same architecture) to predict GCD, from AI data (generated by the above model), and compare to training with correct data - from correct computation of the GCD. When trained on small number of examples (less than \\(100\\) million), models learning from AI data achieve better accuracy (Table 2). We believe this is due to the fact that AI data smoothes away all the hard case, therefore presenting the model with a cleaner signal in the initial stages.\n' +
      '\n' +
      'This pattern changes after extensive training. Table 3 compares performance of models trained on \\(300M\\) and \\(1\\) billion\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c} \\hline \\hline Base & Accuracy & Correct GCD & Correct predictions \\\\ \\hline\n' +
      '10 & 85 & 13 & \\{1,2,4,8,16\\} & \\{1,5,25\\} \\\\\n' +
      '420 & 97 & 38 & \\{1,2,4,8,16\\} & \\{1,3,9\\} & \\{1,5,25\\} \\\\\n' +
      '1000 & 94 & 22 & \\{1,2,4,8,16\\} & \\{1,5,25\\} & \\{1,3\\} \\\\\n' +
      '2017 & 85 & 4 & \\{1,2\\} & \\{1,3\\} & \\\\\n' +
      '2023 & 91 & 16 & \\{1,2,4\\} & \\{1,3\\} & \\{1,7\\} & \\{1,17\\} \\\\\n' +
      '4913 & 93 & 17 & \\{1,2,4\\} & \\{1,3\\} & \\{1,5\\} & \\{1,17\\} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **Initial performances.**\\(4\\)-layer transformers trained to predict GCD, on 300 million examples. Our _test_ set only contains GCD up to \\(100\\), and accuracy is computed on a reweighted test with equal occurance of each GCD. Thus, the Correct GCD lists all those that can be formed from the correct predictions by forming products across the sets (within the first 100 GCD). We freeze the 0th generation model at this stage and use its prediction to generate synthetic data. For each GCD outside the set of its correct predictions, the model will predict the largest GCD it has learned that divides the ground truth.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c|c c} \\hline \\hline  & \\multicolumn{2}{c}{30M examples} & \\multicolumn{2}{c}{60M examples} & \\multicolumn{2}{c}{90M examples} \\\\ Base & AI & Correct & AI & Correct & AI & Correct \\\\ \\hline\n' +
      '10 & 13 & 13 & 13 & 13 & 13 & 13 \\\\\n' +
      '420 & 34 & 34 & 38 & 34 & 38 & 35 \\\\\n' +
      '1000 & 17 & 13 & 22 & 13 & 22 & 14 \\\\\n' +
      '2017 & 4 & 2 & 4 & 2 & 4 & 4 \\\\\n' +
      '2023 & 6 & 6 & 11 & 6 & 11 & 6 \\\\\n' +
      '4913 & 6 & 4 & 7 & 7 & 7 & 7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **Correctly predicted GCD after 30, 60 and 90 million examples.** Dirty and correct datasets.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:28]\n' +
      '\n' +
      'Figure 31: **Learning the GCD.** Learning curve, base 4319. Orange: training on correct GCD. Blue: training on AI generated data.\n' +
      '\n' +
      'Figure 30: **Emergence of skills (groups of GCDs learned together).** Original (bottom) and AI-synthesized data (top). Base 4913. 1 model for clean/AI data, respectively.\n' +
      '\n' +
      'average curves over the 10 seeds one can discern a grokking-like delayed learning for the mixtures with relatively small amounts of AI data. This effect can be studied\n' +
      '\n' +
      'The models used to generate the data were trained on about 300M examples, and correctly predict 22, 16 and 17 GCD below 100 for bases 1000, 2023 and 4913 respectively. We know (Table 3) that more training on AI-data data only will not improve those performances. On the other hand, we know that models trained on clean data will achieve larger performance. Specifically, out of 10 models trained on clean data, for base 1000, all 10 predict 23 GCD or more after 1.4B examples. The median number of examples needed for the models to predict 23 GCD or more is 465M. For base 2023, 7 models out of 10 predict 17 GCD or more after 2.1B examples. The median number of training samples after which the model bests a model trained on dirty data only is 530M. Finally, for base 4913, 9 clean models out of 10 predict more than 18 GCD after 1.7B examples. The median number of samples is 1.1B.\n' +
      '\n' +
      'When zooming in to when the mixture models learn to predict GCD that are "unlearnable" with an AI-trained model, the grokking effect becomes more apparent.\n' +
      '\n' +
      'Table 4 summarizes by listing the time (# of samples) when the mixture models finally learn a GCD that a purely AI-trained model cannot learn, and the delay (in millions samples) since the previous GCD was learned (see also Figure 30 to illustrate the comparison between the clean and the AI-trained model):\n' +
      '\n' +
      'The delay period increases with increasing fraction of AI data in the mix. Thus, Table 4 clearly demonstrates the grokking effect of increasing plateau length with fraction of AI data, as predicted by our theory7.\n' +
      '\n' +
      'Footnote 7: We were constrained to stop the experiments at after about 3B samples for most, due to heavy use of compute resources. This probably explains why for the larger AI-mixtures only a few experiments could successfully find new GCDs - the other experiments where still in the pre-grokking phase when they were stopped.\n' +
      '\n' +
      '## Appendix H Details of Experiments with Llama2\n' +
      '\n' +
      'In the realm of large language models (LLMs), the prevailing approach involves a pretraining and finetuning paradigm. For instance, GPT-3 undergoes pretraining on approximately 45TB of text data from diverse sources. This extensive pretraining endows it with a robust capability for a variety of downstream tasks, employing methods such as zero-shot learning, few-shot\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c c c} \\hline \\hline  & \\multicolumn{3}{c|}{Base 1000} & \\multicolumn{3}{c}{Base 2023} & \\multicolumn{3}{c}{Base 4913} \\\\ mixture rate & successes & samples (M) & delay & successes & sample (M) & delay & successes & samples (M) & delay \\\\ \\hline\n' +
      '0\\% (clean) & 10/10 & 465 & 243 & 7/10 & 530 & 567 & 10/10 & 1180 & 520 \\\\\n' +
      '9\\% & 8/10 & 560 & 320 & 8/10 & 715 & 530 & 9/10 & 910 & 340 \\\\\n' +
      '27\\% & 5/10 & 790 & 560 & 7/10 & 790 & 1220 & 10/10 & 1390 & 680 \\\\\n' +
      '50\\% & 2/10\\({}^{*}\\) & 1310\\({}^{*}\\) & 190\\({}^{*}\\) & 7/10 & 1140 & 1220 & 8/10 & 1280 & 1180 \\\\\n' +
      '73\\% & 0 & - & - & 0 & - & - & 0 & - & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: **Samples until Mixture Models Learn a GCD that AI-trained Models Cannot Learn.** * small number of experiments\n' +
      '\n' +
      'Figure 32: **Grokking in GCD Learning on mixed data.** Error losses of models trained on mixtures of clean and AI generated GCD data. 10 models. From left to right: base 4913, 2023 and 1000.\n' +
      '\n' +
      'learning, or finetuning. Our study evaluates the phenomenon of model collapse in scenarios close to the contemporary\'synthetic data age.\'\n' +
      '\n' +
      'Utilizing one of the most advanced open-source models, Llama-2 7B, our research investigates the effects on LLMs when they undergo finetuning8 with data generated by other LLMs. To ensure the generation of high-quality data and to provide a relevant but not trivial downstream task, we employ the Wikitext-103 dataset. We segment this dataset into chunks of 128 tokens, between each with a stride of 64 tokens, resulting in approximately 2.2 million chunks. Denote this dataset as \\(\\mathcal{D}_{0}\\). The task for generation involves producing the final 32 tokens given the initial 96 tokens from each chunk in the original dataset. In the initial generation (0-th generation), we use the Llama-2 7B FT model, which has been finetuned on \\(\\mathcal{D}_{0}\\), applying a generation loss that focuses solely on the cross-entropy loss of the final 32 tokens. We denote this initial model as \\(\\mathcal{M}_{0}\\), which demonstrates enhanced capacity for the generation task compared to the standard Llama-2 7B model. By querying \\(\\mathcal{M}_{0}\\) with the original 96 tokens from \\(\\mathcal{D}_{0}\\), we generate the dataset \\(\\mathcal{D}_{1}\\) and subsequently finetune Llama-2 7B on this dataset to obtain \\(\\mathcal{M}_{1}\\). This process is sequentially repeated to generate \\(\\mathcal{D}_{i}\\) from \\(\\mathcal{M}_{i-1}\\) and obtain \\(\\mathcal{M}_{i}\\) through finetuning. By comparing the performance of various \\(\\mathcal{M}\\) models on the test set derived from Wikitext-103, also segmented into 128-token chunks, we aim to investigate the model collapse in LLMs.\n' +
      '\n' +
      'Footnote 8: Quoting (Shumailov et al., 2023), we state that one can, in principle, replicate an experiment described here with training an LLM from scratch to demonstrate scaling law decay. Given that training a single moderately large model produces twice the American lifetime worth of CO2 (Strubell et al., 2019), we opted to not run such an experiment and instead focus on a more feasible finetuning setting. Note that just the language experiments described in the paper took weeks to run.\n' +
      '\n' +
      'To prevent information leakage across chunks, we restrict the training to only include the loss on the final 32 tokens for all generations. Consequently, the models are never trained on the first 96 tokens coming from the original corpus. The size of the 2.2 million chunks can provide sufficient data for finetuning while avoiding overfitting, given the capacity of Llama-2 7B. Throughout the finetuning process, we maintain consistent settings using learning rate \\(5e^{-5}\\) for LoRA, using Adam optimizer, dropout rate 0.1, trainable parameter fraction 0.062%. To eliminate the possibility of model collapse due to insufficient sampling and to gain insights into scenarios where more AI-generated data is produced than the model has been trained (or finetuned) on, we consistently utilize a model trained on half the dataset for generating subsequent datasets.\n' +
      '\n' +
      'For completeness, we include Figure 33 with loss on the full chunks and Figure 34 that mix the generated data with original data. The mixing curve also aligns well with the grokking phenomenon predicted by theory.\n' +
      '\n' +
      'Figure 33: **Llama Generated Data.** Llama2 finetuning when the loss for training and evaluation is the cross-entropy for all tokens in the chunks, including the prompt.\n' +
      '\n' +
      '## Appendix I More Studies on Tail Cutting and Tail Narrowing Effects\n' +
      '\n' +
      'Here, we illustrate how tail cutting in the next-token distribution can lead to tail-narrowing for metrics that take the entire sequence into account, like perplexity. Figure 35 illustrates this for the autoregressive bigram model. This effect is likely due to the combinatorial factors we obtain when considering an additive (or multiplicative) measure like perplexity.\n' +
      '\n' +
      'Figure 35: Sequential bigram data: top p = 0.95 leads to similar effect as tail narrowing. 1000 data with sequence length 100.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 꼬리 이야기: 스케일 법칙의 변화로서 모델 붕괴\n' +
      '\n' +
      'Elvis Dohmatob\n' +
      '\n' +
      'Yunzhen Feng\n' +
      '\n' +
      'Pu Yang\n' +
      '\n' +
      'Francois Charton\n' +
      '\n' +
      'Julia Kempe\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '인공지능 모델 크기가 커짐에 따라, 신경망 스케일링 법칙은 대용량 모델의 개선과 원본(인간 또는 자연) 훈련 데이터의 크기를 증가시킬 때 큰 모델의 개선을 예측하는 중요한 도구가 되었다. 그러나 인기 있는 모델의 광범위한 사용은 온라인 데이터와 텍스트의 생태계가 점진적으로 증가된 양의 합성 데이터를 포함하도록 공동 진화한다는 것을 의미한다. 이 문서에서 우리는 다음과 같이 묻습니다. _합성 데이터가 훈련 코퍼스로 들어가는 불가피한 체제에서 스케일링 법칙은 어떻게 변화할까요?_ 미래 모델이 여전히 개선되거나 전체 _(모델) 붕괴로 전락할 운명입니까? 스케일링 법칙의 렌즈를 통해 모델 붕괴의 이론적 틀을 개발한다. 우리는 광범위한 붕괴 현상, 스케일링 손실 분석, 세대 수에 따른 시프트 스케일링, 기술의 "비학습" 및 인간과 합성된 데이터를 혼합할 때 그로킹(grokking)을 발견한다. 우리의 이론은 Llama2라는 대규모 언어 모델을 사용하여 산술 과제와 텍스트 생성에서 트랜스포머를 사용한 대규모 실험을 통해 검증되었다.\n' +
      '\n' +
      '머신러닝, ICML, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '텍스트, 이미지 및 코드에 대한 생성 AI 알고리즘의 획기적인 발전은 "합성 데이터 시대"를 예고하고 있다. 우리는 점점 더 GPT4(Achiam 등, 2023), Stable Diffusion(Rombach 등, 2022) 및 그 계승자와 같은 대규모 모델에 의해 생성된 데이터를 소비한다. 동시에 대규모 모델의 현재 성공의 핵심 동인은 교육을 위해 방대한 양의 웹 규모 데이터를 소비하는 것이다. 더 큰 모델의 개선은 훈련 데이터의 크기의 힘으로서 오류가 떨어지는 스케일링 법칙에 의해 지배되며, 새로운 기술의 출현은 훈련 데이터의 증가된 스케일을 커버하는 것과 밀접하게 연결된 것으로 보인다. 세계에서의 미래에 대한 우리의 이해는 모델들이 다른 모델들(또는 그들 자신의 모델들)에 대해 트레이닝되었다는 것이었고, 합성된 데이터는 단지 시작 단계에 불과하지만, 몇몇 연구들은 모델 붕괴1이라고 불리는, 학습의 완전한 붕괴 가능성을 나타낸다.\n' +
      '\n' +
      '각주 1: 훈련의 끝에서 마지막-계층 특징들의 클러스터링을 지칭하는 신경 붕괴와 혼동되지 않기(Papyan et al., 2020)\n' +
      '\n' +
      '스케일링 법칙들.음성, 번역, 비전, 비디오, 및 수학적 문제 해결을 포함하는 기계 학습의 많은 도메인들에서, 경험적으로 관찰된 신경 스케일링 법칙들(Hestness et al., 2017; Rosenfeld et al., 2020; Kaplan et al., 2020; Hoffmann et al., 2022; Gordon et al., 2021; Henighan et al., 2021; Aghajanyan et al., 2023)은 테스트 에러가 종종 트레이닝 데이터, 모델 크기, 및 컴퓨트의 양과 함께 멱 법칙으로서 떨어진다는 것을 입증한다. 이론적으로 스케일링 법칙은 다양한 설정(예: Hutter (2021); Cabannes et al. (2023) for "LLM-like" models)에서 파생되었다.\n' +
      '\n' +
      '스케일링 법칙은 더 큰 모델에서 능력의 출현(Wei et al., 2022)과 밀접한 관련이 있으며, 더 작은 모델에서는 존재하지 않는다; 그리고 손실이 감소하면서 나타나는 기술과 관련이 있다(Gordon et al., 2021; Arora & Goyal, 2023). 이것은 "스케일링만 하면 됩니다"라는 현재 일반적인 이야기를 강화합니다.\n' +
      '\n' +
      'Model Collapse.Current LLMs (Devlin et al., 2018; Liu et al., 2019; Brown et al., 2020; Touvron et al., 2023), in\n' +
      '\n' +
      '도 1: (만화) Top-p(핵) 샘플링, LLM 생성의 온도 스케일링 및 유한 샘플 바이어스는 절단되거나 좁아진 "꼬리"(왼쪽)로 이어져 스케일링 법칙의 손실(오른쪽 상단) 및 기술 손실(오른쪽 하단)을 유발한다. 여기에서 우리는 2와 3을 두 가지 기술로 사용하여 최대 공약수(GCD)를 계산하는 것을 시각화한다.\n' +
      '\n' +
      'GPT-4(Achiam et al., 2023)는 주로 인간 생성 텍스트에 대해 학습되었으며, 유사하게 DALL-E(Ramesh et al., 2021), Stable Diffusion(Rombach et al., 2022), Midjourney(Midjourney, 2023)와 같은 확산 모델은 웹 스케일 이미지 데이터 세트에 대해 학습되었다. 이러한 교육 코퍼라는 이미 인터넷에서 사용할 수 있는 모든 깨끗한 데이터를 잠재적으로 고갈시킬 수 있습니다. 이러한 점점 더 인기 있는 모델로 생성된 합성 데이터의 수가 증가하기 시작하여 종종 "실제" 데이터와 구별할 수 없는 웹을 채우기 시작한다. 따라서 우리는 이미 우리의 훈련 말뭉치가 합성 데이터와 돌이킬 수 없을 정도로 뒤섞여 있는 미래로 달려갔고 이 상황은 더 악화될 것이다. 최근 연구에서는 결과 모델의 잠재적인 극적인 악화에 주의를 기울이고 있으며, 이는 _"모델 붕괴"_라고 하는 효과이다(Shumailov et al., 2023). 이 현상의 요인은 다양한 설정에서 _경험적으로_ 입증되었다(Hataya 등, 2023; Martinez 등, 2023; Bohacek and Farid, 2023; Briesch 등, 2023; Guo 등, 2023). 이론적으로, 반복적 훈련이 자가 생성(또는 혼합) 데이터에 미치는 영향을 분석하기 위한 몇 가지 작업이 등장하고 있다(관련 작업 참조): (Shumailov et al., 2023) co the term _"model collapse"_ to the complete reversion to the mean, Alemohammad et al. (2023) analysis _"self-consuming loop"_ and Bertrand et al. (2023) shows iterative synthetic training leads to _"clueless generator"_.\n' +
      '\n' +
      '첫 번째 경고 표지판이 준비되면 다음과 같이 묻습니다\n' +
      '\n' +
      '_현재 스케일링 패러다임이 훈련 코퍼스 내의 합성 데이터에 의해 어떻게 영향을 받습니까?_\n' +
      '\n' +
      '이를 위해 우리는 LLM 스타일 모델의 스케일링 동작을 주의 깊게 확대한다. 스케일링 법칙의 이론적 유도는 입력 피처("헤비 테일 인, 파워 스케일링 법칙 아웃")에서 항상 헤비 테일 분포(파워-법칙, 일명 Zipf)를 가정한다. 이 분포는 형식입니다.\n' +
      '\n' +
      '\\[p_{i}\\propto i^{-\\beta},\\ \\ i=1,2,\\ldots \\tag{1}\\]\n' +
      '\n' +
      '이러한 분포는 Zipf 법칙(Zipf, 1935)의 단어 빈도 분포부터 생물학적 자료, 지진 규모, 금융 자료 등에 이르기까지 자연 데이터 세트에서 어디에나 존재한다. - LLM과 같은 대규모 모델에서 소비되는 데이터입니다. 그러나 AI 모델은 그러한 데이터에 대해 훈련될 때 어떤 분포를 생성합니까? 그림 2는 대규모 LLM(Llama2-7B)과 산술 과제에 대해 훈련된 변압기 모델에 대한 경험적 답을 제공한다. 중꼬리 데이터의 재생은 (1) 분포의 꼬리를 "절단"하고/하거나 (2) 꼬리를 "좁게"하는 두 가지 가능한 방식으로 분포에 영향을 미친다(만화 삽화는 그림 1 참조). 이를 유도하는 메커니즘은 유한 샘플링 편향(Shumailov et al. (2023)에서 이미 제안된 바와 같이)-Zipf 설정에서 유도를 위해 섹션 2를 참조)을 제외하고 모델의 생성 알고리즘에서 의도적인 선택에서 비롯된다: 추론에서 잘린 다음 토큰 예측을 통한 LLM에서 (예를 들어, 온도를 낮추어 확률 분포를 집중시키는 _top_ 또는 _top__\\(k\\)_ 잘림을 통해 더 가능성 있는 토큰을 선택하는 것), 잘림을 통한 GAN과 같은 비전 모델에서 또는 지침을 통한 확산 모델에서.\n' +
      '\n' +
      '**주요 기여의 요약** 주요 이론적 기여에 대한 고수준의 요약을 제시하며, 그 중 일부는 그림 3에서 강조된다. 이러한 이론적 예측을 경험적으로 검증한다(그림 4 참조). (1) 최대 공약수를 예측하도록 훈련된 변압기 모델(차톤, 2023)에 대해 Wikitext-103의 대략 \\(2M\\) 샘플 데이터 세트에 대한 LLM, 미세 조정 Llama2-7B(Touvron 등, 2023)에 대한 대규모 실험에서 이러한 이론적 예측을 경험적으로 검증한다.\n' +
      '\n' +
      '식 (1)과 같이 참 분포를 가정하면 AI 데이터 생성 데이터의 크기 \\(T\\)의 데이터셋에 대해 모델을 학습시키는 것을 고려한다. 합성된 데이터는 테일이 일부 유한 랭크 \\(k\\)에서 절단되거나 테일이 더 작은 지수로 좁아진 실제 데이터 분포의 버전에 해당한다. 주요 연구결과는 다음과 같다.\n' +
      '\n' +
      '_(1) 이중 스케일링 법칙._ 단순화(비자기회귀) LM(Hutter, 2021) 및 완구 비그램 LLMs에서 모델 붕괴를 설명하는 새로운 스케일링 법칙을 수립합니다.\n' +
      '\n' +
      '그림 2: **AI 생성 데이터의 꼬리: 상단.** Llama2–7B를 앵커 모델로 사용하여 측정한 위키텍스트-103 테스트 세트의 복잡성 다이어그램입니다. 위키텍스트-파인튜닝된 라마2-7B에 쿼리하여 AI 데이터를 생성하며, 이는 원래 집합과 비교된다. 복잡도는 AI 및 원본 데이터 세트 모두에서 생성된 위치에 대해서만 계산된다. AI 데이터는 \\((p,T)\\)의 다양한 설정에 대해 생성됩니다. **아래.** 랜덤 정수 쌍의 최대 공약수 분포(원래 데이터(청색) 크기 조정 \\(p(GCD=k)\\propto k^{-2}\\))입니다. 변압기는 \\(300M\\) 샘플에 대해 이 작업에 대해 훈련되고 무작위로 샘플링된 정수 쌍의 테스트 세트에서 생성기로 사용되어 절단된 GCD 분포를 제공한다.\n' +
      '\n' +
      '(정리 2.1 및 4.2 참조)2\n' +
      '\n' +
      '각주 2: 표기 \\(f(T)\\lesssim g(T)\\)는 충분히 큰 \\(T)와 절대 상수 \\(C)\\에 대해 \\(f(T)\\leq Cg(T)\\)를 의미하는 반면, \\(f(T)\\asymp g(T)\\)는 \\(f(T)\\lesssim g(T)\\lesssim f(T)\\를 의미한다.\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-c}+k^{-c^{\\prime}}. \\tag{2}\\]\n' +
      '\n' +
      '또는 등가적으로(Corollary 2.2 참조) 유한 표본 유도 컷오프 \\(k=k(T_{0})\\)에 대해 생성 모델이 \\(T_{0}\\) 데이터 양에 대해 훈련될 때 \\(E_{test}\\asymp T^{-c}+T_{0}^{-c^{\\prime\\prime}}\\) 여기서 지수 \\(c,c^{\\prime},c^{\\prime\\prime}\\)는 참 분포의 꼬리 거동에만 의존합니다. 이 결과는 그림 3에 나와 있다.\n' +
      '\n' +
      'AI-"tail-narrowing"의 경우, 데이터가 더 작은 지수 \\(\\beta^{\\prime}\\in(1,\\beta)\\로 무거운 꼬리로 남아 있을 때, 하류 Hutter LLM은 (Corollary 2.3)으로 스케일링될 것이다.\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-(\\beta-1)/\\beta^{\\prime}}. \\tag{3}\\]\n' +
      '\n' +
      '_(2) 메모리 제한 모델에 대한 트리플 스케일링 법칙._ 우리는 (Cabannes et al., 2023)에서 연구된 단순 연상 기억 모델을 고려하고 (정리 5.1) 형태의 새로운 스케일링 법칙을 설정한다.\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-c}+d^{-c_{q}}+k^{-c^{\\prime}}, \\tag{4}\\]\n' +
      '\n' +
      '여기서 \\(d\\)는 임베딩 차원이고 모델 용량에 대한 s 프록시를 제공하며, 지수 \\(c_{q}\\)는 \\(\\beta\\)와 훈련 중에 모델의 메모리를 업데이트하는 데 사용되는 특정 알고리즘 \\(q\\) 모두에 의존한다.\n' +
      '\n' +
      '_(3) 다수의 세대들에 걸친 모델 붕괴._ 모델의 각 세대가 이전 세대에 의해 생산된 데이터를 소비하는 AI 데이터 생성(11)의 \\(n\\)-폴드 재귀에 대해, 우리는 형식의 보편성 원리를 확립한다.\n' +
      '\n' +
      '\\[E_{test}=E_{test}^{clean}+n\\times\\text{ new scaling terms}, \\tag{5}\\]\n' +
      '\n' +
      '여기서 \\(E_{test}^{clean}\\)는 깨끗한 데이터(AI 생성 아님)에 대해 학습된 모델의 일반적인 테스트 오류입니다. 이는 식 (2)와 식 (4)에서 \\(k^{-c^{\\prime}}\\)가 \\(nk^{-c^{\\prime}}}\\)로 대체됨을 의미한다. 이러한 곱셈적 저하에 대한 한 가지 가능한 해석은 시간이 지남에 따라(즉, 세대 수가 증가함에 따라) 야생에서 대규모 언어 모델(ChatGPT와 같은)의 영향이 학습이 불가능할 때까지 웹의 오염이 될 것이라는 것이다. 이는 클린/비-AI-생성 데이터의 가치 및 비용을 증가시킬 가능성이 있다.\n' +
      '\n' +
      '_(4) 완화 전략._ 정리 3.2에서 우리는 AI 생성 데이터와 소량의 깨끗한 데이터를 혼합하면 그로킹 현상을 도입하여 모델 붕괴를 완화한다는 것을 보여준다. 고원의 길이는 \\(k^{\\beta}/\\pi\\) 순서이며, 여기서 \\(\\pi\\)는 실제 분포(즉, 깨끗한 데이터)에서 나온 훈련 데이터의 비율이다. (\\pi=0\\)(즉, AI 생성 데이터만 사용 가능)이 \\(\\pi=0\\)일 때, 이 고원은 (2) 및 (4)에서와 같이 영원히 지속됩니다. \\(\\pi>0\\)이 작더라도 안정기는 마침내 멈추고 오차는 계속 감소한다. 이 그로킹 현상은 (Hutter (2021); Cabannes et al. (2023)의 모델에서와 같이) _deterministic_ Ground truth 레이블의 설정에서 성립합니다. 변압기 모델의 경우, 이러한 결정론적 설정이 산술 작업에서 예를 들어 발견되며, GCD 변압기 실험에서 이를 경험적으로 입증한다. 그로킹 효과는 확률적 설정에서 감쇠되며, 여기서 S자형 학습 곡선으로 이어질 수 있다(도 19 참조). 또한 AI 데이터를 추가하는 것이 도움이 될 수 있는 체제를 식별하고 AI 데이터 효과를 완화하기 위해 "꼬리" 데이터를 선별하는 방법을 논의한다.\n' +
      '\n' +
      '관련 작업 이론적으로 스케일링 법칙은 비모수 모델(Schmidt-Hieber, 2017; Suzuki, 2019; Bordelon et al., 2020), 가우시안 설계 하의 커널 체제(Spigler et al., 2020; Cui et al., 2021, 2022; Maloney et al., 2022) 또는 이산 데이터를 갖는 암기 유사 설정(Hutter, 2021; Debowski, 2023; Michaud et al., 2023)에서 다양한 설정으로 도출되었다. 유한 모델 용량과 최적화를 고려하여 Cabannes et al.(2023)은 최근 제약 용량 연관 메모리에서 스케일링 법칙을 증명하였고, 우리의 Triplet 스케일링 법칙은 이 작업을 기반으로 한다.\n' +
      '\n' +
      '소수의 작품도 "합성 데이터 시대"에서 모델의 행동에 대한 이론적 설명을 제공하기 시작한다. (Shumailov et al., 2023) 속성 모델은 두 가지 메커니즘으로 붕괴된다: 유한 샘플링 바이어스가 낮은 확률 "꼬리"를 차단하여 점점 더 많은 피크 분포 및 함수 근사 오차로 이어진다; 이들은 이론적으로 (단일) 가우시안 사례를 분석하고 VAE, 가우시안 혼합물 및 OPT 언어 모델(125M 파라미터)에 대한 경험적 증거를 제공한다. 비전 모델의 맥락에서 Alemohammad et al. (2023)은 각 세대에서 데이터의 분산을 좁히는 샘플링 바이어스를 도입하여 "_자기 소비 루프"_를 분석하고, GAN에 대한 경험적 입증 및 확산 확률 모델의 노이즈 제거 외에도 가우시안 모델에 대한 이론적 분석을 제공한다. 마지막으로, VAE(Kingma & Welling, 2014), 확산 모델 및 정규화 흐름에 적용 가능한 생성 모델에서 안정성 및 유효성에 대한 데이터 구성의 중요한 역할을 밝히는 Bertrand et al.(2023)의 연구를 언급한다. 그들은 실제 분포를 나타내는 깨끗한 데이터와 생성기의 이전 반복에서 합성된 데이터의 혼합을 포함하는 시나리오를 탐구한다. 그들의 분석은 데이터 믹스가 합성 데이터로만 구성된 경우 생성 프로세스가 시간이 지남에 따라 퇴보할 가능성이 있음을 보여준다(_"무모한 생성기"_). 반복에 걸쳐 고정 소수점 분석을 사용하면 믹스에서 깨끗한 데이터의 비율이 충분히 높을 때 생성기가 특정 기술적 조건에서 학습 능력을 유지한다는 것을 알 수 있다. 최근 논문(Fan et al., 2023)은 텍스트-이미지 모델에 대한 합성 데이터에 대한 열화 스케일링 법칙 훈련을 경험적으로 관찰한다. 3\n' +
      '\n' +
      '각주 3: 관련 및 선행 작업에 대한 보다 상세한 설명은 부록 A에서 찾을 수 있다\n' +
      '\n' +
      '우리가 아는 한, 우리의 작업은 AI 데이터 유발 현상의 풍부한 새로운 풍경을 제공하기 위해 스케일링 법칙과 창발 능력의 맥락에서 모델 붕괴를 이론적이고 경험적으로 분석한 최초의 것이다.\n' +
      '\n' +
      '## 2 A 결정적 무한 메모리 모델\n' +
      '\n' +
      '여기서는 Hutter(2021)가 연구한 (i) _무한 기억_ 및 (ii) _결정론적 그라운드 트루스_ 레이블링 함수 \\(i\\mapsto y_{i}\\)의 가장 간단한 경우에 대한 이론의 핵심을 제시한다. 두 제한 모두 나중에 섹션에서 해제되며, 여기서 우리는 또한 _확률적 자기회귀_ 버전(섹션 4) 및 _제한된 메모리_ 모델(섹션 5)을 분석한다. 토큰 \\(i\\)는 식 (1)의 Zipf 법칙에 따라 그려지며, 이는 예를 들어 언어의 다양한 메트릭의 분포를 모델링한다. 멱법칙의 출현에 대한 또 다른 해석은 Michaud et al. (2023)의 "양자화 가설" 논문에 의해 제시된다: 예를 들어, 문제를 해결하는 데 필요한 각각의 \\(i\\)를 별개의 기술로 생각할 수 있으며, 따라서 스킬은 다른 속도 \\(p_{i}\\)에서 발생한다. 형상 매개변수 \\(\\beta>1\\)는 이 분포의 꼬리의 길이를 제어합니다. \\(\\beta\\)의 더 큰 값은 더 긴 꼬리에 해당합니다.\n' +
      '\n' +
      '### 모델 붕괴를 유발하는 요인?\n' +
      '\n' +
      'Tail Cutting. 언급된 바와 같이, AI 생성 알고리즘(top-\\(p\\) 또는 top-\\(k\\) 다음 토큰 예측과 같은)에서 의도적인 선택은 즉시 \\(k\\)에서 잘린 꼬리로 이어진다. 기술로 볼 때, 우리는 \\(k\\) 번째로 빈번한 결과("기술")만을 고려한다고 말할 수 있다. 그러나 의도적으로 꼬리를 절단하지 않는 경우에도 훈련 집합(샘플링 바이어스)의 유한 크기 \\(T_{0}\\)는 효과적인 꼬리 절단을 유도한다. 이것은 다음과 같이 볼 수 있다: 크기 \\(T_{0}\\)의 iid 데이터 세트를 샘플링하고 히스토그램 \\(p_{\\text{AI}}\\)을 추정한다; 이 새로운 분포는 AI 데이터 생성자의 역할을 한다. 정수 \\(i\\)는 평균적으로 \\(T_{0}p_{i}\\)인 횟수 \\(p_{\\text{AI}}\\)의 지원에 나타난다. 대략적으로 말해서, 이것은 \\(p_{\\text{AI}}\\)의 지원이 \\(\\{i\\mid p_{i}\\leq C/T_{0}\\}=\\{i\\mid i\\leq k\\}\\)임을 의미하고, 여기서\n' +
      '\n' +
      '각주 4: 이것은 표준 농도 인수를 통해 엄격하게 만들어질 수 있다.\n' +
      '\n' +
      '\\[k=k(T_{0})\\asymp T_{0}^{1/\\beta}. \\tag{6}\\]\n' +
      '\n' +
      '따라서, 변환 \\(p\\to p_{\\text{AI}}\\)은 순위 \\(k\\)에서 \\(p\\)의 꼬리를 잘라내는 것에 해당하며, 여기서 \\(k\\)는 위에서 주어진 것과 같다.\n' +
      '\n' +
      'Tail Narrowing.그림 2 (Llama2의 경우)는 꼬리 절단 외에도 AI 생성 동안 꼬리 좁힘 효과가 발생함을 보여준다. 이에 대한 한 가지 메커니즘은 다음 토큰 예측 동안 온도를 낮추는 것이다. (i\\)번째 토큰인 \\(p_{i}=e^{z_{i}}/\\sum_{j}e^{z_{j}}\\에 대한 로짓 \\(z_{i}\\)에 대한 소프트맥스 분포를 가정합니다. 일반 온도에 대한 \\(q_{i}^{T}=e^{z_{i}/T}/\\sum_{j}e^{z_{j}/T}\\)를 정의합니다. 그리고 \\(p_{i}\\asymp i^{-\\beta}\\)는 \\(q_{i}^{T}\\asymp i^{-\\beta/T}\\) (1차)로 변한다. 우리는 온도 스케일링이 \\(T>1\\)에 대한 꼬리의 좁힘을 직접적으로 유발한다는 것을 알 수 있다. 다른 메커니즘이 작용할 수 있는데, 예를 들어, 복잡성을 갖는 자기회귀 모델의 경우, 토큰-와이즈 테일 절단은 시퀀스-퍼플렉시스에 대한 테일 좁힘을 초래할 수 있다(도 35 및 부록 I의 논의 참조).\n' +
      '\n' +
      '### Hutter LLM에서의 새로운 스케일링 법칙\n' +
      '\n' +
      '결정론적 지상-진실 레이블링 함수 \\(i\\mapsto j_{i}\\)에 대해, 다운스트림 Hutter "LLM"(Hutter, 2021)을 고려한다.\n' +
      '\n' +
      '\\[\\widehat{f}(i):=\\begin{cases}j_{i},&\\text{ if }(i,j_{i})\\in\\mathcal{D}_{T},\\\\ \\perp,&\\text{ otherwise,}\\end{cases} \\tag{7}\\]\n' +
      '\n' +
      'constructed on a sample \\(\\mathcal{D}_{T}:=\\{(i_{t},j_{t})\\mid t\\in[T]\\}\\) of size \\(T\\) from unmitigated Zipf distribution \\(p\\) (1), the test error comply to following scaling law Hutter (2021)\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-(1-1/\\beta)}. \\tag{8}\\]\n' +
      '\n' +
      '이제, \\(q\\)를 k-꼬리 절단 버전의 \\(p\\), 즉, \\(i\\leq k\\) 및 \\(q_{i}=0\\)이면 \\(q_{i}\\asymp p_{i}\\)로 하자. 크기 \\(T\\)의 \\(\\mathcal{D}_{T}\\)에 "훈련"을 구성하면 이제 \\(q\\)에서 이 모델의 테스트 오류(w.r.t에서 실제 데이터 분포 \\(p\\))는 다음과 같습니다.\n' +
      '\n' +
      '\\[E_{test}:=\\mathbb{P}_{i\\sim p}(\\widehat{f}(i)\\neq j_{i})=\\sum_{i\\geq 1}p_{i} \\mathbb{P}(\\widehat{f}(i)\\neq j_{i}). \\tag{9}\\]\n' +
      '\n' +
      '즉, 인공지능 분포(q\\)로부터 데이터를 학습하고, 원래의 분포(p\\)를 테스트한다. 우리는 꼬리 절단을 위한 다음의 스케일링 법칙을 증명한다(모든 증명은 부록 C로 강등됨):\n' +
      '\n' +
      '**정리 2.1**.: _지수 \\(\\beta>1\\)가 있는 롱테일 실제 데이터를 고려하고 AI 생성 데이터에 대한 컷오프를 \\(k\\)로 둡니다. 그런 다음 AI의 큰 \\(k\\) 및 \\(T\\) 샘플에 대해 다운스트림 "LLM"의 테스트 오류는 \\(E_{test}\\asymp T^{-(\\beta-1)/\\beta}+k^{-(\\beta-1)}\\asymp\\min(T,k^{\\beta})^{-(\\beta-1)/\\beta}\\와 같다._\n' +
      '\n' +
      '따라서, \\(T\\gtrsim k^{\\beta}\\)는 AI 생성 샘플 크기 \\(T\\)가 "확장 가능한" 리소스가 되는 것을 중단한다: 더 많은 AI 생성 샘플을 수집한다고 해서 다운스트림 모델의 성능이 향상되지 않을 것이며, 즉 성능 플래토스와 스케일링이 손실된다. 결과는 그림 3, 왼쪽 및 그림 8(부록 B)에서 경험적으로 설명된다.\n' +
      '\n' +
      'AI-발생기 자체가 \\(T_{0}\\) 샘플에 대해 훈련되었다고 가정할 때, 유한 샘플링 편향으로부터 꼬리 절단으로부터 유래하는 스케일링 손실의 유사한 손실을 얻는다(식 (6):\n' +
      '\n' +
      '**상표 2.2** ("유한 초기 샘플 크기"): _ \\(c=1-1/\\beta\\)를 사용 하 여 다음과 같습니다._\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-c}+T_{0}^{-c}. \\tag{10}\\]\n' +
      '\n' +
      '이러한 이론적 내용은 그림 9에서 실증적으로 확인된다.\n' +
      '\n' +
      '꼬리 좁힘의 경우, 스케일링 거동은 변한다; 고원 대신에, 우리는 더 느린 감쇠율을 얻는다:\n' +
      '\n' +
      '**연상 2.3** ("꼬리 좁힘"): _정리 2.1의 설정에서 AI 생성 데이터를 더 작은 지수 \\(\\beta^{\\prime}\\in(1,\\beta)\\이지만 긴 꼬리 데이터라고도 간주합니다. 그런 다음 AI 생성 데이터로 훈련된 다운스트림 Hutter LIM은 \\(E_{test}\\asymp T^{-(\\beta-1)/\\beta^{\\prime}}\\)로 확장된다.__\n' +
      '\n' +
      '### AI 데이터의 다중 생성 중 붕괴\n' +
      '\n' +
      '이제 여러 세대에 걸쳐 스케일링의 이전 손실의 누적 영향을 조사한다. \\(n\\)-폴더 재귀형 AI 데이터 생성, 즉\n' +
      '\n' +
      '\\[p\\to p_{\\text{AI}(1)}\\to p_{\\text{AI}(2)}\\to\\ldots\\to p_{\\text{AI}(n)}. \\tag{11}\\]\n' +
      '\n' +
      '각 화살표는 크기 \\(T_{0}\\)의 샘플을 그리는 것에 대응한다. (10)으로 이어지는 인수의 \\(n\\)배를 반복하면, \\(n\\) 세대의 \\(T\\) 샘플에 대한 학습 및 실제 데이터 분포에 대한 테스트를 위한 테스트 오류 \\(E_{test}^{(n)}=E_{test}^{(n)}(T)\\에 대해 다음과 같은 스케일링을 얻으며,\n' +
      '\n' +
      '\\[\\begin{split} E_{test}^{(n)}&\\asymp T^{-c}+\\underbrace {T_{0}^{-c}+\\ldots+T_{0}^{-c}}_{n\\text{ times}}\\\\ &=T^{-c}+C_{n}T_{0}^{-c}=T^{-c}\\left(n(T/T_{0})^{c}+1\\right), \\end{split} \\tag{12}\\]\n' +
      '\n' +
      '여기서 \\(c:=1-1/\\beta\\). 우리는 다음과 같은 결과를 추론한다.\n' +
      '\n' +
      '**정리 2.4** (형식).: _모델 붕괴(문헌에서 언급한 대로)는 iff \\(n\\gg(T_{0}/T)^{c}\\)가 발생합니다._\n' +
      '\n' +
      '_예를 들어, \\(T_{0}\\gg T\\)(e.g \\(T_{0}\\geq CT\\log T\\)) 및 \\(n\\)가 상수(e.g \\(n=25\\))인 경우, AI 데이터의 \\(n\\) 생성에 대해 학습하면 모델 붕괴가 발생하지 않는다. 반면에 \\(T_{0}\\lesssim T\\)이면 결국 모델 붕괴가 발생한다._\n' +
      '\n' +
      '특히 \\(T_{0}\\asymp T\\)를 취하면 다음과 같다.\n' +
      '\n' +
      '\\[E_{test}^{(n)}\\asymp C_{n}T^{-c}\\asymp nT^{-c}. \\tag{13}\\]\n' +
      '\n' +
      '손실이 세대 수에 따라 선형적으로 확장되는 방법에 유의하십시오. 도 3, 중간, 증가된 숫자를 설명하는 방법\n' +
      '\n' +
      '그림 4: **실험 결과(섹션 6의 세부 정보). 왼쪽 줄거리 위키텍스트-103 데이터 세트에서 라마2-7B를 미세 조정하기 위한 스케일링 법칙 ’ 0-gen\'은 원본 데이터를 활용하는 반면, 후속 세대는 이전 모델에 의해 생성된 데이터를 활용한다. 중간 줄거리 두 정수의 GCD를 예측하도록 훈련된 변압기 모델의 스케일링 법칙. 데이터는 \\(300K\\) 샘플에 대해 훈련된 0세대 모델에서 합성된다. 이론에 의해 예측된 합성 데이터에 대해 훈련된 모델의 테이퍼 오프 스케일링을 주목한다. GCD 변환기가 원본(하단) 및 AI 데이터(상단)에서 학습한 기술”(새로운 GCD의 폭발)입니다. 우리는 스케일링의 소멸이 깨끗한 데이터에 대해 훈련된 모델에 의해 숙달된 능력의 소멸로 이어지는 방법을 확인합니다.**\n' +
      '\n' +
      '그림 3: **단순화된 LLM에 대한 주요 결과의 그림**. 왼쪽 줄거리 이중 스케일링 법칙에 대한 실증적 확인. 데이터의 참분포는 지수 \\(\\beta=3/2\\)를 갖는 Zipf이다. 파선은 \\(k^{-(\\beta-1)}\\)에 해당하며, \\(T\\)와 \\(k\\)의 값을 달리하면 다음과 같다. 중간 줄거리 모델이 여러 세대에 걸쳐 무너집니다. 다시 \\(\\beta=3/2\\), \\(T_{0}=T\\) 모든 세대에 걸쳐 추가 꼬리 절단 없이 5회 재생한다. 올바른 줄거리. 정리 3.2에 의해 완벽하게 예측되는 그로킹 거동에 주목하라. 실제 데이터의 비율에 대해 주어진 값 \\(\\pi\\)에 대해, 파선은 점근선 \\(E_{test}\\asymp(\\pi T)^{-c}\\)이고 각 안정기는 정리에 의해 예측되는 차수 \\(k^{d}/\\pi\\)의 길이를 갖는다. 다른 \\(k\\) 값이 있는 유사한 결과는 그림 10을 참조 하세요.** 세대의 손실 크기 조정 곡선은 점진적으로 오른쪽으로 이동 합니다. 이것은 결국 모델 붕괴로 이어진다.\n' +
      '\n' +
      '## 3 데이터 혼합을 통한 모델 붕괴 완화\n' +
      '\n' +
      '여기서는 AI 오염 데이터를 보완하기 위해 실제 데이터 분포에서 적은 양의 데이터라도 획득하여 모델 붕괴를 완화할 수 있는 가능성을 탐구한다. 두 가지 현상을 연구한다 : (1) 원본 데이터의 \\(\\pi\\)-분율과 AI 생성 데이터의 \\((1-\\pi)\\)분율을 혼합할 경우, 원본 모델의 스케일링 법칙에 따라 최종적으로 다시 감소하기 위해 증가하는 훈련 데이터로 테스트 손실 고원(test loss plateaus)을 테스트하는 놀라운 "grokking" 현상을 보이고, (2) 누락된 "tail"을 보상하고자 하는 시나리오에서는 원본 분포의 tail에서 일부 데이터를 획득하여 주의 깊게 수행할 필요가 있음을 보여준다: tail에서 "너무 깊은" 데이터를 얻는 것은 무의미하지만 정확한 "missing" tail에 더 가까운 데이터는 유익할 수 있다. 모든 증명은 부록 C에서 찾을 수 있다.\n' +
      '\n' +
      '### 누락 테일 획득\n' +
      '\n' +
      '꼬리 절단의 효과와 스케일링에서 발생하는 고원에 대응하기 위해 꼬리를 강조하는 선별된 데이터를 추가하는 데 의존할 수 있다. 다음 정리 3.1은 이 효과를 연구하는데, 특히 우리가 "오버슈트"하고 너무 깊은 큐레이트 꼬리만 한다면 우리의 노력은 무의미할 것임을 보여준다. 오히려 잘린 꼬리 \\(k\\)( \\((1+o(1))\\)의 계수 내에서 \\(k\\)) 주위에 미세한 선이 있는데, 여기서 우리는 원하는 효과를 얻기 위해 데이터 큐레이션 노력, 즉 고원을 넘어 스케일링의 리턴을 배치해야 한다.\n' +
      '\n' +
      '우리가 \\(i=N,N+1,\\ldots\\)에 해당하는 실제 데이터 분포의 꼬리의 청크를 "구매"한다고 가정하자; 분포가 \\(\\pi\\)(따라서, \\(\\{N,N+1,\\ldots\\}\\)에서 지원됨)이라고 가정하자. 이제 \\(k\\), \\(N\\) 및 \\(T\\)는 \\(N/k\\to C\\), \\(C\\in[1,\\infty]\\)로 무한한 경향이 있다고 하자. 우리는 다음과 같은 급격한 위상 천이를 가지고 있다.\n' +
      '\n' +
      '**정리 3.1**.: _(A) If \\(C=1\\), e.g if \\(N=k+\\sqrt{k}\\), then \\(E_{test}\\asymp T^{-c}\\). 즉, AI 생성 데이터의 테일-초핑 효과를 완벽하게 어닐링한다._\n' +
      '\n' +
      '_(B) \\(C>1\\) 이면 \\(E_{test}\\asymp T^{-c}+k^{-\\alpha}\\) (정리 2.1의 결과를 복구하므로 실제 데이터 분포의 \\(N\\) 꼬리를 "구매"하는 것은 가치가 없습니다._\n' +
      '\n' +
      '### Grokking 현상\n' +
      '\n' +
      '여기서는 소량의 원본 데이터라도 테스트 오류가 안정되고 결국 계속 감소하는 그로킹 현상을 도입하여 위의 "스케일링 법칙 붕괴"를 완화할 수 있는 방법을 보여준다.\n' +
      '\n' +
      '**정리 3.2** (꼬리 절단을 사용한 Grokking): _비율 \\(\\pi\\)이 실제 분포 \\(p\\)에서 나오고 나머지는 순위 \\(k\\)에서 꼬리가 잘린 \\(p\\)의 버전 \\(p^{\\prime}\\)에서 나오는 크기 \\(T\\) 샘플을 고려 합니다. Hutter LLM 정의 din (7)에 대해 다음과 같은 스케일링 법칙이 있다._\n' +
      '\n' +
      '_(A) **Early-Stage Dynamics.** \\(T\\ll k^{\\beta}/\\pi\\)의 경우 다음과 같습니다._\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-(1-1/\\beta)}+k^{-(\\beta-1)}. \\tag{14}\\]\n' +
      '\n' +
      '_따라서, 이 스테이지 동안, 일부 깨끗한 데이터를 획득하는 데 소비된 돈은 상각되지 않는다!_\n' +
      '\n' +
      '_(B) **Later-Stage Dynamics.** \\(T\\geq Ck^{\\beta}/\\pi\\) (여기서 \\(C\\)는 절대 상수)인 즉시 해당_\n' +
      '\n' +
      '\\[E_{test}\\asymp(\\pi T)^{-(1-1/\\beta)}. \\tag{15}\\]\n' +
      '\n' +
      '따라서, 이 단계에서 우리는 오염되지 않은 샘플 크기 법칙 스케일링 \\(T^{-(1-1/\\beta)}\\), 최대 승산 상수 \\(\\pi^{-(1-1/\\beta)}\\) (데이터 가격의 상승으로 볼 수 있음)을 복구한다. 고정 \\(T\\) 및 조정 \\(\\pi\\)의 경우 이 오류율은 \\(\\pi^{-(1-1/\\beta)}\\)와 같이 확장되며, 이는 또 다른 확장 법칙입니다._\n' +
      '\n' +
      '효과적으로, 위의 정리는 임의의 고정된 \\(\\pi\\in(0,1)\\)-에 대해 테스트 오차가 아무리 작더라도 \\(T\\)를 예측한다. 그 결과는 그림 3, 오른쪽에서 경험적으로 확인된다(다른 예시는 그림 10 참조).\n' +
      '\n' +
      '우리는 GCD(부록 G 참조)를 계산하도록 훈련된 변압기 모델에 대해 이 새로운 현상을 실험적으로 확인했으며, 이는 산술 작업과 같이 결정론적 지상 진리를 기반으로 하는 더 넓은 등급의 LLM에 대한 적용 가능성을 나타낸다.\n' +
      '\n' +
      '부록 C.4에서 우리는 합성 데이터의 _꼬리 좁힘_의 경우에 유사한 정리를 명시하고 증명한다.\n' +
      '\n' +
      'AI 데이터와의 혼합의 이점 위의 기계를 사용하면 AI 데이터가 성능 향상에 도움이 될 수 있는 특정 체제를 분석할 수 있습니다.\n' +
      '\n' +
      '\\(T=T_{real}+T_{AI}\\)와 \\(\\pi=T_{real}/T\\)를 취하면 다음과 같은 정리 3.2의 중요한 결론이 나온다.\n' +
      '\n' +
      '**상관 3.3**.: _\\(T_{real}\\ll k^{\\beta}\\의 경우, \\(E_{test}\\asymp(T_{real}+T_{AI})^{-(1-1/\\beta)}+k^{-(\\beta-1)})_\n' +
      '\n' +
      '그림 5는 AI 데이터가 안정기에 도달할 때 특정 시점까지 성능을 높일 수 있는 방법을 보여준다. 이 결과는 때때로 이유에 대한 이해에 기여할 수 있다.\n' +
      '\n' +
      '그림 5: \\(T_{real}\\) 실제 데이터와 \\(T_{AI}\\) AI 데이터를 혼합합니다. 점선은 실제 데이터 단독의 테스트 오류를 나타냅니다. \\ (k=1,000,\\beta=3/2\\).\n' +
      '\n' +
      'AI 생성 데이터를 추가하면 특히 더 강력한 모델에 의해 생성될 때 더 나은 모델로 이어질 수 있다(예: He et al. (2023); Shipard et al. (2023); Bansal and Grover (2023); Lin et al. (2023)). 자세한 내용은 부록 A를 참조하십시오.\n' +
      '\n' +
      '## 4 A 테일 빅그램 모델\n' +
      '\n' +
      '이제 더 복잡한 모델로 진행하여 LLM(다음 토큰 예측)의 _확률적_ 및 _자기회귀적_ 특성을 포착하는 데 더 가까워집니다. 이 섹션에서는 데이터 생성 프로세스를 정의하고, 새로운 모델(Hutter++)을 정의하고, 원래의 스케일링 법칙(깨끗한 데이터로)이 여전히 유지된다는 것을 확립할 것이다. 그런 다음 AI 데이터에 대한 스케일링의 유사한 손실을 보여줍니다.\n' +
      '\n' +
      '첫 번째 기본 단계는 결정론적 허터 예측 \\(i\\mapsto y_{i}\\)을 멱법칙 감쇠로 \\(\\mathbb{N}_{*}\\)에 확률 분포 \\(p(j|i)\\)로 대체하기 위해 _확률적_ 그라운드 트루스 레이블을 고려하는 것이다(식 (1)에서와 같이). 가장 빈번한 다음 토큰 \\(j\\)이 이전 토큰 \\(i\\)에 의존한다는 사실을 설명하기 위해 모델을 만듭니다.\n' +
      '\n' +
      '\\[p(j\\mid i)\\propto\\pi_{i}(j)^{-\\beta}, \\tag{16}\\]\n' +
      '\n' +
      '( \\(j^{-\\beta}\\)) 대신에, 여기서 \\(\\pi_{i}\\)는 출력의 순서를 제공하는 모든 \\(i\\)와 연관된 순열이다. 요약하자면, 우리는 데이터를 쌍 \\((i,j)\\)로 생각하는데, 여기서 \\(i\\)의 분포는 결정론적 허터 설정에서와 같이 일부 \\(p(i)\\)에 의해 지배되고, \\(p(j|i)\\)는 식 (16)에 의해 주어진다.\n' +
      '\n' +
      '이 설정은 앞의 출력을 다음 입력으로 사용하여 시퀀스를 단계적으로 생성하여 _자동 회귀_ 할 수 있습니다. 우리는 위의 쌍 \\((i,j)\\)과 같이 각각의 연속적인 토큰 쌍을 생각할 수 있는데, 한계 분포 \\(p(i)\\)가 변하는 유일한 차이이다. 따라서 우리는 다음과 같은 (온화한 기술적 조건을 제외하고) \\(p(i)\\)에 대한 가정을 하지 않을 것이다. 증명은 부록 D에서 찾을 수 있다.\n' +
      '\n' +
      '### Hutter++ 알고리즘\n' +
      '\n' +
      '우리는 이제 빅그램에 적응된 허터 모델(7)의 확장을 제시한다. \\(n_{T}(i)=\\sum_{t=1}^{T}1[i_{t}=i]\\)은 데이터세트 \\(\\mathcal{D}_{T}\\)에서 컨텍스트 \\(i_{t}\\)가 나타나는 횟수이고 \\(n_{T}(i,j)=\\sum_{t=1}^{T}1[(i_{t},j_{t})=(i,j)]\\)은 데이터세트에서 쌍 \\((i,j)\\)이 나타나는 횟수이다. \\(n_{T}(i)\\sim Bin(T,p_{i})\\)에 유의하십시오. \\(n_{T}(i)\\geq 1\\)을 정의하는 즉시\n' +
      '\n' +
      '\\[q_{T}(j\\mid i):=n_{T}(i,j)/n_{T}(i).\\]\n' +
      '\n' +
      '이것은 \\(n_{T}(i)\\) 크기의 iid 샘플을 기반으로 하는 \\(p(\\cdot\\mid i)\\)의 경험적 버전이다. 이론적 분석을 위해 총 변동(TV)에 기반한 다음 테스트 오류 메트릭을 고려해야 한다.\n' +
      '\n' +
      '\\[E_{test}:=\\sum_{i}p_{i}\\operatorname{\\mathbb{E}}\\left[TV(q_{T}(\\cdot\\mid i), p(\\cdot\\mid i))\\right], \\tag{17}\\]\n' +
      '\n' +
      '여기서, \\(TV(a,b):=\\sum_{j}|a_{j}-b_{j}|\\)는 총 변동 거리이고, 기대치는 \\(q_{T}\\)의 랜덤성 이상이다. 여기에서 자산은 (Berend and Kontorovich, 2012)를 사용하여 \\(\\operatorname{\\mathbb{E}}\\left[TV(q_{T}(\\cdot\\mid i),p(\\cdot\\mid i))\\right]\\를 제어할 수 있다는 것이다. TV는 _핑커의 부등식_ 덕분에 KL-발산의 제곱근에 의해 상한이 지정됩니다. 이것은 우리의 결과가 LLM과 같이 복잡성 손실이 있는 자기회귀 모델의 설정에도 적용될 수 있음을 나타낸다.\n' +
      '\n' +
      '### Hutter++에 대한 스케일링 법칙\n' +
      '\n' +
      '식 16과 같이 비결정적 출력의 경우를 생각해 보면, \\(\\pi_{1},\\pi_{2},\\ldots\\)는 \\(\\mathbb{N}_{*}\\)에서 \\(\\mathbb{N}_{*}\\)까지의 함수이다.\n' +
      '\n' +
      '**정리 4.1**.: _(\\beta\\in(1,\\infty)\\setminus\\{2\\}\\) 및 \\(c:=\\min(1-1/\\beta,1/2)\\)를 가정합니다. \\(\\sum_{i}p_{i}^{1-c}<\\infty\\)이면 \\(E_{test}\\lesssim T^{-c}\\). 또한 \\(\\beta\\in(1,2)\\)과 매핑 \\(\\pi_{1},\\pi_{2},\\ldots\\)이 순열이면 \\(E_{test}\\asymp T^{-c}\\)__\n' +
      '\n' +
      '따라서 제안된 Hutter++ 알고리즘은 고전적인 설정(Hutter, 2021)과 정확히 동일한 스케일링 법칙을 유도한다!\n' +
      '\n' +
      '### 확률적 설정에서 모델 붕괴\n' +
      '\n' +
      '이제 우리는 확률적 설정에서 모델 붕괴를 이해하고 위에 제시된 Hutter++를 고려하는 주요 문제로 돌아간다. 따라서 학습자가 조건 분포의 \\(k\\) 번째 머리 \\(p\\cdot\\mid i)\\를 포함하는 최대 크기 \\(T\\)의 데이터 세트에만 액세스할 수 있다고 가정합니다. 즉, \\(i\\sim p\\), \\(j\\sim p(j\\mid i)1[j\\leq k]\\)(적절하게 정규화됨), 여기서 \\(p(\\cdot\\mid i)\\)는 식 (16)과 같다.\n' +
      '\n' +
      '**정리 4.2**.: _(A) \\(\\beta\\in(1,\\infty)\\setminus\\{2\\}\\) 및 \\(\\sum_{i}p_{i}^{1-c}<\\infty\\) 여기서 \\(c:=\\min(1-1/\\beta,1/2)\\) 이전과 같이 \\(E_{test}\\lesssim T^{-c}+k^{-\\beta c}\\)._\n' +
      '\n' +
      '또한, \\(\\pi_{1},\\pi_{2},\\ldots\\)가 순열이고 \\(\\sum_{i}p_{i}^{1-c}<\\infty\\)인 경우 \\(E_{test}\\asymp T^{-c}+k^{-\\beta c}\\).\n' +
      '\n' +
      '**자기회귀 빅그램** 마찬가지로 이러한 결과는 자기회귀 빅그램 모델에 대해 유지되며, 여기서 \\(p(i_{1},i_{2},\\ldots,i_{L})=p(i_{1})\\prod_{\\ell=1}^{L-1}p(i_{\\ell+1}\\mid i_{ \\ell})\\) 및 각 \\(p(j\\mid i)\\)는 (16)과 같다. 그 결과는 <부록 B>의 <그림 11>에서 실증적으로 확인된다.\n' +
      '\n' +
      '**여러 세대.** 정리 2.4의 증명 역학이 이 설정에 적용됩니다. 식 (13)이 확률적 데이터 분포에 대해 계속 유지된다는 것을 예시하는 부록 B의 그림 12를 참조한다.\n' +
      '\n' +
      '**혼합물에 대한 그록킹** 기술적으로 이 그록킹 현상은 허터 LLM 및 제한된 용량 연관 메모리 모델과 같은 결정론적 그라운드 트루스 레이블이 있는 모델에만 적용됩니다. 빅그램(또는 텍스트 LLM)의 _확률적_ 설정의 경우 정리는 순수한 형태로 유지될 수 없는데, 왜냐하면 우리가 두 분포(깨끗한 분포와 합성)의 혼합물에서 훈련하지만 깨끗한 분포에서만 테스트하면 이 두 분포 사이의 거리는 항상 테스트 오차의 하한이 되기 때문이다. 그러나 우리는 "매끄러운" 그로킹 법칙의 잔해가 S자형 스케일링 형태로 지속된다는 것을 알 수 있다(부록 B의 그림 19 참조).\n' +
      '\n' +
      '## 5가지 용량 제한 메모리 모델: 트리플 크기 조정 법칙\n' +
      '\n' +
      '이제 우리는 _용량_을 모델링할 수 있는 Hutter LLM의 유한 메모리 확장에 의존한다. 따라서 우리는 다음과 같은 단순 연상 기억 모델의 맥락에서 모델 붕괴 현상을 연구한다(Cabannes et al., 2023).\n' +
      '\n' +
      '\\[f_{T}(i) :=\\arg\\max_{y}H_{T}(i,y),\\text{ where} \\tag{18}\\] \\[H_{T}(i,y) :=e_{i}^{\\top}M_{T}u_{y},\\] \\[M_{T} :=\\sum_{i}q_{T}(i)e_{i}u_{f_{*}(i)}^{\\top}\\in\\mathbb{R}^{d\\times d}}.\\]\n' +
      '\n' +
      '이것은 (Hutter, 2021)의 무한 메모리 모델의 변압기 같은 유한 메모리 확장이다. 그런 다음 정수 \\(d\\geq 1\\)는 결과 모델의 "용량" 역할을 합니다. 여기서, \\(f_{*}:[N]\\rightarrow[m]\\)은 미지의 함수로서, 예를 들어, 축소 modulo \\(m\\), i.e \\(f_{*}(i):=((i-1)\\text{ mod }m)+1\\); \\(q_{T}=q(\\mathcal{D}_{T})\\)는 임의의 학습자를 인코딩하는 \\([N]\\) 상의 확률 분포이며, 이를 이용하여 추정되고 iid 샘플 \\(\\mathcal{D}_{t}=\\{(i_{t},y_{t})\\ |\\ t\\in[T]\\}\\)의 크기 \\(T\\)는 형태의 \\([N]\\times[m]\\) 상의 확률 분포로부터 수집된다.\n' +
      '\n' +
      '\\[i\\sim p=\\operatorname{Zipf}(\\beta),\\quad y=f_{*}(i) \\tag{19}\\]\n' +
      '\n' +
      '임베딩 벡터 \\(e_{1},e_{2},\\dots e_{N}\\)와 \\(u_{1},u_{2},\\dots,u_{m}\\)는 \\(\\mathbb{R}^{d}\\)의 단위벡터 체계로서, 행렬 \\(\\mathbb{R}^{d\\times d}\\)이 입출력 쌍 \\((i,j)\\)을 기억하도록 구성하였다. 즉, \\((i_{i}^{\\top}Mu_{f_{*}(i)}\\approx q_{T}(i)\\) if \\((i,f_{*}(i))\\in\\mathcal{D}_{T}\\)이다. 가중치 \\(q_{T}(i)\\)는 다른 메모리가 다른 메모리보다 더 빨리 기억되도록 한다.\n' +
      '\n' +
      'Cabannes et al. (2023)은 \\(\\mathbb{R}^{d}\\)에서 단위구상의 균일한 분포로부터 iid 랜덤 임베딩을 사용할 것을 제안하였다. 이 설정에서 \\(q\\)의 다른 선택에 대해 다음과 같은 일반 스케일링 법칙이 설정되었다.\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-(1-1/\\beta)}+d^{-c_{q}}, \\tag{20}\\]\n' +
      '\n' +
      '여기서 지수 \\(c_{q}\\in(0,\\infty)\\)는 \\(\\beta\\)와 알고리즘 \\(q\\)에 의존한다. 예를 들어, \\(q\\)가 계수 측정치 \\(q_{T}(i):=n_{T}(i)/T\\)(SGD의 최소값)를 인코딩할 때 \\(c_{q}=(1-1/\\beta)/2\\in(0,1/2)\\인 것으로 나타났다. 또 다른 알고리즘 \\(q_{T}(i):=1[n_{T}(i)\\geq 1]/\\sum_{\\ell}1[n_{T}(\\ell)\\geq 1]\\) ( ADAM의 최소값)은 \\(c_{q}=\\beta-1\\)을 갖는 임의의 임베딩에 기초한 모든 알고리즘에 걸쳐 최적의 에러율을 달성하는 것으로 제안되었다.\n' +
      '\n' +
      '이 원고의 주요 초점인 모델 붕괴의 맥락에서 다음과 같다.\n' +
      '\n' +
      '**정리 5.1** (트리플렛 스케일링 법칙). _Cabannes et al., 2023)에서 고려 되는 모든 알고리즘 \\(q\\)에 대해 다음과 같은 트리플렛 스케일링 법칙 w.r.t 샘플 크기 \\(T\\), 임베딩 차원 \\(d\\) 및 주파수 차단 \\(k\\),_\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-(1-1/\\beta)}+d^{-c_{q}}+k^{-(\\beta-1)}. \\tag{21}\\]\n' +
      '\n' +
      '이 결과는 그림 21에서 경험적으로 확인되고 부록 E에서 입증된다. 더 간단한 모델에서와 같이 합성 데이터에 대해 유사하게 테이퍼-오프 스케일링 곡선을 생성한다. 섹션 2의 세대에 걸친 스케일링 손실과 섹션 3의 그로킹 현상에 대한 증거는 이 모델에도 이어지며 보편성을 보여준다.\n' +
      '\n' +
      '## 6 Experiments\n' +
      '\n' +
      '이 섹션에서는 이론적으로 수행한 다양한 예측의 증거를 입증하기 위해 실험 결과를 제시한다. 복잡도가 증가하는 네 가지 시나리오를 제시한다. 경험적 Hutter++ 모델, _복잡도 손실이 있는 자기회귀 빅그램 모델, 두 정수의 GCD를 예측하는 산술 변환기(Charton, 2023) 및 대규모 LLM(Llama2-7B, Touvron 등, 2023)은 대규모 데이터 코퍼스(Wikidata-103)에서 훈련된다.\n' +
      '\n' +
      '경험적 관찰(그림 2 참조) 또는 두꺼운 꼬리 데이터에 대한 유한 데이터 샘플링 편향의 영향에 의해 동기화된 이론적 분석에서 생성된 데이터가 꼬리 컷오프 또는 꼬리 좁힘 패턴을 따른다고 가정했다. 후속 실험에서 우리는 생성된 데이터 분포에 혼합 효과를 제공하기 위해 널리 배치된 top-p 선택 또는 온도 스케일링 메커니즘을 허용하기 위해 꼬리 절단/협박에 대한 이론적 가정에서 벗어난다.\n' +
      '\n' +
      '**경험적 Hutter++ 모델.** 그림 6에서는 원본 분포의 \\(T_{0}=100,000\\) 샘플에 대해 훈련된 초기 모델을 사용합니다. Gen 1 라인의 경우 데이터는 모두 이 초기 모델에서 생성됩니다. 2세대부터 모델은 이전 세대의 가장 성능이 뛰어난 모델에 의해 생성된 데이터에 대해 반복적으로 훈련되어 모델이 부적절한 샘플링으로 인해 붕괴될 가능성을 효과적으로 제거한다. 세대 1의 경우 세대 간 모델 성능의 급격한 감소와 함께 데이터 확장성의 현저한 저하가 관찰된다. 이러한 관찰은 우리의 이론적 결과를 검증할 뿐만 아니라 우리의 가정을 재확인한다. 그림 16과 같이 온도 스케일링에서도 유사한 패턴이 분명하다.\n' +
      '\n' +
      '그림 6: **Hutter++ on Bigram with limited data and top-p.** 초기 모델은 \\(T_{0}=100,000\\) 샘플에 대해 학습됩니다. Gen 1에 대한 \\(T\\) 샘플을 생성한다. Gen 2 모델에서 시작하는 것은 이전 세대의 가장 강력한 모델에 의해 생성된 데이터에 대해 훈련된다. Top-p-0.95 절단과 \\(\\beta=3/2\\).\n' +
      '\n' +
      '**복잡성 손실이 있는 자기 회귀 비그램 모델** 자기 회귀 비그램 모델을 조사하기 위해 "실제" LLM으로 한 단계 더 이동합니다. 이제 데이터 세트는 모든 토큰에 대해 학습된 모델을 사용하여 식 (16)을 따르는 순차적으로 생성된 정수로 구성된다. 테스트 세트의 평균 복잡도 점수를 테스트 오류 메트릭으로 사용한다. 우리의 연구는 상위 p 추론, 온도 스케일링, 제한된 실제 데이터 및 점진적으로 더 큰 AI 데이터 세트에 대한 훈련과 같은 다양한 효과를 포함한다. 섹션 4의 결과와 일치하게, 우리는 세대에 걸쳐 스케일링 손실 및 점진적 모델 붕괴의 동일한 패턴을 관찰한다. 관련 수치는 부록 F에 나와 있다.\n' +
      '\n' +
      '**변환기 GCD 학습** "야생에서" 이론에 대한 첫 번째 예는 산술 작업에 대한 서열 간 변환기 모델에 대한 것입니다. 차튼(2023)에 이어 일부 염기 \\(B\\)에서 숫자의 서열로 인코딩된 두 정수의 최대 공약수(GCD)를 예측하는 것입니다. 이 설정은 장난감 모델과 대규모 LLM 사이의 완벽한 중간 단계이며, 기본 데이터는 결정론적 특성을 갖는 반면 변압기 아키텍처와 상당한 모델에 대한 훈련 알고리즘을 사용한다. 모델을 훈련하는 과정에서 새로운 GCD를 점진적으로 학습하고 이미 학습된 GCD를 가진 제품도 함께 학습한다. 따라서 우리는 보통 "폭발"에서 배우는 이러한 각각의 학습된 그룹을 새로운 기술로 볼 수 있다. 이를 위해 300M\\ 샘플 이후의 모델을 AI 데이터 생성기로 사용한다. 그림 4에서 우리는 단일 세대에 대해 예측된 스케일링 법칙을 검증하고 생성된 데이터로 독점적으로 훈련할 때 기술의 \'비학습\'과 혼합물로 훈련할 때 그로킹 효과를 관찰한다. 전체 설명 및 자세한 내용과 그림은 부록 G를 참조하십시오.\n' +
      '\n' +
      '**LLM에 대한 실험** LoRA를 사용하여 Llama2를 미세 조정하여 다음 미세 조정 반복을 위한 합성 AI 데이터를 생성합니다. Shumailov et al.(2023)의 설정에 영감을 받아 128개의 토큰으로 구성된 약 2.2백만 개의 시퀀스로 분할된 Wikidata-103을 사용한다. AI 데이터는 원래 시퀀스의 처음 96개의 토큰을 프롬프트로 사용하여 프롬프트 완료를 통해 생성된다. 모델은 정보 유출을 방지하기 위해 마지막 32개의 토큰에만 훈련되며, 즉 모델은 동일한 32개의 토큰의 그라운드 트루스에 훈련된다. 평가는 동일한 32개의 토큰에 대해서만 수행됩니다. 우리는 모든 세대에 걸쳐 top-p 0.9와 온도 0.9를 사용한다. 그림 4(왼쪽)에 표시된 결과는 여러 세대에 걸쳐 스케일링 법칙 붕괴를 보여준다. 첫 번째 생성된 데이터 세트는 여전히 유용하지만 제한된 정보를 포함하고 있으며 두 번째 세대의 데이터의 유용성은 현저하게 감소한다. 이러한 현상은 스케일링 법칙 및 모델 붕괴의 예상되는 손실을 확증하며, 모델 붕괴가 여기에서 훨씬 더 두드러진다는 것을 추가로 나타내며, 차세대 LLM 훈련의 과제를 강조한다. 자세한 내용과 결과는 부록 H에 나와 있다.\n' +
      '\n' +
      '또한 실제 데이터와 AI 생성 데이터의 비율을 혼합하는 실험을 수행한다. 그림 7은 모든 미세 조정 단계에서 원본 데이터의 무작위 2%를 AI 데이터와 혼합하는 효과를 보여준다. 정리 3.2에서 예측된 대로 그로킹 곡선의 출현으로 모델 붕괴를 상당히 완화한다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '"합성 데이터 시대"의 도래에서 우리의 작업은 현재의 신경 스케일링 법칙의 종말을 신호하고 훈련 말뭉치가 AI 생성 데이터로 풍부한 세계에서 수수께끼 같은 새로운 현상의 배열로 문을 연다. 우리는 스케일링 법칙이 지속되지 않는다는 것을 보여준다; 테스트 오류는 변경되고 덜 무거운 꼬리 데이터 분포로 인해 줄어든다. 이미 이전 작업에서 언급했듯이 완전 합성 데이터 세계에서는 학습이 중단되고 모델이 퇴보합니다. 스케일링이 중단되고 완전히 되돌아가게 됩니다. 그러나 우리가 보여준 것처럼 신중한 혼합 및 데이터 큐레이션에서 새로운 기회가 발생하며 깨끗한 데이터와 합성된 데이터의 상호 작용에서 흥미로운 효과가 있다. 우리는 새로운 학습 고원을 인식하고, 예를 들어 깨끗한 데이터와 합성 데이터의 혼합에서 의도하지 않은 조기 정지에 이르기까지 변경된 학습 곡선에 적응해야 한다. 우리 작업의 주목할만한 특징은 우리의 이론이 _유효하다는 것이다. - 우리는 두 가지 다른 설정에서 관련 대형 모델에 대한 예측된 현상을 관찰한다.\n' +
      '\n' +
      '종합하면, 우리의 기여는 합성된 데이터의 더 책임감 있는 또는 "붕괴 인식" 확산을 요구한다. 크기 조정은 필요한 모든 것이 아닙니다. 합성 데이터에 대한 효과적인 워터마킹에 대한 더 많은 작업이 필요하며, 원본의 사람 주석이 있는 데이터와 더 구별할 수 있도록 해야 합니다. 따라서, 클린/리얼 데이터는 "스케일링을 넘어" 시대를 예고하고 있는 바와 같이, 미래에 더욱 가치 있는 자원이 될 것이다.\n' +
      '\n' +
      '그림 7: **Lama 생성 데이터와 원본 데이터 혼합** 그림 4 왼쪽을 기준으로 생성된 데이터를 원본 데이터와 추가로 혼합합니다. 비율 98 대 2입니다. 원본 데이터를 추가하면 모델 붕괴가 크게 완화됩니다. 혼합 곡선이 그림 3에서와 같이 그로킹 현상의 예측 곡선을 어떻게 검증하는지 살펴보십시오.\n' +
      '\n' +
      '## 8 Acknowledgements\n' +
      '\n' +
      'YF와 JK는 NSF NRT 훈련 보조금 상 1922658을 통해 지원을 인정한다. YF와 PY는 토론과 제안에 대해 Di He에게 감사하고 싶다. 이 작업은 부분적으로 NYU IT 고성능 컴퓨팅 리소스, 서비스 및 직원 전문 지식을 통해 지원되었습니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al.(2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. _ arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Aghajanyan et al. (2023) Aghajanyan, A., Yu, L., Conneau, A., Hsu, W. -N., Hambardzumyan, K., Zhang, S., Roller, S., Goyal, N., Levy, O., and Zettlemoyer, L. 생성 혼합 모달 언어 모델에 대한 확장법, 2023.\n' +
      '* Alemohammad et al. (2023) Alemohammad, S., Casco-Rodriguez, J., Luzi, L., Humayun, A. I., Babaei, H., LeJeune, D., Siahkoohi, A., and Baraniuk, R. G. Self-consuming generative models go mad. _ arXiv preprint arxiv:2307.01850_, 2023.\n' +
      '* Arora & Goyal (2023) Arora, S. 및 고얄, A. 언어 모델에서 복잡한 기술의 출현에 대한 이론. _ arXiv preprint arXiv:2307.15936_, 2023.\n' +
      '* Azizi et al. (2023) Azizi, S., Kornblith, S., Saharia, C., Norouzi, M., and Fleet, D. J. Synthetic data from diffusion models improves imagenet classification. _ Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.\n' +
      '* Bansal & Grover (2023) Bansal, H. and Grover, A. Lving reality to imagination: Robust classification via generated datasets. 신뢰할 수 있고 신뢰할 수 있는 대규모 머신 러닝 모델에 대한 _ICLR 2023 워크샵_, 2023.\n' +
      '* Berend & Kontorovich (2012) Berend, D. and Kontorovich, A. On the convergence of the empirical distribution _ ArXiv Preprint_, 2012.\n' +
      '* Bertrand et al. (2023) Bertrand, Q., Bose, A. J., Duplessis, A., Jiralerspong, M., and Gidel, G. On the stability of iterative retraining of generative models on their own data. _ arXiv preprint arxiv:2310.00429_, 2023.\n' +
      '* Bohacek & Farid (2023) Bohacek, M. 그리고 파리드, H. Nepotistically trained generative models collapse, 2023.\n' +
      '* Bordelon 등(2020) Bordelon, B., Canatar, A., and Pehlevan, C. Spectrum dependent learning curves in kernel regression and wide neural networks. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pp. 1024-1034. PMLR, 2020.\n' +
      '* Briesch et al. (2023) Briesch, M., Sobania, D., and Rothlauf, F. Large language models suffer from their own output: An analysis of the self-consuming training loop, 2023.\n' +
      '* Brown et al.(2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. 언어 모델들은 소수-샷 학습자들이다. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020.\n' +
      '* Burg et al. (2023) Burg, M. F., Wenzel, F., Zietlow, D., Horn, M., Makansi, O., Locatello, F., and Russell, C. Image retrieval outperforms diffusion models on data augmentation _ Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.\n' +
      '* Cabannes et al. (2023) Cabannes, V., Dohmatob, E., and Bietti, A. Scaling laws for associative memories, 2023.\n' +
      '* Caponnetto & de Vito (2007) Caponnetto, A. and de Vito, E. Optimal rates for the regularized least-squares algorithm _ Foundations of Computational Mathematics_, 7:331-368, 2007.\n' +
      '* Charton (2023) Charton, F. Can transformers learn the greatest common divisor?, 2023.\n' +
      '* Chen et al. (2023) Chen, M. F., Roberts, N., Bhatia, K., WANG, J., Zhang, C., Sala, F., and Re, C. Skill-it! 언어 모델을 이해하고 훈련하기 위한 데이터 기반 기술 프레임워크 《신경 정보 처리 시스템에 관한 제37차 회의》 2023년.\n' +
      '* Cui et al. (2021) Cui, H., Loureiro, B., Krzakala, F., and Zdeborova, L. 커널 회귀 분석의 일반화 오류율: 노이즈가 없는 체제에서 노이즈 레짐으로의 교차입니다. Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), _Advances in Neural Information Processing Systems_, 2021.\n' +
      '* Cui et al. (2022) Cui, H., Loureiro, B., Krzakala, F., and Zdeborova, L. 커널 회귀 분석의 일반화 오류율: 노이즈가 없는 레짐에서 노이즈 레짐으로의 교차입니다. _ Journal of Statistical Mechanics: Theory and Experiment_, 2022(11):114004, nov 2022.\n' +
      '* Cui et al. (2023) Cui, H., Loureiro, B., Krzakala, F., and Zdeborova, L. 원본 및 용량 조건에서 커널 분류에 대한 법칙을 조정하는 동안 오류가 발생했습니다. _ Machine Learning: Science and Technology_, 4(3):035033, August 2023. ISSN 2632-2153. doi: 10.1088/2632-2153/acf041.\n' +
      '* Dai et al. (2023) Dai, H., Liu, Z., Liao, W., Huang, X., Cao, Y., Wu, Z., Zhao, L., Xu, S., Liu, W., Liu, N., Li, S., Zhu, D., Cai, H., Sun, L., Li, Q., Shen, D., Liu, T., and Li, X. Auggpt: 텍스트 데이터 증강을 위한 채팅 활용, 2023.\n' +
      '\n' +
      '* Debowski (2023) Debowski, L. 신경 스케일링 법칙의 단순한 모델: 다주기 산타페 프로세스, 2023.\n' +
      '* Devlin et al. (2018) Devlin, J., Chang, M. -W., Lee, K., and Toutanova, K. 버트: 언어 이해를 위해 딥 양방향 변압기를 사전 훈련합니다. _ arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* Fan 등(2023) Fan, L., Chen, K., Krishnan, D., Katabi, D., Isola, P., and Tian, Y. 모델 훈련을 위한 합성 이미지의 크기 조정 법칙 지금은. _ arXiv preprint arXiv:2312.04567_, 2023.\n' +
      '* Gordon 등(2021) Gordon, M. A., Duh, K., and Kaplan, J. Data and parameter scaling laws for neural machine translation. 모엔스 -F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 5915-5922, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.478.\n' +
      '* Guo et al. (2023) Guo, Y., Shang, G., Vazirgiannis, M., and Clavel, C. The curious decline of linguistic diversity: Training language models on synthetic text, 2023.\n' +
      '* Hataya et al. (2023) Hataya, R., Bao, H., and Arai, H. Will large-scale generative models corrupt future datasets? In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pp. 20555-20565, October 2023.\n' +
      '* He et al.(2023) He, R., Sun, S., Yu, X., Xue, C., Zhang, W., Torr, P., Bai, S., and QI, X. 생성 모델의 합성 데이터는 이미지 인식 준비가 되었나요? <표상학습에 관한 제11차 국제회의> 2023에서.\n' +
      '* Henighan et al.(2021) Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S., et al. Scaling laws for autoregressive generative modeling. _ arXiv preprint arXiv:2103.05847_, 2021.\n' +
      '* Hernandez et al. (2021) Hernandez, D., Kaplan, J., Henighan, T., and McCandlish, S. 전송에 대한 크기 조정 법칙입니다. _ arXiv preprint arXiv:2102.01293_, 2021.\n' +
      '* Hestness et al. (2017) Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y. 딥러닝 스케일링은 경험적으로 예측 가능합니다. _ arXiv preprint arXiv:1712.00409_, 2017.\n' +
      '* Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. 컴퓨팅 최적화 대용량 언어 모델, 2022를 훈련합니다.\n' +
      '* Huang et al. (2022) Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve, 2022.\n' +
      '* Hutter (2021) Hutter, M. 학습 곡선 이론입니다. _ arXiv preprint arXiv:2102.04074_, 2021.\n' +
      '* Kaplan 등(2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. _ arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* Kingma and Welling (2014) Kingma, D. P. and Welling, M. 변분 베이 자동 인코딩 2014년 _국제 학습 표상 회의_ 에서.\n' +
      '* Lin et al.(2023) Lin, S., Wang, K., Zeng, X., and Zhao, R. 소샷 객체 탐지에 대한 합성 데이터의 성능을 탐색합니다. In _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pp. 638-647, 2023. doi: 10.1109/CVPRW59228.2023.00071.\n' +
      '* Liu et al. (2019) Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 로베르타: 강력하게 최적화된 버트 사전 훈련 접근법입니다. _ arXiv preprint arXiv:1907.11692_, 2019.\n' +
      '* Maloney et al. (2022) Maloney, A., Roberts, D. A., and Sully, J. A solvable model of neural scaling laws, 2022.\n' +
      '* Martinez et al. (2023a) Martinez, G., Watson, L., Reviriego, P., Hernandez, J. A., Juarez, M., and Sarkar, R. 생성 인공지능(ai)과 인터넷의 결합: 진화와 퇴화를 향해 가는 것? _ arXiv preprint arxiv: 2303.01255_, 2023a.\n' +
      '* Martinez et al. (2023b) Martinez, G., Watson, L., Reviriego, P., Hernandez, J. A., Juarez, M., and Sarkar, R. 생성 인공지능과 인터넷의 상호 작용을 이해하기 위해서입니다. _ arXiv preprint arxiv: 2306.06130_, 2023b.\n' +
      '* McKenzie et al. (2023) McKenzie, I. R., Lyzhov, A., Pieler, M. M., Parrish, A., Mueller, A., Prabhu, A., McLean, E., Shen, X., Cavanagh, J., Gritsevskiy, A. G., Kauffman, D., Kirtland, A. T., Zhou, Z., Zhang, Y., Huang, S., Wurgaft, D., Weiss, M., Ross, A., Recchia, G., Liu, A., Liu, J., Tseng, T., Korbak, T., Kim, N., Bowman, S. R., and Perez, E. Inverse scaling: When larger is not better. _ Transactions on Machine Learning Research_, 2023. ISSN 2835-8856.\n' +
      '* Michaud et al. (2023) Michaud, E. J., Liu, Z., Girit, U., and Tegmark, M. 신경 스케일링의 양자화 모델. 《신경 정보 처리 시스템에 관한 제37차 회의》 2023년.\n' +
      '* Midjourney (2023) Midjourney. Midjourney ai, 2023. URL [https://www.midjourney.com/](https://www.midjourney.com/)\n' +
      '\n' +
      '* Mobahi et al. (2020) Mobahi, H., Farajtabar, M., and Bartlett, P. Self-distillation amify regularization in Hilbert space. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 3351-3361. Curran Associates, Inc., 2020.\n' +
      '* Nitanda & Suzuki (2021) Nitanda, A. and Suzuki, T. 신경 탄젠트 커널 체제에서 평균 확률적 기울기 하강에 대한 최적 비율. 2021년 _International Conference on Learning Representations_ 에서.\n' +
      '* Papyan et al.(2020) Papyan, V., Han, X., and Donoho, D. L. Prevalence of neural collapse during the terminal phase of deep learning training _ Proceedings of the National Academy of Sciences_, 117(40):24652-24663, 2020.\n' +
      '* Ramesh et al. (2021) Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image generation. 인마일라 및 Zhang, T (eds.), _Proceedings of the 38th International Conference on Machine Learning_, Volume 139 of _Proceedings of Machine Learning Research_, pp. 8821-8831. PMLR, 18-24 Jul 2021.\n' +
      '* Rombach et al.(2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 10684-10695, June 2022.\n' +
      '* Rosenfeld et al. (2020) Rosenfeld, J. S., Rosenfeld, A., Belinkov, Y., and Shavit, N. 규모에 걸친 일반화 오차에 대한 건설적 예측 2020년 _International Conference on Learning Representations_ 에서.\n' +
      '* Schmidt-Hieber (2017) Schmidt-Hieber, J. relu 활성화 함수가 있는 심층 신경망을 사용하는 비모수 회귀 _ 통계실록_, 48, 08 2017. doi: 10.1214/19-AOS1875.\n' +
      '* Shipard et al. (2023) Shipard, J., Wiliem, A., Thanh, K. N., Xiang, W., and Fookes, C. Diversity is definitely needed: Improving model-agnostic zero-shot classification via stable diffusion, 2023.\n' +
      '* Shumailov et al. (2023) Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., and Anderson, R. 재귀의 저주: 생성된 데이터에 대한 훈련은 모델을 잊게 만듭니다. _ arXiv preprint arxiv:2305.17493_, 2023.\n' +
      '* Spigler 등 (2020) Spigler, S., Geiger, M., and Wyart, M. 커널 방법의 점근 학습 곡선: 경험적 데이터 대 교사-학생 패러다임. _ Journal of Statistical Mechanics: Theory and Experiment_, 2020(12):124001, December 2020. ISSN 1742-5468. doi: 10.1088/1742-5468/abc61d.\n' +
      '* Strubell et al.(2019) Strubell, E., Ganesh, A., and McCallum, A. Energy and policy considerations for deep learning in nlp. _ ArXiv_, abs/1906.02243, 2019.\n' +
      '* Suzuki (2019) Suzuki, T. 베소브와 혼합된 매끄러운 베소브 공간에서 학습을 위한 심층 reLU 네트워크의 적응성: 최적의 비율과 차원의 저주. 2019년 _International Conference on Learning Representations_ 에서.\n' +
      '* Touvron 등(2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Wang et al. (2023) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), _Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754.\n' +
      '* Wei et al. (2022) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. 대형 언어 모델의 최신 능력입니다. _ Transactions on Machine Learning Research_, 2022. ISSN 2835-8856. Survey Certification.\n' +
      '* Xu et al. (2023) Xu, C., Guo, D., Duan, N., and McAuley, J. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. 2023년 자연 언어 처리에서의 경험적 방법에 관한 회의\n' +
      '* Zipf(1935) Zipf, G. The psycho-biology of language: a introduction to dynamic philology. 1935년\n' +
      '\n' +
      '## 부록 이전 작업\n' +
      '\n' +
      '모델 붕괴: 이 현상은 언어와 이미지 생성의 맥락에서 _최근_ 문헌에 나타났다. 몇몇 최근의 연구는 다양한 설정에서 _empirically_ 이 현상의 측면을 보여준다(Hataya et al., 2023; Martinez et al., 2023a;b; Bohacek & Farid, 2023; Briesch et al., 2023; Guo et al., 2023; Fan et al., 2023). 최근 몇 가지 작업만이 현재 개요를 설명하는 이론적 분석(Shumailov et al., 2023; Alemohammad et al., 2023; Bertrand et al., 2023)을 제공한다.\n' +
      '\n' +
      '(Shumailov et al., 2023)는 모델 붕괴를 정의하고, 모델을 훈련시킬 때 유한 샘플링(낮은 확률 데이터의 절단을 초래함)과 함수 근사 오차(모델은 참 분포를 모델링하기에 충분히 표현적이지 않음)의 두 가지 메커니즘에 기인한다. 그들은 유한한 "트레이닝 데이터"를 반복적으로 재샘플링할 때 생성된 분포가 점점 더 피크가 된다는 것을 관찰한다(그리고 단일 가우시안에서는 증명한다). 경험적으로 연구된 다른 모델은 MNIST에서 (2) 가우시안과 VAE의 혼합물이다. 언어 모델을 연구하기 위해 (Shumailov et al., 2023) Wikidata2에서 Meta의 OPT-125M 모델을 반복적으로 미세 조정한다. 새로운 텍스트의 생성을 위해 그들은 5-웨이 빔 탐색을 사용하며, 이는 본질적으로 (근사하게) 낮은 복잡도 데이터만을 생성한다.\n' +
      '\n' +
      '(Alemohammad et al., 2023)는 그들이 "자기 소비" 또는 "자가 포식" 루프라고 부르는 것의 생성 이미지 모델에 대한 경험적 및 분석적 분석을 수행한다. 그들은 각 세대에서 충분한 새로운 실제 데이터가 없으면 미래 모델은 반드시 정밀도나 재현율이 감소할 것이라고 결론지었다. 그들은 각 새로운 AI 세대의 영향을 일반 _샘플링 바이어스_\\(0\\leq\\lambda\\leq 1\\)를 통해 모델링한다. 이미지 생성의 경우, 이는 다양성(적합하게 정량화됨)보다 품질을 선호하는 생성에서의 특징 파라미터를 지칭한다. 보다 정확하게는 \\(\\lambda=1\\)은 비편향 샘플링에 해당하고 \\(\\lambda=0\\)은 생성 분포의 모드로부터 샘플링에 해당한다. \\ (\\lambda\\) 모델은 BigGAN 및 StyleGAN의 절단 또는 확산 모델의 지침과 같이 생성 모델링 실무에서 일반적으로 사용되는 샘플링 방법을 편향했다. 가우시안 분포의 경우 \\(\\lambda\\)는 다음 세대의 분산의 축소 인자이다. 그들의 경험적 작업은 이론적 및 경험적 관찰 모두에 대해 FFHQ 및 MNIST 및 단일 가우시안에서 이미지 생성을 위한 GAN 및 노이즈 확산 확률 모델을 연구한다. (Shumailov et al., 2023)에서와 같이, 그들은 추정 오차만으로 반복 횟수에 따라 분산이 사라지는 것을 관찰(그리고 단일 가우시안 경우에 대해 증명)한다. (Alemohammad et al., 2023) 또한 더 많은 양의 합성 데이터가 궁극적인 열화로 이어지기 전에 적당한 양의 합성 데이터가 원본 데이터와 혼합되는 체제에서 성능의 초기 증가를 경험적으로 관찰한다. 이는 실제 데이터와 혼합된 합성 데이터가 일부 시나리오에서 성능을 향상시키는 방법을 보여주는 대규모 결과를 모방할 수 있습니다(아래 _합성 데이터의 이점_ 참조). 실제로, 가장 단순한 형태에서, 데이터 증강(회전, 크로핑 등) ML 훈련에서 널리 퍼진 매우 유익한 관행인 데이터 생성의 가장 간단한 형태로 볼 수 있다.\n' +
      '\n' +
      '이미지 생성의 맥락에서 Bertrand et al.(2023)의 연구를 언급하며, 이는 생성 모델에서 안정성과 효율성에 대한 데이터 구성의 중요한 역할을 조명한다. 그들은 실제 분포를 나타내는 깨끗한 데이터와 생성기의 이전 반복에서 합성된 데이터의 혼합을 포함하는 시나리오를 탐구한다. 이들의 분석은 데이터 믹스가 합성 데이터만으로 구성된 경우 생성 과정이 시간이 지남에 따라 퇴화되어 \'무모한 생성기\'로 묘사되는 것으로 이어질 가능성이 있음을 밝히고 있다. 따라서 생성자는 붕괴합니다. 생성자는 모델링하려는 데이터 분포의 본질을 포착하는 능력을 점진적으로 상실합니다. 반대로, 그들은 믹스에서 깨끗한 데이터의 비율이 충분히 높을 때 생성기가 특정 기술적 조건에서 진정한 데이터 분포를 학습하고 정확하게 반영하는 능력을 유지한다는 것을 발견했다. 이 작업은 생성 모델의 안정성과 효과에서 데이터 구성의 중요한 역할을 조명한다.\n' +
      '\n' +
      '여러 경험적 연구들이 자체 생성 데이터에 대한 트레이닝의 해로운 영향을 확인한다: 이미지 생성의 맥락에서, (Martinez et al., 2023a;b) AI 생성 데이터에 대해 트레이닝된 모델들의 열화를 보고한다. 특히, 그들은 AI 생성 데이터의 연속적인 세대에서 훈련할 때 시각적 열화를 입증하기 위해 디노이징 확산 암시적 모델과 소수의(상대적으로 작은) 데이터 세트(예: 난초, MNIST)를 사용한다. (Hataya et al., 2023) _"는 생성된 이미지가 다운스트림 성능에 부정적인 영향을 미치는 반면, 유의성은 태스크 및 생성된 이미지의 양에 따라 달라집니다. 텍스트의 경우 (Briesch 등, 2023) 자체 소비 루프의 반복적인 붕괴를 입증하기 위해 큐레이트된 10K 논리 표현 데이터 세트에서 _nanoGPT6_ 을 사용합니다. 모델 및 데이터 세트는 처음부터 훈련을 허용하기에 충분히 작습니다. (Guo et al., 2023)는 반복적으로 미세 조정된 LLMs에 걸쳐 언어 다양성 메트릭의 감소를 관찰한다.\n' +
      '\n' +
      '각주 6: [https://github.com/karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)완화:알고 있는 범위 내에서는 모델 붕괴에 대한 완화 전략에 대한 엄격한 이론(또는 경험적 증명)이 아직 오지 않았으며, (Bertrand et al., 2023)(아래 참조)에서 주목할 만한 예외가 하나 있습니다. 여러 연구에서는 예를 들어 워터마킹 전략에 대한 연구에 동기를 부여하는 것과 같이 AI 생성 이미지 또는 텍스트에 대한 재교육을 피하기 위해 _탐지_의 필요성에 대해 논의한다. (Bertrand et al., 2023)은 여러 기술적 가정 하에서 합성 데이터와 원본 데이터의 혼합물에 대한 반복적 재훈련을 분석하고, 반복적 재훈련의 안정성을 지배하는 고정 포인트들이 존재한다는 것을 발견한다.\n' +
      '\n' +
      '합성 데이터의 이점 대부분 이러한 결과는 이미지 데이터에 관련되지만 더 나은 모델을 훈련하는 데 AI 합성 데이터의 이점을 보여주는 다양한 결과가 있으며, 특히 확산 모델의 맥락에서(Azizi et al., 2023; He et al., 2023; Shipard et al., 2023; Bansal and Grover, 2023; Lin et al., 2023) 뿐만 아니라(Dai et al. (2023 참조); Xu et al. (2023 참조); Huang et al. (2022 참조); Wang et al. (2023 참조). 바람에 모델 붕괴 경고를 던지거나 데이터 증강의 경우와 같이 약한 양의 합성 데이터(또는 더 많은 양의 "약간 합성" 데이터)가 도움이 되는 보호된 모서리에 정착할 수 있다고 주장할 수 있다. 특히, 합성 데이터가 다운스트림 태스크(분류 모델들을 돕는 확산 모델들로부터 합성된 이미지들)와 다른 사용 사례에 대해 트레이닝된 모델에 의해 생성되거나 더 강한 모델에 의해 생성될 때 합성 데이터의 이점들이 종종 관찰된다(He et al., 2023; Shipard et al., 2023; Bansal and Grover, 2023; Lin et al., 2023). 그러나 다른 연구에서는 생성된 데이터의 이점을 비판적으로 분석한다. (Burg et al., 2023) 확산 모델의 합성 데이터는 확산 모델의 _사전 훈련 데이터_ 만을 사용하여 분류와 같은 다운스트림 작업을 개선하는 데 도움이 되지만 (온화한 1세대 모델 붕괴의 증거로 해석할 수 있는) 훨씬 더 강력한 성능을 제공한다. 대체로 생성 모델을 사용한 데이터 증강의 영향은 여전히 완전히 이해되지 않았다고 말하는 것이 타당하다.\n' +
      '\n' +
      '스케일링 법칙: 신경 스케일링 법칙은 비전, 언어 및 음성에서 어디에서나 관찰되어 왔다. 초기 대규모 실증 연구는 (Hestness et al., 2017; Rosenfeld et al., 2020)에서 수행되며, 다양한 학습 시나리오에 걸쳐 멱법칙 스케일링을 보여준다. 그 다음 OpenAI(Kaplan et al., 2020) 및 DeepMind(Hoffmann et al., 2022)의 잘 알려진 대규모 연구가 뒤따르며, 이는 광범위한 스케일 세트에 걸쳐 LLM에서 멱함수 스케일링을 경험적으로 입증한다. 본질적으로, 이것은 실증적으로 그것을 확립한다.\n' +
      '\n' +
      '\\[L(N,D)\\sim N_{C}\\cdot N^{-\\alpha_{N}}+D_{C}\\cdot D^{-\\alpha_{D}},\\]\n' +
      '\n' +
      '여기서, \\(L\\)는 토큰당 크로스 엔트로피 손실( nats)이고, \\(N,D\\)는 각각 (비 임베딩) 파라미터 및 데이터의 수이며, \\(N_{C},D_{C}\\) 및 \\(\\alpha_{N},\\alpha_{D}\\)는 데이터 분포와 모델 사양에 의해 결정되는 상수이다.\n' +
      '\n' +
      '본 연구는 다양한 시나리오(비전 트랜스포머, 비디오 모델링, 멀티모달 모델 및 수학적 문제 해결)에서 더 많은 멱함수 관계를 입증하기 위해 확장되었다(Henighan et al., 2021). 기계 번역(MT) 설정에서, (Gordon et al., 2021) BLEU와 같은 표준 벤치마크에 대한 스케일링 법칙을 정량화하고 교차 엔트로피 파워-법칙 스케일링을 통해 설명하며, 따라서 메트릭에 걸친 스케일링 법칙의 첫 번째 보편성을 상정한다. (Hernandez et al., 2021)은 전이학습을 위한 유사한 경험적 멱법칙 스케일링을 보여주고 (Aghajanyan et al., 2023)은 혼합모달 언어 모델에서 법칙 스케일링을 위한 방대한 실험적 증거를 제공한다.\n' +
      '\n' +
      '그러나 몇 가지 결과는 스케일링을 개선된 손실에 대한 만병통치약으로 보는 견해에 미묘한 영향을 미쳤다. 예를 들어, (McKenzie et al., 2023)는 훈련 목표 또는 데이터의 결함이 U자형 스케일링으로 이어지는 "역 sclaing"에 대한 증거를 제시한다.\n' +
      '\n' +
      '스케일링 법칙에 대한 이론적 모델: 이론적 각도에서 스케일링 법칙은 대형 기초 모델이 등장하기 전에도 분석적으로 나타났다. 예를 들어, Caponnetto와 de Vito (2007)는 정규화된 최소 자승 커널 알고리즘의 멱법칙 일반화 오류를 특성화한다. 최적화의 역할도 이 설정(Nitanda and Suzuki, 2021)에서 고려될 수 있다. 비모수 문헌에서 예를 들어 (Schmidt-Hieber, 2017) 및 (Suzuki, 2019)는 특정 목표 함수를 피팅하는 데 있어 심층 신경망의 테스트 오류 스케일링을 도출하고 (Bordelon 등, 2020) 스펙트럼 의존성을 분석한다.\n' +
      '\n' +
      '보다 최근에, 스케일링 법칙들은 가우시안 설계 하에서 커널 모델들에 대해, 예를 들어 회귀에 대한 (Spigler 외, 2020; Cui 외, 2021; Cui 외, 2022) 및 분류에 대한 (Cui 외, 2023)에서 보여졌다. (Maloney et al., 2022)는 회귀의 맥락에서 무작위 특징 모델에 대한 스케일링 법칙을 연구한다. 헤비 테일 데이터 스케일링 법칙에 대한 암기의 맥락에서 무한-메모리 설정(Hutter, 2021), "양자화된" 스킬(Michaud 등, 2023) 및 특정 랜덤 데이터-생성 프로세스(Debowski, 2023)에 대해 보여졌다. 모델 용량과 최적화를 고려할 때 Cabannes et al.(2023)은 최근 제약-용량 연관 메모리에서 스케일링 법칙을 증명하였다.\n' +
      '\n' +
      '그러나 우리가 아는 한 자가 소비 루프의 경우 스케일링의 붕괴를 다루는 논문은 거의 없다. 주목할 만한 예는 커널 설정에서 자기-(지식-증류)의 맥락에서 반복된 재훈련을 연구하는 (Mobahi et al., 2020)이다. 그러나 이 분석은 가우스 설계로 커널 설정에 위치하기 때문만이 아니라 "생성" 단계가 다음 단계 훈련을 위해 신중하게 최적화되는 증류 설정을 가정하기 때문에 우리의 작업과 매우 다르다. 야생에서 합성된 데이터의 경우, 이러한 가정은 당연히 이루어질 수 없다.\n' +
      '\n' +
      '"기술"과 스케일링 법칙의 출현: 스케일링 법칙은 모델 훈련에 대한 가성비 스타일의 트레이드오프에 대한 통찰력을 제공한다. 그러나, 교차 엔트로피 손실은 그 자체로 목표가 아니다: 우리는 모델을 확장함에 따라 더 크고 더 큰 기술이 부여된 모델을 훈련시키고 싶다. 예를 들어, (Gordon et al., 2021)는 교차 엔트로피 손실을 갖는 MT에 대한 BLEU 점수의 스케일링을 위한 직관 및 경험들을 제공한다.\n' +
      '\n' +
      '\\[BLEU(L)\\approx Ce^{-kL},\\]\n' +
      '\n' +
      '규모와 함께 우수한 BLEU 성능의 "출현"을 입증합니다. 이러한 유형의 "출현"은 "출현"의 작업 정의가 "더 작은 모델에는 존재하지 않지만 더 큰 모델에서는 나타난다"는 (Wei et al., 2022)에서 크게 확인되었다. 이러한 의미에서, (Wei et al., 2022)는 Multi-Task NLU, Modular 산술, 단어 비스크램블링 및 음역과 같이 스케일로 나타나는 많은 수의 "기술"을 경험적으로 입증한다.\n' +
      '\n' +
      '기술의 출현을 위한 스케일링 법칙의 필요성에 대한 기초를 제공하는 이론적 모델은 최근 (아로라와 고얄, 2023)에 의해 주어졌다. 그들은 LLM에서 교차 엔트로피 손실을 기본 기술과 연결하는 모델에서 스케일링 법칙을 출발점으로 하는 "출현"을 분석하여 스케일링 법칙이 모델이 효율적으로 학습(및 일반화)할 수 있음을 보여준다.\n' +
      '\n' +
      '스케일링 법칙과 창발 기술 사이의 연관성을 강화하는 것은 비록 반대 방향이지만, (Michaud et al., 2023) "quanta"에서 나오는 기술이 손실의 스케일링 법칙을 의미한다고 가정한다. 관련하여, (Chen et al., 2023)은 스킬의 출현을 촉진하기 위해 데이터 큐레이션 메커니즘을 도출하기 위해 스킬의 계층을 가정하지만, 스킬이 직접 스케일링 법칙을 암시하지는 않는다.\n' +
      '\n' +
      '## 부록 B 섹션 2, 3 및 4에 대한 기본 그림\n' +
      '\n' +
      'Hutter LLM. 그림 8, 9 및 10은 간단한 Hutter LLM에 대한 우리의 이론을 추가로 보여준다.\n' +
      '\n' +
      'Hutter++.이제 우리는 이론적인 가정을 벗어나지 않고 섹션 4의 식 (16)과 같이 일반화된 Hutter 모델에 대해 개발한 이론에서 얻은 예측의 보완적인 삽화를 제공한다. 우리는 또한 섹션 2의 무한 기억 모델에서 이론이 어떻게 이 비그램 설정에서 계속 유지되는지 보여준다. 그림 11은 정리 4.2의 스케일링 법칙을 확인한다.\n' +
      '\n' +
      '그림 3(중간)에서 우리는 Hutter LLM에서 n배 합성된 데이터에서 번역된 스케일링 곡선의 그림을 보았다. 그림 12는 약간 더 복잡한 테일드 바이그램 모델에 대한 이 현상을 보여준다.\n' +
      '\n' +
      '그림 8: **Varying \\(T\\)에 대한 Hutter LLM 확장** 정리 2.1의 경험적 확인입니다. 여기서 \\(\\beta=3/2\\) 및 오차 막대는 샘플링 AI 생성 데이터의 \\(10\\) iid 실행(즉, 분포 \\(q\\))에 해당합니다. 파선은 Hutter rate \\(T^{-(\\beta-1)/\\beta}\\)에 해당하며, \\(k\\)와 \\(T\\)의 값을 달리한다. 왼쪽 그림 3은 \\(T\\) 및 \\(k\\)의 여러 설정에 대해 동일한 것을 보여줍니다. 정리와 완벽하게 일치함을 주목하라.\n' +
      '\n' +
      '그림 9: **Varying에 대한 Hutter LLM 크기 조정 \\(k\\).** 크기 \\(T_{0}\\)의 샘플은 \\(p_{\\text{AI}}\\)를 통해 실제 분포 \\(p\\)에 근사하는 데 사용됩니다. 그런 다음, \\(p_{\\text{AI}}}\\로부터 \\(T\\) 크기의 표본에 대해 Hutter-type 모델을 학습하고, 실제 데이터 분포 \\(p\\)에 대해 평가한다. 각 수평선은 \\(T_{0}\\)의 다른 값에 대해 점근선 \\(k^{-\\beta c}\\asymp T_{0}^{-c}\\)에 해당한다. 사선은 \\(T^{-c}\\)에 해당한다.\n' +
      '\n' +
      '도 3(중간) 및 도 12는 모두 각각의 모델이 자신의 선행자(\\(T_{0}=T\\))만큼 많은 트레이닝 데이터를 소비하는 설정을 예시한다. 우리는 이제 각 연속 모델이 이전 모델과 엄격하게 동일한 양의 훈련 데이터를 가지고 있다는 가정을 완화한다. 우리는 0세대 모델이 1세대를 위한 AI 데이터를 생성하기 위해 원본 데이터 양의 \\(T_{0}\\)(여기서는 \\(T_{0}=100,000\\))에 대해 훈련된다고 가정한다. 2세대를 시작으로 모든 미래 세대는 이전 세대(여기서는 \\(T=1,000,000\\) 데이터에서 가장 강력한 모델에 의해 생성된 데이터에 대해 훈련된다. 그림 13(Hutter LLM에 대한) 및 14(쌍체 빅그램 데이터에 대한 Hutter++에 대한)는 결과적인 스케일링 거동을 보여준다. 우리는 합성 데이터 생성마다 top-p 꼬리 절단 메커니즘과 온도 스케일링 메커니즘을 추가하여 이 설정을 더욱 취한다. 그림 6은 \\(p=0.95\\)에서, 그림 16은 \\(0.9\\)에서 절단한다.\n' +
      '\n' +
      '우리는 이제 빅그램 설정에서 깨끗한 데이터와 합성된 데이터의 혼합을 연구한다. 그림 17과 18은 합성할 때 top-p tail-cutting을 추가하고 가장 큰 모델의 합성 데이터와 연속적으로 혼합되는 \\(T_{0}=10,000\\) 원본 데이터 샘플로 시작한다. 이 설정에서 우리는 AI 데이터가 증가한 스케일링 법칙의 회귀를 관찰한다. 이는 결정론적 허터 설정에서 그림 20의 주황색 곡선과 비교할 필요가 있다. 비그램 모델의 확률적 특성은 여기에서 새로운 효과로 이어진다.\n' +
      '\n' +
      '그림 11: **Hutter++에 대한 모델 붕괴.** 정리 4.2의 경험적 확인입니다. 여기서 \\(p(j\\mid i)\\)는 (16)과 같으며 \\(\\beta=7/5\\)입니다. 수평 파선은 \\(k)의 다른 값에 대해 \\(k^{-\\beta c}\\)에 해당하며, 여기서 \\(c:=\\min(1-1/\\beta,1/2)\\). 대각선 파선은 \\(T^{-c}\\)(컷오프 없이 고전적인 오류율)에 해당한다.\n' +
      '\n' +
      '그림 12: **쌍체 빅그램 데이터에 대한 Hutter++ 모델.** 9회 동안 추가 꼬리 절단 없이 모든 세대에 걸쳐 \\(\\beta=3/2\\), \\(T_{0}=T\\)가 있는 확률적 쌍체 빅그램 데이터에 대한 정리 2.4의 경험적 확인입니다. 그 결과는 모델 붕괴를 세대별로 검증한다.\n' +
      '\n' +
      '그림 10: 정리 3.2의 경험적 검증. 파선은 오염이 없을 때 전체적으로 유지되는 \\(T^{-(1-1/\\beta)}\\) 스케일링 법칙에 해당한다. 정리에 의해 예측되는 그로킹 행동을 주목하라. 이 실험을 위해 참 데이터 분포의 Zipf 지수 \\(p\\)는 \\(\\beta=2\\)이다.\n' +
      '\n' +
      '## 무한 메모리 (허터) 모델에 대 한 부록 C 증명 (섹션 2 및 3)\n' +
      '\n' +
      '### 정리 2.1의 증명\n' +
      '\n' +
      '모형 \\(\\widehat{f}\\)이 학습 데이터 세트 \\(\\mathcal{D}_{T}\\)에서 \\(i\\)번째 "기술"이 발생하지 않은 경우에만 \\(i\\)에 오류가 발생함을 관찰합니다. 즉, 모든 \\(t\\in[T]\\)에 대해 (1) \\(i\\geq k+1\\) 또는 (2) \\(1\\leq i\\leq k\\) 및 \\(i_{t}\\neq i\\). 추측해 보자\n' +
      '\n' +
      '\\[E_{test}=\\mathbb{P}_{i\\sim p}(\\widehat{f}(i)\\neq y_{i}) =\\sum_{i\\geq k+1}p_{i}+\\sum_{1\\leq i\\leq k}p_{i}(1-p_{i})^{T}\\] \\[\\asymp k^{-(\\beta-1)}+\\sum_{1\\leq i\\leq k}p_{i}e^{-p_{i}T},\\]\n' +
      '\n' +
      '여기서 \\(c:=1-1/\\beta\\in(0,1)\\)와 \\(\\sum_{i\\geq k+1}i^{-\\beta}\\asymp k^{-(\\beta-1)}\\)는 큰 \\(k\\)에 대한 기본적인 사실을 사용하였다. 두 번째 합을 위해, 우리는 다음의 보조행렬이 필요할 것이다.\n' +
      '\n' +
      '**Lemma C.1**. _다음 ID는 홀드_\n' +
      '\n' +
      '\\[T^{c}\\sum_{i=1}^{k}p_{i}e^{-Tp_{i}}\\asymp\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T)=O(1), \\tag{22}\\]\n' +
      '\n' +
      '_where \\(\\Gamma(s,x):=\\int_{x}^{\\infty}u^{s-1}e^{-u}\\mathrm{d}u\\)는 불완전 감마 함수를 정의한다. 특히 \\(k=\\infty\\)와 큰 \\(T\\)의 경우 \\(\\sum_{i=1}^{\\infty}p_{i}e^{-Tp_{i}}\\asymp T^{-c}\\)를 갖는다._\n' +
      '\n' +
      '증명: \\(z\\in(0,1)\\)에 대한 함수 \\(h(z):=ze^{-Tz}\\)를 고려합니다. 그 유도체는 \\(h^{\\prime}(z)=e^{-Tz}(1-Tz)\\)이다. 따라서 \\(h\\)는 \\((0,1/T)\\)에서 증가하고 \\((1/T,\\infty)\\)에서 감소한다. 또한 \\(p_{i}\\leq 1/T\\) iff \\(i\\geq T^{1/\\beta}\\)에 유의하세요. 추측해 보자\n' +
      '\n' +
      '\\[\\sum_{i=1}^{k}p_{i}e^{-Tp_{i}}\\asymp\\int_{1}^{k}x^{-\\beta}e^{-Tx^{-\\beta}} \\mathrm{d}x.\\]\n' +
      '\n' +
      '변수 \\(u=u(x):=Tx^{-\\beta}\\)의 변화 하에서 \\(x=x(u)=(u/T)^{-1/\\beta}\\)와 \\(\\mathrm{d}x=-(T^{1/\\beta}u^{-1-1/\\beta}/\\beta)\\mathrm{d}u\\)가 존재한다.\n' +
      '\n' +
      '그림 14: **경험적 Hutter++ 모델.** 그림 6과 동일한 설정입니다. 초기 모델은 \\(T_{0}=100,000\\) 샘플에 대해 훈련되었습니다. 상위 p 추론 또는 온도 크기 조정은 사용되지 않습니다. \\ (\\beta=3/2\\). 이 설정에서는 유한 표본 편의에서도 온화한 모델 붕괴가 발생한다.\n' +
      '\n' +
      '그림 13: 결정론적 레이블링 기능이 있는 **경험적 허터 LLM.** Bigram 모델입니다. 초기 모델은 \\(T_{0}=100,000\\) 샘플에 대해 훈련되었다. Gen 1에 대한 \\(T\\) 샘플을 생성합니다. Gen 2 모델에서 시작하는 것은 이전 세대의 가장 강력한 모델에 의해 생성된 데이터에 대해 훈련됩니다. \\ (\\beta=3/2\\). 이 설정에서는 유한 표본 편향에서 온화한 모델 붕괴가 발생한다.\n' +
      '\n' +
      '또한 \\(u(1)=T\\) 및 \\(u(k)=Tk^{-\\beta}\\). 추측해 보자\n' +
      '\n' +
      '\\mathrm{d}u\\] \\[\\asymp T^{-(1-1/\\beta)}\\int_{Tk^{-\\beta}}^{T}u^{-1-1/\\beta}/\\beta)\\mathrm{d}u\\] \\[\\asymp T^{-(1-1/\\beta)}\\int_{Tk^{ -\\beta}e^{-Tx^{-\\beta}}\\mathrm{d}x=\\int_{Tk^{ -\\beta}}^{T}(u/T)e^{-Tx^{-\\beta}}\\left(\\Gamma(1-1/\\beta,T)\\right)\\] \\[=T^{-c}\\left(\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T)\\right),\\]\\\n' +
      '\n' +
      '그리고 우리는 첫 번째 부분을 끝냈다.\n' +
      '\n' +
      '두 번째 부분의 경우 큰 \\(T\\)에 대해 \\(\\Gamma(c,T)=o(1)\\)에 유의하여\n' +
      '\n' +
      '\\[(\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T))|_{k=\\infty}=\\Gamma(c,0)-\\Gamma(c,T)=\\Theta (1)-o(1)=\\Theta(1),\\]\n' +
      '\n' +
      '그 결과 다음과 같은 결론을 얻었다.\n' +
      '\n' +
      '우리는 이제 \\(k\\)와 \\(T\\)의 상대적 스케일링에 대해 두 가지 별개의 경우를 고려한다.\n' +
      '\n' +
      '- Case 1: \\(T\\gtrsim k^{\\beta}\\).Here, we have thanks to Lemma C.1\n' +
      '\n' +
      '\\[E_{test}\\asymp k^{-(\\beta-1)}+O(T^{-c})\\asymp k^{-(\\beta-1)}, \\tag{23}\\]\n' +
      '\n' +
      'since \\(k^{-(\\beta-1)}\\gtrsim T^{-(\\beta-1)/\\beta}=T^{-c}\\).\n' +
      '\n' +
      '- 사례 2: \\(1\\ll T\\lesssim k^{\\beta}\\).여기서, Lemma C.1 덕분에 \\(\\Gamma(c,T)=o(1)\\) 및 \\(\\Gamma(c,Tk^{-\\beta})=\\theta(1)\\). 추측해 보자\n' +
      '\n' +
      '\\[E_{test}\\asymp k^{-(\\beta-1)}+T^{-c}\\left(\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T) \\right)\\asymp k^{-(\\beta-1)}+T^{-c}\\asymp T^{-c}, \\tag{24}\\]\n' +
      '\n' +
      'since \\(k^{-(\\beta-1)}\\lesssim T^{-(\\beta-1)/\\beta}=T^{-c}\\). 그런 다음 상황을 종합하면 청구된 결과가 나옵니다.\n' +
      '\n' +
      '### Corollary 2.3의 증명\n' +
      '\n' +
      '실제로 \\(p_{i}\\propto i^{-\\beta}\\)와 \\((p_{AI})_{i}=q_{i}\\propto i^{-\\beta^{\\prime}}\\)를 둔다. 그럼\n' +
      '\n' +
      '\\[E_{test}\\asymp\\sum_{i}p_{i}(1-q_{i})^{T}\\asymp\\sum_{i}p_{i}e^{-q_{i}T}\\asymp\\int_{ 1}^{\\infty}x^{-\\beta}e^{-x^{-\\beta^{\\prime}}T}\\mathrm{d}x. \\tag{25}\\]\n' +
      '\n' +
      '설정 \\(u=x^{-\\beta^{\\prime}}T\\)은 \\(x=T^{1/\\beta^{\\prime}}u^{-1/\\beta^{\\prime}}\\)를 부여하고, \\(\\mathrm{d}x=-(T^{1/\\beta^{\\prime}}/\\beta^{\\prime})u^{-(1+1/\\beta^{\\prime})} \\mathrm{d}u\\)를 부여한다. 추측해 보자\n' +
      '\n' +
      '\\[E_{test} \\asymp T^{-(\\beta-1)/\\beta^{\\prime}\\int_{1}^{T}u^{\\beta/\\beta^{\\prime}\\int_{1}^{T}u^{(\\beta-1)/\\beta^{\\prime}-1}e^{-u}\\mathrm{d}u\\] \\[\\asymp T^{-c}\\Gamma(c,T)=T^{-c}(1+o(1)),\\text{ with }c:=(\\beta-1)/\\beta^{\\prime}.\\]\n' +
      '\n' +
      '즉, \\(E_{test}\\asymp T^{-c}\\).\n' +
      '\n' +
      '### 정리 증명 3.2 및 결과 3.3\n' +
      '\n' +
      '모델 학습에 사용할 수 있는 \\(T\\) 샘플을 가정하면, \\(\\pi T\\)는 실제 분포 \\(p=Zipf(\\beta)\\)의 샘플이고 \\((1-\\pi)T\\)는 AI 데이터 분포 \\(p^{\\prime}\\)의 버전인 \\(p^{\\prime}\\)로, 즉 순위가 \\(k\\)에서 꼬리가 잘린 \\(p\\)이다.\n' +
      '\n' +
      '그림 19: **S자형 "Smoothed Grokking."** Hutter++ 모델이 포함된 빅그램 데이터, 비율 50 대 50의 AI 생성 데이터와 깨끗한 데이터를 혼합합니다. grokking 라인은 확률적 설정에서 평활화됩니다. 10,000, 1,000, 100개의 데이터를 이용하여 라인 1, 2, 3을 생성하여 생성 모델을 학습시킨다. 그림 17과 비교할 때, 우리는 지금 접근 가능한 실제 데이터의 수를 제한하지 않는다 \\ (\\beta=3/2\\).\n' +
      '\n' +
      '그림 17: **혼합이 있는 경험적 Hutter++ 모델** 초기 "깨끗한" 데이터 세트는 \\(T_{0}=10,000\\) 샘플로 구성됩니다. 미래 세대의 경우 데이터를 합성하기 위해 가장 큰 모델을 사용한다. 원(T\\leq 20,000\\)의 경우 학습 데이터는 클린 데이터와 생성된 데이터의 동일한 혼합이고, 원(T>20,000\\)의 경우 모든 클린 데이터가 사용되며, 나머지 학습 데이터는 합성(따라서 클린 데이터의 비율이 감소함)된다. Top-p 0.9, 온도 스케일링 없음 \\(\\beta=3/2\\).\n' +
      '\n' +
      '그림 18: **혼합이 있는 경험적 Hutter++ 모델** 상위 p 90, 온도 크기 조정 없음 및 \\(\\beta=3/2\\)를 사용하여 그림 17과 동일한 설정입니다.\n' +
      '\n' +
      '\\(p^{\\prime}_{i}\\propto p_{i}1[i\\leq k]\\). 따라서 데이터세트는 \\(q_{i}=\\pi p_{i}+(1-\\pi)p^{\\prime}_{i}\\)로 주어진 분포로부터 도출된다. Hutter LLM 이후 쓰기의 테스트 오류\n' +
      '\n' +
      '\\[\\begin{split} E_{test}&=\\sum_{i\\geq 1}p_{i}(1-p_{i})^{T}=\\sum_{1\\leq i\\leq k}p_{i}(1-p_{i})^{T}+\\sum_{i\\geq k+1}p_{i}(1-\\pi p_{i})^{T}\\\\ &\\asymp\\sum_{1\\leq i\\leq k}p_{i}e^{-p_{i}T}+\\sum_{i\\geq k+1}p_{i} e^{-\\pi p_{i}T}.\\end{split} \\tag{26}\\]\\\n' +
      '\n' +
      '이제 Lemma C.1 덕분에 모든 정수 \\(1\\leq r<R\\leq\\infty\\)와 큰 \\(z\\)에 대해 하나는 분명히 있다.\n' +
      '\n' +
      '\\[\\sum_{r\\leq i\\leq R}p_{i}e^{-p_{i}z}\\asymp z^{-c}\\left(\\Gamma(c,zR^{-\\beta})- \\Gamma(c,zr^{-\\beta})\\right), \\tag{27}\\\n' +
      '\n' +
      '여기서, \\(c=1-1/\\beta\\in(0,1)\\) 및 \\(\\Gamma\\)는 (상부) 불완전 감마 함수이다. (27)을 \\((r,k,z)=(1,k,T)\\)로 적용하면\n' +
      '\n' +
      '\\[T^{c}\\sum_{1\\leq i\\leq k}p_{i}e^{-p_{i}T}\\asymp\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T)=\\begin{cases}\\theta(1)-o(1)=\\theta(1),&\\text{ if }1\\ll T\\lesssim k^{\\beta},\\\\ o(1)-o(1)=o(1),&\\text{ if }T\\gtrsim k^{\\beta}\\gg 1.\\end{cases} \\tag{28}\\]\n' +
      '\n' +
      '한편, (27)을 \\((r,k,z)=(k+1,\\infty,\\pi T)\\)로 적용하고 \\(\\pi=\\theta(1)\\)로 가정하면 다음과 같다.\n' +
      '\n' +
      '\\[\\sum_{i\\geq k+1}p_{i}e^{-\\pi p_{i}T}\\asymp(\\pi T)^{-c}\\gamma(c,\\pi T(k+1)^{- \\beta})\\asymp\\begin{cases}(\\pi T)^{-c},&\\text{ if }\\pi T\\gtrsim k^{\\beta}\\gg 1,\\\\ (k+1)^{-\\beta c}\\asymp k^{-\\beta c},&\\text{ if }k^{\\beta}\\gg\\pi T.\\end{cases} \\tag{29}\\]\n' +
      '\n' +
      '물건을 종합하면 결과가 나온다.\n' +
      '\n' +
      'Bertrand et al.(2023)도 반복적인 재훈련을 위해 이러한 혼합물을 공식적으로 연구한다는 것을 상기한다. 그들의 설정에서 그들은 모델 붕괴 영역을 묘사하는 혼합 비율에서 고정점의 존재를 보여준다. 이러한 결과는 상호 보완적이며 우리의 결과와 모순되지 않는다: 혼합, 많은 반복 및 데이터 붕괴를 결합하여 이전 정리가 별도로 다루는 다양한 이론 조건 하에서(스케일링 법칙에 중점을 두지 않음) 효과의 조합을 연구한다.\n' +
      '\n' +
      'Tail Narrowing을 위한 Grokking\n' +
      '\n' +
      '**정리 C.2** (꼬리가 좁아지는 Grokking).: _크기 \\(T\\)의 샘플을 고려 합니다. 이 샘플 중 비율 \\(\\pi\\)은 실제 분포 \\(p=Zip(\\beta)\\)에서 나오고 나머지는 버전 \\(p^{\\prime}=Zip(\\beta^{\\prime})\\)에서 나옵니다. Hutter LLM,_\n' +
      '\n' +
      '\\[E_{test}\\asymp(\\pi T)^{-c}+((1-\\pi)T^{-c^{\\prime}}), \\tag{30}\\]\n' +
      '\n' +
      '_where \\(c:=(\\beta-1)/\\beta\\) and \\(c^{\\prime}:=(\\beta-1)/\\beta^{\\prime}\\)._\n' +
      '\n' +
      '_Define \\(\\overline{T}:=(\\pi/(1-\\pi))^{-a}\\), 여기서 \\(a:=s/(1-s)\\), \\(s:=\\beta/\\beta^{\\prime}\\). 그러면,_\n' +
      '\n' +
      '_(A) **Early-Stage Dynamics.** For_ \\(T\\lesssim\\overline{T}\\)_, it holds_ \\(E_{test}\\asymp((1-\\pi)T)^{-c^{\\prime}}\\)_. 따라서, 만약_ \\(\\beta^{\\prime}>\\beta\\)_이면, 일부 깨끗한 데이터를 획득하는 데 소비된 돈은 상각되지 않는다!_\n' +
      '\n' +
      '_(B) **Later-Stage Dynamics.**_ \\(T\\gtrsim\\overline{T}\\)_ 즉시_ \\(E_{test}\\asymp(\\pi T)^{-c}\\)_를 유지합니다. 유사하게, 오염되지 않은 샘플 크기 법칙 스케일링_ \\(T^{-c}\\)_을 복구한다. 고정_ \\(T\\) _ 및 튜닝_ \\(\\pi\\)_의 경우 이 오류율은_ \\(\\pi^{-c}\\)_와 같이 조정됩니다._\n' +
      '\n' +
      '증명. \\(q\\)를 \\(p\\)와 \\(p^{\\prime}\\)의 혼합으로 하자. 우리는 \\(\\beta^{\\prime}\\geq\\beta\\)에 대한 결과를 증명하고, \\(\\beta^{\\prime}\\leq\\beta\\)의 경우는 유사하다. 그래서 글을 쓸 수도 있고\n' +
      '\n' +
      '\\[E_{test}=\\sum_{i\\geq 1}p_{i}e^{-\\pi i^{- \\beta}+(1-\\pi)i^{-\\beta^{\\prime}}}\\asymp\\sum_{1\\leq i\\leq\\overline{T}^{1/\\beta}}p_{i}e^{-\\pi i^{-\\beta}}+\\sum_{i\\geq\\overline{T}^{1/\\beta}}}p_{i}e^{-(1-\\pi)i ^{-\\beta^{\\prime}}}, \\tag{31}\\]\n' +
      '\n' +
      '여기서 우리는 \\((1-\\pi)i^{-\\beta^{\\prime}}\\geq\\pi i^{-\\beta}\\) iff \\(i\\leq(\\pi/(1-\\pi))^{-1/(\\beta^{\\prime}-\\beta)}=\\overline{T}^{1/\\beta}\\)라는 사실을 사용하였다. 그 후 결과는 (27)부터 이어진다.\n' +
      '\n' +
      '_Remark C.3_: \\(E_{test}\\)는 \\(\\pi\\)의 함수를 감소시키므로 깨끗한 데이터가 항상 도움이 된다고 결론짓겠습니다. 실제로 (26)에서 파생 w.r.t \\(\\pi\\)는 \\(E^{\\prime}_{test}(\\pi)=-T\\sum_{i\\geq k+1}p_{i}^{2}(1-\\pi p_{i})^{T-1}\\leq 0\\이다.\n' +
      '\n' +
      '### 흥미로운 우회: 고정 크기 AI 데이터 세트에 대한 Grokking.\n' +
      '\n' +
      '이제 AI 합성 데이터 집합이 고정된 크기 \\(T_{AI}\\)(예: 웹의 냉동 청크)인 반면 깨끗한 데이터 집합 크기는 확장 가능한 매개변수 \\(T_{real}\\)인 시나리오를 고려합니다. \\(T=T_{real}+T_{AI}\\)와 \\(\\pi=T_{real}/T\\)를 취하면, 우리는 정리 3.2의 다음과 같은 결과를 얻을 수 있다.\n' +
      '\n' +
      '**연상 C.4**.: _다음이 있습니다._\n' +
      '\n' +
      '_(A) 초기 단계 역학. For_ \\(T_{real}\\ll k^{\\beta}\\)_, it holds that_\n' +
      '\n' +
      '\\[E_{test}\\asymp(T_{real}+T_{AI})^{-(1-1/\\beta)}+k^{-(\\beta-1)} \\tag{32}\\]\n' +
      '\n' +
      '_(B) 후단계 역학. _ \\(T_{real}\\geq Ck^{\\beta}\\) _(여기서_ \\(C\\) _는 절대 상수)인 순간, 다음과 같이 성립한다.\n' +
      '\n' +
      '\\[E_{test}\\asymp T_{real}^{-(1-1/\\beta)}. \\tag{33}\\]\n' +
      '\n' +
      '3절에서 언급했듯이 AI 합성 데이터는 실제 데이터가 부족한 레짐에서 도움이 된다. 한 번 더 많은 실제 데이터를 사용할 수 있게 되면, 인공지능이 합성한 데이터를 잊어버려 정상적인 스케일링 법칙 w.r.t \\(T_{real}\\)을 복구한다. 그림 20은 다양한 설정에서 이러한 현상을 보여준다.\n' +
      '\n' +
      '### Theorem 3.1의 증명\n' +
      '\n' +
      '명확하게,\n' +
      '\n' +
      '\\[\\pi_{i}\\asymp\\begin{cases}N^{\\alpha}p_{i},&\\text{if }i\\geq N,\\\\ 0,&\\text{else,}\\end{cases} \\tag{34}\\]\n' +
      '\n' +
      '여기서 \\(\\alpha:=\\beta-1\\). 이는 정규화 상수가 \\(\\sum_{i\\geq N}p_{i}=\\sum_{i\\geq N}i^{-\\beta}\\asymp N^{-\\alpha}\\)이기 때문이다. 이제 이 분포를 동일한 가중치 \\(1/2\\)와 함께 \\(q\\)와 혼합하여 새 분포를 얻습니다.\n' +
      '\n' +
      '\\[q_{i}^{\\prime}=q_{i}/2+\\pi_{i}/2,&\\text{if }i\\leq k,\\\\ \\pi_{i}/2,&\\text{if }k\\geq N,\\\\ 0,&\\text{otherwise}\\end{cases} \\tag{35}\\] \\[\\asymp\\begin{cases}p_{i},&\\text{if }i\\leq k,\\\\ N^{\\alpha}p_{i},&\\text{if }k\\geq N,\\\\ 0,&\\text{otherwise,}\\end{cases}\\]\n' +
      '\n' +
      '그림 20: **Hutter LLM.** 데이터의 실제 분포는 지수 \\(\\beta=2\\)가 있는 Zipf입니다. 여기에서 확장 가능한 리소스는 깨끗한 데이터 또는 AI 데이터 생성 데이터이며, 순위 \\(k\\)에서 꼬리 절단이 있는 실제 데이터의 버전에 해당한다(여기에서 \\(k=10\\)를 사용한다). 우리는 다른 자원의 고정된 양(여기서는 \\(T^{\\prime}=10^{4}\\) 샘플)과 혼합하거나 전혀 혼합하지 않는다. 그런 다음 \\(T\\)를 크랭킹하여 확장 가능한 리소스를 확장합니다. Corollary 3.3에서 예측한 것처럼, 주황색 곡선은 항상 그로크입니다: AI 합성 데이터는 실제 데이터가 부족한 체제에서 유용합니다; 다시 한 번 실제 데이터가 모델 그로크에서 잠시 사용 가능하게 된 다음 AI 합성 데이터를 잊어버립니다. 녹색 곡선(AI 데이터만)과 적색 곡선(AI + 실제 데이터)은 선택적 리소스(실제 데이터)가 스케일링되고 있지 않기 때문에 그로kk가 되지 않는다는 점에 유의하라; 만약 그것이 또한 스케일링된다면, 녹색과 적색은 (그림 3에서와 같이) 증명 가능하게 그로kk가 될 것이다. 대각선 파선은 표준 Hutter 스케일링 법칙 \\(E_{test}\\asymp T^{-c}\\), 여기서 \\(c:=1-1/\\beta\\)에 해당한다. 수평 파선은 정리 2.1에 의해 예측된 \\(E_{test}\\asymp k^{-(\\beta-1)}\\)와 \\(E_{test}\\asymp T^{\\prime-c}\\)에 해당한다.\n' +
      '\n' +
      '단순화를 위해, \\(N\\geq k+1\\)를 가정하자(그렇지 않으면, 우리는 모두 \\(p\\)) 이 분포에서 \\(T\\) 크기의 iid 샘플로부터 "Hutter" LLM을 빌드합니다 (이는 \\(q\\)의 \\(T\\) 샘플과 \\(pi\\)의 \\(T\\) 샘플을 혼합하는 것과 동일합니다). 그러면, 테스트 에러가 다음과 같이 주어지는 것을 쉽게 알 수 있다.\n' +
      '\n' +
      '\\[E_{test}=\\sum_{i\\geq 1}p_{i}(1-q_{i}^{\\prime})^{T}\\asymp\\sum_{1 \\leq i\\leq k}p_{i}(1-p_{i})^{T}+ \\tag{36}\\] \\[\\sum_{k+1\\leq i\\leq N-1}p_{i}+\\sum_{i\\geq N}p_{i}(1-N^{\\alpha}p_{i})^{T}}.\\]\n' +
      '\n' +
      '이전 계산 덕분에 우리는 큰 \\(k\\), \\(N\\) 및 \\(T\\)에 대해 알고 있다.\n' +
      '\n' +
      '* 첫 번째 합은 순서 \\(T^{-c}\\left(\\Gamma(c,Tk^{-\\beta})-\\Gamma(c,T)\\right)=O(T^{-c})\\이다.\n' +
      '* 세 번째 합은 \\(T^{-c}\\left(\\Gamma(c,0)-\\Gamma(c,TN^{\\alpha}N^{-\\beta})\\right)=T^{-c}\\left( \\Gamma(c,0)-\\Gamma(c,TN)\\right)\\asymp T^{-c}\\이다.\n' +
      '* 두 번째 합은 \\(k^{-\\alpha}-N^{-\\alpha}=((\\frac{N}{k})^{\\alpha}-1)N^{-\\alpha}\\이고, 여기서 \\(\\alpha:=\\beta-1\\).\n' +
      '\n' +
      '추측해 보자\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-c}+\\left(\\left(\\frac{N}{k}\\right)^{\\alpha}-1 \\right)N^{-\\alpha},\\text{ for large }k,N,T, \\tag{37}\\]\n' +
      '\n' +
      '그리고 그 결과는 다음과 같다.\n' +
      '\n' +
      '## 꼬리 빅그램 모델에 대 한 부록 D 증명 (섹션 4)\n' +
      '\n' +
      '### Warm-up: 클래식 Hutter 설정 다시 보기\n' +
      '\n' +
      '정상성 검사로서 식 (17)의 프레임워크를 사용하여 고전적인 Hutter에서와 같이 모든 \\(i\\)에 대해 \\(p(\\cdot\\mid i)=\\delta_{y_{i}}\\)인 비자기회귀 설정을 순간적으로 고려하자. 그런 다음 쉬운 계산은 다음을 보여줍니다.\n' +
      '\n' +
      '\\[TV(q_{T}(\\cdot\\mid i),p(\\cdot\\mid i))=1-q_{T}(y_{i}\\mid i)+ \\sum_{j\\neq y_{i}}q_{T}(j\\mid i)=2(1-q_{T}(y_{i}\\mid i))\\]\n' +
      '\n' +
      '이제, 작성에 의해, \\(q_{T}(y_{i}\\mid i)=1[i\\in\\mathcal{D}_{T}]\\). 따라서,\n' +
      '\n' +
      '\\[\\mathbb{E}\\left[1-q_{T}(y_{i}\\mid i)\\right]=\\mathbb{P}(i\\not\\in \\mathcal{D}_{T})=(1-p_{i})^{T}.\\]\n' +
      '\n' +
      '추측해 보자\n' +
      '\n' +
      '\\[\\mathbb{E}\\left[TV(q_{T}(\\cdot\\mid i),p(\\cdot\\mid i))\\right]=2(1-p_{i})^{T}.\\]\n' +
      '\n' +
      'Therefore,\n' +
      '\n' +
      '\\[E_{test}=\\sum_{i}p_{i}\\mathbb{E}\\left[TV(q_{T}(\\cdot\\mid i),p( \\cdot\\mid i))\\right] \\tag{38}\\] \\[=2\\sum_{i}p_{i}(1-p_{i})^{T}\\asymp T^{-(1-1/\\beta)},\\]\n' +
      '\n' +
      '그리고 고전적인 허터 결과를 복구한다! 따라서 (17)에 정의된 테스트 메트릭은 개념적으로 올바른 방향을 가리키고 있습니다.\n' +
      '\n' +
      '### Theorem 4.1의 증명\n' +
      '\n' +
      '그 증명은 (Berend & Kontorovich, 2012)의 결과에 근거할 것이다. ()Upper-Bound.Observe that for any choice for mappings \\(\\pi_{1},\\pi_{2},\\ldots\\), we have\n' +
      '\n' +
      '\\[a_{T}(i) :=\\sum_{j\\,|\\,p(j|i)\\leq 1/n_{T}(i)}p(j\\mid i)\\asymp\\sum_{j\\,|\\,p(j|i)\\geq n_{T}(i)^{1/\\beta}}k^{-\\beta}\\asymp n_{T}(i)^{-1/2}\\sum_{j\\,|\\,p(j|i)\\geq n_{T}(i)\\sqrt{p(j\\mid i)}\\asymp n_{T}(i)^{-1/2}\\sum_{j\\,|\\,p(j|i)\\leq n_{T}(i)^{1/\\beta}}\\pi_{i}(j)^{-\\beta/2}\\] \\[\\lesssim n_{T}(i)^{-1/2}\\sum_{k\\,|\\,k\\leq n_{T}(i)^{1/\\beta}}k^{-\\beta/2}\n' +
      '\n' +
      '우리는 임의의 \\(i\\)에 대해 \\(c_{T}(i):=a_{T}(i)+b_{T}(i)\\lesssim n_{T}(i)^{-c}\\라고 추론한다. 중요한 것은 숨겨진 상수가 \\(i\\)에 의존하지 않는다는 것입니다. 따라서 [Lemma 9](Berend & Kontorovich, 2012) 덕분에 우리는\n' +
      '\n' +
      '[E_{test}\\leq\\sum_{i}p_{i}\\mathbb{E}\\left[n_{T}(i)\\right] \\lesssim\\sum_{i}p_{i}\\mathbb{E}\\left[n_{T}(i)\\right]\\stackrel{{ (*)}}{{\\leq}}\\sum_{i}p_{i}(\\mathbb{E}\\left[n_{T}(i)\\right])^{-c}= \\sum_{i}p_{i}(Tp_{i})^{-c}= T^{-c}\\sum_{i}p_{i}^{1-c} \\tag{39}\\] \\[\\lesssim T^{-c},\\\n' +
      '\n' +
      '여기에서 우리는 함수 \\(x\\mapsto x^{-c}\\)가 오목하기 때문에 (*)에서 젠슨의 부등식을 사용했다.\n' +
      '\n' +
      'Lower-Bound.WLOG6은 \\(\\pi_{i}(j)=j\\)에 의해 정의된 순열의 다음과 같은 특정 선택을 고려한다(즉, \\(i\\)에 의존하지 않는다). 그럼\n' +
      '\n' +
      '각주 6: 음수가 아닌 수( \\(a_{T}(i)\\) 및 \\(b_{T}(i)\\)와 같은)의 합이 가능한 시리즈는 값을 변경하지 않고 재정렬할 수 있다.\n' +
      '\n' +
      '\\[a_{T}(i) =\\sum_{j\\geq nT(i)^{1/\\beta}}j^{-\\beta}\\asymp nT}(i)^{-(1-1/ \\beta)},\\] \\[b_{T}(i) =n_{T}(i)^{-1/2}\\sum_{j\\leq nT(i)^{1/\\beta}}j^{-\\beta}\\asymp nT}(i)^{-c}}.\\]\n' +
      '\n' +
      '\\(E_{test}\\)와 [명제 5](Berend & Kontorovich, 2012)의 정의 덕분에, 우리는 만약 \\(\\beta\\in(1,2)\\)이라면, 그렇게 추론한다.\n' +
      '\n' +
      '[E_{test}\\geq\\sum_{i}p_{i}\\mathbb{E}\\left[(a_{T}(i)+b_{T}(i)-n_{T}(i)^{-1/2}) \\right]\\asymp\\sum_{i}p_{i}\\mathbb{E}\\left[n_{T}(i)^{-1/2}) \\right]\\asymp\\sum_{i}p_{i}\\mathbb{E}\\left[n_{T}(i)^{-1/2}) \\tag{40}\\]\n' +
      '\n' +
      'i.e \\(E_{test}\\gtrsim\\sum_{i}p_{i}\\mathbb{E}\\left[n_{T}(i)^{-c}\\right]\\). 이제 \\(n_{T}(i)\\sim Bin(T,p_{i})\\이기 때문에 표준 이항 농도 인수는 \\(n_{T}(i)\\leq 1.5Tp_{i}\\) w.p \\(1-e^{-Cp_{i}T}\\) 여기서 \\(C\\)는 절대 상수임을 알려준다. 추측해 보자\n' +
      '\n' +
      '\\[E_{test}\\gtrsim\\sum_{i}p_{i}(1.5Tp_{i})^{-c}(1-e^{-Cp_{i}T})\\asymp T^{-c} \\sum_{i}p_{i}^{1-c}-T^{-c}\\underbrace{\\sum_{i}p_{i}^{1-c}e^{-Cp_{i}T}}_{o(1)} \\asymp T^{-c},\\]\n' +
      '\n' +
      '그것은 증명을 완성한다.\n' +
      '\n' +
      '### Theorem의 증명 4.2\n' +
      '\n' +
      '정리의 증명(4.1)의 (39)와 (40)에서 \\(n_{T}(i)\\wedge k^{\\beta}\\)를 \\(n_{T}(i)\\wedge k^{\\beta})^{-c}=n_{T}(i)^{-c}\\lor k^{-\\beta c}\\asymp n_{T}(i)^{-c}+k^{-\\beta c}\\)라는 기본적인 사실을 이용하면 충분하다. 나머지 증명은 정리 4.1의 증명으로 진행된다.\n' +
      '\n' +
      '### Extensions\n' +
      '\n' +
      '위의 설정은 다음 설정으로 확장될 수 있습니다.\n' +
      '\n' +
      '\\[p(j\\mid i)=\\rho(\\pi_{i}(j)),\\]\n' +
      '\n' +
      '여기서 \\(\\rho\\)는 \\(\\mathbb{N}_{*}\\) 상의 분포이다. 특히 \\(\\rho(z)\\propto z^{-\\beta}\\)를 취하면 위에서 고려한 설정을 복구한다. 정리 4.1의 증명 역학이 여기에 적용되어야 함은 명백하며, 이는 \\(\\rho\\)에 명시적으로 의존하는 확장 법칙으로 이어진다.\n' +
      '\n' +
      '## Triplet Scaling Law의 부록 E 증명 및 예시 (정리 5.1)\n' +
      '\n' +
      '임의의 \\(i\\)에 대해 평균적으로 \\(p\\)에서 \\(1/p_{i}\\) iid 샘플을 사용하여 컨텍스트 \\(i\\)를 한 번 이상 확인합니다. 순위 \\(k\\)에서 꼬리 절단의 효과는 표본 크기 \\(T\\)를 \\(\\min(T,T_{k})\\)로 대체하는 데 효과적이며, 여기서 \\(T_{k}=\\max\\{1/p_{i}\\mid i\\in[k]\\}\\)이다. \\(p=Zipf(\\beta)\\)의 경우 \\(T_{k}=1/p_{k}\\asymp k^{\\beta}\\)를 갖는다. 한편, Zipf 데이터에 대한 (Cabannes et al., 2023)에서 제안된 모델(18)은, 테스트 에러가 기입된다.\n' +
      '\n' +
      '\\[E_{test}\\asymp T^{-c}+d^{-c_{q}}, \\tag{41}\\]\n' +
      '\n' +
      '여기서, \\(c:=1-1/\\beta\\in(0,1)\\)과 지수 \\(c_{q}\\in(0,\\infty)\\)는 \\(\\beta\\)와 (18)의 메모리 행렬에 임베딩을 갱신하는 알고리즘 \\(q\\)에 의존한다. We deduce the tail-cutting at rank \\(k\\) changes the test error to\n' +
      '\n' +
      '\\[E_{test}\\asymp\\min(T,T_{k})^{-c}+d^{-c_{q}}\\asymp T^{-c}+k^{-\\beta c}+d^{-c_{q}},\\]\n' +
      '\n' +
      '(p<0.05).\n' +
      '\n' +
      '그림 21은 트리플렛 스케일링 법칙을 확인한다.\n' +
      '\n' +
      '그림 21: **용량 제한 메모리 모델.** 정리 5.1에서 확립된 트리플렛 스케일링 법칙의 경험적 확인\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:25]\n' +
      '\n' +
      '그림 24: **복잡성 손실이 있는 자기 회귀 빅그램 모델** 각 시퀀스 데이터의 길이는 100입니다. \\(T_{0}=10,000\\) 샘플에 대해 학습된 초기 모델입니다. Gen 1에 대한 \\(T\\) 샘플을 생성한다. Gen 2 모델에서 시작하는 것은 이전 세대의 가장 강력한 모델에 의해 생성된 데이터에 대해 훈련된다. Top-p 1, 온도 1, \\(\\beta=3/2\\).\n' +
      '\n' +
      '그림 25: **복잡성 손실이 있는 자기 회귀 빅그램 모델** 각 시퀀스 데이터의 길이는 100입니다. \\(T_{0}=10,000\\) 샘플에 대해 학습된 초기 모델입니다. Gen 1에 대한 \\(T\\) 샘플을 생성한다. Gen 2 모델에서 시작하는 것은 이전 세대의 가장 강력한 모델에 의해 생성된 데이터에 대해 훈련된다. Top-p 1, 온도 1, \\(\\beta=3/2\\).\n' +
      '\n' +
      '그림 26: **복잡성 손실이 있는 자기 회귀 빅그램 모델** 각 시퀀스 데이터의 길이는 100입니다. 그림 25와 동일한 설정입니다. Top-p 0.9, 온도 1, \\(\\beta=3/2\\).\n' +
      '\n' +
      '## Transformer 산술 실험 부록 G 세부 정보 및 결과\n' +
      '\n' +
      '차톤(Charton,2023)은 일부 염기 \\(B\\)에서 숫자의 서열로 인코딩된 두 양의 정수의 최대 공약수(GCD)를 예측하기 위해 서열 간 변환기를 훈련시킨다. 그는 모델 예측이 결정론적임을 관찰한다: GCD \\(k\\)가 있는 모든 쌍 \\((a,b)\\)에 대해 모델은 단일 값 \\(f(k)\\를 예측한다. 예측은 GCD가 밑변의 제수 또는 작은 소수의 곱일 때 정확하다(즉, \\(f(k)=k\\)). 다른 모든 경우에, 모델 예측은 \\(k\\)를 분할하는 \\(f(l)=l\\)이 되도록 하는 가장 큰 정확한 예측(즉, \\(l\\))이다. 올바른 예측 목록 \\(\\mathcal{L}\\)은 인코딩 기본 \\(B\\)에 따라 달라집니다. 예를 들어, \\(B=10\\)의 경우, 3억 개의 예제 후에 모델은 \\(\\mathcal{L}=\\{1,2,4,5,8,10,16,20,25,40,50,80,100...\\}\\)을 정확하게 예측하고, \\(20\\)와 \\(30\\)의 GCD는 \\(10\\)로 올바르게 예측하지만, \\(210\\)와 \\(140\\)의 GCD는 \\(70\\) 대신 \\(10\\)로 잘못 예측한다.\n' +
      '\n' +
      '이 모델을 사용하여 "더러운" 학습 데이터 \\(\\mathcal{D}(B)\\): 베이스 \\(B\\)를 사용하여 훈련된 변압기에 의해 생성된 정수 \\((a,b)\\)와 그 (때로는 부정확한) 의사 GCD 쌍을 균일하게 샘플링한다. 참고: 이 데이터 세트는 원하는 만큼 클 수 있습니다. 또한 정확한 학습 데이터세트 \\(\\mathcal{C}(B)\\), 샘플링 쌍 \\((a,b)\\) 및 올바른 GCD를 생성한다.\n' +
      '\n' +
      '이 실험에서 우리는 \\(B)\\의 다른 값에 대해 \\(\\mathcal{D}(B)\\)와 \\(\\mathcal{C}(B)\\에 대한 모델을 학습한다. 우리의 목표는 "더러운" 데이터에 대한 광범위한 훈련이 모델 정확도에 영향을 미치는지 여부를 결정하는 것이다.\n' +
      '\n' +
      '우리는 \\(6\\) 염기: \\(B=10,420,1000,2017,2023\\) 및 \\(4913\\)에 초점을 맞추고, 1백만에서 100만 사이의 정수 약 \\(300\\)에 걸쳐 트랜스포머를 올바른 GCD에 훈련시킨 후 표 1에 나열된 성능을 달성한다. 여기서 정확도는 모델이 올바르게 예측할 수 있는 무작위 균일 쌍 \\((a,b)\\)의 비율을 나타내며, 올바른 GCD는 모델이 올바르게 예측할 수 있는 \\(100\\) 아래의 GCD 수 \\(f(k)=k\\)), 올바른 모델 예측은 관련 집합의 숫자의 곱이다. 이 모델들은 \\(\\mathcal{D}(B)\\)를 생성하는 데 사용된다.\n' +
      '\n' +
      '이 실험에서 모든 모델은 4개의 레이어, 512개의 치수 및 8개의 주의 헤드를 갖는다. 인코더 전용 모델(17.2M 파라미터)과 인코더-디코더 모델(38.7M 파라미터)의 두 가지 아키텍처를 고려한다. 인코더 전용 모델은 \\(2.25\\)배 더 적은 파라미터를 가지며, 두 배 더 빠르게 훈련되며, 성능 페널티가 발생하지 않는다.\n' +
      '\n' +
      '그런 다음 AI 데이터(위의 모델에 의해 생성됨)에서 GCD를 예측하기 위해 새 모델(동일한 아키텍처로)을 훈련하고 GCD의 올바른 계산에서 올바른 데이터로 훈련하는 것과 비교한다. 소수의 예(1억 원 미만)에 대해 훈련될 때, AI 데이터로부터 학습하는 모델들은 더 나은 정확도를 달성한다(표 2). 이것은 AI 데이터가 모든 하드 케이스를 매끄럽게 하기 때문에 초기 단계에서 더 깨끗한 신호로 모델을 제시하기 때문이라고 생각합니다.\n' +
      '\n' +
      '이 패턴은 광범위한 훈련 후에 변경됩니다. 표 3은 \\(300M\\) 및 \\(1\\) billion에 대해 훈련된 모델의 성능을 비교한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c} \\hline \\hline Base & Accuracy & Correct GCD & Correct predictions \\\\ \\hline\n' +
      '10 & 85 & 13 & \\{1,2,4,8,16\\} & \\{1,5,25\\} \\\\\n' +
      '420 & 97 & 38 & \\{1,2,4,8,16\\} & \\{1,3,9\\} & \\{1,5,25\\} \\\\\n' +
      '1000 & 94 & 22 & \\{1,2,4,8,16\\} & \\{1,5,25\\} & \\{1,3\\} \\\\\n' +
      '2017 & 85 & 4 & \\{1,2\\} & \\{1,3\\} & \\\\\n' +
      '2023 & 91 & 16 & \\{1,2,4\\} & \\{1,3\\} & \\{1,7\\} & \\{1,17\\} \\\\\n' +
      '4913 & 93 & 17 & \\{1,2,4\\} & \\{1,3\\} & \\{1,5\\} & \\{1,17\\} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 3억 개의 예제에서 GCD를 예측하도록 훈련된 **초기 성능.**\\(4\\)-계층 변압기입니다. 우리의 _test_ 집합에는 최대 \\(100\\)의 GCD만 포함되어 있으며 정확도는 각 GCD가 동일한 발생으로 재가중 테스트에서 계산됩니다. 따라서, Correct GCD는 세트들(처음 100 GCD 내)에 걸쳐 생성물들을 형성함으로써 정확한 예측들로부터 형성될 수 있는 모든 것들을 열거한다. 우리는 이 단계에서 0세대 모델을 동결하고 그 예측을 사용하여 합성 데이터를 생성한다. 정확한 예측 집합 밖의 각 GCD에 대해 모델은 지상 진리를 나누는 가장 큰 GCD를 예측할 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c|c c|c c} \\hline \\hline  & \\multicolumn{2}{c}{30M examples} & \\multicolumn{2}{c}{60M examples} & \\multicolumn{2}{c}{90M examples} \\\\ Base & AI & Correct & AI & Correct & AI & Correct \\\\ \\hline\n' +
      '10 & 13 & 13 & 13 & 13\n' +
      '420 & 34 & 34 & 38 & 38 & 35 \\\n' +
      '1000 & 17 & 13 & 13 & 22 & 14 \\\n' +
      '2017 & 4 & 2 & 4 & 2 & 4 & 4 & 4 \\\n' +
      '2023 & 6 & 6 & 11 & 11 & 6 \\\n' +
      '4913 & 6 & 4 & 7 & 7 & 7 & 7 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **30, 60 및 9000만 예 후에 GCD를 올바르게 예측했습니다.* * 지저분하고 올바른 데이터 세트입니다.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:28]\n' +
      '\n' +
      '그림 31: **Learning the GCD.** Learning curve, base 4319. Orange: training on correct GCD. 파란색: AI 생성 데이터에 대한 훈련입니다.\n' +
      '\n' +
      '그림 30: **기술의 출현(GCD 그룹이 함께 학습됨).** 원본(아래) 및 AI 합성 데이터(위)입니다. 베이스 4913. 각각 클린/AI 데이터에 대한 1 모델.\n' +
      '\n' +
      '10개의 종자에 대한 평균 곡선은 상대적으로 적은 양의 AI 데이터가 있는 혼합물에 대한 그로킹과 같은 지연 학습을 식별할 수 있다. 이러한 효과는 연구될 수 있다.\n' +
      '\n' +
      '데이터를 생성하는 데 사용된 모델은 약 300M 예제에 대해 훈련되었으며, 염기 1000, 2023 및 4913에 대해 각각 100 미만의 22, 16 및 17 GCD를 정확하게 예측했다. 우리는 AI-데이터 데이터에 대한 더 많은 훈련만이 이러한 성능을 향상시키지 않을 것이라는 것을 알고 있다(표 3). 반면에, 우리는 깨끗한 데이터에 대해 훈련된 모델이 더 큰 성능을 달성할 것이라는 것을 알고 있다. 구체적으로, 클린 데이터에 대해 트레이닝된 10개의 모델 중, 베이스 1000에 대해, 10개 모두는 1.4B 예 후에 23개의 GCD 이상을 예측한다. 모델이 23 GCD 이상을 예측하는 데 필요한 예제의 중앙값은 465M이다. 베이스 2023의 경우 10개 중 7개 모델이 2.1B 예 후에 17개 GCD 이상을 예측한다. 더티 데이터에 대해서만 훈련된 모델을 가장 잘 훈련시킨 훈련 샘플의 중앙값은 530M이다. 마지막으로 염기 4913의 경우 10개 중 9개의 깨끗한 모델이 1.7B 예 후에 18개 이상의 GCD를 예측한다. 표본의 중앙값은 1.1B입니다.\n' +
      '\n' +
      '혼합 모델이 AI 훈련 모델로 "학습할 수 없는" GCD를 예측하는 방법을 배울 때로 확대하면 그로킹 효과가 더 분명해진다.\n' +
      '\n' +
      '표 4는 혼합물 모델들이 순수하게 AI-트레이닝된 모델이 학습할 수 없는 GCD를 최종적으로 학습하는 시간(#의 샘플)을 나열하여 요약하고, 이전 GCD가 학습되었기 이후의 지연(백만 샘플 단위)을 요약한다(청정 모델과 AI-트레이닝된 모델 간의 비교를 설명하기 위해 도 30 참조):\n' +
      '\n' +
      '지연 기간은 혼합에서 AI 데이터의 비율이 증가함에 따라 증가한다. 따라서 표 4는 우리의 이론7에 의해 예측된 바와 같이 AI 데이터의 분율로 안정기 길이를 증가시키는 그로킹 효과를 명확하게 보여준다.\n' +
      '\n' +
      '각주 7: 컴퓨팅 리소스의 과도한 사용으로 인해 대부분의 경우 약 3B 샘플 후에 실험을 중단하도록 제한되었다. 이것은 아마도 더 큰 AI 혼합물의 경우 몇 가지 실험만이 새로운 GCD를 성공적으로 찾을 수 있었던 이유를 설명할 것이다.\n' +
      '\n' +
      '## 부록 H Llama2를 사용한 실험 세부 정보\n' +
      '\n' +
      '대형 언어 모델(LLM)의 영역에서, 기존의 접근법은 사전 훈련 및 미세 조정 패러다임을 포함한다. 예를 들어 GPT-3는 다양한 출처에서 약 45TB의 텍스트 데이터에 대한 사전 교육을 받는다. 이러한 광범위한 사전 훈련은 제로 샷 학습, 적은 샷과 같은 방법을 사용하는 다양한 다운스트림 작업에 대한 강력한 기능을 제공한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c|c c c|c c c} \\hline \\hline  & \\multicolumn{3}{c|}{Base 1000} & \\multicolumn{3}{c}{Base 2023} & \\multicolumn{3}{c}{Base 4913} \\\\ mixture rate & successes & samples (M) & delay & successes & sample (M) & delay & successes & samples (M) & delay \\\\ \\hline\n' +
      '0\\% (clean) & 10/10 & 465 & 243 & 7/10 & 530 & 567 & 10/10 & 1180 & 520 \\\\\n' +
      '9\\% & 8/10 & 560 & 320 & 8/10 & 715 & 530 & 9/10 & 910 & 340 \\\\\n' +
      '27\\% & 5/10 & 790 & 560 & 7/10 & 790 & 1220 & 10/10 & 1390 & 680 \\\\\n' +
      '50\\% & 2/10\\({}^{*}\\) & 1310\\({}^{*}\\) & 190\\({}^{*}\\) & 7/10 & 1140 & 1220 & 8/10 & 1280 & 1180 \\\\\n' +
      '73\\% & 0 & - & - & 0 & - & - & 0 & - & - \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **혼합 모델까지 샘플 AI 학습 모델이 학습할 수 없는 GCD를 학습합니다.* * 적은 수의 실험\n' +
      '\n' +
      '그림 32: **혼합 데이터에 대한 GCD 학습에서 Grokking.** 깨끗한 데이터와 AI 생성 GCD 데이터의 혼합에 대해 학습된 모델의 오류 손실입니다. 10개 모델. 왼쪽에서 오른쪽으로: 베이스 4913, 2023, 1000.\n' +
      '\n' +
      '학습 또는 미세 조정. 우리의 연구는 현대 합성 데이터 시대에 가까운 시나리오에서 모델 붕괴 현상을 평가한다.\'\n' +
      '\n' +
      '가장 진보된 오픈 소스 모델 중 하나인 라마-2 7B를 사용하여, 본 연구는 LLM이 다른 LLM에서 생성된 데이터로 미세 조정 8을 겪을 때 LLM에 미치는 영향을 조사한다. 고품질 데이터의 생성을 보장하고 관련 있지만 사소한 다운스트림 작업을 제공하기 위해 위키텍스트-103 데이터 세트를 사용한다. 이 데이터 세트를 128개 토큰의 청크로 분할하고 각각 64개 토큰의 스트라이드를 사용하여 약 220만 개의 청크를 생성한다. 이 데이터 집합을 \\(\\mathcal{D}_{0}\\)로 표현합니다. 생성 작업은 원본 데이터 세트의 각 청크로부터 초기 96개의 토큰이 주어진 최종 32개의 토큰을 생성하는 것을 포함한다. 초기 세대(0세대)에서는 \\(\\mathcal{D}_{0}\\)에 세밀하게 조정된 Llama-2 7B FT 모델을 사용하여 최종 32개 토큰의 교차 엔트로피 손실에만 초점을 맞춘 세대 손실을 적용한다. 이 초기 모델은 \\(\\mathcal{M}_{0}\\)로 표시되며, 이는 표준 Llama-2 7B 모델에 비해 생성 작업에 대한 향상된 용량을 보여준다. \\(\\mathcal{M}_{0}\\)로부터 원래의 96개의 토큰으로 \\(\\mathcal{M}_{0}\\)을 질의하여 데이터세트 \\(\\mathcal{D}_{1}\\)를 생성한 후 이 데이터세트에서 라마-2 7B를 미세조정하여 \\(\\mathcal{M}_{1}\\)을 얻는다. 이러한 과정을 순차적으로 반복하여 \\(\\mathcal{M}_{i-1}\\)로부터 \\(\\mathcal{D}_{i}\\)를 생성하고, 미세조정을 통해 \\(\\mathcal{M}_{i}\\)를 얻는다. Wikitext-103에서 파생된 테스트 세트에서 128토큰 청크로 분할된 다양한 \\(\\mathcal{M}\\) 모델의 성능을 비교하여 LLM에서 모델 붕괴를 조사하는 것을 목표로 한다.\n' +
      '\n' +
      '각주 8: 인용(Shumailov et al., 2023)을 인용하면, 우리는 원칙적으로 스케일링 법칙 붕괴를 입증하기 위해 처음부터 LLM을 훈련시키면서 여기에 설명된 실험을 복제할 수 있다고 명시한다. 단일 적당히 큰 모델을 훈련하면 미국의 수명 값인 CO2의 두 배가 생성된다는 점을 감안할 때(Strubell 등, 2019), 우리는 그러한 실험을 실행하지 않고 대신 더 실현 가능한 미세 조정 설정에 초점을 맞추기로 결정했다. 논문에 설명된 언어 실험만 실행하는 데 몇 주가 걸렸습니다.\n' +
      '\n' +
      '청크 간 정보 유출을 방지하기 위해 모든 세대에 대한 최종 32개 토큰의 손실만 포함하도록 교육을 제한한다. 결과적으로 모델은 원래 말뭉치에서 나오는 처음 96개의 토큰에 대해 훈련되지 않는다. 220만 청크의 크기는 라마-2 7B의 용량을 고려할 때, 오버피팅을 피하면서 미세조정을 위한 충분한 데이터를 제공할 수 있다. 미세조정 과정 내내 LoRA의 학습률 \\(5e^{-5}\\), Adam optimizer, dropout rate 0.1, trainingable parameter fraction 0.062%를 사용하여 일관된 설정을 유지한다. 샘플링 부족으로 인한 모델 붕괴 가능성을 제거하고 모델이 훈련(또는 미세 조정)된 것보다 더 많은 AI 생성 데이터가 생성되는 시나리오에 대한 통찰력을 얻기 위해 후속 데이터 세트를 생성하기 위해 데이터 세트의 절반에 대해 훈련된 모델을 일관되게 활용한다.\n' +
      '\n' +
      '완전성을 위해 전체 청크에 손실이 있는 그림 33과 생성된 데이터를 원본 데이터와 혼합하는 그림 34를 포함한다. 혼합 곡선은 이론에서 예측한 그로킹 현상과도 잘 부합한다.\n' +
      '\n' +
      '그림 33: **Lama Generated Data.** 학습 및 평가를 위한 손실이 프롬프트를 포함한 청크의 모든 토큰에 대한 교차 엔트로피인 경우 Lama2 미세 조정.\n' +
      '\n' +
      '## 부록 I 꼬리 자르기 및 꼬리 좁힘 효과에 대한 추가 연구\n' +
      '\n' +
      '여기서는 다음 토큰 분포에서 꼬리 절단이 복잡성과 같이 전체 시퀀스를 고려하는 메트릭에 대해 꼬리 좁힘으로 이어질 수 있는 방법을 설명한다. 그림 35는 자기회귀 바이그램 모델에 대한 이를 보여준다. 이 효과는 복잡도와 같은 가법(또는 승법) 측정을 고려할 때 얻을 수 있는 조합 요인 때문일 수 있다.\n' +
      '\n' +
      '그림 35: 순차적인 빅그램 데이터: 상위 p = 0.95는 꼬리 좁힘과 유사한 효과를 가져온다. 시퀀스 길이가 100인 1000개의 데이터.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.07965] Rho-1: Not All Tokens Are What You Need</title><meta property="og:description" content="Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens.
Challenging this norm, we posit that “Not all tokens in a corpus are equally important for langua…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Rho-1: Not All Tokens Are What You Need">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Rho-1: Not All Tokens Are What You Need">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.07965">

<!--Generated on Sun May  5 19:41:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span id="id15.id1" class="ltx_text ltx_font_smallcaps">Rho-1</span>: Not All Tokens Are What You Need</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenghao Lin<math id="id1.1.m1.1" class="ltx_Math" alttext="~{}~{}^{\chi\phi}" display="inline"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mrow id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml"><mi id="id1.1.m1.1.1.1.2" xref="id1.1.m1.1.1.1.2.cmml">χ</mi><mo lspace="0em" rspace="0em" id="id1.1.m1.1.1.1.1" xref="id1.1.m1.1.1.1.1.cmml">​</mo><mi id="id1.1.m1.1.1.1.3" xref="id1.1.m1.1.1.1.3.cmml">ϕ</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><apply id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"><times id="id1.1.m1.1.1.1.1.cmml" xref="id1.1.m1.1.1.1.1"></times><ci id="id1.1.m1.1.1.1.2.cmml" xref="id1.1.m1.1.1.1.2">𝜒</ci><ci id="id1.1.m1.1.1.1.3.cmml" xref="id1.1.m1.1.1.1.3">italic-ϕ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">~{}~{}^{\chi\phi}</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Zhibin Gou<sup id="id16.15.id1" class="ltx_sup"><span id="id16.15.id1.1" class="ltx_text ltx_font_italic">⋆πϕ</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Yeyun Gong<math id="id3.3.m3.1" class="ltx_Math" alttext="~{}~{}^{\phi}" display="inline"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mi id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><ci id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">~{}~{}^{\phi}</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Xiao Liu<sup id="id17.16.id2" class="ltx_sup"><span id="id17.16.id2.1" class="ltx_text ltx_font_italic">ϕ</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Yelong Shen<sup id="id18.17.id3" class="ltx_sup"><span id="id18.17.id3.1" class="ltx_text ltx_font_italic">ϕ</span></sup>
<br class="ltx_break"><span id="id11.11.6" class="ltx_text ltx_font_bold">
Ruochen Xu<sup id="id11.11.6.1" class="ltx_sup"><span id="id11.11.6.1.1" class="ltx_text ltx_font_medium ltx_font_italic">ϕ</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Chen Lin<sup id="id11.11.6.2" class="ltx_sup"><span id="id11.11.6.2.1" class="ltx_text ltx_font_medium ltx_font_italic">⋄χ</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Yujiu Yang<sup id="id11.11.6.3" class="ltx_sup"><span id="id11.11.6.3.1" class="ltx_text ltx_font_medium ltx_font_italic">⋄π</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Jian Jiao<sup id="id11.11.6.4" class="ltx_sup"><span id="id11.11.6.4.1" class="ltx_text ltx_font_medium ltx_font_italic">ϕ</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Nan Duan<sup id="id11.11.6.5" class="ltx_sup"><span id="id11.11.6.5.1" class="ltx_text ltx_font_medium ltx_font_italic">ϕ</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Weizhu Chen<sup id="id11.11.6.6" class="ltx_sup"><span id="id11.11.6.6.1" class="ltx_text ltx_font_medium ltx_font_italic">ϕ</span></sup></span>

<br class="ltx_break"><sup id="id19.18.id4" class="ltx_sup"><span id="id19.18.id4.1" class="ltx_text ltx_font_italic">χ</span></sup>Xiamen University <sup id="id20.19.id5" class="ltx_sup"><span id="id20.19.id5.1" class="ltx_text ltx_font_italic">π</span></sup>Tsinghua University <sup id="id21.20.id6" class="ltx_sup"><span id="id21.20.id6.1" class="ltx_text ltx_font_italic">ϕ</span></sup>Microsoft 
<br class="ltx_break"><a target="_blank" href="https://aka.ms/rho" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aka.ms/rho</a>
</span><span class="ltx_author_notes">Equal contribution. See author contributions for details. Work done during their internships at Microsoft Research Asia. 🖂:&nbsp;<span id="id22.21.id1" class="ltx_text ltx_font_typewriter">zhenghaolin@stu.xmu.edu.cn</span>;&nbsp;&nbsp;<span id="id23.22.id2" class="ltx_text ltx_font_typewriter">zebgou@gmail.com</span>Correspondence authors.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id24.id1">이전의 언어 모델 사전 트레이닝 방법들은 모든 트레이닝 토큰들에 대해 일률적으로 다음 토큰 예측 손실을 적용하였다. 이 규범에 도전하여 <span class="ltx_text ltx_font_italic" id="id24.id1.1">“말뭉치의 모든 토큰이 언어 모델 훈련에 동등하게 중요한 것은 아니다”</span>이라고 가정한다. 우리의 초기 분석은 언어 모델의 토큰 수준 훈련 역학에 대해 조사하며, 서로 다른 토큰에 대한 뚜렷한 손실 패턴을 드러낸다. 이러한 통찰력을 활용하여 <span class="ltx_text ltx_font_smallcaps" id="id24.id1.2">Rho-1</span>이라는 새로운 언어 모델을 소개한다. 말뭉치의 모든 다음 토큰을 예측하는 방법을 배우는 기존 LM과 달리 <span class="ltx_text ltx_font_smallcaps" id="id24.id1.3">Rho-1</span>은 원하는 분포와 정렬된 유용한 토큰을 선택적으로 훈련하는 SLM(Selective Language Modeling)을 사용합니다. 이 접근법은 참조 모델을 사용하여 토큰들을 사전 트레이닝한 후, 더 높은 초과 손실을 갖는 토큰들에 집중된 손실로 언어 모델을 트레이닝하는 것을 포함한다. 15B OpenWebMath 코퍼스에 대한 지속적인 사전 훈련 시 <span class="ltx_text ltx_font_smallcaps" id="id24.id1.4">Rho-1</span>은 9개의 수학 과제에서 최대 30%의 적은 샷 정확도에서 절대적인 개선을 보여준다. 미세 조정 후 <span class="ltx_text ltx_font_smallcaps" id="id24.id1.5">Rho-1</span>-1B 및 7B는 MATH 데이터 세트에서 각각 40.6% 및 51.8%의 최첨단 결과를 달성했습니다. DeepSeekMath와 사전 훈련 토큰의 3%만 일치합니다. 또한 80B 일반 토큰에 대한 사전 학습 시 <span class="ltx_text ltx_font_smallcaps" id="id24.id1.6">Rho-1</span>은 15개의 다양한 태스크에 걸쳐 평균 6.8%의 향상을 달성하여 언어 모델 사전 학습의 효율성과 성능을 모두 향상시켰다.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.4.2" class="ltx_text" style="font-size:90%;">
We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. <span id="S0.F1.4.2.1" class="ltx_text ltx_font_smallcaps">Rho-1</span> is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling.
SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5-10x faster.
</span></figcaption>
</figure>
<figure id="S0.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x2.png" id="S0.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F2.5.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S0.F2.6.2" class="ltx_text" style="font-size:90%;">
<span id="S0.F2.6.2.1" class="ltx_text ltx_font_bold">Upper:</span> Even an extensively filtered pretraining corpus contains token-level noise.
<span id="S0.F2.6.2.2" class="ltx_text ltx_font_bold">Left:</span> Previous Causal Language Modeling (CLM) trains on all tokens.
<span id="S0.F2.6.2.3" class="ltx_text ltx_font_bold">Right:</span> Our proposed Selective Language Modeling (SLM) selectively applies loss on those useful and clean tokens.
</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">모델 매개변수 및 데이터 세트 크기를 확장하면 대규모 언어 모델에서 다음 토큰 예측 정확도가 지속적으로 상승하여 인공 지능 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et al.</span>, <a class="ltx_ref" href="#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Brown et al.</span>, <a class="ltx_ref" href="#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">OpenAI</span>, <a class="ltx_ref" href="#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Team et al.</span>, <a class="ltx_ref" href="#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>에서 상당한 발전을 가져왔다. 그러나 사용 가능한 모든 데이터에 대한 교육이 항상 최적이거나 실현 가능한 것은 아니다. 그 결과, 학습 문서를 선택하기 위해 다양한 휴리스틱과 분류기 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Brown et al.</span>, <a class="ltx_ref" href="#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Wenzek et al.</span>, <a class="ltx_ref" href="#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>를 사용하는 데이터 필터링의 관행이 중요해졌다. 이러한 기술은 데이터 품질을 크게 개선하고 모델 성능을 향상시킵니다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">그러나 철저한 문서 수준 필터링에도 불구하고 고품질 데이터 세트에는 <a class="ltx_ref ltx_refmacro_autoref" href="#S0.F2" title="Figure 2 ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> (상단)에 설명된 것처럼 학습에 부정적인 영향을 미칠 수 있는 많은 노이즈 토큰이 여전히 포함되어 있다. 이러한 토큰을 제거하면 텍스트의 의미가 변경될 수 있지만 지나치게 엄격한 필터링은 유용한 데이터 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Welbl et al.</span>, <a class="ltx_ref" href="#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Muennighoff et al.</span>, <a class="ltx_ref" href="#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>를 배제하고 편향 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dodge et al.</span>, <a class="ltx_ref" href="#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Longpre et al.</span>, <a class="ltx_ref" href="#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>로 이어질 수 있다. 또한, 연구는 웹 데이터의 분포가 다운스트림 애플리케이션에 대한 이상적인 분포와 본질적으로 일치하지 않는다는 것을 나타낸다. 예를 들어, 토큰 레벨에서의 공통 코퍼스는 예측하기 어려운 환각 또는 매우 모호한 토큰과 같은 바람직하지 않은 콘텐츠를 포함할 수 있다. 모든 토큰에 동일한 손실을 적용하면 비이익 토큰에 대한 계산이 낭비될 수 있으며, 이는 LLM의 잠재력을 단지 평범한 지능으로 제한할 수 있다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">언어 모델이 토큰 수준에서 학습하는 방법을 탐구하기 위해 처음에 훈련 역학, 특히 일반적인 사전 훈련 동안 토큰 수준 손실이 어떻게 진화하는지 조사했다. <a class="ltx_ref ltx_refmacro_autoref" href="#S2.SS1" title="2.1 Not All Tokens Are Equal: Training Dynamics of Token Loss ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.1</span></a>에서는 서로 다른 체크포인트에서 모델의 토큰 복잡도를 평가하고 토큰을 서로 다른 유형으로 분류했다. 우리의 연구 결과는 상당한 손실 감소가 훈련 동안 선택된 토큰 그룹으로 제한된다는 것을 보여준다. 많은 토큰은 이미 학습된 "쉬운 토큰"이고, 일부는 가변적인 손실을 나타내고 수렴에 저항하는 "하드 토큰"이다. 이러한 토큰은 수많은 비효율적인 그라디언트 업데이트로 이어질 수 있습니다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">이러한 분석을 기반으로 새로운 SLM(Selective Language Modeling) 목적으로 훈련된 <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">Rho-1</span> 모델을 소개한다. <a class="ltx_ref ltx_refmacro_autoref" href="#S0.F2" title="Figure 2 ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>(Right)에 도시된 바와 같이, 이 접근법은 전체 시퀀스를 모델에 입력하고 원하지 않는 토큰의 손실을 선택적으로 제거한다. 세부 파이프라인은 <a class="ltx_ref ltx_refmacro_autoref" href="#S2.F4" title="Figure 4 ‣ Reference Modeling ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>: 먼저 SLM은 고품질 말뭉치에 대한 참조 언어 모델을 학습한다. 이 모델은 유틸리티 메트릭을 설정하여 원하는 분포에 따라 토큰을 점수화하여 부정한 토큰과 관련 없는 토큰을 자연스럽게 필터링합니다. 둘째, SLM은 참조 모델을 사용하여 손실(<a class="ltx_ref ltx_refmacro_autoref" href="#S2.SS2.SSS0.Px2" title="Reference Modeling ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.2</span></a>)을 사용하여 말뭉치에서 각 토큰에 점수를 매긴다. 마지막으로, 레퍼런스와 트레이닝 모델 사이에서 높은 초과 손실을 나타내는 토큰들에 대해서만 언어 모델을 트레이닝하고, 다운스트림 애플리케이션들(<a class="ltx_ref ltx_refmacro_autoref" href="#S2.SS2" title="2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.2</span></a>)에 가장 유리한 토큰들을 선택적으로 트레이닝한다.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">우리는 포괄적인 실험을 통해 SLM이 사전 훈련 동안 토큰 효율성을 크게 향상시키고 다운스트림 태스크에서 성능을 향상시킨다는 것을 보여준다. 또한, 본 연구 결과는 SLM이 목표 분포와 관련된 토큰을 효과적으로 식별하여 선택된 토큰으로 훈련된 모델에 대한 벤치마크에 대한 복잡도 점수를 향상시킨다는 것을 나타낸다. <a class="ltx_ref ltx_refmacro_autoref" href="#S3.SS2" title="3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§3.2</span></a>는 수학 연속 사전 훈련에서 SLM의 효과를 보여줍니다. 1B 및 7B <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.1">Rho-1</span>이 GSM8k 및 MATH 데이터 세트에서 CLM 훈련 기준선을 16% 이상 능가합니다. SLM은 <a class="ltx_ref ltx_refmacro_autoref" href="#S0.F1" title="Figure 1 ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>와 같이 최대 10배 빠른 기준선 정확도에 도달한다. 놀랍게도 <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.2">Rho-1</span>-7B는 DeepSeekMath에서 요구하는 500B 토큰과 비교하여 15B 토큰만 사용하는 DeepSeekMath-7B의 최첨단 성능과 일치한다. 미세 조정 시 <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.3">Rho-1</span>-1B 및 7B는 MATH에서 각각 40.6% 및 51.8%를 달성합니다. 특히, <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.4">Rho-1</span>-1B는 초기 GPT-4의 CoT 성능 42.5%에 근접하여 40% 정확도를 초과하는 최초의 1B LM이다. <a class="ltx_ref ltx_refmacro_autoref" href="#S3.SS3" title="3.3 General Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§3.3</span></a>는 일반적인 사전 훈련에서 SLM의 효능을 확인한다: SLM을 사용하여 80B 토큰에 Tinyllama-1B를 훈련하면 15개의 벤치마크에서 평균 6.8% 향상되며 코드 및 수학 과제에서 10% 이상의 이득이 발생한다.</p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x3.png" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F3.3.1.1" style="font-size:90%;">Figure 3</span>:</span><span class="ltx_text ltx_font_bold" id="S1.F3.4.2" style="font-size:90%;">The loss of four categories of tokens during pretraining. <span class="ltx_text ltx_font_medium" id="S1.F3.4.2.1">(a)는 프리트레이닝 동안 H→H, L→H, H→L 및 L→L 토큰의 손실을 보여준다. (b)와 (c)는 각각 사전 훈련 중 L→L과 H→H에서 토큰의 손실이 변동하는 세 가지 경우를 보여준다. </span></span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Selective Language Modeling</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Not All Tokens Are Equal: Training Dynamics of Token Loss</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p1.1">우리의 조사는 표준 사전 훈련 동안 개별 토큰의 손실이 어떻게 진화하는지에 대한 비판적 관점에서 시작한다. 우리는 OpenWebMath의 15B 토큰으로 Tinyllama-1B를 사전 교육하여 모든 1B 토큰 후에 체크포인트를 저장합니다. 그런 다음 약 320,000개의 토큰의 유효성 검사 세트를 사용하여 이러한 간격에서 토큰 수준 손실을 평가한다. <a class="ltx_ref ltx_refmacro_autoref" href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>(a)는 토큰의 손실 궤적에 따라 지속성 높은 손실(H→H), 증가 손실(L→H), 감소 손실(H→L) 및 일관된 낮은 손실(L→L)의 네 가지 범주로 나뉜다. 이러한 범주에 대한 자세한 내용은 <a class="ltx_ref ltx_refmacro_autoref" href="#A2.SS1" title="B.1 More Details of Four Categories Tokens ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§B.1</span></a>를 참조하십시오. 우리의 분석에서는 토큰의 26%만이 현저한 손실 감소(H→L)를 나타내는 반면, 대다수(51%)는 L→L 범주에 남아 있음을 밝혀 이미 학습되었음을 나타낸다. 흥미로운 사실은 토큰의 11%가 지속적으로 도전적(H→H)이며, 이는 높은 애레토릭 불확실성 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hüllermeier and Waegeman</span>, <a class="ltx_ref" href="#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> 때문일 수 있다. 또한 12%의 토큰은 훈련 중에 예상치 못한 손실 증가(L→H)를 경험한다.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p2.1">우리의 두 번째 관찰은 상당한 수의 토큰 손실이 지속적인 변동을 나타내고 수렴에 저항한다는 것이다. <a class="ltx_ref ltx_refmacro_autoref" href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> (b) 및 (c)에 묘사된 바와 같이 많은 L→L 및 H→H 토큰의 손실은 트레이닝 동안 높은 분산을 나타낸다. <a class="ltx_ref ltx_refmacro_autoref" href="#A2.SS2" title="B.2 Non-Converging Tokens in Pretrainig ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§B.2</span></a>에서 우리는 이러한 토큰의 내용을 시각화하고 분석하며 많은 토큰이 잡음이 있음을 발견하는데, 이는 우리의 가설과 일치한다.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p3.1">결과적으로, 우리는 트레이닝 동안 각각의 토큰과 연관된 손실이 전체 손실과 같이 부드럽게 감소하지 않고, 대신에 상이한 토큰들 사이에 복잡한 트레이닝 동적(training dynamic)이 있음을 배운다. 훈련 중에 모델에 집중할 적절한 토큰을 선택할 수 있다면 모델의 훈련 궤적을 안정화하고 효율성을 높일 수 있을 것이다.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Selective Language Modeling</h3>

<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Overview</h4>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">문서-레벨 필터링에서 참조 모델의 실행에 영감을 받아, "선택적 언어 모델링(Selective Language Modeling; SLM)"이라고 불리는 토큰-레벨 데이터 선택의 간단한 파이프라인을 제안한다. 우리의 방법은 <a class="ltx_ref ltx_refmacro_autoref" href="#S2.F4" title="Figure 4 ‣ Reference Modeling ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>에 묘사된 바와 같이 세 단계로 구성된다. 우리는 선별된 고품질 데이터 세트에서 참조 모델을 훈련하는 것으로 시작한다. 그런 다음 이 모델은 사전 훈련 코퍼스 내에서 각 토큰의 손실을 평가한다. 마지막 단계에서는 학습과 참조 모델 간의 초과 손실이 높은 토큰에 초점을 맞추어 언어 모델을 선택적으로 학습한다. 직관은 초과 손실이 높은 토큰이 더 학습 가능하고 원하는 분포와 더 잘 정렬되어, 무관하거나 품질이 낮은 토큰을 자연스럽게 배제한다는 것이다. 아래에서는 각 단계에 대한 상세한 설명을 제공한다.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reference Modeling</h4>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.2">원하는 데이터 분포를 반영하는 고품질 데이터 세트를 선별하는 것으로 시작합니다. 우리는 선별된 데이터에 대해 표준 교차 엔트로피 손실을 사용하여 참조 모델(RM)을 학습한다. 그런 다음 결과 RM은 더 큰 사전 훈련 코퍼스 내에서 토큰 손실을 평가하는 데 사용된다. RM이 이 토큰에 할당할 확률을 기반으로 토큰 <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS2.SSS0.Px2.p1.2.m2.1a"><msub id="S2.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.2.m2.1c">x_{i}</annotation></semantics></math>의 참조 손실(<math alttext="\mathcal{L}_{\text{ref}}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p1.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3a.cmml">ref</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3a.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3"><mtext id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3">ref</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.1.m1.1c">\mathcal{L}_{\text{ref}}</annotation></semantics></math>)을 계산한다. 계산은 다음과 같이 공식화된다:</p>
</div>
<div id="S2.SS2.SSS0.Px2.p2" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="\mathcal{L}_{\text{ref}}(x_{i})=-\log P(x_{i}|x{<i})" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.3.2.cmml">ℒ</mi><mtext id="S2.E1.m1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.3.3a.cmml">ref</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">=</mo><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><mo rspace="0.167em" id="S2.E1.m1.2.2.2a" xref="S2.E1.m1.2.2.2.cmml">−</mo><mrow id="S2.E1.m1.2.2.2.1" xref="S2.E1.m1.2.2.2.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.3" xref="S2.E1.m1.2.2.2.1.3.cmml"><mi id="S2.E1.m1.2.2.2.1.3.1" xref="S2.E1.m1.2.2.2.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E1.m1.2.2.2.1.3a" xref="S2.E1.m1.2.2.2.1.3.cmml">⁡</mo><mi id="S2.E1.m1.2.2.2.1.3.2" xref="S2.E1.m1.2.2.2.1.3.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.2" xref="S2.E1.m1.2.2.2.1.2.cmml">​</mo><mrow id="S2.E1.m1.2.2.2.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.cmml"><msub id="S2.E1.m1.2.2.2.1.1.1.1.2.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.2.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.2.cmml">x</mi><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.2.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.3.cmml">i</mi></msub><mo fence="false" id="S2.E1.m1.2.2.2.1.1.1.1.2.1" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.cmml">|</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml">x</mi></mrow><mo id="S2.E1.m1.2.2.2.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.cmml">&lt;</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.3.cmml">i</mi></mrow><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3"></eq><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><times id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.2"></times><apply id="S2.E1.m1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.3.2">ℒ</ci><ci id="S2.E1.m1.1.1.1.3.3a.cmml" xref="S2.E1.m1.1.1.1.3.3"><mtext mathsize="70%" id="S2.E1.m1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.3.3">ref</mtext></ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><minus id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2"></minus><apply id="S2.E1.m1.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1"><times id="S2.E1.m1.2.2.2.1.2.cmml" xref="S2.E1.m1.2.2.2.1.2"></times><apply id="S2.E1.m1.2.2.2.1.3.cmml" xref="S2.E1.m1.2.2.2.1.3"><log id="S2.E1.m1.2.2.2.1.3.1.cmml" xref="S2.E1.m1.2.2.2.1.3.1"></log><ci id="S2.E1.m1.2.2.2.1.3.2.cmml" xref="S2.E1.m1.2.2.2.1.3.2">𝑃</ci></apply><apply id="S2.E1.m1.2.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1"><lt id="S2.E1.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1"></lt><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2"><csymbol cd="latexml" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1">conditional</csymbol><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.2">𝑥</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3">𝑥</ci></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\mathcal{L}_{\text{ref}}(x_{i})=-\log P(x_{i}|x{&lt;i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.SSS0.Px2.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p3.1">각 토큰에 대해 <math alttext="\mathcal{L}_{\text{ref}}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p3.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p3.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p3.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3a.cmml">ref</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3a.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3"><mtext id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3">ref</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.1.m1.1c">\mathcal{L}_{\text{ref}}</annotation></semantics></math>를 평가하여 선택적 사전 훈련에 대한 참조 손실을 설정하여 언어 모델링에서 가장 영향력 있는 토큰에 집중할 수 있도록 한다.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x4.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="81" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.3.1.1" style="font-size:90%;">Figure 4</span>:</span><span class="ltx_text ltx_font_bold" id="S2.F4.4.2" style="font-size:90%;">The pipeline of Selective Language Modeling (SLM). <span class="ltx_text ltx_font_medium" id="S2.F4.4.2.1"></span></span></figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">The pipeline of Selective Language Modeling (SLM).<span id="S2.F4.4.2.1" class="ltx_text ltx_font_medium">
SLM optimizes language model performance by concentrating on valuable, clean tokens during pre-training.
It involves three steps:
(Step 1) Initially, train a reference model on high-quality data.
(Step 2) Then, score each token’s loss in a corpus using the reference model.
(Step 3) Finally, train the language model selectively on tokens that show higher excess loss compared to the reference loss.
</span></span></figcaption>
</figure>
</section>
<section id="S2.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Selective Pretraining</h4>

<div id="S2.SS2.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p1.1">인과 언어 모델링(CLM)은 교차 엔트로피 손실을 사용한다는 점에 유의한다:</p>
</div>
<div id="S2.SS2.SSS0.Px3.p2" class="ltx_para">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.3" class="ltx_Math" alttext="\mathcal{L}_{\text{CLM}}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}\log P(x_{i}|x_{<i};\theta)" display="block"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml"><mrow id="S2.E2.m1.3.3.3" xref="S2.E2.m1.3.3.3.cmml"><msub id="S2.E2.m1.3.3.3.2" xref="S2.E2.m1.3.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.3.3.3.2.2" xref="S2.E2.m1.3.3.3.2.2.cmml">ℒ</mi><mtext id="S2.E2.m1.3.3.3.2.3" xref="S2.E2.m1.3.3.3.2.3a.cmml">CLM</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.3.1" xref="S2.E2.m1.3.3.3.1.cmml">​</mo><mrow id="S2.E2.m1.3.3.3.3.2" xref="S2.E2.m1.3.3.3.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.3.3.2.1" xref="S2.E2.m1.3.3.3.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S2.E2.m1.3.3.3.3.2.2" xref="S2.E2.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.3.3.2" xref="S2.E2.m1.3.3.2.cmml">=</mo><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.cmml"><mo id="S2.E2.m1.3.3.1a" xref="S2.E2.m1.3.3.1.cmml">−</mo><mrow id="S2.E2.m1.3.3.1.1" xref="S2.E2.m1.3.3.1.1.cmml"><mfrac id="S2.E2.m1.3.3.1.1.3" xref="S2.E2.m1.3.3.1.1.3.cmml"><mn id="S2.E2.m1.3.3.1.1.3.2" xref="S2.E2.m1.3.3.1.1.3.2.cmml">1</mn><mi id="S2.E2.m1.3.3.1.1.3.3" xref="S2.E2.m1.3.3.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.2" xref="S2.E2.m1.3.3.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.1.cmml"><munderover id="S2.E2.m1.3.3.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E2.m1.3.3.1.1.1.2.2.2" xref="S2.E2.m1.3.3.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.E2.m1.3.3.1.1.1.2.2.3" xref="S2.E2.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.2.2.3.2" xref="S2.E2.m1.3.3.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E2.m1.3.3.1.1.1.2.2.3.1" xref="S2.E2.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E2.m1.3.3.1.1.1.2.2.3.3" xref="S2.E2.m1.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m1.3.3.1.1.1.2.3" xref="S2.E2.m1.3.3.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S2.E2.m1.3.3.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><mrow id="S2.E2.m1.3.3.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.3.1" xref="S2.E2.m1.3.3.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E2.m1.3.3.1.1.1.1.3a" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml">⁡</mo><mi id="S2.E2.m1.3.3.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.3.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">x</mi><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">θ</mi></mrow></mrow><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3"><eq id="S2.E2.m1.3.3.2.cmml" xref="S2.E2.m1.3.3.2"></eq><apply id="S2.E2.m1.3.3.3.cmml" xref="S2.E2.m1.3.3.3"><times id="S2.E2.m1.3.3.3.1.cmml" xref="S2.E2.m1.3.3.3.1"></times><apply id="S2.E2.m1.3.3.3.2.cmml" xref="S2.E2.m1.3.3.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.2.1.cmml" xref="S2.E2.m1.3.3.3.2">subscript</csymbol><ci id="S2.E2.m1.3.3.3.2.2.cmml" xref="S2.E2.m1.3.3.3.2.2">ℒ</ci><ci id="S2.E2.m1.3.3.3.2.3a.cmml" xref="S2.E2.m1.3.3.3.2.3"><mtext mathsize="70%" id="S2.E2.m1.3.3.3.2.3.cmml" xref="S2.E2.m1.3.3.3.2.3">CLM</mtext></ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝜃</ci></apply><apply id="S2.E2.m1.3.3.1.cmml" xref="S2.E2.m1.3.3.1"><minus id="S2.E2.m1.3.3.1.2.cmml" xref="S2.E2.m1.3.3.1"></minus><apply id="S2.E2.m1.3.3.1.1.cmml" xref="S2.E2.m1.3.3.1.1"><times id="S2.E2.m1.3.3.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.3"><divide id="S2.E2.m1.3.3.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.3"></divide><cn type="integer" id="S2.E2.m1.3.3.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.3.2">1</cn><ci id="S2.E2.m1.3.3.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.3.3">𝑁</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1"><apply id="S2.E2.m1.3.3.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S2.E2.m1.3.3.1.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.2.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2">subscript</csymbol><sum id="S2.E2.m1.3.3.1.1.1.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.2"></sum><apply id="S2.E2.m1.3.3.1.1.1.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3"><eq id="S2.E2.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S2.E2.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S2.E2.m1.3.3.1.1.1.2.2.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.3.3.1.1.1.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2.3">𝑁</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1"><times id="S2.E2.m1.3.3.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3"><log id="S2.E2.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.1"></log><ci id="S2.E2.m1.3.3.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.2">𝑃</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2">𝑥</ci><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3">𝑖</ci></apply><list id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><lt id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝜃</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">\mathcal{L}_{\text{CLM}}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}\log P(x_{i}|x_{&lt;i};\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.SSS0.Px3.p3" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p3.10" class="ltx_p">Here, <math id="S2.SS2.SSS0.Px3.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{L_{\text{CLM}}}(\theta)" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p3.1.m1.1.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml"><msub id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3a.cmml">CLM</mtext></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1.cmml">​</mo><mrow id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml"><mo stretchy="false" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2.1" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml">(</mo><mi id="S2.SS2.SSS0.Px3.p3.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2"><times id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1"></times><apply id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2">ℒ</ci><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3a.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3"><mtext mathsize="70%" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3">CLM</mtext></ci></apply><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.1.m1.1c">\mathcal{L_{\text{CLM}}}(\theta)</annotation></semantics></math> represents the loss function parameterized by model <math id="S2.SS2.SSS0.Px3.p3.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.2.m2.1a"><mi id="S2.SS2.SSS0.Px3.p3.2.m2.1.1" xref="S2.SS2.SSS0.Px3.p3.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.2.m2.1b"><ci id="S2.SS2.SSS0.Px3.p3.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.2.m2.1c">\theta</annotation></semantics></math>. <math id="S2.SS2.SSS0.Px3.p3.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.3.m3.1a"><mi id="S2.SS2.SSS0.Px3.p3.3.m3.1.1" xref="S2.SS2.SSS0.Px3.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.3.m3.1b"><ci id="S2.SS2.SSS0.Px3.p3.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.3.m3.1c">N</annotation></semantics></math> is the length of the sequence, <math id="S2.SS2.SSS0.Px3.p3.4.m4.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.4.m4.1a"><msub id="S2.SS2.SSS0.Px3.p3.4.m4.1.1" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.4.m4.1b"><apply id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.4.m4.1c">x_{i}</annotation></semantics></math> is the <math id="S2.SS2.SSS0.Px3.p3.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.5.m5.1a"><mi id="S2.SS2.SSS0.Px3.p3.5.m5.1.1" xref="S2.SS2.SSS0.Px3.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.5.m5.1b"><ci id="S2.SS2.SSS0.Px3.p3.5.m5.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.5.m5.1c">i</annotation></semantics></math>-th token in the sequence, and <math id="S2.SS2.SSS0.Px3.p3.6.m6.1" class="ltx_Math" alttext="x_{<i}" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.6.m6.1a"><msub id="S2.SS2.SSS0.Px3.p3.6.m6.1.1" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2.cmml">x</mi><mrow id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2.cmml"></mi><mo id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1.cmml">&lt;</mo><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.6.m6.1b"><apply id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2">𝑥</ci><apply id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3"><lt id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1"></lt><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2">absent</csymbol><ci id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.6.m6.1c">x_{&lt;i}</annotation></semantics></math> represents all tokens before the <math id="S2.SS2.SSS0.Px3.p3.7.m7.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.7.m7.1a"><mi id="S2.SS2.SSS0.Px3.p3.7.m7.1.1" xref="S2.SS2.SSS0.Px3.p3.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.7.m7.1b"><ci id="S2.SS2.SSS0.Px3.p3.7.m7.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.7.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.7.m7.1c">i</annotation></semantics></math>-th token. In contrast, Selective Language Modeling (SLM) trains the language model with a focus on tokens that exhibit a high excess loss when compared to the reference model. The excess loss (<math id="S2.SS2.SSS0.Px3.p3.8.m8.1" class="ltx_Math" alttext="\mathcal{L}_{\Delta}" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.8.m8.1a"><msub id="S2.SS2.SSS0.Px3.p3.8.m8.1.1" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2.cmml">ℒ</mi><mi mathvariant="normal" id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3.cmml">Δ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.8.m8.1b"><apply id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3">Δ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.8.m8.1c">\mathcal{L}_{\Delta}</annotation></semantics></math>) for a token <math id="S2.SS2.SSS0.Px3.p3.9.m9.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.9.m9.1a"><msub id="S2.SS2.SSS0.Px3.p3.9.m9.1.1" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.9.m9.1b"><apply id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.9.m9.1c">x_{i}</annotation></semantics></math> is defined as the difference between the current training model loss (<math id="S2.SS2.SSS0.Px3.p3.10.m10.1" class="ltx_Math" alttext="\mathcal{L}_{\theta}" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.10.m10.1a"><msub id="S2.SS2.SSS0.Px3.p3.10.m10.1.1" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2.cmml">ℒ</mi><mi id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.10.m10.1b"><apply id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.10.m10.1c">\mathcal{L}_{\theta}</annotation></semantics></math>) and the reference loss:</p>
</div>
<div id="S2.SS2.SSS0.Px3.p4" class="ltx_para">
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.3" class="ltx_Math" alttext="\mathcal{L}_{\Delta}(x_{i})=\mathcal{L}_{\theta}(x_{i})-\mathcal{L}_{\text{ref}}(x_{i})" display="block"><semantics id="S2.E3.m1.3a"><mrow id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.3" xref="S2.E3.m1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.3.2.cmml">ℒ</mi><mi mathvariant="normal" id="S2.E3.m1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.3.3.cmml">Δ</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.2.cmml">​</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.3.3.4" xref="S2.E3.m1.3.3.4.cmml">=</mo><mrow id="S2.E3.m1.3.3.3" xref="S2.E3.m1.3.3.3.cmml"><mrow id="S2.E3.m1.2.2.2.1" xref="S2.E3.m1.2.2.2.1.cmml"><msub id="S2.E3.m1.2.2.2.1.3" xref="S2.E3.m1.2.2.2.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.2.2.2.1.3.2" xref="S2.E3.m1.2.2.2.1.3.2.cmml">ℒ</mi><mi id="S2.E3.m1.2.2.2.1.3.3" xref="S2.E3.m1.2.2.2.1.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.2.1.2" xref="S2.E3.m1.2.2.2.1.2.cmml">​</mo><mrow id="S2.E3.m1.2.2.2.1.1.1" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.2.1.1.1.2" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml">(</mo><msub id="S2.E3.m1.2.2.2.1.1.1.1" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml"><mi id="S2.E3.m1.2.2.2.1.1.1.1.2" xref="S2.E3.m1.2.2.2.1.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.2.2.2.1.1.1.1.3" xref="S2.E3.m1.2.2.2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E3.m1.2.2.2.1.1.1.3" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.3.3.3.3" xref="S2.E3.m1.3.3.3.3.cmml">−</mo><mrow id="S2.E3.m1.3.3.3.2" xref="S2.E3.m1.3.3.3.2.cmml"><msub id="S2.E3.m1.3.3.3.2.3" xref="S2.E3.m1.3.3.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.3.3.3.2.3.2" xref="S2.E3.m1.3.3.3.2.3.2.cmml">ℒ</mi><mtext id="S2.E3.m1.3.3.3.2.3.3" xref="S2.E3.m1.3.3.3.2.3.3a.cmml">ref</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.2.2" xref="S2.E3.m1.3.3.3.2.2.cmml">​</mo><mrow id="S2.E3.m1.3.3.3.2.1.1" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.3.3.3.2.1.1.2" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml">(</mo><msub id="S2.E3.m1.3.3.3.2.1.1.1" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml"><mi id="S2.E3.m1.3.3.3.2.1.1.1.2" xref="S2.E3.m1.3.3.3.2.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.3.3.3.2.1.1.1.3" xref="S2.E3.m1.3.3.3.2.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E3.m1.3.3.3.2.1.1.3" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.3b"><apply id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3"><eq id="S2.E3.m1.3.3.4.cmml" xref="S2.E3.m1.3.3.4"></eq><apply id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><times id="S2.E3.m1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.3.2">ℒ</ci><ci id="S2.E3.m1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.3.3">Δ</ci></apply><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E3.m1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E3.m1.3.3.3.cmml" xref="S2.E3.m1.3.3.3"><minus id="S2.E3.m1.3.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3"></minus><apply id="S2.E3.m1.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.1"><times id="S2.E3.m1.2.2.2.1.2.cmml" xref="S2.E3.m1.2.2.2.1.2"></times><apply id="S2.E3.m1.2.2.2.1.3.cmml" xref="S2.E3.m1.2.2.2.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.1.3.1.cmml" xref="S2.E3.m1.2.2.2.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.2.1.3.2.cmml" xref="S2.E3.m1.2.2.2.1.3.2">ℒ</ci><ci id="S2.E3.m1.2.2.2.1.3.3.cmml" xref="S2.E3.m1.2.2.2.1.3.3">𝜃</ci></apply><apply id="S2.E3.m1.2.2.2.1.1.1.1.cmml" xref="S2.E3.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.2.1.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.2.1.1.1.1.2">𝑥</ci><ci id="S2.E3.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.2.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E3.m1.3.3.3.2.cmml" xref="S2.E3.m1.3.3.3.2"><times id="S2.E3.m1.3.3.3.2.2.cmml" xref="S2.E3.m1.3.3.3.2.2"></times><apply id="S2.E3.m1.3.3.3.2.3.cmml" xref="S2.E3.m1.3.3.3.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.2.3.1.cmml" xref="S2.E3.m1.3.3.3.2.3">subscript</csymbol><ci id="S2.E3.m1.3.3.3.2.3.2.cmml" xref="S2.E3.m1.3.3.3.2.3.2">ℒ</ci><ci id="S2.E3.m1.3.3.3.2.3.3a.cmml" xref="S2.E3.m1.3.3.3.2.3.3"><mtext mathsize="70%" id="S2.E3.m1.3.3.3.2.3.3.cmml" xref="S2.E3.m1.3.3.3.2.3.3">ref</mtext></ci></apply><apply id="S2.E3.m1.3.3.3.2.1.1.1.cmml" xref="S2.E3.m1.3.3.3.2.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.2.1.1.1.1.cmml" xref="S2.E3.m1.3.3.3.2.1.1">subscript</csymbol><ci id="S2.E3.m1.3.3.3.2.1.1.1.2.cmml" xref="S2.E3.m1.3.3.3.2.1.1.1.2">𝑥</ci><ci id="S2.E3.m1.3.3.3.2.1.1.1.3.cmml" xref="S2.E3.m1.3.3.3.2.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.3c">\mathcal{L}_{\Delta}(x_{i})=\mathcal{L}_{\theta}(x_{i})-\mathcal{L}_{\text{ref}}(x_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.SSS0.Px3.p5" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p5.1">우리는 토큰 선택 비율 <math alttext="k\%" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p5.1.m1.1"><semantics id="S2.SS2.SSS0.Px3.p5.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p5.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p5.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p5.1.m1.1c">k\%</annotation></semantics></math>를 도입하는데, 이것은 그들의 초과 손실에 기초하여 포함될 토큰의 비율을 결정한다. 선택된 토큰들에 대한 교차 엔트로피 손실은 다음과 같이 계산된다:</p>
</div>
<div id="S2.SS2.SSS0.Px3.p6" class="ltx_para">
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.4" class="ltx_Math" alttext="\mathcal{L}_{\text{SLM}}(\theta)=-\frac{1}{N*k\%}\sum_{i=1}^{N}I_{k\%}(x_{i})\cdot\log P(x_{i}|x_{<i};\theta)" display="block"><semantics id="S2.E4.m1.4a"><mrow id="S2.E4.m1.4.4" xref="S2.E4.m1.4.4.cmml"><mrow id="S2.E4.m1.4.4.4" xref="S2.E4.m1.4.4.4.cmml"><msub id="S2.E4.m1.4.4.4.2" xref="S2.E4.m1.4.4.4.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.4.4.4.2.2" xref="S2.E4.m1.4.4.4.2.2.cmml">ℒ</mi><mtext id="S2.E4.m1.4.4.4.2.3" xref="S2.E4.m1.4.4.4.2.3a.cmml">SLM</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.4.1" xref="S2.E4.m1.4.4.4.1.cmml">​</mo><mrow id="S2.E4.m1.4.4.4.3.2" xref="S2.E4.m1.4.4.4.cmml"><mo stretchy="false" id="S2.E4.m1.4.4.4.3.2.1" xref="S2.E4.m1.4.4.4.cmml">(</mo><mi id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S2.E4.m1.4.4.4.3.2.2" xref="S2.E4.m1.4.4.4.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.4.4.3" xref="S2.E4.m1.4.4.3.cmml">=</mo><mrow id="S2.E4.m1.4.4.2" xref="S2.E4.m1.4.4.2.cmml"><mo id="S2.E4.m1.4.4.2a" xref="S2.E4.m1.4.4.2.cmml">−</mo><mrow id="S2.E4.m1.4.4.2.2" xref="S2.E4.m1.4.4.2.2.cmml"><mfrac id="S2.E4.m1.4.4.2.2.4" xref="S2.E4.m1.4.4.2.2.4.cmml"><mn id="S2.E4.m1.4.4.2.2.4.2" xref="S2.E4.m1.4.4.2.2.4.2.cmml">1</mn><mrow id="S2.E4.m1.4.4.2.2.4.3" xref="S2.E4.m1.4.4.2.2.4.3.cmml"><mi id="S2.E4.m1.4.4.2.2.4.3.2" xref="S2.E4.m1.4.4.2.2.4.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E4.m1.4.4.2.2.4.3.1" xref="S2.E4.m1.4.4.2.2.4.3.1.cmml">∗</mo><mrow id="S2.E4.m1.4.4.2.2.4.3.3" xref="S2.E4.m1.4.4.2.2.4.3.3.cmml"><mi id="S2.E4.m1.4.4.2.2.4.3.3.2" xref="S2.E4.m1.4.4.2.2.4.3.3.2.cmml">k</mi><mo id="S2.E4.m1.4.4.2.2.4.3.3.1" xref="S2.E4.m1.4.4.2.2.4.3.3.1.cmml">%</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.2.2.3" xref="S2.E4.m1.4.4.2.2.3.cmml">​</mo><mrow id="S2.E4.m1.4.4.2.2.2" xref="S2.E4.m1.4.4.2.2.2.cmml"><munderover id="S2.E4.m1.4.4.2.2.2.3" xref="S2.E4.m1.4.4.2.2.2.3.cmml"><mo movablelimits="false" id="S2.E4.m1.4.4.2.2.2.3.2.2" xref="S2.E4.m1.4.4.2.2.2.3.2.2.cmml">∑</mo><mrow id="S2.E4.m1.4.4.2.2.2.3.2.3" xref="S2.E4.m1.4.4.2.2.2.3.2.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.3.2.3.2" xref="S2.E4.m1.4.4.2.2.2.3.2.3.2.cmml">i</mi><mo id="S2.E4.m1.4.4.2.2.2.3.2.3.1" xref="S2.E4.m1.4.4.2.2.2.3.2.3.1.cmml">=</mo><mn id="S2.E4.m1.4.4.2.2.2.3.2.3.3" xref="S2.E4.m1.4.4.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S2.E4.m1.4.4.2.2.2.3.3" xref="S2.E4.m1.4.4.2.2.2.3.3.cmml">N</mi></munderover><mrow id="S2.E4.m1.4.4.2.2.2.2" xref="S2.E4.m1.4.4.2.2.2.2.cmml"><mrow id="S2.E4.m1.3.3.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.cmml"><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S2.E4.m1.3.3.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml">I</mi><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2.cmml">k</mi><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1.cmml">%</mo></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.3.3.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo rspace="0.055em" stretchy="false" id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S2.E4.m1.3.3.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml">⋅</mo><mrow id="S2.E4.m1.3.3.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.3.1" xref="S2.E4.m1.3.3.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E4.m1.3.3.1.1.1.1.1.3a" xref="S2.E4.m1.3.3.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.E4.m1.3.3.1.1.1.1.1.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.3.2.cmml">P</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.2.2.2.2.3" xref="S2.E4.m1.4.4.2.2.2.2.3.cmml">​</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.4.4.2.2.2.2.2.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml">(</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml"><msub id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2.cmml">x</mi><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.2.cmml">|</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml"><msub id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml">;</mo><mi id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">θ</mi></mrow></mrow><mo stretchy="false" id="S2.E4.m1.4.4.2.2.2.2.2.1.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.4b"><apply id="S2.E4.m1.4.4.cmml" xref="S2.E4.m1.4.4"><eq id="S2.E4.m1.4.4.3.cmml" xref="S2.E4.m1.4.4.3"></eq><apply id="S2.E4.m1.4.4.4.cmml" xref="S2.E4.m1.4.4.4"><times id="S2.E4.m1.4.4.4.1.cmml" xref="S2.E4.m1.4.4.4.1"></times><apply id="S2.E4.m1.4.4.4.2.cmml" xref="S2.E4.m1.4.4.4.2"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.4.2.1.cmml" xref="S2.E4.m1.4.4.4.2">subscript</csymbol><ci id="S2.E4.m1.4.4.4.2.2.cmml" xref="S2.E4.m1.4.4.4.2.2">ℒ</ci><ci id="S2.E4.m1.4.4.4.2.3a.cmml" xref="S2.E4.m1.4.4.4.2.3"><mtext mathsize="70%" id="S2.E4.m1.4.4.4.2.3.cmml" xref="S2.E4.m1.4.4.4.2.3">SLM</mtext></ci></apply><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">𝜃</ci></apply><apply id="S2.E4.m1.4.4.2.cmml" xref="S2.E4.m1.4.4.2"><minus id="S2.E4.m1.4.4.2.3.cmml" xref="S2.E4.m1.4.4.2"></minus><apply id="S2.E4.m1.4.4.2.2.cmml" xref="S2.E4.m1.4.4.2.2"><times id="S2.E4.m1.4.4.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.3"></times><apply id="S2.E4.m1.4.4.2.2.4.cmml" xref="S2.E4.m1.4.4.2.2.4"><divide id="S2.E4.m1.4.4.2.2.4.1.cmml" xref="S2.E4.m1.4.4.2.2.4"></divide><cn type="integer" id="S2.E4.m1.4.4.2.2.4.2.cmml" xref="S2.E4.m1.4.4.2.2.4.2">1</cn><apply id="S2.E4.m1.4.4.2.2.4.3.cmml" xref="S2.E4.m1.4.4.2.2.4.3"><times id="S2.E4.m1.4.4.2.2.4.3.1.cmml" xref="S2.E4.m1.4.4.2.2.4.3.1"></times><ci id="S2.E4.m1.4.4.2.2.4.3.2.cmml" xref="S2.E4.m1.4.4.2.2.4.3.2">𝑁</ci><apply id="S2.E4.m1.4.4.2.2.4.3.3.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3"><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.4.3.3.1.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3.1">percent</csymbol><ci id="S2.E4.m1.4.4.2.2.4.3.3.2.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3.2">𝑘</ci></apply></apply></apply><apply id="S2.E4.m1.4.4.2.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2"><apply id="S2.E4.m1.4.4.2.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3">superscript</csymbol><apply id="S2.E4.m1.4.4.2.2.2.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.3.2.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3">subscript</csymbol><sum id="S2.E4.m1.4.4.2.2.2.3.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.2"></sum><apply id="S2.E4.m1.4.4.2.2.2.3.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3"><eq id="S2.E4.m1.4.4.2.2.2.3.2.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3.1"></eq><ci id="S2.E4.m1.4.4.2.2.2.3.2.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3.2">𝑖</ci><cn type="integer" id="S2.E4.m1.4.4.2.2.2.3.2.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.4.4.2.2.2.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3.3">𝑁</ci></apply><apply id="S2.E4.m1.4.4.2.2.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2"><times id="S2.E4.m1.4.4.2.2.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.3"></times><apply id="S2.E4.m1.3.3.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1"><ci id="S2.E4.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.2">⋅</ci><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1"><times id="S2.E4.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.2"></times><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.2">𝐼</ci><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3"><csymbol cd="latexml" id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1">percent</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2">𝑘</ci></apply></apply><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E4.m1.3.3.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3"><log id="S2.E4.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3.1"></log><ci id="S2.E4.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3.2">𝑃</ci></apply></apply><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1"><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.2">conditional</csymbol><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3">subscript</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2">𝑥</ci><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3">𝑖</ci></apply><list id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1"><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2">𝑥</ci><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3"><lt id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">𝜃</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.4c">\mathcal{L}_{\text{SLM}}(\theta)=-\frac{1}{N*k\%}\sum_{i=1}^{N}I_{k\%}(x_{i})\cdot\log P(x_{i}|x_{&lt;i};\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.SSS0.Px3.p7" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p7.3" class="ltx_p">Here, <math id="S2.SS2.SSS0.Px3.p7.1.m1.1" class="ltx_Math" alttext="N*k\%" display="inline"><semantics id="S2.SS2.SSS0.Px3.p7.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p7.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1.cmml">∗</mo><mrow id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1"><times id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1"></times><ci id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2">𝑁</ci><apply id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.1.m1.1c">N*k\%</annotation></semantics></math> defines the number of tokens that fall within the top <math id="S2.SS2.SSS0.Px3.p7.2.m2.1" class="ltx_Math" alttext="k\%" display="inline"><semantics id="S2.SS2.SSS0.Px3.p7.2.m2.1a"><mrow id="S2.SS2.SSS0.Px3.p7.2.m2.1.1" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.2.m2.1b"><apply id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.2.m2.1c">k\%</annotation></semantics></math> of excess loss. The indicator function <math id="S2.SS2.SSS0.Px3.p7.3.m3.1" class="ltx_Math" alttext="I_{k\%}(x_{i})" display="inline"><semantics id="S2.SS2.SSS0.Px3.p7.3.m3.1a"><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.cmml"><msub id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2.cmml">I</mi><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1.cmml">%</mo></mrow></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2.cmml">​</mo><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.3.m3.1b"><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1"><times id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2"></times><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2">𝐼</ci><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2">𝑘</ci></apply></apply><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.3.m3.1c">I_{k\%}(x_{i})</annotation></semantics></math> is defined as:</p>
</div>
<div id="S2.SS2.SSS0.Px3.p8" class="ltx_para">
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1.5" class="ltx_Math" alttext="I_{k\%}(x_{i})=\begin{cases}1&amp;\text{if }x_{i}\text{ is in the top }k\%\text{ of }\mathcal{L}_{\Delta}\\
0&amp;\text{otherwise}\end{cases}" display="block"><semantics id="S2.E5.m1.5a"><mrow id="S2.E5.m1.5.5" xref="S2.E5.m1.5.5.cmml"><mrow id="S2.E5.m1.5.5.1" xref="S2.E5.m1.5.5.1.cmml"><msub id="S2.E5.m1.5.5.1.3" xref="S2.E5.m1.5.5.1.3.cmml"><mi id="S2.E5.m1.5.5.1.3.2" xref="S2.E5.m1.5.5.1.3.2.cmml">I</mi><mrow id="S2.E5.m1.5.5.1.3.3" xref="S2.E5.m1.5.5.1.3.3.cmml"><mi id="S2.E5.m1.5.5.1.3.3.2" xref="S2.E5.m1.5.5.1.3.3.2.cmml">k</mi><mo id="S2.E5.m1.5.5.1.3.3.1" xref="S2.E5.m1.5.5.1.3.3.1.cmml">%</mo></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.5.5.1.2" xref="S2.E5.m1.5.5.1.2.cmml">​</mo><mrow id="S2.E5.m1.5.5.1.1.1" xref="S2.E5.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.5.5.1.1.1.2" xref="S2.E5.m1.5.5.1.1.1.1.cmml">(</mo><msub id="S2.E5.m1.5.5.1.1.1.1" xref="S2.E5.m1.5.5.1.1.1.1.cmml"><mi id="S2.E5.m1.5.5.1.1.1.1.2" xref="S2.E5.m1.5.5.1.1.1.1.2.cmml">x</mi><mi id="S2.E5.m1.5.5.1.1.1.1.3" xref="S2.E5.m1.5.5.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E5.m1.5.5.1.1.1.3" xref="S2.E5.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.5.5.2" xref="S2.E5.m1.5.5.2.cmml">=</mo><mrow id="S2.E5.m1.4.4" xref="S2.E5.m1.5.5.3.1.cmml"><mo id="S2.E5.m1.4.4.5" xref="S2.E5.m1.5.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S2.E5.m1.4.4.4" xref="S2.E5.m1.5.5.3.1.cmml"><mtr id="S2.E5.m1.4.4.4a" xref="S2.E5.m1.5.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4b" xref="S2.E5.m1.5.5.3.1.cmml"><mn id="S2.E5.m1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4c" xref="S2.E5.m1.5.5.3.1.cmml"><mrow id="S2.E5.m1.2.2.2.2.2.1" xref="S2.E5.m1.2.2.2.2.2.1.cmml"><mtext id="S2.E5.m1.2.2.2.2.2.1.2" xref="S2.E5.m1.2.2.2.2.2.1.2a.cmml">if&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.2.2.1.1" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">​</mo><msub id="S2.E5.m1.2.2.2.2.2.1.3" xref="S2.E5.m1.2.2.2.2.2.1.3.cmml"><mi id="S2.E5.m1.2.2.2.2.2.1.3.2" xref="S2.E5.m1.2.2.2.2.2.1.3.2.cmml">x</mi><mi id="S2.E5.m1.2.2.2.2.2.1.3.3" xref="S2.E5.m1.2.2.2.2.2.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.2.2.1.1a" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">​</mo><mtext id="S2.E5.m1.2.2.2.2.2.1.4" xref="S2.E5.m1.2.2.2.2.2.1.4a.cmml">&nbsp;is in the top&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.2.2.1.1b" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">​</mo><mrow id="S2.E5.m1.2.2.2.2.2.1.5" xref="S2.E5.m1.2.2.2.2.2.1.5.cmml"><mi id="S2.E5.m1.2.2.2.2.2.1.5.2" xref="S2.E5.m1.2.2.2.2.2.1.5.2.cmml">k</mi><mo id="S2.E5.m1.2.2.2.2.2.1.5.1" xref="S2.E5.m1.2.2.2.2.2.1.5.1.cmml">%</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.2.2.1.1c" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">​</mo><mtext id="S2.E5.m1.2.2.2.2.2.1.6" xref="S2.E5.m1.2.2.2.2.2.1.6a.cmml">&nbsp;of&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.2.2.1.1d" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">​</mo><msub id="S2.E5.m1.2.2.2.2.2.1.7" xref="S2.E5.m1.2.2.2.2.2.1.7.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.2.2.2.2.2.1.7.2" xref="S2.E5.m1.2.2.2.2.2.1.7.2.cmml">ℒ</mi><mi mathvariant="normal" id="S2.E5.m1.2.2.2.2.2.1.7.3" xref="S2.E5.m1.2.2.2.2.2.1.7.3.cmml">Δ</mi></msub></mrow></mtd></mtr><mtr id="S2.E5.m1.4.4.4d" xref="S2.E5.m1.5.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4e" xref="S2.E5.m1.5.5.3.1.cmml"><mn id="S2.E5.m1.3.3.3.3.1.1" xref="S2.E5.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4f" xref="S2.E5.m1.5.5.3.1.cmml"><mtext id="S2.E5.m1.4.4.4.4.2.1" xref="S2.E5.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.5b"><apply id="S2.E5.m1.5.5.cmml" xref="S2.E5.m1.5.5"><eq id="S2.E5.m1.5.5.2.cmml" xref="S2.E5.m1.5.5.2"></eq><apply id="S2.E5.m1.5.5.1.cmml" xref="S2.E5.m1.5.5.1"><times id="S2.E5.m1.5.5.1.2.cmml" xref="S2.E5.m1.5.5.1.2"></times><apply id="S2.E5.m1.5.5.1.3.cmml" xref="S2.E5.m1.5.5.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.1.3.1.cmml" xref="S2.E5.m1.5.5.1.3">subscript</csymbol><ci id="S2.E5.m1.5.5.1.3.2.cmml" xref="S2.E5.m1.5.5.1.3.2">𝐼</ci><apply id="S2.E5.m1.5.5.1.3.3.cmml" xref="S2.E5.m1.5.5.1.3.3"><csymbol cd="latexml" id="S2.E5.m1.5.5.1.3.3.1.cmml" xref="S2.E5.m1.5.5.1.3.3.1">percent</csymbol><ci id="S2.E5.m1.5.5.1.3.3.2.cmml" xref="S2.E5.m1.5.5.1.3.3.2">𝑘</ci></apply></apply><apply id="S2.E5.m1.5.5.1.1.1.1.cmml" xref="S2.E5.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.1.1.1.1.1.cmml" xref="S2.E5.m1.5.5.1.1.1">subscript</csymbol><ci id="S2.E5.m1.5.5.1.1.1.1.2.cmml" xref="S2.E5.m1.5.5.1.1.1.1.2">𝑥</ci><ci id="S2.E5.m1.5.5.1.1.1.1.3.cmml" xref="S2.E5.m1.5.5.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E5.m1.5.5.3.1.cmml" xref="S2.E5.m1.4.4"><csymbol cd="latexml" id="S2.E5.m1.5.5.3.1.1.cmml" xref="S2.E5.m1.4.4.5">cases</csymbol><cn type="integer" id="S2.E5.m1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1">1</cn><apply id="S2.E5.m1.2.2.2.2.2.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1"><times id="S2.E5.m1.2.2.2.2.2.1.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.1"></times><ci id="S2.E5.m1.2.2.2.2.2.1.2a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.2"><mtext id="S2.E5.m1.2.2.2.2.2.1.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.2">if&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.2.2.2.1.3.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3">subscript</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.3.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3.2">𝑥</ci><ci id="S2.E5.m1.2.2.2.2.2.1.3.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3.3">𝑖</ci></apply><ci id="S2.E5.m1.2.2.2.2.2.1.4a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.4"><mtext id="S2.E5.m1.2.2.2.2.2.1.4.cmml" xref="S2.E5.m1.2.2.2.2.2.1.4">&nbsp;is in the top&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.5.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5"><csymbol cd="latexml" id="S2.E5.m1.2.2.2.2.2.1.5.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5.1">percent</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.5.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5.2">𝑘</ci></apply><ci id="S2.E5.m1.2.2.2.2.2.1.6a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.6"><mtext id="S2.E5.m1.2.2.2.2.2.1.6.cmml" xref="S2.E5.m1.2.2.2.2.2.1.6">&nbsp;of&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.7.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.2.2.2.1.7.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7">subscript</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.7.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7.2">ℒ</ci><ci id="S2.E5.m1.2.2.2.2.2.1.7.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7.3">Δ</ci></apply></apply><cn type="integer" id="S2.E5.m1.3.3.3.3.1.1.cmml" xref="S2.E5.m1.3.3.3.3.1.1">0</cn><ci id="S2.E5.m1.4.4.4.4.2.1a.cmml" xref="S2.E5.m1.4.4.4.4.2.1"><mtext id="S2.E5.m1.4.4.4.4.2.1.cmml" xref="S2.E5.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.5c">I_{k\%}(x_{i})=\begin{cases}1&amp;\text{if }x_{i}\text{ is in the top }k\%\text{ of }\mathcal{L}_{\Delta}\\
0&amp;\text{otherwise}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.SSS0.Px3.p9" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p9.1">이는 언어 모델이 학습하는데 가장 유익하다고 여겨지는 토큰에만 손실이 적용되도록 보장한다. 실제로, 토큰 선택은 그들의 초과 손실에 따라 일괄적으로 토큰들을 랭킹하고 트레이닝을 위해 토큰들의 상위 <math alttext="k\%" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p9.1.m1.1"><semantics id="S2.SS2.SSS0.Px3.p9.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p9.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p9.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p9.1.m1.1c">k\%</annotation></semantics></math>만을 사용함으로써 구현될 수 있다. 이 프로세스는 사전 훈련 동안 추가 비용을 발생시키지 않고 원하지 않는 토큰에 대한 손실을 제거하여 우리의 접근법을 효율적이고 쉽게 통합한다.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">우리는 수학적 영역과 일반 영역 모두에서 모델을 지속적으로 사전 훈련하고 SLM의 효과를 이해하기 위해 절제 및 분석 실험을 설계했다.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Setup</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reference Model Training</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">수학적 참조 모델을 훈련하기 위해 0.5B 고품질 수학 관련 토큰 데이터 세트를 수집했다. 이 데이터 세트는 GPT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu et al.</span>, <a class="ltx_ref" href="#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Huang et al.</span>, <a class="ltx_ref" href="#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>의 합성 데이터와 수동으로 선별된 데이터 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yue et al.</span>, <a class="ltx_ref" href="#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Ni et al.</span>, <a class="ltx_ref" href="#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>의 혼합이다. 일반 참조 모델의 경우 Tulu-v2 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ivison et al.</span>, <a class="ltx_ref" href="#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> 및 OpenHermes-2.5 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Teknium</span>, <a class="ltx_ref" href="#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>와 같은 오픈 소스 데이터 세트에서 1.9B 토큰의 코퍼스를 컴파일했다. 우리는 3개의 에포크에 대한 참조 모델을 훈련시켰다. 최대 학습률은 코사인 감쇠 스케줄을 적용하여 1B 모델의 경우 5e-5, 7B 모델의 경우 1e-5로 설정하였다. 최대 시퀀스 길이를 1B 모델의 경우 2048, 7B 모델의 경우 4096으로 설정하여 모델 입력을 위해 여러 샘플을 이러한 길이로 패킹했다. 모든 주요 실험에서 우리는 <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.1">same</em> 기본 모델로 연속 사전 훈련 모델과 참조 모델을 초기화했다.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.22.4.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.6.3" class="ltx_text" style="font-size:90%;">
<span id="S3.T1.6.3.1" class="ltx_text ltx_font_bold">Few-shot CoT reasoning results of math pretraining.</span> All models are tested with few-shot prompting. Previous best results are highlighted in <span id="S3.T1.6.3.2" class="ltx_text" style="color:#1E90FF;">blue</span>, while our best results are in <span id="S3.T1.6.3.3" class="ltx_text" style="color:#BF0040;">purple</span>. <sup id="S3.T1.6.3.4" class="ltx_sup">∗</sup>Only unique math-related tokens are calculated. For <span id="S3.T1.6.3.5" class="ltx_text ltx_font_smallcaps">Rho-1</span>, we calculate only the selected tokens that are used for training. <sup id="S3.T1.6.3.6" class="ltx_sup">†</sup>We use OpenAI’s MATH subset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lightman et&nbsp;al.</span>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> for evaluation, since some original test samples have been used in public training sets such as PRM800k. <sup id="S3.T1.6.3.7" class="ltx_sup">‡</sup>The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.
</span></figcaption>
<div id="S3.T1.13" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:427.9pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-50.9pt,50.2pt) scale(0.809792493259618,0.809792493259618) ;">
<table id="S3.T1.13.7" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.10.4.4" class="ltx_tr">
<td id="S3.T1.10.4.4.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.5.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T1.7.1.1.1" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><math id="S3.T1.7.1.1.1.m1.1" class="ltx_Math" alttext="|\bm{\theta}|" display="inline"><semantics id="S3.T1.7.1.1.1.m1.1a"><mrow id="S3.T1.7.1.1.1.m1.1.2.2" xref="S3.T1.7.1.1.1.m1.1.2.1.cmml"><mo stretchy="false" id="S3.T1.7.1.1.1.m1.1.2.2.1" xref="S3.T1.7.1.1.1.m1.1.2.1.1.cmml">|</mo><mi id="S3.T1.7.1.1.1.m1.1.1" xref="S3.T1.7.1.1.1.m1.1.1.cmml">𝜽</mi><mo stretchy="false" id="S3.T1.7.1.1.1.m1.1.2.2.2" xref="S3.T1.7.1.1.1.m1.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.1.1.1.m1.1b"><apply id="S3.T1.7.1.1.1.m1.1.2.1.cmml" xref="S3.T1.7.1.1.1.m1.1.2.2"><abs id="S3.T1.7.1.1.1.m1.1.2.1.1.cmml" xref="S3.T1.7.1.1.1.m1.1.2.2.1"></abs><ci id="S3.T1.7.1.1.1.m1.1.1.cmml" xref="S3.T1.7.1.1.1.m1.1.1">𝜽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.1.1.1.m1.1c">|\bm{\theta}|</annotation></semantics></math></td>
<td id="S3.T1.10.4.4.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.6.1" class="ltx_text ltx_font_bold">Data</span></td>
<td id="S3.T1.8.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T1.8.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T1.8.2.2.2.1.2" class="ltx_tr">
<td id="S3.T1.8.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.8.2.2.2.1.2.1.1" class="ltx_text ltx_font_bold">Uniq.</span></td>
</tr>
<tr id="S3.T1.8.2.2.2.1.1" class="ltx_tr">
<td id="S3.T1.8.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S3.T1.8.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Toks</span><sup id="S3.T1.8.2.2.2.1.1.1.2" class="ltx_sup">∗</sup>
</td>
</tr>
</tbody></table>
</td>
<td id="S3.T1.10.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T1.10.4.4.7.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T1.10.4.4.7.1.1" class="ltx_tr">
<td id="S3.T1.10.4.4.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.7.1.1.1.1" class="ltx_text ltx_font_bold">Train</span></td>
</tr>
<tr id="S3.T1.10.4.4.7.1.2" class="ltx_tr">
<td id="S3.T1.10.4.4.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.7.1.2.1.1" class="ltx_text ltx_font_bold">Toks</span></td>
</tr>
</tbody></table>
</td>
<td id="S3.T1.10.4.4.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.8.1" class="ltx_text ltx_font_bold">GSM8K</span></td>
<td id="S3.T1.9.3.3.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.9.3.3.3.1" class="ltx_text ltx_font_bold">MATH<sup id="S3.T1.9.3.3.3.1.1" class="ltx_sup"><span id="S3.T1.9.3.3.3.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span></td>
<td id="S3.T1.10.4.4.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.9.1" class="ltx_text ltx_font_bold">SVAMP</span></td>
<td id="S3.T1.10.4.4.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.10.1" class="ltx_text ltx_font_bold">ASDiv</span></td>
<td id="S3.T1.10.4.4.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.11.1" class="ltx_text ltx_font_bold">MAWPS</span></td>
<td id="S3.T1.10.4.4.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.12.1" class="ltx_text ltx_font_bold">TAB</span></td>
<td id="S3.T1.10.4.4.13" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.13.1" class="ltx_text ltx_font_bold">MQA</span></td>
<td id="S3.T1.10.4.4.14" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T1.10.4.4.14.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T1.10.4.4.14.1.1" class="ltx_tr">
<td id="S3.T1.10.4.4.14.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.14.1.1.1.1" class="ltx_text ltx_font_bold">MMLU</span></td>
</tr>
<tr id="S3.T1.10.4.4.14.1.2" class="ltx_tr">
<td id="S3.T1.10.4.4.14.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.14.1.2.1.1" class="ltx_text ltx_font_bold">STEM</span></td>
</tr>
</tbody></table>
</td>
<td id="S3.T1.10.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.4.1" class="ltx_text ltx_font_bold">SAT<sup id="S3.T1.10.4.4.4.1.1" class="ltx_sup"><span id="S3.T1.10.4.4.4.1.1.1" class="ltx_text ltx_font_medium">‡</span></sup></span></td>
<td id="S3.T1.10.4.4.15" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.15.1" class="ltx_text ltx_font_bold">AVG</span></td>
</tr>
<tr id="S3.T1.13.7.8.1" class="ltx_tr">
<td id="S3.T1.13.7.8.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="15"><span id="S3.T1.13.7.8.1.1.1" class="ltx_text ltx_font_typewriter">1-2B Base Models</span></td>
</tr>
<tr id="S3.T1.13.7.9.2" class="ltx_tr">
<td id="S3.T1.13.7.9.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/Tinyllama/Tinyllama-1.1B-intermediate-step-1431k-3T" title="" class="ltx_ref ltx_href">Tinyllama</a></td>
<td id="S3.T1.13.7.9.2.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td id="S3.T1.13.7.9.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.9.2.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.9.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.9.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2.9</td>
<td id="S3.T1.13.7.9.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">3.2</td>
<td id="S3.T1.13.7.9.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">11.0</td>
<td id="S3.T1.13.7.9.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">18.1</td>
<td id="S3.T1.13.7.9.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">20.4</td>
<td id="S3.T1.13.7.9.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">12.5</td>
<td id="S3.T1.13.7.9.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14.6</td>
<td id="S3.T1.13.7.9.2.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">16.1</td>
<td id="S3.T1.13.7.9.2.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">21.9</td>
<td id="S3.T1.13.7.9.2.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">13.4</td>
</tr>
<tr id="S3.T1.13.7.10.3" class="ltx_tr">
<td id="S3.T1.13.7.10.3.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/microsoft/phi-1_5" title="" class="ltx_ref ltx_href">Phi-1.5</a></td>
<td id="S3.T1.13.7.10.3.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td id="S3.T1.13.7.10.3.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.10.3.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.10.3.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.10.3.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">32.4</td>
<td id="S3.T1.13.7.10.3.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">4.2</td>
<td id="S3.T1.13.7.10.3.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">43.4</td>
<td id="S3.T1.13.7.10.3.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">53.1</td>
<td id="S3.T1.13.7.10.3.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">66.2</td>
<td id="S3.T1.13.7.10.3.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">24.4</td>
<td id="S3.T1.13.7.10.3.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">14.3</td>
<td id="S3.T1.13.7.10.3.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">21.8</td>
<td id="S3.T1.13.7.10.3.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">18.8</td>
<td id="S3.T1.13.7.10.3.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">31.0</td>
</tr>
<tr id="S3.T1.13.7.11.4" class="ltx_tr">
<td id="S3.T1.13.7.11.4.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/Qwen/Qwen1.5-1.8B" title="" class="ltx_ref ltx_href">Qwen1.5</a></td>
<td id="S3.T1.13.7.11.4.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">1.8B</td>
<td id="S3.T1.13.7.11.4.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.11.4.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.11.4.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.11.4.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.11.4.6.1" class="ltx_text" style="color:#1E90FF;">36.1</span></td>
<td id="S3.T1.13.7.11.4.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">6.8</td>
<td id="S3.T1.13.7.11.4.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.11.4.8.1" class="ltx_text" style="color:#1E90FF;">48.5</span></td>
<td id="S3.T1.13.7.11.4.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.11.4.9.1" class="ltx_text" style="color:#1E90FF;">63.6</span></td>
<td id="S3.T1.13.7.11.4.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.11.4.10.1" class="ltx_text" style="color:#1E90FF;">79.0</span></td>
<td id="S3.T1.13.7.11.4.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">29.2</td>
<td id="S3.T1.13.7.11.4.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">25.1</td>
<td id="S3.T1.13.7.11.4.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">31.3</td>
<td id="S3.T1.13.7.11.4.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">40.6</td>
<td id="S3.T1.13.7.11.4.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.11.4.15.1" class="ltx_text" style="color:#1E90FF;">40.0</span></td>
</tr>
<tr id="S3.T1.13.7.12.5" class="ltx_tr">
<td id="S3.T1.13.7.12.5.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/google/gemma-2b" title="" class="ltx_ref ltx_href">Gemma</a></td>
<td id="S3.T1.13.7.12.5.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">2.0B</td>
<td id="S3.T1.13.7.12.5.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.12.5.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.12.5.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.12.5.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">18.8</td>
<td id="S3.T1.13.7.12.5.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">11.4</td>
<td id="S3.T1.13.7.12.5.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">38.0</td>
<td id="S3.T1.13.7.12.5.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">56.6</td>
<td id="S3.T1.13.7.12.5.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">72.5</td>
<td id="S3.T1.13.7.12.5.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.12.5.11.1" class="ltx_text" style="color:#1E90FF;">36.9</span></td>
<td id="S3.T1.13.7.12.5.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.12.5.12.1" class="ltx_text" style="color:#1E90FF;">26.8</span></td>
<td id="S3.T1.13.7.12.5.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.12.5.13.1" class="ltx_text" style="color:#1E90FF;">34.4</span></td>
<td id="S3.T1.13.7.12.5.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">50.0</td>
<td id="S3.T1.13.7.12.5.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">38.4</td>
</tr>
<tr id="S3.T1.13.7.13.6" class="ltx_tr">
<td id="S3.T1.13.7.13.6.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">DeepSeekLLM</td>
<td id="S3.T1.13.7.13.6.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td id="S3.T1.13.7.13.6.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.13.6.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.13.6.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">150B</td>
<td id="S3.T1.13.7.13.6.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">11.5</td>
<td id="S3.T1.13.7.13.6.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">8.9</td>
<td id="S3.T1.13.7.13.6.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.13.6.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.13.6.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.13.6.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.13.6.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.13.6.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">29.6</td>
<td id="S3.T1.13.7.13.6.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">31.3</td>
<td id="S3.T1.13.7.13.6.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S3.T1.13.7.14.7" class="ltx_tr">
<td id="S3.T1.13.7.14.7.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">DeepSeekMath</td>
<td id="S3.T1.13.7.14.7.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td id="S3.T1.13.7.14.7.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">120B</td>
<td id="S3.T1.13.7.14.7.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">150B</td>
<td id="S3.T1.13.7.14.7.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">23.8</td>
<td id="S3.T1.13.7.14.7.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.14.7.7.1" class="ltx_text" style="color:#1E90FF;">13.6</span></td>
<td id="S3.T1.13.7.14.7.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">33.1</td>
<td id="S3.T1.13.7.14.7.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.14.7.14.1" class="ltx_text" style="color:#1E90FF;">56.3</span></td>
<td id="S3.T1.13.7.14.7.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S3.T1.13.7.15.8" class="ltx_tr">
<td id="S3.T1.13.7.15.8.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="15"><span id="S3.T1.13.7.15.8.1.1" class="ltx_text ltx_font_typewriter">Continual Pretraining on Tinyllama-1B</span></td>
</tr>
<tr id="S3.T1.13.7.16.9" class="ltx_tr">
<td id="S3.T1.13.7.16.9.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Tinyllama-CT</td>
<td id="S3.T1.13.7.16.9.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td id="S3.T1.13.7.16.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.16.9.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.16.9.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">15B</td>
<td id="S3.T1.13.7.16.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">6.4</td>
<td id="S3.T1.13.7.16.9.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2.4</td>
<td id="S3.T1.13.7.16.9.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">21.7</td>
<td id="S3.T1.13.7.16.9.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">36.7</td>
<td id="S3.T1.13.7.16.9.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">47.7</td>
<td id="S3.T1.13.7.16.9.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">17.9</td>
<td id="S3.T1.13.7.16.9.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">13.9</td>
<td id="S3.T1.13.7.16.9.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">23.0</td>
<td id="S3.T1.13.7.16.9.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">25.0</td>
<td id="S3.T1.13.7.16.9.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">21.6</td>
</tr>
<tr id="S3.T1.13.7.17.10" class="ltx_tr">
<td id="S3.T1.13.7.17.10.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S3.T1.13.7.17.10.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math</td>
<td id="S3.T1.13.7.17.10.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td id="S3.T1.13.7.17.10.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.17.10.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.17.10.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">9B</td>
<td id="S3.T1.13.7.17.10.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">29.8</td>
<td id="S3.T1.13.7.17.10.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">14.0</td>
<td id="S3.T1.13.7.17.10.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">49.2</td>
<td id="S3.T1.13.7.17.10.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">61.4</td>
<td id="S3.T1.13.7.17.10.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">79.8</td>
<td id="S3.T1.13.7.17.10.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">25.8</td>
<td id="S3.T1.13.7.17.10.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">30.4</td>
<td id="S3.T1.13.7.17.10.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.17.10.13.1" class="ltx_text" style="color:#BF0040;">24.7</span></td>
<td id="S3.T1.13.7.17.10.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">28.1</td>
<td id="S3.T1.13.7.17.10.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">38.1</td>
</tr>
<tr id="S3.T1.11.5.5" class="ltx_tr">
<td id="S3.T1.11.5.5.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><math id="S3.T1.11.5.5.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T1.11.5.5.1.m1.1a"><mi mathvariant="normal" id="S3.T1.11.5.5.1.m1.1.1" xref="S3.T1.11.5.5.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T1.11.5.5.1.m1.1b"><ci id="S3.T1.11.5.5.1.m1.1.1.cmml" xref="S3.T1.11.5.5.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.5.5.1.m1.1c">\Delta</annotation></semantics></math></td>
<td id="S3.T1.11.5.5.2" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.11.5.5.3" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.11.5.5.4" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.11.5.5.5" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#E6E6E6;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.5.1" class="ltx_text" style="background-color:#E6E6E6;">-40%</span></td>
<td id="S3.T1.11.5.5.6" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.6.1" class="ltx_text" style="background-color:#D2DCFA;">+23.4</span></td>
<td id="S3.T1.11.5.5.7" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.7.1" class="ltx_text" style="background-color:#D2DCFA;">+11.6</span></td>
<td id="S3.T1.11.5.5.8" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.8.1" class="ltx_text" style="background-color:#D2DCFA;">+27.5</span></td>
<td id="S3.T1.11.5.5.9" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.9.1" class="ltx_text" style="background-color:#D2DCFA;">+24.7</span></td>
<td id="S3.T1.11.5.5.10" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.10.1" class="ltx_text" style="background-color:#D2DCFA;">+32.1</span></td>
<td id="S3.T1.11.5.5.11" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.11.1" class="ltx_text" style="background-color:#D2DCFA;">+7.9</span></td>
<td id="S3.T1.11.5.5.12" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.12.1" class="ltx_text" style="background-color:#D2DCFA;">+16.5</span></td>
<td id="S3.T1.11.5.5.13" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.13.1" class="ltx_text" style="background-color:#D2DCFA;">+1.7</span></td>
<td id="S3.T1.11.5.5.14" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.14.1" class="ltx_text" style="background-color:#D2DCFA;">+3.1</span></td>
<td id="S3.T1.11.5.5.15" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.15.1" class="ltx_text ltx_font_bold" style="background-color:#D2DCFA;">+16.5</span></td>
</tr>
<tr id="S3.T1.13.7.18.11" class="ltx_tr">
<td id="S3.T1.13.7.18.11.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S3.T1.13.7.18.11.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math</td>
<td id="S3.T1.13.7.18.11.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td id="S3.T1.13.7.18.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.18.11.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.18.11.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">30B</td>
<td id="S3.T1.13.7.18.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.6.1" class="ltx_text" style="color:#BF0040;">36.2</span></td>
<td id="S3.T1.13.7.18.11.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.7.1" class="ltx_text" style="color:#BF0040;">15.6</span></td>
<td id="S3.T1.13.7.18.11.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.8.1" class="ltx_text" style="color:#BF0040;">52.1</span></td>
<td id="S3.T1.13.7.18.11.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.9.1" class="ltx_text" style="color:#BF0040;">67.0</span></td>
<td id="S3.T1.13.7.18.11.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.10.1" class="ltx_text" style="color:#BF0040;">83.9</span></td>
<td id="S3.T1.13.7.18.11.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.11.1" class="ltx_text" style="color:#BF0040;">29.0</span></td>
<td id="S3.T1.13.7.18.11.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.12.1" class="ltx_text" style="color:#BF0040;">32.5</span></td>
<td id="S3.T1.13.7.18.11.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">23.3</td>
<td id="S3.T1.13.7.18.11.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.14.1" class="ltx_text" style="color:#BF0040;">28.1</span></td>
<td id="S3.T1.13.7.18.11.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.15.1" class="ltx_text" style="color:#BF0040;">40.9</span></td>
</tr>
<tr id="S3.T1.12.6.6" class="ltx_tr">
<td id="S3.T1.12.6.6.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="15">
<math id="S3.T1.12.6.6.1.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S3.T1.12.6.6.1.m1.1a"><mo id="S3.T1.12.6.6.1.m1.1.1" xref="S3.T1.12.6.6.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S3.T1.12.6.6.1.m1.1b"><geq id="S3.T1.12.6.6.1.m1.1.1.cmml" xref="S3.T1.12.6.6.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.6.6.1.m1.1c">\geq</annotation></semantics></math><span id="S3.T1.12.6.6.1.1" class="ltx_text ltx_font_typewriter"> 7B Base Models</span>
</td>
</tr>
<tr id="S3.T1.13.7.19.12" class="ltx_tr">
<td id="S3.T1.13.7.19.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/meta-llama/Llama-2-7b-hf" title="" class="ltx_ref ltx_href">LLaMA-2</a></td>
<td id="S3.T1.13.7.19.12.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.19.12.3" class="ltx_td ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.13.7.19.12.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.19.12.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.19.12.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14.0</td>
<td id="S3.T1.13.7.19.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">3.6</td>
<td id="S3.T1.13.7.19.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">39.5</td>
<td id="S3.T1.13.7.19.12.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">51.7</td>
<td id="S3.T1.13.7.19.12.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">63.5</td>
<td id="S3.T1.13.7.19.12.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">30.9</td>
<td id="S3.T1.13.7.19.12.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">12.4</td>
<td id="S3.T1.13.7.19.12.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">32.7</td>
<td id="S3.T1.13.7.19.12.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">34.4</td>
<td id="S3.T1.13.7.19.12.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">31.4</td>
</tr>
<tr id="S3.T1.13.7.20.13" class="ltx_tr">
<td id="S3.T1.13.7.20.13.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/mistralai/Mistral-7B-v0.1" title="" class="ltx_ref ltx_href">Mistral</a></td>
<td id="S3.T1.13.7.20.13.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.20.13.3" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.13.7.20.13.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.20.13.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.20.13.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">41.2</td>
<td id="S3.T1.13.7.20.13.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">11.6</td>
<td id="S3.T1.13.7.20.13.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">64.7</td>
<td id="S3.T1.13.7.20.13.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">68.5</td>
<td id="S3.T1.13.7.20.13.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">87.5</td>
<td id="S3.T1.13.7.20.13.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">52.9</td>
<td id="S3.T1.13.7.20.13.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">33.0</td>
<td id="S3.T1.13.7.20.13.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">49.5</td>
<td id="S3.T1.13.7.20.13.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">59.4</td>
<td id="S3.T1.13.7.20.13.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">52.0</td>
</tr>
<tr id="S3.T1.13.7.21.14" class="ltx_tr">
<td id="S3.T1.13.7.21.14.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td id="S3.T1.13.7.21.14.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">8B</td>
<td id="S3.T1.13.7.21.14.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td id="S3.T1.13.7.21.14.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">164B</td>
<td id="S3.T1.13.7.21.14.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">16.2</td>
<td id="S3.T1.13.7.21.14.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">14.1</td>
<td id="S3.T1.13.7.21.14.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">35.6</td>
<td id="S3.T1.13.7.21.14.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S3.T1.13.7.22.15" class="ltx_tr">
<td id="S3.T1.13.7.22.15.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td id="S3.T1.13.7.22.15.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">62B</td>
<td id="S3.T1.13.7.22.15.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td id="S3.T1.13.7.22.15.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">109B</td>
<td id="S3.T1.13.7.22.15.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">52.4</td>
<td id="S3.T1.13.7.22.15.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">27.6</td>
<td id="S3.T1.13.7.22.15.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">53.9</td>
<td id="S3.T1.13.7.22.15.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S3.T1.13.7.23.16" class="ltx_tr">
<td id="S3.T1.13.7.23.16.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td id="S3.T1.13.7.23.16.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">540B</td>
<td id="S3.T1.13.7.23.16.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td id="S3.T1.13.7.23.16.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">26B</td>
<td id="S3.T1.13.7.23.16.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">58.8</td>
<td id="S3.T1.13.7.23.16.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">33.6</td>
<td id="S3.T1.13.7.23.16.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.23.16.13.1" class="ltx_text" style="color:#1E90FF;">63.9</span></td>
<td id="S3.T1.13.7.23.16.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S3.T1.13.7.24.17" class="ltx_tr">
<td id="S3.T1.13.7.24.17.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/EleutherAI/llemma_7b" title="" class="ltx_ref ltx_href">LLemma</a></td>
<td id="S3.T1.13.7.24.17.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.24.17.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PPile</td>
<td id="S3.T1.13.7.24.17.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">55B</td>
<td id="S3.T1.13.7.24.17.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">200B</td>
<td id="S3.T1.13.7.24.17.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">38.8</td>
<td id="S3.T1.13.7.24.17.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">17.2</td>
<td id="S3.T1.13.7.24.17.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">56.1</td>
<td id="S3.T1.13.7.24.17.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">69.1</td>
<td id="S3.T1.13.7.24.17.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">82.4</td>
<td id="S3.T1.13.7.24.17.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">48.7</td>
<td id="S3.T1.13.7.24.17.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">41.0</td>
<td id="S3.T1.13.7.24.17.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">45.4</td>
<td id="S3.T1.13.7.24.17.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">59.4</td>
<td id="S3.T1.13.7.24.17.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">50.9</td>
</tr>
<tr id="S3.T1.13.7.25.18" class="ltx_tr">
<td id="S3.T1.13.7.25.18.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/EleutherAI/llemma_34b" title="" class="ltx_ref ltx_href">LLemma</a></td>
<td id="S3.T1.13.7.25.18.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">34B</td>
<td id="S3.T1.13.7.25.18.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PPile</td>
<td id="S3.T1.13.7.25.18.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">55B</td>
<td id="S3.T1.13.7.25.18.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">50B</td>
<td id="S3.T1.13.7.25.18.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">54.2</td>
<td id="S3.T1.13.7.25.18.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">23.0</td>
<td id="S3.T1.13.7.25.18.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">67.9</td>
<td id="S3.T1.13.7.25.18.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">75.7</td>
<td id="S3.T1.13.7.25.18.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">90.1</td>
<td id="S3.T1.13.7.25.18.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">57.0</td>
<td id="S3.T1.13.7.25.18.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">49.8</td>
<td id="S3.T1.13.7.25.18.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">54.7</td>
<td id="S3.T1.13.7.25.18.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">68.8</td>
<td id="S3.T1.13.7.25.18.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">60.1</td>
</tr>
<tr id="S3.T1.13.7.26.19" class="ltx_tr">
<td id="S3.T1.13.7.26.19.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/internlm/internlm2-math-base-7b" title="" class="ltx_ref ltx_href">Intern-Math</a></td>
<td id="S3.T1.13.7.26.19.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.26.19.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.26.19.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">31B</td>
<td id="S3.T1.13.7.26.19.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">125B</td>
<td id="S3.T1.13.7.26.19.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">41.8</td>
<td id="S3.T1.13.7.26.19.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">14.4</td>
<td id="S3.T1.13.7.26.19.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">61.6</td>
<td id="S3.T1.13.7.26.19.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">66.8</td>
<td id="S3.T1.13.7.26.19.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">83.7</td>
<td id="S3.T1.13.7.26.19.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">50.0</td>
<td id="S3.T1.13.7.26.19.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">57.3</td>
<td id="S3.T1.13.7.26.19.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">24.8</td>
<td id="S3.T1.13.7.26.19.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">37.5</td>
<td id="S3.T1.13.7.26.19.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">48.7</td>
</tr>
<tr id="S3.T1.13.7.27.20" class="ltx_tr">
<td id="S3.T1.13.7.27.20.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/internlm/internlm2-math-base-20b" title="" class="ltx_ref ltx_href">Intern-Math</a></td>
<td id="S3.T1.13.7.27.20.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">20B</td>
<td id="S3.T1.13.7.27.20.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.27.20.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">31B</td>
<td id="S3.T1.13.7.27.20.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">125B</td>
<td id="S3.T1.13.7.27.20.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.27.20.6.1" class="ltx_text" style="color:#1E90FF;">65.4</span></td>
<td id="S3.T1.13.7.27.20.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">30.0</td>
<td id="S3.T1.13.7.27.20.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.27.20.8.1" class="ltx_text" style="color:#1E90FF;">75.7</span></td>
<td id="S3.T1.13.7.27.20.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">79.3</td>
<td id="S3.T1.13.7.27.20.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.27.20.10.1" class="ltx_text" style="color:#1E90FF;">94.0</span></td>
<td id="S3.T1.13.7.27.20.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">50.9</td>
<td id="S3.T1.13.7.27.20.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">38.5</td>
<td id="S3.T1.13.7.27.20.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">53.1</td>
<td id="S3.T1.13.7.27.20.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">71.9</td>
<td id="S3.T1.13.7.27.20.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">62.1</td>
</tr>
<tr id="S3.T1.13.7.28.21" class="ltx_tr">
<td id="S3.T1.13.7.28.21.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/deepseek-ai/deepseek-math-7b-base" title="" class="ltx_ref ltx_href">DeepSeekMath</a></td>
<td id="S3.T1.13.7.28.21.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.28.21.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.28.21.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">120B</td>
<td id="S3.T1.13.7.28.21.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">500B</td>
<td id="S3.T1.13.7.28.21.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">64.1</td>
<td id="S3.T1.13.7.28.21.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.7.1" class="ltx_text" style="color:#1E90FF;">34.2</span></td>
<td id="S3.T1.13.7.28.21.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">74.0</td>
<td id="S3.T1.13.7.28.21.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.9.1" class="ltx_text" style="color:#1E90FF;">83.9</span></td>
<td id="S3.T1.13.7.28.21.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">92.4</td>
<td id="S3.T1.13.7.28.21.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.11.1" class="ltx_text" style="color:#1E90FF;">63.4</span></td>
<td id="S3.T1.13.7.28.21.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.12.1" class="ltx_text" style="color:#1E90FF;">62.4</span></td>
<td id="S3.T1.13.7.28.21.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">56.4</td>
<td id="S3.T1.13.7.28.21.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.14.1" class="ltx_text" style="color:#1E90FF;">84.4</span></td>
<td id="S3.T1.13.7.28.21.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.15.1" class="ltx_text" style="color:#1E90FF;">68.4</span></td>
</tr>
<tr id="S3.T1.13.7.29.22" class="ltx_tr">
<td id="S3.T1.13.7.29.22.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="15"><span id="S3.T1.13.7.29.22.1.1" class="ltx_text ltx_font_typewriter">Continual Pretraining on Mistral-7B</span></td>
</tr>
<tr id="S3.T1.13.7.30.23" class="ltx_tr">
<td id="S3.T1.13.7.30.23.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Mistral-CT</td>
<td id="S3.T1.13.7.30.23.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.30.23.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.30.23.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.30.23.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">15B</td>
<td id="S3.T1.13.7.30.23.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">42.9</td>
<td id="S3.T1.13.7.30.23.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">22.2</td>
<td id="S3.T1.13.7.30.23.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">68.6</td>
<td id="S3.T1.13.7.30.23.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">71.0</td>
<td id="S3.T1.13.7.30.23.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">86.1</td>
<td id="S3.T1.13.7.30.23.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">45.1</td>
<td id="S3.T1.13.7.30.23.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">47.7</td>
<td id="S3.T1.13.7.30.23.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">52.6</td>
<td id="S3.T1.13.7.30.23.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">65.6</td>
<td id="S3.T1.13.7.30.23.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">55.8</td>
</tr>
<tr id="S3.T1.13.7.31.24" class="ltx_tr">
<td id="S3.T1.13.7.31.24.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S3.T1.13.7.31.24.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math</td>
<td id="S3.T1.13.7.31.24.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.31.24.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.31.24.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.31.24.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">10.5B</td>
<td id="S3.T1.13.7.31.24.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.6.1" class="ltx_text" style="color:#BF0040;">66.9</span></td>
<td id="S3.T1.13.7.31.24.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.7.1" class="ltx_text" style="color:#BF0040;">31.0</span></td>
<td id="S3.T1.13.7.31.24.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.8.1" class="ltx_text" style="color:#BF0040;">77.8</span></td>
<td id="S3.T1.13.7.31.24.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.9.1" class="ltx_text" style="color:#BF0040;">79.0</span></td>
<td id="S3.T1.13.7.31.24.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.10.1" class="ltx_text" style="color:#BF0040;">93.9</span></td>
<td id="S3.T1.13.7.31.24.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.11.1" class="ltx_text" style="color:#BF0040;">49.9</span></td>
<td id="S3.T1.13.7.31.24.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.12.1" class="ltx_text" style="color:#BF0040;">58.7</span></td>
<td id="S3.T1.13.7.31.24.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.13.1" class="ltx_text" style="color:#BF0040;">54.6</span></td>
<td id="S3.T1.13.7.31.24.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.14.1" class="ltx_text" style="color:#BF0040;">84.4</span></td>
<td id="S3.T1.13.7.31.24.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.15.1" class="ltx_text" style="color:#BF0040;">66.2</span></td>
</tr>
<tr id="S3.T1.13.7.7" class="ltx_tr">
<td id="S3.T1.13.7.7.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><math id="S3.T1.13.7.7.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T1.13.7.7.1.m1.1a"><mi mathvariant="normal" id="S3.T1.13.7.7.1.m1.1.1" xref="S3.T1.13.7.7.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T1.13.7.7.1.m1.1b"><ci id="S3.T1.13.7.7.1.m1.1.1.cmml" xref="S3.T1.13.7.7.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.7.7.1.m1.1c">\Delta</annotation></semantics></math></td>
<td id="S3.T1.13.7.7.2" class="ltx_td ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.13.7.7.3" class="ltx_td ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.13.7.7.4" class="ltx_td ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.13.7.7.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="background-color:#E6E6E6;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.5.1" class="ltx_text" style="background-color:#E6E6E6;">-30%</span></td>
<td id="S3.T1.13.7.7.6" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.6.1" class="ltx_text" style="background-color:#D2DCFA;">+24.0</span></td>
<td id="S3.T1.13.7.7.7" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.7.1" class="ltx_text" style="background-color:#D2DCFA;">+8.8</span></td>
<td id="S3.T1.13.7.7.8" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.8.1" class="ltx_text" style="background-color:#D2DCFA;">+9.2</span></td>
<td id="S3.T1.13.7.7.9" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.9.1" class="ltx_text" style="background-color:#D2DCFA;">+8.0</span></td>
<td id="S3.T1.13.7.7.10" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.10.1" class="ltx_text" style="background-color:#D2DCFA;">+7.8</span></td>
<td id="S3.T1.13.7.7.11" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.11.1" class="ltx_text" style="background-color:#D2DCFA;">+4.8</span></td>
<td id="S3.T1.13.7.7.12" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.12.1" class="ltx_text" style="background-color:#D2DCFA;">+11.0</span></td>
<td id="S3.T1.13.7.7.13" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.13.1" class="ltx_text" style="background-color:#D2DCFA;">+2.0</span></td>
<td id="S3.T1.13.7.7.14" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.14.1" class="ltx_text" style="background-color:#D2DCFA;">+18.8</span></td>
<td id="S3.T1.13.7.7.15" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.15.1" class="ltx_text ltx_font_bold" style="background-color:#D2DCFA;">+10.4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Corpus</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">수학적 추론을 위해 Common Crawl의 수학 관련 웹 페이지에서 가져온 약 14B 토큰으로 구성된 OpenWebM(OWM) 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Paster et al.</span>, <a class="ltx_ref" href="#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>를 활용한다. 일반 도메인에서는 SlimPajama <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Daria et al.</span>, <a class="ltx_ref" href="#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>와 StarCoderData <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et al.</span>, <a class="ltx_ref" href="#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">2023a</span></a>)</cite>(Tinyllama corpus의 양쪽 부분)를 OpenWebMath와 결합하여 6:3:1의 혼합 비율을 가진 총 800억 개의 토큰에 대해 학습한다.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Setting</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">수학 사전 훈련은 Tinyllama-1.1B 모델 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang et al.</span>, <a class="ltx_ref" href="#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>와 Mistral-7B 모델 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jiang et al.</span>, <a class="ltx_ref" href="#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>에서 각각 8e-5와 2e-5의 학습률을 가지고 사전 훈련을 계속한다. 일반 도메인의 경우 Tinyllama-1.1B 모델의 학습률을 1e-4로 설정한다. 배치 크기는 두 도메인에 대해 균일하게 1M 토큰으로 설정된다. 토큰 선택 비율과 관련하여 Tinyllama-1.1B 모델의 경우 60%, Mistral-7B 모델의 경우 70%를 사용한다.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Baseline Setting</h4>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">우리는 규칙적인 인과 언어 모델링을 통해 지속적으로 사전 훈련된 모델(Tinyllama-CT 및 Mistral-CT)을 기준선으로 사용한다. 또한, <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.SSS0.Px4.p1.1.1">Rho-1</span>을 잘 알려진 상위 성능 기준선과 비교하며, Gemma<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Team et al.</span>, <a class="ltx_ref" href="#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, Qwen1.5 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bai et al.</span>, <a class="ltx_ref" href="#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Phi-1.5 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et al.</span>, <a class="ltx_ref" href="#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">2023b</span></a>)</cite>, DeepSeekLLM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">DeepSeek-AI</span>, <a class="ltx_ref" href="#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, DeepSeekMath <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et al.</span>, <a class="ltx_ref" href="#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">2024a</span></a>)</cite>, CodeLlama <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Roziere et al.</span>, <a class="ltx_ref" href="#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Mistral <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jiang et al.</span>, <a class="ltx_ref" href="#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Minerva <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lewkowycz et al.</span>, <a class="ltx_ref" href="#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, Tinyllama <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang et al.</span>, <a class="ltx_ref" href="#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, LLemma <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Azerbayev et al.</span>, <a class="ltx_ref" href="#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, InternLM2-Math <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ying et al.</span>, <a class="ltx_ref" href="#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>를 포함한다 미세 조정 결과를 위해 기존 베스트 모델인 MAmmoTH<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yue et al.</span>, <a class="ltx_ref" href="#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>와 ToRA<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gou et al.</span>, <a class="ltx_ref" href="#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>와 비교한다.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation Setup</h4>

<div id="S3.SS1.SSS0.Px5.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px5.p1.1">사전 훈련된 모델을 종합적으로 평가하기 위해 다양한 작업에서 소수의 샷 기능과 미세 조정 성능을 비교한다. 일반 과제는 lm-eval-harness<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank" title="">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span>을, 수학 과제는 math-eval-harness<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ZubinGou/math-evaluation-harness" target="_blank" title="">https://github.com/ZubinGou/math-evaluation-harness</a></span></span></span>을 채택한다. 우리는 추론 속도를 높이기 위해 vllm (v0.3.2) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kwon et al.</span>, <a class="ltx_ref" href="#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>를 사용한다. 우리의 평가에 대한 자세한 내용은 <a class="ltx_ref ltx_refmacro_autoref" href="#A3" title="Appendix C Evalution Details ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Appendix C</span></a>에서 찾을 수 있다.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.5.1.1" style="font-size:90%;">Table 2</span>:</span><span class="ltx_text ltx_font_bold" id="S3.T2.6.2" style="font-size:90%;">Tool-integrated reasoning results of math pretraining. </span></figcaption>
<div id="S3.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:234.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-50.6pt,27.2pt) scale(0.810840269113284,0.810840269113284) ;">
<table id="S3.T2.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.2.2.3.1" class="ltx_tr">
<td id="S3.T2.2.2.3.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T2.2.2.3.1.2" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S3.T2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.3.1" class="ltx_text ltx_font_bold">Tools</span></td>
<td id="S3.T2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.4.1" class="ltx_text ltx_font_bold">SFT Data</span></td>
<td id="S3.T2.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.5.1" class="ltx_text ltx_font_bold">GSM8k</span></td>
<td id="S3.T2.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.6.1" class="ltx_text ltx_font_bold">MATH</span></td>
<td id="S3.T2.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.7.1" class="ltx_text ltx_font_bold">SVAMP</span></td>
<td id="S3.T2.2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.8.1" class="ltx_text ltx_font_bold">ASDiv</span></td>
<td id="S3.T2.2.2.3.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.9.1" class="ltx_text ltx_font_bold">MAWPS</span></td>
<td id="S3.T2.2.2.3.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.10.1" class="ltx_text ltx_font_bold">TAB</span></td>
<td id="S3.T2.2.2.3.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.11.1" class="ltx_text ltx_font_bold">GSM-H</span></td>
<td id="S3.T2.2.2.3.1.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S3.T2.2.2.3.1.12.1" class="ltx_text ltx_font_bold">AVG</span></td>
</tr>
<tr id="S3.T2.2.2.4.2" class="ltx_tr">
<td id="S3.T2.2.2.4.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="4"><span id="S3.T2.2.2.4.2.1.1" class="ltx_text ltx_font_bold">Used for SFT?</span></td>
<td id="S3.T2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.2.1" class="ltx_text" style="color:#326400;">✓</span></td>
<td id="S3.T2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.3.1" class="ltx_text" style="color:#326400;">✓</span></td>
<td id="S3.T2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.4.1" class="ltx_text" style="color:#C80000;">✗</span></td>
<td id="S3.T2.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.5.1" class="ltx_text" style="color:#C80000;">✗</span></td>
<td id="S3.T2.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.6.1" class="ltx_text" style="color:#C80000;">✗</span></td>
<td id="S3.T2.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.7.1" class="ltx_text" style="color:#C80000;">✗</span></td>
<td id="S3.T2.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.8.1" class="ltx_text" style="color:#C80000;">✗</span></td>
</tr>
<tr id="S3.T2.2.2.5.3" class="ltx_tr">
<td id="S3.T2.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="12"><span id="S3.T2.2.2.5.3.1.1" class="ltx_text ltx_font_typewriter">Previous Models</span></td>
</tr>
<tr id="S3.T2.2.2.6.4" class="ltx_tr">
<td id="S3.T2.2.2.6.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">GPT4-0314</td>
<td id="S3.T2.2.2.6.4.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.6.4.3.1" class="ltx_text" style="color:#C80000;">✗</span></td>
<td id="S3.T2.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">92.0</td>
<td id="S3.T2.2.2.6.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">42.5</td>
<td id="S3.T2.2.2.6.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">93.1</td>
<td id="S3.T2.2.2.6.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">91.3</td>
<td id="S3.T2.2.2.6.4.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">97.6</td>
<td id="S3.T2.2.2.6.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">67.1</td>
<td id="S3.T2.2.2.6.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">64.7</td>
<td id="S3.T2.2.2.6.4.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">78.3</td>
</tr>
<tr id="S3.T2.2.2.7.5" class="ltx_tr">
<td id="S3.T2.2.2.7.5.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">GPT4-0314 (PAL)</td>
<td id="S3.T2.2.2.7.5.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.7.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.7.5.3.1" class="ltx_text" style="color:#326400;">✓</span></td>
<td id="S3.T2.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.7.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">94.2</td>
<td id="S3.T2.2.2.7.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">51.8</td>
<td id="S3.T2.2.2.7.5.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">94.8</td>
<td id="S3.T2.2.2.7.5.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">92.6</td>
<td id="S3.T2.2.2.7.5.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">97.7</td>
<td id="S3.T2.2.2.7.5.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">95.9</td>
<td id="S3.T2.2.2.7.5.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">77.6</td>
<td id="S3.T2.2.2.7.5.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">86.4</td>
</tr>
<tr id="S3.T2.2.2.8.6" class="ltx_tr">
<td id="S3.T2.2.2.8.6.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">MAmmoTH</td>
<td id="S3.T2.2.2.8.6.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">70B</td>
<td id="S3.T2.2.2.8.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.8.6.3.1" class="ltx_text" style="color:#326400;">✓</span></td>
<td id="S3.T2.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MI-260k</td>
<td id="S3.T2.2.2.8.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
<td id="S3.T2.2.2.8.6.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">41.8</td>
<td id="S3.T2.2.2.8.6.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">82.4</td>
<td id="S3.T2.2.2.8.6.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.8.6.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.8.6.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.8.6.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.8.6.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S3.T2.2.2.9.7" class="ltx_tr">
<td id="S3.T2.2.2.9.7.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA</td>
<td id="S3.T2.2.2.9.7.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td id="S3.T2.2.2.9.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.9.7.3.1" class="ltx_text" style="color:#326400;">✓</span></td>
<td id="S3.T2.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.9.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">68.8</td>
<td id="S3.T2.2.2.9.7.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">40.1</td>
<td id="S3.T2.2.2.9.7.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">68.2</td>
<td id="S3.T2.2.2.9.7.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">73.9</td>
<td id="S3.T2.2.2.9.7.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">88.8</td>
<td id="S3.T2.2.2.9.7.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">42.4</td>
<td id="S3.T2.2.2.9.7.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">54.6</td>
<td id="S3.T2.2.2.9.7.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">62.4</td>
</tr>
<tr id="S3.T2.2.2.10.8" class="ltx_tr">
<td id="S3.T2.2.2.10.8.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA</td>
<td id="S3.T2.2.2.10.8.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">70B</td>
<td id="S3.T2.2.2.10.8.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.10.8.3.1" class="ltx_text" style="color:#326400;">✓</span></td>
<td id="S3.T2.2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.10.8.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">84.3</td>
<td id="S3.T2.2.2.10.8.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">49.7</td>
<td id="S3.T2.2.2.10.8.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">82.7</td>
<td id="S3.T2.2.2.10.8.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">86.8</td>
<td id="S3.T2.2.2.10.8.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">93.8</td>
<td id="S3.T2.2.2.10.8.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">74.0</td>
<td id="S3.T2.2.2.10.8.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">67.2</td>
<td id="S3.T2.2.2.10.8.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
</tr>
<tr id="S3.T2.2.2.11.9" class="ltx_tr">
<td id="S3.T2.2.2.11.9.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">DeepSeekMath</td>
<td id="S3.T2.2.2.11.9.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td id="S3.T2.2.2.11.9.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.11.9.3.1" class="ltx_text" style="color:#326400;">✓</span></td>
<td id="S3.T2.2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.11.9.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">79.8</td>
<td id="S3.T2.2.2.11.9.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">52.0</td>
<td id="S3.T2.2.2.11.9.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">80.1</td>
<td id="S3.T2.2.2.11.9.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">87.1</td>
<td id="S3.T2.2.2.11.9.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">93.8</td>
<td id="S3.T2.2.2.11.9.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">85.8</td>
<td id="S3.T2.2.2.11.9.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">63.1</td>
<td id="S3.T2.2.2.11.9.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">77.4</td>
</tr>
<tr id="S3.T2.2.2.12.10" class="ltx_tr">
<td id="S3.T2.2.2.12.10.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="12"><span id="S3.T2.2.2.12.10.1.1" class="ltx_text ltx_font_typewriter">Our Pretrained Models</span></td>
</tr>
<tr id="S3.T2.2.2.13.11" class="ltx_tr">
<td id="S3.T2.2.2.13.11.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">TinyLlama-CT</td>
<td id="S3.T2.2.2.13.11.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">1B</td>
<td id="S3.T2.2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.13.11.3.1" class="ltx_text" style="color:#326400;">✓</span></td>
<td id="S3.T2.2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">51.4</td>
<td id="S3.T2.2.2.13.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">38.4</td>
<td id="S3.T2.2.2.13.11.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">53.4</td>
<td id="S3.T2.2.2.13.11.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">66.7</td>
<td id="S3.T2.2.2.13.11.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">81.7</td>
<td id="S3.T2.2.2.13.11.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">20.5</td>
<td id="S3.T2.2.2.13.11.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">42.8</td>
<td id="S3.T2.2.2.13.11.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">50.7</td>
</tr>
<tr id="S3.T2.2.2.14.12" class="ltx_tr">
<td id="S3.T2.2.2.14.12.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T2.2.2.14.12.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math</td>
<td id="S3.T2.2.2.14.12.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">1B</td>
<td id="S3.T2.2.2.14.12.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.3.1" class="ltx_text" style="color:#326400;">✓</span></td>
<td id="S3.T2.2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.14.12.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.5.1" class="ltx_text" style="color:#BF0040;">59.4</span></td>
<td id="S3.T2.2.2.14.12.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.6.1" class="ltx_text" style="color:#BF0040;">40.6</span></td>
<td id="S3.T2.2.2.14.12.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.7.1" class="ltx_text" style="color:#BF0040;">60.7</span></td>
<td id="S3.T2.2.2.14.12.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.8.1" class="ltx_text" style="color:#BF0040;">74.2</span></td>
<td id="S3.T2.2.2.14.12.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.9.1" class="ltx_text" style="color:#BF0040;">88.6</span></td>
<td id="S3.T2.2.2.14.12.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.10.1" class="ltx_text" style="color:#BF0040;">26.7</span></td>
<td id="S3.T2.2.2.14.12.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.11.1" class="ltx_text" style="color:#BF0040;">48.1</span></td>
<td id="S3.T2.2.2.14.12.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.12.1" class="ltx_text" style="color:#BF0040;">56.9</span></td>
</tr>
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S3.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T2.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S3.T2.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\Delta</annotation></semantics></math></td>
<td id="S3.T2.1.1.1.2" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.1.1.1.3" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.5.1" class="ltx_text" style="background-color:#D2DCFA;">+8.0</span></td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.6.1" class="ltx_text" style="background-color:#D2DCFA;">+2.2</span></td>
<td id="S3.T2.1.1.1.7" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.7.1" class="ltx_text" style="background-color:#D2DCFA;">+7.3</span></td>
<td id="S3.T2.1.1.1.8" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.8.1" class="ltx_text" style="background-color:#D2DCFA;">+7.5</span></td>
<td id="S3.T2.1.1.1.9" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.9.1" class="ltx_text" style="background-color:#D2DCFA;">+6.9</span></td>
<td id="S3.T2.1.1.1.10" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.10.1" class="ltx_text" style="background-color:#D2DCFA;">+6.2</span></td>
<td id="S3.T2.1.1.1.11" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.11.1" class="ltx_text" style="background-color:#D2DCFA;">+5.3</span></td>
<td id="S3.T2.1.1.1.12" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.12.1" class="ltx_text ltx_font_bold" style="background-color:#D2DCFA;">+6.2</span></td>
</tr>
<tr id="S3.T2.2.2.15.13" class="ltx_tr">
<td id="S3.T2.2.2.15.13.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Mistral-CT</td>
<td id="S3.T2.2.2.15.13.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td id="S3.T2.2.2.15.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.15.13.3.1" class="ltx_text" style="color:#326400;">✓</span></td>
<td id="S3.T2.2.2.15.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.15.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">77.5</td>
<td id="S3.T2.2.2.15.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">48.4</td>
<td id="S3.T2.2.2.15.13.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
<td id="S3.T2.2.2.15.13.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">83.8</td>
<td id="S3.T2.2.2.15.13.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">93.4</td>
<td id="S3.T2.2.2.15.13.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">67.5</td>
<td id="S3.T2.2.2.15.13.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">60.4</td>
<td id="S3.T2.2.2.15.13.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">72.6</td>
</tr>
<tr id="S3.T2.2.2.16.14" class="ltx_tr">
<td id="S3.T2.2.2.16.14.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T2.2.2.16.14.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math</td>
<td id="S3.T2.2.2.16.14.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td id="S3.T2.2.2.16.14.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.3.1" class="ltx_text" style="color:#326400;">✓</span></td>
<td id="S3.T2.2.2.16.14.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.16.14.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.5.1" class="ltx_text" style="color:#BF0040;">81.3</span></td>
<td id="S3.T2.2.2.16.14.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.6.1" class="ltx_text" style="color:#BF0040;">51.8</span></td>
<td id="S3.T2.2.2.16.14.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.7.1" class="ltx_text" style="color:#BF0040;">80.8</span></td>
<td id="S3.T2.2.2.16.14.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.8.1" class="ltx_text" style="color:#BF0040;">85.5</span></td>
<td id="S3.T2.2.2.16.14.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.9.1" class="ltx_text" style="color:#BF0040;">94.5</span></td>
<td id="S3.T2.2.2.16.14.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.10.1" class="ltx_text" style="color:#BF0040;">70.1</span></td>
<td id="S3.T2.2.2.16.14.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.11.1" class="ltx_text" style="color:#BF0040;">63.1</span></td>
<td id="S3.T2.2.2.16.14.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.12.1" class="ltx_text" style="color:#BF0040;">75.3</span></td>
</tr>
<tr id="S3.T2.2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.2.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S3.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T2.2.2.2.1.m1.1a"><mi mathvariant="normal" id="S3.T2.2.2.2.1.m1.1.1" xref="S3.T2.2.2.2.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.1.m1.1b"><ci id="S3.T2.2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.2.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.1.m1.1c">\Delta</annotation></semantics></math></td>
<td id="S3.T2.2.2.2.2" class="ltx_td ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.2.2.2.3" class="ltx_td ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.2.2.2.4" class="ltx_td ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.5.1" class="ltx_text" style="background-color:#D2DCFA;">+3.8</span></td>
<td id="S3.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.6.1" class="ltx_text" style="background-color:#D2DCFA;">+3.4</span></td>
<td id="S3.T2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.7.1" class="ltx_text" style="background-color:#D2DCFA;">+3.9</span></td>
<td id="S3.T2.2.2.2.8" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.8.1" class="ltx_text" style="background-color:#D2DCFA;">+1.7</span></td>
<td id="S3.T2.2.2.2.9" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.9.1" class="ltx_text" style="background-color:#D2DCFA;">+1.1</span></td>
<td id="S3.T2.2.2.2.10" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.10.1" class="ltx_text" style="background-color:#D2DCFA;">+2.6</span></td>
<td id="S3.T2.2.2.2.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.11.1" class="ltx_text" style="background-color:#D2DCFA;">+2.7</span></td>
<td id="S3.T2.2.2.2.12" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.12.1" class="ltx_text ltx_font_bold" style="background-color:#D2DCFA;">+2.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Math Pre-training Results</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Few-shot CoT Reasoning Results</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">우리는 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lewkowycz et al.</span>, <a class="ltx_ref" href="#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Azerbayev et al.</span>, <a class="ltx_ref" href="#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Shao et al.</span>, <a class="ltx_ref" href="#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">2024b</span></a>)</cite>에 이어 소수의shot chain-of-thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wei et al.</span>, <a class="ltx_ref" href="#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">2022a</span></a>)</cite> 예제로 프롬프트하는 기본 모델을 평가한다. <a class="ltx_ref ltx_refmacro_autoref" href="#S3.T1" title="Table 1 ‣ Reference Model Training ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Table 1</span></a>에 나타난 결과, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.1">Rho-1</span>-Math는 1B 모델에서는 16.5%, 7B 모델에서는 10.4%의 평균 소샷 정확도 향상을 달성하였다. 또한 OpenWebMath에서 여러 epoch에 대한 학습 후 <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.2">Rho-1</span>은 평균 소수 샷 정확도를 40.9%로 더욱 높일 수 있음을 발견했다. 5,000억 개의 수학 관련 토큰에 대해 사전 훈련된 DeepSeekMath-7B와 비교하여 <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.3">Rho-1</span>-7B만 사전 훈련된 150억 개의 토큰(105억 개의 토큰을 선택)에 대해 유사한 결과를 달성하여 접근법의 효율성을 입증했다.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Tool-Integrated Reasoning Results</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.1">Rho-1</span> 및 69k ToRA 말뭉치 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gou et al.</span>, <a class="ltx_ref" href="#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>의 기준선 모델은 도구 통합 추론 형식의 16k GPT-4 생성 궤적과 LLaMA를 사용한 53k 답변 증강 샘플로 구성된다. <a class="ltx_ref ltx_refmacro_autoref" href="#S3.T2" title="Table 2 ‣ Evaluation Setup ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Table 2</span></a>, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.2">Rho-1</span>-1B 및 <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.3">Rho-1</span>-7B는 MATH 데이터 세트에서 각각 40.6% 및 51.8%의 최신 상태를 달성했습니다. (<em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.4">e.g.,</em> TabMWP and GSM-Hard), <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.5">Rho-1</span> 또한 <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.6">Rho-1</span> -Math-1B 및 <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.7">Rho-1</span> -Math-7B에서 평균 몇 번의 샷 정확도 향상이 6.2%로 어느 정도 일반화 가능성을 보여줍니다.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="256" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.4.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.5.2" class="ltx_text" style="font-size:90%;">
<span id="S3.F5.5.2.1" class="ltx_text ltx_font_bold">General pretraining results.</span>
We continual pretraining Tinyllama-1B on 80G general tokens. Tinyllama-CT is etrained with CLM, while <span id="S3.F5.5.2.2" class="ltx_text ltx_font_smallcaps">Rho-1</span> is trained with our proposed SLM.
</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>General Pre-training Results</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p1.1">우리는 800억 토큰에 대한 Tinyllama-1.1B를 지속적으로 훈련하여 일반적인 사전 훈련에서 SLM의 효능을 확인한다. <a class="ltx_ref ltx_refmacro_autoref" href="#S3.F5" title="Figure 5 ‣ Tool-Integrated Reasoning Results ‣ 3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>에 표시된 결과는 Tinyllama가 이미 이러한 토큰의 대다수에 대해 광범위한 훈련을 받았지만 SLM의 적용은 직접 연속 사전 훈련에 비해 15개의 벤치마크에서 평균 6.8%의 향상을 산출한다는 것을 나타낸다. 특히 코드와 수학 과제에서 개선이 두드러져 10%를 넘어섰다.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="149" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.3.1.1" style="font-size:90%;">Figure 6</span>:</span><span class="ltx_text ltx_font_bold" id="S3.F6.4.2" style="font-size:90%;">The dynamics of pretraining loss and downstream loss. <span class="ltx_text ltx_font_medium" id="S3.F6.4.2.1"> (a) 및 (c)는 SLM 및 CLM 방법 모두에서 사전 훈련 동안 SLM에 의해 선택/선택되지 않은 토큰의 손실을 나타내는 반면, (b)는 다운스트림 말뭉치에서 SLM 및 CLM 방법의 손실을 나타낸다. 총 40억 토큰으로 사전 훈련하는 과정을 통해 위의 결과를 검정하였다. </span></span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Analysis</h3>

<section id="S3.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Selected Token Loss Aligns Better with Downstream Performance</h4>

<div id="S3.SS4.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">참조 모델을 사용하여 토큰을 필터링하고 다운스트림 손실과의 관계를 관찰하면서 모든/선택된 토큰에 대한 훈련 후 유효성 검사 손실의 변화를 탐색한다. <a class="ltx_ref ltx_refmacro_autoref" href="#S3.F6" title="Figure 6 ‣ 3.3 General Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>와 같이 약 4B 토큰을 사전 훈련하고 사전 훈련 과정에서 서로 다른 사전 훈련 방법과 검증 세트에 손실의 변동 곡선을 표시했다. 참조 모델에 의해 선택된 토큰에서 <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px1.p1.1.1">Rho-1</span>의 평균 손실 감소가 정규 사전 훈련에 비해 더 중요하다는 것을 관찰할 수 있다. 반대로 선택되지 않은 토큰에서는 정규 사전 훈련의 평균 손실 감소가 더 크다. 무화과(a), 무화과(b)를 무화과(c)와 연관시키면 선택된 토큰에 대해 훈련된 모델이 다운스트림 손실이 더 크게 감소한다는 것을 발견하는 것은 어렵지 않은 반면, 일반적인 사전 훈련은 훈련 단계에서 모든 토큰의 평균 손실을 감소시키기는 하지만 다운스트림 손실이 크게 감소하기 어렵다. 따라서 사전 교육을 위한 토큰을 선택하는 것이 더 효율적일 것으로 기대한다.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x7.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.3.1.1" style="font-size:90%;">Figure 7</span>:</span><span class="ltx_text ltx_font_bold" id="S3.F7.4.2" style="font-size:90%;">The relationship between the selected tokens / unselected tokens loss in SLM and downstream task performance. <span class="ltx_text ltx_font_medium" id="S3.F7.4.2.1"> The y-axis represents the average few-shot accuracy on GSM8k and MATH. x축은 해당 체크포인트(2B, 5B, 8B, 11B, 14B)에서 선택된 토큰/선택되지 않은 토큰에 대한 평균 손실을 나타낸다. </span></span></figcaption>
</figure>
<div id="S3.SS4.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p2.1">또한, 동시 연구 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gadre et al.</span>, <a class="ltx_ref" href="#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>와 유사한 <a class="ltx_ref ltx_refmacro_autoref" href="#S3.F7" title="Figure 7 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>의 멱 법칙을 통해 선택된 토큰의 손실을 다운스트림 태스크 성능과 연관시킨다. 그래프의 데이터 포인트로부터 피팅된 곡선을 관찰하면 SLM에 의해 선택된 토큰의 평균 손실은 다운스트림 태스크의 성능과 양의 상관 관계를 나타내는 반면, 선택되지 않은 토큰의 평균 손실은 다운스트림 태스크의 성능과 음의 상관 관계를 나타낸다. 따라서 모델의 궁극적인 성과에 도움이 되기 위해 모든 토큰의 손실이 감소할 필요는 없다. 자세한 내용은 <a class="ltx_ref ltx_refmacro_autoref" href="#A4" title="Appendix D Relate the Selected Tokens’ Loss to Downstream Task Performance ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Appendix D</span></a>를 참조하십시오.</p>
</div>
<figure id="S3.SS4.SSS0.Px1.2" class="ltx_table">
<div id="S3.SS4.SSS0.Px1.2.3" class="ltx_block">
<figure id="S3.F8" class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" style="width:212.5pt;"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x8.png" id="S3.SS4.SSS0.Px1.1.1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="311" alt="Refer to caption">
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.3.1.1" style="font-size:90%;">Figure 8</span>:</span><span class="ltx_text ltx_font_bold" id="S3.F8.4.2" style="font-size:90%;">The PPL of tokens selected by different checkpoint. <span class="ltx_text ltx_font_medium" id="S3.F8.4.2.1">2B, 5B, 8B, 11B, 14B에서 선택한 토큰의 PPL을 테스트합니다. </span></span></figcaption><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">The PPL of tokens selected by different checkpoint.<span id="S3.F8.4.2.1" class="ltx_text ltx_font_medium"> We test the PPL of the tokens selected at 2B, 5B, 8B, 11B, and 14B.
</span></span></figcaption>
</figure>
<figure id="S3.F9" class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" style="width:208.1pt;"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x9.png" id="S3.SS4.SSS0.Px1.2.2.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="347" alt="Refer to caption">
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F9.3.1.1" style="font-size:90%;">Figure 9</span>:</span><span class="ltx_text ltx_font_bold" id="S3.F9.4.2" style="font-size:90%;">토큰 선택 비율의 효과. <span class="ltx_text ltx_font_medium" id="S3.F9.4.2.1">We train 1B LM with SLM objective on 5B tokens. </span></span></figcaption>
</figure>
</div>
</figure>
</section>
<section id="S3.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">What Tokens are Selected with SLM?</h4>

<div id="S3.SS4.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.1">우리는 SLM 방법으로 선택된 토큰을 사전 훈련에서 분석하여 작동 메커니즘을 더 탐구하는 것을 목표로 한다. 이를 위해 OpenWebMath를 사용하여 <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px2.p1.1.1">Rho-1</span> 훈련 중 토큰 선택 프로세스를 시각화합니다. <a class="ltx_ref ltx_refmacro_autoref" href="#A5.SS1" title="E.1 Token Selected Examples ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§E.1</span></a>에서는 <span class="ltx_text" id="S3.SS4.SSS0.Px2.p1.1.2" style="color:#1E90FF;">blue</span> 실제 사전 훈련 중에 유지 된 토큰을 강조 표시 했습니다. SLM 방법에 의해 선택된 토큰의 대부분은 수학과 밀접한 관련이 있으며, 수학적 내용과 관련된 원래 말뭉치의 부분에 대해 모델을 효과적으로 훈련시킨다는 것을 관찰한다.</p>
</div>
<div id="S3.SS4.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p2.1">또한, 훈련 과정에서 다양한 체크포인트에 대한 토큰 필터링의 차이를 조사하고 다른 체크포인트에서 이러한 토큰의 복잡성을 테스트했다. <a class="ltx_ref ltx_refmacro_autoref" href="#S3.F8" title="Figure 8 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>에 예시된 바와 같이, 우리는 후기 체크포인트에 의해 선택된 토큰이 훈련의 후기 단계로 갈수록 더 높은 복잡도를 갖고 초기 단계에서 더 낮은 복잡도를 갖는 경향이 있음을 발견했다. 이는 모델이 먼저 학습 가능한 공간이 더 큰 토큰을 최적화하여 학습 효율을 높일 수 있음을 시사할 수 있다. 또한, 선택된 토큰의 손실에 대한 샘플별 "이중 하강" <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Nakkiran et al.</span>, <a class="ltx_ref" href="#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>를 발견했는데, 여기서 선택 토큰의 복잡도는 감소하기 전에 처음에 증가한다. 이는 각 체크포인트에서 가장 필요한 사람들을 대상으로 초과손실을 기준으로 토큰을 선택하는 효과일 수 있다.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effect of Token Select Ratio</h4>

<div id="S3.SS4.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS4.SSS0.Px3.p1.1">우리는 SLM의 토큰 선택 비율이 미치는 영향을 조사한다. 일반적으로, 선택 비율은 이전에 Masked Language Models (MLMs) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Devlin et al.</span>, <a class="ltx_ref" href="#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Liu et al.</span>, <a class="ltx_ref" href="#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>의 훈련에서 사용된 접근법과 유사하게 휴리스틱 규칙에 의해 정의된다. <a class="ltx_ref ltx_refmacro_autoref" href="#S3.F9" title="Figure 9 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 9</span></a>에 도시된 바와 같이, 선택된 토큰은 원래 토큰의 약 60%를 차지하기에 적합하다.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Weak-to-Strong Generization</h4>

<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.5.1.1" style="font-size:90%;">Table 3</span>:</span><span class="ltx_text ltx_font_bold" id="S3.T3.6.2" style="font-size:90%;">Weak-to-Strong generization result on math benchmark. <span class="ltx_text ltx_font_medium" id="S3.T3.6.2.1"></span></span></figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S3.T3.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Weak-to-Strong generization result on math benchmark.<span id="S3.T3.6.2.1" class="ltx_text ltx_font_medium">
</span></span></figcaption>
<div id="S3.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:73.2pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.2pt,2.5pt) scale(0.934457108709658,0.934457108709658) ;">
<table id="S3.T3.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.2.2.2" class="ltx_tr">
<th id="S3.T3.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.3.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S3.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.4.1" class="ltx_text ltx_font_bold">GSM8K</span></th>
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">MATH<sup id="S3.T3.1.1.1.1.1.1" class="ltx_sup"><span id="S3.T3.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span></th>
<th id="S3.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.5.1" class="ltx_text ltx_font_bold">SVAMP</span></th>
<th id="S3.T3.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.6.1" class="ltx_text ltx_font_bold">ASDiv</span></th>
<th id="S3.T3.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.7.1" class="ltx_text ltx_font_bold">MAWPS</span></th>
<th id="S3.T3.2.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.8.1" class="ltx_text ltx_font_bold">TAB</span></th>
<th id="S3.T3.2.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.9.1" class="ltx_text ltx_font_bold">MQA</span></th>
<th id="S3.T3.2.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T3.2.2.2.10.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T3.2.2.2.10.1.1" class="ltx_tr">
<td id="S3.T3.2.2.2.10.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.10.1.1.1.1" class="ltx_text ltx_font_bold">MMLU</span></td>
</tr>
<tr id="S3.T3.2.2.2.10.1.2" class="ltx_tr">
<td id="S3.T3.2.2.2.10.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.10.1.2.1.1" class="ltx_text ltx_font_bold">STEM</span></td>
</tr>
</tbody></table>
</th>
<th id="S3.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.2.1" class="ltx_text ltx_font_bold">SAT<sup id="S3.T3.2.2.2.2.1.1" class="ltx_sup"><span id="S3.T3.2.2.2.2.1.1.1" class="ltx_text ltx_font_medium">‡</span></sup></span></th>
<th id="S3.T3.2.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.11.1" class="ltx_text ltx_font_bold">AVG</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.2.2.3.1" class="ltx_tr">
<th id="S3.T3.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Llama-2-7B-CT</th>
<td id="S3.T3.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">28.4</td>
<td id="S3.T3.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">13.6</td>
<td id="S3.T3.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">50.3</td>
<td id="S3.T3.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">62.8</td>
<td id="S3.T3.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">79.5</td>
<td id="S3.T3.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">37.6</td>
<td id="S3.T3.2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">34.1</td>
<td id="S3.T3.2.2.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">41.6</td>
<td id="S3.T3.2.2.3.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">43.5</td>
<td id="S3.T3.2.2.3.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">43.5</td>
</tr>
<tr id="S3.T3.2.2.4.2" class="ltx_tr">
<th id="S3.T3.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">Llama-2-7B-CT w/ 1B RM</th>
<td id="S3.T3.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">29.8</td>
<td id="S3.T3.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">16.0</td>
<td id="S3.T3.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">55.5</td>
<td id="S3.T3.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">63.7</td>
<td id="S3.T3.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">80.4</td>
<td id="S3.T3.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">37.9</td>
<td id="S3.T3.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">34.3</td>
<td id="S3.T3.2.2.4.2.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">38.2</td>
<td id="S3.T3.2.2.4.2.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">43.8</td>
<td id="S3.T3.2.2.4.2.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">44.4</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS4.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS4.SSS0.Px4.p1.1">참조 및 연속 사전 훈련에 동일한 기본 모델을 사용하는 주요 실험과는 별도로, 더 작은 참조 모델이 더 큰 모델의 사전 훈련을 효과적으로 안내할 수 있는지 조사한다. 우리는 Tinyllma-1.1B를 참조 모델로 사용하고 Llama-2-7B를 수학에 지속적으로 사전 훈련한다. <a class="ltx_ref ltx_refmacro_autoref" href="#S3.T3" title="Table 3 ‣ Weak-to-Strong Generization ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Table 3</span></a>에 제시된 결과는 작은 모델과 큰 모델 사이의 상당한 격차에도 불구하고 토큰 선택에 작은 참조 모델을 사용하는 것이 더 큰 모델의 사전 훈련에 여전히 이점을 얻을 수 있음을 나타낸다. 참조 모델과 학습 모델이 다른 어휘를 갖는 경우, 향후 작업을 위해 떠나는 토큰 정렬 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wan et al.</span>, <a class="ltx_ref" href="#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et al.</span>, <a class="ltx_ref" href="#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>를 수행하는 것을 고려할 수 있다.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Works</h2>

<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Data Optimization</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">사전 학습 말뭉치의 최적화는 사전 학습 데이터 혼합의 품질과 규모를 향상시켜 언어 모델 학습의 성능과 효율성을 극대화하는 것이다. 여기에는 일반적으로 크롤링 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Raffel et al.</span>, <a class="ltx_ref" href="#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> 또는 합성 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Polu and Sutskever</span>, <a class="ltx_ref" href="#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Gunasekar et al.</span>, <a class="ltx_ref" href="#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, de-duplication <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lee et al.</span>, <a class="ltx_ref" href="#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Kandpal et al.</span>, <a class="ltx_ref" href="#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Tirumala et al.</span>, <a class="ltx_ref" href="#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, 필터링 및 선택 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Albalak et al.</span>, <a class="ltx_ref" href="#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, 데이터 구성 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xie et al.</span>, <a class="ltx_ref" href="#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> 및 커리큘럼 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chen et al.</span>, <a class="ltx_ref" href="#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">MA et al.</span>, <a class="ltx_ref" href="#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>를 통한 데이터 수집이 포함된다.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Selection</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">미세 조정을 위한 데이터 선택은 품질 개선 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et al.</span>, <a class="ltx_ref" href="#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">2023d</span></a>)</cite>, 다양성 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Liu et al.</span>, <a class="ltx_ref" href="#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, 분포 매칭 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et al.</span>, <a class="ltx_ref" href="#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">2023e</span></a>; <span class="ltx_text" style="font-size:90%;">Xia et al.</span>, <a class="ltx_ref" href="#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Ni et al.</span>, <a class="ltx_ref" href="#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>를 중심으로 광범위하게 연구되어 왔다. 사전 학습을 위해 휴리스틱 기반(<em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.1.1">e.g.,</em> language and item count filtering), 분류기 기반 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Brown et al.</span>, <a class="ltx_ref" href="#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, perplexity 기반 접근법 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wenzek et al.</span>, <a class="ltx_ref" href="#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>를 포함하여 다양한 경량 필터가 활용된다. 예를 들어 대규모 공개 RedPajama-Data-v2 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Computer</span>, <a class="ltx_ref" href="#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>는 데이터 필터링 및 재가중화를 위해 40개 이상의 품질 지표를 활용한다. 그럼에도 불구하고 블록리스트 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Raffel et al.</span>, <a class="ltx_ref" href="#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>와 Safety API 필터링 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Welbl et al.</span>, <a class="ltx_ref" href="#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>와 같은 엄격한 필터링은 평가 손실을 해치거나 bias <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dodge et al.</span>, <a class="ltx_ref" href="#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>를 유발하는 것으로 나타났다. 우리가 아는 한, 우리는 가장 근본적인 세분성에서 데이터 품질과 정보 밀도를 향상시키는 것을 목표로 하는 토큰 수준 데이터 선택을 처음으로 탐구한다.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Language Model Training Dynamics</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">언어 모델의 훈련 역학을 조사하는 것은 훈련 과정 전반에 걸쳐 그들의 행동을 이해하는 데 필수적이다. 본 연구는 내적 표상 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Saphra and Lopez</span>, <a class="ltx_ref" href="#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, 언어적 지식의 습득 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Choshen et al.</span>, <a class="ltx_ref" href="#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Liu et al.</span>, <a class="ltx_ref" href="#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, 그로킹 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Power et al.</span>, <a class="ltx_ref" href="#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> 현상을 연구한다. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xia et al.</span> (<a class="ltx_ref" href="#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>에 의한 분석은 다양한 크기의 모델에서 토큰 수준의 훈련 궤적을 조사하는 우리와 가장 관련이 있다. 그러나 우리의 연구 결과는 <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xia et al.</span> (<a class="ltx_ref" href="#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>의 연구 결과와 다르며, 이는 복잡성의 변화가 거의 없는 토큰이 "이미 학습되었다"고 가정한다. 우리는 수렴에 저항하는 "쉬운 토큰"과 "하드 토큰"을 포함한 토큰 패턴의 스펙트럼을 식별한다. 이를 인식하여 영향력 있는 토큰을 대상으로 하여 학습 과정을 최적화하는 선택적 언어 모델링 방법을 제안한다.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scaling Laws</h4>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS0.SSS0.Px4.p1.1">스케일링 법칙은 매개변수 수, 데이터 크기 및 계산과 같은 요인이 언어 모델 성능 및 행동에 미치는 영향을 발견하는 데 도움이 됩니다. 이들 연구는 보통 멱법칙 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et al.</span>, <a class="ltx_ref" href="#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Hernandez et al.</span>, <a class="ltx_ref" href="#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, 최적 자원 할당 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hoffmann et al.</span>, <a class="ltx_ref" href="#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, 다운스트림 태스크 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wei et al.</span>, <a class="ltx_ref" href="#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">2022b</span></a>; <span class="ltx_text" style="font-size:90%;">Isik et al.</span>, <a class="ltx_ref" href="#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Gadre et al.</span>, <a class="ltx_ref" href="#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, 아키텍처 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tay et al.</span>, <a class="ltx_ref" href="#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, 암기 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tirumala et al.</span>, <a class="ltx_ref" href="#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Carlini et al.</span>, <a class="ltx_ref" href="#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Henighan et al.</span>, <a class="ltx_ref" href="#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Biderman et al.</span>, <a class="ltx_ref" href="#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, 반복 데이터 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hernandez et al.</span>, <a class="ltx_ref" href="#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Muennighoff et al.</span>, <a class="ltx_ref" href="#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Xue et al.</span>, <a class="ltx_ref" href="#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>를 통해 예측 가능한 스케일링에 초점을 맞춘다. 모델 성능에 관한 대부분의 스케일링 법칙은 모든 트레이닝 토큰에 대한 상호-엔토리 손실을 연구하는 반면, 우리는 원하는 분포의 토큰 손실에 초점을 맞춘다.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Future Work</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Generalization</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">수학 연속 사전 훈련에서 <a class="ltx_ref ltx_refmacro_autoref" href="#S3.F6" title="Figure 6 ‣ 3.3 General Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>에 묘사된 바와 같이 SLM만을 사용한 훈련은 참조 모델이 초점을 맞춘 도메인으로 빠르게 수렴하고 선택되지 않은 토큰의 손실이 크게 증가한다. 증가된 손실에서 편향과 같은 부작용이 아직 관찰되지 않았지만 텍스트 및 코드에 대한 일반적인 사전 훈련 손실은 <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Ouyang et al.</span> (<a class="ltx_ref" href="#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> 및 <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Azerbayev et al.</span> (<a class="ltx_ref" href="#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>에서 제안한 바와 같이 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Goodhart and Goodhart</span>, <a class="ltx_ref" href="#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">1984</span></a>)</cite>에 과적합하는 것을 방지할 수 있다. 또한, 향후 노력은 참조 모델의 말뭉치 범위를 넓히고 DeepSpeedMath <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et al.</span>, <a class="ltx_ref" href="#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">2024a</span></a>)</cite>로 예시된 바와 같이 사전 훈련 데이터 크기를 확대할 수 있다.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scalability</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">Due to budget constraints, we have only verified the effectiveness of our method on smaller models (&lt;=7B parameters) and smaller datasets (&lt;100B tokens). Smaller models benefit significantly from removing the loss of irrelevant tokens and focusing on important ones. However, it’s possible that very large models trained on extensive corpora may naturally develop this inductive bias to compress useful data (<em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px2.p1.1.1">i.e.,</em> compressing everything), although it may sounds inefficient for now. Therefore, future works should study whether this selective language modeling technique can scale to very large models and data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et al.</span>, <a class="ltx_ref" href="#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>.</p>예산 제약으로 인해 우리는 더 작은 모델(<=7B 매개변수)과 더 작은 데이터 세트(<100B 토큰)에 대한 방법의 효과만을 검증했다. 더 작은 모델은 관련 없는 토큰의 손실을 제거하고 중요한 토큰에 집중함으로써 상당한 이점을 얻을 수 있습니다. 그러나 광범위한 말뭉치에 대해 훈련된 매우 큰 모델이 유용한 데이터를 압축하기 위해 이러한 유도성 편향을 자연적으로 개발할 수 있다(<em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px2.p1.1.1">i.e.,</em> 압축 모든 것). 따라서 향후 연구에서는 이러한 선택적 언어 모델링 기법이 매우 큰 모델과 데이터 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et al.</span>, <a class="ltx_ref" href="#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>로 확장할 수 있는지 연구해야 한다.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Is training a reference model necessary?</h4>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">토큰 점수를 매기기 위해서는 고품질 참조 모델이 필요합니다. 이것은 소량의 고품질 데이터로 훈련된 기본 모델 또는 성능이 뛰어난 오픈 소스 모델일 수 있다. 사실, 참조 모델에서 입력 로그프로브 또는 복잡성만 필요하기 때문에 더 강력한 독점 모델 API를 사용할 수도 있다. 토큰을 입력하고 API에서 반환한 입력의 로그 확률을 참조 점수로 사용할 수 있습니다. 우리는 이것을 미래의 일을 위해 남겨둔다.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">How to improve upon SLM?</h4>

<div id="S5.SS0.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.1">SLM의 많은 자연적인 확장들이 있는데, <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px4.p1.1.1">e.g.,</em> reweighting tokens은 robustness를 향상시킬 수 있다; reinforcement learning과 함께 프리트레이닝을 가이드하기 위해 보상 모델로 참조 모델을 사용한다; Overfitting을 줄이기 위해 다수의 참조 모델을 채택한다; 지속적인 개선을 위한 토큰 레벨 커리큘럼 학습 및 반복 전략을 설계한다, <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px4.p1.1.2">etc</em>.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Expanding the Use of SLM</h4>

<div id="S5.SS0.SSS0.Px5.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS0.SSS0.Px5.p1.1">SLM은 많은 SFT 데이터 세트의 노이즈 및 분포 불일치를 해결하기 위해 감독 미세 조정으로 확장될 수 있다. 다른 잠재적인 응용 프로그램은 정렬, <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px5.p1.1.1">e.g.,</em> 도움성, 진실성 및 무해성을 강조하기 위해 참조 모델을 훈련함으로써 사전 훈련 단계 동안 기본적으로 정렬된 기본 모델을 얻을 수 있다.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Kaplan et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Scaling laws for neural language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2001.08361</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Brown et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">, 33:1877–1901, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.4.4.1" class="ltx_text" style="font-size:90%;">OpenAI [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">Gpt-4 technical report, 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Team et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M Dai, Anja Hauth, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Gemini: a family of highly capable multimodal models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.11805</em><span id="bib.bib4.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Wenzek et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Ccnet: Extracting high quality monolingual datasets from web crawl data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.00359</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Welbl et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa&nbsp;Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Challenges in detoxifying language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Findings of the Association for Computational Linguistics: EMNLP 2021</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pages 2447–2469, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Muennighoff et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le&nbsp;Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin&nbsp;A Raffel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Scaling data-constrained language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib7.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Dodge et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Documenting large webtext corpora: A case study on the colossal clean crawled corpus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em><span id="bib.bib8.11.3" class="ltx_text" style="font-size:90%;">, pages 1286–1305, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Longpre et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">A pretrainer’s guide to training data: Measuring the effects of data age, domain coverage, quality, &amp; toxicity.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.13169</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Tay et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Yi&nbsp;Tay, Mostafa Dehghani, Samira Abnar, Hyung&nbsp;Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh&nbsp;Q Tran, Dani Yogatama, and Donald Metzler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Scaling laws vs model architectures: How does inductive bias influence scaling?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.10551</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Wettig et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Should you mask 15% in masked language modeling?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.9.1" class="ltx_text" style="font-size:90%;">In Andreas Vlachos and Isabelle Augenstein, editors, </span><em id="bib.bib11.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em><span id="bib.bib11.11.3" class="ltx_text" style="font-size:90%;">, pages 2985–3000, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.18653/v1/2023.eacl-main.217</span><span id="bib.bib11.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://aclanthology.org/2023.eacl-main.217" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://aclanthology.org/2023.eacl-main.217</a><span id="bib.bib11.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.4.4.1" class="ltx_text" style="font-size:90%;">Hüllermeier and Waegeman [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text" style="font-size:90%;">
Eyke Hüllermeier and Willem Waegeman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib12.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Machine learning</em><span id="bib.bib12.9.2" class="ltx_text" style="font-size:90%;">, 110(3):457–506, 2021.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Yu et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Longhui Yu, Weisen Jiang, Han Shi, YU&nbsp;Jincheng, Zhengying Liu, Yu&nbsp;Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Metamath: Bootstrap your own mathematical questions for large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib13.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Huang et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Key-point-driven data synthesis with its enhancement on mathematical reasoning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2403.02333</em><span id="bib.bib14.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Yue et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Xiang Yue, Xingwei Qu, Ge&nbsp;Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu&nbsp;Su, and Wenhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Mammoth: Building math generalist models through hybrid instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Ni et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Exploring the mystery of influential data for mathematical reasoning, 2024.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Ivison et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah&nbsp;A Smith, Iz&nbsp;Beltagy, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Camels in a changing climate: Enhancing lm adaptation with tulu 2.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2311.10702</em><span id="bib.bib17.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.4.4.1" class="ltx_text" style="font-size:90%;">Teknium [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">
Teknium.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://huggingface.co/datasets/teknium/OpenHermes-2.5" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://huggingface.co/datasets/teknium/OpenHermes-2.5</a><span id="bib.bib18.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Lightman et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Let’s verify step by step.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.20050</em><span id="bib.bib19.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Paster et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Keiran Paster, Marco&nbsp;Dos Santos, Zhangir Azerbayev, and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Openwebmath: An open dataset of high-quality mathematical web text, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Daria et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Soboleva Daria, Al-Khateeb Faisal, Myers Robert Steeves&nbsp;Jacob R, Hestness Joel, and Dey Nolan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">SlimPajama: A 627B token cleaned and deduplicated version of RedPajama.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama</a><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.10.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://huggingface.co/datasets/cerebras/SlimPajama-627B" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://huggingface.co/datasets/cerebras/SlimPajama-627B</a><span id="bib.bib21.11.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Li et&nbsp;al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Raymond Li, Loubna&nbsp;Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry&nbsp;Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh&nbsp;Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra&nbsp;Murthy V, Jason Stillerman, Siva&nbsp;Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn&nbsp;Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos&nbsp;Muñoz Ferrandis, Sean Hughes, Thomas
Wolf, Arjun Guha, Leandro von Werra, and Harm de&nbsp;Vries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Starcoder: may the source be with you!
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, abs/2305.06161, 2023a.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Tinyllama: An open-source small language model, 2024.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Jiang et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Mistral 7b.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.06825</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Team et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir&nbsp;Sanjay Kale, Juliette Love, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Gemma: Open models based on gemini research and technology.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2403.08295</em><span id="bib.bib25.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Bai et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Qwen technical report.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2309.16609</em><span id="bib.bib26.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Li et&nbsp;al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie&nbsp;Del Giorno, Suriya Gunasekar, and Yin&nbsp;Tat Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Textbooks are all you need ii: phi-1.5 technical report, 2023b.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.4.4.1" class="ltx_text" style="font-size:90%;">DeepSeek-AI [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text" style="font-size:90%;">
DeepSeek-AI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">Deepseek llm: Scaling open-source language models with longtermism.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.02954</em><span id="bib.bib28.9.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.10.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://github.com/deepseek-ai/DeepSeek-LLM" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/deepseek-ai/DeepSeek-LLM</a><span id="bib.bib28.11.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Shao et&nbsp;al. [2024a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y.&nbsp;Wu, and Daya Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024a.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/2402.03300" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/2402.03300</a><span id="bib.bib29.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Roziere et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing&nbsp;Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Code llama: Open foundation models for code.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib30.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.12950</em><span id="bib.bib30.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Lewkowycz et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Solving quantitative reasoning problems with language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib31.10.2" class="ltx_text" style="font-size:90%;">, 35:3843–3857, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Azerbayev et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco&nbsp;Dos Santos, Stephen McAleer, Albert&nbsp;Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Llemma: An open language model for mathematics.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.10631</em><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Ying et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Internlm-math: Open math large language models toward verifiable reasoning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib33.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.06332</em><span id="bib.bib33.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Gou et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Tora: A tool-integrated reasoning agent for mathematical problem solving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib34.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib34.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Kwon et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Efficient memory management for large language model serving with pagedattention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Wei et&nbsp;al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed&nbsp;Chi, Quoc&nbsp;V Le, Denny Zhou, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Chain-of-thought prompting elicits reasoning in large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib36.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib36.11.3" class="ltx_text" style="font-size:90%;">, volume&nbsp;35, pages 24824–24837, 2022a.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Shao et&nbsp;al. [2024b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK&nbsp;Li, Y&nbsp;Wu, and Daya Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib37.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.03300</em><span id="bib.bib37.10.2" class="ltx_text" style="font-size:90%;">, 2024b.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Gadre et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Samir&nbsp;Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros&nbsp;G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Language models scale reliably with over-training and on downstream tasks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib38.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Preprint</em><span id="bib.bib38.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Nakkiran et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Deep double descent: Where bigger models and more data hurt.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib39.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Statistical Mechanics: Theory and Experiment</em><span id="bib.bib39.10.2" class="ltx_text" style="font-size:90%;">, 2021(12):124003, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Devlin et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">BERT: pre-training of deep bidirectional transformers for language understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NAACL-HLT (1)</em><span id="bib.bib40.11.3" class="ltx_text" style="font-size:90%;">, pages 4171–4186. Association for Computational Linguistics, 2019.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Liu et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Roberta: A robustly optimized BERT pretraining approach.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib41.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib41.10.2" class="ltx_text" style="font-size:90%;">, abs/1907.11692, 2019.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Li et&nbsp;al. [2023c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Xiang&nbsp;Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Contrastive decoding: Open-ended text generation as optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACL (1)</em><span id="bib.bib42.11.3" class="ltx_text" style="font-size:90%;">, pages 12286–12312. Association for Computational Linguistics, 2023c.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Wan et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Knowledge fusion of large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://openreview.net/forum?id=jiDsk12qcz" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://openreview.net/forum?id=jiDsk12qcz</a><span id="bib.bib43.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Fu et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Specializing smaller language models towards multi-step reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib44.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib44.11.3" class="ltx_text" style="font-size:90%;">, pages 10421–10430. PMLR, 2023.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Raffel et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Exploring the limits of transfer learning with a unified text-to-text transformer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib45.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of machine learning research</em><span id="bib.bib45.10.2" class="ltx_text" style="font-size:90%;">, 21(140):1–67, 2020.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.4.4.1" class="ltx_text" style="font-size:90%;">Polu and Sutskever [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.6.1" class="ltx_text" style="font-size:90%;">
Stanislas Polu and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">Generative language modeling for automated theorem proving.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib46.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.03393</em><span id="bib.bib46.9.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Gunasekar et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Suriya Gunasekar, Yi&nbsp;Zhang, Jyoti Aneja, Caio César&nbsp;Teodoro Mendes, Allie Del&nbsp;Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de&nbsp;Rosa, Olli Saarikivi, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Textbooks are all you need.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib47.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.11644</em><span id="bib.bib47.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Lee et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Deduplicating training data makes language models better.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.06499</em><span id="bib.bib48.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Kandpal et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Nikhil Kandpal, Eric Wallace, and Colin Raffel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Deduplicating training data mitigates privacy risks in language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib49.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib49.11.3" class="ltx_text" style="font-size:90%;">, pages 10697–10707. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Tirumala et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">D4: Improving llm pretraining via document de-duplication and diversification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib50.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib50.11.3" class="ltx_text" style="font-size:90%;">, volume&nbsp;36, 2023.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Albalak et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Alon Albalak, Yanai Elazar, Sang&nbsp;Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William&nbsp;Yang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">A survey on data selection for language models, 2024.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Xie et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Sang&nbsp;Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy&nbsp;S Liang, Quoc&nbsp;V Le, Tengyu Ma, and Adams&nbsp;Wei Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Doremi: Optimizing data mixtures speeds up language model pretraining.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib52.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib52.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Chen et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce&nbsp;Zhang, Frederic Sala, and Christopher Ré.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">Skill-it! a data-driven skills framework for understanding and training language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib53.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib53.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">MA et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu&nbsp;Jiang, Changjian Wang, and Shanshan Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">At which training stage does code data help LLMs reasoning?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib54.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span id="bib.bib54.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://openreview.net/forum?id=KIPJKST4gw" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://openreview.net/forum?id=KIPJKST4gw</a><span id="bib.bib54.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Li et&nbsp;al. [2023d]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib55.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.12032</em><span id="bib.bib55.10.2" class="ltx_text" style="font-size:90%;">, 2023d.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text" style="font-size:90%;">Liu et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib56.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib56.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Li et&nbsp;al. [2023e]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">One shot learning as instruction data prospector for large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib57.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.10302</em><span id="bib.bib57.10.2" class="ltx_text" style="font-size:90%;">, 2023e.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">Xia et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Less: Selecting influential data for targeted instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib58.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.04333</em><span id="bib.bib58.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.4.4.1" class="ltx_text" style="font-size:90%;">Computer [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.6.1" class="ltx_text" style="font-size:90%;">
Together Computer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">Redpajama: an open dataset for training large language models, 10 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://github.com/togethercomputer/RedPajama-Data" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/togethercomputer/RedPajama-Data</a><span id="bib.bib59.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.4.4.1" class="ltx_text" style="font-size:90%;">Saphra and Lopez [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.6.1" class="ltx_text" style="font-size:90%;">
Naomi Saphra and Adam Lopez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.7.1" class="ltx_text" style="font-size:90%;">Understanding learning dynamics of language models with svcca.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib60.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.00225</em><span id="bib.bib60.9.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.5.5.1" class="ltx_text" style="font-size:90%;">Choshen et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text" style="font-size:90%;">
Leshem Choshen, Guy Hacohen, Daphna Weinshall, and Omri Abend.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.8.1" class="ltx_text" style="font-size:90%;">The grammar-learning trajectories of neural language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib61.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2109.06096</em><span id="bib.bib61.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.5.5.1" class="ltx_text" style="font-size:90%;">Liu et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.7.1" class="ltx_text" style="font-size:90%;">
Leo&nbsp;Z Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah&nbsp;A Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.8.1" class="ltx_text" style="font-size:90%;">Probing across time: What does roberta know and when?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib62.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2104.07885</em><span id="bib.bib62.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.5.5.1" class="ltx_text" style="font-size:90%;">Power et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.7.1" class="ltx_text" style="font-size:90%;">
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.8.1" class="ltx_text" style="font-size:90%;">Grokking: Generalization beyond overfitting on small algorithmic datasets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib63.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.02177</em><span id="bib.bib63.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.5.5.1" class="ltx_text" style="font-size:90%;">Xia et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.7.1" class="ltx_text" style="font-size:90%;">
Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi&nbsp;Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.8.1" class="ltx_text" style="font-size:90%;">Training trajectories of language models across scales.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib64.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.09803</em><span id="bib.bib64.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.5.5.1" class="ltx_text" style="font-size:90%;">Hernandez et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.7.1" class="ltx_text" style="font-size:90%;">
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.8.1" class="ltx_text" style="font-size:90%;">Scaling laws for transfer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib65.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2102.01293</em><span id="bib.bib65.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.5.5.1" class="ltx_text" style="font-size:90%;">Hoffmann et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.7.1" class="ltx_text" style="font-size:90%;">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.8.1" class="ltx_text" style="font-size:90%;">Training compute-optimal large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib66.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.15556</em><span id="bib.bib66.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.5.5.1" class="ltx_text" style="font-size:90%;">Wei et&nbsp;al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.7.1" class="ltx_text" style="font-size:90%;">
Jason Wei, Yi&nbsp;Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.8.1" class="ltx_text" style="font-size:90%;">Emergent abilities of large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib67.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.07682</em><span id="bib.bib67.10.2" class="ltx_text" style="font-size:90%;">, 2022b.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.5.5.1" class="ltx_text" style="font-size:90%;">Isik et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.7.1" class="ltx_text" style="font-size:90%;">
Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.8.1" class="ltx_text" style="font-size:90%;">Scaling laws for downstream task performance of large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib68.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.04177</em><span id="bib.bib68.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.5.5.1" class="ltx_text" style="font-size:90%;">Tirumala et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.7.1" class="ltx_text" style="font-size:90%;">
Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.8.1" class="ltx_text" style="font-size:90%;">Memorization without overfitting: Analyzing the training dynamics of large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib69.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib69.10.2" class="ltx_text" style="font-size:90%;">, 35:38274–38290, 2022.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.5.5.1" class="ltx_text" style="font-size:90%;">Carlini et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.7.1" class="ltx_text" style="font-size:90%;">
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.8.1" class="ltx_text" style="font-size:90%;">Quantifying memorization across neural language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib70.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2202.07646</em><span id="bib.bib70.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.5.5.1" class="ltx_text" style="font-size:90%;">Henighan et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.7.1" class="ltx_text" style="font-size:90%;">
Tom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort, Nicholas Schiefer, and Christopher Olah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.8.1" class="ltx_text" style="font-size:90%;">Superposition, memorization, and double descent.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib71.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Transformer Circuits Thread</em><span id="bib.bib71.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.5.5.1" class="ltx_text" style="font-size:90%;">Biderman et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.7.1" class="ltx_text" style="font-size:90%;">
Stella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.8.1" class="ltx_text" style="font-size:90%;">Emergent and predictable memorization in large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib72.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib72.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.5.5.1" class="ltx_text" style="font-size:90%;">Hernandez et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.7.1" class="ltx_text" style="font-size:90%;">
Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.8.1" class="ltx_text" style="font-size:90%;">Scaling laws and interpretability of learning from repeated data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib73.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.10487</em><span id="bib.bib73.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.5.5.1" class="ltx_text" style="font-size:90%;">Xue et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.7.1" class="ltx_text" style="font-size:90%;">
Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.8.1" class="ltx_text" style="font-size:90%;">To repeat or not to repeat: Insights from scaling llm under token-crisis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib74.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib74.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.4.4.1" class="ltx_text" style="font-size:90%;">Goodhart and Goodhart [1984]</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.6.1" class="ltx_text" style="font-size:90%;">
Charles&nbsp;AE Goodhart and CAE Goodhart.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib75.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Problems of monetary management: the UK experience</em><span id="bib.bib75.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.9.1" class="ltx_text" style="font-size:90%;">Springer, 1984.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.5.5.1" class="ltx_text" style="font-size:90%;">Ouyang et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.7.1" class="ltx_text" style="font-size:90%;">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.8.1" class="ltx_text" style="font-size:90%;">Training language models to follow instructions with human feedback.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib76.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib76.10.2" class="ltx_text" style="font-size:90%;">, 35:27730–27744, 2022.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.5.5.1" class="ltx_text" style="font-size:90%;">Cobbe et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.7.1" class="ltx_text" style="font-size:90%;">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.8.1" class="ltx_text" style="font-size:90%;">Training verifiers to solve math word problems, 2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/2110.14168" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/2110.14168</a><span id="bib.bib77.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.5.5.1" class="ltx_text" style="font-size:90%;">Hendrycks et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.7.1" class="ltx_text" style="font-size:90%;">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.8.1" class="ltx_text" style="font-size:90%;">Measuring mathematical problem solving with the math dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib78.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib78.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.5.5.1" class="ltx_text" style="font-size:90%;">Gao et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.7.1" class="ltx_text" style="font-size:90%;">
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.8.1" class="ltx_text" style="font-size:90%;">Pal: Program-aided language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib79.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2211.10435</em><span id="bib.bib79.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib80.5.5.1" class="ltx_text" style="font-size:90%;">Patel et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib80.7.1" class="ltx_text" style="font-size:90%;">
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.8.1" class="ltx_text" style="font-size:90%;">Are NLP models really able to solve simple math word problems?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib80.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em><span id="bib.bib80.11.3" class="ltx_text" style="font-size:90%;">, pages 2080–2094, Online, June 2021. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.18653/v1/2021.naacl-main.168</span><span id="bib.bib80.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://aclanthology.org/2021.naacl-main.168" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://aclanthology.org/2021.naacl-main.168</a><span id="bib.bib80.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib81.5.5.1" class="ltx_text" style="font-size:90%;">Miao et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib81.7.1" class="ltx_text" style="font-size:90%;">
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.8.1" class="ltx_text" style="font-size:90%;">A diverse corpus for evaluating and developing English math word problem solvers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib81.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em><span id="bib.bib81.11.3" class="ltx_text" style="font-size:90%;">, pages 975–984, Online, July 2020. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.18653/v1/2020.acl-main.92</span><span id="bib.bib81.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://aclanthology.org/2020.acl-main.92" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://aclanthology.org/2020.acl-main.92</a><span id="bib.bib81.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib82.5.5.1" class="ltx_text" style="font-size:90%;">Koncel-Kedziorski et&nbsp;al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib82.7.1" class="ltx_text" style="font-size:90%;">
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.8.1" class="ltx_text" style="font-size:90%;">MAWPS: A math word problem repository.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib82.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em><span id="bib.bib82.11.3" class="ltx_text" style="font-size:90%;">, pages 1152–1157, San Diego, California, June 2016. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.18653/v1/N16-1136</span><span id="bib.bib82.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://aclanthology.org/N16-1136" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://aclanthology.org/N16-1136</a><span id="bib.bib82.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib83.5.5.1" class="ltx_text" style="font-size:90%;">Lu et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib83.7.1" class="ltx_text" style="font-size:90%;">
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying&nbsp;Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.8.1" class="ltx_text" style="font-size:90%;">Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib83.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Eleventh International Conference on Learning Representations</em><span id="bib.bib83.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://openreview.net/forum?id=DHyHRBwJUTN" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://openreview.net/forum?id=DHyHRBwJUTN</a><span id="bib.bib83.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib84.5.5.1" class="ltx_text" style="font-size:90%;">Amini et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib84.7.1" class="ltx_text" style="font-size:90%;">
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.8.1" class="ltx_text" style="font-size:90%;">Mathqa: Towards interpretable math word problem solving with operation-based formalisms.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib84.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.13319</em><span id="bib.bib84.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib85.5.5.1" class="ltx_text" style="font-size:90%;">Hendrycks et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib85.7.1" class="ltx_text" style="font-size:90%;">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.8.1" class="ltx_text" style="font-size:90%;">Measuring massive multitask language understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib85.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.03300</em><span id="bib.bib85.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib86.5.5.1" class="ltx_text" style="font-size:90%;">Gao et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib86.7.1" class="ltx_text" style="font-size:90%;">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le&nbsp;Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.8.1" class="ltx_text" style="font-size:90%;">A framework for few-shot language model evaluation, 12 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://zenodo.org/records/10256836" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://zenodo.org/records/10256836</a><span id="bib.bib86.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib87.5.5.1" class="ltx_text" style="font-size:90%;">Suzgun et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib87.7.1" class="ltx_text" style="font-size:90%;">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V Le, Ed&nbsp;H Chi, Denny Zhou, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.8.1" class="ltx_text" style="font-size:90%;">Challenging big-bench tasks and whether chain-of-thought can solve them.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib87.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.09261</em><span id="bib.bib87.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib88.5.5.1" class="ltx_text" style="font-size:90%;">Zhong et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib88.7.1" class="ltx_text" style="font-size:90%;">
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.8.1" class="ltx_text" style="font-size:90%;">Agieval: A human-centric benchmark for evaluating foundation models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib88.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.06364</em><span id="bib.bib88.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib89.5.5.1" class="ltx_text" style="font-size:90%;">Clark et&nbsp;al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib89.7.1" class="ltx_text" style="font-size:90%;">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.8.1" class="ltx_text" style="font-size:90%;">Think you have solved question answering? try arc, the ai2 reasoning challenge.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib89.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1803.05457</em><span id="bib.bib89.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib90.5.5.1" class="ltx_text" style="font-size:90%;">Clark et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib90.7.1" class="ltx_text" style="font-size:90%;">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.8.1" class="ltx_text" style="font-size:90%;">Boolq: Exploring the surprising difficulty of natural yes/no questions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib90.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.10044</em><span id="bib.bib90.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib91.5.5.1" class="ltx_text" style="font-size:90%;">Bisk et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib91.7.1" class="ltx_text" style="font-size:90%;">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.8.1" class="ltx_text" style="font-size:90%;">Piqa: Reasoning about physical commonsense in natural language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib91.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</em><span id="bib.bib91.11.3" class="ltx_text" style="font-size:90%;">, volume&nbsp;34, pages 7432–7439, 2020.
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib92.5.5.1" class="ltx_text" style="font-size:90%;">Zellers et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib92.7.1" class="ltx_text" style="font-size:90%;">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.8.1" class="ltx_text" style="font-size:90%;">Hellaswag: Can a machine really finish your sentence?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib92.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.07830</em><span id="bib.bib92.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib93.5.5.1" class="ltx_text" style="font-size:90%;">Sakaguchi et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib93.7.1" class="ltx_text" style="font-size:90%;">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.8.1" class="ltx_text" style="font-size:90%;">Winogrande: An adversarial winograd schema challenge at scale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib93.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Communications of the ACM</em><span id="bib.bib93.10.2" class="ltx_text" style="font-size:90%;">, 64(9):99–106, 2021.
</span>
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib94.5.5.1" class="ltx_text" style="font-size:90%;">Mihaylov et&nbsp;al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib94.7.1" class="ltx_text" style="font-size:90%;">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.8.1" class="ltx_text" style="font-size:90%;">Can a suit of armor conduct electricity? a new dataset for open book question answering.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib94.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1809.02789</em><span id="bib.bib94.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib95.5.5.1" class="ltx_text" style="font-size:90%;">Zheng et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib95.7.1" class="ltx_text" style="font-size:90%;">
Qinkai Zheng, Xiao Xia, Xu&nbsp;Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.8.1" class="ltx_text" style="font-size:90%;">Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib95.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em><span id="bib.bib95.11.3" class="ltx_text" style="font-size:90%;">, pages 5673–5684, 2023.
</span>
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib96.5.5.1" class="ltx_text" style="font-size:90%;">Clark et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib96.7.1" class="ltx_text" style="font-size:90%;">
Jonathan&nbsp;H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib96.8.1" class="ltx_text" style="font-size:90%;">Tydi qa: A benchmark for information-seeking question answering in ty pologically di verse languages.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib96.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics</em><span id="bib.bib96.10.2" class="ltx_text" style="font-size:90%;">, 8:454–470, 2020.
</span>
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib97.5.5.1" class="ltx_text" style="font-size:90%;">Austin et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib97.7.1" class="ltx_text" style="font-size:90%;">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib97.8.1" class="ltx_text" style="font-size:90%;">Program synthesis with large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib97.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2108.07732</em><span id="bib.bib97.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib98.5.5.1" class="ltx_text" style="font-size:90%;">Guo et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib98.7.1" class="ltx_text" style="font-size:90%;">
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y&nbsp;Wu, YK&nbsp;Li, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib98.8.1" class="ltx_text" style="font-size:90%;">Deepseek-coder: When the large language model meets programming–the rise of code intelligence.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib98.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.14196</em><span id="bib.bib98.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Author Contributions</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p" id="A1.p1.1">정하오린은 세부 토큰 선정 프로세스를 설계 및 구현하고, 광범위한 예비 실험을 수행했으며, 사전 훈련 및 평가 파이프라인을 개발했으며, 대부분의 사전 훈련 실험 및 분석을 수행하고, 기준선을 구현했으며, 글쓰기에 크게 기여했다. Zhibin Gou는 예비 제안을 제시했고, 토큰 재가중치에 초과 손실을 사용하는 방법을 소개했으며, 컴파일된 고품질 말뭉치, 훈련된 참조 모델, 미세 조정 및 평가 파이프라인을 설정하고, 실험 분석을 설계했으며, 글쓰기에 크게 기여했다. 예윤공은 초기 프로젝트를 제안하고 웨이주 첸과 공동주도로 프로젝트를 진행했으며 실험과 글쓰기에 대한 광범위한 조언과 지침을 제공하고 팀 협업과 자원 관리를 총괄했다. 샤오 류, 옌룽 쉰, 뤄첸 쉬, 첸린, 유주양, 지안 자오, 난두안이 연구 멘토링을 제공하고 프로젝트를 조정했으며 글쓰기에 기여했다.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Analysis and Visualization of Tokens in Pretraining</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>More Details of Four Categories Tokens</h3>

<figure id="A2.F10" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x10.png" id="A2.F10.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="608" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F10.7.1.1" style="font-size:90%;">Figure 10</span>:</span><span class="ltx_text ltx_font_bold" id="A2.F10.8.2" style="font-size:90%;">4개의 카테고리의 토큰을 포함하는 샘플 텍스트. <span class="ltx_text ltx_font_medium" id="A2.F10.8.2.1">그 중 <span class="ltx_text" id="A2.F10.8.2.1.1" style="color:#1E90FF;">blue</span>은 categorie H→L의 토큰을 나타내고, <span class="ltx_text" id="A2.F10.8.2.1.2" style="color:#228B22;">green</span>은 categorie L→L의 토큰을 나타내고, <span class="ltx_text" id="A2.F10.8.2.1.3" style="color:#FAAA00;">yellow</span>은 categorie H→H의 토큰을 나타내고, <span class="ltx_text" id="A2.F10.8.2.1.4" style="color:#FA8072;">red</span>은 categorie L→H의 토큰을 나타낸다. </span></span></figcaption>
</figure>
<div id="A2.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A2.SS1.p1.1">토큰을 H→H, L→H, H→L, L→L의 네 가지 범주로 분류한다. 훈련 과정에서 각 10억 토큰 훈련 데이터에 대한 훈련 후 각 토큰의 손실을 수집했다. 그런 다음 선형 피팅을 사용하고 훈련 과정에서 손실이 감소했는지 여부에 대한 증거로 첫 번째 포인트와 마지막 포인트 간의 손실 차이를 취했다.</p>
</div>
<div id="A2.SS1.p2" class="ltx_para">
<p class="ltx_p" id="A2.SS1.p2.1">구체적으로, 토큰의 손실 <math alttext="(l_{0},l_{1},...,l_{n})" class="ltx_Math" display="inline" id="A2.SS1.p2.1.m1.4"><semantics id="A2.SS1.p2.1.m1.4a"><mrow id="A2.SS1.p2.1.m1.4.4.3" xref="A2.SS1.p2.1.m1.4.4.4.cmml"><mo id="A2.SS1.p2.1.m1.4.4.3.4" stretchy="false" xref="A2.SS1.p2.1.m1.4.4.4.cmml">(</mo><msub id="A2.SS1.p2.1.m1.2.2.1.1" xref="A2.SS1.p2.1.m1.2.2.1.1.cmml"><mi id="A2.SS1.p2.1.m1.2.2.1.1.2" xref="A2.SS1.p2.1.m1.2.2.1.1.2.cmml">l</mi><mn id="A2.SS1.p2.1.m1.2.2.1.1.3" xref="A2.SS1.p2.1.m1.2.2.1.1.3.cmml">0</mn></msub><mo id="A2.SS1.p2.1.m1.4.4.3.5" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="A2.SS1.p2.1.m1.3.3.2.2" xref="A2.SS1.p2.1.m1.3.3.2.2.cmml"><mi id="A2.SS1.p2.1.m1.3.3.2.2.2" xref="A2.SS1.p2.1.m1.3.3.2.2.2.cmml">l</mi><mn id="A2.SS1.p2.1.m1.3.3.2.2.3" xref="A2.SS1.p2.1.m1.3.3.2.2.3.cmml">1</mn></msub><mo id="A2.SS1.p2.1.m1.4.4.3.6" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><mi id="A2.SS1.p2.1.m1.1.1" mathvariant="normal" xref="A2.SS1.p2.1.m1.1.1.cmml">…</mi><mo id="A2.SS1.p2.1.m1.4.4.3.7" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="A2.SS1.p2.1.m1.4.4.3.3" xref="A2.SS1.p2.1.m1.4.4.3.3.cmml"><mi id="A2.SS1.p2.1.m1.4.4.3.3.2" xref="A2.SS1.p2.1.m1.4.4.3.3.2.cmml">l</mi><mi id="A2.SS1.p2.1.m1.4.4.3.3.3" xref="A2.SS1.p2.1.m1.4.4.3.3.3.cmml">n</mi></msub><mo id="A2.SS1.p2.1.m1.4.4.3.8" stretchy="false" xref="A2.SS1.p2.1.m1.4.4.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.1.m1.4b"><vector id="A2.SS1.p2.1.m1.4.4.4.cmml" xref="A2.SS1.p2.1.m1.4.4.3"><apply id="A2.SS1.p2.1.m1.2.2.1.1.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="A2.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1.2">𝑙</ci><cn id="A2.SS1.p2.1.m1.2.2.1.1.3.cmml" type="integer" xref="A2.SS1.p2.1.m1.2.2.1.1.3">0</cn></apply><apply id="A2.SS1.p2.1.m1.3.3.2.2.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.3.3.2.2.1.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="A2.SS1.p2.1.m1.3.3.2.2.2.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2.2">𝑙</ci><cn id="A2.SS1.p2.1.m1.3.3.2.2.3.cmml" type="integer" xref="A2.SS1.p2.1.m1.3.3.2.2.3">1</cn></apply><ci id="A2.SS1.p2.1.m1.1.1.cmml" xref="A2.SS1.p2.1.m1.1.1">…</ci><apply id="A2.SS1.p2.1.m1.4.4.3.3.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.4.4.3.3.1.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3">subscript</csymbol><ci id="A2.SS1.p2.1.m1.4.4.3.3.2.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3.2">𝑙</ci><ci id="A2.SS1.p2.1.m1.4.4.3.3.3.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3.3">𝑛</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.1.m1.4c">(l_{0},l_{1},...,l_{n})</annotation></semantics></math>의 시퀀스를 가지고 있다고 가정하자. 우리의 목표는 각 데이터 포인트 간의 차이의 제곱합과 선형 예측값을 최소화하는 것이다:</p>
</div>
<div id="A2.SS1.p3" class="ltx_para">
<table id="A2.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.E6.m1.3" class="ltx_Math" alttext="f(a,b)=\text{minimize}\sum_{i=0}^{n}(l_{i}-(ax_{i}+b))^{2}," display="block"><semantics id="A2.E6.m1.3a"><mrow id="A2.E6.m1.3.3.1" xref="A2.E6.m1.3.3.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1" xref="A2.E6.m1.3.3.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.3" xref="A2.E6.m1.3.3.1.1.3.cmml"><mi id="A2.E6.m1.3.3.1.1.3.2" xref="A2.E6.m1.3.3.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="A2.E6.m1.3.3.1.1.3.1" xref="A2.E6.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="A2.E6.m1.3.3.1.1.3.3.2" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml"><mo stretchy="false" id="A2.E6.m1.3.3.1.1.3.3.2.1" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">(</mo><mi id="A2.E6.m1.1.1" xref="A2.E6.m1.1.1.cmml">a</mi><mo id="A2.E6.m1.3.3.1.1.3.3.2.2" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">,</mo><mi id="A2.E6.m1.2.2" xref="A2.E6.m1.2.2.cmml">b</mi><mo stretchy="false" id="A2.E6.m1.3.3.1.1.3.3.2.3" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="A2.E6.m1.3.3.1.1.2" xref="A2.E6.m1.3.3.1.1.2.cmml">=</mo><mrow id="A2.E6.m1.3.3.1.1.1" xref="A2.E6.m1.3.3.1.1.1.cmml"><mtext id="A2.E6.m1.3.3.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.3a.cmml">minimize</mtext><mo lspace="0em" rspace="0em" id="A2.E6.m1.3.3.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="A2.E6.m1.3.3.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.cmml"><munderover id="A2.E6.m1.3.3.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="A2.E6.m1.3.3.1.1.1.1.2.2.2" xref="A2.E6.m1.3.3.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.2.2.3" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.2.2.3.2" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="A2.E6.m1.3.3.1.1.1.1.2.2.3.1" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="A2.E6.m1.3.3.1.1.1.1.2.2.3.3" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mi id="A2.E6.m1.3.3.1.1.1.1.2.3" xref="A2.E6.m1.3.3.1.1.1.1.2.3.cmml">n</mi></munderover><msup id="A2.E6.m1.3.3.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml"><msub id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml">l</mi><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">x</mi><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">b</mi></mrow><mo stretchy="false" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="A2.E6.m1.3.3.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><mo id="A2.E6.m1.3.3.1.2" xref="A2.E6.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E6.m1.3b"><apply id="A2.E6.m1.3.3.1.1.cmml" xref="A2.E6.m1.3.3.1"><eq id="A2.E6.m1.3.3.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.2"></eq><apply id="A2.E6.m1.3.3.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.3"><times id="A2.E6.m1.3.3.1.1.3.1.cmml" xref="A2.E6.m1.3.3.1.1.3.1"></times><ci id="A2.E6.m1.3.3.1.1.3.2.cmml" xref="A2.E6.m1.3.3.1.1.3.2">𝑓</ci><interval closure="open" id="A2.E6.m1.3.3.1.1.3.3.1.cmml" xref="A2.E6.m1.3.3.1.1.3.3.2"><ci id="A2.E6.m1.1.1.cmml" xref="A2.E6.m1.1.1">𝑎</ci><ci id="A2.E6.m1.2.2.cmml" xref="A2.E6.m1.2.2">𝑏</ci></interval></apply><apply id="A2.E6.m1.3.3.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1"><times id="A2.E6.m1.3.3.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.2"></times><ci id="A2.E6.m1.3.3.1.1.1.3a.cmml" xref="A2.E6.m1.3.3.1.1.1.3"><mtext id="A2.E6.m1.3.3.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.3">minimize</mtext></ci><apply id="A2.E6.m1.3.3.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1"><apply id="A2.E6.m1.3.3.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2">superscript</csymbol><apply id="A2.E6.m1.3.3.1.1.1.1.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.2.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="A2.E6.m1.3.3.1.1.1.1.2.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.2"></sum><apply id="A2.E6.m1.3.3.1.1.1.1.2.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3"><eq id="A2.E6.m1.3.3.1.1.1.1.2.2.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.1"></eq><ci id="A2.E6.m1.3.3.1.1.1.1.2.2.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="A2.E6.m1.3.3.1.1.1.1.2.2.3.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.3">0</cn></apply></apply><ci id="A2.E6.m1.3.3.1.1.1.1.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.3">𝑛</ci></apply><apply id="A2.E6.m1.3.3.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1">superscript</csymbol><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1"><minus id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2"></minus><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2">𝑙</ci><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1"><plus id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2"><times id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1"></times><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑎</ci><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2">𝑥</ci><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3">𝑏</ci></apply></apply><cn type="integer" id="A2.E6.m1.3.3.1.1.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E6.m1.3c">f(a,b)=\text{minimize}\sum_{i=0}^{n}(l_{i}-(ax_{i}+b))^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="A2.SS1.p4" class="ltx_para">
<p id="A2.SS1.p4.6" class="ltx_p">where <math id="A2.SS1.p4.1.m1.1" class="ltx_Math" alttext="x_{0}=0" display="inline"><semantics id="A2.SS1.p4.1.m1.1a"><mrow id="A2.SS1.p4.1.m1.1.1" xref="A2.SS1.p4.1.m1.1.1.cmml"><msub id="A2.SS1.p4.1.m1.1.1.2" xref="A2.SS1.p4.1.m1.1.1.2.cmml"><mi id="A2.SS1.p4.1.m1.1.1.2.2" xref="A2.SS1.p4.1.m1.1.1.2.2.cmml">x</mi><mn id="A2.SS1.p4.1.m1.1.1.2.3" xref="A2.SS1.p4.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="A2.SS1.p4.1.m1.1.1.1" xref="A2.SS1.p4.1.m1.1.1.1.cmml">=</mo><mn id="A2.SS1.p4.1.m1.1.1.3" xref="A2.SS1.p4.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.1.m1.1b"><apply id="A2.SS1.p4.1.m1.1.1.cmml" xref="A2.SS1.p4.1.m1.1.1"><eq id="A2.SS1.p4.1.m1.1.1.1.cmml" xref="A2.SS1.p4.1.m1.1.1.1"></eq><apply id="A2.SS1.p4.1.m1.1.1.2.cmml" xref="A2.SS1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.1.m1.1.1.2.1.cmml" xref="A2.SS1.p4.1.m1.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.1.m1.1.1.2.2.cmml" xref="A2.SS1.p4.1.m1.1.1.2.2">𝑥</ci><cn type="integer" id="A2.SS1.p4.1.m1.1.1.2.3.cmml" xref="A2.SS1.p4.1.m1.1.1.2.3">0</cn></apply><cn type="integer" id="A2.SS1.p4.1.m1.1.1.3.cmml" xref="A2.SS1.p4.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.1.m1.1c">x_{0}=0</annotation></semantics></math> is the initial checkpoint and <math id="A2.SS1.p4.2.m2.1" class="ltx_Math" alttext="x_{n}=n" display="inline"><semantics id="A2.SS1.p4.2.m2.1a"><mrow id="A2.SS1.p4.2.m2.1.1" xref="A2.SS1.p4.2.m2.1.1.cmml"><msub id="A2.SS1.p4.2.m2.1.1.2" xref="A2.SS1.p4.2.m2.1.1.2.cmml"><mi id="A2.SS1.p4.2.m2.1.1.2.2" xref="A2.SS1.p4.2.m2.1.1.2.2.cmml">x</mi><mi id="A2.SS1.p4.2.m2.1.1.2.3" xref="A2.SS1.p4.2.m2.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p4.2.m2.1.1.1" xref="A2.SS1.p4.2.m2.1.1.1.cmml">=</mo><mi id="A2.SS1.p4.2.m2.1.1.3" xref="A2.SS1.p4.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.2.m2.1b"><apply id="A2.SS1.p4.2.m2.1.1.cmml" xref="A2.SS1.p4.2.m2.1.1"><eq id="A2.SS1.p4.2.m2.1.1.1.cmml" xref="A2.SS1.p4.2.m2.1.1.1"></eq><apply id="A2.SS1.p4.2.m2.1.1.2.cmml" xref="A2.SS1.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.2.m2.1.1.2.1.cmml" xref="A2.SS1.p4.2.m2.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.2.m2.1.1.2.2.cmml" xref="A2.SS1.p4.2.m2.1.1.2.2">𝑥</ci><ci id="A2.SS1.p4.2.m2.1.1.2.3.cmml" xref="A2.SS1.p4.2.m2.1.1.2.3">𝑛</ci></apply><ci id="A2.SS1.p4.2.m2.1.1.3.cmml" xref="A2.SS1.p4.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.2.m2.1c">x_{n}=n</annotation></semantics></math> is the final checkpoint. Substituting these into the fitted equation, we can obtain the Loss values at the start and end after fitting: <math id="A2.SS1.p4.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{\text{start}}=b" display="inline"><semantics id="A2.SS1.p4.3.m3.1a"><mrow id="A2.SS1.p4.3.m3.1.1" xref="A2.SS1.p4.3.m3.1.1.cmml"><msub id="A2.SS1.p4.3.m3.1.1.2" xref="A2.SS1.p4.3.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.3.m3.1.1.2.2" xref="A2.SS1.p4.3.m3.1.1.2.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.3.m3.1.1.2.3" xref="A2.SS1.p4.3.m3.1.1.2.3a.cmml">start</mtext></msub><mo id="A2.SS1.p4.3.m3.1.1.1" xref="A2.SS1.p4.3.m3.1.1.1.cmml">=</mo><mi id="A2.SS1.p4.3.m3.1.1.3" xref="A2.SS1.p4.3.m3.1.1.3.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.3.m3.1b"><apply id="A2.SS1.p4.3.m3.1.1.cmml" xref="A2.SS1.p4.3.m3.1.1"><eq id="A2.SS1.p4.3.m3.1.1.1.cmml" xref="A2.SS1.p4.3.m3.1.1.1"></eq><apply id="A2.SS1.p4.3.m3.1.1.2.cmml" xref="A2.SS1.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.3.m3.1.1.2.1.cmml" xref="A2.SS1.p4.3.m3.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.3.m3.1.1.2.2.cmml" xref="A2.SS1.p4.3.m3.1.1.2.2">ℒ</ci><ci id="A2.SS1.p4.3.m3.1.1.2.3a.cmml" xref="A2.SS1.p4.3.m3.1.1.2.3"><mtext mathsize="70%" id="A2.SS1.p4.3.m3.1.1.2.3.cmml" xref="A2.SS1.p4.3.m3.1.1.2.3">start</mtext></ci></apply><ci id="A2.SS1.p4.3.m3.1.1.3.cmml" xref="A2.SS1.p4.3.m3.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.3.m3.1c">\mathcal{L}_{\text{start}}=b</annotation></semantics></math> and <math id="A2.SS1.p4.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{\text{end}}=an+b" display="inline"><semantics id="A2.SS1.p4.4.m4.1a"><mrow id="A2.SS1.p4.4.m4.1.1" xref="A2.SS1.p4.4.m4.1.1.cmml"><msub id="A2.SS1.p4.4.m4.1.1.2" xref="A2.SS1.p4.4.m4.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.4.m4.1.1.2.2" xref="A2.SS1.p4.4.m4.1.1.2.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.4.m4.1.1.2.3" xref="A2.SS1.p4.4.m4.1.1.2.3a.cmml">end</mtext></msub><mo id="A2.SS1.p4.4.m4.1.1.1" xref="A2.SS1.p4.4.m4.1.1.1.cmml">=</mo><mrow id="A2.SS1.p4.4.m4.1.1.3" xref="A2.SS1.p4.4.m4.1.1.3.cmml"><mrow id="A2.SS1.p4.4.m4.1.1.3.2" xref="A2.SS1.p4.4.m4.1.1.3.2.cmml"><mi id="A2.SS1.p4.4.m4.1.1.3.2.2" xref="A2.SS1.p4.4.m4.1.1.3.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p4.4.m4.1.1.3.2.1" xref="A2.SS1.p4.4.m4.1.1.3.2.1.cmml">​</mo><mi id="A2.SS1.p4.4.m4.1.1.3.2.3" xref="A2.SS1.p4.4.m4.1.1.3.2.3.cmml">n</mi></mrow><mo id="A2.SS1.p4.4.m4.1.1.3.1" xref="A2.SS1.p4.4.m4.1.1.3.1.cmml">+</mo><mi id="A2.SS1.p4.4.m4.1.1.3.3" xref="A2.SS1.p4.4.m4.1.1.3.3.cmml">b</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.4.m4.1b"><apply id="A2.SS1.p4.4.m4.1.1.cmml" xref="A2.SS1.p4.4.m4.1.1"><eq id="A2.SS1.p4.4.m4.1.1.1.cmml" xref="A2.SS1.p4.4.m4.1.1.1"></eq><apply id="A2.SS1.p4.4.m4.1.1.2.cmml" xref="A2.SS1.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.4.m4.1.1.2.1.cmml" xref="A2.SS1.p4.4.m4.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.4.m4.1.1.2.2.cmml" xref="A2.SS1.p4.4.m4.1.1.2.2">ℒ</ci><ci id="A2.SS1.p4.4.m4.1.1.2.3a.cmml" xref="A2.SS1.p4.4.m4.1.1.2.3"><mtext mathsize="70%" id="A2.SS1.p4.4.m4.1.1.2.3.cmml" xref="A2.SS1.p4.4.m4.1.1.2.3">end</mtext></ci></apply><apply id="A2.SS1.p4.4.m4.1.1.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3"><plus id="A2.SS1.p4.4.m4.1.1.3.1.cmml" xref="A2.SS1.p4.4.m4.1.1.3.1"></plus><apply id="A2.SS1.p4.4.m4.1.1.3.2.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2"><times id="A2.SS1.p4.4.m4.1.1.3.2.1.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.1"></times><ci id="A2.SS1.p4.4.m4.1.1.3.2.2.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.2">𝑎</ci><ci id="A2.SS1.p4.4.m4.1.1.3.2.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.3">𝑛</ci></apply><ci id="A2.SS1.p4.4.m4.1.1.3.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.4.m4.1c">\mathcal{L}_{\text{end}}=an+b</annotation></semantics></math>. The change in loss can then be expressed as: <math id="A2.SS1.p4.5.m5.1" class="ltx_Math" alttext="\Delta\mathcal{L}=\mathcal{L}_{\text{end}}-\mathcal{L}_{\text{start}}" display="inline"><semantics id="A2.SS1.p4.5.m5.1a"><mrow id="A2.SS1.p4.5.m5.1.1" xref="A2.SS1.p4.5.m5.1.1.cmml"><mrow id="A2.SS1.p4.5.m5.1.1.2" xref="A2.SS1.p4.5.m5.1.1.2.cmml"><mi mathvariant="normal" id="A2.SS1.p4.5.m5.1.1.2.2" xref="A2.SS1.p4.5.m5.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p4.5.m5.1.1.2.1" xref="A2.SS1.p4.5.m5.1.1.2.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.2.3" xref="A2.SS1.p4.5.m5.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p4.5.m5.1.1.1" xref="A2.SS1.p4.5.m5.1.1.1.cmml">=</mo><mrow id="A2.SS1.p4.5.m5.1.1.3" xref="A2.SS1.p4.5.m5.1.1.3.cmml"><msub id="A2.SS1.p4.5.m5.1.1.3.2" xref="A2.SS1.p4.5.m5.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.3.2.2" xref="A2.SS1.p4.5.m5.1.1.3.2.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.5.m5.1.1.3.2.3" xref="A2.SS1.p4.5.m5.1.1.3.2.3a.cmml">end</mtext></msub><mo id="A2.SS1.p4.5.m5.1.1.3.1" xref="A2.SS1.p4.5.m5.1.1.3.1.cmml">−</mo><msub id="A2.SS1.p4.5.m5.1.1.3.3" xref="A2.SS1.p4.5.m5.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.3.3.2" xref="A2.SS1.p4.5.m5.1.1.3.3.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.5.m5.1.1.3.3.3" xref="A2.SS1.p4.5.m5.1.1.3.3.3a.cmml">start</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.5.m5.1b"><apply id="A2.SS1.p4.5.m5.1.1.cmml" xref="A2.SS1.p4.5.m5.1.1"><eq id="A2.SS1.p4.5.m5.1.1.1.cmml" xref="A2.SS1.p4.5.m5.1.1.1"></eq><apply id="A2.SS1.p4.5.m5.1.1.2.cmml" xref="A2.SS1.p4.5.m5.1.1.2"><times id="A2.SS1.p4.5.m5.1.1.2.1.cmml" xref="A2.SS1.p4.5.m5.1.1.2.1"></times><ci id="A2.SS1.p4.5.m5.1.1.2.2.cmml" xref="A2.SS1.p4.5.m5.1.1.2.2">Δ</ci><ci id="A2.SS1.p4.5.m5.1.1.2.3.cmml" xref="A2.SS1.p4.5.m5.1.1.2.3">ℒ</ci></apply><apply id="A2.SS1.p4.5.m5.1.1.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3"><minus id="A2.SS1.p4.5.m5.1.1.3.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.1"></minus><apply id="A2.SS1.p4.5.m5.1.1.3.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="A2.SS1.p4.5.m5.1.1.3.2.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2">subscript</csymbol><ci id="A2.SS1.p4.5.m5.1.1.3.2.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2.2">ℒ</ci><ci id="A2.SS1.p4.5.m5.1.1.3.2.3a.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2.3"><mtext mathsize="70%" id="A2.SS1.p4.5.m5.1.1.3.2.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2.3">end</mtext></ci></apply><apply id="A2.SS1.p4.5.m5.1.1.3.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="A2.SS1.p4.5.m5.1.1.3.3.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3">subscript</csymbol><ci id="A2.SS1.p4.5.m5.1.1.3.3.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3.2">ℒ</ci><ci id="A2.SS1.p4.5.m5.1.1.3.3.3a.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3.3"><mtext mathsize="70%" id="A2.SS1.p4.5.m5.1.1.3.3.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3.3">start</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.5.m5.1c">\Delta\mathcal{L}=\mathcal{L}_{\text{end}}-\mathcal{L}_{\text{start}}</annotation></semantics></math>. Meanwhile, we represent the average Loss of the last checkpoint as <math id="A2.SS1.p4.6.m6.1" class="ltx_Math" alttext="\mathcal{L}_{\text{mean}}" display="inline"><semantics id="A2.SS1.p4.6.m6.1a"><msub id="A2.SS1.p4.6.m6.1.1" xref="A2.SS1.p4.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.6.m6.1.1.2" xref="A2.SS1.p4.6.m6.1.1.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.6.m6.1.1.3" xref="A2.SS1.p4.6.m6.1.1.3a.cmml">mean</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.6.m6.1b"><apply id="A2.SS1.p4.6.m6.1.1.cmml" xref="A2.SS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="A2.SS1.p4.6.m6.1.1.1.cmml" xref="A2.SS1.p4.6.m6.1.1">subscript</csymbol><ci id="A2.SS1.p4.6.m6.1.1.2.cmml" xref="A2.SS1.p4.6.m6.1.1.2">ℒ</ci><ci id="A2.SS1.p4.6.m6.1.1.3a.cmml" xref="A2.SS1.p4.6.m6.1.1.3"><mtext mathsize="70%" id="A2.SS1.p4.6.m6.1.1.3.cmml" xref="A2.SS1.p4.6.m6.1.1.3">mean</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.6.m6.1c">\mathcal{L}_{\text{mean}}</annotation></semantics></math>.</p>
</div>
<div id="A2.SS1.p5" class="ltx_para">
<p id="A2.SS1.p5.7" class="ltx_p">Next, we can classify the tokens based on <math id="A2.SS1.p5.1.m1.1" class="ltx_Math" alttext="\Delta\mathcal{L}" display="inline"><semantics id="A2.SS1.p5.1.m1.1a"><mrow id="A2.SS1.p5.1.m1.1.1" xref="A2.SS1.p5.1.m1.1.1.cmml"><mi mathvariant="normal" id="A2.SS1.p5.1.m1.1.1.2" xref="A2.SS1.p5.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p5.1.m1.1.1.1" xref="A2.SS1.p5.1.m1.1.1.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.1.m1.1.1.3" xref="A2.SS1.p5.1.m1.1.1.3.cmml">ℒ</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.1.m1.1b"><apply id="A2.SS1.p5.1.m1.1.1.cmml" xref="A2.SS1.p5.1.m1.1.1"><times id="A2.SS1.p5.1.m1.1.1.1.cmml" xref="A2.SS1.p5.1.m1.1.1.1"></times><ci id="A2.SS1.p5.1.m1.1.1.2.cmml" xref="A2.SS1.p5.1.m1.1.1.2">Δ</ci><ci id="A2.SS1.p5.1.m1.1.1.3.cmml" xref="A2.SS1.p5.1.m1.1.1.3">ℒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.1.m1.1c">\Delta\mathcal{L}</annotation></semantics></math> and the <math id="A2.SS1.p5.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\text{mean}}" display="inline"><semantics id="A2.SS1.p5.2.m2.1a"><msub id="A2.SS1.p5.2.m2.1.1" xref="A2.SS1.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.2.m2.1.1.2" xref="A2.SS1.p5.2.m2.1.1.2.cmml">ℒ</mi><mtext id="A2.SS1.p5.2.m2.1.1.3" xref="A2.SS1.p5.2.m2.1.1.3a.cmml">mean</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.2.m2.1b"><apply id="A2.SS1.p5.2.m2.1.1.cmml" xref="A2.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS1.p5.2.m2.1.1.1.cmml" xref="A2.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="A2.SS1.p5.2.m2.1.1.2.cmml" xref="A2.SS1.p5.2.m2.1.1.2">ℒ</ci><ci id="A2.SS1.p5.2.m2.1.1.3a.cmml" xref="A2.SS1.p5.2.m2.1.1.3"><mtext mathsize="70%" id="A2.SS1.p5.2.m2.1.1.3.cmml" xref="A2.SS1.p5.2.m2.1.1.3">mean</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.2.m2.1c">\mathcal{L}_{\text{mean}}</annotation></semantics></math>. We categorize tokens with <math id="A2.SS1.p5.3.m3.1" class="ltx_Math" alttext="\Delta\mathcal{L}<-0.2" display="inline"><semantics id="A2.SS1.p5.3.m3.1a"><mrow id="A2.SS1.p5.3.m3.1.1" xref="A2.SS1.p5.3.m3.1.1.cmml"><mrow id="A2.SS1.p5.3.m3.1.1.2" xref="A2.SS1.p5.3.m3.1.1.2.cmml"><mi mathvariant="normal" id="A2.SS1.p5.3.m3.1.1.2.2" xref="A2.SS1.p5.3.m3.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p5.3.m3.1.1.2.1" xref="A2.SS1.p5.3.m3.1.1.2.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.3.m3.1.1.2.3" xref="A2.SS1.p5.3.m3.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p5.3.m3.1.1.1" xref="A2.SS1.p5.3.m3.1.1.1.cmml">&lt;</mo><mrow id="A2.SS1.p5.3.m3.1.1.3" xref="A2.SS1.p5.3.m3.1.1.3.cmml"><mo id="A2.SS1.p5.3.m3.1.1.3a" xref="A2.SS1.p5.3.m3.1.1.3.cmml">−</mo><mn id="A2.SS1.p5.3.m3.1.1.3.2" xref="A2.SS1.p5.3.m3.1.1.3.2.cmml">0.2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.3.m3.1b"><apply id="A2.SS1.p5.3.m3.1.1.cmml" xref="A2.SS1.p5.3.m3.1.1"><lt id="A2.SS1.p5.3.m3.1.1.1.cmml" xref="A2.SS1.p5.3.m3.1.1.1"></lt><apply id="A2.SS1.p5.3.m3.1.1.2.cmml" xref="A2.SS1.p5.3.m3.1.1.2"><times id="A2.SS1.p5.3.m3.1.1.2.1.cmml" xref="A2.SS1.p5.3.m3.1.1.2.1"></times><ci id="A2.SS1.p5.3.m3.1.1.2.2.cmml" xref="A2.SS1.p5.3.m3.1.1.2.2">Δ</ci><ci id="A2.SS1.p5.3.m3.1.1.2.3.cmml" xref="A2.SS1.p5.3.m3.1.1.2.3">ℒ</ci></apply><apply id="A2.SS1.p5.3.m3.1.1.3.cmml" xref="A2.SS1.p5.3.m3.1.1.3"><minus id="A2.SS1.p5.3.m3.1.1.3.1.cmml" xref="A2.SS1.p5.3.m3.1.1.3"></minus><cn type="float" id="A2.SS1.p5.3.m3.1.1.3.2.cmml" xref="A2.SS1.p5.3.m3.1.1.3.2">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.3.m3.1c">\Delta\mathcal{L}&lt;-0.2</annotation></semantics></math> as H→L (loss decreases from high to low) category tokens, and tokens with <math id="A2.SS1.p5.4.m4.1" class="ltx_Math" alttext="\Delta\mathcal{L}>0.2" display="inline"><semantics id="A2.SS1.p5.4.m4.1a"><mrow id="A2.SS1.p5.4.m4.1.1" xref="A2.SS1.p5.4.m4.1.1.cmml"><mrow id="A2.SS1.p5.4.m4.1.1.2" xref="A2.SS1.p5.4.m4.1.1.2.cmml"><mi mathvariant="normal" id="A2.SS1.p5.4.m4.1.1.2.2" xref="A2.SS1.p5.4.m4.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p5.4.m4.1.1.2.1" xref="A2.SS1.p5.4.m4.1.1.2.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.4.m4.1.1.2.3" xref="A2.SS1.p5.4.m4.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p5.4.m4.1.1.1" xref="A2.SS1.p5.4.m4.1.1.1.cmml">&gt;</mo><mn id="A2.SS1.p5.4.m4.1.1.3" xref="A2.SS1.p5.4.m4.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.4.m4.1b"><apply id="A2.SS1.p5.4.m4.1.1.cmml" xref="A2.SS1.p5.4.m4.1.1"><gt id="A2.SS1.p5.4.m4.1.1.1.cmml" xref="A2.SS1.p5.4.m4.1.1.1"></gt><apply id="A2.SS1.p5.4.m4.1.1.2.cmml" xref="A2.SS1.p5.4.m4.1.1.2"><times id="A2.SS1.p5.4.m4.1.1.2.1.cmml" xref="A2.SS1.p5.4.m4.1.1.2.1"></times><ci id="A2.SS1.p5.4.m4.1.1.2.2.cmml" xref="A2.SS1.p5.4.m4.1.1.2.2">Δ</ci><ci id="A2.SS1.p5.4.m4.1.1.2.3.cmml" xref="A2.SS1.p5.4.m4.1.1.2.3">ℒ</ci></apply><cn type="float" id="A2.SS1.p5.4.m4.1.1.3.cmml" xref="A2.SS1.p5.4.m4.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.4.m4.1c">\Delta\mathcal{L}&gt;0.2</annotation></semantics></math> as L→H (loss increases from low to high) category tokens. If <math id="A2.SS1.p5.5.m5.1" class="ltx_Math" alttext="-0.2\leq\Delta\mathcal{L}\leq 0.2" display="inline"><semantics id="A2.SS1.p5.5.m5.1a"><mrow id="A2.SS1.p5.5.m5.1.1" xref="A2.SS1.p5.5.m5.1.1.cmml"><mrow id="A2.SS1.p5.5.m5.1.1.2" xref="A2.SS1.p5.5.m5.1.1.2.cmml"><mo id="A2.SS1.p5.5.m5.1.1.2a" xref="A2.SS1.p5.5.m5.1.1.2.cmml">−</mo><mn id="A2.SS1.p5.5.m5.1.1.2.2" xref="A2.SS1.p5.5.m5.1.1.2.2.cmml">0.2</mn></mrow><mo id="A2.SS1.p5.5.m5.1.1.3" xref="A2.SS1.p5.5.m5.1.1.3.cmml">≤</mo><mrow id="A2.SS1.p5.5.m5.1.1.4" xref="A2.SS1.p5.5.m5.1.1.4.cmml"><mi mathvariant="normal" id="A2.SS1.p5.5.m5.1.1.4.2" xref="A2.SS1.p5.5.m5.1.1.4.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p5.5.m5.1.1.4.1" xref="A2.SS1.p5.5.m5.1.1.4.1.cmml">​</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.5.m5.1.1.4.3" xref="A2.SS1.p5.5.m5.1.1.4.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p5.5.m5.1.1.5" xref="A2.SS1.p5.5.m5.1.1.5.cmml">≤</mo><mn id="A2.SS1.p5.5.m5.1.1.6" xref="A2.SS1.p5.5.m5.1.1.6.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.5.m5.1b"><apply id="A2.SS1.p5.5.m5.1.1.cmml" xref="A2.SS1.p5.5.m5.1.1"><and id="A2.SS1.p5.5.m5.1.1a.cmml" xref="A2.SS1.p5.5.m5.1.1"></and><apply id="A2.SS1.p5.5.m5.1.1b.cmml" xref="A2.SS1.p5.5.m5.1.1"><leq id="A2.SS1.p5.5.m5.1.1.3.cmml" xref="A2.SS1.p5.5.m5.1.1.3"></leq><apply id="A2.SS1.p5.5.m5.1.1.2.cmml" xref="A2.SS1.p5.5.m5.1.1.2"><minus id="A2.SS1.p5.5.m5.1.1.2.1.cmml" xref="A2.SS1.p5.5.m5.1.1.2"></minus><cn type="float" id="A2.SS1.p5.5.m5.1.1.2.2.cmml" xref="A2.SS1.p5.5.m5.1.1.2.2">0.2</cn></apply><apply id="A2.SS1.p5.5.m5.1.1.4.cmml" xref="A2.SS1.p5.5.m5.1.1.4"><times id="A2.SS1.p5.5.m5.1.1.4.1.cmml" xref="A2.SS1.p5.5.m5.1.1.4.1"></times><ci id="A2.SS1.p5.5.m5.1.1.4.2.cmml" xref="A2.SS1.p5.5.m5.1.1.4.2">Δ</ci><ci id="A2.SS1.p5.5.m5.1.1.4.3.cmml" xref="A2.SS1.p5.5.m5.1.1.4.3">ℒ</ci></apply></apply><apply id="A2.SS1.p5.5.m5.1.1c.cmml" xref="A2.SS1.p5.5.m5.1.1"><leq id="A2.SS1.p5.5.m5.1.1.5.cmml" xref="A2.SS1.p5.5.m5.1.1.5"></leq><share href="#A2.SS1.p5.5.m5.1.1.4.cmml" id="A2.SS1.p5.5.m5.1.1d.cmml" xref="A2.SS1.p5.5.m5.1.1"></share><cn type="float" id="A2.SS1.p5.5.m5.1.1.6.cmml" xref="A2.SS1.p5.5.m5.1.1.6">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.5.m5.1c">-0.2\leq\Delta\mathcal{L}\leq 0.2</annotation></semantics></math> and <math id="A2.SS1.p5.6.m6.1" class="ltx_Math" alttext="l_{n}\leq\mathcal{L}_{\text{mean}}" display="inline"><semantics id="A2.SS1.p5.6.m6.1a"><mrow id="A2.SS1.p5.6.m6.1.1" xref="A2.SS1.p5.6.m6.1.1.cmml"><msub id="A2.SS1.p5.6.m6.1.1.2" xref="A2.SS1.p5.6.m6.1.1.2.cmml"><mi id="A2.SS1.p5.6.m6.1.1.2.2" xref="A2.SS1.p5.6.m6.1.1.2.2.cmml">l</mi><mi id="A2.SS1.p5.6.m6.1.1.2.3" xref="A2.SS1.p5.6.m6.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p5.6.m6.1.1.1" xref="A2.SS1.p5.6.m6.1.1.1.cmml">≤</mo><msub id="A2.SS1.p5.6.m6.1.1.3" xref="A2.SS1.p5.6.m6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.6.m6.1.1.3.2" xref="A2.SS1.p5.6.m6.1.1.3.2.cmml">ℒ</mi><mtext id="A2.SS1.p5.6.m6.1.1.3.3" xref="A2.SS1.p5.6.m6.1.1.3.3a.cmml">mean</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.6.m6.1b"><apply id="A2.SS1.p5.6.m6.1.1.cmml" xref="A2.SS1.p5.6.m6.1.1"><leq id="A2.SS1.p5.6.m6.1.1.1.cmml" xref="A2.SS1.p5.6.m6.1.1.1"></leq><apply id="A2.SS1.p5.6.m6.1.1.2.cmml" xref="A2.SS1.p5.6.m6.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p5.6.m6.1.1.2.1.cmml" xref="A2.SS1.p5.6.m6.1.1.2">subscript</csymbol><ci id="A2.SS1.p5.6.m6.1.1.2.2.cmml" xref="A2.SS1.p5.6.m6.1.1.2.2">𝑙</ci><ci id="A2.SS1.p5.6.m6.1.1.2.3.cmml" xref="A2.SS1.p5.6.m6.1.1.2.3">𝑛</ci></apply><apply id="A2.SS1.p5.6.m6.1.1.3.cmml" xref="A2.SS1.p5.6.m6.1.1.3"><csymbol cd="ambiguous" id="A2.SS1.p5.6.m6.1.1.3.1.cmml" xref="A2.SS1.p5.6.m6.1.1.3">subscript</csymbol><ci id="A2.SS1.p5.6.m6.1.1.3.2.cmml" xref="A2.SS1.p5.6.m6.1.1.3.2">ℒ</ci><ci id="A2.SS1.p5.6.m6.1.1.3.3a.cmml" xref="A2.SS1.p5.6.m6.1.1.3.3"><mtext mathsize="70%" id="A2.SS1.p5.6.m6.1.1.3.3.cmml" xref="A2.SS1.p5.6.m6.1.1.3.3">mean</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.6.m6.1c">l_{n}\leq\mathcal{L}_{\text{mean}}</annotation></semantics></math>, then tokens are classified as L→L (loss remains low); if <math id="A2.SS1.p5.7.m7.1" class="ltx_Math" alttext="l_{n}>\mathcal{L}_{\text{mean}}" display="inline"><semantics id="A2.SS1.p5.7.m7.1a"><mrow id="A2.SS1.p5.7.m7.1.1" xref="A2.SS1.p5.7.m7.1.1.cmml"><msub id="A2.SS1.p5.7.m7.1.1.2" xref="A2.SS1.p5.7.m7.1.1.2.cmml"><mi id="A2.SS1.p5.7.m7.1.1.2.2" xref="A2.SS1.p5.7.m7.1.1.2.2.cmml">l</mi><mi id="A2.SS1.p5.7.m7.1.1.2.3" xref="A2.SS1.p5.7.m7.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p5.7.m7.1.1.1" xref="A2.SS1.p5.7.m7.1.1.1.cmml">&gt;</mo><msub id="A2.SS1.p5.7.m7.1.1.3" xref="A2.SS1.p5.7.m7.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.7.m7.1.1.3.2" xref="A2.SS1.p5.7.m7.1.1.3.2.cmml">ℒ</mi><mtext id="A2.SS1.p5.7.m7.1.1.3.3" xref="A2.SS1.p5.7.m7.1.1.3.3a.cmml">mean</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.7.m7.1b"><apply id="A2.SS1.p5.7.m7.1.1.cmml" xref="A2.SS1.p5.7.m7.1.1"><gt id="A2.SS1.p5.7.m7.1.1.1.cmml" xref="A2.SS1.p5.7.m7.1.1.1"></gt><apply id="A2.SS1.p5.7.m7.1.1.2.cmml" xref="A2.SS1.p5.7.m7.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p5.7.m7.1.1.2.1.cmml" xref="A2.SS1.p5.7.m7.1.1.2">subscript</csymbol><ci id="A2.SS1.p5.7.m7.1.1.2.2.cmml" xref="A2.SS1.p5.7.m7.1.1.2.2">𝑙</ci><ci id="A2.SS1.p5.7.m7.1.1.2.3.cmml" xref="A2.SS1.p5.7.m7.1.1.2.3">𝑛</ci></apply><apply id="A2.SS1.p5.7.m7.1.1.3.cmml" xref="A2.SS1.p5.7.m7.1.1.3"><csymbol cd="ambiguous" id="A2.SS1.p5.7.m7.1.1.3.1.cmml" xref="A2.SS1.p5.7.m7.1.1.3">subscript</csymbol><ci id="A2.SS1.p5.7.m7.1.1.3.2.cmml" xref="A2.SS1.p5.7.m7.1.1.3.2">ℒ</ci><ci id="A2.SS1.p5.7.m7.1.1.3.3a.cmml" xref="A2.SS1.p5.7.m7.1.1.3.3"><mtext mathsize="70%" id="A2.SS1.p5.7.m7.1.1.3.3.cmml" xref="A2.SS1.p5.7.m7.1.1.3.3">mean</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.7.m7.1c">l_{n}&gt;\mathcal{L}_{\text{mean}}</annotation></semantics></math>, they are classified as H→H (loss remains high). In <a href="#A2.F10" title="Figure 10 ‣ B.1 More Details of Four Categories Tokens ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;10</span></a>, we visualize examples of the four categories of tokens in actual text.</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Non-Converging Tokens in Pretrainig</h3>

<figure id="A2.F11" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x11.png" id="A2.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="511" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F11.4.1.1" style="font-size:90%;">Figure 11</span>:</span><span class="ltx_text ltx_font_bold" id="A2.F11.5.2" style="font-size:90%;">An example of abnormal state of token perplexity during pretrainig process. <span class="ltx_text ltx_font_medium" id="A2.F11.5.2.1"> <span class="ltx_text" id="A2.F11.5.2.1.1" style="color:#FF8C00;">orange</span>은 사전 훈련 프로세스 동안 상당한 이상이 있었던 토큰을 나타냅니다. </span></span></figcaption>
</figure>
<div id="A2.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A2.SS2.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="#S2.SS1" title="2.1 Not All Tokens Are Equal: Training Dynamics of Token Loss ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.1</span></a>에서는 훈련 과정에서 소수의 토큰만이 H→L 범주에 속한다고 언급하였다. 나머지 범주인 H→H와 L→L 토큰 중 훈련 시 유의한 변동을 보이는 토큰이 있다. 나아가 H→L 토큰을 효과적으로 학습하지 못하는 경우도 있다. 따라서 분석에서 상당한 변동성과 뚜렷한 손실을 나타내는 이러한 범주에서 해당 토큰을 구체적으로 선택합니다. 훈련 과정에서 비정상적인 행동을 보이는 이러한 토큰을 시각화합니다. <a class="ltx_ref ltx_refmacro_autoref" href="#A2.F11" title="Figure 11 ‣ B.2 Non-Converging Tokens in Pretrainig ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>에 예시된 바와 같이, 우리는 이러한 토큰의 대부분이 다소 혼란스러운 말뭉치에서 비롯된다는 것을 발견한다. 예를 들어, 말뭉치는 사용자 정의 기호, 이해할 수 없는 횡설수설, 시간표 및 서지 참조와 같은 정보의 혼합을 포함할 수 있다. 정규 텍스트의 세그먼트 내에서, 또한 공통 접속사, 단어 접미사 및 구두점 표시의 사용에 변동이 있을 수 있다. 후자는 훈련에 반드시 재앙이 아닐 수 있다; 사실, 그것은 정상적인 발생을 나타낼 수 있다. 그러나 전자로 인한 손실을 효과적으로 완화할 수 있다면 보다 안정적이고 효율적인 모델 학습으로 이어질 수 있다.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Evalution Details</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Math Evalution</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.p1.1">초등부터 대학 수준까지 다양한 어려움, 여러 수학적 영역, 객관식, 개방형 문항을 포함한 다양한 문항 유형을 포괄하여 다양한 수학 추론 벤치마크에 걸쳐 모형에 대한 종합적인 평가를 실시하였다. 우리의 벤치마크로는 GSM8k<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Cobbe et al.</span>, <a class="ltx_ref" href="#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, MATH<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et al.</span>, <a class="ltx_ref" href="#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, GSM-Hard<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gao et al.</span>, <a class="ltx_ref" href="#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, SVAMP<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Patel et al.</span>, <a class="ltx_ref" href="#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, ASDIV<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Miao et al.</span>, <a class="ltx_ref" href="#bib.bib81" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, MAWPS<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Koncel-Kedziorski et al.</span>, <a class="ltx_ref" href="#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>, TabMWP(TAB)<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lu et al.</span>, <a class="ltx_ref" href="#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, MathQA(MQA)<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Amini et al.</span>, <a class="ltx_ref" href="#bib.bib84" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, MMLU-STEM<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et al.</span>, <a class="ltx_ref" href="#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, SAT<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Azerbayev et al.</span>, <a class="ltx_ref" href="#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> 등이 있다.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>General Evalution</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS2.p1.1">일반 도메인 평가에서는 lm-evaluation-harness <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gao et al.</span>, <a class="ltx_ref" href="#bib.bib86" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>와 MMLU <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et al.</span>, <a class="ltx_ref" href="#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, BBH <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Suzgun et al.</span>, <a class="ltx_ref" href="#bib.bib87" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, AGIEval <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et al.</span>, <a class="ltx_ref" href="#bib.bib88" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, ARC-Easy and ARC-Challenge <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et al.</span>, <a class="ltx_ref" href="#bib.bib89" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, BoolQ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et al.</span>, <a class="ltx_ref" href="#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, PIQA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bisk et al.</span>, <a class="ltx_ref" href="#bib.bib91" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, Hellaswag <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zellers et al.</span>, <a class="ltx_ref" href="#bib.bib92" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, WinoGrande <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sakaguchi et al.</span>, <a class="ltx_ref" href="#bib.bib93" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, OpenBookQA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Mihaylov et al.</span>, <a class="ltx_ref" href="#bib.bib94" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>를 따랐다. HumanEval <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zheng et al.</span>, <a class="ltx_ref" href="#bib.bib95" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>와 TydiQA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et al.</span>, <a class="ltx_ref" href="#bib.bib96" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>에 대해 open-instrcut <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ivison et al.</span>, <a class="ltx_ref" href="#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>의 평가 파이프라인을 따르고 HumanEval의 경우 Pass@1과 Pass@10, TydiQA의 경우 F1을 보고한다. MBPP <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Austin et al.</span>, <a class="ltx_ref" href="#bib.bib97" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> 벤치마크의 경우 DeepSeek-Coder <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Guo et al.</span>, <a class="ltx_ref" href="#bib.bib98" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>의 평가 파이프라인을 따르고 Pass@1과 Pass@10을 보고한다.</p>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Relate the Selected Tokens’ Loss to Downstream Task Performance</h2>

<div id="A4.p1" class="ltx_para">
<p class="ltx_p" id="A4.p1.1">이 섹션에서는 선택한 토큰의 손실과 다운스트림 작업의 성능을 상관시키는 세부 정보를 선언합니다. 동시 연구는 다운스트림 태스크 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gadre et al.</span>, <a class="ltx_ref" href="#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>에서 모델의 성능에 대한 스케일링 법칙의 영향을 연구하기 위해 유사한 방법을 탐구했다. 여기에서 우리의 분석은 선택/비선택 토큰에 대한 손실 감소/증가와 다운스트림 태스크에 대한 모델의 성능 사이의 관계를 설명하는 것을 목표로 한다는 점에서 다르다.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p class="ltx_p" id="A4.p2.1">모델의 다운스트림 태스크 성능을 측정하기 위한 표준으로 MATH와 GSM8K의 평균 정확도를 사용한다. <a class="ltx_ref ltx_refmacro_autoref" href="#S3.F7" title="Figure 7 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>의 데이터 포인트 추세를 기반으로 다운스트림 태스크의 평균 정확도와 선택/비선택 토큰 손실 간의 관계를 제안하며,</p>
</div>
<div id="A4.p3" class="ltx_para">
<table id="A4.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A4.E7.m1.3" class="ltx_Math" alttext="Acc(\mathcal{L})=\log(a*\mathcal{L}+c)" display="block"><semantics id="A4.E7.m1.3a"><mrow id="A4.E7.m1.3.3" xref="A4.E7.m1.3.3.cmml"><mrow id="A4.E7.m1.3.3.3" xref="A4.E7.m1.3.3.3.cmml"><mi id="A4.E7.m1.3.3.3.2" xref="A4.E7.m1.3.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="A4.E7.m1.3.3.3.1" xref="A4.E7.m1.3.3.3.1.cmml">​</mo><mi id="A4.E7.m1.3.3.3.3" xref="A4.E7.m1.3.3.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A4.E7.m1.3.3.3.1a" xref="A4.E7.m1.3.3.3.1.cmml">​</mo><mi id="A4.E7.m1.3.3.3.4" xref="A4.E7.m1.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="A4.E7.m1.3.3.3.1b" xref="A4.E7.m1.3.3.3.1.cmml">​</mo><mrow id="A4.E7.m1.3.3.3.5.2" xref="A4.E7.m1.3.3.3.cmml"><mo stretchy="false" id="A4.E7.m1.3.3.3.5.2.1" xref="A4.E7.m1.3.3.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="A4.E7.m1.1.1" xref="A4.E7.m1.1.1.cmml">ℒ</mi><mo stretchy="false" id="A4.E7.m1.3.3.3.5.2.2" xref="A4.E7.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="A4.E7.m1.3.3.2" xref="A4.E7.m1.3.3.2.cmml">=</mo><mrow id="A4.E7.m1.3.3.1.1" xref="A4.E7.m1.3.3.1.2.cmml"><mi id="A4.E7.m1.2.2" xref="A4.E7.m1.2.2.cmml">log</mi><mo id="A4.E7.m1.3.3.1.1a" xref="A4.E7.m1.3.3.1.2.cmml">⁡</mo><mrow id="A4.E7.m1.3.3.1.1.1" xref="A4.E7.m1.3.3.1.2.cmml"><mo stretchy="false" id="A4.E7.m1.3.3.1.1.1.2" xref="A4.E7.m1.3.3.1.2.cmml">(</mo><mrow id="A4.E7.m1.3.3.1.1.1.1" xref="A4.E7.m1.3.3.1.1.1.1.cmml"><mrow id="A4.E7.m1.3.3.1.1.1.1.2" xref="A4.E7.m1.3.3.1.1.1.1.2.cmml"><mi id="A4.E7.m1.3.3.1.1.1.1.2.2" xref="A4.E7.m1.3.3.1.1.1.1.2.2.cmml">a</mi><mo lspace="0.222em" rspace="0.222em" id="A4.E7.m1.3.3.1.1.1.1.2.1" xref="A4.E7.m1.3.3.1.1.1.1.2.1.cmml">∗</mo><mi class="ltx_font_mathcaligraphic" id="A4.E7.m1.3.3.1.1.1.1.2.3" xref="A4.E7.m1.3.3.1.1.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A4.E7.m1.3.3.1.1.1.1.1" xref="A4.E7.m1.3.3.1.1.1.1.1.cmml">+</mo><mi id="A4.E7.m1.3.3.1.1.1.1.3" xref="A4.E7.m1.3.3.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="A4.E7.m1.3.3.1.1.1.3" xref="A4.E7.m1.3.3.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.E7.m1.3b"><apply id="A4.E7.m1.3.3.cmml" xref="A4.E7.m1.3.3"><eq id="A4.E7.m1.3.3.2.cmml" xref="A4.E7.m1.3.3.2"></eq><apply id="A4.E7.m1.3.3.3.cmml" xref="A4.E7.m1.3.3.3"><times id="A4.E7.m1.3.3.3.1.cmml" xref="A4.E7.m1.3.3.3.1"></times><ci id="A4.E7.m1.3.3.3.2.cmml" xref="A4.E7.m1.3.3.3.2">𝐴</ci><ci id="A4.E7.m1.3.3.3.3.cmml" xref="A4.E7.m1.3.3.3.3">𝑐</ci><ci id="A4.E7.m1.3.3.3.4.cmml" xref="A4.E7.m1.3.3.3.4">𝑐</ci><ci id="A4.E7.m1.1.1.cmml" xref="A4.E7.m1.1.1">ℒ</ci></apply><apply id="A4.E7.m1.3.3.1.2.cmml" xref="A4.E7.m1.3.3.1.1"><log id="A4.E7.m1.2.2.cmml" xref="A4.E7.m1.2.2"></log><apply id="A4.E7.m1.3.3.1.1.1.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1"><plus id="A4.E7.m1.3.3.1.1.1.1.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1.1"></plus><apply id="A4.E7.m1.3.3.1.1.1.1.2.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2"><times id="A4.E7.m1.3.3.1.1.1.1.2.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.1"></times><ci id="A4.E7.m1.3.3.1.1.1.1.2.2.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.2">𝑎</ci><ci id="A4.E7.m1.3.3.1.1.1.1.2.3.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.3">ℒ</ci></apply><ci id="A4.E7.m1.3.3.1.1.1.1.3.cmml" xref="A4.E7.m1.3.3.1.1.1.1.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.E7.m1.3c">Acc(\mathcal{L})=\log(a*\mathcal{L}+c)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div id="A4.p4" class="ltx_para">
<p class="ltx_p" id="A4.p4.6">파라미터 <math alttext="a" class="ltx_Math" display="inline" id="A4.p4.1.m1.1"><semantics id="A4.p4.1.m1.1a"><mi id="A4.p4.1.m1.1.1" xref="A4.p4.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="A4.p4.1.m1.1b"><ci id="A4.p4.1.m1.1.1.cmml" xref="A4.p4.1.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.1.m1.1c">a</annotation></semantics></math> 및 <math alttext="c" class="ltx_Math" display="inline" id="A4.p4.2.m2.1"><semantics id="A4.p4.2.m2.1a"><mi id="A4.p4.2.m2.1.1" xref="A4.p4.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="A4.p4.2.m2.1b"><ci id="A4.p4.2.m2.1.1.cmml" xref="A4.p4.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.2.m2.1c">c</annotation></semantics></math>는 데이터로부터 피팅된다. 선택된 토큰의 손실 <math alttext="\mathcal{L}_{s}" class="ltx_Math" display="inline" id="A4.p4.3.m3.1"><semantics id="A4.p4.3.m3.1a"><msub id="A4.p4.3.m3.1.1" xref="A4.p4.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.p4.3.m3.1.1.2" xref="A4.p4.3.m3.1.1.2.cmml">ℒ</mi><mi id="A4.p4.3.m3.1.1.3" xref="A4.p4.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p4.3.m3.1b"><apply id="A4.p4.3.m3.1.1.cmml" xref="A4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="A4.p4.3.m3.1.1.1.cmml" xref="A4.p4.3.m3.1.1">subscript</csymbol><ci id="A4.p4.3.m3.1.1.2.cmml" xref="A4.p4.3.m3.1.1.2">ℒ</ci><ci id="A4.p4.3.m3.1.1.3.cmml" xref="A4.p4.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.3.m3.1c">\mathcal{L}_{s}</annotation></semantics></math>가 피팅에 사용된다면, <math alttext="a&gt;0" class="ltx_Math" display="inline" id="A4.p4.4.m4.1"><semantics id="A4.p4.4.m4.1a"><mrow id="A4.p4.4.m4.1.1" xref="A4.p4.4.m4.1.1.cmml"><mi id="A4.p4.4.m4.1.1.2" xref="A4.p4.4.m4.1.1.2.cmml">a</mi><mo id="A4.p4.4.m4.1.1.1" xref="A4.p4.4.m4.1.1.1.cmml">&gt;</mo><mn id="A4.p4.4.m4.1.1.3" xref="A4.p4.4.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.4.m4.1b"><apply id="A4.p4.4.m4.1.1.cmml" xref="A4.p4.4.m4.1.1"><gt id="A4.p4.4.m4.1.1.1.cmml" xref="A4.p4.4.m4.1.1.1"></gt><ci id="A4.p4.4.m4.1.1.2.cmml" xref="A4.p4.4.m4.1.1.2">𝑎</ci><cn id="A4.p4.4.m4.1.1.3.cmml" type="integer" xref="A4.p4.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.4.m4.1c">a&gt;0</annotation></semantics></math>가 된다. 반대로, 선택되지 않은 토큰의 손실 <math alttext="\mathcal{L}_{us}" class="ltx_Math" display="inline" id="A4.p4.5.m5.1"><semantics id="A4.p4.5.m5.1a"><msub id="A4.p4.5.m5.1.1" xref="A4.p4.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.p4.5.m5.1.1.2" xref="A4.p4.5.m5.1.1.2.cmml">ℒ</mi><mrow id="A4.p4.5.m5.1.1.3" xref="A4.p4.5.m5.1.1.3.cmml"><mi id="A4.p4.5.m5.1.1.3.2" xref="A4.p4.5.m5.1.1.3.2.cmml">u</mi><mo id="A4.p4.5.m5.1.1.3.1" lspace="0em" rspace="0em" xref="A4.p4.5.m5.1.1.3.1.cmml">​</mo><mi id="A4.p4.5.m5.1.1.3.3" xref="A4.p4.5.m5.1.1.3.3.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.p4.5.m5.1b"><apply id="A4.p4.5.m5.1.1.cmml" xref="A4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="A4.p4.5.m5.1.1.1.cmml" xref="A4.p4.5.m5.1.1">subscript</csymbol><ci id="A4.p4.5.m5.1.1.2.cmml" xref="A4.p4.5.m5.1.1.2">ℒ</ci><apply id="A4.p4.5.m5.1.1.3.cmml" xref="A4.p4.5.m5.1.1.3"><times id="A4.p4.5.m5.1.1.3.1.cmml" xref="A4.p4.5.m5.1.1.3.1"></times><ci id="A4.p4.5.m5.1.1.3.2.cmml" xref="A4.p4.5.m5.1.1.3.2">𝑢</ci><ci id="A4.p4.5.m5.1.1.3.3.cmml" xref="A4.p4.5.m5.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.5.m5.1c">\mathcal{L}_{us}</annotation></semantics></math>가 피팅에 사용된다면, <math alttext="a&lt;0" class="ltx_Math" display="inline" id="A4.p4.6.m6.1"><semantics id="A4.p4.6.m6.1a"><mrow id="A4.p4.6.m6.1.1" xref="A4.p4.6.m6.1.1.cmml"><mi id="A4.p4.6.m6.1.1.2" xref="A4.p4.6.m6.1.1.2.cmml">a</mi><mo id="A4.p4.6.m6.1.1.1" xref="A4.p4.6.m6.1.1.1.cmml">&lt;</mo><mn id="A4.p4.6.m6.1.1.3" xref="A4.p4.6.m6.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.6.m6.1b"><apply id="A4.p4.6.m6.1.1.cmml" xref="A4.p4.6.m6.1.1"><lt id="A4.p4.6.m6.1.1.1.cmml" xref="A4.p4.6.m6.1.1.1"></lt><ci id="A4.p4.6.m6.1.1.2.cmml" xref="A4.p4.6.m6.1.1.2">𝑎</ci><cn id="A4.p4.6.m6.1.1.3.cmml" type="integer" xref="A4.p4.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.6.m6.1c">a&lt;0</annotation></semantics></math>가 된다. 따라서, 우리는 선택된 토큰에 대한 모델을 훈련시키는 것이 다운스트림 태스크에 대한 모델의 성능을 효과적으로 향상시킬 수 있는 반면, 선택되지 않은 토큰은 다운스트림 태스크에서 모델의 성능에 해로운 영향을 미칠 수 있다고 믿는다.</p>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Examples of Tokens Selected by SLM</h2>

<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Token Selected Examples</h3>

<figure id="A5.F12" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x12.png" id="A5.F12.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="617" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F12.4.1.1" style="font-size:90%;">그림 12</span>:</span><span class="ltx_text ltx_font_bold" id="A5.F12.5.2" style="font-size:90%;"><span class="ltx_text ltx_font_smallcaps" id="A5.F12.5.2.1">Rho-1</span>의 ToT 트레이닝 프로세스 동안 토큰을 선택하는 특정 예. <span class="ltx_text ltx_font_medium" id="A5.F12.5.2.2"> <span class="ltx_text" id="A5.F12.5.2.2.1" style="color:#1E90FF;">blue</span>은 ToT 훈련 프로세스 동안 훈련된 실제 토큰을 나타내는 반면, 나머지 검은색 토큰은 ToT 훈련 프로세스 동안 훈련되지 않습니다. </span></span></figcaption>
</figure>
<div id="A5.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A5.SS1.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="#A5.F12" title="Figure 12 ‣ E.1 Token Selected Examples ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 12</span></a>에서는 SLM 방법으로 선택한 토큰의 몇 가지 예를 제시하며, <span class="ltx_text" id="A5.SS1.p1.1.1" style="color:#1E90FF;">blue</span>은 사전 훈련 과정에서 실제로 선택한 토큰을 나타낸다.</p>
</div>
</section>
<section id="A5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Dynamic Token Selected</h3>

<figure id="A5.F13" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.07965/assets/x13.png" id="A5.F13.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="706" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F13.7.1.1" style="font-size:90%;">그림 13</span>:</span><span class="ltx_text ltx_font_medium" id="A5.F13.8.2.1.1" style="font-size:90%;">학습 과정에서 동적 토큰 선택 변경 예제 <span class="ltx_text" id="A5.F13.8.2.1.1" style="color:#0000FF;">deep blue</span>, <span class="ltx_text" id="A5.F13.8.2.1.2" style="color:#1E90FF;">light blue</span>, black, <span class="ltx_text" id="A5.F13.8.2.1.3" style="color:#FFB496;">light orange</span>, <span class="ltx_text" id 파란색은 토큰이 선택되는 경향이 높다는 것을 나타내는 반면, 주황색이 많을수록 토큰이 선택되는 경향이 낮다는 것을 나타냅니다. </span></span></figcaption>
</figure>
<div id="A5.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A5.SS2.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="#A5.F13" title="Figure 13 ‣ E.2 Dynamic Token Selected ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 13</span></a>에서는 SLM 훈련 과정 전반에 걸쳐 토큰 선택 경향의 동적 변화를 보여준다. 토큰 선택에서 현재 경향을 분석하기 위해 훈련 과정 중 4개의 체크포인트(0%, 33%, 66%, 100%)를 선택했다. 토큰 선택에 대한 선호도는 일반적으로 <span class="ltx_text" id="A5.SS2.p1.1.1" 스타일="color:#0000FF;">deep blue</span>, <span class="ltx_text" id="A5.SS2.p1.1.2" 스타일="color:#1E90FF;">blue</span>, black, <span class="ltx_text" id="A5.SS2.p1.1.3" 스타일="color:#FFB496;">orange</span>, <span class="ltx_text" id="A5.SS2.p1.1.4" 스타일="color:#FF6400;">dark orange</span>으로 각각 표시됩니다.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2404.07964" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2404.07965" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2404.07965">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.07965" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2404.07967" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 19:41:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Rho-1: Not All Tokens Are What You Need</title>
<!--Generated on Thu Apr 11 15:02:20 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2404.07965v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2404.07965v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2404.07965v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2404.07965v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S1" title="1 Introduction ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2" title="2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Selective Language Modeling</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2.SS1" title="2.1 Not All Tokens Are Equal: Training Dynamics of Token Loss ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Not All Tokens Are Equal: Training Dynamics of Token Loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2.SS2" title="2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Selective Language Modeling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2.SS2.SSS0.Px1" title="Overview ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2.SS2.SSS0.Px2" title="Reference Modeling ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Reference Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2.SS2.SSS0.Px3" title="Selective Pretraining ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Selective Pretraining</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3" title="3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1" title="3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1.SSS0.Px1" title="Reference Model Training ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Reference Model Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1.SSS0.Px2" title="Pretraining Corpus ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Pretraining Corpus</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1.SSS0.Px3" title="Pretraining Setting ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Pretraining Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1.SSS0.Px4" title="Baseline Setting ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Baseline Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1.SSS0.Px5" title="Evaluation Setup ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Evaluation Setup</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS2" title="3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Math Pre-training Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS2.SSS0.Px1" title="Few-shot CoT Reasoning Results ‣ 3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Few-shot CoT Reasoning Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS2.SSS0.Px2" title="Tool-Integrated Reasoning Results ‣ 3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Tool-Integrated Reasoning Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS3" title="3.3 General Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>General Pre-training Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS4" title="3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS4.SSS0.Px1" title="Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Selected Token Loss Aligns Better with Downstream Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS4.SSS0.Px2" title="What Tokens are Selected with SLM? ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">What Tokens are Selected with SLM?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS4.SSS0.Px3" title="Effect of Token Select Ratio ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Effect of Token Select Ratio</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS4.SSS0.Px4" title="Weak-to-Strong Generization ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Weak-to-Strong Generization</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S4" title="4 Related Works ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S4.SS0.SSS0.Px1" title="Pretraining Data Optimization ‣ 4 Related Works ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Pretraining Data Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S4.SS0.SSS0.Px2" title="Data Selection ‣ 4 Related Works ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Data Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S4.SS0.SSS0.Px3" title="Language Model Training Dynamics ‣ 4 Related Works ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Language Model Training Dynamics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S4.SS0.SSS0.Px4" title="Scaling Laws ‣ 4 Related Works ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Scaling Laws</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5" title="5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5.SS0.SSS0.Px1" title="Generalization ‣ 5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Generalization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5.SS0.SSS0.Px2" title="Scalability ‣ 5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Scalability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5.SS0.SSS0.Px3" title="Is training a reference model necessary? ‣ 5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Is training a reference model necessary?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5.SS0.SSS0.Px4" title="How to improve upon SLM? ‣ 5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">How to improve upon SLM?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5.SS0.SSS0.Px5" title="Expanding the Use of SLM ‣ 5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Expanding the Use of SLM</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A1" title="Appendix A Author Contributions ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Author Contributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A2" title="Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Analysis and Visualization of Tokens in Pretraining</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A2.SS1" title="B.1 More Details of Four Categories Tokens ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>More Details of Four Categories Tokens</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A2.SS2" title="B.2 Non-Converging Tokens in Pretrainig ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Non-Converging Tokens in Pretrainig</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A3" title="Appendix C Evalution Details ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Evalution Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A3.SS1" title="C.1 Math Evalution ‣ Appendix C Evalution Details ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Math Evalution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A3.SS2" title="C.2 General Evalution ‣ Appendix C Evalution Details ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>General Evalution</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A4" title="Appendix D Relate the Selected Tokens’ Loss to Downstream Task Performance ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Relate the Selected Tokens’ Loss to Downstream Task Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A5" title="Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Examples of Tokens Selected by SLM</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A5.SS1" title="E.1 Token Selected Examples ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.1 </span>Token Selected Examples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A5.SS2" title="E.2 Dynamic Token Selected ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.2 </span>Dynamic Token Selected</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: fontawesome</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2404.07965v1 [cs.CL] 11 Apr 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_smallcaps" id="id15.id1">Rho-1</span>: Not All Tokens Are What You Need</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenghao Lin<math alttext="~{}~{}^{\chi\phi}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mrow id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml"><mi id="id1.1.m1.1.1.1.2" xref="id1.1.m1.1.1.1.2.cmml">χ</mi><mo id="id1.1.m1.1.1.1.1" xref="id1.1.m1.1.1.1.1.cmml">⁢</mo><mi id="id1.1.m1.1.1.1.3" xref="id1.1.m1.1.1.1.3.cmml">ϕ</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><apply id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"><times id="id1.1.m1.1.1.1.1.cmml" xref="id1.1.m1.1.1.1.1"></times><ci id="id1.1.m1.1.1.1.2.cmml" xref="id1.1.m1.1.1.1.2">𝜒</ci><ci id="id1.1.m1.1.1.1.3.cmml" xref="id1.1.m1.1.1.1.3">italic-ϕ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">~{}~{}^{\chi\phi}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT italic_χ italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Zhibin Gou<math alttext="{}^{\star\pi\phi}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mrow id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml"><mi id="id2.2.m2.1.1.1.2" xref="id2.2.m2.1.1.1.2.cmml"></mi><mo id="id2.2.m2.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="id2.2.m2.1.1.1.1.cmml">⋆</mo><mrow id="id2.2.m2.1.1.1.3" xref="id2.2.m2.1.1.1.3.cmml"><mi id="id2.2.m2.1.1.1.3.2" xref="id2.2.m2.1.1.1.3.2.cmml">π</mi><mo id="id2.2.m2.1.1.1.3.1" xref="id2.2.m2.1.1.1.3.1.cmml">⁢</mo><mi id="id2.2.m2.1.1.1.3.3" xref="id2.2.m2.1.1.1.3.3.cmml">ϕ</mi></mrow></mrow></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><apply id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1"><ci id="id2.2.m2.1.1.1.1.cmml" xref="id2.2.m2.1.1.1.1">⋆</ci><csymbol cd="latexml" id="id2.2.m2.1.1.1.2.cmml" xref="id2.2.m2.1.1.1.2">absent</csymbol><apply id="id2.2.m2.1.1.1.3.cmml" xref="id2.2.m2.1.1.1.3"><times id="id2.2.m2.1.1.1.3.1.cmml" xref="id2.2.m2.1.1.1.3.1"></times><ci id="id2.2.m2.1.1.1.3.2.cmml" xref="id2.2.m2.1.1.1.3.2">𝜋</ci><ci id="id2.2.m2.1.1.1.3.3.cmml" xref="id2.2.m2.1.1.1.3.3">italic-ϕ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{\star\pi\phi}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT ⋆ italic_π italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Yeyun Gong<math alttext="~{}~{}^{\phi}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mi id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><ci id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">~{}~{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Xiao Liu<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><msup id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mi id="id4.4.m4.1.1a" xref="id4.4.m4.1.1.cmml"></mi><mi id="id4.4.m4.1.1.1" xref="id4.4.m4.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><ci id="id4.4.m4.1.1.1.cmml" xref="id4.4.m4.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Yelong Shen<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id5.5.m5.1"><semantics id="id5.5.m5.1a"><msup id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml"><mi id="id5.5.m5.1.1a" xref="id5.5.m5.1.1.cmml"></mi><mi id="id5.5.m5.1.1.1" xref="id5.5.m5.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><apply id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1"><ci id="id5.5.m5.1.1.1.cmml" xref="id5.5.m5.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id5.5.m5.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id11.11.6">
Ruochen Xu<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id6.6.1.m1.1"><semantics id="id6.6.1.m1.1a"><msup id="id6.6.1.m1.1.1" xref="id6.6.1.m1.1.1.cmml"><mi id="id6.6.1.m1.1.1a" xref="id6.6.1.m1.1.1.cmml"></mi><mi id="id6.6.1.m1.1.1.1" xref="id6.6.1.m1.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id6.6.1.m1.1b"><apply id="id6.6.1.m1.1.1.cmml" xref="id6.6.1.m1.1.1"><ci id="id6.6.1.m1.1.1.1.cmml" xref="id6.6.1.m1.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.1.m1.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id6.6.1.m1.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Chen Lin<math alttext="{}^{\diamond\chi}" class="ltx_Math" display="inline" id="id7.7.2.m2.1"><semantics id="id7.7.2.m2.1a"><msup id="id7.7.2.m2.1.1" xref="id7.7.2.m2.1.1.cmml"><mi id="id7.7.2.m2.1.1a" xref="id7.7.2.m2.1.1.cmml"></mi><mrow id="id7.7.2.m2.1.1.1" xref="id7.7.2.m2.1.1.1.cmml"><mo id="id7.7.2.m2.1.1.1a" mathvariant="normal" rspace="0em" xref="id7.7.2.m2.1.1.1.cmml">⋄</mo><mi id="id7.7.2.m2.1.1.1.2" xref="id7.7.2.m2.1.1.1.2.cmml">χ</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id7.7.2.m2.1b"><apply id="id7.7.2.m2.1.1.cmml" xref="id7.7.2.m2.1.1"><apply id="id7.7.2.m2.1.1.1.cmml" xref="id7.7.2.m2.1.1.1"><ci id="id7.7.2.m2.1.1.1.1.cmml" xref="id7.7.2.m2.1.1.1">normal-⋄</ci><ci id="id7.7.2.m2.1.1.1.2.cmml" xref="id7.7.2.m2.1.1.1.2">𝜒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.2.m2.1c">{}^{\diamond\chi}</annotation><annotation encoding="application/x-llamapun" id="id7.7.2.m2.1d">start_FLOATSUPERSCRIPT ⋄ italic_χ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Yujiu Yang<math alttext="{}^{\diamond\pi}" class="ltx_Math" display="inline" id="id8.8.3.m3.1"><semantics id="id8.8.3.m3.1a"><msup id="id8.8.3.m3.1.1" xref="id8.8.3.m3.1.1.cmml"><mi id="id8.8.3.m3.1.1a" xref="id8.8.3.m3.1.1.cmml"></mi><mrow id="id8.8.3.m3.1.1.1" xref="id8.8.3.m3.1.1.1.cmml"><mo id="id8.8.3.m3.1.1.1a" mathvariant="normal" rspace="0em" xref="id8.8.3.m3.1.1.1.cmml">⋄</mo><mi id="id8.8.3.m3.1.1.1.2" xref="id8.8.3.m3.1.1.1.2.cmml">π</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id8.8.3.m3.1b"><apply id="id8.8.3.m3.1.1.cmml" xref="id8.8.3.m3.1.1"><apply id="id8.8.3.m3.1.1.1.cmml" xref="id8.8.3.m3.1.1.1"><ci id="id8.8.3.m3.1.1.1.1.cmml" xref="id8.8.3.m3.1.1.1">normal-⋄</ci><ci id="id8.8.3.m3.1.1.1.2.cmml" xref="id8.8.3.m3.1.1.1.2">𝜋</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.8.3.m3.1c">{}^{\diamond\pi}</annotation><annotation encoding="application/x-llamapun" id="id8.8.3.m3.1d">start_FLOATSUPERSCRIPT ⋄ italic_π end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Jian Jiao<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id9.9.4.m4.1"><semantics id="id9.9.4.m4.1a"><msup id="id9.9.4.m4.1.1" xref="id9.9.4.m4.1.1.cmml"><mi id="id9.9.4.m4.1.1a" xref="id9.9.4.m4.1.1.cmml"></mi><mi id="id9.9.4.m4.1.1.1" xref="id9.9.4.m4.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id9.9.4.m4.1b"><apply id="id9.9.4.m4.1.1.cmml" xref="id9.9.4.m4.1.1"><ci id="id9.9.4.m4.1.1.1.cmml" xref="id9.9.4.m4.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.9.4.m4.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id9.9.4.m4.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Nan Duan<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id10.10.5.m5.1"><semantics id="id10.10.5.m5.1a"><msup id="id10.10.5.m5.1.1" xref="id10.10.5.m5.1.1.cmml"><mi id="id10.10.5.m5.1.1a" xref="id10.10.5.m5.1.1.cmml"></mi><mi id="id10.10.5.m5.1.1.1" xref="id10.10.5.m5.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id10.10.5.m5.1b"><apply id="id10.10.5.m5.1.1.cmml" xref="id10.10.5.m5.1.1"><ci id="id10.10.5.m5.1.1.1.cmml" xref="id10.10.5.m5.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.10.5.m5.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id10.10.5.m5.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Weizhu Chen<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id11.11.6.m6.1"><semantics id="id11.11.6.m6.1a"><msup id="id11.11.6.m6.1.1" xref="id11.11.6.m6.1.1.cmml"><mi id="id11.11.6.m6.1.1a" xref="id11.11.6.m6.1.1.cmml"></mi><mi id="id11.11.6.m6.1.1.1" xref="id11.11.6.m6.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id11.11.6.m6.1b"><apply id="id11.11.6.m6.1.1.cmml" xref="id11.11.6.m6.1.1"><ci id="id11.11.6.m6.1.1.1.cmml" xref="id11.11.6.m6.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.11.6.m6.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id11.11.6.m6.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<br class="ltx_break"><math alttext="{}^{\chi}" class="ltx_Math" display="inline" id="id12.12.m6.1"><semantics id="id12.12.m6.1a"><msup id="id12.12.m6.1.1" xref="id12.12.m6.1.1.cmml"><mi id="id12.12.m6.1.1a" xref="id12.12.m6.1.1.cmml"></mi><mi id="id12.12.m6.1.1.1" xref="id12.12.m6.1.1.1.cmml">χ</mi></msup><annotation-xml encoding="MathML-Content" id="id12.12.m6.1b"><apply id="id12.12.m6.1.1.cmml" xref="id12.12.m6.1.1"><ci id="id12.12.m6.1.1.1.cmml" xref="id12.12.m6.1.1.1">𝜒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.12.m6.1c">{}^{\chi}</annotation><annotation encoding="application/x-llamapun" id="id12.12.m6.1d">start_FLOATSUPERSCRIPT italic_χ end_FLOATSUPERSCRIPT</annotation></semantics></math>Xiamen University <math alttext="{}^{\pi}" class="ltx_Math" display="inline" id="id13.13.m7.1"><semantics id="id13.13.m7.1a"><msup id="id13.13.m7.1.1" xref="id13.13.m7.1.1.cmml"><mi id="id13.13.m7.1.1a" xref="id13.13.m7.1.1.cmml"></mi><mi id="id13.13.m7.1.1.1" xref="id13.13.m7.1.1.1.cmml">π</mi></msup><annotation-xml encoding="MathML-Content" id="id13.13.m7.1b"><apply id="id13.13.m7.1.1.cmml" xref="id13.13.m7.1.1"><ci id="id13.13.m7.1.1.1.cmml" xref="id13.13.m7.1.1.1">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id13.13.m7.1c">{}^{\pi}</annotation><annotation encoding="application/x-llamapun" id="id13.13.m7.1d">start_FLOATSUPERSCRIPT italic_π end_FLOATSUPERSCRIPT</annotation></semantics></math>Tsinghua University <math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id14.14.m8.1"><semantics id="id14.14.m8.1a"><msup id="id14.14.m8.1.1" xref="id14.14.m8.1.1.cmml"><mi id="id14.14.m8.1.1a" xref="id14.14.m8.1.1.cmml"></mi><mi id="id14.14.m8.1.1.1" xref="id14.14.m8.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id14.14.m8.1b"><apply id="id14.14.m8.1.1.cmml" xref="id14.14.m8.1.1"><ci id="id14.14.m8.1.1.1.cmml" xref="id14.14.m8.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id14.14.m8.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id14.14.m8.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>Microsoft 
<br class="ltx_break"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aka.ms/rho" title="">https://aka.ms/rho</a>
</span><span class="ltx_author_notes">Equal contribution. See author contributions for details. Work done during their internships at Microsoft Research Asia. 🖂:&nbsp;<span class="ltx_text ltx_font_typewriter" id="id16.15.id1">zhenghaolin@stu.xmu.edu.cn</span>;&nbsp;&nbsp;<span class="ltx_text ltx_font_typewriter" id="id17.16.id2">zebgou@gmail.com</span>Correspondence authors.</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id18.id1">이전의 언어 모델 사전 트레이닝 방법들은 모든 트레이닝 토큰들에 대해 일률적으로 다음 토큰 예측 손실을 적용하였다. 이 규범에 도전하여 <span class="ltx_text ltx_font_italic" id="id18.id1.1">“말뭉치의 모든 토큰이 언어 모델 훈련에 동등하게 중요한 것은 아니다”</span>이라고 가정한다. 우리의 초기 분석은 언어 모델의 토큰 수준 훈련 역학에 대해 조사하며, 서로 다른 토큰에 대한 뚜렷한 손실 패턴을 드러낸다. 이러한 통찰력을 활용하여 <span class="ltx_text ltx_font_smallcaps" id="id18.id1.2">Rho-1</span>이라는 새로운 언어 모델을 소개한다. 말뭉치에서 모든 다음 토큰을 예측하는 방법을 배우는 기존 LM과 달리 <span class="ltx_text ltx_font_smallcaps" id="id18.id1.3">Rho-1</span>은 원하는 분포와 정렬된 유용한 토큰을 선택적으로 훈련하는 SLM(Selective Language Modeling)을 사용합니다. 이 접근법은 참조 모델을 사용하여 토큰들을 사전 트레이닝한 후, 더 높은 초과 손실을 갖는 토큰들에 집중된 손실로 언어 모델을 트레이닝하는 것을 포함한다. 15B OpenWebMath 코퍼스에 대한 지속적인 사전 훈련 시 <span class="ltx_text ltx_font_smallcaps" id="id18.id1.4">Rho-1</span>은 9개의 수학 과제에서 최대 30%의 적은 샷 정확도에서 절대적인 개선을 보여준다. 미세 조정 후 <span class="ltx_text ltx_font_smallcaps" id="id18.id1.5">Rho-1</span>-1B 및 7B는 MATH 데이터 세트에서 각각 40.6% 및 51.8%의 최첨단 결과를 달성했습니다. DeepSeekMath와 사전 훈련 토큰의 3%만 일치합니다. 또한 80B 일반 토큰에 대한 사전 학습 시 <span class="ltx_text ltx_font_smallcaps" id="id18.id1.6">Rho-1</span>은 15개의 다양한 태스크에서 평균 6.8%의 향상을 달성하여 언어 모델 사전 학습의 효율성과 성능을 모두 향상시켰다.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="338" id="S0.F1.g1" src="https://arxiv.org/html/2404.07965v1/x1.png" width="789">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S0.F1.4.2" style="font-size:90%;">
We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. <span class="ltx_text ltx_font_smallcaps" id="S0.F1.4.2.1">Rho-1</span> is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling.
SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5-10x faster.
</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S0.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="320" id="S0.F2.g1" src="https://arxiv.org/html/2404.07965v1/x2.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F2.5.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S0.F2.6.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S0.F2.6.2.1">Upper:</span> Even an extensively filtered pretraining corpus contains token-level noise.
<span class="ltx_text ltx_font_bold" id="S0.F2.6.2.2">Left:</span> Previous Causal Language Modeling (CLM) trains on all tokens.
<span class="ltx_text ltx_font_bold" id="S0.F2.6.2.3">Right:</span> Our proposed Selective Language Modeling (SLM) selectively applies loss on those useful and clean tokens.
</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">모델 매개변수 및 데이터 세트 크기를 확장하면 대규모 언어 모델에서 다음 토큰 예측 정확도가 지속적으로 상승하여 인공 지능 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Brown et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">OpenAI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Team et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>에서 상당한 발전을 가져왔다. 그러나 사용 가능한 모든 데이터에 대한 교육이 항상 최적이거나 실현 가능한 것은 아니다. 그 결과, 학습 문서를 선택하기 위해 다양한 휴리스틱과 분류기 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Brown et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Wenzek et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>를 사용하는 데이터 필터링의 관행이 중요해졌다. 이러한 기술은 데이터 품질을 크게 개선하고 모델 성능을 향상시킵니다.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">그러나 철저한 문서 수준 필터링에도 불구하고 고품질 데이터 세트에는 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S0.F2" title="Figure 2 ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> (상단)에 설명된 것처럼 학습에 부정적인 영향을 미칠 수 있는 많은 노이즈 토큰이 여전히 포함되어 있다. 이러한 토큰을 제거하면 텍스트의 의미가 변경될 수 있지만 지나치게 엄격한 필터링은 유용한 데이터 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Welbl et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Muennighoff et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>를 배제하고 편향 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dodge et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Longpre et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>로 이어질 수 있다. 또한, 연구는 웹 데이터의 분포가 다운스트림 애플리케이션에 대한 이상적인 분포와 본질적으로 일치하지 않는다는 것을 나타낸다. 예를 들어, 토큰 레벨에서의 공통 코퍼스는 예측하기 어려운 환각 또는 매우 모호한 토큰과 같은 바람직하지 않은 콘텐츠를 포함할 수 있다. 모든 토큰에 동일한 손실을 적용하면 비이익 토큰에 대한 계산이 낭비될 수 있으며, 이는 LLM의 잠재력을 단지 평범한 지능으로 제한할 수 있다.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">언어 모델이 토큰 수준에서 학습하는 방법을 탐구하기 위해 처음에 훈련 역학, 특히 일반적인 사전 훈련 동안 토큰 수준 손실이 어떻게 진화하는지 조사했다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.SS1" title="2.1 Not All Tokens Are Equal: Training Dynamics of Token Loss ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.1</span></a>에서는 서로 다른 체크포인트에서 모델의 토큰 복잡도를 평가하고 토큰을 서로 다른 유형으로 분류했다. 우리의 연구 결과는 상당한 손실 감소가 훈련 동안 선택된 토큰 그룹으로 제한된다는 것을 보여준다. 많은 토큰은 이미 학습된 "쉬운 토큰"이고, 일부는 가변적인 손실을 나타내고 수렴에 저항하는 "하드 토큰"이다. 이러한 토큰은 수많은 비효율적인 그라디언트 업데이트로 이어질 수 있습니다.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">이러한 분석을 기반으로 새로운 SLM(Selective Language Modeling) 목적으로 훈련된 <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">Rho-1</span> 모델을 소개한다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S0.F2" title="Figure 2 ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>(Right)에 도시된 바와 같이, 이 접근법은 전체 시퀀스를 모델에 입력하고 원하지 않는 토큰의 손실을 선택적으로 제거한다. 세부 파이프라인은 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.F4" title="Figure 4 ‣ Reference Modeling ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>: 먼저 SLM은 고품질 말뭉치에 대한 참조 언어 모델을 학습한다. 이 모델은 유틸리티 메트릭을 설정하여 원하는 분포에 따라 토큰을 점수화하여 부정한 토큰과 관련 없는 토큰을 자연스럽게 필터링합니다. 둘째, SLM은 참조 모델을 사용하여 손실(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.SS2.SSS0.Px2" title="Reference Modeling ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.2</span></a>)을 사용하여 말뭉치에서 각 토큰에 점수를 매긴다. 마지막으로, 레퍼런스와 트레이닝 모델 사이에서 높은 초과 손실을 나타내는 토큰들에 대해서만 언어 모델을 트레이닝하고, 다운스트림 애플리케이션들(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.SS2" title="2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.2</span></a>)에 가장 유리한 토큰들을 선택적으로 트레이닝한다.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">우리는 포괄적인 실험을 통해 SLM이 사전 훈련 동안 토큰 효율성을 크게 향상시키고 다운스트림 태스크에서 성능을 향상시킨다는 것을 보여준다. 또한, 본 연구 결과는 SLM이 목표 분포와 관련된 토큰을 효과적으로 식별하여 선택된 토큰으로 훈련된 모델에 대한 벤치마크에 대한 복잡도 점수를 향상시킨다는 것을 나타낸다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.SS2" title="3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§3.2</span></a>는 수학 연속 사전 훈련에서 SLM의 효과를 보여줍니다. 1B 및 7B <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.1">Rho-1</span>이 GSM8k 및 MATH 데이터 세트에서 CLM 훈련 기준선을 16% 이상 능가합니다. SLM은 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S0.F1" title="Figure 1 ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>와 같이 최대 10배 빠른 기준선 정확도에 도달한다. 놀랍게도 <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.2">Rho-1</span>-7B는 DeepSeekMath에서 요구하는 500B 토큰과 비교하여 15B 토큰만 사용하는 DeepSeekMath-7B의 최첨단 성능과 일치한다. 미세 조정 시 <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.3">Rho-1</span>-1B 및 7B는 MATH에서 각각 40.6% 및 51.8%를 달성합니다. 특히, <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.4">Rho-1</span>-1B는 초기 GPT-4의 CoT 성능 42.5%에 근접하여 40% 정확도를 초과하는 최초의 1B LM이다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.SS3" title="3.3 General Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§3.3</span></a>는 일반적인 사전 훈련에서 SLM의 효능을 확인한다: SLM을 사용하여 80B 토큰에 Tinyllama-1B를 훈련하면 15개의 벤치마크에서 평균 6.8% 향상되며 코드 및 수학 과제에서 10% 이상의 이득이 발생한다.</p>
</div>
<figure class="ltx_figure" id="S1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="239" id="S1.F3.g1" src="https://arxiv.org/html/2404.07965v1/x3.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F3.3.1.1" style="font-size:90%;">Figure 3</span>:</span><span class="ltx_text ltx_font_bold" id="S1.F3.4.2" style="font-size:90%;">The loss of four categories of tokens during pretraining. <span class="ltx_text ltx_font_medium" id="S1.F3.4.2.1">(a)는 프리트레이닝 동안 H→H, L→H, H→L 및 L→L 토큰의 손실을 보여준다. (b)와 (c)는 각각 사전 훈련 중 L→L과 H→H에서 토큰의 손실이 변동하는 세 가지 경우를 보여준다. </span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Selective Language Modeling</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Not All Tokens Are Equal: Training Dynamics of Token Loss</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">우리의 조사는 표준 사전 훈련 동안 개별 토큰의 손실이 어떻게 진화하는지에 대한 비판적 관점에서 시작한다. 우리는 OpenWebMath의 15B 토큰으로 Tinyllama-1B를 사전 교육하여 모든 1B 토큰 후에 체크포인트를 저장합니다. 그런 다음 약 320,000개의 토큰의 유효성 검사 세트를 사용하여 이러한 간격에서 토큰 수준 손실을 평가한다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>(a)는 토큰의 손실 궤적에 따라 지속성 높은 손실(H→H), 증가 손실(L→H), 감소 손실(H→L) 및 일관된 낮은 손실(L→L)의 네 가지 범주로 나뉜다. 이러한 범주에 대한 자세한 내용은 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A2.SS1" title="B.1 More Details of Four Categories Tokens ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§B.1</span></a>를 참조하십시오. 우리의 분석에서는 토큰의 26%만이 현저한 손실 감소(H→L)를 나타내는 반면, 대다수(51%)는 L→L 범주에 남아 있음을 밝혀 이미 학습되었음을 나타낸다. 흥미로운 사실은 토큰의 11%가 지속적으로 도전적(H→H)이며, 이는 높은 애레토릭 불확실성 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hüllermeier and Waegeman</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> 때문일 수 있다. 또한 12%의 토큰은 훈련 중에 예상치 못한 손실 증가(L→H)를 경험한다.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">우리의 두 번째 관찰은 상당한 수의 토큰 손실이 지속적인 변동을 나타내고 수렴에 저항한다는 것이다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> (b) 및 (c)에 묘사된 바와 같이 많은 L→L 및 H→H 토큰의 손실은 트레이닝 동안 높은 분산을 나타낸다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A2.SS2" title="B.2 Non-Converging Tokens in Pretrainig ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§B.2</span></a>에서 우리는 이러한 토큰의 내용을 시각화하고 분석하며 많은 토큰이 잡음이 있음을 발견하는데, 이는 우리의 가설과 일치한다.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">결과적으로, 우리는 트레이닝 동안 각각의 토큰과 연관된 손실이 전체 손실과 같이 부드럽게 감소하지 않고, 대신에 상이한 토큰들 사이에 복잡한 트레이닝 동적(training dynamic)이 있음을 배운다. 훈련 중에 모델에 집중할 적절한 토큰을 선택할 수 있다면 모델의 훈련 궤적을 안정화하고 효율성을 높일 수 있을 것이다.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Selective Language Modeling</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Overview</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">문서-레벨 필터링에서 참조 모델의 실행에 영감을 받아, "선택적 언어 모델링(Selective Language Modeling; SLM)"이라고 불리는 토큰-레벨 데이터 선택의 간단한 파이프라인을 제안한다. 우리의 방법은 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.F4" title="Figure 4 ‣ Reference Modeling ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>에 묘사된 바와 같이 세 단계로 구성된다. 우리는 선별된 고품질 데이터 세트에서 참조 모델을 훈련하는 것으로 시작한다. 그런 다음 이 모델은 사전 훈련 코퍼스 내에서 각 토큰의 손실을 평가한다. 마지막 단계에서는 학습과 참조 모델 간의 초과 손실이 높은 토큰에 초점을 맞추어 언어 모델을 선택적으로 학습한다. 직관은 초과 손실이 높은 토큰이 더 학습 가능하고 원하는 분포와 더 잘 정렬되어, 무관하거나 품질이 낮은 토큰을 자연스럽게 배제한다는 것이다. 아래에서는 각 단계에 대한 상세한 설명을 제공한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Reference Modeling</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.2">원하는 데이터 분포를 반영하는 고품질 데이터 세트를 선별하는 것으로 시작합니다. 우리는 선별된 데이터에 대해 표준 교차 엔트로피 손실을 사용하여 참조 모델(RM)을 학습한다. 그런 다음 결과 RM은 더 큰 사전 훈련 코퍼스 내에서 토큰 손실을 평가하는 데 사용된다. RM이 이 토큰에 할당할 확률을 기반으로 토큰 <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS2.SSS0.Px2.p1.2.m2.1a"><msub id="S2.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.2.m2.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>의 참조 손실(<math alttext="\mathcal{L}_{\text{ref}}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p1.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3a.cmml">ref</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3a.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3"><mtext id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3">ref</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.1.m1.1c">\mathcal{L}_{\text{ref}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT</annotation></semantics></math>)을 계산한다. 계산은 다음과 같이 공식화된다:</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{ref}}(x_{i})=-\log P(x_{i}|x{<i})" class="ltx_Math" display="block" id="S2.E1.m1.2"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.3.2.cmml">ℒ</mi><mtext id="S2.E1.m1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.3.3a.cmml">ref</mtext></msub><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">=</mo><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><mo id="S2.E1.m1.2.2.2a" rspace="0.167em" xref="S2.E1.m1.2.2.2.cmml">−</mo><mrow id="S2.E1.m1.2.2.2.1" xref="S2.E1.m1.2.2.2.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.3" xref="S2.E1.m1.2.2.2.1.3.cmml"><mi id="S2.E1.m1.2.2.2.1.3.1" xref="S2.E1.m1.2.2.2.1.3.1.cmml">log</mi><mo id="S2.E1.m1.2.2.2.1.3a" lspace="0.167em" xref="S2.E1.m1.2.2.2.1.3.cmml">⁡</mo><mi id="S2.E1.m1.2.2.2.1.3.2" xref="S2.E1.m1.2.2.2.1.3.2.cmml">P</mi></mrow><mo id="S2.E1.m1.2.2.2.1.2" xref="S2.E1.m1.2.2.2.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.2.2.2.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mo id="S2.E1.m1.2.2.2.1.1.1.2" stretchy="false" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.cmml"><msub id="S2.E1.m1.2.2.2.1.1.1.1.2.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.2.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.2.cmml">x</mi><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.2.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.3.cmml">i</mi></msub><mo fence="false" id="S2.E1.m1.2.2.2.1.1.1.1.2.1" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.cmml">|</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml">x</mi></mrow><mo id="S2.E1.m1.2.2.2.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.cmml">&lt;</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.3.cmml">i</mi></mrow><mo id="S2.E1.m1.2.2.2.1.1.1.3" stretchy="false" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3"></eq><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><times id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.2"></times><apply id="S2.E1.m1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.3.2">ℒ</ci><ci id="S2.E1.m1.1.1.1.3.3a.cmml" xref="S2.E1.m1.1.1.1.3.3"><mtext id="S2.E1.m1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.E1.m1.1.1.1.3.3">ref</mtext></ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><minus id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2"></minus><apply id="S2.E1.m1.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1"><times id="S2.E1.m1.2.2.2.1.2.cmml" xref="S2.E1.m1.2.2.2.1.2"></times><apply id="S2.E1.m1.2.2.2.1.3.cmml" xref="S2.E1.m1.2.2.2.1.3"><log id="S2.E1.m1.2.2.2.1.3.1.cmml" xref="S2.E1.m1.2.2.2.1.3.1"></log><ci id="S2.E1.m1.2.2.2.1.3.2.cmml" xref="S2.E1.m1.2.2.2.1.3.2">𝑃</ci></apply><apply id="S2.E1.m1.2.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1"><lt id="S2.E1.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1"></lt><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2"><csymbol cd="latexml" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1">conditional</csymbol><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.2">𝑥</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3">𝑥</ci></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\mathcal{L}_{\text{ref}}(x_{i})=-\log P(x_{i}|x{&lt;i})</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.2d">caligraphic_L start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = - roman_log italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x &lt; italic_i )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p3.1">각 토큰에 대해 <math alttext="\mathcal{L}_{\text{ref}}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p3.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p3.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p3.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3a.cmml">ref</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3a.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3"><mtext id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3">ref</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.1.m1.1c">\mathcal{L}_{\text{ref}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT</annotation></semantics></math>를 평가하여 선택적 사전 훈련에 대한 참조 손실을 설정하여 언어 모델링에서 가장 영향력 있는 토큰에 집중할 수 있도록 한다.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="146" id="S2.F4.g1" src="https://arxiv.org/html/2404.07965v1/x4.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.3.1.1" style="font-size:90%;">Figure 4</span>:</span><span class="ltx_text ltx_font_bold" id="S2.F4.4.2" style="font-size:90%;">The pipeline of Selective Language Modeling (SLM). <span class="ltx_text ltx_font_medium" id="S2.F4.4.2.1"></span></span></figcaption>
SLM optimizes language model performance by concentrating on valuable, clean tokens during pre-training.
It involves three steps:
(Step 1) Initially, train a reference model on high-quality data.
(Step 2) Then, score each token’s loss in a corpus using the reference model.
(Step 3) Finally, train the language model selectively on tokens that show higher excess loss compared to the reference loss.
</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Selective Pretraining</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p1.1">인과 언어 모델링(CLM)은 교차 엔트로피 손실을 사용한다는 점에 유의한다:</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{CLM}}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}\log P(x_{i}|x_{<i}%
;\theta)" class="ltx_Math" display="block" id="S2.E2.m1.3"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml"><mrow id="S2.E2.m1.3.3.3" xref="S2.E2.m1.3.3.3.cmml"><msub id="S2.E2.m1.3.3.3.2" xref="S2.E2.m1.3.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.3.3.3.2.2" xref="S2.E2.m1.3.3.3.2.2.cmml">ℒ</mi><mtext id="S2.E2.m1.3.3.3.2.3" xref="S2.E2.m1.3.3.3.2.3a.cmml">CLM</mtext></msub><mo id="S2.E2.m1.3.3.3.1" xref="S2.E2.m1.3.3.3.1.cmml">⁢</mo><mrow id="S2.E2.m1.3.3.3.3.2" xref="S2.E2.m1.3.3.3.cmml"><mo id="S2.E2.m1.3.3.3.3.2.1" stretchy="false" xref="S2.E2.m1.3.3.3.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">θ</mi><mo id="S2.E2.m1.3.3.3.3.2.2" stretchy="false" xref="S2.E2.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.3.3.2" xref="S2.E2.m1.3.3.2.cmml">=</mo><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.cmml"><mo id="S2.E2.m1.3.3.1a" xref="S2.E2.m1.3.3.1.cmml">−</mo><mrow id="S2.E2.m1.3.3.1.1" xref="S2.E2.m1.3.3.1.1.cmml"><mfrac id="S2.E2.m1.3.3.1.1.3" xref="S2.E2.m1.3.3.1.1.3.cmml"><mn id="S2.E2.m1.3.3.1.1.3.2" xref="S2.E2.m1.3.3.1.1.3.2.cmml">1</mn><mi id="S2.E2.m1.3.3.1.1.3.3" xref="S2.E2.m1.3.3.1.1.3.3.cmml">N</mi></mfrac><mo id="S2.E2.m1.3.3.1.1.2" xref="S2.E2.m1.3.3.1.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.1.cmml"><munderover id="S2.E2.m1.3.3.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.2.cmml"><mo id="S2.E2.m1.3.3.1.1.1.2.2.2" movablelimits="false" xref="S2.E2.m1.3.3.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.E2.m1.3.3.1.1.1.2.2.3" xref="S2.E2.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.2.2.3.2" xref="S2.E2.m1.3.3.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E2.m1.3.3.1.1.1.2.2.3.1" xref="S2.E2.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E2.m1.3.3.1.1.1.2.2.3.3" xref="S2.E2.m1.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m1.3.3.1.1.1.2.3" xref="S2.E2.m1.3.3.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S2.E2.m1.3.3.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><mrow id="S2.E2.m1.3.3.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.3.1" xref="S2.E2.m1.3.3.1.1.1.1.3.1.cmml">log</mi><mo id="S2.E2.m1.3.3.1.1.1.1.3a" lspace="0.167em" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml">⁡</mo><mi id="S2.E2.m1.3.3.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S2.E2.m1.3.3.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">x</mi><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">θ</mi></mrow></mrow><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3"><eq id="S2.E2.m1.3.3.2.cmml" xref="S2.E2.m1.3.3.2"></eq><apply id="S2.E2.m1.3.3.3.cmml" xref="S2.E2.m1.3.3.3"><times id="S2.E2.m1.3.3.3.1.cmml" xref="S2.E2.m1.3.3.3.1"></times><apply id="S2.E2.m1.3.3.3.2.cmml" xref="S2.E2.m1.3.3.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.2.1.cmml" xref="S2.E2.m1.3.3.3.2">subscript</csymbol><ci id="S2.E2.m1.3.3.3.2.2.cmml" xref="S2.E2.m1.3.3.3.2.2">ℒ</ci><ci id="S2.E2.m1.3.3.3.2.3a.cmml" xref="S2.E2.m1.3.3.3.2.3"><mtext id="S2.E2.m1.3.3.3.2.3.cmml" mathsize="70%" xref="S2.E2.m1.3.3.3.2.3">CLM</mtext></ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝜃</ci></apply><apply id="S2.E2.m1.3.3.1.cmml" xref="S2.E2.m1.3.3.1"><minus id="S2.E2.m1.3.3.1.2.cmml" xref="S2.E2.m1.3.3.1"></minus><apply id="S2.E2.m1.3.3.1.1.cmml" xref="S2.E2.m1.3.3.1.1"><times id="S2.E2.m1.3.3.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.3"><divide id="S2.E2.m1.3.3.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.3"></divide><cn id="S2.E2.m1.3.3.1.1.3.2.cmml" type="integer" xref="S2.E2.m1.3.3.1.1.3.2">1</cn><ci id="S2.E2.m1.3.3.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.3.3">𝑁</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1"><apply id="S2.E2.m1.3.3.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S2.E2.m1.3.3.1.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.2.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2">subscript</csymbol><sum id="S2.E2.m1.3.3.1.1.1.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.2"></sum><apply id="S2.E2.m1.3.3.1.1.1.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3"><eq id="S2.E2.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S2.E2.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3.2">𝑖</ci><cn id="S2.E2.m1.3.3.1.1.1.2.2.3.3.cmml" type="integer" xref="S2.E2.m1.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.3.3.1.1.1.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2.3">𝑁</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1"><times id="S2.E2.m1.3.3.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3"><log id="S2.E2.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.1"></log><ci id="S2.E2.m1.3.3.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.2">𝑃</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2">𝑥</ci><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3">𝑖</ci></apply><list id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><lt id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝜃</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">\mathcal{L}_{\text{CLM}}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}\log P(x_{i}|x_{&lt;i}%
;\theta)</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.3d">caligraphic_L start_POSTSUBSCRIPT CLM end_POSTSUBSCRIPT ( italic_θ ) = - divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_log italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ; italic_θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p3">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p3.10">여기서, <math alttext="\mathcal{L_{\text{CLM}}}(\theta)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.1.m1.1"><semantics id="S2.SS2.SSS0.Px3.p3.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p3.1.m1.1.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml"><msub id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3a.cmml">CLM</mtext></msub><mo id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml"><mo id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2.1" stretchy="false" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml">(</mo><mi id="S2.SS2.SSS0.Px3.p3.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.1.cmml">θ</mi><mo id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2.2" stretchy="false" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2"><times id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1"></times><apply id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2">ℒ</ci><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3a.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3"><mtext id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3.cmml" mathsize="70%" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3">CLM</mtext></ci></apply><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.1.m1.1c">\mathcal{L_{\text{CLM}}}(\theta)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT CLM end_POSTSUBSCRIPT ( italic_θ )</annotation></semantics></math>는 모델 <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.2.m2.1"><semantics id="S2.SS2.SSS0.Px3.p3.2.m2.1a"><mi id="S2.SS2.SSS0.Px3.p3.2.m2.1.1" xref="S2.SS2.SSS0.Px3.p3.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.2.m2.1b"><ci id="S2.SS2.SSS0.Px3.p3.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.2.m2.1d">italic_θ</annotation></semantics></math>에 의해 파라미터화된 손실 함수를 나타낸다. <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.3.m3.1"><semantics id="S2.SS2.SSS0.Px3.p3.3.m3.1a"><mi id="S2.SS2.SSS0.Px3.p3.3.m3.1.1" xref="S2.SS2.SSS0.Px3.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.3.m3.1b"><ci id="S2.SS2.SSS0.Px3.p3.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.3.m3.1d">italic_N</annotation></semantics></math>는 시퀀스의 길이이고, <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.4.m4.1"><semantics id="S2.SS2.SSS0.Px3.p3.4.m4.1a"><msub id="S2.SS2.SSS0.Px3.p3.4.m4.1.1" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.4.m4.1b"><apply id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.4.m4.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>는 시퀀스 내의 <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.5.m5.1"><semantics id="S2.SS2.SSS0.Px3.p3.5.m5.1a"><mi id="S2.SS2.SSS0.Px3.p3.5.m5.1.1" xref="S2.SS2.SSS0.Px3.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.5.m5.1b"><ci id="S2.SS2.SSS0.Px3.p3.5.m5.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.5.m5.1d">italic_i</annotation></semantics></math>-th 토큰이고, <math alttext="x_{&lt;i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.6.m6.1"><semantics id="S2.SS2.SSS0.Px3.p3.6.m6.1a"><msub id="S2.SS2.SSS0.Px3.p3.6.m6.1.1" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2.cmml">x</mi><mrow id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2.cmml"></mi><mo id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1.cmml">&lt;</mo><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.6.m6.1b"><apply id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2">𝑥</ci><apply id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3"><lt id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1"></lt><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2">absent</csymbol><ci id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.6.m6.1c">x_{&lt;i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.6.m6.1d">italic_x start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT</annotation></semantics></math>는 <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.7.m7.1"><semantics id="S2.SS2.SSS0.Px3.p3.7.m7.1a"><mi id="S2.SS2.SSS0.Px3.p3.7.m7.1.1" xref="S2.SS2.SSS0.Px3.p3.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.7.m7.1b"><ci id="S2.SS2.SSS0.Px3.p3.7.m7.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.7.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.7.m7.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.7.m7.1d">italic_i</annotation></semantics></math>-th 토큰 이전의 모든 토큰을 나타낸다. 대조적으로, 선택적 언어 모델링(SLM)은 참조 모델과 비교할 때 높은 초과 손실을 나타내는 토큰에 초점을 맞추어 언어 모델을 훈련시킨다. 토큰 <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.9.m9.1"><semantics id="S2.SS2.SSS0.Px3.p3.9.m9.1a"><msub id="S2.SS2.SSS0.Px3.p3.9.m9.1.1" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.9.m9.1b"><apply id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.9.m9.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.9.m9.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>에 대한 초과 손실(<math alttext="\mathcal{L}_{\Delta}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.8.m8.1"><semantics id="S2.SS2.SSS0.Px3.p3.8.m8.1a"><msub id="S2.SS2.SSS0.Px3.p3.8.m8.1.1" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2.cmml">ℒ</mi><mi id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3" mathvariant="normal" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3.cmml">Δ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.8.m8.1b"><apply id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3">Δ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.8.m8.1c">\mathcal{L}_{\Delta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.8.m8.1d">caligraphic_L start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT</annotation></semantics></math>)은 현재 훈련 모델 손실(<math alttext="\mathcal{L}_{\theta}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.10.m10.1"><semantics id="S2.SS2.SSS0.Px3.p3.10.m10.1a"><msub id="S2.SS2.SSS0.Px3.p3.10.m10.1.1" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2.cmml">ℒ</mi><mi id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.10.m10.1b"><apply id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.10.m10.1c">\mathcal{L}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.10.m10.1d">caligraphic_L start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>)과 기준 손실의 차이로 정의된다:</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p4">
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\Delta}(x_{i})=\mathcal{L}_{\theta}(x_{i})-\mathcal{L}_{\text{ref%
}}(x_{i})" class="ltx_Math" display="block" id="S2.E3.m1.3"><semantics id="S2.E3.m1.3a"><mrow id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.3" xref="S2.E3.m1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.3.2.cmml">ℒ</mi><mi id="S2.E3.m1.1.1.1.3.3" mathvariant="normal" xref="S2.E3.m1.1.1.1.3.3.cmml">Δ</mi></msub><mo id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mo id="S2.E3.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E3.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.3.3.4" xref="S2.E3.m1.3.3.4.cmml">=</mo><mrow id="S2.E3.m1.3.3.3" xref="S2.E3.m1.3.3.3.cmml"><mrow id="S2.E3.m1.2.2.2.1" xref="S2.E3.m1.2.2.2.1.cmml"><msub id="S2.E3.m1.2.2.2.1.3" xref="S2.E3.m1.2.2.2.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.2.2.2.1.3.2" xref="S2.E3.m1.2.2.2.1.3.2.cmml">ℒ</mi><mi id="S2.E3.m1.2.2.2.1.3.3" xref="S2.E3.m1.2.2.2.1.3.3.cmml">θ</mi></msub><mo id="S2.E3.m1.2.2.2.1.2" xref="S2.E3.m1.2.2.2.1.2.cmml">⁢</mo><mrow id="S2.E3.m1.2.2.2.1.1.1" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml"><mo id="S2.E3.m1.2.2.2.1.1.1.2" stretchy="false" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml">(</mo><msub id="S2.E3.m1.2.2.2.1.1.1.1" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml"><mi id="S2.E3.m1.2.2.2.1.1.1.1.2" xref="S2.E3.m1.2.2.2.1.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.2.2.2.1.1.1.1.3" xref="S2.E3.m1.2.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E3.m1.2.2.2.1.1.1.3" stretchy="false" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.3.3.3.3" xref="S2.E3.m1.3.3.3.3.cmml">−</mo><mrow id="S2.E3.m1.3.3.3.2" xref="S2.E3.m1.3.3.3.2.cmml"><msub id="S2.E3.m1.3.3.3.2.3" xref="S2.E3.m1.3.3.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.3.3.3.2.3.2" xref="S2.E3.m1.3.3.3.2.3.2.cmml">ℒ</mi><mtext id="S2.E3.m1.3.3.3.2.3.3" xref="S2.E3.m1.3.3.3.2.3.3a.cmml">ref</mtext></msub><mo id="S2.E3.m1.3.3.3.2.2" xref="S2.E3.m1.3.3.3.2.2.cmml">⁢</mo><mrow id="S2.E3.m1.3.3.3.2.1.1" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml"><mo id="S2.E3.m1.3.3.3.2.1.1.2" stretchy="false" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml">(</mo><msub id="S2.E3.m1.3.3.3.2.1.1.1" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml"><mi id="S2.E3.m1.3.3.3.2.1.1.1.2" xref="S2.E3.m1.3.3.3.2.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.3.3.3.2.1.1.1.3" xref="S2.E3.m1.3.3.3.2.1.1.1.3.cmml">i</mi></msub><mo id="S2.E3.m1.3.3.3.2.1.1.3" stretchy="false" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.3b"><apply id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3"><eq id="S2.E3.m1.3.3.4.cmml" xref="S2.E3.m1.3.3.4"></eq><apply id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><times id="S2.E3.m1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.3.2">ℒ</ci><ci id="S2.E3.m1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.3.3">Δ</ci></apply><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E3.m1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E3.m1.3.3.3.cmml" xref="S2.E3.m1.3.3.3"><minus id="S2.E3.m1.3.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3"></minus><apply id="S2.E3.m1.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.1"><times id="S2.E3.m1.2.2.2.1.2.cmml" xref="S2.E3.m1.2.2.2.1.2"></times><apply id="S2.E3.m1.2.2.2.1.3.cmml" xref="S2.E3.m1.2.2.2.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.1.3.1.cmml" xref="S2.E3.m1.2.2.2.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.2.1.3.2.cmml" xref="S2.E3.m1.2.2.2.1.3.2">ℒ</ci><ci id="S2.E3.m1.2.2.2.1.3.3.cmml" xref="S2.E3.m1.2.2.2.1.3.3">𝜃</ci></apply><apply id="S2.E3.m1.2.2.2.1.1.1.1.cmml" xref="S2.E3.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.2.1.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.2.1.1.1.1.2">𝑥</ci><ci id="S2.E3.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.2.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E3.m1.3.3.3.2.cmml" xref="S2.E3.m1.3.3.3.2"><times id="S2.E3.m1.3.3.3.2.2.cmml" xref="S2.E3.m1.3.3.3.2.2"></times><apply id="S2.E3.m1.3.3.3.2.3.cmml" xref="S2.E3.m1.3.3.3.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.2.3.1.cmml" xref="S2.E3.m1.3.3.3.2.3">subscript</csymbol><ci id="S2.E3.m1.3.3.3.2.3.2.cmml" xref="S2.E3.m1.3.3.3.2.3.2">ℒ</ci><ci id="S2.E3.m1.3.3.3.2.3.3a.cmml" xref="S2.E3.m1.3.3.3.2.3.3"><mtext id="S2.E3.m1.3.3.3.2.3.3.cmml" mathsize="70%" xref="S2.E3.m1.3.3.3.2.3.3">ref</mtext></ci></apply><apply id="S2.E3.m1.3.3.3.2.1.1.1.cmml" xref="S2.E3.m1.3.3.3.2.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.2.1.1.1.1.cmml" xref="S2.E3.m1.3.3.3.2.1.1">subscript</csymbol><ci id="S2.E3.m1.3.3.3.2.1.1.1.2.cmml" xref="S2.E3.m1.3.3.3.2.1.1.1.2">𝑥</ci><ci id="S2.E3.m1.3.3.3.2.1.1.1.3.cmml" xref="S2.E3.m1.3.3.3.2.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.3c">\mathcal{L}_{\Delta}(x_{i})=\mathcal{L}_{\theta}(x_{i})-\mathcal{L}_{\text{ref%
}}(x_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.3d">caligraphic_L start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = caligraphic_L start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - caligraphic_L start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p5">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p5.1">우리는 토큰 선택 비율 <math alttext="k\%" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p5.1.m1.1"><semantics id="S2.SS2.SSS0.Px3.p5.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p5.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p5.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p5.1.m1.1c">k\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p5.1.m1.1d">italic_k %</annotation></semantics></math>를 도입하는데, 이것은 그들의 초과 손실에 기초하여 포함될 토큰의 비율을 결정한다. 선택된 토큰들에 대한 교차 엔트로피 손실은 다음과 같이 계산된다:</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p6">
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{SLM}}(\theta)=-\frac{1}{N*k\%}\sum_{i=1}^{N}I_{k\%}(x_{i})%
\cdot\log P(x_{i}|x_{<i};\theta)" class="ltx_Math" display="block" id="S2.E4.m1.4"><semantics id="S2.E4.m1.4a"><mrow id="S2.E4.m1.4.4" xref="S2.E4.m1.4.4.cmml"><mrow id="S2.E4.m1.4.4.4" xref="S2.E4.m1.4.4.4.cmml"><msub id="S2.E4.m1.4.4.4.2" xref="S2.E4.m1.4.4.4.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.4.4.4.2.2" xref="S2.E4.m1.4.4.4.2.2.cmml">ℒ</mi><mtext id="S2.E4.m1.4.4.4.2.3" xref="S2.E4.m1.4.4.4.2.3a.cmml">SLM</mtext></msub><mo id="S2.E4.m1.4.4.4.1" xref="S2.E4.m1.4.4.4.1.cmml">⁢</mo><mrow id="S2.E4.m1.4.4.4.3.2" xref="S2.E4.m1.4.4.4.cmml"><mo id="S2.E4.m1.4.4.4.3.2.1" stretchy="false" xref="S2.E4.m1.4.4.4.cmml">(</mo><mi id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">θ</mi><mo id="S2.E4.m1.4.4.4.3.2.2" stretchy="false" xref="S2.E4.m1.4.4.4.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.4.4.3" xref="S2.E4.m1.4.4.3.cmml">=</mo><mrow id="S2.E4.m1.4.4.2" xref="S2.E4.m1.4.4.2.cmml"><mo id="S2.E4.m1.4.4.2a" xref="S2.E4.m1.4.4.2.cmml">−</mo><mrow id="S2.E4.m1.4.4.2.2" xref="S2.E4.m1.4.4.2.2.cmml"><mfrac id="S2.E4.m1.4.4.2.2.4" xref="S2.E4.m1.4.4.2.2.4.cmml"><mn id="S2.E4.m1.4.4.2.2.4.2" xref="S2.E4.m1.4.4.2.2.4.2.cmml">1</mn><mrow id="S2.E4.m1.4.4.2.2.4.3" xref="S2.E4.m1.4.4.2.2.4.3.cmml"><mi id="S2.E4.m1.4.4.2.2.4.3.2" xref="S2.E4.m1.4.4.2.2.4.3.2.cmml">N</mi><mo id="S2.E4.m1.4.4.2.2.4.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E4.m1.4.4.2.2.4.3.1.cmml">*</mo><mrow id="S2.E4.m1.4.4.2.2.4.3.3" xref="S2.E4.m1.4.4.2.2.4.3.3.cmml"><mi id="S2.E4.m1.4.4.2.2.4.3.3.2" xref="S2.E4.m1.4.4.2.2.4.3.3.2.cmml">k</mi><mo id="S2.E4.m1.4.4.2.2.4.3.3.1" xref="S2.E4.m1.4.4.2.2.4.3.3.1.cmml">%</mo></mrow></mrow></mfrac><mo id="S2.E4.m1.4.4.2.2.3" xref="S2.E4.m1.4.4.2.2.3.cmml">⁢</mo><mrow id="S2.E4.m1.4.4.2.2.2" xref="S2.E4.m1.4.4.2.2.2.cmml"><munderover id="S2.E4.m1.4.4.2.2.2.3" xref="S2.E4.m1.4.4.2.2.2.3.cmml"><mo id="S2.E4.m1.4.4.2.2.2.3.2.2" movablelimits="false" xref="S2.E4.m1.4.4.2.2.2.3.2.2.cmml">∑</mo><mrow id="S2.E4.m1.4.4.2.2.2.3.2.3" xref="S2.E4.m1.4.4.2.2.2.3.2.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.3.2.3.2" xref="S2.E4.m1.4.4.2.2.2.3.2.3.2.cmml">i</mi><mo id="S2.E4.m1.4.4.2.2.2.3.2.3.1" xref="S2.E4.m1.4.4.2.2.2.3.2.3.1.cmml">=</mo><mn id="S2.E4.m1.4.4.2.2.2.3.2.3.3" xref="S2.E4.m1.4.4.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S2.E4.m1.4.4.2.2.2.3.3" xref="S2.E4.m1.4.4.2.2.2.3.3.cmml">N</mi></munderover><mrow id="S2.E4.m1.4.4.2.2.2.2" xref="S2.E4.m1.4.4.2.2.2.2.cmml"><mrow id="S2.E4.m1.3.3.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.cmml"><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S2.E4.m1.3.3.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml">I</mi><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2.cmml">k</mi><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1.cmml">%</mo></mrow></msub><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.3.3.1.1.1.1.1.2" rspace="0.222em" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml">⋅</mo><mrow id="S2.E4.m1.3.3.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.3.1" xref="S2.E4.m1.3.3.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.E4.m1.3.3.1.1.1.1.1.3a" lspace="0.167em" xref="S2.E4.m1.3.3.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.E4.m1.3.3.1.1.1.1.1.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.3.2.cmml">P</mi></mrow></mrow><mo id="S2.E4.m1.4.4.2.2.2.2.3" xref="S2.E4.m1.4.4.2.2.2.2.3.cmml">⁢</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml"><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.2" stretchy="false" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml">(</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml"><msub id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2.cmml">x</mi><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.2.cmml">|</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml"><msub id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml">;</mo><mi id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">θ</mi></mrow></mrow><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.3" stretchy="false" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.4b"><apply id="S2.E4.m1.4.4.cmml" xref="S2.E4.m1.4.4"><eq id="S2.E4.m1.4.4.3.cmml" xref="S2.E4.m1.4.4.3"></eq><apply id="S2.E4.m1.4.4.4.cmml" xref="S2.E4.m1.4.4.4"><times id="S2.E4.m1.4.4.4.1.cmml" xref="S2.E4.m1.4.4.4.1"></times><apply id="S2.E4.m1.4.4.4.2.cmml" xref="S2.E4.m1.4.4.4.2"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.4.2.1.cmml" xref="S2.E4.m1.4.4.4.2">subscript</csymbol><ci id="S2.E4.m1.4.4.4.2.2.cmml" xref="S2.E4.m1.4.4.4.2.2">ℒ</ci><ci id="S2.E4.m1.4.4.4.2.3a.cmml" xref="S2.E4.m1.4.4.4.2.3"><mtext id="S2.E4.m1.4.4.4.2.3.cmml" mathsize="70%" xref="S2.E4.m1.4.4.4.2.3">SLM</mtext></ci></apply><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">𝜃</ci></apply><apply id="S2.E4.m1.4.4.2.cmml" xref="S2.E4.m1.4.4.2"><minus id="S2.E4.m1.4.4.2.3.cmml" xref="S2.E4.m1.4.4.2"></minus><apply id="S2.E4.m1.4.4.2.2.cmml" xref="S2.E4.m1.4.4.2.2"><times id="S2.E4.m1.4.4.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.3"></times><apply id="S2.E4.m1.4.4.2.2.4.cmml" xref="S2.E4.m1.4.4.2.2.4"><divide id="S2.E4.m1.4.4.2.2.4.1.cmml" xref="S2.E4.m1.4.4.2.2.4"></divide><cn id="S2.E4.m1.4.4.2.2.4.2.cmml" type="integer" xref="S2.E4.m1.4.4.2.2.4.2">1</cn><apply id="S2.E4.m1.4.4.2.2.4.3.cmml" xref="S2.E4.m1.4.4.2.2.4.3"><times id="S2.E4.m1.4.4.2.2.4.3.1.cmml" xref="S2.E4.m1.4.4.2.2.4.3.1"></times><ci id="S2.E4.m1.4.4.2.2.4.3.2.cmml" xref="S2.E4.m1.4.4.2.2.4.3.2">𝑁</ci><apply id="S2.E4.m1.4.4.2.2.4.3.3.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3"><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.4.3.3.1.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3.1">percent</csymbol><ci id="S2.E4.m1.4.4.2.2.4.3.3.2.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3.2">𝑘</ci></apply></apply></apply><apply id="S2.E4.m1.4.4.2.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2"><apply id="S2.E4.m1.4.4.2.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3">superscript</csymbol><apply id="S2.E4.m1.4.4.2.2.2.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.3.2.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3">subscript</csymbol><sum id="S2.E4.m1.4.4.2.2.2.3.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.2"></sum><apply id="S2.E4.m1.4.4.2.2.2.3.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3"><eq id="S2.E4.m1.4.4.2.2.2.3.2.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3.1"></eq><ci id="S2.E4.m1.4.4.2.2.2.3.2.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3.2">𝑖</ci><cn id="S2.E4.m1.4.4.2.2.2.3.2.3.3.cmml" type="integer" xref="S2.E4.m1.4.4.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.4.4.2.2.2.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3.3">𝑁</ci></apply><apply id="S2.E4.m1.4.4.2.2.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2"><times id="S2.E4.m1.4.4.2.2.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.3"></times><apply id="S2.E4.m1.3.3.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1"><ci id="S2.E4.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.2">⋅</ci><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1"><times id="S2.E4.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.2"></times><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.2">𝐼</ci><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3"><csymbol cd="latexml" id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1">percent</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2">𝑘</ci></apply></apply><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E4.m1.3.3.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3"><log id="S2.E4.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3.1"></log><ci id="S2.E4.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3.2">𝑃</ci></apply></apply><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1"><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.2">conditional</csymbol><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3">subscript</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2">𝑥</ci><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3">𝑖</ci></apply><list id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1"><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2">𝑥</ci><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3"><lt id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">𝜃</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.4c">\mathcal{L}_{\text{SLM}}(\theta)=-\frac{1}{N*k\%}\sum_{i=1}^{N}I_{k\%}(x_{i})%
\cdot\log P(x_{i}|x_{&lt;i};\theta)</annotation><annotation encoding="application/x-llamapun" id="S2.E4.m1.4d">caligraphic_L start_POSTSUBSCRIPT SLM end_POSTSUBSCRIPT ( italic_θ ) = - divide start_ARG 1 end_ARG start_ARG italic_N * italic_k % end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT italic_k % end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ⋅ roman_log italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ; italic_θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p7">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p7.3">Here, <math alttext="N*k\%" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p7.1.m1.1"><semantics id="S2.SS2.SSS0.Px3.p7.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p7.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2.cmml">N</mi><mo id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1.cmml">*</mo><mrow id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1"><times id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1"></times><ci id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2">𝑁</ci><apply id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.1.m1.1c">N*k\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p7.1.m1.1d">italic_N * italic_k %</annotation></semantics></math> defines the number of tokens that fall within the top <math alttext="k\%" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p7.2.m2.1"><semantics id="S2.SS2.SSS0.Px3.p7.2.m2.1a"><mrow id="S2.SS2.SSS0.Px3.p7.2.m2.1.1" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.2.m2.1b"><apply id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.2.m2.1c">k\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p7.2.m2.1d">italic_k %</annotation></semantics></math> of excess loss. The indicator function <math alttext="I_{k\%}(x_{i})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p7.3.m3.1"><semantics id="S2.SS2.SSS0.Px3.p7.3.m3.1a"><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.cmml"><msub id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2.cmml">I</mi><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1.cmml">%</mo></mrow></msub><mo id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2.cmml">⁢</mo><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml"><mo id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.2" stretchy="false" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.3" stretchy="false" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.3.m3.1b"><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1"><times id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2"></times><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2">𝐼</ci><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2">𝑘</ci></apply></apply><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.3.m3.1c">I_{k\%}(x_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p7.3.m3.1d">italic_I start_POSTSUBSCRIPT italic_k % end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> is defined as:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p8">
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I_{k\%}(x_{i})=\begin{cases}1&amp;\text{if }x_{i}\text{ is in the top }k\%\text{ %
of }\mathcal{L}_{\Delta}\\
0&amp;\text{otherwise}\end{cases}" class="ltx_Math" display="block" id="S2.E5.m1.5"><semantics id="S2.E5.m1.5a"><mrow id="S2.E5.m1.5.5" xref="S2.E5.m1.5.5.cmml"><mrow id="S2.E5.m1.5.5.1" xref="S2.E5.m1.5.5.1.cmml"><msub id="S2.E5.m1.5.5.1.3" xref="S2.E5.m1.5.5.1.3.cmml"><mi id="S2.E5.m1.5.5.1.3.2" xref="S2.E5.m1.5.5.1.3.2.cmml">I</mi><mrow id="S2.E5.m1.5.5.1.3.3" xref="S2.E5.m1.5.5.1.3.3.cmml"><mi id="S2.E5.m1.5.5.1.3.3.2" xref="S2.E5.m1.5.5.1.3.3.2.cmml">k</mi><mo id="S2.E5.m1.5.5.1.3.3.1" xref="S2.E5.m1.5.5.1.3.3.1.cmml">%</mo></mrow></msub><mo id="S2.E5.m1.5.5.1.2" xref="S2.E5.m1.5.5.1.2.cmml">⁢</mo><mrow id="S2.E5.m1.5.5.1.1.1" xref="S2.E5.m1.5.5.1.1.1.1.cmml"><mo id="S2.E5.m1.5.5.1.1.1.2" stretchy="false" xref="S2.E5.m1.5.5.1.1.1.1.cmml">(</mo><msub id="S2.E5.m1.5.5.1.1.1.1" xref="S2.E5.m1.5.5.1.1.1.1.cmml"><mi id="S2.E5.m1.5.5.1.1.1.1.2" xref="S2.E5.m1.5.5.1.1.1.1.2.cmml">x</mi><mi id="S2.E5.m1.5.5.1.1.1.1.3" xref="S2.E5.m1.5.5.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E5.m1.5.5.1.1.1.3" stretchy="false" xref="S2.E5.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.5.5.2" xref="S2.E5.m1.5.5.2.cmml">=</mo><mrow id="S2.E5.m1.4.4" xref="S2.E5.m1.5.5.3.1.cmml"><mo id="S2.E5.m1.4.4.5" xref="S2.E5.m1.5.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E5.m1.4.4.4" rowspacing="0pt" xref="S2.E5.m1.5.5.3.1.cmml"><mtr id="S2.E5.m1.4.4.4a" xref="S2.E5.m1.5.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4b" xref="S2.E5.m1.5.5.3.1.cmml"><mn id="S2.E5.m1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4c" xref="S2.E5.m1.5.5.3.1.cmml"><mrow id="S2.E5.m1.2.2.2.2.2.1" xref="S2.E5.m1.2.2.2.2.2.1.cmml"><mtext id="S2.E5.m1.2.2.2.2.2.1.2" xref="S2.E5.m1.2.2.2.2.2.1.2a.cmml">if&nbsp;</mtext><mo id="S2.E5.m1.2.2.2.2.2.1.1" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">⁢</mo><msub id="S2.E5.m1.2.2.2.2.2.1.3" xref="S2.E5.m1.2.2.2.2.2.1.3.cmml"><mi id="S2.E5.m1.2.2.2.2.2.1.3.2" xref="S2.E5.m1.2.2.2.2.2.1.3.2.cmml">x</mi><mi id="S2.E5.m1.2.2.2.2.2.1.3.3" xref="S2.E5.m1.2.2.2.2.2.1.3.3.cmml">i</mi></msub><mo id="S2.E5.m1.2.2.2.2.2.1.1a" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">⁢</mo><mtext id="S2.E5.m1.2.2.2.2.2.1.4" xref="S2.E5.m1.2.2.2.2.2.1.4a.cmml">&nbsp;is in the top&nbsp;</mtext><mo id="S2.E5.m1.2.2.2.2.2.1.1b" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">⁢</mo><mrow id="S2.E5.m1.2.2.2.2.2.1.5" xref="S2.E5.m1.2.2.2.2.2.1.5.cmml"><mi id="S2.E5.m1.2.2.2.2.2.1.5.2" xref="S2.E5.m1.2.2.2.2.2.1.5.2.cmml">k</mi><mo id="S2.E5.m1.2.2.2.2.2.1.5.1" xref="S2.E5.m1.2.2.2.2.2.1.5.1.cmml">%</mo></mrow><mo id="S2.E5.m1.2.2.2.2.2.1.1c" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">⁢</mo><mtext id="S2.E5.m1.2.2.2.2.2.1.6" xref="S2.E5.m1.2.2.2.2.2.1.6a.cmml">&nbsp;of&nbsp;</mtext><mo id="S2.E5.m1.2.2.2.2.2.1.1d" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">⁢</mo><msub id="S2.E5.m1.2.2.2.2.2.1.7" xref="S2.E5.m1.2.2.2.2.2.1.7.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.2.2.2.2.2.1.7.2" xref="S2.E5.m1.2.2.2.2.2.1.7.2.cmml">ℒ</mi><mi id="S2.E5.m1.2.2.2.2.2.1.7.3" mathvariant="normal" xref="S2.E5.m1.2.2.2.2.2.1.7.3.cmml">Δ</mi></msub></mrow></mtd></mtr><mtr id="S2.E5.m1.4.4.4d" xref="S2.E5.m1.5.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4e" xref="S2.E5.m1.5.5.3.1.cmml"><mn id="S2.E5.m1.3.3.3.3.1.1" xref="S2.E5.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4f" xref="S2.E5.m1.5.5.3.1.cmml"><mtext id="S2.E5.m1.4.4.4.4.2.1" xref="S2.E5.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.5b"><apply id="S2.E5.m1.5.5.cmml" xref="S2.E5.m1.5.5"><eq id="S2.E5.m1.5.5.2.cmml" xref="S2.E5.m1.5.5.2"></eq><apply id="S2.E5.m1.5.5.1.cmml" xref="S2.E5.m1.5.5.1"><times id="S2.E5.m1.5.5.1.2.cmml" xref="S2.E5.m1.5.5.1.2"></times><apply id="S2.E5.m1.5.5.1.3.cmml" xref="S2.E5.m1.5.5.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.1.3.1.cmml" xref="S2.E5.m1.5.5.1.3">subscript</csymbol><ci id="S2.E5.m1.5.5.1.3.2.cmml" xref="S2.E5.m1.5.5.1.3.2">𝐼</ci><apply id="S2.E5.m1.5.5.1.3.3.cmml" xref="S2.E5.m1.5.5.1.3.3"><csymbol cd="latexml" id="S2.E5.m1.5.5.1.3.3.1.cmml" xref="S2.E5.m1.5.5.1.3.3.1">percent</csymbol><ci id="S2.E5.m1.5.5.1.3.3.2.cmml" xref="S2.E5.m1.5.5.1.3.3.2">𝑘</ci></apply></apply><apply id="S2.E5.m1.5.5.1.1.1.1.cmml" xref="S2.E5.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.1.1.1.1.1.cmml" xref="S2.E5.m1.5.5.1.1.1">subscript</csymbol><ci id="S2.E5.m1.5.5.1.1.1.1.2.cmml" xref="S2.E5.m1.5.5.1.1.1.1.2">𝑥</ci><ci id="S2.E5.m1.5.5.1.1.1.1.3.cmml" xref="S2.E5.m1.5.5.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E5.m1.5.5.3.1.cmml" xref="S2.E5.m1.4.4"><csymbol cd="latexml" id="S2.E5.m1.5.5.3.1.1.cmml" xref="S2.E5.m1.4.4.5">cases</csymbol><cn id="S2.E5.m1.1.1.1.1.1.1.cmml" type="integer" xref="S2.E5.m1.1.1.1.1.1.1">1</cn><apply id="S2.E5.m1.2.2.2.2.2.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1"><times id="S2.E5.m1.2.2.2.2.2.1.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.1"></times><ci id="S2.E5.m1.2.2.2.2.2.1.2a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.2"><mtext id="S2.E5.m1.2.2.2.2.2.1.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.2">if&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.2.2.2.1.3.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3">subscript</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.3.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3.2">𝑥</ci><ci id="S2.E5.m1.2.2.2.2.2.1.3.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3.3">𝑖</ci></apply><ci id="S2.E5.m1.2.2.2.2.2.1.4a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.4"><mtext id="S2.E5.m1.2.2.2.2.2.1.4.cmml" xref="S2.E5.m1.2.2.2.2.2.1.4">&nbsp;is in the top&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.5.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5"><csymbol cd="latexml" id="S2.E5.m1.2.2.2.2.2.1.5.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5.1">percent</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.5.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5.2">𝑘</ci></apply><ci id="S2.E5.m1.2.2.2.2.2.1.6a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.6"><mtext id="S2.E5.m1.2.2.2.2.2.1.6.cmml" xref="S2.E5.m1.2.2.2.2.2.1.6">&nbsp;of&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.7.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.2.2.2.1.7.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7">subscript</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.7.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7.2">ℒ</ci><ci id="S2.E5.m1.2.2.2.2.2.1.7.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7.3">Δ</ci></apply></apply><cn id="S2.E5.m1.3.3.3.3.1.1.cmml" type="integer" xref="S2.E5.m1.3.3.3.3.1.1">0</cn><ci id="S2.E5.m1.4.4.4.4.2.1a.cmml" xref="S2.E5.m1.4.4.4.4.2.1"><mtext id="S2.E5.m1.4.4.4.4.2.1.cmml" xref="S2.E5.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.5c">I_{k\%}(x_{i})=\begin{cases}1&amp;\text{if }x_{i}\text{ is in the top }k\%\text{ %
of }\mathcal{L}_{\Delta}\\
0&amp;\text{otherwise}\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S2.E5.m1.5d">italic_I start_POSTSUBSCRIPT italic_k % end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = { start_ROW start_CELL 1 end_CELL start_CELL if italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is in the top italic_k % of caligraphic_L start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL otherwise end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p9">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p9.1">이는 언어 모델이 학습하는데 가장 유익하다고 여겨지는 토큰에만 손실이 적용되도록 보장한다. 실제로, 토큰 선택은 그들의 초과 손실에 따라 일괄적으로 토큰들을 랭킹하고 트레이닝을 위해 토큰들의 상위 <math alttext="k\%" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p9.1.m1.1"><semantics id="S2.SS2.SSS0.Px3.p9.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p9.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p9.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p9.1.m1.1c">k\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p9.1.m1.1d">italic_k %</annotation></semantics></math>만을 사용함으로써 구현될 수 있다. 이 프로세스는 사전 훈련 동안 추가 비용을 발생시키지 않고 원하지 않는 토큰에 대한 손실을 제거하여 우리의 접근법을 효율적이고 쉽게 통합한다.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">우리는 수학적 영역과 일반 영역 모두에서 모델을 지속적으로 사전 훈련하고 SLM의 효과를 이해하기 위해 절제 및 분석 실험을 설계했다.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Setup</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Reference Model Training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">수학적 참조 모델을 훈련하기 위해 0.5B 고품질 수학 관련 토큰 데이터 세트를 수집했다. 이 데이터 세트는 GPT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Huang et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>의 합성 데이터와 수동으로 선별된 데이터 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yue et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Ni et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>의 혼합이다. 일반 참조 모델의 경우 Tulu-v2 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ivison et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> 및 OpenHermes-2.5 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Teknium</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>와 같은 오픈 소스 데이터 세트에서 1.9B 토큰의 코퍼스를 컴파일했다. 우리는 3개의 에포크에 대한 참조 모델을 훈련시켰다. 최대 학습률은 코사인 감쇠 스케줄을 적용하여 1B 모델의 경우 5e-5, 7B 모델의 경우 1e-5로 설정하였다. 최대 시퀀스 길이를 1B 모델의 경우 2048, 7B 모델의 경우 4096으로 설정하여 모델 입력을 위해 여러 샘플을 이러한 길이로 패킹했다. 모든 주요 실험에서 우리는 <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.1">same</em> 기본 모델로 연속 사전 훈련 모델과 참조 모델을 초기화했다.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.19.4.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.6.3" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.T1.6.3.1">Few-shot CoT reasoning results of math pretraining.</span> All models are tested with few-shot prompting. Previous best results are highlighted in <span class="ltx_text" id="S3.T1.6.3.2" style="color:#1E90FF;">blue</span>, while our best results are in <span class="ltx_text" id="S3.T1.6.3.3" style="color:#BF0040;">purple</span>. <math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T1.4.1.m1.1"><semantics id="S3.T1.4.1.m1.1b"><msup id="S3.T1.4.1.m1.1.1" xref="S3.T1.4.1.m1.1.1.cmml"><mi id="S3.T1.4.1.m1.1.1b" xref="S3.T1.4.1.m1.1.1.cmml"></mi><mo id="S3.T1.4.1.m1.1.1.1" xref="S3.T1.4.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.4.1.m1.1c"><apply id="S3.T1.4.1.m1.1.1.cmml" xref="S3.T1.4.1.m1.1.1"><times id="S3.T1.4.1.m1.1.1.1.cmml" xref="S3.T1.4.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.1.m1.1d">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.1.m1.1e">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>Only unique math-related tokens are calculated. For <span class="ltx_text ltx_font_smallcaps" id="S3.T1.6.3.4">Rho-1</span>, we calculate only the selected tokens that are used for training. <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.5.2.m2.1"><semantics id="S3.T1.5.2.m2.1b"><msup id="S3.T1.5.2.m2.1.1" xref="S3.T1.5.2.m2.1.1.cmml"><mi id="S3.T1.5.2.m2.1.1b" xref="S3.T1.5.2.m2.1.1.cmml"></mi><mo id="S3.T1.5.2.m2.1.1.1" xref="S3.T1.5.2.m2.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.5.2.m2.1c"><apply id="S3.T1.5.2.m2.1.1.cmml" xref="S3.T1.5.2.m2.1.1"><ci id="S3.T1.5.2.m2.1.1.1.cmml" xref="S3.T1.5.2.m2.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.2.m2.1d">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.2.m2.1e">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>We use OpenAI’s MATH subset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lightman et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> for evaluation, since some original test samples have been used in public training sets such as PRM800k. <math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T1.6.3.m3.1"><semantics id="S3.T1.6.3.m3.1b"><msup id="S3.T1.6.3.m3.1.1" xref="S3.T1.6.3.m3.1.1.cmml"><mi id="S3.T1.6.3.m3.1.1b" xref="S3.T1.6.3.m3.1.1.cmml"></mi><mo id="S3.T1.6.3.m3.1.1.1" xref="S3.T1.6.3.m3.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.6.3.m3.1c"><apply id="S3.T1.6.3.m3.1.1.cmml" xref="S3.T1.6.3.m3.1.1"><ci id="S3.T1.6.3.m3.1.1.1.cmml" xref="S3.T1.6.3.m3.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.3.m3.1d">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.3.m3.1e">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math>The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.
</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.13" style="width:433.6pt;height:484.6pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.6pt,21.9pt) scale(0.917124534204067,0.917124534204067) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.13.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.10.4.4">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.10.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.5.1">Model</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S3.T1.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="|\bm{\theta}|" class="ltx_Math" display="inline" id="S3.T1.7.1.1.1.m1.1"><semantics id="S3.T1.7.1.1.1.m1.1a"><mrow id="S3.T1.7.1.1.1.m1.1.2.2" xref="S3.T1.7.1.1.1.m1.1.2.1.cmml"><mo id="S3.T1.7.1.1.1.m1.1.2.2.1" stretchy="false" xref="S3.T1.7.1.1.1.m1.1.2.1.1.cmml">|</mo><mi id="S3.T1.7.1.1.1.m1.1.1" xref="S3.T1.7.1.1.1.m1.1.1.cmml">𝜽</mi><mo id="S3.T1.7.1.1.1.m1.1.2.2.2" stretchy="false" xref="S3.T1.7.1.1.1.m1.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.1.1.1.m1.1b"><apply id="S3.T1.7.1.1.1.m1.1.2.1.cmml" xref="S3.T1.7.1.1.1.m1.1.2.2"><abs id="S3.T1.7.1.1.1.m1.1.2.1.1.cmml" xref="S3.T1.7.1.1.1.m1.1.2.2.1"></abs><ci id="S3.T1.7.1.1.1.m1.1.1.cmml" xref="S3.T1.7.1.1.1.m1.1.1">𝜽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.1.1.1.m1.1c">|\bm{\theta}|</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.1.1.1.m1.1d">| bold_italic_θ |</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.6.1">Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.8.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.8.2.2.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.8.2.2.2.1.2">
<td class="ltx_td ltx_align_center" id="S3.T1.8.2.2.2.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.8.2.2.2.1.2.1.1">Uniq.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.8.2.2.2.1.1">
<td class="ltx_td ltx_align_center" id="S3.T1.8.2.2.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.8.2.2.2.1.1.1.1">Toks</span><math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T1.8.2.2.2.1.1.1.m1.1"><semantics id="S3.T1.8.2.2.2.1.1.1.m1.1a"><msup id="S3.T1.8.2.2.2.1.1.1.m1.1.1" xref="S3.T1.8.2.2.2.1.1.1.m1.1.1.cmml"><mi id="S3.T1.8.2.2.2.1.1.1.m1.1.1a" xref="S3.T1.8.2.2.2.1.1.1.m1.1.1.cmml"></mi><mo id="S3.T1.8.2.2.2.1.1.1.m1.1.1.1" xref="S3.T1.8.2.2.2.1.1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.8.2.2.2.1.1.1.m1.1b"><apply id="S3.T1.8.2.2.2.1.1.1.m1.1.1.cmml" xref="S3.T1.8.2.2.2.1.1.1.m1.1.1"><times id="S3.T1.8.2.2.2.1.1.1.m1.1.1.1.cmml" xref="S3.T1.8.2.2.2.1.1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.2.2.2.1.1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.2.2.2.1.1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.10.4.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.10.4.4.7.1">
<tbody><tr class="ltx_tr" id="S3.T1.10.4.4.7.1.1">
<td class="ltx_td ltx_align_center" id="S3.T1.10.4.4.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.7.1.1.1.1">Train</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.10.4.4.7.1.2">
<td class="ltx_td ltx_align_center" id="S3.T1.10.4.4.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.7.1.2.1.1">Toks</span></td>
</tr>
</tbody></table></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.8.1">GSM8K</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.9.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.9.3.3.3.1">MATH<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.9.3.3.3.1.m1.1"><semantics id="S3.T1.9.3.3.3.1.m1.1a"><msup id="S3.T1.9.3.3.3.1.m1.1.1" xref="S3.T1.9.3.3.3.1.m1.1.1.cmml"><mi id="S3.T1.9.3.3.3.1.m1.1.1a" xref="S3.T1.9.3.3.3.1.m1.1.1.cmml"></mi><mo id="S3.T1.9.3.3.3.1.m1.1.1.1" mathvariant="normal" xref="S3.T1.9.3.3.3.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.9.3.3.3.1.m1.1b"><apply id="S3.T1.9.3.3.3.1.m1.1.1.cmml" xref="S3.T1.9.3.3.3.1.m1.1.1"><ci id="S3.T1.9.3.3.3.1.m1.1.1.1.cmml" xref="S3.T1.9.3.3.3.1.m1.1.1.1">normal-†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.3.3.3.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.3.3.3.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.9.1">SVAMP</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.10.1">ASDiv</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.11.1">MAWPS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.12.1">TAB</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.13.1">MQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.14" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.10.4.4.14.1">
<tbody><tr class="ltx_tr" id="S3.T1.10.4.4.14.1.1">
<td class="ltx_td ltx_align_center" id="S3.T1.10.4.4.14.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.14.1.1.1.1">MMLU</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.10.4.4.14.1.2">
<td class="ltx_td ltx_align_center" id="S3.T1.10.4.4.14.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.14.1.2.1.1">STEM</span></td>
</tr>
</tbody></table></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.10.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.4.1">SAT<math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T1.10.4.4.4.1.m1.1"><semantics id="S3.T1.10.4.4.4.1.m1.1a"><msup id="S3.T1.10.4.4.4.1.m1.1.1" xref="S3.T1.10.4.4.4.1.m1.1.1.cmml"><mi id="S3.T1.10.4.4.4.1.m1.1.1a" xref="S3.T1.10.4.4.4.1.m1.1.1.cmml"></mi><mo id="S3.T1.10.4.4.4.1.m1.1.1.1" mathvariant="normal" xref="S3.T1.10.4.4.4.1.m1.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.10.4.4.4.1.m1.1b"><apply id="S3.T1.10.4.4.4.1.m1.1.1.cmml" xref="S3.T1.10.4.4.4.1.m1.1.1"><ci id="S3.T1.10.4.4.4.1.m1.1.1.1.cmml" xref="S3.T1.10.4.4.4.1.m1.1.1.1">normal-‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.4.4.4.1.m1.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.10.4.4.4.1.m1.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.15.1">AVG</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.8.1">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="15" id="S3.T1.13.7.8.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_typewriter" id="S3.T1.13.7.8.1.1.1">1-2B Base Models</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.9.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.13.7.9.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/Tinyllama/Tinyllama-1.1B-intermediate-step-1431k-3T" title="">Tinyllama</a></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.9.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.9.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.13.7.9.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">2.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">3.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">11.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">18.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">20.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.11" style="padding-left:2.0pt;padding-right:2.0pt;">12.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.12" style="padding-left:2.0pt;padding-right:2.0pt;">14.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.13" style="padding-left:2.0pt;padding-right:2.0pt;">16.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.13.7.9.2.14" style="padding-left:2.0pt;padding-right:2.0pt;">21.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.15" style="padding-left:2.0pt;padding-right:2.0pt;">13.4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.10.3">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.10.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/microsoft/phi-1_5" title="">Phi-1.5</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.10.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.10.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.10.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">32.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">4.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">43.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">53.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.10" style="padding-left:2.0pt;padding-right:2.0pt;">66.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.11" style="padding-left:2.0pt;padding-right:2.0pt;">24.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.12" style="padding-left:2.0pt;padding-right:2.0pt;">14.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.13" style="padding-left:2.0pt;padding-right:2.0pt;">21.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.10.3.14" style="padding-left:2.0pt;padding-right:2.0pt;">18.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.15" style="padding-left:2.0pt;padding-right:2.0pt;">31.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.11.4">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.11.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/Qwen/Qwen1.5-1.8B" title="">Qwen1.5</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.11.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.8B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.11.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.11.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.11.4.6.1" style="color:#1E90FF;">36.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">6.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.11.4.8.1" style="color:#1E90FF;">48.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.11.4.9.1" style="color:#1E90FF;">63.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.11.4.10.1" style="color:#1E90FF;">79.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.11" style="padding-left:2.0pt;padding-right:2.0pt;">29.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.12" style="padding-left:2.0pt;padding-right:2.0pt;">25.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.13" style="padding-left:2.0pt;padding-right:2.0pt;">31.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.11.4.14" style="padding-left:2.0pt;padding-right:2.0pt;">40.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.11.4.15.1" style="color:#1E90FF;">40.0</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.12.5">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.12.5.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/google/gemma-2b" title="">Gemma</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.12.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">2.0B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.12.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.12.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">18.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">11.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">38.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.9" style="padding-left:2.0pt;padding-right:2.0pt;">56.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.10" style="padding-left:2.0pt;padding-right:2.0pt;">72.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.12.5.11.1" style="color:#1E90FF;">36.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.12.5.12.1" style="color:#1E90FF;">26.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.12.5.13.1" style="color:#1E90FF;">34.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.12.5.14" style="padding-left:2.0pt;padding-right:2.0pt;">50.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.15" style="padding-left:2.0pt;padding-right:2.0pt;">38.4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.13.6">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.13.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">DeepSeekLLM</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.13.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.13.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.13.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">150B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">11.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">8.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.13" style="padding-left:2.0pt;padding-right:2.0pt;">29.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.13.6.14" style="padding-left:2.0pt;padding-right:2.0pt;">31.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.14.7">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.14.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">DeepSeekMath</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.14.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.14.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">120B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.14.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">150B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.6" style="padding-left:2.0pt;padding-right:2.0pt;">23.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.14.7.7.1" style="color:#1E90FF;">13.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.13" style="padding-left:2.0pt;padding-right:2.0pt;">33.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.14.7.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.14.7.14.1" style="color:#1E90FF;">56.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.15.8">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="15" id="S3.T1.13.7.15.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_typewriter" id="S3.T1.13.7.15.8.1.1">Continual Pretraining on Tinyllama-1B</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.16.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.13.7.16.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">Tinyllama-CT</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.16.9.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.16.9.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.13.7.16.9.5" style="padding-left:2.0pt;padding-right:2.0pt;">15B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.6" style="padding-left:2.0pt;padding-right:2.0pt;">6.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.7" style="padding-left:2.0pt;padding-right:2.0pt;">2.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.8" style="padding-left:2.0pt;padding-right:2.0pt;">21.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.9" style="padding-left:2.0pt;padding-right:2.0pt;">36.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.10" style="padding-left:2.0pt;padding-right:2.0pt;">47.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.11" style="padding-left:2.0pt;padding-right:2.0pt;">17.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.12" style="padding-left:2.0pt;padding-right:2.0pt;">13.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.13" style="padding-left:2.0pt;padding-right:2.0pt;">23.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.13.7.16.9.14" style="padding-left:2.0pt;padding-right:2.0pt;">25.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.15" style="padding-left:2.0pt;padding-right:2.0pt;">21.6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.17.10">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.17.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_smallcaps" id="S3.T1.13.7.17.10.1.1">Rho-1</span>-Math</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.17.10.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.17.10.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.17.10.5" style="padding-left:2.0pt;padding-right:2.0pt;">9B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.6" style="padding-left:2.0pt;padding-right:2.0pt;">29.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.7" style="padding-left:2.0pt;padding-right:2.0pt;">14.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.8" style="padding-left:2.0pt;padding-right:2.0pt;">49.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.9" style="padding-left:2.0pt;padding-right:2.0pt;">61.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.10" style="padding-left:2.0pt;padding-right:2.0pt;">79.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.11" style="padding-left:2.0pt;padding-right:2.0pt;">25.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.12" style="padding-left:2.0pt;padding-right:2.0pt;">30.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.17.10.13.1" style="color:#BF0040;">24.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.17.10.14" style="padding-left:2.0pt;padding-right:2.0pt;">28.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.15" style="padding-left:2.0pt;padding-right:2.0pt;">38.1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.11.5.5">
<td class="ltx_td ltx_align_left" id="S3.T1.11.5.5.1" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T1.11.5.5.1.m1.1"><semantics id="S3.T1.11.5.5.1.m1.1a"><mi id="S3.T1.11.5.5.1.m1.1.1" mathvariant="normal" xref="S3.T1.11.5.5.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T1.11.5.5.1.m1.1b"><ci id="S3.T1.11.5.5.1.m1.1.1.cmml" xref="S3.T1.11.5.5.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.5.5.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T1.11.5.5.1.m1.1d">roman_Δ</annotation></semantics></math></td>
<td class="ltx_td" id="S3.T1.11.5.5.2" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T1.11.5.5.3" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T1.11.5.5.4" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.11.5.5.5" style="background-color:#E6E6E6;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.5.1" style="background-color:#E6E6E6;">-40%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.6" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.6.1" style="background-color:#D2DCFA;">+23.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.7" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.7.1" style="background-color:#D2DCFA;">+11.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.8" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.8.1" style="background-color:#D2DCFA;">+27.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.9" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.9.1" style="background-color:#D2DCFA;">+24.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.10" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.10.1" style="background-color:#D2DCFA;">+32.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.11" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.11.1" style="background-color:#D2DCFA;">+7.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.12" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.12.1" style="background-color:#D2DCFA;">+16.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.13" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.13.1" style="background-color:#D2DCFA;">+1.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.11.5.5.14" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.14.1" style="background-color:#D2DCFA;">+3.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.15" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.11.5.5.15.1" style="background-color:#D2DCFA;">+16.5</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.18.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.13.7.18.11.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_smallcaps" id="S3.T1.13.7.18.11.1.1">Rho-1</span>-Math</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.18.11.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.18.11.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.13.7.18.11.5" style="padding-left:2.0pt;padding-right:2.0pt;">30B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.6.1" style="color:#BF0040;">36.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.7.1" style="color:#BF0040;">15.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.8.1" style="color:#BF0040;">52.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.9.1" style="color:#BF0040;">67.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.10.1" style="color:#BF0040;">83.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.11.1" style="color:#BF0040;">29.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.12.1" style="color:#BF0040;">32.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.13" style="padding-left:2.0pt;padding-right:2.0pt;">23.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.13.7.18.11.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.14.1" style="color:#BF0040;">28.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.15.1" style="color:#BF0040;">40.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.12.6.6">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="15" id="S3.T1.12.6.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<math alttext="\geq" class="ltx_Math" display="inline" id="S3.T1.12.6.6.1.m1.1"><semantics id="S3.T1.12.6.6.1.m1.1a"><mo id="S3.T1.12.6.6.1.m1.1.1" xref="S3.T1.12.6.6.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S3.T1.12.6.6.1.m1.1b"><geq id="S3.T1.12.6.6.1.m1.1.1.cmml" xref="S3.T1.12.6.6.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.6.6.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S3.T1.12.6.6.1.m1.1d">≥</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S3.T1.12.6.6.1.1"> 7B Base Models</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.19.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.13.7.19.12.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/meta-llama/Llama-2-7b-hf" title="">LLaMA-2</a></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.19.12.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_border_t" id="S3.T1.13.7.19.12.3" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.19.12.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.13.7.19.12.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.6" style="padding-left:2.0pt;padding-right:2.0pt;">14.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.7" style="padding-left:2.0pt;padding-right:2.0pt;">3.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.8" style="padding-left:2.0pt;padding-right:2.0pt;">39.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.9" style="padding-left:2.0pt;padding-right:2.0pt;">51.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.10" style="padding-left:2.0pt;padding-right:2.0pt;">63.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.11" style="padding-left:2.0pt;padding-right:2.0pt;">30.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.12" style="padding-left:2.0pt;padding-right:2.0pt;">12.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.13" style="padding-left:2.0pt;padding-right:2.0pt;">32.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.13.7.19.12.14" style="padding-left:2.0pt;padding-right:2.0pt;">34.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.15" style="padding-left:2.0pt;padding-right:2.0pt;">31.4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.20.13">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.20.13.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/mistralai/Mistral-7B-v0.1" title="">Mistral</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.20.13.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td" id="S3.T1.13.7.20.13.3" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.20.13.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.20.13.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.6" style="padding-left:2.0pt;padding-right:2.0pt;">41.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.7" style="padding-left:2.0pt;padding-right:2.0pt;">11.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.8" style="padding-left:2.0pt;padding-right:2.0pt;">64.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.9" style="padding-left:2.0pt;padding-right:2.0pt;">68.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.10" style="padding-left:2.0pt;padding-right:2.0pt;">87.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.11" style="padding-left:2.0pt;padding-right:2.0pt;">52.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.12" style="padding-left:2.0pt;padding-right:2.0pt;">33.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.13" style="padding-left:2.0pt;padding-right:2.0pt;">49.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.20.13.14" style="padding-left:2.0pt;padding-right:2.0pt;">59.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.15" style="padding-left:2.0pt;padding-right:2.0pt;">52.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.21.14">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.21.14.1" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.21.14.2" style="padding-left:2.0pt;padding-right:2.0pt;">8B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.21.14.4" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.21.14.5" style="padding-left:2.0pt;padding-right:2.0pt;">164B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.6" style="padding-left:2.0pt;padding-right:2.0pt;">16.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.7" style="padding-left:2.0pt;padding-right:2.0pt;">14.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.13" style="padding-left:2.0pt;padding-right:2.0pt;">35.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.21.14.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.22.15">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.22.15.1" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.22.15.2" style="padding-left:2.0pt;padding-right:2.0pt;">62B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.22.15.4" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.22.15.5" style="padding-left:2.0pt;padding-right:2.0pt;">109B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.6" style="padding-left:2.0pt;padding-right:2.0pt;">52.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.7" style="padding-left:2.0pt;padding-right:2.0pt;">27.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.13" style="padding-left:2.0pt;padding-right:2.0pt;">53.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.22.15.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.23.16">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.23.16.1" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.23.16.2" style="padding-left:2.0pt;padding-right:2.0pt;">540B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.23.16.4" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.23.16.5" style="padding-left:2.0pt;padding-right:2.0pt;">26B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.6" style="padding-left:2.0pt;padding-right:2.0pt;">58.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.7" style="padding-left:2.0pt;padding-right:2.0pt;">33.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.23.16.13.1" style="color:#1E90FF;">63.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.23.16.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.24.17">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.24.17.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/EleutherAI/llemma_7b" title="">LLemma</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.24.17.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.3" style="padding-left:2.0pt;padding-right:2.0pt;">PPile</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.24.17.4" style="padding-left:2.0pt;padding-right:2.0pt;">55B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.24.17.5" style="padding-left:2.0pt;padding-right:2.0pt;">200B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.6" style="padding-left:2.0pt;padding-right:2.0pt;">38.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.7" style="padding-left:2.0pt;padding-right:2.0pt;">17.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.8" style="padding-left:2.0pt;padding-right:2.0pt;">56.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.9" style="padding-left:2.0pt;padding-right:2.0pt;">69.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.10" style="padding-left:2.0pt;padding-right:2.0pt;">82.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.11" style="padding-left:2.0pt;padding-right:2.0pt;">48.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.12" style="padding-left:2.0pt;padding-right:2.0pt;">41.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.13" style="padding-left:2.0pt;padding-right:2.0pt;">45.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.24.17.14" style="padding-left:2.0pt;padding-right:2.0pt;">59.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.15" style="padding-left:2.0pt;padding-right:2.0pt;">50.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.25.18">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.25.18.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/EleutherAI/llemma_34b" title="">LLemma</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.25.18.2" style="padding-left:2.0pt;padding-right:2.0pt;">34B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.3" style="padding-left:2.0pt;padding-right:2.0pt;">PPile</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.25.18.4" style="padding-left:2.0pt;padding-right:2.0pt;">55B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.25.18.5" style="padding-left:2.0pt;padding-right:2.0pt;">50B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.6" style="padding-left:2.0pt;padding-right:2.0pt;">54.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.7" style="padding-left:2.0pt;padding-right:2.0pt;">23.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.8" style="padding-left:2.0pt;padding-right:2.0pt;">67.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.9" style="padding-left:2.0pt;padding-right:2.0pt;">75.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.10" style="padding-left:2.0pt;padding-right:2.0pt;">90.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.11" style="padding-left:2.0pt;padding-right:2.0pt;">57.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.12" style="padding-left:2.0pt;padding-right:2.0pt;">49.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.13" style="padding-left:2.0pt;padding-right:2.0pt;">54.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.25.18.14" style="padding-left:2.0pt;padding-right:2.0pt;">68.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.15" style="padding-left:2.0pt;padding-right:2.0pt;">60.1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.26.19">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.26.19.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/internlm/internlm2-math-base-7b" title="">Intern-Math</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.26.19.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.26.19.4" style="padding-left:2.0pt;padding-right:2.0pt;">31B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.26.19.5" style="padding-left:2.0pt;padding-right:2.0pt;">125B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.6" style="padding-left:2.0pt;padding-right:2.0pt;">41.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.7" style="padding-left:2.0pt;padding-right:2.0pt;">14.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.8" style="padding-left:2.0pt;padding-right:2.0pt;">61.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.9" style="padding-left:2.0pt;padding-right:2.0pt;">66.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.10" style="padding-left:2.0pt;padding-right:2.0pt;">83.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.11" style="padding-left:2.0pt;padding-right:2.0pt;">50.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.12" style="padding-left:2.0pt;padding-right:2.0pt;">57.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.13" style="padding-left:2.0pt;padding-right:2.0pt;">24.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.26.19.14" style="padding-left:2.0pt;padding-right:2.0pt;">37.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.15" style="padding-left:2.0pt;padding-right:2.0pt;">48.7</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.27.20">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.27.20.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/internlm/internlm2-math-base-20b" title="">Intern-Math</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.27.20.2" style="padding-left:2.0pt;padding-right:2.0pt;">20B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.27.20.4" style="padding-left:2.0pt;padding-right:2.0pt;">31B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.27.20.5" style="padding-left:2.0pt;padding-right:2.0pt;">125B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.27.20.6.1" style="color:#1E90FF;">65.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.7" style="padding-left:2.0pt;padding-right:2.0pt;">30.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.27.20.8.1" style="color:#1E90FF;">75.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.9" style="padding-left:2.0pt;padding-right:2.0pt;">79.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.27.20.10.1" style="color:#1E90FF;">94.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.11" style="padding-left:2.0pt;padding-right:2.0pt;">50.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.12" style="padding-left:2.0pt;padding-right:2.0pt;">38.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.13" style="padding-left:2.0pt;padding-right:2.0pt;">53.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.27.20.14" style="padding-left:2.0pt;padding-right:2.0pt;">71.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.15" style="padding-left:2.0pt;padding-right:2.0pt;">62.1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.28.21">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.28.21.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/deepseek-ai/deepseek-math-7b-base" title="">DeepSeekMath</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.28.21.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.28.21.4" style="padding-left:2.0pt;padding-right:2.0pt;">120B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.28.21.5" style="padding-left:2.0pt;padding-right:2.0pt;">500B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.6" style="padding-left:2.0pt;padding-right:2.0pt;">64.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.7.1" style="color:#1E90FF;">34.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.8" style="padding-left:2.0pt;padding-right:2.0pt;">74.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.9.1" style="color:#1E90FF;">83.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.10" style="padding-left:2.0pt;padding-right:2.0pt;">92.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.11.1" style="color:#1E90FF;">63.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.12.1" style="color:#1E90FF;">62.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.13" style="padding-left:2.0pt;padding-right:2.0pt;">56.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.28.21.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.14.1" style="color:#1E90FF;">84.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.15.1" style="color:#1E90FF;">68.4</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.29.22">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="15" id="S3.T1.13.7.29.22.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_typewriter" id="S3.T1.13.7.29.22.1.1">Continual Pretraining on Mistral-7B</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.30.23">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.13.7.30.23.1" style="padding-left:2.0pt;padding-right:2.0pt;">Mistral-CT</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.30.23.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.30.23.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.13.7.30.23.5" style="padding-left:2.0pt;padding-right:2.0pt;">15B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.6" style="padding-left:2.0pt;padding-right:2.0pt;">42.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.7" style="padding-left:2.0pt;padding-right:2.0pt;">22.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.8" style="padding-left:2.0pt;padding-right:2.0pt;">68.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.9" style="padding-left:2.0pt;padding-right:2.0pt;">71.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.10" style="padding-left:2.0pt;padding-right:2.0pt;">86.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.11" style="padding-left:2.0pt;padding-right:2.0pt;">45.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.12" style="padding-left:2.0pt;padding-right:2.0pt;">47.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.13" style="padding-left:2.0pt;padding-right:2.0pt;">52.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.13.7.30.23.14" style="padding-left:2.0pt;padding-right:2.0pt;">65.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.15" style="padding-left:2.0pt;padding-right:2.0pt;">55.8</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.31.24">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.31.24.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_smallcaps" id="S3.T1.13.7.31.24.1.1">Rho-1</span>-Math</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.31.24.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.31.24.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.31.24.5" style="padding-left:2.0pt;padding-right:2.0pt;">10.5B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.6.1" style="color:#BF0040;">66.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.7.1" style="color:#BF0040;">31.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.8.1" style="color:#BF0040;">77.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.9.1" style="color:#BF0040;">79.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.10.1" style="color:#BF0040;">93.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.11.1" style="color:#BF0040;">49.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.12.1" style="color:#BF0040;">58.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.13.1" style="color:#BF0040;">54.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.31.24.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.14.1" style="color:#BF0040;">84.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.15.1" style="color:#BF0040;">66.2</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.13.7.7.1" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T1.13.7.7.1.m1.1"><semantics id="S3.T1.13.7.7.1.m1.1a"><mi id="S3.T1.13.7.7.1.m1.1.1" mathvariant="normal" xref="S3.T1.13.7.7.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T1.13.7.7.1.m1.1b"><ci id="S3.T1.13.7.7.1.m1.1.1.cmml" xref="S3.T1.13.7.7.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.7.7.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T1.13.7.7.1.m1.1d">roman_Δ</annotation></semantics></math></td>
<td class="ltx_td ltx_border_bb" id="S3.T1.13.7.7.2" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_bb" id="S3.T1.13.7.7.3" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_bb" id="S3.T1.13.7.7.4" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S3.T1.13.7.7.5" style="background-color:#E6E6E6;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.5.1" style="background-color:#E6E6E6;">-30%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.6" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.6.1" style="background-color:#D2DCFA;">+24.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.7" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.7.1" style="background-color:#D2DCFA;">+8.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.8" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.8.1" style="background-color:#D2DCFA;">+9.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.9" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.9.1" style="background-color:#D2DCFA;">+8.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.10" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.10.1" style="background-color:#D2DCFA;">+7.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.11" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.11.1" style="background-color:#D2DCFA;">+4.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.12" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.12.1" style="background-color:#D2DCFA;">+11.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.13" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.13.1" style="background-color:#D2DCFA;">+2.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.13.7.7.14" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.14.1" style="background-color:#D2DCFA;">+18.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.15" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.13.7.7.15.1" style="background-color:#D2DCFA;">+10.4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Corpus</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">수학적 추론을 위해 Common Crawl의 수학 관련 웹 페이지에서 가져온 약 14B 토큰으로 구성된 OpenWebM(OWM) 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Paster et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>를 활용한다. 일반 도메인에서는 SlimPajama <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Daria et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>와 StarCoderData <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">2023a</span></a>)</cite>(Tinyllama corpus의 양쪽 부분)를 OpenWebMath와 결합하여 6:3:1의 혼합 비율을 가진 총 800억 개의 토큰에 대해 학습한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Setting</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">수학 사전 훈련은 Tinyllama-1.1B 모델 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>와 Mistral-7B 모델 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jiang et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>에서 각각 8e-5와 2e-5의 학습률을 가지고 사전 훈련을 계속한다. 일반 도메인의 경우 Tinyllama-1.1B 모델의 학습률을 1e-4로 설정한다. 배치 크기는 두 도메인에 대해 균일하게 1M 토큰으로 설정된다. 토큰 선택 비율과 관련하여 Tinyllama-1.1B 모델의 경우 60%, Mistral-7B 모델의 경우 70%를 사용한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Baseline Setting</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">우리는 규칙적인 인과 언어 모델링을 통해 지속적으로 사전 훈련된 모델(Tinyllama-CT 및 Mistral-CT)을 기준선으로 사용한다. 또한, <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.SSS0.Px4.p1.1.1">Rho-1</span>을 잘 알려진 상위 성능 기준선과 비교하며, Gemma<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Team et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, Qwen1.5 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bai et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Phi-1.5 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">2023b</span></a>)</cite>, DeepSeekLLM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">DeepSeek-AI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, DeepSeekMath <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">2024a</span></a>)</cite>, CodeLlama <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Roziere et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Mistral <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jiang et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Minerva <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lewkowycz et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, Tinyllama <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, LLemma <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Azerbayev et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, InternLM2-Math <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ying et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>를 포함한다 미세 조정 결과를 위해 기존 베스트 모델인 MAmmoTH<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yue et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>와 ToRA<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gou et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>와 비교한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Evaluation Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px5.p1.1">사전 훈련된 모델을 종합적으로 평가하기 위해 다양한 작업에서 소수의 샷 기능과 미세 조정 성능을 비교한다. 일반 과제는 lm-eval-harness<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EleutherAI/lm-evaluation-harness" title="">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span>을, 수학 과제는 math-eval-harness<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ZubinGou/math-evaluation-harness" title="">https://github.com/ZubinGou/math-evaluation-harness</a></span></span></span>을 채택한다. 우리는 추론 속도를 높이기 위해 vllm (v0.3.2) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kwon et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>를 사용한다. 우리의 평가에 대한 자세한 내용은 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A3" title="Appendix C Evalution Details ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Appendix C</span></a>에서 찾을 수 있다.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.5.1.1" style="font-size:90%;">Table 2</span>:</span><span class="ltx_text ltx_font_bold" id="S3.T2.6.2" style="font-size:90%;">Tool-integrated reasoning results of math pretraining. </span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.2" style="width:433.6pt;height:298.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(7.8pt,-5.3pt) scale(1.03709079480337,1.03709079480337) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.2.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.2.2.3.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S3.T2.2.2.3.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.2.1">Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.3.1">Tools</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.2.2.3.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.4.1">SFT Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.5.1">GSM8k</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.6.1">MATH</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.7.1">SVAMP</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.8.1">ASDiv</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.9" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.9.1">MAWPS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.10" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.10.1">TAB</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.2.2.3.1.11" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.11.1">GSM-H</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.12" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.12.1">AVG</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.4.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" colspan="4" id="S3.T2.2.2.4.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T2.2.2.4.2.1.1">Used for SFT?</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.2.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.4.1" style="color:#C80000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.5.1" style="color:#C80000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.6.1" style="color:#C80000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.7.1" style="color:#C80000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.4.2.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.8.1" style="color:#C80000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="12" id="S3.T2.2.2.5.3.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_typewriter" id="S3.T2.2.2.5.3.1.1">Previous Models</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.6.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.6.4.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT4-0314</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.2.2.6.4.2" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.6.4.3.1" style="color:#C80000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.6.4.4" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.5" style="padding-left:5.0pt;padding-right:5.0pt;">92.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.6" style="padding-left:5.0pt;padding-right:5.0pt;">42.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.7" style="padding-left:5.0pt;padding-right:5.0pt;">93.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.8" style="padding-left:5.0pt;padding-right:5.0pt;">91.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.9" style="padding-left:5.0pt;padding-right:5.0pt;">97.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.10" style="padding-left:5.0pt;padding-right:5.0pt;">67.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.6.4.11" style="padding-left:5.0pt;padding-right:5.0pt;">64.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.12" style="padding-left:5.0pt;padding-right:5.0pt;">78.3</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.7.5">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.7.5.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT4-0314 (PAL)</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.7.5.2" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.7.5.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.7.5.4" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.5" style="padding-left:5.0pt;padding-right:5.0pt;">94.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.6" style="padding-left:5.0pt;padding-right:5.0pt;">51.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.7" style="padding-left:5.0pt;padding-right:5.0pt;">94.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.8" style="padding-left:5.0pt;padding-right:5.0pt;">92.6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.9" style="padding-left:5.0pt;padding-right:5.0pt;">97.7</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.10" style="padding-left:5.0pt;padding-right:5.0pt;">95.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.7.5.11" style="padding-left:5.0pt;padding-right:5.0pt;">77.6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.12" style="padding-left:5.0pt;padding-right:5.0pt;">86.4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.8.6">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.8.6.1" style="padding-left:5.0pt;padding-right:5.0pt;">MAmmoTH</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.8.6.2" style="padding-left:5.0pt;padding-right:5.0pt;">70B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.8.6.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.8.6.4" style="padding-left:5.0pt;padding-right:5.0pt;">MI-260k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.5" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.6" style="padding-left:5.0pt;padding-right:5.0pt;">41.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.7" style="padding-left:5.0pt;padding-right:5.0pt;">82.4</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.8" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.9" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.10" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.8.6.11" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.12" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.9.7">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.9.7.1" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.9.7.2" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.9.7.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.9.7.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.5" style="padding-left:5.0pt;padding-right:5.0pt;">68.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.6" style="padding-left:5.0pt;padding-right:5.0pt;">40.1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.7" style="padding-left:5.0pt;padding-right:5.0pt;">68.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.8" style="padding-left:5.0pt;padding-right:5.0pt;">73.9</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.9" style="padding-left:5.0pt;padding-right:5.0pt;">88.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.10" style="padding-left:5.0pt;padding-right:5.0pt;">42.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.9.7.11" style="padding-left:5.0pt;padding-right:5.0pt;">54.6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.12" style="padding-left:5.0pt;padding-right:5.0pt;">62.4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.10.8">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.10.8.1" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.10.8.2" style="padding-left:5.0pt;padding-right:5.0pt;">70B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.10.8.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.10.8.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.5" style="padding-left:5.0pt;padding-right:5.0pt;">84.3</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.6" style="padding-left:5.0pt;padding-right:5.0pt;">49.7</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.7" style="padding-left:5.0pt;padding-right:5.0pt;">82.7</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.8" style="padding-left:5.0pt;padding-right:5.0pt;">86.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.9" style="padding-left:5.0pt;padding-right:5.0pt;">93.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.10" style="padding-left:5.0pt;padding-right:5.0pt;">74.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.10.8.11" style="padding-left:5.0pt;padding-right:5.0pt;">67.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.12" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.11.9">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.11.9.1" style="padding-left:5.0pt;padding-right:5.0pt;">DeepSeekMath</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.11.9.2" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.11.9.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.11.9.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.5" style="padding-left:5.0pt;padding-right:5.0pt;">79.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.6" style="padding-left:5.0pt;padding-right:5.0pt;">52.0</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.7" style="padding-left:5.0pt;padding-right:5.0pt;">80.1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.8" style="padding-left:5.0pt;padding-right:5.0pt;">87.1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.9" style="padding-left:5.0pt;padding-right:5.0pt;">93.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.10" style="padding-left:5.0pt;padding-right:5.0pt;">85.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.11.9.11" style="padding-left:5.0pt;padding-right:5.0pt;">63.1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.12" style="padding-left:5.0pt;padding-right:5.0pt;">77.4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="12" id="S3.T2.2.2.12.10.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_typewriter" id="S3.T2.2.2.12.10.1.1">Our Pretrained Models</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.13.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.13.11.1" style="padding-left:5.0pt;padding-right:5.0pt;">TinyLlama-CT</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.2.2.13.11.2" style="padding-left:5.0pt;padding-right:5.0pt;">1B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.13.11.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.13.11.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.5" style="padding-left:5.0pt;padding-right:5.0pt;">51.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.6" style="padding-left:5.0pt;padding-right:5.0pt;">38.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.7" style="padding-left:5.0pt;padding-right:5.0pt;">53.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.8" style="padding-left:5.0pt;padding-right:5.0pt;">66.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.9" style="padding-left:5.0pt;padding-right:5.0pt;">81.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.10" style="padding-left:5.0pt;padding-right:5.0pt;">20.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.13.11.11" style="padding-left:5.0pt;padding-right:5.0pt;">42.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.12" style="padding-left:5.0pt;padding-right:5.0pt;">50.7</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.14.12">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.14.12.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_smallcaps" id="S3.T2.2.2.14.12.1.1">Rho-1</span>-Math</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.14.12.2" style="padding-left:5.0pt;padding-right:5.0pt;">1B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.14.12.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.5.1" style="color:#BF0040;">59.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.6.1" style="color:#BF0040;">40.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.7.1" style="color:#BF0040;">60.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.8.1" style="color:#BF0040;">74.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.9.1" style="color:#BF0040;">88.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.10" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.10.1" style="color:#BF0040;">26.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.14.12.11" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.11.1" style="color:#BF0040;">48.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.12" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.12.1" style="color:#BF0040;">56.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T2.1.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.1.m1.1a"><mi id="S3.T2.1.1.1.1.m1.1.1" mathvariant="normal" xref="S3.T2.1.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.1.m1.1d">roman_Δ</annotation></semantics></math></td>
<td class="ltx_td" id="S3.T2.1.1.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td" id="S3.T2.1.1.1.3" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.5" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.5.1" style="background-color:#D2DCFA;">+8.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.6" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.6.1" style="background-color:#D2DCFA;">+2.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.7" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.7.1" style="background-color:#D2DCFA;">+7.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.8" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.8.1" style="background-color:#D2DCFA;">+7.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.9" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.9.1" style="background-color:#D2DCFA;">+6.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.10" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.10.1" style="background-color:#D2DCFA;">+6.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.1.11" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.11.1" style="background-color:#D2DCFA;">+5.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.12" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.12.1" style="background-color:#D2DCFA;">+6.2</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.15.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.15.13.1" style="padding-left:5.0pt;padding-right:5.0pt;">Mistral-CT</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.2.2.15.13.2" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.15.13.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.15.13.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.5" style="padding-left:5.0pt;padding-right:5.0pt;">77.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.6" style="padding-left:5.0pt;padding-right:5.0pt;">48.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.7" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.8" style="padding-left:5.0pt;padding-right:5.0pt;">83.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.9" style="padding-left:5.0pt;padding-right:5.0pt;">93.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.10" style="padding-left:5.0pt;padding-right:5.0pt;">67.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.15.13.11" style="padding-left:5.0pt;padding-right:5.0pt;">60.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.12" style="padding-left:5.0pt;padding-right:5.0pt;">72.6</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.16.14">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.16.14.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_smallcaps" id="S3.T2.2.2.16.14.1.1">Rho-1</span>-Math</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.16.14.2" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.16.14.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.5.1" style="color:#BF0040;">81.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.6.1" style="color:#BF0040;">51.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.7.1" style="color:#BF0040;">80.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.8.1" style="color:#BF0040;">85.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.9.1" style="color:#BF0040;">94.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.10" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.10.1" style="color:#BF0040;">70.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.16.14.11" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.11.1" style="color:#BF0040;">63.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.12" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.12.1" style="color:#BF0040;">75.3</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.2.2.1" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T2.2.2.2.1.m1.1"><semantics id="S3.T2.2.2.2.1.m1.1a"><mi id="S3.T2.2.2.2.1.m1.1.1" mathvariant="normal" xref="S3.T2.2.2.2.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.1.m1.1b"><ci id="S3.T2.2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.2.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.2.1.m1.1d">roman_Δ</annotation></semantics></math></td>
<td class="ltx_td ltx_border_bb" id="S3.T2.2.2.2.2" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_bb" id="S3.T2.2.2.2.3" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="S3.T2.2.2.2.4" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.5" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.5.1" style="background-color:#D2DCFA;">+3.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.6" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.6.1" style="background-color:#D2DCFA;">+3.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.7" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.7.1" style="background-color:#D2DCFA;">+3.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.8" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.8.1" style="background-color:#D2DCFA;">+1.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.9" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.9.1" style="background-color:#D2DCFA;">+1.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.10" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.10.1" style="background-color:#D2DCFA;">+2.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.2.2.2.11" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.11.1" style="background-color:#D2DCFA;">+2.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.12" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.2.12.1" style="background-color:#D2DCFA;">+2.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Math Pre-training Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Few-shot CoT Reasoning Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">우리는 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lewkowycz et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Azerbayev et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Shao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">2024b</span></a>)</cite>에 이어 소수의shot chain-of-thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wei et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">2022a</span></a>)</cite> 예제로 프롬프트하는 기본 모델을 평가한다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.T1" title="Table 1 ‣ Reference Model Training ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Table 1</span></a>에 나타난 결과, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.1">Rho-1</span>-Math는 1B 모델에서는 16.5%, 7B 모델에서는 10.4%의 평균 소샷 정확도 향상을 달성하였다. 또한 OpenWebMath에서 여러 epoch에 대한 학습 후 <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.2">Rho-1</span>은 평균 소수 샷 정확도를 40.9%로 더욱 높일 수 있음을 발견했다. 5,000억 개의 수학 관련 토큰에 대해 사전 훈련된 DeepSeekMath-7B와 비교하여 <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.3">Rho-1</span>-7B만 사전 훈련된 150억 개의 토큰(105억 개의 토큰을 선택)에 대해 유사한 결과를 달성하여 접근법의 효율성을 입증했다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Tool-Integrated Reasoning Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.1">Rho-1</span> 및 69k ToRA 말뭉치 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gou et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>의 기준선 모델은 도구 통합 추론 형식의 16k GPT-4 생성 궤적과 LLaMA를 사용한 53k 답변 증강 샘플로 구성된다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.T2" title="Table 2 ‣ Evaluation Setup ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Table 2</span></a>, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.2">Rho-1</span>-1B 및 <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.3">Rho-1</span>-7B는 MATH 데이터 세트에서 각각 40.6% 및 51.8%의 최신 상태를 달성했습니다. (<em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.4">e.g.,</em> TabMWP and GSM-Hard), <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.5">Rho-1</span> 또한 <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.6">Rho-1</span> -Math-1B 및 <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.7">Rho-1</span> -Math-7B에서 평균 몇 번의 샷 정확도 향상이 6.2%로 어느 정도 일반화 가능성을 보여줍니다.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="462" id="S3.F5.g1" src="https://arxiv.org/html/2404.07965v1/x5.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.4.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.5.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.F5.5.2.1">General pretraining results.</span>
We continual pretraining Tinyllama-1B on 80G general tokens. Tinyllama-CT is etrained with CLM, while <span class="ltx_text ltx_font_smallcaps" id="S3.F5.5.2.2">Rho-1</span> is trained with our proposed SLM.
</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>General Pre-training Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">우리는 800억 토큰에 대한 Tinyllama-1.1B를 지속적으로 훈련하여 일반적인 사전 훈련에서 SLM의 효능을 확인한다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F5" title="Figure 5 ‣ Tool-Integrated Reasoning Results ‣ 3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>에 표시된 결과는 Tinyllama가 이미 이러한 토큰의 대다수에 대해 광범위한 훈련을 받았지만 SLM의 적용은 직접 연속 사전 훈련에 비해 15개의 벤치마크에서 평균 6.8%의 향상을 산출한다는 것을 나타낸다. 특히 코드와 수학 과제에서 개선이 두드러져 10%를 넘어섰다.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S3.F6.g1" src="https://arxiv.org/html/2404.07965v1/x6.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.3.1.1" style="font-size:90%;">Figure 6</span>:</span><span class="ltx_text ltx_font_bold" id="S3.F6.4.2" style="font-size:90%;">The dynamics of pretraining loss and downstream loss. <span class="ltx_text ltx_font_medium" id="S3.F6.4.2.1"> (a) 및 (c)는 SLM 및 CLM 방법 모두에서 사전 훈련 동안 SLM에 의해 선택/선택되지 않은 토큰의 손실을 나타내는 반면, (b)는 다운스트림 말뭉치에서 SLM 및 CLM 방법의 손실을 나타낸다. 총 40억 토큰으로 사전 훈련하는 과정을 통해 위의 결과를 검정하였다. </span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Selected Token Loss Aligns Better with Downstream Performance</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">참조 모델을 사용하여 토큰을 필터링하고 다운스트림 손실과의 관계를 관찰하면서 모든/선택된 토큰에 대한 훈련 후 유효성 검사 손실의 변화를 탐색한다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F6" title="Figure 6 ‣ 3.3 General Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>와 같이 약 4B 토큰을 사전 훈련하고 사전 훈련 과정에서 서로 다른 사전 훈련 방법과 검증 세트에 손실의 변동 곡선을 표시했다. 참조 모델에 의해 선택된 토큰에서 <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px1.p1.1.1">Rho-1</span>의 평균 손실 감소가 정규 사전 훈련에 비해 더 중요하다는 것을 관찰할 수 있다. 반대로 선택되지 않은 토큰에서는 정규 사전 훈련의 평균 손실 감소가 더 크다. 무화과(a), 무화과(b)를 무화과(c)와 연관시키면 선택된 토큰에 대해 훈련된 모델이 다운스트림 손실이 더 크게 감소한다는 것을 발견하는 것은 어렵지 않은 반면, 일반적인 사전 훈련은 훈련 단계에서 모든 토큰의 평균 손실을 감소시키기는 하지만 다운스트림 손실이 크게 감소하기 어렵다. 따라서 사전 교육을 위한 토큰을 선택하는 것이 더 효율적일 것으로 기대한다.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="296" id="S3.F7.g1" src="https://arxiv.org/html/2404.07965v1/x7.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.3.1.1" style="font-size:90%;">Figure 7</span>:</span><span class="ltx_text ltx_font_bold" id="S3.F7.4.2" style="font-size:90%;">The relationship between the selected tokens / unselected tokens loss in SLM and downstream task performance. <span class="ltx_text ltx_font_medium" id="S3.F7.4.2.1"> The y-axis represents the average few-shot accuracy on GSM8k and MATH. x축은 해당 체크포인트(2B, 5B, 8B, 11B, 14B)에서 선택된 토큰/선택되지 않은 토큰에 대한 평균 손실을 나타낸다. </span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p2.1">또한, 동시 연구 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gadre et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>와 유사한 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F7" title="Figure 7 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>의 멱 법칙을 통해 선택된 토큰의 손실을 다운스트림 태스크 성능과 연관시킨다. 그래프의 데이터 포인트로부터 피팅된 곡선을 관찰하면 SLM에 의해 선택된 토큰의 평균 손실은 다운스트림 태스크의 성능과 양의 상관 관계를 나타내는 반면, 선택되지 않은 토큰의 평균 손실은 다운스트림 태스크의 성능과 음의 상관 관계를 나타낸다. 따라서 모델의 궁극적인 성과에 도움이 되기 위해 모든 토큰의 손실이 감소할 필요는 없다. 자세한 내용은 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A4" title="Appendix D Relate the Selected Tokens’ Loss to Downstream Task Performance ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Appendix D</span></a>를 참조하십시오.</p>
</div>
<figure class="ltx_table" id="S3.SS4.SSS0.Px1.2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2"><span class="ltx_inline-para ltx_minipage ltx_flex_size_2 ltx_align_middle" id="S3.SS4.SSS0.Px1.1.1" style="width:212.5pt;">
<span class="ltx_para ltx_align_center" id="S3.SS4.SSS0.Px1.1.1.p1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="559" id="S3.SS4.SSS0.Px1.1.1.p1.g1" src="https://arxiv.org/html/2404.07965v1/x8.png" width="829">
</span>
<span class="ltx_figure ltx_align_center" id="S3.F8">
<span class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.3.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F8.4.2" style="font-size:90%;">The PPL of tokens selected by different checkpoint.<span class="ltx_text ltx_font_medium" id="S3.F8.4.2.1"> We test the PPL of the tokens selected at 2B, 5B, 8B, 11B, and 14B.
</span></span></span>
</span></span></div>
<div class="ltx_flex_cell ltx_flex_size_2"><span class="ltx_inline-para ltx_minipage ltx_flex_size_2 ltx_align_middle" id="S3.SS4.SSS0.Px1.2.2" style="width:208.1pt;">
<span class="ltx_para ltx_align_center" id="S3.SS4.SSS0.Px1.2.2.p1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="625" id="S3.SS4.SSS0.Px1.2.2.p1.g1" src="https://arxiv.org/html/2404.07965v1/x9.png" width="830">
</span>
<span class="ltx_figure ltx_align_center" id="S3.F9">
<span class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F9.3.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F9.4.2" style="font-size:90%;">Effect of token select ratio.<span class="ltx_text ltx_font_medium" id="S3.F9.4.2.1"> We train 1B LM with SLM objective on 5B tokens.</span></span></span>
</span></span></div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">What Tokens are Selected with SLM?</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.1">우리는 SLM 방법으로 선택된 토큰을 사전 훈련에서 분석하여 작동 메커니즘을 더 탐구하는 것을 목표로 한다. 이를 위해 OpenWebMath를 사용하여 <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px2.p1.1.1">Rho-1</span> 훈련 중 토큰 선택 프로세스를 시각화합니다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A5.SS1" title="E.1 Token Selected Examples ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§E.1</span></a>에서는 <span class="ltx_text" id="S3.SS4.SSS0.Px2.p1.1.2" style="color:#1E90FF;">blue</span> 실제 사전 훈련 중에 유지 된 토큰을 강조 표시 했습니다. SLM 방법에 의해 선택된 토큰의 대부분은 수학과 밀접한 관련이 있으며, 수학적 내용과 관련된 원래 말뭉치의 부분에 대해 모델을 효과적으로 훈련시킨다는 것을 관찰한다.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p2.1">또한, 훈련 과정에서 다양한 체크포인트에 대한 토큰 필터링의 차이를 조사하고 다른 체크포인트에서 이러한 토큰의 복잡성을 테스트했다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F8" title="Figure 8 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>에 예시된 바와 같이, 우리는 후기 체크포인트에 의해 선택된 토큰이 훈련의 후기 단계로 갈수록 더 높은 복잡도를 갖고 초기 단계에서 더 낮은 복잡도를 갖는 경향이 있음을 발견했다. 이는 모델이 먼저 학습 가능한 공간이 더 큰 토큰을 최적화하여 학습 효율을 높일 수 있음을 시사할 수 있다. 또한, 선택된 토큰의 손실에 대한 샘플별 "이중 하강" <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Nakkiran et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>를 발견했는데, 여기서 선택 토큰의 복잡도는 감소하기 전에 처음에 증가한다. 이는 각 체크포인트에서 가장 필요한 사람들을 대상으로 초과손실을 기준으로 토큰을 선택하는 효과일 수 있다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Effect of Token Select Ratio</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px3.p1.1">우리는 SLM의 토큰 선택 비율이 미치는 영향을 조사한다. 일반적으로, 선택 비율은 이전에 Masked Language Models (MLMs) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Devlin et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Liu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>의 훈련에서 사용된 접근법과 유사하게 휴리스틱 규칙에 의해 정의된다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F9" title="Figure 9 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 9</span></a>에 도시된 바와 같이, 선택된 토큰은 원래 토큰의 약 60%를 차지하기에 적합하다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Weak-to-Strong Generization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.5.1.1" style="font-size:90%;">Table 3</span>:</span><span class="ltx_text ltx_font_bold" id="S3.T3.6.2" style="font-size:90%;">Weak-to-Strong generization result on math benchmark. <span class="ltx_text ltx_font_medium" id="S3.T3.6.2.1"></span></span></figcaption>
</span></span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.2" style="width:433.6pt;height:79.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(6.1pt,-1.1pt) scale(1.02907802348152,1.02907802348152) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.2.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.3.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.4.1">GSM8K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1">MATH<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.1.m1.1a"><msup id="S3.T3.1.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T3.1.1.1.1.1.m1.1.1a" xref="S3.T3.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="S3.T3.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="S3.T3.1.1.1.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.m1.1.1"><ci id="S3.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.1.1.m1.1.1.1">normal-†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.5.1">SVAMP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.6.1">ASDiv</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.7.1">MAWPS</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.8.1">TAB</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.9.1">MQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.2.2.2.10.1">
<tbody><tr class="ltx_tr" id="S3.T3.2.2.2.10.1.1">
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.10.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.10.1.1.1.1">MMLU</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.2.10.1.2">
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.10.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.10.1.2.1.1">STEM</span></td>
</tr>
</tbody></table></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T3.2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.2.1">SAT<math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T3.2.2.2.2.1.m1.1"><semantics id="S3.T3.2.2.2.2.1.m1.1a"><msup id="S3.T3.2.2.2.2.1.m1.1.1" xref="S3.T3.2.2.2.2.1.m1.1.1.cmml"><mi id="S3.T3.2.2.2.2.1.m1.1.1a" xref="S3.T3.2.2.2.2.1.m1.1.1.cmml"></mi><mo id="S3.T3.2.2.2.2.1.m1.1.1.1" mathvariant="normal" xref="S3.T3.2.2.2.2.1.m1.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.2.1.m1.1b"><apply id="S3.T3.2.2.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.2.2.1.m1.1.1"><ci id="S3.T3.2.2.2.2.1.m1.1.1.1.cmml" xref="S3.T3.2.2.2.2.1.m1.1.1.1">normal-‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.2.1.m1.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.2.1.m1.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.11.1">AVG</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.2.2.3.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Llama-2-7B-CT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">28.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">13.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">50.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">62.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">79.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">37.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">34.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">41.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.2.2.3.1.10" style="padding-left:2.0pt;padding-right:2.0pt;">43.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.11" style="padding-left:2.0pt;padding-right:2.0pt;">43.5</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T3.2.2.4.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">Llama-2-7B-CT w/ 1B RM</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">29.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">16.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">55.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">63.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">80.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">37.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">34.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">38.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T3.2.2.4.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">43.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.11" style="padding-left:2.0pt;padding-right:2.0pt;">44.4</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px4.p1.1">참조 및 연속 사전 훈련에 동일한 기본 모델을 사용하는 주요 실험과는 별도로, 더 작은 참조 모델이 더 큰 모델의 사전 훈련을 효과적으로 안내할 수 있는지 조사한다. 우리는 Tinyllma-1.1B를 참조 모델로 사용하고 Llama-2-7B를 수학에 지속적으로 사전 훈련한다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.T3" title="Table 3 ‣ Weak-to-Strong Generization ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Table 3</span></a>에 제시된 결과는 작은 모델과 큰 모델 사이의 상당한 격차에도 불구하고 토큰 선택에 작은 참조 모델을 사용하는 것이 더 큰 모델의 사전 훈련에 여전히 이점을 얻을 수 있음을 나타낸다. 참조 모델과 학습 모델이 다른 어휘를 갖는 경우, 향후 작업을 위해 떠나는 토큰 정렬 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wan et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>를 수행하는 것을 고려할 수 있다.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Works</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Data Optimization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">사전 학습 말뭉치의 최적화는 사전 학습 데이터 혼합의 품질과 규모를 향상시켜 언어 모델 학습의 성능과 효율성을 극대화하는 것이다. 여기에는 일반적으로 크롤링 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Raffel et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> 또는 합성 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Polu and Sutskever</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Gunasekar et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, de-duplication <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lee et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Kandpal et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Tirumala et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, 필터링 및 선택 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Albalak et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, 데이터 구성 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xie et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> 및 커리큘럼 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chen et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">MA et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>를 통한 데이터 수집이 포함된다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Data Selection</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">미세 조정을 위한 데이터 선택은 품질 개선 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">2023d</span></a>)</cite>, 다양성 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Liu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, 분포 매칭 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">2023e</span></a>; <span class="ltx_text" style="font-size:90%;">Xia et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Ni et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>를 중심으로 광범위하게 연구되어 왔다. 사전 학습을 위해 휴리스틱 기반(<em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.1.1">e.g.,</em> language and item count filtering), 분류기 기반 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Brown et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, perplexity 기반 접근법 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wenzek et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>를 포함하여 다양한 경량 필터가 활용된다. 예를 들어 대규모 공개 RedPajama-Data-v2 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Computer</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>는 데이터 필터링 및 재가중화를 위해 40개 이상의 품질 지표를 활용한다. 그럼에도 불구하고 블록리스트 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Raffel et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>와 Safety API 필터링 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Welbl et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>와 같은 엄격한 필터링은 평가 손실을 해치거나 bias <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dodge et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>를 유발하는 것으로 나타났다. 우리가 아는 한, 우리는 가장 근본적인 세분성에서 데이터 품질과 정보 밀도를 향상시키는 것을 목표로 하는 토큰 수준 데이터 선택을 처음으로 탐구한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Language Model Training Dynamics</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">언어 모델의 훈련 역학을 조사하는 것은 훈련 과정 전반에 걸쳐 그들의 행동을 이해하는 데 필수적이다. 본 연구는 내적 표상 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Saphra and Lopez</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, 언어적 지식의 습득 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Choshen et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Liu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, 그로킹 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Power et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> 현상을 연구한다. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xia et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>에 의한 분석은 다양한 크기의 모델에서 토큰 수준의 훈련 궤적을 조사하는 우리와 가장 관련이 있다. 그러나 우리의 연구 결과는 <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xia et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>의 연구 결과와 다르며, 이는 복잡성의 변화가 거의 없는 토큰이 "이미 학습되었다"고 가정한다. 우리는 수렴에 저항하는 "쉬운 토큰"과 "하드 토큰"을 포함한 토큰 패턴의 스펙트럼을 식별한다. 이를 인식하여 영향력 있는 토큰을 대상으로 하여 학습 과정을 최적화하는 선택적 언어 모델링 방법을 제안한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Scaling Laws</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px4.p1.1">스케일링 법칙은 매개변수 수, 데이터 크기 및 계산과 같은 요인이 언어 모델 성능 및 행동에 미치는 영향을 발견하는 데 도움이 됩니다. 이들 연구는 보통 멱법칙 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Hernandez et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, 최적 자원 할당 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hoffmann et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, 다운스트림 태스크 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wei et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">2022b</span></a>; <span class="ltx_text" style="font-size:90%;">Isik et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Gadre et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, 아키텍처 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tay et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, 암기 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tirumala et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Carlini et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Henighan et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Biderman et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, 반복 데이터 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hernandez et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Muennighoff et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Xue et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>를 통해 예측 가능한 스케일링에 초점을 맞춘다. 모델 성능에 관한 대부분의 스케일링 법칙은 모든 트레이닝 토큰에 대한 상호-엔토리 손실을 연구하는 반면, 우리는 원하는 분포의 토큰 손실에 초점을 맞춘다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Generalization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">수학 연속 사전 훈련에서 <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F6" title="Figure 6 ‣ 3.3 General Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>에 묘사된 바와 같이 SLM만을 사용한 훈련은 참조 모델이 초점을 맞춘 도메인으로 빠르게 수렴하고 선택되지 않은 토큰의 손실이 크게 증가한다. 증가된 손실에서 편향과 같은 부작용이 아직 관찰되지 않았지만 텍스트 및 코드에 대한 일반적인 사전 훈련 손실은 <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Ouyang et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> 및 <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Azerbayev et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>에서 제안한 바와 같이 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Goodhart and Goodhart</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">1984</span></a>)</cite>에 과적합하는 것을 방지할 수 있다. 또한, 향후 노력은 참조 모델의 말뭉치 범위를 넓히고 DeepSpeedMath <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">2024a</span></a>)</cite>로 예시된 바와 같이 사전 훈련 데이터 크기를 확대할 수 있다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Scalability</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">Due to budget constraints, we have only verified the effectiveness of our method on smaller models (&lt;=7B parameters) and smaller datasets (&lt;100B tokens). Smaller models benefit significantly from removing the loss of irrelevant tokens and focusing on important ones. However, it’s possible that very large models trained on extensive corpora may naturally develop this inductive bias to compress useful data (<em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px2.p1.1.1">i.e.,</em> compressing everything), although it may sounds inefficient for now. Therefore, future works should study whether this selective language modeling technique can scale to very large models and data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>.</p>예산 제약으로 인해 우리는 더 작은 모델(<=7B 매개변수)과 더 작은 데이터 세트(<100B 토큰)에 대한 방법의 효과만을 검증했다. 더 작은 모델은 관련 없는 토큰의 손실을 제거하고 중요한 토큰에 집중함으로써 상당한 이점을 얻을 수 있습니다. 그러나 광범위한 말뭉치에 대해 훈련된 매우 큰 모델이 유용한 데이터를 압축하기 위해 이러한 유도성 편향을 자연적으로 개발할 수 있다(<em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px2.p1.1.1">i.e.,</em> 압축 모든 것). 따라서 향후 연구에서는 이러한 선택적 언어 모델링 기법이 매우 큰 모델과 데이터 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>로 확장할 수 있는지 연구해야 한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Is training a reference model necessary?</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">토큰 점수를 매기기 위해서는 고품질 참조 모델이 필요합니다. 이것은 소량의 고품질 데이터로 훈련된 기본 모델 또는 성능이 뛰어난 오픈 소스 모델일 수 있다. 사실, 참조 모델에서 입력 로그프로브 또는 복잡성만 필요하기 때문에 더 강력한 독점 모델 API를 사용할 수도 있다. 토큰을 입력하고 API에서 반환한 입력의 로그 확률을 참조 점수로 사용할 수 있습니다. 우리는 이것을 미래의 일을 위해 남겨둔다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">How to improve upon SLM?</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.1">SLM의 많은 자연적인 확장들이 있는데, <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px4.p1.1.1">e.g.,</em> reweighting tokens은 robustness를 향상시킬 수 있다; reinforcement learning과 함께 프리트레이닝을 가이드하기 위해 보상 모델로 참조 모델을 사용한다; Overfitting을 줄이기 위해 다수의 참조 모델을 채택한다; 지속적인 개선을 위한 토큰 레벨 커리큘럼 학습 및 반복 전략을 설계한다, <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px4.p1.1.2">etc</em>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Expanding the Use of SLM</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px5.p1.1">SLM은 많은 SFT 데이터 세트의 노이즈 및 분포 불일치를 해결하기 위해 감독 미세 조정으로 확장될 수 있다. 다른 잠재적인 응용 프로그램은 정렬, <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px5.p1.1.1">e.g.,</em> 도움성, 진실성 및 무해성을 강조하기 위해 참조 모델을 훈련함으로써 사전 훈련 단계 동안 기본적으로 정렬된 기본 모델을 얻을 수 있다.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Kaplan et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">Scaling laws for neural language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.9.1" style="font-size:90%;">arXiv preprint arXiv:2001.08361</em><span class="ltx_text" id="bib.bib1.10.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Brown et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.9.1" style="font-size:90%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib2.10.2" style="font-size:90%;">, 33:1877–1901, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.4.4.1" style="font-size:90%;">OpenAI [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.6.1" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">Gpt-4 technical report, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Team et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M Dai, Anja Hauth, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">Gemini: a family of highly capable multimodal models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.9.1" style="font-size:90%;">arXiv preprint arXiv:2312.11805</em><span class="ltx_text" id="bib.bib4.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Wenzek et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Ccnet: Extracting high quality monolingual datasets from web crawl data.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.9.1" style="font-size:90%;">arXiv preprint arXiv:1911.00359</em><span class="ltx_text" id="bib.bib5.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Welbl et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa&nbsp;Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">Challenges in detoxifying language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.10.2" style="font-size:90%;">Findings of the Association for Computational Linguistics: EMNLP 2021</em><span class="ltx_text" id="bib.bib6.11.3" style="font-size:90%;">, pages 2447–2469, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Muennighoff et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le&nbsp;Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin&nbsp;A Raffel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Scaling data-constrained language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib7.10.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Dodge et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Documenting large webtext corpora: A case study on the colossal clean crawled corpus.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.2" style="font-size:90%;">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text" id="bib.bib8.11.3" style="font-size:90%;">, pages 1286–1305, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Longpre et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">A pretrainer’s guide to training data: Measuring the effects of data age, domain coverage, quality, &amp; toxicity.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.13169</em><span class="ltx_text" id="bib.bib9.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Tay et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Yi&nbsp;Tay, Mostafa Dehghani, Samira Abnar, Hyung&nbsp;Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh&nbsp;Q Tran, Dani Yogatama, and Donald Metzler.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Scaling laws vs model architectures: How does inductive bias influence scaling?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.9.1" style="font-size:90%;">arXiv preprint arXiv:2207.10551</em><span class="ltx_text" id="bib.bib10.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Wettig et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Should you mask 15% in masked language modeling?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.9.1" style="font-size:90%;">In Andreas Vlachos and Isabelle Augenstein, editors, </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.10.2" style="font-size:90%;">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em><span class="ltx_text" id="bib.bib11.11.3" style="font-size:90%;">, pages 2985–3000, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.12.1" style="font-size:90%;">doi: </span><a class="ltx_ref ltx_Url" href="10.18653/v1/2023.eacl-main.217" style="font-size:90%;" title="">10.18653/v1/2023.eacl-main.217</a><span class="ltx_text" id="bib.bib11.13.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.14.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.eacl-main.217" style="font-size:90%;" title="">https://aclanthology.org/2023.eacl-main.217</a><span class="ltx_text" id="bib.bib11.15.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.4.4.1" style="font-size:90%;">Hüllermeier and Waegeman [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.6.1" style="font-size:90%;">
Eyke Hüllermeier and Willem Waegeman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.8.1" style="font-size:90%;">Machine learning</em><span class="ltx_text" id="bib.bib12.9.2" style="font-size:90%;">, 110(3):457–506, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Yu et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Longhui Yu, Weisen Jiang, Han Shi, YU&nbsp;Jincheng, Zhengying Liu, Yu&nbsp;Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">Metamath: Bootstrap your own mathematical questions for large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib13.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Huang et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib14.8.2" style="font-size:90%;">Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.9.1" style="font-size:90%;">Key-point-driven data synthesis with its enhancement on mathematical reasoning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.10.1" style="font-size:90%;">arXiv preprint arXiv:2403.02333</em><span class="ltx_text" id="bib.bib14.11.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Yue et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Xiang Yue, Xingwei Qu, Ge&nbsp;Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu&nbsp;Su, and Wenhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">Mammoth: Building math generalist models through hybrid instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib15.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Ni et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">Exploring the mystery of influential data for mathematical reasoning, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Ivison et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah&nbsp;A Smith, Iz&nbsp;Beltagy, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Camels in a changing climate: Enhancing lm adaptation with tulu 2.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.9.1" style="font-size:90%;">arXiv preprint arXiv:2311.10702</em><span class="ltx_text" id="bib.bib17.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.4.4.1" style="font-size:90%;">Teknium [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.6.1" style="font-size:90%;">
Teknium.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/teknium/OpenHermes-2.5" style="font-size:90%;" title="">https://huggingface.co/datasets/teknium/OpenHermes-2.5</a><span class="ltx_text" id="bib.bib18.9.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Lightman et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">Let’s verify step by step.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.20050</em><span class="ltx_text" id="bib.bib19.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Paster et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Keiran Paster, Marco&nbsp;Dos Santos, Zhangir Azerbayev, and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Openwebmath: An open dataset of high-quality mathematical web text, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Daria et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Soboleva Daria, Al-Khateeb Faisal, Myers Robert Steeves&nbsp;Jacob R, Hestness Joel, and Dey Nolan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">SlimPajama: A 627B token cleaned and deduplicated version of RedPajama.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama" style="font-size:90%;" title="">https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama</a><span class="ltx_text" id="bib.bib21.9.1" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.10.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/cerebras/SlimPajama-627B" style="font-size:90%;" title="">https://huggingface.co/datasets/cerebras/SlimPajama-627B</a><span class="ltx_text" id="bib.bib21.11.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Raymond Li, Loubna&nbsp;Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry&nbsp;Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh&nbsp;Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra&nbsp;Murthy V, Jason Stillerman, Siva&nbsp;Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn&nbsp;Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos&nbsp;Muñoz Ferrandis, Sean Hughes, Thomas
Wolf, Arjun Guha, Leandro von Werra, and Harm de&nbsp;Vries.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Starcoder: may the source be with you!
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.9.1" style="font-size:90%;">CoRR</em><span class="ltx_text" id="bib.bib22.10.2" style="font-size:90%;">, abs/2305.06161, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Zhang et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Tinyllama: An open-source small language model, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Jiang et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Mistral 7b.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.9.1" style="font-size:90%;">arXiv preprint arXiv:2310.06825</em><span class="ltx_text" id="bib.bib24.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Team et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir&nbsp;Sanjay Kale, Juliette Love, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">Gemma: Open models based on gemini research and technology.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.9.1" style="font-size:90%;">arXiv preprint arXiv:2403.08295</em><span class="ltx_text" id="bib.bib25.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Bai et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">Qwen technical report.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.9.1" style="font-size:90%;">arXiv preprint arXiv:2309.16609</em><span class="ltx_text" id="bib.bib26.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie&nbsp;Del Giorno, Suriya Gunasekar, and Yin&nbsp;Tat Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">Textbooks are all you need ii: phi-1.5 technical report, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.4.4.1" style="font-size:90%;">DeepSeek-AI [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.6.1" style="font-size:90%;">
DeepSeek-AI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">Deepseek llm: Scaling open-source language models with longtermism.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.8.1" style="font-size:90%;">arXiv preprint arXiv:2401.02954</em><span class="ltx_text" id="bib.bib28.9.2" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.10.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/deepseek-ai/DeepSeek-LLM" style="font-size:90%;" title="">https://github.com/deepseek-ai/DeepSeek-LLM</a><span class="ltx_text" id="bib.bib28.11.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Shao et&nbsp;al. [2024a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y.&nbsp;Wu, and Daya Guo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024a.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.9.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.03300" style="font-size:90%;" title="">https://arxiv.org/abs/2402.03300</a><span class="ltx_text" id="bib.bib29.10.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Roziere et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing&nbsp;Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">Code llama: Open foundation models for code.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.9.1" style="font-size:90%;">arXiv preprint arXiv:2308.12950</em><span class="ltx_text" id="bib.bib30.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Lewkowycz et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Solving quantitative reasoning problems with language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib31.10.2" style="font-size:90%;">, 35:3843–3857, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Azerbayev et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco&nbsp;Dos Santos, Stephen McAleer, Albert&nbsp;Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">Llemma: An open language model for mathematics.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.9.1" style="font-size:90%;">arXiv preprint arXiv:2310.10631</em><span class="ltx_text" id="bib.bib32.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Ying et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">Internlm-math: Open math large language models toward verifiable reasoning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.06332</em><span class="ltx_text" id="bib.bib33.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Gou et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">Tora: A tool-integrated reasoning agent for mathematical problem solving.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib34.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib34.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Kwon et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">Efficient memory management for large language model serving with pagedattention.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.10.2" style="font-size:90%;">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em><span class="ltx_text" id="bib.bib35.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Wei et&nbsp;al. [2022a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed&nbsp;Chi, Quoc&nbsp;V Le, Denny Zhou, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">Chain-of-thought prompting elicits reasoning in large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.10.2" style="font-size:90%;">NIPS</em><span class="ltx_text" id="bib.bib36.11.3" style="font-size:90%;">, volume&nbsp;35, pages 24824–24837, 2022a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Shao et&nbsp;al. [2024b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK&nbsp;Li, Y&nbsp;Wu, and Daya Guo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.03300</em><span class="ltx_text" id="bib.bib37.10.2" style="font-size:90%;">, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Gadre et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Samir&nbsp;Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros&nbsp;G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Language models scale reliably with over-training and on downstream tasks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.9.1" style="font-size:90%;">Preprint</em><span class="ltx_text" id="bib.bib38.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">Nakkiran et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">Deep double descent: Where bigger models and more data hurt.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.9.1" style="font-size:90%;">Journal of Statistical Mechanics: Theory and Experiment</em><span class="ltx_text" id="bib.bib39.10.2" style="font-size:90%;">, 2021(12):124003, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.5.5.1" style="font-size:90%;">Devlin et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.8.1" style="font-size:90%;">BERT: pre-training of deep bidirectional transformers for language understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib40.10.2" style="font-size:90%;">NAACL-HLT (1)</em><span class="ltx_text" id="bib.bib40.11.3" style="font-size:90%;">, pages 4171–4186. Association for Computational Linguistics, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Liu et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">Roberta: A robustly optimized BERT pretraining approach.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.9.1" style="font-size:90%;">CoRR</em><span class="ltx_text" id="bib.bib41.10.2" style="font-size:90%;">, abs/1907.11692, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023c]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Xiang&nbsp;Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">Contrastive decoding: Open-ended text generation as optimization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib42.10.2" style="font-size:90%;">ACL (1)</em><span class="ltx_text" id="bib.bib42.11.3" style="font-size:90%;">, pages 12286–12312. Association for Computational Linguistics, 2023c.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">Wan et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">Knowledge fusion of large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib43.10.2" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span class="ltx_text" id="bib.bib43.11.3" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.12.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=jiDsk12qcz" style="font-size:90%;" title="">https://openreview.net/forum?id=jiDsk12qcz</a><span class="ltx_text" id="bib.bib43.13.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">Fu et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib44.8.2" style="font-size:90%;">Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.9.1" style="font-size:90%;">Specializing smaller language models towards multi-step reasoning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.10.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib44.11.2" style="font-size:90%;">International Conference on Machine Learning</em><span class="ltx_text" id="bib.bib44.12.3" style="font-size:90%;">, pages 10421–10430. PMLR, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">Raffel et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.8.1" style="font-size:90%;">Exploring the limits of transfer learning with a unified text-to-text transformer.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.9.1" style="font-size:90%;">Journal of machine learning research</em><span class="ltx_text" id="bib.bib45.10.2" style="font-size:90%;">, 21(140):1–67, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.4.4.1" style="font-size:90%;">Polu and Sutskever [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.6.1" style="font-size:90%;">
Stanislas Polu and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.7.1" style="font-size:90%;">Generative language modeling for automated theorem proving.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.8.1" style="font-size:90%;">arXiv preprint arXiv:2009.03393</em><span class="ltx_text" id="bib.bib46.9.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.5.5.1" style="font-size:90%;">Gunasekar et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">
Suriya Gunasekar, Yi&nbsp;Zhang, Jyoti Aneja, Caio César&nbsp;Teodoro Mendes, Allie Del&nbsp;Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de&nbsp;Rosa, Olli Saarikivi, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">Textbooks are all you need.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.9.1" style="font-size:90%;">arXiv preprint arXiv:2306.11644</em><span class="ltx_text" id="bib.bib47.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.5.5.1" style="font-size:90%;">Lee et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.7.1" style="font-size:90%;">
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.8.1" style="font-size:90%;">Deduplicating training data makes language models better.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.9.1" style="font-size:90%;">arXiv preprint arXiv:2107.06499</em><span class="ltx_text" id="bib.bib48.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib49.5.5.1" style="font-size:90%;">Kandpal et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib49.8.2" style="font-size:90%;">Nikhil Kandpal, Eric Wallace, and Colin Raffel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.9.1" style="font-size:90%;">Deduplicating training data mitigates privacy risks in language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.10.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib49.11.2" style="font-size:90%;">International Conference on Machine Learning</em><span class="ltx_text" id="bib.bib49.12.3" style="font-size:90%;">, pages 10697–10707. PMLR, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib50.5.5.1" style="font-size:90%;">Tirumala et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.7.1" style="font-size:90%;">
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.8.1" style="font-size:90%;">D4: Improving llm pretraining via document de-duplication and diversification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib50.10.2" style="font-size:90%;">NIPS</em><span class="ltx_text" id="bib.bib50.11.3" style="font-size:90%;">, volume&nbsp;36, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib51.5.5.1" style="font-size:90%;">Albalak et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.7.1" style="font-size:90%;">
Alon Albalak, Yanai Elazar, Sang&nbsp;Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William&nbsp;Yang Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.8.1" style="font-size:90%;">A survey on data selection for language models, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib52.5.5.1" style="font-size:90%;">Xie et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.7.1" style="font-size:90%;">
Sang&nbsp;Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy&nbsp;S Liang, Quoc&nbsp;V Le, Tengyu Ma, and Adams&nbsp;Wei Yu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.8.1" style="font-size:90%;">Doremi: Optimizing data mixtures speeds up language model pretraining.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib52.10.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib53.5.5.1" style="font-size:90%;">Chen et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.7.1" style="font-size:90%;">
Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce&nbsp;Zhang, Frederic Sala, and Christopher Ré.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.8.1" style="font-size:90%;">Skill-it! a data-driven skills framework for understanding and training language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib53.10.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib54.5.5.1" style="font-size:90%;">MA et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.7.1" style="font-size:90%;">
YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu&nbsp;Jiang, Changjian Wang, and Shanshan Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.8.1" style="font-size:90%;">At which training stage does code data help LLMs reasoning?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib54.10.2" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span class="ltx_text" id="bib.bib54.11.3" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.12.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=KIPJKST4gw" style="font-size:90%;" title="">https://openreview.net/forum?id=KIPJKST4gw</a><span class="ltx_text" id="bib.bib54.13.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib55.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023d]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.7.1" style="font-size:90%;">
Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.8.1" style="font-size:90%;">From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.9.1" style="font-size:90%;">arXiv preprint arXiv:2308.12032</em><span class="ltx_text" id="bib.bib55.10.2" style="font-size:90%;">, 2023d.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib56.5.5.1" style="font-size:90%;">Liu et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.7.1" style="font-size:90%;">
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.8.1" style="font-size:90%;">What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib56.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib56.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib57.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023e]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.7.1" style="font-size:90%;">
Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.8.1" style="font-size:90%;">One shot learning as instruction data prospector for large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.9.1" style="font-size:90%;">arXiv preprint arXiv:2312.10302</em><span class="ltx_text" id="bib.bib57.10.2" style="font-size:90%;">, 2023e.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib58.5.5.1" style="font-size:90%;">Xia et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.7.1" style="font-size:90%;">
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.8.1" style="font-size:90%;">Less: Selecting influential data for targeted instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.04333</em><span class="ltx_text" id="bib.bib58.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib59.4.4.1" style="font-size:90%;">Computer [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.6.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib59.7.2" style="font-size:90%;">Together Computer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.8.1" style="font-size:90%;">Redpajama: an open dataset for training large language models, 10 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.9.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/togethercomputer/RedPajama-Data" style="font-size:90%;" title="">https://github.com/togethercomputer/RedPajama-Data</a><span class="ltx_text" id="bib.bib59.10.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib60.4.4.1" style="font-size:90%;">Saphra and Lopez [2018]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.6.1" style="font-size:90%;">
Naomi Saphra and Adam Lopez.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.7.1" style="font-size:90%;">Understanding learning dynamics of language models with svcca.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.8.1" style="font-size:90%;">arXiv preprint arXiv:1811.00225</em><span class="ltx_text" id="bib.bib60.9.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib61.5.5.1" style="font-size:90%;">Choshen et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.7.1" style="font-size:90%;">
Leshem Choshen, Guy Hacohen, Daphna Weinshall, and Omri Abend.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.8.1" style="font-size:90%;">The grammar-learning trajectories of neural language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.9.1" style="font-size:90%;">arXiv preprint arXiv:2109.06096</em><span class="ltx_text" id="bib.bib61.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib62.5.5.1" style="font-size:90%;">Liu et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.7.1" style="font-size:90%;">
Leo&nbsp;Z Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah&nbsp;A Smith.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.8.1" style="font-size:90%;">Probing across time: What does roberta know and when?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.9.1" style="font-size:90%;">arXiv preprint arXiv:2104.07885</em><span class="ltx_text" id="bib.bib62.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib63.5.5.1" style="font-size:90%;">Power et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.7.1" style="font-size:90%;">
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.8.1" style="font-size:90%;">Grokking: Generalization beyond overfitting on small algorithmic datasets.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.9.1" style="font-size:90%;">arXiv preprint arXiv:2201.02177</em><span class="ltx_text" id="bib.bib63.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib64.5.5.1" style="font-size:90%;">Xia et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib64.8.2" style="font-size:90%;">Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi&nbsp;Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.9.1" style="font-size:90%;">Training trajectories of language models across scales.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.10.1" style="font-size:90%;">arXiv preprint arXiv:2212.09803</em><span class="ltx_text" id="bib.bib64.11.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib65.5.5.1" style="font-size:90%;">Hernandez et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.7.1" style="font-size:90%;">
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.8.1" style="font-size:90%;">Scaling laws for transfer.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.9.1" style="font-size:90%;">arXiv preprint arXiv:2102.01293</em><span class="ltx_text" id="bib.bib65.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib66.5.5.1" style="font-size:90%;">Hoffmann et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.7.1" style="font-size:90%;">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.8.1" style="font-size:90%;">Training compute-optimal large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.9.1" style="font-size:90%;">arXiv preprint arXiv:2203.15556</em><span class="ltx_text" id="bib.bib66.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib67.5.5.1" style="font-size:90%;">Wei et&nbsp;al. [2022b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.7.1" style="font-size:90%;">
Jason Wei, Yi&nbsp;Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.8.1" style="font-size:90%;">Emergent abilities of large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.9.1" style="font-size:90%;">arXiv preprint arXiv:2206.07682</em><span class="ltx_text" id="bib.bib67.10.2" style="font-size:90%;">, 2022b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib68.5.5.1" style="font-size:90%;">Isik et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.7.1" style="font-size:90%;">
Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.8.1" style="font-size:90%;">Scaling laws for downstream task performance of large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.04177</em><span class="ltx_text" id="bib.bib68.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib69.5.5.1" style="font-size:90%;">Tirumala et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib69.8.2" style="font-size:90%;">Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.9.1" style="font-size:90%;">Memorization without overfitting: Analyzing the training dynamics of large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.10.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib69.11.2" style="font-size:90%;">, 35:38274–38290, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib70.5.5.1" style="font-size:90%;">Carlini et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.7.1" style="font-size:90%;">
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.8.1" style="font-size:90%;">Quantifying memorization across neural language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.9.1" style="font-size:90%;">arXiv preprint arXiv:2202.07646</em><span class="ltx_text" id="bib.bib70.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib71.5.5.1" style="font-size:90%;">Henighan et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.7.1" style="font-size:90%;">
Tom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort, Nicholas Schiefer, and Christopher Olah.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.8.1" style="font-size:90%;">Superposition, memorization, and double descent.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.9.1" style="font-size:90%;">Transformer Circuits Thread</em><span class="ltx_text" id="bib.bib71.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib72.5.5.1" style="font-size:90%;">Biderman et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.7.1" style="font-size:90%;">
Stella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.8.1" style="font-size:90%;">Emergent and predictable memorization in large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib72.10.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib73.5.5.1" style="font-size:90%;">Hernandez et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.7.1" style="font-size:90%;">
Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.8.1" style="font-size:90%;">Scaling laws and interpretability of learning from repeated data.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.9.1" style="font-size:90%;">arXiv preprint arXiv:2205.10487</em><span class="ltx_text" id="bib.bib73.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib74.5.5.1" style="font-size:90%;">Xue et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib74.8.2" style="font-size:90%;">Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.9.1" style="font-size:90%;">To repeat or not to repeat: Insights from scaling llm under token-crisis.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.10.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib74.11.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib75.4.4.1" style="font-size:90%;">Goodhart and Goodhart [1984]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.6.1" style="font-size:90%;">
Charles&nbsp;AE Goodhart and CAE Goodhart.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.7.1" style="font-size:90%;">Problems of monetary management: the UK experience</em><span class="ltx_text" id="bib.bib75.8.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.9.1" style="font-size:90%;">Springer, 1984.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib76.5.5.1" style="font-size:90%;">Ouyang et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.7.1" style="font-size:90%;">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.8.1" style="font-size:90%;">Training language models to follow instructions with human feedback.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.9.1" style="font-size:90%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib76.10.2" style="font-size:90%;">, 35:27730–27744, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib77.5.5.1" style="font-size:90%;">Cobbe et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.7.1" style="font-size:90%;">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.8.1" style="font-size:90%;">Training verifiers to solve math word problems, 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.9.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2110.14168" style="font-size:90%;" title="">https://arxiv.org/abs/2110.14168</a><span class="ltx_text" id="bib.bib77.10.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib78.5.5.1" style="font-size:90%;">Hendrycks et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.7.1" style="font-size:90%;">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.8.1" style="font-size:90%;">Measuring mathematical problem solving with the math dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib78.10.2" style="font-size:90%;">NIPS</em><span class="ltx_text" id="bib.bib78.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib79.5.5.1" style="font-size:90%;">Gao et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib79.8.2" style="font-size:90%;">Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.9.1" style="font-size:90%;">Pal: Program-aided language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.10.1" style="font-size:90%;">arXiv preprint arXiv:2211.10435</em><span class="ltx_text" id="bib.bib79.11.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib80.5.5.1" style="font-size:90%;">Patel et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.7.1" style="font-size:90%;">
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.8.1" style="font-size:90%;">Are NLP models really able to solve simple math word problems?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib80.10.2" style="font-size:90%;">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em><span class="ltx_text" id="bib.bib80.11.3" style="font-size:90%;">, pages 2080–2094, Online, June 2021. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.12.1" style="font-size:90%;">doi: </span><a class="ltx_ref ltx_Url" href="10.18653/v1/2021.naacl-main.168" style="font-size:90%;" title="">10.18653/v1/2021.naacl-main.168</a><span class="ltx_text" id="bib.bib80.13.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.14.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.naacl-main.168" style="font-size:90%;" title="">https://aclanthology.org/2021.naacl-main.168</a><span class="ltx_text" id="bib.bib80.15.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib81.5.5.1" style="font-size:90%;">Miao et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.7.1" style="font-size:90%;">
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.8.1" style="font-size:90%;">A diverse corpus for evaluating and developing English math word problem solvers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib81.10.2" style="font-size:90%;">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em><span class="ltx_text" id="bib.bib81.11.3" style="font-size:90%;">, pages 975–984, Online, July 2020. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.12.1" style="font-size:90%;">doi: </span><a class="ltx_ref ltx_Url" href="10.18653/v1/2020.acl-main.92" style="font-size:90%;" title="">10.18653/v1/2020.acl-main.92</a><span class="ltx_text" id="bib.bib81.13.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.14.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.92" style="font-size:90%;" title="">https://aclanthology.org/2020.acl-main.92</a><span class="ltx_text" id="bib.bib81.15.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib82.5.5.1" style="font-size:90%;">Koncel-Kedziorski et&nbsp;al. [2016]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.7.1" style="font-size:90%;">
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.8.1" style="font-size:90%;">MAWPS: A math word problem repository.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib82.10.2" style="font-size:90%;">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em><span class="ltx_text" id="bib.bib82.11.3" style="font-size:90%;">, pages 1152–1157, San Diego, California, June 2016. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.12.1" style="font-size:90%;">doi: </span><a class="ltx_ref ltx_Url" href="10.18653/v1/N16-1136" style="font-size:90%;" title="">10.18653/v1/N16-1136</a><span class="ltx_text" id="bib.bib82.13.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.14.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N16-1136" style="font-size:90%;" title="">https://aclanthology.org/N16-1136</a><span class="ltx_text" id="bib.bib82.15.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib83.5.5.1" style="font-size:90%;">Lu et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.7.1" style="font-size:90%;">
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying&nbsp;Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.8.1" style="font-size:90%;">Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib83.10.2" style="font-size:90%;">The Eleventh International Conference on Learning Representations</em><span class="ltx_text" id="bib.bib83.11.3" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.12.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=DHyHRBwJUTN" style="font-size:90%;" title="">https://openreview.net/forum?id=DHyHRBwJUTN</a><span class="ltx_text" id="bib.bib83.13.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib84.5.5.1" style="font-size:90%;">Amini et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.7.1" style="font-size:90%;">
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.8.1" style="font-size:90%;">Mathqa: Towards interpretable math word problem solving with operation-based formalisms.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.9.1" style="font-size:90%;">arXiv preprint arXiv:1905.13319</em><span class="ltx_text" id="bib.bib84.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib85.5.5.1" style="font-size:90%;">Hendrycks et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.7.1" style="font-size:90%;">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.8.1" style="font-size:90%;">Measuring massive multitask language understanding.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.9.1" style="font-size:90%;">arXiv preprint arXiv:2009.03300</em><span class="ltx_text" id="bib.bib85.10.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib86.5.5.1" style="font-size:90%;">Gao et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.7.1" style="font-size:90%;">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le&nbsp;Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.8.1" style="font-size:90%;">A framework for few-shot language model evaluation, 12 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.9.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/10256836" style="font-size:90%;" title="">https://zenodo.org/records/10256836</a><span class="ltx_text" id="bib.bib86.10.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib87.5.5.1" style="font-size:90%;">Suzgun et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.7.1" style="font-size:90%;">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V Le, Ed&nbsp;H Chi, Denny Zhou, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.8.1" style="font-size:90%;">Challenging big-bench tasks and whether chain-of-thought can solve them.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.9.1" style="font-size:90%;">arXiv preprint arXiv:2210.09261</em><span class="ltx_text" id="bib.bib87.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib88.5.5.1" style="font-size:90%;">Zhong et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.7.1" style="font-size:90%;">
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.8.1" style="font-size:90%;">Agieval: A human-centric benchmark for evaluating foundation models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.06364</em><span class="ltx_text" id="bib.bib88.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib89.5.5.1" style="font-size:90%;">Clark et&nbsp;al. [2018]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.7.1" style="font-size:90%;">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.8.1" style="font-size:90%;">Think you have solved question answering? try arc, the ai2 reasoning challenge.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.9.1" style="font-size:90%;">arXiv preprint arXiv:1803.05457</em><span class="ltx_text" id="bib.bib89.10.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib90.5.5.1" style="font-size:90%;">Clark et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.7.1" style="font-size:90%;">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.8.1" style="font-size:90%;">Boolq: Exploring the surprising difficulty of natural yes/no questions.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.9.1" style="font-size:90%;">arXiv preprint arXiv:1905.10044</em><span class="ltx_text" id="bib.bib90.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib91.5.5.1" style="font-size:90%;">Bisk et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.7.1" style="font-size:90%;">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.8.1" style="font-size:90%;">Piqa: Reasoning about physical commonsense in natural language.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib91.10.2" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</em><span class="ltx_text" id="bib.bib91.11.3" style="font-size:90%;">, volume&nbsp;34, pages 7432–7439, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib92.5.5.1" style="font-size:90%;">Zellers et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.7.1" style="font-size:90%;">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.8.1" style="font-size:90%;">Hellaswag: Can a machine really finish your sentence?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.9.1" style="font-size:90%;">arXiv preprint arXiv:1905.07830</em><span class="ltx_text" id="bib.bib92.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib93.5.5.1" style="font-size:90%;">Sakaguchi et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib93.7.1" style="font-size:90%;">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib93.8.1" style="font-size:90%;">Winogrande: An adversarial winograd schema challenge at scale.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.9.1" style="font-size:90%;">Communications of the ACM</em><span class="ltx_text" id="bib.bib93.10.2" style="font-size:90%;">, 64(9):99–106, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib94.5.5.1" style="font-size:90%;">Mihaylov et&nbsp;al. [2018]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.7.1" style="font-size:90%;">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.8.1" style="font-size:90%;">Can a suit of armor conduct electricity? a new dataset for open book question answering.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.9.1" style="font-size:90%;">arXiv preprint arXiv:1809.02789</em><span class="ltx_text" id="bib.bib94.10.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib95.5.5.1" style="font-size:90%;">Zheng et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.7.1" style="font-size:90%;">
Qinkai Zheng, Xiao Xia, Xu&nbsp;Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.8.1" style="font-size:90%;">Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib95.10.2" style="font-size:90%;">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em><span class="ltx_text" id="bib.bib95.11.3" style="font-size:90%;">, pages 5673–5684, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib96.5.5.1" style="font-size:90%;">Clark et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib96.7.1" style="font-size:90%;">
Jonathan&nbsp;H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib96.8.1" style="font-size:90%;">Tydi qa: A benchmark for information-seeking question answering in ty pologically di verse languages.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.9.1" style="font-size:90%;">Transactions of the Association for Computational Linguistics</em><span class="ltx_text" id="bib.bib96.10.2" style="font-size:90%;">, 8:454–470, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib97.5.5.1" style="font-size:90%;">Austin et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib97.7.1" style="font-size:90%;">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib97.8.1" style="font-size:90%;">Program synthesis with large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.9.1" style="font-size:90%;">arXiv preprint arXiv:2108.07732</em><span class="ltx_text" id="bib.bib97.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib98.5.5.1" style="font-size:90%;">Guo et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.7.1" style="font-size:90%;">
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y&nbsp;Wu, YK&nbsp;Li, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.8.1" style="font-size:90%;">Deepseek-coder: When the large language model meets programming–the rise of code intelligence.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.9.1" style="font-size:90%;">arXiv preprint arXiv:2401.14196</em><span class="ltx_text" id="bib.bib98.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Author Contributions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">정하오린은 세부 토큰 선정 프로세스를 설계 및 구현하고, 광범위한 예비 실험을 수행했으며, 사전 훈련 및 평가 파이프라인을 개발했으며, 대부분의 사전 훈련 실험 및 분석을 수행하고, 기준선을 구현했으며, 글쓰기에 크게 기여했다. Zhibin Gou는 예비 제안을 제시했고, 토큰 재가중치에 초과 손실을 사용하는 방법을 소개했으며, 컴파일된 고품질 말뭉치, 훈련된 참조 모델, 미세 조정 및 평가 파이프라인을 설정하고, 실험 분석을 설계했으며, 글쓰기에 크게 기여했다. 예윤공은 초기 프로젝트를 제안하고 웨이주 첸과 공동주도로 프로젝트를 진행했으며 실험과 글쓰기에 대한 광범위한 조언과 지침을 제공하고 팀 협업과 자원 관리를 총괄했다. 샤오 류, 옌룽 쉰, 뤄첸 쉬, 첸린, 유주양, 지안 자오, 난두안이 연구 멘토링을 제공하고 프로젝트를 조정했으며 글쓰기에 기여했다.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Analysis and Visualization of Tokens in Pretraining</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>More Details of Four Categories Tokens</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A2.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1094" id="A2.F10.g1" src="https://arxiv.org/html/2404.07965v1/x10.png" width="831">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F10.7.1.1" style="font-size:90%;">Figure 10</span>:</span><span class="ltx_text ltx_font_bold" id="A2.F10.8.2" style="font-size:90%;">4개의 카테고리의 토큰을 포함하는 샘플 텍스트. <span class="ltx_text ltx_font_medium" id="A2.F10.8.2.1">그 중 <span class="ltx_text" id="A2.F10.8.2.1.1" style="color:#1E90FF;">blue</span>은 categorie H→L의 토큰을 나타내고, <span class="ltx_text" id="A2.F10.8.2.1.2" style="color:#228B22;">green</span>은 categorie L→L의 토큰을 나타내고, <span class="ltx_text" id="A2.F10.8.2.1.3" style="color:#FAAA00;">yellow</span>은 categorie H→H의 토큰을 나타내고, <span class="ltx_text" id="A2.F10.8.2.1.4" style="color:#FA8072;">red</span>은 categorie L→H의 토큰을 나타낸다. </span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">토큰을 H→H, L→H, H→L, L→L의 네 가지 범주로 분류한다. 훈련 과정에서 각 10억 토큰 훈련 데이터에 대한 훈련 후 각 토큰의 손실을 수집했다. 그런 다음 선형 피팅을 사용하고 훈련 과정에서 손실이 감소했는지 여부에 대한 증거로 첫 번째 포인트와 마지막 포인트 간의 손실 차이를 취했다.</p>
</div>
<div class="ltx_para" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1">구체적으로, 토큰의 손실 <math alttext="(l_{0},l_{1},...,l_{n})" class="ltx_Math" display="inline" id="A2.SS1.p2.1.m1.4"><semantics id="A2.SS1.p2.1.m1.4a"><mrow id="A2.SS1.p2.1.m1.4.4.3" xref="A2.SS1.p2.1.m1.4.4.4.cmml"><mo id="A2.SS1.p2.1.m1.4.4.3.4" stretchy="false" xref="A2.SS1.p2.1.m1.4.4.4.cmml">(</mo><msub id="A2.SS1.p2.1.m1.2.2.1.1" xref="A2.SS1.p2.1.m1.2.2.1.1.cmml"><mi id="A2.SS1.p2.1.m1.2.2.1.1.2" xref="A2.SS1.p2.1.m1.2.2.1.1.2.cmml">l</mi><mn id="A2.SS1.p2.1.m1.2.2.1.1.3" xref="A2.SS1.p2.1.m1.2.2.1.1.3.cmml">0</mn></msub><mo id="A2.SS1.p2.1.m1.4.4.3.5" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="A2.SS1.p2.1.m1.3.3.2.2" xref="A2.SS1.p2.1.m1.3.3.2.2.cmml"><mi id="A2.SS1.p2.1.m1.3.3.2.2.2" xref="A2.SS1.p2.1.m1.3.3.2.2.2.cmml">l</mi><mn id="A2.SS1.p2.1.m1.3.3.2.2.3" xref="A2.SS1.p2.1.m1.3.3.2.2.3.cmml">1</mn></msub><mo id="A2.SS1.p2.1.m1.4.4.3.6" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><mi id="A2.SS1.p2.1.m1.1.1" mathvariant="normal" xref="A2.SS1.p2.1.m1.1.1.cmml">…</mi><mo id="A2.SS1.p2.1.m1.4.4.3.7" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="A2.SS1.p2.1.m1.4.4.3.3" xref="A2.SS1.p2.1.m1.4.4.3.3.cmml"><mi id="A2.SS1.p2.1.m1.4.4.3.3.2" xref="A2.SS1.p2.1.m1.4.4.3.3.2.cmml">l</mi><mi id="A2.SS1.p2.1.m1.4.4.3.3.3" xref="A2.SS1.p2.1.m1.4.4.3.3.3.cmml">n</mi></msub><mo id="A2.SS1.p2.1.m1.4.4.3.8" stretchy="false" xref="A2.SS1.p2.1.m1.4.4.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.1.m1.4b"><vector id="A2.SS1.p2.1.m1.4.4.4.cmml" xref="A2.SS1.p2.1.m1.4.4.3"><apply id="A2.SS1.p2.1.m1.2.2.1.1.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="A2.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1.2">𝑙</ci><cn id="A2.SS1.p2.1.m1.2.2.1.1.3.cmml" type="integer" xref="A2.SS1.p2.1.m1.2.2.1.1.3">0</cn></apply><apply id="A2.SS1.p2.1.m1.3.3.2.2.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.3.3.2.2.1.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="A2.SS1.p2.1.m1.3.3.2.2.2.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2.2">𝑙</ci><cn id="A2.SS1.p2.1.m1.3.3.2.2.3.cmml" type="integer" xref="A2.SS1.p2.1.m1.3.3.2.2.3">1</cn></apply><ci id="A2.SS1.p2.1.m1.1.1.cmml" xref="A2.SS1.p2.1.m1.1.1">…</ci><apply id="A2.SS1.p2.1.m1.4.4.3.3.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.4.4.3.3.1.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3">subscript</csymbol><ci id="A2.SS1.p2.1.m1.4.4.3.3.2.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3.2">𝑙</ci><ci id="A2.SS1.p2.1.m1.4.4.3.3.3.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3.3">𝑛</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.1.m1.4c">(l_{0},l_{1},...,l_{n})</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.1.m1.4d">( italic_l start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_l start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>의 시퀀스를 가지고 있다고 가정하자. 우리의 목표는 각 데이터 포인트 간의 차이의 제곱합과 선형 예측값을 최소화하는 것이다:</p>
</div>
<div class="ltx_para" id="A2.SS1.p3">
<table class="ltx_equation ltx_eqn_table" id="A2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f(a,b)=\text{minimize}\sum_{i=0}^{n}(l_{i}-(ax_{i}+b))^{2}," class="ltx_Math" display="block" id="A2.E6.m1.3"><semantics id="A2.E6.m1.3a"><mrow id="A2.E6.m1.3.3.1" xref="A2.E6.m1.3.3.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1" xref="A2.E6.m1.3.3.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.3" xref="A2.E6.m1.3.3.1.1.3.cmml"><mi id="A2.E6.m1.3.3.1.1.3.2" xref="A2.E6.m1.3.3.1.1.3.2.cmml">f</mi><mo id="A2.E6.m1.3.3.1.1.3.1" xref="A2.E6.m1.3.3.1.1.3.1.cmml">⁢</mo><mrow id="A2.E6.m1.3.3.1.1.3.3.2" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml"><mo id="A2.E6.m1.3.3.1.1.3.3.2.1" stretchy="false" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">(</mo><mi id="A2.E6.m1.1.1" xref="A2.E6.m1.1.1.cmml">a</mi><mo id="A2.E6.m1.3.3.1.1.3.3.2.2" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">,</mo><mi id="A2.E6.m1.2.2" xref="A2.E6.m1.2.2.cmml">b</mi><mo id="A2.E6.m1.3.3.1.1.3.3.2.3" stretchy="false" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="A2.E6.m1.3.3.1.1.2" xref="A2.E6.m1.3.3.1.1.2.cmml">=</mo><mrow id="A2.E6.m1.3.3.1.1.1" xref="A2.E6.m1.3.3.1.1.1.cmml"><mtext id="A2.E6.m1.3.3.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.3a.cmml">minimize</mtext><mo id="A2.E6.m1.3.3.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="A2.E6.m1.3.3.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.cmml"><munderover id="A2.E6.m1.3.3.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.2.cmml"><mo id="A2.E6.m1.3.3.1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="A2.E6.m1.3.3.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.2.2.3" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.2.2.3.2" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="A2.E6.m1.3.3.1.1.1.1.2.2.3.1" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="A2.E6.m1.3.3.1.1.1.1.2.2.3.3" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mi id="A2.E6.m1.3.3.1.1.1.1.2.3" xref="A2.E6.m1.3.3.1.1.1.1.2.3.cmml">n</mi></munderover><msup id="A2.E6.m1.3.3.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.2" stretchy="false" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml"><msub id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml">l</mi><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">a</mi><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">x</mi><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">b</mi></mrow><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.3" stretchy="false" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="A2.E6.m1.3.3.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><mo id="A2.E6.m1.3.3.1.2" xref="A2.E6.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E6.m1.3b"><apply id="A2.E6.m1.3.3.1.1.cmml" xref="A2.E6.m1.3.3.1"><eq id="A2.E6.m1.3.3.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.2"></eq><apply id="A2.E6.m1.3.3.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.3"><times id="A2.E6.m1.3.3.1.1.3.1.cmml" xref="A2.E6.m1.3.3.1.1.3.1"></times><ci id="A2.E6.m1.3.3.1.1.3.2.cmml" xref="A2.E6.m1.3.3.1.1.3.2">𝑓</ci><interval closure="open" id="A2.E6.m1.3.3.1.1.3.3.1.cmml" xref="A2.E6.m1.3.3.1.1.3.3.2"><ci id="A2.E6.m1.1.1.cmml" xref="A2.E6.m1.1.1">𝑎</ci><ci id="A2.E6.m1.2.2.cmml" xref="A2.E6.m1.2.2">𝑏</ci></interval></apply><apply id="A2.E6.m1.3.3.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1"><times id="A2.E6.m1.3.3.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.2"></times><ci id="A2.E6.m1.3.3.1.1.1.3a.cmml" xref="A2.E6.m1.3.3.1.1.1.3"><mtext id="A2.E6.m1.3.3.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.3">minimize</mtext></ci><apply id="A2.E6.m1.3.3.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1"><apply id="A2.E6.m1.3.3.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2">superscript</csymbol><apply id="A2.E6.m1.3.3.1.1.1.1.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.2.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="A2.E6.m1.3.3.1.1.1.1.2.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.2"></sum><apply id="A2.E6.m1.3.3.1.1.1.1.2.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3"><eq id="A2.E6.m1.3.3.1.1.1.1.2.2.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.1"></eq><ci id="A2.E6.m1.3.3.1.1.1.1.2.2.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.2">𝑖</ci><cn id="A2.E6.m1.3.3.1.1.1.1.2.2.3.3.cmml" type="integer" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.3">0</cn></apply></apply><ci id="A2.E6.m1.3.3.1.1.1.1.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.3">𝑛</ci></apply><apply id="A2.E6.m1.3.3.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1">superscript</csymbol><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1"><minus id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2"></minus><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2">𝑙</ci><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1"><plus id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2"><times id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1"></times><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑎</ci><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2">𝑥</ci><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3">𝑏</ci></apply></apply><cn id="A2.E6.m1.3.3.1.1.1.1.1.3.cmml" type="integer" xref="A2.E6.m1.3.3.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E6.m1.3c">f(a,b)=\text{minimize}\sum_{i=0}^{n}(l_{i}-(ax_{i}+b))^{2},</annotation><annotation encoding="application/x-llamapun" id="A2.E6.m1.3d">italic_f ( italic_a , italic_b ) = minimize ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - ( italic_a italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_b ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A2.SS1.p4">
<p class="ltx_p" id="A2.SS1.p4.6">여기서 <math alttext="x_{0}=0" class="ltx_Math" display="inline" id="A2.SS1.p4.1.m1.1"><semantics id="A2.SS1.p4.1.m1.1a"><mrow id="A2.SS1.p4.1.m1.1.1" xref="A2.SS1.p4.1.m1.1.1.cmml"><msub id="A2.SS1.p4.1.m1.1.1.2" xref="A2.SS1.p4.1.m1.1.1.2.cmml"><mi id="A2.SS1.p4.1.m1.1.1.2.2" xref="A2.SS1.p4.1.m1.1.1.2.2.cmml">x</mi><mn id="A2.SS1.p4.1.m1.1.1.2.3" xref="A2.SS1.p4.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="A2.SS1.p4.1.m1.1.1.1" xref="A2.SS1.p4.1.m1.1.1.1.cmml">=</mo><mn id="A2.SS1.p4.1.m1.1.1.3" xref="A2.SS1.p4.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.1.m1.1b"><apply id="A2.SS1.p4.1.m1.1.1.cmml" xref="A2.SS1.p4.1.m1.1.1"><eq id="A2.SS1.p4.1.m1.1.1.1.cmml" xref="A2.SS1.p4.1.m1.1.1.1"></eq><apply id="A2.SS1.p4.1.m1.1.1.2.cmml" xref="A2.SS1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.1.m1.1.1.2.1.cmml" xref="A2.SS1.p4.1.m1.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.1.m1.1.1.2.2.cmml" xref="A2.SS1.p4.1.m1.1.1.2.2">𝑥</ci><cn id="A2.SS1.p4.1.m1.1.1.2.3.cmml" type="integer" xref="A2.SS1.p4.1.m1.1.1.2.3">0</cn></apply><cn id="A2.SS1.p4.1.m1.1.1.3.cmml" type="integer" xref="A2.SS1.p4.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.1.m1.1c">x_{0}=0</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.1.m1.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0</annotation></semantics></math>는 초기 체크포인트이고 <math alttext="x_{n}=n" class="ltx_Math" display="inline" id="A2.SS1.p4.2.m2.1"><semantics id="A2.SS1.p4.2.m2.1a"><mrow id="A2.SS1.p4.2.m2.1.1" xref="A2.SS1.p4.2.m2.1.1.cmml"><msub id="A2.SS1.p4.2.m2.1.1.2" xref="A2.SS1.p4.2.m2.1.1.2.cmml"><mi id="A2.SS1.p4.2.m2.1.1.2.2" xref="A2.SS1.p4.2.m2.1.1.2.2.cmml">x</mi><mi id="A2.SS1.p4.2.m2.1.1.2.3" xref="A2.SS1.p4.2.m2.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p4.2.m2.1.1.1" xref="A2.SS1.p4.2.m2.1.1.1.cmml">=</mo><mi id="A2.SS1.p4.2.m2.1.1.3" xref="A2.SS1.p4.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.2.m2.1b"><apply id="A2.SS1.p4.2.m2.1.1.cmml" xref="A2.SS1.p4.2.m2.1.1"><eq id="A2.SS1.p4.2.m2.1.1.1.cmml" xref="A2.SS1.p4.2.m2.1.1.1"></eq><apply id="A2.SS1.p4.2.m2.1.1.2.cmml" xref="A2.SS1.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.2.m2.1.1.2.1.cmml" xref="A2.SS1.p4.2.m2.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.2.m2.1.1.2.2.cmml" xref="A2.SS1.p4.2.m2.1.1.2.2">𝑥</ci><ci id="A2.SS1.p4.2.m2.1.1.2.3.cmml" xref="A2.SS1.p4.2.m2.1.1.2.3">𝑛</ci></apply><ci id="A2.SS1.p4.2.m2.1.1.3.cmml" xref="A2.SS1.p4.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.2.m2.1c">x_{n}=n</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_n</annotation></semantics></math>는 최종 체크포인트이다. 이를 적합 방정식에 대입하면, 적합 후 시작과 끝에서 Loss 값을 구할 수 있다 : <math alttext="\mathcal{L}_{\text{start}}=b" class="ltx_Math" display="inline" id="A2.SS1.p4.3.m3.1"><semantics id="A2.SS1.p4.3.m3.1a"><mrow id="A2.SS1.p4.3.m3.1.1" xref="A2.SS1.p4.3.m3.1.1.cmml"><msub id="A2.SS1.p4.3.m3.1.1.2" xref="A2.SS1.p4.3.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.3.m3.1.1.2.2" xref="A2.SS1.p4.3.m3.1.1.2.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.3.m3.1.1.2.3" xref="A2.SS1.p4.3.m3.1.1.2.3a.cmml">start</mtext></msub><mo id="A2.SS1.p4.3.m3.1.1.1" xref="A2.SS1.p4.3.m3.1.1.1.cmml">=</mo><mi id="A2.SS1.p4.3.m3.1.1.3" xref="A2.SS1.p4.3.m3.1.1.3.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.3.m3.1b"><apply id="A2.SS1.p4.3.m3.1.1.cmml" xref="A2.SS1.p4.3.m3.1.1"><eq id="A2.SS1.p4.3.m3.1.1.1.cmml" xref="A2.SS1.p4.3.m3.1.1.1"></eq><apply id="A2.SS1.p4.3.m3.1.1.2.cmml" xref="A2.SS1.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.3.m3.1.1.2.1.cmml" xref="A2.SS1.p4.3.m3.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.3.m3.1.1.2.2.cmml" xref="A2.SS1.p4.3.m3.1.1.2.2">ℒ</ci><ci id="A2.SS1.p4.3.m3.1.1.2.3a.cmml" xref="A2.SS1.p4.3.m3.1.1.2.3"><mtext id="A2.SS1.p4.3.m3.1.1.2.3.cmml" mathsize="70%" xref="A2.SS1.p4.3.m3.1.1.2.3">start</mtext></ci></apply><ci id="A2.SS1.p4.3.m3.1.1.3.cmml" xref="A2.SS1.p4.3.m3.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.3.m3.1c">\mathcal{L}_{\text{start}}=b</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT start end_POSTSUBSCRIPT = italic_b</annotation></semantics></math> 및 <math alttext="\mathcal{L}_{\text{end}}=an+b" class="ltx_Math" display="inline" id="A2.SS1.p4.4.m4.1"><semantics id="A2.SS1.p4.4.m4.1a"><mrow id="A2.SS1.p4.4.m4.1.1" xref="A2.SS1.p4.4.m4.1.1.cmml"><msub id="A2.SS1.p4.4.m4.1.1.2" xref="A2.SS1.p4.4.m4.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.4.m4.1.1.2.2" xref="A2.SS1.p4.4.m4.1.1.2.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.4.m4.1.1.2.3" xref="A2.SS1.p4.4.m4.1.1.2.3a.cmml">end</mtext></msub><mo id="A2.SS1.p4.4.m4.1.1.1" xref="A2.SS1.p4.4.m4.1.1.1.cmml">=</mo><mrow id="A2.SS1.p4.4.m4.1.1.3" xref="A2.SS1.p4.4.m4.1.1.3.cmml"><mrow id="A2.SS1.p4.4.m4.1.1.3.2" xref="A2.SS1.p4.4.m4.1.1.3.2.cmml"><mi id="A2.SS1.p4.4.m4.1.1.3.2.2" xref="A2.SS1.p4.4.m4.1.1.3.2.2.cmml">a</mi><mo id="A2.SS1.p4.4.m4.1.1.3.2.1" xref="A2.SS1.p4.4.m4.1.1.3.2.1.cmml">⁢</mo><mi id="A2.SS1.p4.4.m4.1.1.3.2.3" xref="A2.SS1.p4.4.m4.1.1.3.2.3.cmml">n</mi></mrow><mo id="A2.SS1.p4.4.m4.1.1.3.1" xref="A2.SS1.p4.4.m4.1.1.3.1.cmml">+</mo><mi id="A2.SS1.p4.4.m4.1.1.3.3" xref="A2.SS1.p4.4.m4.1.1.3.3.cmml">b</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.4.m4.1b"><apply id="A2.SS1.p4.4.m4.1.1.cmml" xref="A2.SS1.p4.4.m4.1.1"><eq id="A2.SS1.p4.4.m4.1.1.1.cmml" xref="A2.SS1.p4.4.m4.1.1.1"></eq><apply id="A2.SS1.p4.4.m4.1.1.2.cmml" xref="A2.SS1.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.4.m4.1.1.2.1.cmml" xref="A2.SS1.p4.4.m4.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.4.m4.1.1.2.2.cmml" xref="A2.SS1.p4.4.m4.1.1.2.2">ℒ</ci><ci id="A2.SS1.p4.4.m4.1.1.2.3a.cmml" xref="A2.SS1.p4.4.m4.1.1.2.3"><mtext id="A2.SS1.p4.4.m4.1.1.2.3.cmml" mathsize="70%" xref="A2.SS1.p4.4.m4.1.1.2.3">end</mtext></ci></apply><apply id="A2.SS1.p4.4.m4.1.1.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3"><plus id="A2.SS1.p4.4.m4.1.1.3.1.cmml" xref="A2.SS1.p4.4.m4.1.1.3.1"></plus><apply id="A2.SS1.p4.4.m4.1.1.3.2.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2"><times id="A2.SS1.p4.4.m4.1.1.3.2.1.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.1"></times><ci id="A2.SS1.p4.4.m4.1.1.3.2.2.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.2">𝑎</ci><ci id="A2.SS1.p4.4.m4.1.1.3.2.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.3">𝑛</ci></apply><ci id="A2.SS1.p4.4.m4.1.1.3.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.4.m4.1c">\mathcal{L}_{\text{end}}=an+b</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.4.m4.1d">caligraphic_L start_POSTSUBSCRIPT end end_POSTSUBSCRIPT = italic_a italic_n + italic_b</annotation></semantics></math>. 그 후 손실의 변화는 <math alttext="\Delta\mathcal{L}=\mathcal{L}_{\text{end}}-\mathcal{L}_{\text{start}}" class="ltx_Math" display="inline" id="A2.SS1.p4.5.m5.1"><semantics id="A2.SS1.p4.5.m5.1a"><mrow id="A2.SS1.p4.5.m5.1.1" xref="A2.SS1.p4.5.m5.1.1.cmml"><mrow id="A2.SS1.p4.5.m5.1.1.2" xref="A2.SS1.p4.5.m5.1.1.2.cmml"><mi id="A2.SS1.p4.5.m5.1.1.2.2" mathvariant="normal" xref="A2.SS1.p4.5.m5.1.1.2.2.cmml">Δ</mi><mo id="A2.SS1.p4.5.m5.1.1.2.1" xref="A2.SS1.p4.5.m5.1.1.2.1.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.2.3" xref="A2.SS1.p4.5.m5.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p4.5.m5.1.1.1" xref="A2.SS1.p4.5.m5.1.1.1.cmml">=</mo><mrow id="A2.SS1.p4.5.m5.1.1.3" xref="A2.SS1.p4.5.m5.1.1.3.cmml"><msub id="A2.SS1.p4.5.m5.1.1.3.2" xref="A2.SS1.p4.5.m5.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.3.2.2" xref="A2.SS1.p4.5.m5.1.1.3.2.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.5.m5.1.1.3.2.3" xref="A2.SS1.p4.5.m5.1.1.3.2.3a.cmml">end</mtext></msub><mo id="A2.SS1.p4.5.m5.1.1.3.1" xref="A2.SS1.p4.5.m5.1.1.3.1.cmml">−</mo><msub id="A2.SS1.p4.5.m5.1.1.3.3" xref="A2.SS1.p4.5.m5.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.3.3.2" xref="A2.SS1.p4.5.m5.1.1.3.3.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.5.m5.1.1.3.3.3" xref="A2.SS1.p4.5.m5.1.1.3.3.3a.cmml">start</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.5.m5.1b"><apply id="A2.SS1.p4.5.m5.1.1.cmml" xref="A2.SS1.p4.5.m5.1.1"><eq id="A2.SS1.p4.5.m5.1.1.1.cmml" xref="A2.SS1.p4.5.m5.1.1.1"></eq><apply id="A2.SS1.p4.5.m5.1.1.2.cmml" xref="A2.SS1.p4.5.m5.1.1.2"><times id="A2.SS1.p4.5.m5.1.1.2.1.cmml" xref="A2.SS1.p4.5.m5.1.1.2.1"></times><ci id="A2.SS1.p4.5.m5.1.1.2.2.cmml" xref="A2.SS1.p4.5.m5.1.1.2.2">Δ</ci><ci id="A2.SS1.p4.5.m5.1.1.2.3.cmml" xref="A2.SS1.p4.5.m5.1.1.2.3">ℒ</ci></apply><apply id="A2.SS1.p4.5.m5.1.1.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3"><minus id="A2.SS1.p4.5.m5.1.1.3.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.1"></minus><apply id="A2.SS1.p4.5.m5.1.1.3.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="A2.SS1.p4.5.m5.1.1.3.2.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2">subscript</csymbol><ci id="A2.SS1.p4.5.m5.1.1.3.2.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2.2">ℒ</ci><ci id="A2.SS1.p4.5.m5.1.1.3.2.3a.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2.3"><mtext id="A2.SS1.p4.5.m5.1.1.3.2.3.cmml" mathsize="70%" xref="A2.SS1.p4.5.m5.1.1.3.2.3">end</mtext></ci></apply><apply id="A2.SS1.p4.5.m5.1.1.3.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="A2.SS1.p4.5.m5.1.1.3.3.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3">subscript</csymbol><ci id="A2.SS1.p4.5.m5.1.1.3.3.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3.2">ℒ</ci><ci id="A2.SS1.p4.5.m5.1.1.3.3.3a.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3.3"><mtext id="A2.SS1.p4.5.m5.1.1.3.3.3.cmml" mathsize="70%" xref="A2.SS1.p4.5.m5.1.1.3.3.3">start</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.5.m5.1c">\Delta\mathcal{L}=\mathcal{L}_{\text{end}}-\mathcal{L}_{\text{start}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.5.m5.1d">roman_Δ caligraphic_L = caligraphic_L start_POSTSUBSCRIPT end end_POSTSUBSCRIPT - caligraphic_L start_POSTSUBSCRIPT start end_POSTSUBSCRIPT</annotation></semantics></math>와 같이 표현될 수 있다. 한편, 마지막 체크포인트의 평균 Loss를 <math alttext="\mathcal{L}_{\text{mean}}" class="ltx_Math" display="inline" id="A2.SS1.p4.6.m6.1"><semantics id="A2.SS1.p4.6.m6.1a"><msub id="A2.SS1.p4.6.m6.1.1" xref="A2.SS1.p4.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.6.m6.1.1.2" xref="A2.SS1.p4.6.m6.1.1.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.6.m6.1.1.3" xref="A2.SS1.p4.6.m6.1.1.3a.cmml">mean</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.6.m6.1b"><apply id="A2.SS1.p4.6.m6.1.1.cmml" xref="A2.SS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="A2.SS1.p4.6.m6.1.1.1.cmml" xref="A2.SS1.p4.6.m6.1.1">subscript</csymbol><ci id="A2.SS1.p4.6.m6.1.1.2.cmml" xref="A2.SS1.p4.6.m6.1.1.2">ℒ</ci><ci id="A2.SS1.p4.6.m6.1.1.3a.cmml" xref="A2.SS1.p4.6.m6.1.1.3"><mtext id="A2.SS1.p4.6.m6.1.1.3.cmml" mathsize="70%" xref="A2.SS1.p4.6.m6.1.1.3">mean</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.6.m6.1c">\mathcal{L}_{\text{mean}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.6.m6.1d">caligraphic_L start_POSTSUBSCRIPT mean end_POSTSUBSCRIPT</annotation></semantics></math>로 표현한다.</p>
</div>
<div class="ltx_para" id="A2.SS1.p5">
<p class="ltx_p" id="A2.SS1.p5.7">다음으로, <math alttext="\Delta\mathcal{L}" class="ltx_Math" display="inline" id="A2.SS1.p5.1.m1.1"><semantics id="A2.SS1.p5.1.m1.1a"><mrow id="A2.SS1.p5.1.m1.1.1" xref="A2.SS1.p5.1.m1.1.1.cmml"><mi id="A2.SS1.p5.1.m1.1.1.2" mathvariant="normal" xref="A2.SS1.p5.1.m1.1.1.2.cmml">Δ</mi><mo id="A2.SS1.p5.1.m1.1.1.1" xref="A2.SS1.p5.1.m1.1.1.1.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.1.m1.1.1.3" xref="A2.SS1.p5.1.m1.1.1.3.cmml">ℒ</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.1.m1.1b"><apply id="A2.SS1.p5.1.m1.1.1.cmml" xref="A2.SS1.p5.1.m1.1.1"><times id="A2.SS1.p5.1.m1.1.1.1.cmml" xref="A2.SS1.p5.1.m1.1.1.1"></times><ci id="A2.SS1.p5.1.m1.1.1.2.cmml" xref="A2.SS1.p5.1.m1.1.1.2">Δ</ci><ci id="A2.SS1.p5.1.m1.1.1.3.cmml" xref="A2.SS1.p5.1.m1.1.1.3">ℒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.1.m1.1c">\Delta\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.1.m1.1d">roman_Δ caligraphic_L</annotation></semantics></math>와 <math alttext="\mathcal{L}_{\text{mean}}" class="ltx_Math" display="inline" id="A2.SS1.p5.2.m2.1"><semantics id="A2.SS1.p5.2.m2.1a"><msub id="A2.SS1.p5.2.m2.1.1" xref="A2.SS1.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.2.m2.1.1.2" xref="A2.SS1.p5.2.m2.1.1.2.cmml">ℒ</mi><mtext id="A2.SS1.p5.2.m2.1.1.3" xref="A2.SS1.p5.2.m2.1.1.3a.cmml">mean</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.2.m2.1b"><apply id="A2.SS1.p5.2.m2.1.1.cmml" xref="A2.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS1.p5.2.m2.1.1.1.cmml" xref="A2.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="A2.SS1.p5.2.m2.1.1.2.cmml" xref="A2.SS1.p5.2.m2.1.1.2">ℒ</ci><ci id="A2.SS1.p5.2.m2.1.1.3a.cmml" xref="A2.SS1.p5.2.m2.1.1.3"><mtext id="A2.SS1.p5.2.m2.1.1.3.cmml" mathsize="70%" xref="A2.SS1.p5.2.m2.1.1.3">mean</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.2.m2.1c">\mathcal{L}_{\text{mean}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT mean end_POSTSUBSCRIPT</annotation></semantics></math>를 기준으로 토큰을 분류할 수 있다. <math alttext="\Delta\mathcal{L}&lt;-0.2" class="ltx_Math" display="inline" id="A2.SS1.p5.3.m3.1"><semantics id="A2.SS1.p5.3.m3.1a"><mrow id="A2.SS1.p5.3.m3.1.1" xref="A2.SS1.p5.3.m3.1.1.cmml"><mrow id="A2.SS1.p5.3.m3.1.1.2" xref="A2.SS1.p5.3.m3.1.1.2.cmml"><mi id="A2.SS1.p5.3.m3.1.1.2.2" mathvariant="normal" xref="A2.SS1.p5.3.m3.1.1.2.2.cmml">Δ</mi><mo id="A2.SS1.p5.3.m3.1.1.2.1" xref="A2.SS1.p5.3.m3.1.1.2.1.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.3.m3.1.1.2.3" xref="A2.SS1.p5.3.m3.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p5.3.m3.1.1.1" xref="A2.SS1.p5.3.m3.1.1.1.cmml">&lt;</mo><mrow id="A2.SS1.p5.3.m3.1.1.3" xref="A2.SS1.p5.3.m3.1.1.3.cmml"><mo id="A2.SS1.p5.3.m3.1.1.3a" xref="A2.SS1.p5.3.m3.1.1.3.cmml">−</mo><mn id="A2.SS1.p5.3.m3.1.1.3.2" xref="A2.SS1.p5.3.m3.1.1.3.2.cmml">0.2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.3.m3.1b"><apply id="A2.SS1.p5.3.m3.1.1.cmml" xref="A2.SS1.p5.3.m3.1.1"><lt id="A2.SS1.p5.3.m3.1.1.1.cmml" xref="A2.SS1.p5.3.m3.1.1.1"></lt><apply id="A2.SS1.p5.3.m3.1.1.2.cmml" xref="A2.SS1.p5.3.m3.1.1.2"><times id="A2.SS1.p5.3.m3.1.1.2.1.cmml" xref="A2.SS1.p5.3.m3.1.1.2.1"></times><ci id="A2.SS1.p5.3.m3.1.1.2.2.cmml" xref="A2.SS1.p5.3.m3.1.1.2.2">Δ</ci><ci id="A2.SS1.p5.3.m3.1.1.2.3.cmml" xref="A2.SS1.p5.3.m3.1.1.2.3">ℒ</ci></apply><apply id="A2.SS1.p5.3.m3.1.1.3.cmml" xref="A2.SS1.p5.3.m3.1.1.3"><minus id="A2.SS1.p5.3.m3.1.1.3.1.cmml" xref="A2.SS1.p5.3.m3.1.1.3"></minus><cn id="A2.SS1.p5.3.m3.1.1.3.2.cmml" type="float" xref="A2.SS1.p5.3.m3.1.1.3.2">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.3.m3.1c">\Delta\mathcal{L}&lt;-0.2</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.3.m3.1d">roman_Δ caligraphic_L &lt; - 0.2</annotation></semantics></math>를 H→L(손실이 높음에서 낮음으로 감소) 카테고리 토큰으로, <math alttext="\Delta\mathcal{L}&gt;0.2" class="ltx_Math" display="inline" id="A2.SS1.p5.4.m4.1"><semantics id="A2.SS1.p5.4.m4.1a"><mrow id="A2.SS1.p5.4.m4.1.1" xref="A2.SS1.p5.4.m4.1.1.cmml"><mrow id="A2.SS1.p5.4.m4.1.1.2" xref="A2.SS1.p5.4.m4.1.1.2.cmml"><mi id="A2.SS1.p5.4.m4.1.1.2.2" mathvariant="normal" xref="A2.SS1.p5.4.m4.1.1.2.2.cmml">Δ</mi><mo id="A2.SS1.p5.4.m4.1.1.2.1" xref="A2.SS1.p5.4.m4.1.1.2.1.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.4.m4.1.1.2.3" xref="A2.SS1.p5.4.m4.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p5.4.m4.1.1.1" xref="A2.SS1.p5.4.m4.1.1.1.cmml">&gt;</mo><mn id="A2.SS1.p5.4.m4.1.1.3" xref="A2.SS1.p5.4.m4.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.4.m4.1b"><apply id="A2.SS1.p5.4.m4.1.1.cmml" xref="A2.SS1.p5.4.m4.1.1"><gt id="A2.SS1.p5.4.m4.1.1.1.cmml" xref="A2.SS1.p5.4.m4.1.1.1"></gt><apply id="A2.SS1.p5.4.m4.1.1.2.cmml" xref="A2.SS1.p5.4.m4.1.1.2"><times id="A2.SS1.p5.4.m4.1.1.2.1.cmml" xref="A2.SS1.p5.4.m4.1.1.2.1"></times><ci id="A2.SS1.p5.4.m4.1.1.2.2.cmml" xref="A2.SS1.p5.4.m4.1.1.2.2">Δ</ci><ci id="A2.SS1.p5.4.m4.1.1.2.3.cmml" xref="A2.SS1.p5.4.m4.1.1.2.3">ℒ</ci></apply><cn id="A2.SS1.p5.4.m4.1.1.3.cmml" type="float" xref="A2.SS1.p5.4.m4.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.4.m4.1c">\Delta\mathcal{L}&gt;0.2</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.4.m4.1d">roman_Δ caligraphic_L &gt; 0.2</annotation></semantics></math>를 L→H(손실이 낮음에서 높음으로 증가) 카테고리 토큰으로 분류한다. <math alttext="-0.2\leq\Delta\mathcal{L}\leq 0.2" class="ltx_Math" display="inline" id="A2.SS1.p5.5.m5.1"><semantics id="A2.SS1.p5.5.m5.1a"><mrow id="A2.SS1.p5.5.m5.1.1" xref="A2.SS1.p5.5.m5.1.1.cmml"><mrow id="A2.SS1.p5.5.m5.1.1.2" xref="A2.SS1.p5.5.m5.1.1.2.cmml"><mo id="A2.SS1.p5.5.m5.1.1.2a" xref="A2.SS1.p5.5.m5.1.1.2.cmml">−</mo><mn id="A2.SS1.p5.5.m5.1.1.2.2" xref="A2.SS1.p5.5.m5.1.1.2.2.cmml">0.2</mn></mrow><mo id="A2.SS1.p5.5.m5.1.1.3" xref="A2.SS1.p5.5.m5.1.1.3.cmml">≤</mo><mrow id="A2.SS1.p5.5.m5.1.1.4" xref="A2.SS1.p5.5.m5.1.1.4.cmml"><mi id="A2.SS1.p5.5.m5.1.1.4.2" mathvariant="normal" xref="A2.SS1.p5.5.m5.1.1.4.2.cmml">Δ</mi><mo id="A2.SS1.p5.5.m5.1.1.4.1" xref="A2.SS1.p5.5.m5.1.1.4.1.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.5.m5.1.1.4.3" xref="A2.SS1.p5.5.m5.1.1.4.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p5.5.m5.1.1.5" xref="A2.SS1.p5.5.m5.1.1.5.cmml">≤</mo><mn id="A2.SS1.p5.5.m5.1.1.6" xref="A2.SS1.p5.5.m5.1.1.6.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.5.m5.1b"><apply id="A2.SS1.p5.5.m5.1.1.cmml" xref="A2.SS1.p5.5.m5.1.1"><and id="A2.SS1.p5.5.m5.1.1a.cmml" xref="A2.SS1.p5.5.m5.1.1"></and><apply id="A2.SS1.p5.5.m5.1.1b.cmml" xref="A2.SS1.p5.5.m5.1.1"><leq id="A2.SS1.p5.5.m5.1.1.3.cmml" xref="A2.SS1.p5.5.m5.1.1.3"></leq><apply id="A2.SS1.p5.5.m5.1.1.2.cmml" xref="A2.SS1.p5.5.m5.1.1.2"><minus id="A2.SS1.p5.5.m5.1.1.2.1.cmml" xref="A2.SS1.p5.5.m5.1.1.2"></minus><cn id="A2.SS1.p5.5.m5.1.1.2.2.cmml" type="float" xref="A2.SS1.p5.5.m5.1.1.2.2">0.2</cn></apply><apply id="A2.SS1.p5.5.m5.1.1.4.cmml" xref="A2.SS1.p5.5.m5.1.1.4"><times id="A2.SS1.p5.5.m5.1.1.4.1.cmml" xref="A2.SS1.p5.5.m5.1.1.4.1"></times><ci id="A2.SS1.p5.5.m5.1.1.4.2.cmml" xref="A2.SS1.p5.5.m5.1.1.4.2">Δ</ci><ci id="A2.SS1.p5.5.m5.1.1.4.3.cmml" xref="A2.SS1.p5.5.m5.1.1.4.3">ℒ</ci></apply></apply><apply id="A2.SS1.p5.5.m5.1.1c.cmml" xref="A2.SS1.p5.5.m5.1.1"><leq id="A2.SS1.p5.5.m5.1.1.5.cmml" xref="A2.SS1.p5.5.m5.1.1.5"></leq><share href="#A2.SS1.p5.5.m5.1.1.4.cmml" id="A2.SS1.p5.5.m5.1.1d.cmml" xref="A2.SS1.p5.5.m5.1.1"></share><cn id="A2.SS1.p5.5.m5.1.1.6.cmml" type="float" xref="A2.SS1.p5.5.m5.1.1.6">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.5.m5.1c">-0.2\leq\Delta\mathcal{L}\leq 0.2</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.5.m5.1d">- 0.2 ≤ roman_Δ caligraphic_L ≤ 0.2</annotation></semantics></math> 및 <math alttext="l_{n}\leq\mathcal{L}_{\text{mean}}" class="ltx_Math" display="inline" id="A2.SS1.p5.6.m6.1"><semantics id="A2.SS1.p5.6.m6.1a"><mrow id="A2.SS1.p5.6.m6.1.1" xref="A2.SS1.p5.6.m6.1.1.cmml"><msub id="A2.SS1.p5.6.m6.1.1.2" xref="A2.SS1.p5.6.m6.1.1.2.cmml"><mi id="A2.SS1.p5.6.m6.1.1.2.2" xref="A2.SS1.p5.6.m6.1.1.2.2.cmml">l</mi><mi id="A2.SS1.p5.6.m6.1.1.2.3" xref="A2.SS1.p5.6.m6.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p5.6.m6.1.1.1" xref="A2.SS1.p5.6.m6.1.1.1.cmml">≤</mo><msub id="A2.SS1.p5.6.m6.1.1.3" xref="A2.SS1.p5.6.m6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.6.m6.1.1.3.2" xref="A2.SS1.p5.6.m6.1.1.3.2.cmml">ℒ</mi><mtext id="A2.SS1.p5.6.m6.1.1.3.3" xref="A2.SS1.p5.6.m6.1.1.3.3a.cmml">mean</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.6.m6.1b"><apply id="A2.SS1.p5.6.m6.1.1.cmml" xref="A2.SS1.p5.6.m6.1.1"><leq id="A2.SS1.p5.6.m6.1.1.1.cmml" xref="A2.SS1.p5.6.m6.1.1.1"></leq><apply id="A2.SS1.p5.6.m6.1.1.2.cmml" xref="A2.SS1.p5.6.m6.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p5.6.m6.1.1.2.1.cmml" xref="A2.SS1.p5.6.m6.1.1.2">subscript</csymbol><ci id="A2.SS1.p5.6.m6.1.1.2.2.cmml" xref="A2.SS1.p5.6.m6.1.1.2.2">𝑙</ci><ci id="A2.SS1.p5.6.m6.1.1.2.3.cmml" xref="A2.SS1.p5.6.m6.1.1.2.3">𝑛</ci></apply><apply id="A2.SS1.p5.6.m6.1.1.3.cmml" xref="A2.SS1.p5.6.m6.1.1.3"><csymbol cd="ambiguous" id="A2.SS1.p5.6.m6.1.1.3.1.cmml" xref="A2.SS1.p5.6.m6.1.1.3">subscript</csymbol><ci id="A2.SS1.p5.6.m6.1.1.3.2.cmml" xref="A2.SS1.p5.6.m6.1.1.3.2">ℒ</ci><ci id="A2.SS1.p5.6.m6.1.1.3.3a.cmml" xref="A2.SS1.p5.6.m6.1.1.3.3"><mtext id="A2.SS1.p5.6.m6.1.1.3.3.cmml" mathsize="70%" xref="A2.SS1.p5.6.m6.1.1.3.3">mean</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.6.m6.1c">l_{n}\leq\mathcal{L}_{\text{mean}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.6.m6.1d">italic_l start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ≤ caligraphic_L start_POSTSUBSCRIPT mean end_POSTSUBSCRIPT</annotation></semantics></math>이면, 토큰은 L→L(손실이 낮게 유지됨), <math alttext="l_{n}&gt;\mathcal{L}_{\text{mean}}" class="ltx_Math" display="inline" id="A2.SS1.p5.7.m7.1"><semantics id="A2.SS1.p5.7.m7.1a"><mrow id="A2.SS1.p5.7.m7.1.1" xref="A2.SS1.p5.7.m7.1.1.cmml"><msub id="A2.SS1.p5.7.m7.1.1.2" xref="A2.SS1.p5.7.m7.1.1.2.cmml"><mi id="A2.SS1.p5.7.m7.1.1.2.2" xref="A2.SS1.p5.7.m7.1.1.2.2.cmml">l</mi><mi id="A2.SS1.p5.7.m7.1.1.2.3" xref="A2.SS1.p5.7.m7.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p5.7.m7.1.1.1" xref="A2.SS1.p5.7.m7.1.1.1.cmml">&gt;</mo><msub id="A2.SS1.p5.7.m7.1.1.3" xref="A2.SS1.p5.7.m7.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.7.m7.1.1.3.2" xref="A2.SS1.p5.7.m7.1.1.3.2.cmml">ℒ</mi><mtext id="A2.SS1.p5.7.m7.1.1.3.3" xref="A2.SS1.p5.7.m7.1.1.3.3a.cmml">mean</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.7.m7.1b"><apply id="A2.SS1.p5.7.m7.1.1.cmml" xref="A2.SS1.p5.7.m7.1.1"><gt id="A2.SS1.p5.7.m7.1.1.1.cmml" xref="A2.SS1.p5.7.m7.1.1.1"></gt><apply id="A2.SS1.p5.7.m7.1.1.2.cmml" xref="A2.SS1.p5.7.m7.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p5.7.m7.1.1.2.1.cmml" xref="A2.SS1.p5.7.m7.1.1.2">subscript</csymbol><ci id="A2.SS1.p5.7.m7.1.1.2.2.cmml" xref="A2.SS1.p5.7.m7.1.1.2.2">𝑙</ci><ci id="A2.SS1.p5.7.m7.1.1.2.3.cmml" xref="A2.SS1.p5.7.m7.1.1.2.3">𝑛</ci></apply><apply id="A2.SS1.p5.7.m7.1.1.3.cmml" xref="A2.SS1.p5.7.m7.1.1.3"><csymbol cd="ambiguous" id="A2.SS1.p5.7.m7.1.1.3.1.cmml" xref="A2.SS1.p5.7.m7.1.1.3">subscript</csymbol><ci id="A2.SS1.p5.7.m7.1.1.3.2.cmml" xref="A2.SS1.p5.7.m7.1.1.3.2">ℒ</ci><ci id="A2.SS1.p5.7.m7.1.1.3.3a.cmml" xref="A2.SS1.p5.7.m7.1.1.3.3"><mtext id="A2.SS1.p5.7.m7.1.1.3.3.cmml" mathsize="70%" xref="A2.SS1.p5.7.m7.1.1.3.3">mean</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.7.m7.1c">l_{n}&gt;\mathcal{L}_{\text{mean}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.7.m7.1d">italic_l start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT &gt; caligraphic_L start_POSTSUBSCRIPT mean end_POSTSUBSCRIPT</annotation></semantics></math>이면, H→H(손실이 높게 유지됨)로 분류된다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A2.F10" title="Figure 10 ‣ B.1 More Details of Four Categories Tokens ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 10</span></a>에서는 토큰의 네 가지 범주의 예를 실제 텍스트로 시각화한다.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Non-Converging Tokens in Pretrainig</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A2.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="921" id="A2.F11.g1" src="https://arxiv.org/html/2404.07965v1/x11.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F11.4.1.1" style="font-size:90%;">Figure 11</span>:</span><span class="ltx_text ltx_font_bold" id="A2.F11.5.2" style="font-size:90%;">An example of abnormal state of token perplexity during pretrainig process. <span class="ltx_text ltx_font_medium" id="A2.F11.5.2.1"> <span class="ltx_text" id="A2.F11.5.2.1.1" style="color:#FF8C00;">orange</span>은 사전 훈련 프로세스 동안 상당한 이상이 있었던 토큰을 나타냅니다. </span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.SS1" title="2.1 Not All Tokens Are Equal: Training Dynamics of Token Loss ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.1</span></a>에서는 훈련 과정에서 소수의 토큰만이 H→L 범주에 속한다고 언급하였다. 나머지 범주인 H→H와 L→L 토큰 중 훈련 시 유의한 변동을 보이는 토큰이 있다. 나아가 H→L 토큰을 효과적으로 학습하지 못하는 경우도 있다. 따라서 분석에서 상당한 변동성과 뚜렷한 손실을 나타내는 이러한 범주에서 해당 토큰을 구체적으로 선택합니다. 훈련 과정에서 비정상적인 행동을 보이는 이러한 토큰을 시각화합니다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A2.F11" title="Figure 11 ‣ B.2 Non-Converging Tokens in Pretrainig ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>에 예시된 바와 같이, 우리는 이러한 토큰의 대부분이 다소 혼란스러운 말뭉치에서 비롯된다는 것을 발견한다. 예를 들어, 말뭉치는 사용자 정의 기호, 이해할 수 없는 횡설수설, 시간표 및 서지 참조와 같은 정보의 혼합을 포함할 수 있다. 정규 텍스트의 세그먼트 내에서, 또한 공통 접속사, 단어 접미사 및 구두점 표시의 사용에 변동이 있을 수 있다. 후자는 훈련에 반드시 재앙이 아닐 수 있다; 사실, 그것은 정상적인 발생을 나타낼 수 있다. 그러나 전자로 인한 손실을 효과적으로 완화할 수 있다면 보다 안정적이고 효율적인 모델 학습으로 이어질 수 있다.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Evalution Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Math Evalution</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">초등부터 대학 수준까지 다양한 어려움, 여러 수학적 영역, 객관식, 개방형 문항을 포함한 다양한 문항 유형을 포괄하여 다양한 수학 추론 벤치마크에 걸쳐 모형에 대한 종합적인 평가를 실시하였다. 우리의 벤치마크로는 GSM8k<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Cobbe et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, MATH<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, GSM-Hard<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, SVAMP<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Patel et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, ASDIV<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Miao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib81" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, MAWPS<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Koncel-Kedziorski et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>, TabMWP(TAB)<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, MathQA(MQA)<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Amini et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib84" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, MMLU-STEM<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, SAT<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Azerbayev et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> 등이 있다.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>General Evalution</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">일반 도메인 평가에서는 lm-evaluation-harness <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib86" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>와 MMLU <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, BBH <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Suzgun et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib87" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, AGIEval <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib88" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, ARC-Easy and ARC-Challenge <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib89" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, BoolQ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, PIQA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bisk et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib91" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, Hellaswag <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zellers et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib92" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, WinoGrande <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sakaguchi et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib93" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, OpenBookQA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Mihaylov et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib94" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>를 따랐다. HumanEval <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zheng et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib95" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>와 TydiQA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib96" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>에 대해 open-instrcut <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ivison et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>의 평가 파이프라인을 따르고 HumanEval의 경우 Pass@1과 Pass@10, TydiQA의 경우 F1을 보고한다. MBPP <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Austin et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib97" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> 벤치마크의 경우 DeepSeek-Coder <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Guo et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib98" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>의 평가 파이프라인을 따르고 Pass@1과 Pass@10을 보고한다.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Relate the Selected Tokens’ Loss to Downstream Task Performance</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">이 섹션에서는 선택한 토큰의 손실과 다운스트림 작업의 성능을 상관시키는 세부 정보를 선언합니다. 동시 연구는 다운스트림 태스크 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gadre et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>에서 모델의 성능에 대한 스케일링 법칙의 영향을 연구하기 위해 유사한 방법을 탐구했다. 여기에서 우리의 분석은 선택/비선택 토큰에 대한 손실 감소/증가와 다운스트림 태스크에 대한 모델의 성능 사이의 관계를 설명하는 것을 목표로 한다는 점에서 다르다.</p>
</div>
<div class="ltx_para" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">모델의 다운스트림 태스크 성능을 측정하기 위한 표준으로 MATH와 GSM8K의 평균 정확도를 사용한다. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F7" title="Figure 7 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>의 데이터 포인트 추세를 기반으로 다운스트림 태스크의 평균 정확도와 선택/비선택 토큰 손실 간의 관계를 제안하며,</p>
</div>
<div class="ltx_para" id="A4.p3">
<table class="ltx_equation ltx_eqn_table" id="A4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Acc(\mathcal{L})=\log(a*\mathcal{L}+c)" class="ltx_Math" display="block" id="A4.E7.m1.3"><semantics id="A4.E7.m1.3a"><mrow id="A4.E7.m1.3.3" xref="A4.E7.m1.3.3.cmml"><mrow id="A4.E7.m1.3.3.3" xref="A4.E7.m1.3.3.3.cmml"><mi id="A4.E7.m1.3.3.3.2" xref="A4.E7.m1.3.3.3.2.cmml">A</mi><mo id="A4.E7.m1.3.3.3.1" xref="A4.E7.m1.3.3.3.1.cmml">⁢</mo><mi id="A4.E7.m1.3.3.3.3" xref="A4.E7.m1.3.3.3.3.cmml">c</mi><mo id="A4.E7.m1.3.3.3.1a" xref="A4.E7.m1.3.3.3.1.cmml">⁢</mo><mi id="A4.E7.m1.3.3.3.4" xref="A4.E7.m1.3.3.3.4.cmml">c</mi><mo id="A4.E7.m1.3.3.3.1b" xref="A4.E7.m1.3.3.3.1.cmml">⁢</mo><mrow id="A4.E7.m1.3.3.3.5.2" xref="A4.E7.m1.3.3.3.cmml"><mo id="A4.E7.m1.3.3.3.5.2.1" stretchy="false" xref="A4.E7.m1.3.3.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="A4.E7.m1.1.1" xref="A4.E7.m1.1.1.cmml">ℒ</mi><mo id="A4.E7.m1.3.3.3.5.2.2" stretchy="false" xref="A4.E7.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="A4.E7.m1.3.3.2" xref="A4.E7.m1.3.3.2.cmml">=</mo><mrow id="A4.E7.m1.3.3.1.1" xref="A4.E7.m1.3.3.1.2.cmml"><mi id="A4.E7.m1.2.2" xref="A4.E7.m1.2.2.cmml">log</mi><mo id="A4.E7.m1.3.3.1.1a" xref="A4.E7.m1.3.3.1.2.cmml">⁡</mo><mrow id="A4.E7.m1.3.3.1.1.1" xref="A4.E7.m1.3.3.1.2.cmml"><mo id="A4.E7.m1.3.3.1.1.1.2" stretchy="false" xref="A4.E7.m1.3.3.1.2.cmml">(</mo><mrow id="A4.E7.m1.3.3.1.1.1.1" xref="A4.E7.m1.3.3.1.1.1.1.cmml"><mrow id="A4.E7.m1.3.3.1.1.1.1.2" xref="A4.E7.m1.3.3.1.1.1.1.2.cmml"><mi id="A4.E7.m1.3.3.1.1.1.1.2.2" xref="A4.E7.m1.3.3.1.1.1.1.2.2.cmml">a</mi><mo id="A4.E7.m1.3.3.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="A4.E7.m1.3.3.1.1.1.1.2.1.cmml">*</mo><mi class="ltx_font_mathcaligraphic" id="A4.E7.m1.3.3.1.1.1.1.2.3" xref="A4.E7.m1.3.3.1.1.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A4.E7.m1.3.3.1.1.1.1.1" xref="A4.E7.m1.3.3.1.1.1.1.1.cmml">+</mo><mi id="A4.E7.m1.3.3.1.1.1.1.3" xref="A4.E7.m1.3.3.1.1.1.1.3.cmml">c</mi></mrow><mo id="A4.E7.m1.3.3.1.1.1.3" stretchy="false" xref="A4.E7.m1.3.3.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.E7.m1.3b"><apply id="A4.E7.m1.3.3.cmml" xref="A4.E7.m1.3.3"><eq id="A4.E7.m1.3.3.2.cmml" xref="A4.E7.m1.3.3.2"></eq><apply id="A4.E7.m1.3.3.3.cmml" xref="A4.E7.m1.3.3.3"><times id="A4.E7.m1.3.3.3.1.cmml" xref="A4.E7.m1.3.3.3.1"></times><ci id="A4.E7.m1.3.3.3.2.cmml" xref="A4.E7.m1.3.3.3.2">𝐴</ci><ci id="A4.E7.m1.3.3.3.3.cmml" xref="A4.E7.m1.3.3.3.3">𝑐</ci><ci id="A4.E7.m1.3.3.3.4.cmml" xref="A4.E7.m1.3.3.3.4">𝑐</ci><ci id="A4.E7.m1.1.1.cmml" xref="A4.E7.m1.1.1">ℒ</ci></apply><apply id="A4.E7.m1.3.3.1.2.cmml" xref="A4.E7.m1.3.3.1.1"><log id="A4.E7.m1.2.2.cmml" xref="A4.E7.m1.2.2"></log><apply id="A4.E7.m1.3.3.1.1.1.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1"><plus id="A4.E7.m1.3.3.1.1.1.1.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1.1"></plus><apply id="A4.E7.m1.3.3.1.1.1.1.2.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2"><times id="A4.E7.m1.3.3.1.1.1.1.2.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.1"></times><ci id="A4.E7.m1.3.3.1.1.1.1.2.2.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.2">𝑎</ci><ci id="A4.E7.m1.3.3.1.1.1.1.2.3.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.3">ℒ</ci></apply><ci id="A4.E7.m1.3.3.1.1.1.1.3.cmml" xref="A4.E7.m1.3.3.1.1.1.1.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.E7.m1.3c">Acc(\mathcal{L})=\log(a*\mathcal{L}+c)</annotation><annotation encoding="application/x-llamapun" id="A4.E7.m1.3d">italic_A italic_c italic_c ( caligraphic_L ) = roman_log ( italic_a * caligraphic_L + italic_c )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A4.p4">
<p class="ltx_p" id="A4.p4.6">파라미터 <math alttext="a" class="ltx_Math" display="inline" id="A4.p4.1.m1.1"><semantics id="A4.p4.1.m1.1a"><mi id="A4.p4.1.m1.1.1" xref="A4.p4.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="A4.p4.1.m1.1b"><ci id="A4.p4.1.m1.1.1.cmml" xref="A4.p4.1.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.1.m1.1c">a</annotation><annotation encoding="application/x-llamapun" id="A4.p4.1.m1.1d">italic_a</annotation></semantics></math> 및 <math alttext="c" class="ltx_Math" display="inline" id="A4.p4.2.m2.1"><semantics id="A4.p4.2.m2.1a"><mi id="A4.p4.2.m2.1.1" xref="A4.p4.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="A4.p4.2.m2.1b"><ci id="A4.p4.2.m2.1.1.cmml" xref="A4.p4.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.2.m2.1c">c</annotation><annotation encoding="application/x-llamapun" id="A4.p4.2.m2.1d">italic_c</annotation></semantics></math>는 데이터로부터 피팅된다. 선택된 토큰의 손실 <math alttext="\mathcal{L}_{s}" class="ltx_Math" display="inline" id="A4.p4.3.m3.1"><semantics id="A4.p4.3.m3.1a"><msub id="A4.p4.3.m3.1.1" xref="A4.p4.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.p4.3.m3.1.1.2" xref="A4.p4.3.m3.1.1.2.cmml">ℒ</mi><mi id="A4.p4.3.m3.1.1.3" xref="A4.p4.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p4.3.m3.1b"><apply id="A4.p4.3.m3.1.1.cmml" xref="A4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="A4.p4.3.m3.1.1.1.cmml" xref="A4.p4.3.m3.1.1">subscript</csymbol><ci id="A4.p4.3.m3.1.1.2.cmml" xref="A4.p4.3.m3.1.1.2">ℒ</ci><ci id="A4.p4.3.m3.1.1.3.cmml" xref="A4.p4.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.3.m3.1c">\mathcal{L}_{s}</annotation><annotation encoding="application/x-llamapun" id="A4.p4.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>가 피팅에 사용된다면, <math alttext="a&gt;0" class="ltx_Math" display="inline" id="A4.p4.4.m4.1"><semantics id="A4.p4.4.m4.1a"><mrow id="A4.p4.4.m4.1.1" xref="A4.p4.4.m4.1.1.cmml"><mi id="A4.p4.4.m4.1.1.2" xref="A4.p4.4.m4.1.1.2.cmml">a</mi><mo id="A4.p4.4.m4.1.1.1" xref="A4.p4.4.m4.1.1.1.cmml">&gt;</mo><mn id="A4.p4.4.m4.1.1.3" xref="A4.p4.4.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.4.m4.1b"><apply id="A4.p4.4.m4.1.1.cmml" xref="A4.p4.4.m4.1.1"><gt id="A4.p4.4.m4.1.1.1.cmml" xref="A4.p4.4.m4.1.1.1"></gt><ci id="A4.p4.4.m4.1.1.2.cmml" xref="A4.p4.4.m4.1.1.2">𝑎</ci><cn id="A4.p4.4.m4.1.1.3.cmml" type="integer" xref="A4.p4.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.4.m4.1c">a&gt;0</annotation><annotation encoding="application/x-llamapun" id="A4.p4.4.m4.1d">italic_a &gt; 0</annotation></semantics></math>가 된다. 반대로, 선택되지 않은 토큰의 손실 <math alttext="\mathcal{L}_{us}" class="ltx_Math" display="inline" id="A4.p4.5.m5.1"><semantics id="A4.p4.5.m5.1a"><msub id="A4.p4.5.m5.1.1" xref="A4.p4.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.p4.5.m5.1.1.2" xref="A4.p4.5.m5.1.1.2.cmml">ℒ</mi><mrow id="A4.p4.5.m5.1.1.3" xref="A4.p4.5.m5.1.1.3.cmml"><mi id="A4.p4.5.m5.1.1.3.2" xref="A4.p4.5.m5.1.1.3.2.cmml">u</mi><mo id="A4.p4.5.m5.1.1.3.1" xref="A4.p4.5.m5.1.1.3.1.cmml">⁢</mo><mi id="A4.p4.5.m5.1.1.3.3" xref="A4.p4.5.m5.1.1.3.3.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.p4.5.m5.1b"><apply id="A4.p4.5.m5.1.1.cmml" xref="A4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="A4.p4.5.m5.1.1.1.cmml" xref="A4.p4.5.m5.1.1">subscript</csymbol><ci id="A4.p4.5.m5.1.1.2.cmml" xref="A4.p4.5.m5.1.1.2">ℒ</ci><apply id="A4.p4.5.m5.1.1.3.cmml" xref="A4.p4.5.m5.1.1.3"><times id="A4.p4.5.m5.1.1.3.1.cmml" xref="A4.p4.5.m5.1.1.3.1"></times><ci id="A4.p4.5.m5.1.1.3.2.cmml" xref="A4.p4.5.m5.1.1.3.2">𝑢</ci><ci id="A4.p4.5.m5.1.1.3.3.cmml" xref="A4.p4.5.m5.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.5.m5.1c">\mathcal{L}_{us}</annotation><annotation encoding="application/x-llamapun" id="A4.p4.5.m5.1d">caligraphic_L start_POSTSUBSCRIPT italic_u italic_s end_POSTSUBSCRIPT</annotation></semantics></math>가 피팅에 사용된다면, <math alttext="a&lt;0" class="ltx_Math" display="inline" id="A4.p4.6.m6.1"><semantics id="A4.p4.6.m6.1a"><mrow id="A4.p4.6.m6.1.1" xref="A4.p4.6.m6.1.1.cmml"><mi id="A4.p4.6.m6.1.1.2" xref="A4.p4.6.m6.1.1.2.cmml">a</mi><mo id="A4.p4.6.m6.1.1.1" xref="A4.p4.6.m6.1.1.1.cmml">&lt;</mo><mn id="A4.p4.6.m6.1.1.3" xref="A4.p4.6.m6.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.6.m6.1b"><apply id="A4.p4.6.m6.1.1.cmml" xref="A4.p4.6.m6.1.1"><lt id="A4.p4.6.m6.1.1.1.cmml" xref="A4.p4.6.m6.1.1.1"></lt><ci id="A4.p4.6.m6.1.1.2.cmml" xref="A4.p4.6.m6.1.1.2">𝑎</ci><cn id="A4.p4.6.m6.1.1.3.cmml" type="integer" xref="A4.p4.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.6.m6.1c">a&lt;0</annotation><annotation encoding="application/x-llamapun" id="A4.p4.6.m6.1d">italic_a &lt; 0</annotation></semantics></math>가 된다. 따라서, 우리는 선택된 토큰에 대한 모델을 훈련시키는 것이 다운스트림 태스크에 대한 모델의 성능을 효과적으로 향상시킬 수 있는 반면, 선택되지 않은 토큰은 다운스트림 태스크에서 모델의 성능에 해로운 영향을 미칠 수 있다고 믿는다.</p>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Examples of Tokens Selected by SLM</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Token Selected Examples</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A5.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1111" id="A5.F12.g1" src="https://arxiv.org/html/2404.07965v1/x12.png" width="829">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F12.4.1.1" style="font-size:90%;">그림 12</span>:</span><span class="ltx_text ltx_font_bold" id="A5.F12.5.2" style="font-size:90%;"><span class="ltx_text ltx_font_smallcaps" id="A5.F12.5.2.1">Rho-1</span>의 ToT 트레이닝 프로세스 동안 토큰을 선택하는 특정 예. <span class="ltx_text ltx_font_medium" id="A5.F12.5.2.2"> <span class="ltx_text" id="A5.F12.5.2.2.1" style="color:#1E90FF;">blue</span>은 ToT 훈련 프로세스 동안 훈련된 실제 토큰을 나타내는 반면, 나머지 검은색 토큰은 ToT 훈련 프로세스 동안 훈련되지 않습니다. </span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A5.SS1.p1">
<p class="ltx_p" id="A5.SS1.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A5.F12" title="Figure 12 ‣ E.1 Token Selected Examples ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 12</span></a>에서는 SLM 방법으로 선택한 토큰의 몇 가지 예를 제시하며, <span class="ltx_text" id="A5.SS1.p1.1.1" style="color:#1E90FF;">blue</span>은 사전 훈련 과정에서 실제로 선택한 토큰을 나타낸다.</p>
</div>
</section>
<section class="ltx_subsection" id="A5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Dynamic Token Selected</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A5.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1271" id="A5.F13.g1" src="https://arxiv.org/html/2404.07965v1/x13.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F13.7.1.1" style="font-size:90%;">그림 13</span>:</span><span class="ltx_text ltx_font_medium" id="A5.F13.8.2.1.1" style="font-size:90%;">학습 과정에서 동적 토큰 선택 변경 예제 <span class="ltx_text" id="A5.F13.8.2.1.1" style="color:#0000FF;">deep blue</span>, <span class="ltx_text" id="A5.F13.8.2.1.2" style="color:#1E90FF;">light blue</span>, black, <span class="ltx_text" id="A5.F13.8.2.1.3" style="color:#FFB496;">light orange</span>, <span class="ltx_text" id 파란색은 토큰이 선택되는 경향이 높다는 것을 나타내는 반면, 주황색이 많을수록 토큰이 선택되는 경향이 낮다는 것을 나타냅니다. </span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A5.SS2.p1">
<p class="ltx_p" id="A5.SS2.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A5.F13" title="Figure 13 ‣ E.2 Dynamic Token Selected ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure 13</span></a>에서는 SLM 훈련 과정 전반에 걸쳐 토큰 선택 경향의 동적 변화를 보여준다. 토큰 선택에서 현재 경향을 분석하기 위해 훈련 과정 중 4개의 체크포인트(0%, 33%, 66%, 100%)를 선택했다. 토큰 선택에 대한 선호도는 일반적으로 <span class="ltx_text" id="A5.SS2.p1.1.1" 스타일="color:#0000FF;">deep blue</span>, <span class="ltx_text" id="A5.SS2.p1.1.2" 스타일="color:#1E90FF;">blue</span>, black, <span class="ltx_text" id="A5.SS2.p1.1.3" 스타일="color:#FFB496;">orange</span>, <span class="ltx_text" id="A5.SS2.p1.1.4" 스타일="color:#FF6400;">dark orange</span>으로 각각 표시됩니다.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
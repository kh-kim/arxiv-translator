<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.07965] Rho-1: Not All Tokens Are What You Need</title><meta property="og:description" content="Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens.
Challenging this norm, we posit that â€œNot all tokens in a corpus are equally important for languaâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Rho-1: Not All Tokens Are What You Need">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Rho-1: Not All Tokens Are What You Need">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.07965">

<!--Generated on Sun May  5 19:41:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span id="id15.id1" class="ltx_text ltx_font_smallcaps">Rho-1</span>: Not All Tokens Are What You Need</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenghao Lin<math id="id1.1.m1.1" class="ltx_Math" alttext="~{}~{}^{\chi\phi}" display="inline"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mrow id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml"><mi id="id1.1.m1.1.1.1.2" xref="id1.1.m1.1.1.1.2.cmml">Ï‡</mi><mo lspace="0em" rspace="0em" id="id1.1.m1.1.1.1.1" xref="id1.1.m1.1.1.1.1.cmml">â€‹</mo><mi id="id1.1.m1.1.1.1.3" xref="id1.1.m1.1.1.1.3.cmml">Ï•</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><apply id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"><times id="id1.1.m1.1.1.1.1.cmml" xref="id1.1.m1.1.1.1.1"></times><ci id="id1.1.m1.1.1.1.2.cmml" xref="id1.1.m1.1.1.1.2">ğœ’</ci><ci id="id1.1.m1.1.1.1.3.cmml" xref="id1.1.m1.1.1.1.3">italic-Ï•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">~{}~{}^{\chi\phi}</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Zhibin Gou<sup id="id16.15.id1" class="ltx_sup"><span id="id16.15.id1.1" class="ltx_text ltx_font_italic">â‹†Ï€Ï•</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Yeyun Gong<math id="id3.3.m3.1" class="ltx_Math" alttext="~{}~{}^{\phi}" display="inline"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mi id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">Ï•</mi></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><ci id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1">italic-Ï•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">~{}~{}^{\phi}</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Xiao Liu<sup id="id17.16.id2" class="ltx_sup"><span id="id17.16.id2.1" class="ltx_text ltx_font_italic">Ï•</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Yelong Shen<sup id="id18.17.id3" class="ltx_sup"><span id="id18.17.id3.1" class="ltx_text ltx_font_italic">Ï•</span></sup>
<br class="ltx_break"><span id="id11.11.6" class="ltx_text ltx_font_bold">
Ruochen Xu<sup id="id11.11.6.1" class="ltx_sup"><span id="id11.11.6.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Ï•</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Chen Lin<sup id="id11.11.6.2" class="ltx_sup"><span id="id11.11.6.2.1" class="ltx_text ltx_font_medium ltx_font_italic">â‹„Ï‡</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Yujiu Yang<sup id="id11.11.6.3" class="ltx_sup"><span id="id11.11.6.3.1" class="ltx_text ltx_font_medium ltx_font_italic">â‹„Ï€</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Jian Jiao<sup id="id11.11.6.4" class="ltx_sup"><span id="id11.11.6.4.1" class="ltx_text ltx_font_medium ltx_font_italic">Ï•</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Nan Duan<sup id="id11.11.6.5" class="ltx_sup"><span id="id11.11.6.5.1" class="ltx_text ltx_font_medium ltx_font_italic">Ï•</span></sup>&nbsp;&nbsp;&nbsp;&nbsp;
Weizhu Chen<sup id="id11.11.6.6" class="ltx_sup"><span id="id11.11.6.6.1" class="ltx_text ltx_font_medium ltx_font_italic">Ï•</span></sup></span>

<br class="ltx_break"><sup id="id19.18.id4" class="ltx_sup"><span id="id19.18.id4.1" class="ltx_text ltx_font_italic">Ï‡</span></sup>Xiamen Universityâ€ƒ<sup id="id20.19.id5" class="ltx_sup"><span id="id20.19.id5.1" class="ltx_text ltx_font_italic">Ï€</span></sup>Tsinghua Universityâ€ƒ<sup id="id21.20.id6" class="ltx_sup"><span id="id21.20.id6.1" class="ltx_text ltx_font_italic">Ï•</span></sup>Microsoft 
<br class="ltx_break"><a target="_blank" href="https://aka.ms/rho" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aka.ms/rho</a>
</span><span class="ltx_author_notes">Equal contribution. See author contributions for details. Work done during their internships at Microsoft Research Asia. ğŸ–‚:&nbsp;<span id="id22.21.id1" class="ltx_text ltx_font_typewriter">zhenghaolin@stu.xmu.edu.cn</span>;&nbsp;&nbsp;<span id="id23.22.id2" class="ltx_text ltx_font_typewriter">zebgou@gmail.com</span>Correspondence authors.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id24.id1" class="ltx_p">Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens.
Challenging this norm, we posit that <span id="id24.id1.1" class="ltx_text ltx_font_italic">â€œNot all tokens in a corpus are equally important for language model trainingâ€</span>.
Our initial analysis delves into token-level training dynamics of language model, revealing distinct loss patterns for different tokens.
Leveraging these insights, we introduce a new language model called <span id="id24.id1.2" class="ltx_text ltx_font_smallcaps">Rho-1</span>. Unlike traditional LMs that learn to predict every next token in a corpus, <span id="id24.id1.3" class="ltx_text ltx_font_smallcaps">Rho-1</span> employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution.
This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher excess loss.
When continual pretraining on 15B OpenWebMath corpus, <span id="id24.id1.4" class="ltx_text ltx_font_smallcaps">Rho-1</span> yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks.
After fine-tuning, <span id="id24.id1.5" class="ltx_text ltx_font_smallcaps">Rho-1</span>-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively â€” matching DeepSeekMath with only 3% of the pretraining tokens.
Furthermore, when pretraining on 80B general tokens, <span id="id24.id1.6" class="ltx_text ltx_font_smallcaps">Rho-1</span> achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2404.07965/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.4.2" class="ltx_text" style="font-size:90%;">
We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. <span id="S0.F1.4.2.1" class="ltx_text ltx_font_smallcaps">Rho-1</span> is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling.
SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5-10x faster.
</span></figcaption>
</figure>
<figure id="S0.F2" class="ltx_figure"><img src="/html/2404.07965/assets/x2.png" id="S0.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F2.5.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S0.F2.6.2" class="ltx_text" style="font-size:90%;">
<span id="S0.F2.6.2.1" class="ltx_text ltx_font_bold">Upper:</span> Even an extensively filtered pretraining corpus contains token-level noise.
<span id="S0.F2.6.2.2" class="ltx_text ltx_font_bold">Left:</span> Previous Causal Language Modeling (CLM) trains on all tokens.
<span id="S0.F2.6.2.3" class="ltx_text ltx_font_bold">Right:</span> Our proposed Selective Language Modeling (SLM) selectively applies loss on those useful and clean tokens.
</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Scaling up model parameters and dataset size has consistently elevated the next-token prediction accuracy in large language models, yielding significant advancements in artificial intelligence <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et&nbsp;al.</span>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Brown et&nbsp;al.</span>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">OpenAI</span>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Team et&nbsp;al.</span>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
However, training on all available data is not always optimal or feasible. As a result, the practice of data filtering has become crucial, using various heuristics and classifiers <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Brown et&nbsp;al.</span>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Wenzek et&nbsp;al.</span>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> to select training documents.
These techniques significantly improve data quality and boost model performance.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, despite thorough document-level filtering, high-quality datasets still contain many noisy tokens that can negatively affect training, as illustrated in <a href="#S0.F2" title="Figure 2 â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;2</span></a> (Upper).
Removing such tokens might alter the textâ€™s meaning, while overly strict filtering could exclude useful data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Welbl et&nbsp;al.</span>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Muennighoff et&nbsp;al.</span>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and lead to biases <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dodge et&nbsp;al.</span>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Longpre et&nbsp;al.</span>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
Furthermore, research indicates that the distribution of web data does not inherently align with the ideal distribution for downstream applications&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tay et&nbsp;al.</span>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Wettig et&nbsp;al.</span>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
For example, common corpus at the token level may include undesirable content like hallucinations or highly ambiguous tokens that are hard to predict.
Applying the same loss to all tokens can result in wasted computation on non-beneficial tokens, possibly limiting LLMâ€™s potential to merely mediocre intelligence.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining.
In <a href="#S2.SS1" title="2.1 Not All Tokens Are Equal: Training Dynamics of Token Loss â€£ 2 Selective Language Modeling â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Â§2.1</span></a>, we evaluated the modelâ€™s token perplexity at different checkpoints and categorized tokens into different types.
Our findings reveal that significant loss reduction is limited to a select group of tokens during training.
Many tokens are â€œeasy tokensâ€ that are already learned, and some are â€œhard tokensâ€ that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Based on these analyses, we introduce <span id="S1.p4.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span> models trained with a novel Selective Language Modeling (SLM) objective.
As shown in <a href="#S0.F2" title="Figure 2 â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;2</span></a> (Right), this approach inputs the full sequence into the model and selectively removes the loss of undesired tokens.
The detailed pipeline is depicted in&nbsp;<a href="#S2.F4" title="Figure 4 â€£ Reference Modeling â€£ 2.2 Selective Language Modeling â€£ 2 Selective Language Modeling â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;4</span></a>:
First, SLM trains a reference language model on high-quality corpora. This model establishes utility metrics to score tokens according to the desired distribution, naturally filtering out unclean and irrelevant tokens.
Second, SLM uses the reference model to score each token in a corpus using its loss (<a href="#S2.SS2.SSS0.Px2" title="Reference Modeling â€£ 2.2 Selective Language Modeling â€£ 2 Selective Language Modeling â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Â§2.2</span></a>).
Finally, we train a language model only on those tokens that exhibit a high excess loss between the reference and the training model, selectively learning the tokens that best benefit downstream applications (<a href="#S2.SS2" title="2.2 Selective Language Modeling â€£ 2 Selective Language Modeling â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Â§2.2</span></a>).</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We show through comprehensive experiments that SLM significantly enhances token efficiency during pretraining and improves performance on downstream tasks. Furthermore, our findings indicate that SLM effectively identifies tokens relevant to the target distribution, resulting in improved perplexity scores on benchmarks for models trained with the selected tokens.
<a href="#S3.SS2" title="3.2 Math Pre-training Results â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Â§3.2</span></a> shows the effectiveness of SLM on math continual pretraining: both 1B and 7B <span id="S1.p5.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span> outperform CLM-trained baselines by over 16% on the GSM8k and MATH datasets. SLM reaches baseline accuracy up to 10x faster, as shown in <a href="#S0.F1" title="Figure 1 â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;1</span></a>.
Remarkably, <span id="S1.p5.1.2" class="ltx_text ltx_font_smallcaps">Rho-1</span>-7B matches the state-of-the-art performance of DeepSeekMath-7B using only 15B tokens, compared to the 500B tokens required by DeepSeekMath.
Upon fine-tuning, <span id="S1.p5.1.3" class="ltx_text ltx_font_smallcaps">Rho-1</span>-1B and 7B achieve 40.6% and 51.8% on MATH, respectively.
Notably, <span id="S1.p5.1.4" class="ltx_text ltx_font_smallcaps">Rho-1</span>-1B is the first 1B LM to exceed 40% accuracy, nearing the early GPT-4â€™s CoT performance of 42.5%.
<a href="#S3.SS3" title="3.3 General Pre-training Results â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Â§3.3</span></a> confirms the efficacy of SLM in general pretraining:
Training Tinyllama-1B on 80B tokens with SLM improves 6.8% on average across 15 benchmarks, with gains over 10% in code and math tasks.</p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2404.07965/assets/x3.png" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S1.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">The loss of four categories of tokens during pretraining.<span id="S1.F3.4.2.1" class="ltx_text ltx_font_medium"> (a) shows the loss of Hâ†’H, Lâ†’H, Hâ†’L, and Lâ†’L tokens during pretraining. (b) and (c) show three cases of fluctuating tokensâ€™ loss in Lâ†’L and Hâ†’H during pretraining, respectively.</span></span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Selective Language Modeling</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Not All Tokens Are Equal: Training Dynamics of Token Loss</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Our investigation begins with a critical look at how individual tokensâ€™ losses evolve during standard pre-training. We continue pre-training Tinyllama-1B with 15B tokens from OpenWebMath, saving checkpoints after every 1B tokens. We then evaluate token-level loss at these intervals using the validation set of approximately 320,000 tokens.
<a href="#S1.F3" title="Figure 3 â€£ 1 Introduction â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;3</span></a>(a) reveals a striking pattern: tokens fall into four categories based on their loss trajectoryâ€”persistent high loss (Hâ†’H), increasing loss (Lâ†’H), decreasing loss (Hâ†’L), and consistent low loss (Lâ†’L). For further details on these categories, see <a href="#A2.SS1" title="B.1 More Details of Four Categories Tokens â€£ Appendix B Analysis and Visualization of Tokens in Pretraining â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Â§B.1</span></a>.
Our analysis uncovers that a mere 26% of tokens show a notable loss reduction (Hâ†’L), while the majority (51%) remain in the Lâ†’L category, indicating they have already been learned.
Interestingly, 11% of the tokens are persistently challenging (Hâ†’H), likely due to high aleatoric uncertainty <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">HÃ¼llermeier and Waegeman</span>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.
Additionally, 12% of tokens experience an unexpected loss increase (Lâ†’H) during training.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Our second observation is that a significant number of token losses exhibit persistent fluctuations, and resist convergence. The loss of many Lâ†’L and Hâ†’H tokens, as depicted in <a href="#S1.F3" title="Figure 3 â€£ 1 Introduction â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;3</span></a> (b) and (c), show high variance during training.
In <a href="#A2.SS2" title="B.2 Non-Converging Tokens in Pretrainig â€£ Appendix B Analysis and Visualization of Tokens in Pretraining â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Â§B.2</span></a>, we visualize and analyze the content of these tokens and find that many of them are noisy, which is consistent with our hypothesis.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Consequently, we learn that the loss associated with each token during training does not decrease smoothly like the overall loss; instead, there is a complex training dynamic among different tokens.
If we can select the appropriate tokens for the model to focus on during training, we may be able to stabilize the trajectory of the modelâ€™s training and enhance its efficiency.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Selective Language Modeling</h3>

<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Overview</h4>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">Inspired by the practice of reference model in document-level filtering, we propose a simple pipeline of token-level data selection, termed â€œSelective Language Modeling (SLM)â€.
Our method comprises three steps, as depicted in <a href="#S2.F4" title="Figure 4 â€£ Reference Modeling â€£ 2.2 Selective Language Modeling â€£ 2 Selective Language Modeling â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;4</span></a>.
We begin by training a reference model on a curated, high-quality dataset. This model then assesses the loss of each token within the pretraining corpus.
In the final phase, we train the language model selectively, focusing on tokens with high excess loss between the training and reference model.
The intuition is that tokens with high excess loss are more learnable and better aligned with the desired distribution, naturally excluding tokens that are either irrelevant or of low quality.
Below, we provide a detailed description of each step.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reference Modeling</h4>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p1.2" class="ltx_p">We begin by curating a high-quality dataset that reflects the desired data distribution.
We train a reference model (RM) using standard cross-entropy loss on the curated data. The resulting RM is then used to assess the token loss within a larger pretraining corpus.
We compute the reference loss (<math id="S2.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{ref}}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">â„’</mi><mtext id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3a.cmml">ref</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2">â„’</ci><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3a.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3">ref</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.1.m1.1c">\mathcal{L}_{\text{ref}}</annotation></semantics></math>) of a token <math id="S2.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.2.m2.1a"><msub id="S2.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2">ğ‘¥</ci><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.2.m2.1c">x_{i}</annotation></semantics></math> based on the probability that the RM assigns to this token. The calculation is formalized as follows:</p>
</div>
<div id="S2.SS2.SSS0.Px2.p2" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="\mathcal{L}_{\text{ref}}(x_{i})=-\log P(x_{i}|x{<i})" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.3.2.cmml">â„’</mi><mtext id="S2.E1.m1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.3.3a.cmml">ref</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">=</mo><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><mo rspace="0.167em" id="S2.E1.m1.2.2.2a" xref="S2.E1.m1.2.2.2.cmml">âˆ’</mo><mrow id="S2.E1.m1.2.2.2.1" xref="S2.E1.m1.2.2.2.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.3" xref="S2.E1.m1.2.2.2.1.3.cmml"><mi id="S2.E1.m1.2.2.2.1.3.1" xref="S2.E1.m1.2.2.2.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E1.m1.2.2.2.1.3a" xref="S2.E1.m1.2.2.2.1.3.cmml">â¡</mo><mi id="S2.E1.m1.2.2.2.1.3.2" xref="S2.E1.m1.2.2.2.1.3.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.1.2" xref="S2.E1.m1.2.2.2.1.2.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.2.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.cmml"><msub id="S2.E1.m1.2.2.2.1.1.1.1.2.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.2.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.2.cmml">x</mi><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.2.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.3.cmml">i</mi></msub><mo fence="false" id="S2.E1.m1.2.2.2.1.1.1.1.2.1" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.cmml">|</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml">x</mi></mrow><mo id="S2.E1.m1.2.2.2.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.cmml">&lt;</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.3.cmml">i</mi></mrow><mo stretchy="false" id="S2.E1.m1.2.2.2.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3"></eq><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><times id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.2"></times><apply id="S2.E1.m1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.3.2">â„’</ci><ci id="S2.E1.m1.1.1.1.3.3a.cmml" xref="S2.E1.m1.1.1.1.3.3"><mtext mathsize="70%" id="S2.E1.m1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.3.3">ref</mtext></ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><minus id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2"></minus><apply id="S2.E1.m1.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1"><times id="S2.E1.m1.2.2.2.1.2.cmml" xref="S2.E1.m1.2.2.2.1.2"></times><apply id="S2.E1.m1.2.2.2.1.3.cmml" xref="S2.E1.m1.2.2.2.1.3"><log id="S2.E1.m1.2.2.2.1.3.1.cmml" xref="S2.E1.m1.2.2.2.1.3.1"></log><ci id="S2.E1.m1.2.2.2.1.3.2.cmml" xref="S2.E1.m1.2.2.2.1.3.2">ğ‘ƒ</ci></apply><apply id="S2.E1.m1.2.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1"><lt id="S2.E1.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1"></lt><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2"><csymbol cd="latexml" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1">conditional</csymbol><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.2">ğ‘¥</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.3">ğ‘–</ci></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3">ğ‘¥</ci></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\mathcal{L}_{\text{ref}}(x_{i})=-\log P(x_{i}|x{&lt;i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p3.1" class="ltx_p">By evaluating <math id="S2.SS2.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{ref}}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p3.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p3.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2.cmml">â„’</mi><mtext id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3a.cmml">ref</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2">â„’</ci><ci id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3a.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3"><mtext mathsize="70%" id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3">ref</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.1.m1.1c">\mathcal{L}_{\text{ref}}</annotation></semantics></math> for each token, we establish the reference loss for selective pretraining, allowing us to focus on the most influential tokens in language modeling.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2404.07965/assets/x4.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="81" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">The pipeline of Selective Language Modeling (SLM).<span id="S2.F4.4.2.1" class="ltx_text ltx_font_medium">
SLM optimizes language model performance by concentrating on valuable, clean tokens during pre-training.
It involves three steps:
(Step 1) Initially, train a reference model on high-quality data.
(Step 2) Then, score each tokenâ€™s loss in a corpus using the reference model.
(Step 3) Finally, train the language model selectively on tokens that show higher excess loss compared to the reference loss.
</span></span></figcaption>
</figure>
</section>
<section id="S2.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Selective Pretraining</h4>

<div id="S2.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p1.1" class="ltx_p">Note that causal language modeling (CLM) employs the cross-entropy loss:</p>
</div>
<div id="S2.SS2.SSS0.Px3.p2" class="ltx_para">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.3" class="ltx_Math" alttext="\mathcal{L}_{\text{CLM}}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}\log P(x_{i}|x_{<i};\theta)" display="block"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml"><mrow id="S2.E2.m1.3.3.3" xref="S2.E2.m1.3.3.3.cmml"><msub id="S2.E2.m1.3.3.3.2" xref="S2.E2.m1.3.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.3.3.3.2.2" xref="S2.E2.m1.3.3.3.2.2.cmml">â„’</mi><mtext id="S2.E2.m1.3.3.3.2.3" xref="S2.E2.m1.3.3.3.2.3a.cmml">CLM</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.3.1" xref="S2.E2.m1.3.3.3.1.cmml">â€‹</mo><mrow id="S2.E2.m1.3.3.3.3.2" xref="S2.E2.m1.3.3.3.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.3.3.2.1" xref="S2.E2.m1.3.3.3.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">Î¸</mi><mo stretchy="false" id="S2.E2.m1.3.3.3.3.2.2" xref="S2.E2.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.3.3.2" xref="S2.E2.m1.3.3.2.cmml">=</mo><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.cmml"><mo id="S2.E2.m1.3.3.1a" xref="S2.E2.m1.3.3.1.cmml">âˆ’</mo><mrow id="S2.E2.m1.3.3.1.1" xref="S2.E2.m1.3.3.1.1.cmml"><mfrac id="S2.E2.m1.3.3.1.1.3" xref="S2.E2.m1.3.3.1.1.3.cmml"><mn id="S2.E2.m1.3.3.1.1.3.2" xref="S2.E2.m1.3.3.1.1.3.2.cmml">1</mn><mi id="S2.E2.m1.3.3.1.1.3.3" xref="S2.E2.m1.3.3.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.2" xref="S2.E2.m1.3.3.1.1.2.cmml">â€‹</mo><mrow id="S2.E2.m1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.1.cmml"><munderover id="S2.E2.m1.3.3.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E2.m1.3.3.1.1.1.2.2.2" xref="S2.E2.m1.3.3.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S2.E2.m1.3.3.1.1.1.2.2.3" xref="S2.E2.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.2.2.3.2" xref="S2.E2.m1.3.3.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E2.m1.3.3.1.1.1.2.2.3.1" xref="S2.E2.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E2.m1.3.3.1.1.1.2.2.3.3" xref="S2.E2.m1.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m1.3.3.1.1.1.2.3" xref="S2.E2.m1.3.3.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S2.E2.m1.3.3.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><mrow id="S2.E2.m1.3.3.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.3.1" xref="S2.E2.m1.3.3.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E2.m1.3.3.1.1.1.1.3a" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml">â¡</mo><mi id="S2.E2.m1.3.3.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.3.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">x</mi><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">Î¸</mi></mrow></mrow><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3"><eq id="S2.E2.m1.3.3.2.cmml" xref="S2.E2.m1.3.3.2"></eq><apply id="S2.E2.m1.3.3.3.cmml" xref="S2.E2.m1.3.3.3"><times id="S2.E2.m1.3.3.3.1.cmml" xref="S2.E2.m1.3.3.3.1"></times><apply id="S2.E2.m1.3.3.3.2.cmml" xref="S2.E2.m1.3.3.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.2.1.cmml" xref="S2.E2.m1.3.3.3.2">subscript</csymbol><ci id="S2.E2.m1.3.3.3.2.2.cmml" xref="S2.E2.m1.3.3.3.2.2">â„’</ci><ci id="S2.E2.m1.3.3.3.2.3a.cmml" xref="S2.E2.m1.3.3.3.2.3"><mtext mathsize="70%" id="S2.E2.m1.3.3.3.2.3.cmml" xref="S2.E2.m1.3.3.3.2.3">CLM</mtext></ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">ğœƒ</ci></apply><apply id="S2.E2.m1.3.3.1.cmml" xref="S2.E2.m1.3.3.1"><minus id="S2.E2.m1.3.3.1.2.cmml" xref="S2.E2.m1.3.3.1"></minus><apply id="S2.E2.m1.3.3.1.1.cmml" xref="S2.E2.m1.3.3.1.1"><times id="S2.E2.m1.3.3.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.3"><divide id="S2.E2.m1.3.3.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.3"></divide><cn type="integer" id="S2.E2.m1.3.3.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.3.2">1</cn><ci id="S2.E2.m1.3.3.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.3.3">ğ‘</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1"><apply id="S2.E2.m1.3.3.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S2.E2.m1.3.3.1.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.2.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2">subscript</csymbol><sum id="S2.E2.m1.3.3.1.1.1.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.2"></sum><apply id="S2.E2.m1.3.3.1.1.1.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3"><eq id="S2.E2.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S2.E2.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S2.E2.m1.3.3.1.1.1.2.2.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.3.3.1.1.1.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2.3">ğ‘</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1"><times id="S2.E2.m1.3.3.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3"><log id="S2.E2.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.1"></log><ci id="S2.E2.m1.3.3.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.2">ğ‘ƒ</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2">ğ‘¥</ci><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><list id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><lt id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">ğœƒ</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">\mathcal{L}_{\text{CLM}}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}\log P(x_{i}|x_{&lt;i};\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.SSS0.Px3.p3" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p3.10" class="ltx_p">Here, <math id="S2.SS2.SSS0.Px3.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{L_{\text{CLM}}}(\theta)" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p3.1.m1.1.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml"><msub id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2.cmml">â„’</mi><mtext id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3a.cmml">CLM</mtext></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1.cmml">â€‹</mo><mrow id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml"><mo stretchy="false" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2.1" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml">(</mo><mi id="S2.SS2.SSS0.Px3.p3.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.1.cmml">Î¸</mi><mo stretchy="false" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2"><times id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1"></times><apply id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2">â„’</ci><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3a.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3"><mtext mathsize="70%" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3">CLM</mtext></ci></apply><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.1">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.1.m1.1c">\mathcal{L_{\text{CLM}}}(\theta)</annotation></semantics></math> represents the loss function parameterized by model <math id="S2.SS2.SSS0.Px3.p3.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.2.m2.1a"><mi id="S2.SS2.SSS0.Px3.p3.2.m2.1.1" xref="S2.SS2.SSS0.Px3.p3.2.m2.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.2.m2.1b"><ci id="S2.SS2.SSS0.Px3.p3.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.2.m2.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.2.m2.1c">\theta</annotation></semantics></math>. <math id="S2.SS2.SSS0.Px3.p3.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.3.m3.1a"><mi id="S2.SS2.SSS0.Px3.p3.3.m3.1.1" xref="S2.SS2.SSS0.Px3.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.3.m3.1b"><ci id="S2.SS2.SSS0.Px3.p3.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.3.m3.1c">N</annotation></semantics></math> is the length of the sequence, <math id="S2.SS2.SSS0.Px3.p3.4.m4.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.4.m4.1a"><msub id="S2.SS2.SSS0.Px3.p3.4.m4.1.1" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.4.m4.1b"><apply id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2">ğ‘¥</ci><ci id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.4.m4.1c">x_{i}</annotation></semantics></math> is the <math id="S2.SS2.SSS0.Px3.p3.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.5.m5.1a"><mi id="S2.SS2.SSS0.Px3.p3.5.m5.1.1" xref="S2.SS2.SSS0.Px3.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.5.m5.1b"><ci id="S2.SS2.SSS0.Px3.p3.5.m5.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.5.m5.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.5.m5.1c">i</annotation></semantics></math>-th token in the sequence, and <math id="S2.SS2.SSS0.Px3.p3.6.m6.1" class="ltx_Math" alttext="x_{<i}" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.6.m6.1a"><msub id="S2.SS2.SSS0.Px3.p3.6.m6.1.1" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2.cmml">x</mi><mrow id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2.cmml"></mi><mo id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1.cmml">&lt;</mo><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.6.m6.1b"><apply id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2">ğ‘¥</ci><apply id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3"><lt id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1"></lt><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2">absent</csymbol><ci id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.6.m6.1c">x_{&lt;i}</annotation></semantics></math> represents all tokens before the <math id="S2.SS2.SSS0.Px3.p3.7.m7.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.7.m7.1a"><mi id="S2.SS2.SSS0.Px3.p3.7.m7.1.1" xref="S2.SS2.SSS0.Px3.p3.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.7.m7.1b"><ci id="S2.SS2.SSS0.Px3.p3.7.m7.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.7.m7.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.7.m7.1c">i</annotation></semantics></math>-th token.
In contrast, Selective Language Modeling (SLM) trains the language model with a focus on tokens that exhibit a high excess loss when compared to the reference model.
The excess loss (<math id="S2.SS2.SSS0.Px3.p3.8.m8.1" class="ltx_Math" alttext="\mathcal{L}_{\Delta}" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.8.m8.1a"><msub id="S2.SS2.SSS0.Px3.p3.8.m8.1.1" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2.cmml">â„’</mi><mi mathvariant="normal" id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3.cmml">Î”</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.8.m8.1b"><apply id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2">â„’</ci><ci id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3">Î”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.8.m8.1c">\mathcal{L}_{\Delta}</annotation></semantics></math>) for a token <math id="S2.SS2.SSS0.Px3.p3.9.m9.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.9.m9.1a"><msub id="S2.SS2.SSS0.Px3.p3.9.m9.1.1" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.9.m9.1b"><apply id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2">ğ‘¥</ci><ci id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.9.m9.1c">x_{i}</annotation></semantics></math> is defined as the difference between the current training model loss (<math id="S2.SS2.SSS0.Px3.p3.10.m10.1" class="ltx_Math" alttext="\mathcal{L}_{\theta}" display="inline"><semantics id="S2.SS2.SSS0.Px3.p3.10.m10.1a"><msub id="S2.SS2.SSS0.Px3.p3.10.m10.1.1" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2.cmml">â„’</mi><mi id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.10.m10.1b"><apply id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2">â„’</ci><ci id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.10.m10.1c">\mathcal{L}_{\theta}</annotation></semantics></math>) and the reference loss:</p>
</div>
<div id="S2.SS2.SSS0.Px3.p4" class="ltx_para">
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.3" class="ltx_Math" alttext="\mathcal{L}_{\Delta}(x_{i})=\mathcal{L}_{\theta}(x_{i})-\mathcal{L}_{\text{ref}}(x_{i})" display="block"><semantics id="S2.E3.m1.3a"><mrow id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.3" xref="S2.E3.m1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.3.2.cmml">â„’</mi><mi mathvariant="normal" id="S2.E3.m1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.3.3.cmml">Î”</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.3.3.4" xref="S2.E3.m1.3.3.4.cmml">=</mo><mrow id="S2.E3.m1.3.3.3" xref="S2.E3.m1.3.3.3.cmml"><mrow id="S2.E3.m1.2.2.2.1" xref="S2.E3.m1.2.2.2.1.cmml"><msub id="S2.E3.m1.2.2.2.1.3" xref="S2.E3.m1.2.2.2.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.2.2.2.1.3.2" xref="S2.E3.m1.2.2.2.1.3.2.cmml">â„’</mi><mi id="S2.E3.m1.2.2.2.1.3.3" xref="S2.E3.m1.2.2.2.1.3.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.2.1.2" xref="S2.E3.m1.2.2.2.1.2.cmml">â€‹</mo><mrow id="S2.E3.m1.2.2.2.1.1.1" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.2.1.1.1.2" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml">(</mo><msub id="S2.E3.m1.2.2.2.1.1.1.1" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml"><mi id="S2.E3.m1.2.2.2.1.1.1.1.2" xref="S2.E3.m1.2.2.2.1.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.2.2.2.1.1.1.1.3" xref="S2.E3.m1.2.2.2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E3.m1.2.2.2.1.1.1.3" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.3.3.3.3" xref="S2.E3.m1.3.3.3.3.cmml">âˆ’</mo><mrow id="S2.E3.m1.3.3.3.2" xref="S2.E3.m1.3.3.3.2.cmml"><msub id="S2.E3.m1.3.3.3.2.3" xref="S2.E3.m1.3.3.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.3.3.3.2.3.2" xref="S2.E3.m1.3.3.3.2.3.2.cmml">â„’</mi><mtext id="S2.E3.m1.3.3.3.2.3.3" xref="S2.E3.m1.3.3.3.2.3.3a.cmml">ref</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.2.2" xref="S2.E3.m1.3.3.3.2.2.cmml">â€‹</mo><mrow id="S2.E3.m1.3.3.3.2.1.1" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.3.3.3.2.1.1.2" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml">(</mo><msub id="S2.E3.m1.3.3.3.2.1.1.1" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml"><mi id="S2.E3.m1.3.3.3.2.1.1.1.2" xref="S2.E3.m1.3.3.3.2.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.3.3.3.2.1.1.1.3" xref="S2.E3.m1.3.3.3.2.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E3.m1.3.3.3.2.1.1.3" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.3b"><apply id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3"><eq id="S2.E3.m1.3.3.4.cmml" xref="S2.E3.m1.3.3.4"></eq><apply id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><times id="S2.E3.m1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.3.2">â„’</ci><ci id="S2.E3.m1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.3.3">Î”</ci></apply><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S2.E3.m1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S2.E3.m1.3.3.3.cmml" xref="S2.E3.m1.3.3.3"><minus id="S2.E3.m1.3.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3"></minus><apply id="S2.E3.m1.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.1"><times id="S2.E3.m1.2.2.2.1.2.cmml" xref="S2.E3.m1.2.2.2.1.2"></times><apply id="S2.E3.m1.2.2.2.1.3.cmml" xref="S2.E3.m1.2.2.2.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.1.3.1.cmml" xref="S2.E3.m1.2.2.2.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.2.1.3.2.cmml" xref="S2.E3.m1.2.2.2.1.3.2">â„’</ci><ci id="S2.E3.m1.2.2.2.1.3.3.cmml" xref="S2.E3.m1.2.2.2.1.3.3">ğœƒ</ci></apply><apply id="S2.E3.m1.2.2.2.1.1.1.1.cmml" xref="S2.E3.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.2.1.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.2.1.1.1.1.2">ğ‘¥</ci><ci id="S2.E3.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.2.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S2.E3.m1.3.3.3.2.cmml" xref="S2.E3.m1.3.3.3.2"><times id="S2.E3.m1.3.3.3.2.2.cmml" xref="S2.E3.m1.3.3.3.2.2"></times><apply id="S2.E3.m1.3.3.3.2.3.cmml" xref="S2.E3.m1.3.3.3.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.2.3.1.cmml" xref="S2.E3.m1.3.3.3.2.3">subscript</csymbol><ci id="S2.E3.m1.3.3.3.2.3.2.cmml" xref="S2.E3.m1.3.3.3.2.3.2">â„’</ci><ci id="S2.E3.m1.3.3.3.2.3.3a.cmml" xref="S2.E3.m1.3.3.3.2.3.3"><mtext mathsize="70%" id="S2.E3.m1.3.3.3.2.3.3.cmml" xref="S2.E3.m1.3.3.3.2.3.3">ref</mtext></ci></apply><apply id="S2.E3.m1.3.3.3.2.1.1.1.cmml" xref="S2.E3.m1.3.3.3.2.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.2.1.1.1.1.cmml" xref="S2.E3.m1.3.3.3.2.1.1">subscript</csymbol><ci id="S2.E3.m1.3.3.3.2.1.1.1.2.cmml" xref="S2.E3.m1.3.3.3.2.1.1.1.2">ğ‘¥</ci><ci id="S2.E3.m1.3.3.3.2.1.1.1.3.cmml" xref="S2.E3.m1.3.3.3.2.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.3c">\mathcal{L}_{\Delta}(x_{i})=\mathcal{L}_{\theta}(x_{i})-\mathcal{L}_{\text{ref}}(x_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.SSS0.Px3.p5" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p5.1" class="ltx_p">We introduce a token selection ratio <math id="S2.SS2.SSS0.Px3.p5.1.m1.1" class="ltx_Math" alttext="k\%" display="inline"><semantics id="S2.SS2.SSS0.Px3.p5.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p5.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p5.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p5.1.m1.1c">k\%</annotation></semantics></math>, which determines the proportion of tokens to be included based on their excess loss.
The cross-entropy loss for the selected tokens is computed as follows:</p>
</div>
<div id="S2.SS2.SSS0.Px3.p6" class="ltx_para">
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.4" class="ltx_Math" alttext="\mathcal{L}_{\text{SLM}}(\theta)=-\frac{1}{N*k\%}\sum_{i=1}^{N}I_{k\%}(x_{i})\cdot\log P(x_{i}|x_{<i};\theta)" display="block"><semantics id="S2.E4.m1.4a"><mrow id="S2.E4.m1.4.4" xref="S2.E4.m1.4.4.cmml"><mrow id="S2.E4.m1.4.4.4" xref="S2.E4.m1.4.4.4.cmml"><msub id="S2.E4.m1.4.4.4.2" xref="S2.E4.m1.4.4.4.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.4.4.4.2.2" xref="S2.E4.m1.4.4.4.2.2.cmml">â„’</mi><mtext id="S2.E4.m1.4.4.4.2.3" xref="S2.E4.m1.4.4.4.2.3a.cmml">SLM</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.4.1" xref="S2.E4.m1.4.4.4.1.cmml">â€‹</mo><mrow id="S2.E4.m1.4.4.4.3.2" xref="S2.E4.m1.4.4.4.cmml"><mo stretchy="false" id="S2.E4.m1.4.4.4.3.2.1" xref="S2.E4.m1.4.4.4.cmml">(</mo><mi id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">Î¸</mi><mo stretchy="false" id="S2.E4.m1.4.4.4.3.2.2" xref="S2.E4.m1.4.4.4.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.4.4.3" xref="S2.E4.m1.4.4.3.cmml">=</mo><mrow id="S2.E4.m1.4.4.2" xref="S2.E4.m1.4.4.2.cmml"><mo id="S2.E4.m1.4.4.2a" xref="S2.E4.m1.4.4.2.cmml">âˆ’</mo><mrow id="S2.E4.m1.4.4.2.2" xref="S2.E4.m1.4.4.2.2.cmml"><mfrac id="S2.E4.m1.4.4.2.2.4" xref="S2.E4.m1.4.4.2.2.4.cmml"><mn id="S2.E4.m1.4.4.2.2.4.2" xref="S2.E4.m1.4.4.2.2.4.2.cmml">1</mn><mrow id="S2.E4.m1.4.4.2.2.4.3" xref="S2.E4.m1.4.4.2.2.4.3.cmml"><mi id="S2.E4.m1.4.4.2.2.4.3.2" xref="S2.E4.m1.4.4.2.2.4.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E4.m1.4.4.2.2.4.3.1" xref="S2.E4.m1.4.4.2.2.4.3.1.cmml">âˆ—</mo><mrow id="S2.E4.m1.4.4.2.2.4.3.3" xref="S2.E4.m1.4.4.2.2.4.3.3.cmml"><mi id="S2.E4.m1.4.4.2.2.4.3.3.2" xref="S2.E4.m1.4.4.2.2.4.3.3.2.cmml">k</mi><mo id="S2.E4.m1.4.4.2.2.4.3.3.1" xref="S2.E4.m1.4.4.2.2.4.3.3.1.cmml">%</mo></mrow></mrow></mfrac><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.2.2.3" xref="S2.E4.m1.4.4.2.2.3.cmml">â€‹</mo><mrow id="S2.E4.m1.4.4.2.2.2" xref="S2.E4.m1.4.4.2.2.2.cmml"><munderover id="S2.E4.m1.4.4.2.2.2.3" xref="S2.E4.m1.4.4.2.2.2.3.cmml"><mo movablelimits="false" id="S2.E4.m1.4.4.2.2.2.3.2.2" xref="S2.E4.m1.4.4.2.2.2.3.2.2.cmml">âˆ‘</mo><mrow id="S2.E4.m1.4.4.2.2.2.3.2.3" xref="S2.E4.m1.4.4.2.2.2.3.2.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.3.2.3.2" xref="S2.E4.m1.4.4.2.2.2.3.2.3.2.cmml">i</mi><mo id="S2.E4.m1.4.4.2.2.2.3.2.3.1" xref="S2.E4.m1.4.4.2.2.2.3.2.3.1.cmml">=</mo><mn id="S2.E4.m1.4.4.2.2.2.3.2.3.3" xref="S2.E4.m1.4.4.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S2.E4.m1.4.4.2.2.2.3.3" xref="S2.E4.m1.4.4.2.2.2.3.3.cmml">N</mi></munderover><mrow id="S2.E4.m1.4.4.2.2.2.2" xref="S2.E4.m1.4.4.2.2.2.2.cmml"><mrow id="S2.E4.m1.3.3.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.cmml"><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S2.E4.m1.3.3.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml">I</mi><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2.cmml">k</mi><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1.cmml">%</mo></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.3.3.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo rspace="0.055em" stretchy="false" id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S2.E4.m1.3.3.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml">â‹…</mo><mrow id="S2.E4.m1.3.3.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.3.1" xref="S2.E4.m1.3.3.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E4.m1.3.3.1.1.1.1.1.3a" xref="S2.E4.m1.3.3.1.1.1.1.1.3.cmml">â¡</mo><mi id="S2.E4.m1.3.3.1.1.1.1.1.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.3.2.cmml">P</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.2.2.2.2.3" xref="S2.E4.m1.4.4.2.2.2.2.3.cmml">â€‹</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S2.E4.m1.4.4.2.2.2.2.2.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml">(</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml"><msub id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2.cmml">x</mi><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.2.cmml">|</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml"><msub id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml">;</mo><mi id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">Î¸</mi></mrow></mrow><mo stretchy="false" id="S2.E4.m1.4.4.2.2.2.2.2.1.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.4b"><apply id="S2.E4.m1.4.4.cmml" xref="S2.E4.m1.4.4"><eq id="S2.E4.m1.4.4.3.cmml" xref="S2.E4.m1.4.4.3"></eq><apply id="S2.E4.m1.4.4.4.cmml" xref="S2.E4.m1.4.4.4"><times id="S2.E4.m1.4.4.4.1.cmml" xref="S2.E4.m1.4.4.4.1"></times><apply id="S2.E4.m1.4.4.4.2.cmml" xref="S2.E4.m1.4.4.4.2"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.4.2.1.cmml" xref="S2.E4.m1.4.4.4.2">subscript</csymbol><ci id="S2.E4.m1.4.4.4.2.2.cmml" xref="S2.E4.m1.4.4.4.2.2">â„’</ci><ci id="S2.E4.m1.4.4.4.2.3a.cmml" xref="S2.E4.m1.4.4.4.2.3"><mtext mathsize="70%" id="S2.E4.m1.4.4.4.2.3.cmml" xref="S2.E4.m1.4.4.4.2.3">SLM</mtext></ci></apply><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">ğœƒ</ci></apply><apply id="S2.E4.m1.4.4.2.cmml" xref="S2.E4.m1.4.4.2"><minus id="S2.E4.m1.4.4.2.3.cmml" xref="S2.E4.m1.4.4.2"></minus><apply id="S2.E4.m1.4.4.2.2.cmml" xref="S2.E4.m1.4.4.2.2"><times id="S2.E4.m1.4.4.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.3"></times><apply id="S2.E4.m1.4.4.2.2.4.cmml" xref="S2.E4.m1.4.4.2.2.4"><divide id="S2.E4.m1.4.4.2.2.4.1.cmml" xref="S2.E4.m1.4.4.2.2.4"></divide><cn type="integer" id="S2.E4.m1.4.4.2.2.4.2.cmml" xref="S2.E4.m1.4.4.2.2.4.2">1</cn><apply id="S2.E4.m1.4.4.2.2.4.3.cmml" xref="S2.E4.m1.4.4.2.2.4.3"><times id="S2.E4.m1.4.4.2.2.4.3.1.cmml" xref="S2.E4.m1.4.4.2.2.4.3.1"></times><ci id="S2.E4.m1.4.4.2.2.4.3.2.cmml" xref="S2.E4.m1.4.4.2.2.4.3.2">ğ‘</ci><apply id="S2.E4.m1.4.4.2.2.4.3.3.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3"><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.4.3.3.1.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3.1">percent</csymbol><ci id="S2.E4.m1.4.4.2.2.4.3.3.2.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3.2">ğ‘˜</ci></apply></apply></apply><apply id="S2.E4.m1.4.4.2.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2"><apply id="S2.E4.m1.4.4.2.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3">superscript</csymbol><apply id="S2.E4.m1.4.4.2.2.2.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.3.2.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3">subscript</csymbol><sum id="S2.E4.m1.4.4.2.2.2.3.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.2"></sum><apply id="S2.E4.m1.4.4.2.2.2.3.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3"><eq id="S2.E4.m1.4.4.2.2.2.3.2.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3.1"></eq><ci id="S2.E4.m1.4.4.2.2.2.3.2.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3.2">ğ‘–</ci><cn type="integer" id="S2.E4.m1.4.4.2.2.2.3.2.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.4.4.2.2.2.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3.3">ğ‘</ci></apply><apply id="S2.E4.m1.4.4.2.2.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2"><times id="S2.E4.m1.4.4.2.2.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.3"></times><apply id="S2.E4.m1.3.3.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1"><ci id="S2.E4.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.2">â‹…</ci><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1"><times id="S2.E4.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.2"></times><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.2">ğ¼</ci><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3"><csymbol cd="latexml" id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1">percent</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2">ğ‘˜</ci></apply></apply><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S2.E4.m1.3.3.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3"><log id="S2.E4.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3.1"></log><ci id="S2.E4.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3.2">ğ‘ƒ</ci></apply></apply><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1"><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.2">conditional</csymbol><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3">subscript</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2">ğ‘¥</ci><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3">ğ‘–</ci></apply><list id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1"><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2">ğ‘¥</ci><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3"><lt id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">ğœƒ</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.4c">\mathcal{L}_{\text{SLM}}(\theta)=-\frac{1}{N*k\%}\sum_{i=1}^{N}I_{k\%}(x_{i})\cdot\log P(x_{i}|x_{&lt;i};\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.SSS0.Px3.p7" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p7.3" class="ltx_p">Here, <math id="S2.SS2.SSS0.Px3.p7.1.m1.1" class="ltx_Math" alttext="N*k\%" display="inline"><semantics id="S2.SS2.SSS0.Px3.p7.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p7.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1.cmml">âˆ—</mo><mrow id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1"><times id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1"></times><ci id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2">ğ‘</ci><apply id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.1.m1.1c">N*k\%</annotation></semantics></math> defines the number of tokens that fall within the top <math id="S2.SS2.SSS0.Px3.p7.2.m2.1" class="ltx_Math" alttext="k\%" display="inline"><semantics id="S2.SS2.SSS0.Px3.p7.2.m2.1a"><mrow id="S2.SS2.SSS0.Px3.p7.2.m2.1.1" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.2.m2.1b"><apply id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.2.m2.1c">k\%</annotation></semantics></math> of excess loss. The indicator function <math id="S2.SS2.SSS0.Px3.p7.3.m3.1" class="ltx_Math" alttext="I_{k\%}(x_{i})" display="inline"><semantics id="S2.SS2.SSS0.Px3.p7.3.m3.1a"><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.cmml"><msub id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2.cmml">I</mi><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1.cmml">%</mo></mrow></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2.cmml">â€‹</mo><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.3.m3.1b"><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1"><times id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2"></times><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2">ğ¼</ci><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2">ğ‘˜</ci></apply></apply><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2">ğ‘¥</ci><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.3.m3.1c">I_{k\%}(x_{i})</annotation></semantics></math> is defined as:</p>
</div>
<div id="S2.SS2.SSS0.Px3.p8" class="ltx_para">
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1.5" class="ltx_Math" alttext="I_{k\%}(x_{i})=\begin{cases}1&amp;\text{if }x_{i}\text{ is in the top }k\%\text{ of }\mathcal{L}_{\Delta}\\
0&amp;\text{otherwise}\end{cases}" display="block"><semantics id="S2.E5.m1.5a"><mrow id="S2.E5.m1.5.5" xref="S2.E5.m1.5.5.cmml"><mrow id="S2.E5.m1.5.5.1" xref="S2.E5.m1.5.5.1.cmml"><msub id="S2.E5.m1.5.5.1.3" xref="S2.E5.m1.5.5.1.3.cmml"><mi id="S2.E5.m1.5.5.1.3.2" xref="S2.E5.m1.5.5.1.3.2.cmml">I</mi><mrow id="S2.E5.m1.5.5.1.3.3" xref="S2.E5.m1.5.5.1.3.3.cmml"><mi id="S2.E5.m1.5.5.1.3.3.2" xref="S2.E5.m1.5.5.1.3.3.2.cmml">k</mi><mo id="S2.E5.m1.5.5.1.3.3.1" xref="S2.E5.m1.5.5.1.3.3.1.cmml">%</mo></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.5.5.1.2" xref="S2.E5.m1.5.5.1.2.cmml">â€‹</mo><mrow id="S2.E5.m1.5.5.1.1.1" xref="S2.E5.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.5.5.1.1.1.2" xref="S2.E5.m1.5.5.1.1.1.1.cmml">(</mo><msub id="S2.E5.m1.5.5.1.1.1.1" xref="S2.E5.m1.5.5.1.1.1.1.cmml"><mi id="S2.E5.m1.5.5.1.1.1.1.2" xref="S2.E5.m1.5.5.1.1.1.1.2.cmml">x</mi><mi id="S2.E5.m1.5.5.1.1.1.1.3" xref="S2.E5.m1.5.5.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.E5.m1.5.5.1.1.1.3" xref="S2.E5.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.5.5.2" xref="S2.E5.m1.5.5.2.cmml">=</mo><mrow id="S2.E5.m1.4.4" xref="S2.E5.m1.5.5.3.1.cmml"><mo id="S2.E5.m1.4.4.5" xref="S2.E5.m1.5.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S2.E5.m1.4.4.4" xref="S2.E5.m1.5.5.3.1.cmml"><mtr id="S2.E5.m1.4.4.4a" xref="S2.E5.m1.5.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4b" xref="S2.E5.m1.5.5.3.1.cmml"><mn id="S2.E5.m1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4c" xref="S2.E5.m1.5.5.3.1.cmml"><mrow id="S2.E5.m1.2.2.2.2.2.1" xref="S2.E5.m1.2.2.2.2.2.1.cmml"><mtext id="S2.E5.m1.2.2.2.2.2.1.2" xref="S2.E5.m1.2.2.2.2.2.1.2a.cmml">if&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.2.2.1.1" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">â€‹</mo><msub id="S2.E5.m1.2.2.2.2.2.1.3" xref="S2.E5.m1.2.2.2.2.2.1.3.cmml"><mi id="S2.E5.m1.2.2.2.2.2.1.3.2" xref="S2.E5.m1.2.2.2.2.2.1.3.2.cmml">x</mi><mi id="S2.E5.m1.2.2.2.2.2.1.3.3" xref="S2.E5.m1.2.2.2.2.2.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.2.2.1.1a" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">â€‹</mo><mtext id="S2.E5.m1.2.2.2.2.2.1.4" xref="S2.E5.m1.2.2.2.2.2.1.4a.cmml">&nbsp;is in the top&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.2.2.1.1b" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">â€‹</mo><mrow id="S2.E5.m1.2.2.2.2.2.1.5" xref="S2.E5.m1.2.2.2.2.2.1.5.cmml"><mi id="S2.E5.m1.2.2.2.2.2.1.5.2" xref="S2.E5.m1.2.2.2.2.2.1.5.2.cmml">k</mi><mo id="S2.E5.m1.2.2.2.2.2.1.5.1" xref="S2.E5.m1.2.2.2.2.2.1.5.1.cmml">%</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.2.2.1.1c" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">â€‹</mo><mtext id="S2.E5.m1.2.2.2.2.2.1.6" xref="S2.E5.m1.2.2.2.2.2.1.6a.cmml">&nbsp;of&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.2.2.1.1d" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">â€‹</mo><msub id="S2.E5.m1.2.2.2.2.2.1.7" xref="S2.E5.m1.2.2.2.2.2.1.7.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.2.2.2.2.2.1.7.2" xref="S2.E5.m1.2.2.2.2.2.1.7.2.cmml">â„’</mi><mi mathvariant="normal" id="S2.E5.m1.2.2.2.2.2.1.7.3" xref="S2.E5.m1.2.2.2.2.2.1.7.3.cmml">Î”</mi></msub></mrow></mtd></mtr><mtr id="S2.E5.m1.4.4.4d" xref="S2.E5.m1.5.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4e" xref="S2.E5.m1.5.5.3.1.cmml"><mn id="S2.E5.m1.3.3.3.3.1.1" xref="S2.E5.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4f" xref="S2.E5.m1.5.5.3.1.cmml"><mtext id="S2.E5.m1.4.4.4.4.2.1" xref="S2.E5.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.5b"><apply id="S2.E5.m1.5.5.cmml" xref="S2.E5.m1.5.5"><eq id="S2.E5.m1.5.5.2.cmml" xref="S2.E5.m1.5.5.2"></eq><apply id="S2.E5.m1.5.5.1.cmml" xref="S2.E5.m1.5.5.1"><times id="S2.E5.m1.5.5.1.2.cmml" xref="S2.E5.m1.5.5.1.2"></times><apply id="S2.E5.m1.5.5.1.3.cmml" xref="S2.E5.m1.5.5.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.1.3.1.cmml" xref="S2.E5.m1.5.5.1.3">subscript</csymbol><ci id="S2.E5.m1.5.5.1.3.2.cmml" xref="S2.E5.m1.5.5.1.3.2">ğ¼</ci><apply id="S2.E5.m1.5.5.1.3.3.cmml" xref="S2.E5.m1.5.5.1.3.3"><csymbol cd="latexml" id="S2.E5.m1.5.5.1.3.3.1.cmml" xref="S2.E5.m1.5.5.1.3.3.1">percent</csymbol><ci id="S2.E5.m1.5.5.1.3.3.2.cmml" xref="S2.E5.m1.5.5.1.3.3.2">ğ‘˜</ci></apply></apply><apply id="S2.E5.m1.5.5.1.1.1.1.cmml" xref="S2.E5.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.1.1.1.1.1.cmml" xref="S2.E5.m1.5.5.1.1.1">subscript</csymbol><ci id="S2.E5.m1.5.5.1.1.1.1.2.cmml" xref="S2.E5.m1.5.5.1.1.1.1.2">ğ‘¥</ci><ci id="S2.E5.m1.5.5.1.1.1.1.3.cmml" xref="S2.E5.m1.5.5.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S2.E5.m1.5.5.3.1.cmml" xref="S2.E5.m1.4.4"><csymbol cd="latexml" id="S2.E5.m1.5.5.3.1.1.cmml" xref="S2.E5.m1.4.4.5">cases</csymbol><cn type="integer" id="S2.E5.m1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1">1</cn><apply id="S2.E5.m1.2.2.2.2.2.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1"><times id="S2.E5.m1.2.2.2.2.2.1.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.1"></times><ci id="S2.E5.m1.2.2.2.2.2.1.2a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.2"><mtext id="S2.E5.m1.2.2.2.2.2.1.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.2">if&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.2.2.2.1.3.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3">subscript</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.3.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3.2">ğ‘¥</ci><ci id="S2.E5.m1.2.2.2.2.2.1.3.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3.3">ğ‘–</ci></apply><ci id="S2.E5.m1.2.2.2.2.2.1.4a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.4"><mtext id="S2.E5.m1.2.2.2.2.2.1.4.cmml" xref="S2.E5.m1.2.2.2.2.2.1.4">&nbsp;is in the top&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.5.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5"><csymbol cd="latexml" id="S2.E5.m1.2.2.2.2.2.1.5.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5.1">percent</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.5.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5.2">ğ‘˜</ci></apply><ci id="S2.E5.m1.2.2.2.2.2.1.6a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.6"><mtext id="S2.E5.m1.2.2.2.2.2.1.6.cmml" xref="S2.E5.m1.2.2.2.2.2.1.6">&nbsp;of&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.7.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.2.2.2.1.7.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7">subscript</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.7.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7.2">â„’</ci><ci id="S2.E5.m1.2.2.2.2.2.1.7.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7.3">Î”</ci></apply></apply><cn type="integer" id="S2.E5.m1.3.3.3.3.1.1.cmml" xref="S2.E5.m1.3.3.3.3.1.1">0</cn><ci id="S2.E5.m1.4.4.4.4.2.1a.cmml" xref="S2.E5.m1.4.4.4.4.2.1"><mtext id="S2.E5.m1.4.4.4.4.2.1.cmml" xref="S2.E5.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.5c">I_{k\%}(x_{i})=\begin{cases}1&amp;\text{if }x_{i}\text{ is in the top }k\%\text{ of }\mathcal{L}_{\Delta}\\
0&amp;\text{otherwise}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.SSS0.Px3.p9" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p9.1" class="ltx_p">This ensures that the loss is applied only to the tokens that are deemed most beneficial for the language model to learn from.
In practice, token selection can be implemented by ranking the tokens in a batch according to their excess loss and using only the top <math id="S2.SS2.SSS0.Px3.p9.1.m1.1" class="ltx_Math" alttext="k\%" display="inline"><semantics id="S2.SS2.SSS0.Px3.p9.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p9.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p9.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p9.1.m1.1c">k\%</annotation></semantics></math> of tokens for training.
This process eliminates the loss for undesired tokens without incurring additional costs during pretraining, making our approach both efficient and easily integrated.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We continually pretrained models in both mathematical and general domain and designed ablation and analysis experiments to understand the effectiveness of SLM.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Setup</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reference Model Training</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">To train our mathematical reference model, we gathered a dataset of 0.5B high-quality, math-related tokens. This dataset is a blend of synthetic data from GPT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu et&nbsp;al.</span>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Huang et&nbsp;al.</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and manually curated data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yue et&nbsp;al.</span>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Ni et&nbsp;al.</span>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.
For the general reference model, we compiled a corpus of 1.9B tokens from open-source datasets, such as Tulu-v2 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ivison et&nbsp;al.</span>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> and OpenHermes-2.5 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Teknium</span>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
We trained the reference models for 3 epochs. The maximum learning rate was set at 5e-5 for 1B models and 1e-5 for 7B models, applying a cosine decay schedule.
We set the maximum sequence lengths to 2048 for 1B models and 4096 for 7B models, packing multiple samples into these lengths for model input.
In all main experiments, we initialized the continual pretraining model and the reference model with the <em id="S3.SS1.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">same</em> base model.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.22.4.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.6.3" class="ltx_text" style="font-size:90%;">
<span id="S3.T1.6.3.1" class="ltx_text ltx_font_bold">Few-shot CoT reasoning results of math pretraining.</span> All models are tested with few-shot prompting. Previous best results are highlighted in <span id="S3.T1.6.3.2" class="ltx_text" style="color:#1E90FF;">blue</span>, while our best results are in <span id="S3.T1.6.3.3" class="ltx_text" style="color:#BF0040;">purple</span>. <sup id="S3.T1.6.3.4" class="ltx_sup">âˆ—</sup>Only unique math-related tokens are calculated. For <span id="S3.T1.6.3.5" class="ltx_text ltx_font_smallcaps">Rho-1</span>, we calculate only the selected tokens that are used for training. <sup id="S3.T1.6.3.6" class="ltx_sup">â€ </sup>We use OpenAIâ€™s MATH subset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lightman et&nbsp;al.</span>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> for evaluation, since some original test samples have been used in public training sets such as PRM800k. <sup id="S3.T1.6.3.7" class="ltx_sup">â€¡</sup>The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.
</span></figcaption>
<div id="S3.T1.13" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:427.9pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-50.9pt,50.2pt) scale(0.809792493259618,0.809792493259618) ;">
<table id="S3.T1.13.7" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.10.4.4" class="ltx_tr">
<td id="S3.T1.10.4.4.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.5.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T1.7.1.1.1" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><math id="S3.T1.7.1.1.1.m1.1" class="ltx_Math" alttext="|\bm{\theta}|" display="inline"><semantics id="S3.T1.7.1.1.1.m1.1a"><mrow id="S3.T1.7.1.1.1.m1.1.2.2" xref="S3.T1.7.1.1.1.m1.1.2.1.cmml"><mo stretchy="false" id="S3.T1.7.1.1.1.m1.1.2.2.1" xref="S3.T1.7.1.1.1.m1.1.2.1.1.cmml">|</mo><mi id="S3.T1.7.1.1.1.m1.1.1" xref="S3.T1.7.1.1.1.m1.1.1.cmml">ğœ½</mi><mo stretchy="false" id="S3.T1.7.1.1.1.m1.1.2.2.2" xref="S3.T1.7.1.1.1.m1.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.1.1.1.m1.1b"><apply id="S3.T1.7.1.1.1.m1.1.2.1.cmml" xref="S3.T1.7.1.1.1.m1.1.2.2"><abs id="S3.T1.7.1.1.1.m1.1.2.1.1.cmml" xref="S3.T1.7.1.1.1.m1.1.2.2.1"></abs><ci id="S3.T1.7.1.1.1.m1.1.1.cmml" xref="S3.T1.7.1.1.1.m1.1.1">ğœ½</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.1.1.1.m1.1c">|\bm{\theta}|</annotation></semantics></math></td>
<td id="S3.T1.10.4.4.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.6.1" class="ltx_text ltx_font_bold">Data</span></td>
<td id="S3.T1.8.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T1.8.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T1.8.2.2.2.1.2" class="ltx_tr">
<td id="S3.T1.8.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.8.2.2.2.1.2.1.1" class="ltx_text ltx_font_bold">Uniq.</span></td>
</tr>
<tr id="S3.T1.8.2.2.2.1.1" class="ltx_tr">
<td id="S3.T1.8.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S3.T1.8.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Toks</span><sup id="S3.T1.8.2.2.2.1.1.1.2" class="ltx_sup">âˆ—</sup>
</td>
</tr>
</tbody></table>
</td>
<td id="S3.T1.10.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T1.10.4.4.7.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T1.10.4.4.7.1.1" class="ltx_tr">
<td id="S3.T1.10.4.4.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.7.1.1.1.1" class="ltx_text ltx_font_bold">Train</span></td>
</tr>
<tr id="S3.T1.10.4.4.7.1.2" class="ltx_tr">
<td id="S3.T1.10.4.4.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.7.1.2.1.1" class="ltx_text ltx_font_bold">Toks</span></td>
</tr>
</tbody></table>
</td>
<td id="S3.T1.10.4.4.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.8.1" class="ltx_text ltx_font_bold">GSM8K</span></td>
<td id="S3.T1.9.3.3.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.9.3.3.3.1" class="ltx_text ltx_font_bold">MATH<sup id="S3.T1.9.3.3.3.1.1" class="ltx_sup"><span id="S3.T1.9.3.3.3.1.1.1" class="ltx_text ltx_font_medium">â€ </span></sup></span></td>
<td id="S3.T1.10.4.4.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.9.1" class="ltx_text ltx_font_bold">SVAMP</span></td>
<td id="S3.T1.10.4.4.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.10.1" class="ltx_text ltx_font_bold">ASDiv</span></td>
<td id="S3.T1.10.4.4.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.11.1" class="ltx_text ltx_font_bold">MAWPS</span></td>
<td id="S3.T1.10.4.4.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.12.1" class="ltx_text ltx_font_bold">TAB</span></td>
<td id="S3.T1.10.4.4.13" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.13.1" class="ltx_text ltx_font_bold">MQA</span></td>
<td id="S3.T1.10.4.4.14" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T1.10.4.4.14.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T1.10.4.4.14.1.1" class="ltx_tr">
<td id="S3.T1.10.4.4.14.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.14.1.1.1.1" class="ltx_text ltx_font_bold">MMLU</span></td>
</tr>
<tr id="S3.T1.10.4.4.14.1.2" class="ltx_tr">
<td id="S3.T1.10.4.4.14.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.14.1.2.1.1" class="ltx_text ltx_font_bold">STEM</span></td>
</tr>
</tbody></table>
</td>
<td id="S3.T1.10.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.4.1" class="ltx_text ltx_font_bold">SAT<sup id="S3.T1.10.4.4.4.1.1" class="ltx_sup"><span id="S3.T1.10.4.4.4.1.1.1" class="ltx_text ltx_font_medium">â€¡</span></sup></span></td>
<td id="S3.T1.10.4.4.15" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.10.4.4.15.1" class="ltx_text ltx_font_bold">AVG</span></td>
</tr>
<tr id="S3.T1.13.7.8.1" class="ltx_tr">
<td id="S3.T1.13.7.8.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="15"><span id="S3.T1.13.7.8.1.1.1" class="ltx_text ltx_font_typewriter">1-2B Base Models</span></td>
</tr>
<tr id="S3.T1.13.7.9.2" class="ltx_tr">
<td id="S3.T1.13.7.9.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/Tinyllama/Tinyllama-1.1B-intermediate-step-1431k-3T" title="" class="ltx_ref ltx_href">Tinyllama</a></td>
<td id="S3.T1.13.7.9.2.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td id="S3.T1.13.7.9.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.9.2.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.9.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.9.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2.9</td>
<td id="S3.T1.13.7.9.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">3.2</td>
<td id="S3.T1.13.7.9.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">11.0</td>
<td id="S3.T1.13.7.9.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">18.1</td>
<td id="S3.T1.13.7.9.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">20.4</td>
<td id="S3.T1.13.7.9.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">12.5</td>
<td id="S3.T1.13.7.9.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14.6</td>
<td id="S3.T1.13.7.9.2.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">16.1</td>
<td id="S3.T1.13.7.9.2.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">21.9</td>
<td id="S3.T1.13.7.9.2.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">13.4</td>
</tr>
<tr id="S3.T1.13.7.10.3" class="ltx_tr">
<td id="S3.T1.13.7.10.3.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/microsoft/phi-1_5" title="" class="ltx_ref ltx_href">Phi-1.5</a></td>
<td id="S3.T1.13.7.10.3.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td id="S3.T1.13.7.10.3.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.10.3.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.10.3.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.10.3.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">32.4</td>
<td id="S3.T1.13.7.10.3.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">4.2</td>
<td id="S3.T1.13.7.10.3.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">43.4</td>
<td id="S3.T1.13.7.10.3.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">53.1</td>
<td id="S3.T1.13.7.10.3.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">66.2</td>
<td id="S3.T1.13.7.10.3.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">24.4</td>
<td id="S3.T1.13.7.10.3.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">14.3</td>
<td id="S3.T1.13.7.10.3.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">21.8</td>
<td id="S3.T1.13.7.10.3.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">18.8</td>
<td id="S3.T1.13.7.10.3.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">31.0</td>
</tr>
<tr id="S3.T1.13.7.11.4" class="ltx_tr">
<td id="S3.T1.13.7.11.4.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/Qwen/Qwen1.5-1.8B" title="" class="ltx_ref ltx_href">Qwen1.5</a></td>
<td id="S3.T1.13.7.11.4.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">1.8B</td>
<td id="S3.T1.13.7.11.4.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.11.4.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.11.4.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.11.4.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.11.4.6.1" class="ltx_text" style="color:#1E90FF;">36.1</span></td>
<td id="S3.T1.13.7.11.4.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">6.8</td>
<td id="S3.T1.13.7.11.4.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.11.4.8.1" class="ltx_text" style="color:#1E90FF;">48.5</span></td>
<td id="S3.T1.13.7.11.4.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.11.4.9.1" class="ltx_text" style="color:#1E90FF;">63.6</span></td>
<td id="S3.T1.13.7.11.4.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.11.4.10.1" class="ltx_text" style="color:#1E90FF;">79.0</span></td>
<td id="S3.T1.13.7.11.4.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">29.2</td>
<td id="S3.T1.13.7.11.4.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">25.1</td>
<td id="S3.T1.13.7.11.4.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">31.3</td>
<td id="S3.T1.13.7.11.4.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">40.6</td>
<td id="S3.T1.13.7.11.4.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.11.4.15.1" class="ltx_text" style="color:#1E90FF;">40.0</span></td>
</tr>
<tr id="S3.T1.13.7.12.5" class="ltx_tr">
<td id="S3.T1.13.7.12.5.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/google/gemma-2b" title="" class="ltx_ref ltx_href">Gemma</a></td>
<td id="S3.T1.13.7.12.5.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">2.0B</td>
<td id="S3.T1.13.7.12.5.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.12.5.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.12.5.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.12.5.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">18.8</td>
<td id="S3.T1.13.7.12.5.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">11.4</td>
<td id="S3.T1.13.7.12.5.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">38.0</td>
<td id="S3.T1.13.7.12.5.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">56.6</td>
<td id="S3.T1.13.7.12.5.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">72.5</td>
<td id="S3.T1.13.7.12.5.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.12.5.11.1" class="ltx_text" style="color:#1E90FF;">36.9</span></td>
<td id="S3.T1.13.7.12.5.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.12.5.12.1" class="ltx_text" style="color:#1E90FF;">26.8</span></td>
<td id="S3.T1.13.7.12.5.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.12.5.13.1" class="ltx_text" style="color:#1E90FF;">34.4</span></td>
<td id="S3.T1.13.7.12.5.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">50.0</td>
<td id="S3.T1.13.7.12.5.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">38.4</td>
</tr>
<tr id="S3.T1.13.7.13.6" class="ltx_tr">
<td id="S3.T1.13.7.13.6.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">DeepSeekLLM</td>
<td id="S3.T1.13.7.13.6.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td id="S3.T1.13.7.13.6.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.13.6.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.13.6.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">150B</td>
<td id="S3.T1.13.7.13.6.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">11.5</td>
<td id="S3.T1.13.7.13.6.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">8.9</td>
<td id="S3.T1.13.7.13.6.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.13.6.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.13.6.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.13.6.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.13.6.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.13.6.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">29.6</td>
<td id="S3.T1.13.7.13.6.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">31.3</td>
<td id="S3.T1.13.7.13.6.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S3.T1.13.7.14.7" class="ltx_tr">
<td id="S3.T1.13.7.14.7.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">DeepSeekMath</td>
<td id="S3.T1.13.7.14.7.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td id="S3.T1.13.7.14.7.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">120B</td>
<td id="S3.T1.13.7.14.7.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">150B</td>
<td id="S3.T1.13.7.14.7.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">23.8</td>
<td id="S3.T1.13.7.14.7.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.14.7.7.1" class="ltx_text" style="color:#1E90FF;">13.6</span></td>
<td id="S3.T1.13.7.14.7.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.14.7.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">33.1</td>
<td id="S3.T1.13.7.14.7.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.14.7.14.1" class="ltx_text" style="color:#1E90FF;">56.3</span></td>
<td id="S3.T1.13.7.14.7.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S3.T1.13.7.15.8" class="ltx_tr">
<td id="S3.T1.13.7.15.8.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="15"><span id="S3.T1.13.7.15.8.1.1" class="ltx_text ltx_font_typewriter">Continual Pretraining on Tinyllama-1B</span></td>
</tr>
<tr id="S3.T1.13.7.16.9" class="ltx_tr">
<td id="S3.T1.13.7.16.9.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Tinyllama-CT</td>
<td id="S3.T1.13.7.16.9.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td id="S3.T1.13.7.16.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.16.9.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.16.9.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">15B</td>
<td id="S3.T1.13.7.16.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">6.4</td>
<td id="S3.T1.13.7.16.9.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2.4</td>
<td id="S3.T1.13.7.16.9.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">21.7</td>
<td id="S3.T1.13.7.16.9.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">36.7</td>
<td id="S3.T1.13.7.16.9.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">47.7</td>
<td id="S3.T1.13.7.16.9.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">17.9</td>
<td id="S3.T1.13.7.16.9.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">13.9</td>
<td id="S3.T1.13.7.16.9.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">23.0</td>
<td id="S3.T1.13.7.16.9.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">25.0</td>
<td id="S3.T1.13.7.16.9.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">21.6</td>
</tr>
<tr id="S3.T1.13.7.17.10" class="ltx_tr">
<td id="S3.T1.13.7.17.10.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S3.T1.13.7.17.10.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math</td>
<td id="S3.T1.13.7.17.10.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td id="S3.T1.13.7.17.10.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.17.10.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.17.10.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">9B</td>
<td id="S3.T1.13.7.17.10.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">29.8</td>
<td id="S3.T1.13.7.17.10.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">14.0</td>
<td id="S3.T1.13.7.17.10.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">49.2</td>
<td id="S3.T1.13.7.17.10.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">61.4</td>
<td id="S3.T1.13.7.17.10.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">79.8</td>
<td id="S3.T1.13.7.17.10.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">25.8</td>
<td id="S3.T1.13.7.17.10.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">30.4</td>
<td id="S3.T1.13.7.17.10.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.17.10.13.1" class="ltx_text" style="color:#BF0040;">24.7</span></td>
<td id="S3.T1.13.7.17.10.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">28.1</td>
<td id="S3.T1.13.7.17.10.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">38.1</td>
</tr>
<tr id="S3.T1.11.5.5" class="ltx_tr">
<td id="S3.T1.11.5.5.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><math id="S3.T1.11.5.5.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T1.11.5.5.1.m1.1a"><mi mathvariant="normal" id="S3.T1.11.5.5.1.m1.1.1" xref="S3.T1.11.5.5.1.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S3.T1.11.5.5.1.m1.1b"><ci id="S3.T1.11.5.5.1.m1.1.1.cmml" xref="S3.T1.11.5.5.1.m1.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.5.5.1.m1.1c">\Delta</annotation></semantics></math></td>
<td id="S3.T1.11.5.5.2" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.11.5.5.3" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.11.5.5.4" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.11.5.5.5" class="ltx_td ltx_align_right ltx_border_r" style="background-color:#E6E6E6;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.5.1" class="ltx_text" style="background-color:#E6E6E6;">-40%</span></td>
<td id="S3.T1.11.5.5.6" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.6.1" class="ltx_text" style="background-color:#D2DCFA;">+23.4</span></td>
<td id="S3.T1.11.5.5.7" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.7.1" class="ltx_text" style="background-color:#D2DCFA;">+11.6</span></td>
<td id="S3.T1.11.5.5.8" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.8.1" class="ltx_text" style="background-color:#D2DCFA;">+27.5</span></td>
<td id="S3.T1.11.5.5.9" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.9.1" class="ltx_text" style="background-color:#D2DCFA;">+24.7</span></td>
<td id="S3.T1.11.5.5.10" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.10.1" class="ltx_text" style="background-color:#D2DCFA;">+32.1</span></td>
<td id="S3.T1.11.5.5.11" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.11.1" class="ltx_text" style="background-color:#D2DCFA;">+7.9</span></td>
<td id="S3.T1.11.5.5.12" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.12.1" class="ltx_text" style="background-color:#D2DCFA;">+16.5</span></td>
<td id="S3.T1.11.5.5.13" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.13.1" class="ltx_text" style="background-color:#D2DCFA;">+1.7</span></td>
<td id="S3.T1.11.5.5.14" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.14.1" class="ltx_text" style="background-color:#D2DCFA;">+3.1</span></td>
<td id="S3.T1.11.5.5.15" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.11.5.5.15.1" class="ltx_text ltx_font_bold" style="background-color:#D2DCFA;">+16.5</span></td>
</tr>
<tr id="S3.T1.13.7.18.11" class="ltx_tr">
<td id="S3.T1.13.7.18.11.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S3.T1.13.7.18.11.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math</td>
<td id="S3.T1.13.7.18.11.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td id="S3.T1.13.7.18.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.18.11.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.18.11.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">30B</td>
<td id="S3.T1.13.7.18.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.6.1" class="ltx_text" style="color:#BF0040;">36.2</span></td>
<td id="S3.T1.13.7.18.11.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.7.1" class="ltx_text" style="color:#BF0040;">15.6</span></td>
<td id="S3.T1.13.7.18.11.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.8.1" class="ltx_text" style="color:#BF0040;">52.1</span></td>
<td id="S3.T1.13.7.18.11.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.9.1" class="ltx_text" style="color:#BF0040;">67.0</span></td>
<td id="S3.T1.13.7.18.11.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.10.1" class="ltx_text" style="color:#BF0040;">83.9</span></td>
<td id="S3.T1.13.7.18.11.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.11.1" class="ltx_text" style="color:#BF0040;">29.0</span></td>
<td id="S3.T1.13.7.18.11.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.12.1" class="ltx_text" style="color:#BF0040;">32.5</span></td>
<td id="S3.T1.13.7.18.11.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">23.3</td>
<td id="S3.T1.13.7.18.11.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.14.1" class="ltx_text" style="color:#BF0040;">28.1</span></td>
<td id="S3.T1.13.7.18.11.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.18.11.15.1" class="ltx_text" style="color:#BF0040;">40.9</span></td>
</tr>
<tr id="S3.T1.12.6.6" class="ltx_tr">
<td id="S3.T1.12.6.6.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="15">
<math id="S3.T1.12.6.6.1.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S3.T1.12.6.6.1.m1.1a"><mo id="S3.T1.12.6.6.1.m1.1.1" xref="S3.T1.12.6.6.1.m1.1.1.cmml">â‰¥</mo><annotation-xml encoding="MathML-Content" id="S3.T1.12.6.6.1.m1.1b"><geq id="S3.T1.12.6.6.1.m1.1.1.cmml" xref="S3.T1.12.6.6.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.6.6.1.m1.1c">\geq</annotation></semantics></math><span id="S3.T1.12.6.6.1.1" class="ltx_text ltx_font_typewriter"> 7B Base Models</span>
</td>
</tr>
<tr id="S3.T1.13.7.19.12" class="ltx_tr">
<td id="S3.T1.13.7.19.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/meta-llama/Llama-2-7b-hf" title="" class="ltx_ref ltx_href">LLaMA-2</a></td>
<td id="S3.T1.13.7.19.12.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.19.12.3" class="ltx_td ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.13.7.19.12.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.19.12.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.19.12.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14.0</td>
<td id="S3.T1.13.7.19.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">3.6</td>
<td id="S3.T1.13.7.19.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">39.5</td>
<td id="S3.T1.13.7.19.12.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">51.7</td>
<td id="S3.T1.13.7.19.12.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">63.5</td>
<td id="S3.T1.13.7.19.12.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">30.9</td>
<td id="S3.T1.13.7.19.12.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">12.4</td>
<td id="S3.T1.13.7.19.12.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">32.7</td>
<td id="S3.T1.13.7.19.12.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">34.4</td>
<td id="S3.T1.13.7.19.12.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">31.4</td>
</tr>
<tr id="S3.T1.13.7.20.13" class="ltx_tr">
<td id="S3.T1.13.7.20.13.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/mistralai/Mistral-7B-v0.1" title="" class="ltx_ref ltx_href">Mistral</a></td>
<td id="S3.T1.13.7.20.13.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.20.13.3" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.13.7.20.13.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.20.13.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.20.13.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">41.2</td>
<td id="S3.T1.13.7.20.13.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">11.6</td>
<td id="S3.T1.13.7.20.13.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">64.7</td>
<td id="S3.T1.13.7.20.13.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">68.5</td>
<td id="S3.T1.13.7.20.13.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">87.5</td>
<td id="S3.T1.13.7.20.13.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">52.9</td>
<td id="S3.T1.13.7.20.13.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">33.0</td>
<td id="S3.T1.13.7.20.13.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">49.5</td>
<td id="S3.T1.13.7.20.13.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">59.4</td>
<td id="S3.T1.13.7.20.13.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">52.0</td>
</tr>
<tr id="S3.T1.13.7.21.14" class="ltx_tr">
<td id="S3.T1.13.7.21.14.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td id="S3.T1.13.7.21.14.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">8B</td>
<td id="S3.T1.13.7.21.14.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td id="S3.T1.13.7.21.14.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">164B</td>
<td id="S3.T1.13.7.21.14.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">16.2</td>
<td id="S3.T1.13.7.21.14.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">14.1</td>
<td id="S3.T1.13.7.21.14.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">35.6</td>
<td id="S3.T1.13.7.21.14.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.21.14.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S3.T1.13.7.22.15" class="ltx_tr">
<td id="S3.T1.13.7.22.15.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td id="S3.T1.13.7.22.15.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">62B</td>
<td id="S3.T1.13.7.22.15.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td id="S3.T1.13.7.22.15.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">109B</td>
<td id="S3.T1.13.7.22.15.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">52.4</td>
<td id="S3.T1.13.7.22.15.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">27.6</td>
<td id="S3.T1.13.7.22.15.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">53.9</td>
<td id="S3.T1.13.7.22.15.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.22.15.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S3.T1.13.7.23.16" class="ltx_tr">
<td id="S3.T1.13.7.23.16.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td id="S3.T1.13.7.23.16.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">540B</td>
<td id="S3.T1.13.7.23.16.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td id="S3.T1.13.7.23.16.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">26B</td>
<td id="S3.T1.13.7.23.16.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">58.8</td>
<td id="S3.T1.13.7.23.16.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">33.6</td>
<td id="S3.T1.13.7.23.16.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.23.16.13.1" class="ltx_text" style="color:#1E90FF;">63.9</span></td>
<td id="S3.T1.13.7.23.16.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.23.16.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr id="S3.T1.13.7.24.17" class="ltx_tr">
<td id="S3.T1.13.7.24.17.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/EleutherAI/llemma_7b" title="" class="ltx_ref ltx_href">LLemma</a></td>
<td id="S3.T1.13.7.24.17.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.24.17.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PPile</td>
<td id="S3.T1.13.7.24.17.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">55B</td>
<td id="S3.T1.13.7.24.17.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">200B</td>
<td id="S3.T1.13.7.24.17.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">38.8</td>
<td id="S3.T1.13.7.24.17.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">17.2</td>
<td id="S3.T1.13.7.24.17.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">56.1</td>
<td id="S3.T1.13.7.24.17.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">69.1</td>
<td id="S3.T1.13.7.24.17.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">82.4</td>
<td id="S3.T1.13.7.24.17.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">48.7</td>
<td id="S3.T1.13.7.24.17.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">41.0</td>
<td id="S3.T1.13.7.24.17.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">45.4</td>
<td id="S3.T1.13.7.24.17.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">59.4</td>
<td id="S3.T1.13.7.24.17.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">50.9</td>
</tr>
<tr id="S3.T1.13.7.25.18" class="ltx_tr">
<td id="S3.T1.13.7.25.18.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/EleutherAI/llemma_34b" title="" class="ltx_ref ltx_href">LLemma</a></td>
<td id="S3.T1.13.7.25.18.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">34B</td>
<td id="S3.T1.13.7.25.18.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">PPile</td>
<td id="S3.T1.13.7.25.18.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">55B</td>
<td id="S3.T1.13.7.25.18.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">50B</td>
<td id="S3.T1.13.7.25.18.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">54.2</td>
<td id="S3.T1.13.7.25.18.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">23.0</td>
<td id="S3.T1.13.7.25.18.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">67.9</td>
<td id="S3.T1.13.7.25.18.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">75.7</td>
<td id="S3.T1.13.7.25.18.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">90.1</td>
<td id="S3.T1.13.7.25.18.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">57.0</td>
<td id="S3.T1.13.7.25.18.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">49.8</td>
<td id="S3.T1.13.7.25.18.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">54.7</td>
<td id="S3.T1.13.7.25.18.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">68.8</td>
<td id="S3.T1.13.7.25.18.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">60.1</td>
</tr>
<tr id="S3.T1.13.7.26.19" class="ltx_tr">
<td id="S3.T1.13.7.26.19.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/internlm/internlm2-math-base-7b" title="" class="ltx_ref ltx_href">Intern-Math</a></td>
<td id="S3.T1.13.7.26.19.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.26.19.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.26.19.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">31B</td>
<td id="S3.T1.13.7.26.19.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">125B</td>
<td id="S3.T1.13.7.26.19.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">41.8</td>
<td id="S3.T1.13.7.26.19.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">14.4</td>
<td id="S3.T1.13.7.26.19.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">61.6</td>
<td id="S3.T1.13.7.26.19.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">66.8</td>
<td id="S3.T1.13.7.26.19.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">83.7</td>
<td id="S3.T1.13.7.26.19.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">50.0</td>
<td id="S3.T1.13.7.26.19.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">57.3</td>
<td id="S3.T1.13.7.26.19.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">24.8</td>
<td id="S3.T1.13.7.26.19.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">37.5</td>
<td id="S3.T1.13.7.26.19.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">48.7</td>
</tr>
<tr id="S3.T1.13.7.27.20" class="ltx_tr">
<td id="S3.T1.13.7.27.20.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/internlm/internlm2-math-base-20b" title="" class="ltx_ref ltx_href">Intern-Math</a></td>
<td id="S3.T1.13.7.27.20.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">20B</td>
<td id="S3.T1.13.7.27.20.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.27.20.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">31B</td>
<td id="S3.T1.13.7.27.20.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">125B</td>
<td id="S3.T1.13.7.27.20.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.27.20.6.1" class="ltx_text" style="color:#1E90FF;">65.4</span></td>
<td id="S3.T1.13.7.27.20.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">30.0</td>
<td id="S3.T1.13.7.27.20.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.27.20.8.1" class="ltx_text" style="color:#1E90FF;">75.7</span></td>
<td id="S3.T1.13.7.27.20.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">79.3</td>
<td id="S3.T1.13.7.27.20.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.27.20.10.1" class="ltx_text" style="color:#1E90FF;">94.0</span></td>
<td id="S3.T1.13.7.27.20.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">50.9</td>
<td id="S3.T1.13.7.27.20.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">38.5</td>
<td id="S3.T1.13.7.27.20.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">53.1</td>
<td id="S3.T1.13.7.27.20.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">71.9</td>
<td id="S3.T1.13.7.27.20.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">62.1</td>
</tr>
<tr id="S3.T1.13.7.28.21" class="ltx_tr">
<td id="S3.T1.13.7.28.21.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;"><a target="_blank" href="https://huggingface.co/deepseek-ai/deepseek-math-7b-base" title="" class="ltx_ref ltx_href">DeepSeekMath</a></td>
<td id="S3.T1.13.7.28.21.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.28.21.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td id="S3.T1.13.7.28.21.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">120B</td>
<td id="S3.T1.13.7.28.21.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">500B</td>
<td id="S3.T1.13.7.28.21.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">64.1</td>
<td id="S3.T1.13.7.28.21.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.7.1" class="ltx_text" style="color:#1E90FF;">34.2</span></td>
<td id="S3.T1.13.7.28.21.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">74.0</td>
<td id="S3.T1.13.7.28.21.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.9.1" class="ltx_text" style="color:#1E90FF;">83.9</span></td>
<td id="S3.T1.13.7.28.21.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">92.4</td>
<td id="S3.T1.13.7.28.21.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.11.1" class="ltx_text" style="color:#1E90FF;">63.4</span></td>
<td id="S3.T1.13.7.28.21.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.12.1" class="ltx_text" style="color:#1E90FF;">62.4</span></td>
<td id="S3.T1.13.7.28.21.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">56.4</td>
<td id="S3.T1.13.7.28.21.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.14.1" class="ltx_text" style="color:#1E90FF;">84.4</span></td>
<td id="S3.T1.13.7.28.21.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.28.21.15.1" class="ltx_text" style="color:#1E90FF;">68.4</span></td>
</tr>
<tr id="S3.T1.13.7.29.22" class="ltx_tr">
<td id="S3.T1.13.7.29.22.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="15"><span id="S3.T1.13.7.29.22.1.1" class="ltx_text ltx_font_typewriter">Continual Pretraining on Mistral-7B</span></td>
</tr>
<tr id="S3.T1.13.7.30.23" class="ltx_tr">
<td id="S3.T1.13.7.30.23.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Mistral-CT</td>
<td id="S3.T1.13.7.30.23.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.30.23.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.30.23.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.30.23.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">15B</td>
<td id="S3.T1.13.7.30.23.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">42.9</td>
<td id="S3.T1.13.7.30.23.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">22.2</td>
<td id="S3.T1.13.7.30.23.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">68.6</td>
<td id="S3.T1.13.7.30.23.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">71.0</td>
<td id="S3.T1.13.7.30.23.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">86.1</td>
<td id="S3.T1.13.7.30.23.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">45.1</td>
<td id="S3.T1.13.7.30.23.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">47.7</td>
<td id="S3.T1.13.7.30.23.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">52.6</td>
<td id="S3.T1.13.7.30.23.14" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">65.6</td>
<td id="S3.T1.13.7.30.23.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">55.8</td>
</tr>
<tr id="S3.T1.13.7.31.24" class="ltx_tr">
<td id="S3.T1.13.7.31.24.1" class="ltx_td ltx_align_left" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S3.T1.13.7.31.24.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math</td>
<td id="S3.T1.13.7.31.24.2" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td id="S3.T1.13.7.31.24.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td id="S3.T1.13.7.31.24.4" class="ltx_td ltx_align_right" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td id="S3.T1.13.7.31.24.5" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">10.5B</td>
<td id="S3.T1.13.7.31.24.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.6.1" class="ltx_text" style="color:#BF0040;">66.9</span></td>
<td id="S3.T1.13.7.31.24.7" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.7.1" class="ltx_text" style="color:#BF0040;">31.0</span></td>
<td id="S3.T1.13.7.31.24.8" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.8.1" class="ltx_text" style="color:#BF0040;">77.8</span></td>
<td id="S3.T1.13.7.31.24.9" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.9.1" class="ltx_text" style="color:#BF0040;">79.0</span></td>
<td id="S3.T1.13.7.31.24.10" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.10.1" class="ltx_text" style="color:#BF0040;">93.9</span></td>
<td id="S3.T1.13.7.31.24.11" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.11.1" class="ltx_text" style="color:#BF0040;">49.9</span></td>
<td id="S3.T1.13.7.31.24.12" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.12.1" class="ltx_text" style="color:#BF0040;">58.7</span></td>
<td id="S3.T1.13.7.31.24.13" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.13.1" class="ltx_text" style="color:#BF0040;">54.6</span></td>
<td id="S3.T1.13.7.31.24.14" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.14.1" class="ltx_text" style="color:#BF0040;">84.4</span></td>
<td id="S3.T1.13.7.31.24.15" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.31.24.15.1" class="ltx_text" style="color:#BF0040;">66.2</span></td>
</tr>
<tr id="S3.T1.13.7.7" class="ltx_tr">
<td id="S3.T1.13.7.7.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><math id="S3.T1.13.7.7.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T1.13.7.7.1.m1.1a"><mi mathvariant="normal" id="S3.T1.13.7.7.1.m1.1.1" xref="S3.T1.13.7.7.1.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S3.T1.13.7.7.1.m1.1b"><ci id="S3.T1.13.7.7.1.m1.1.1.cmml" xref="S3.T1.13.7.7.1.m1.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.7.7.1.m1.1c">\Delta</annotation></semantics></math></td>
<td id="S3.T1.13.7.7.2" class="ltx_td ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.13.7.7.3" class="ltx_td ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.13.7.7.4" class="ltx_td ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S3.T1.13.7.7.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="background-color:#E6E6E6;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.5.1" class="ltx_text" style="background-color:#E6E6E6;">-30%</span></td>
<td id="S3.T1.13.7.7.6" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.6.1" class="ltx_text" style="background-color:#D2DCFA;">+24.0</span></td>
<td id="S3.T1.13.7.7.7" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.7.1" class="ltx_text" style="background-color:#D2DCFA;">+8.8</span></td>
<td id="S3.T1.13.7.7.8" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.8.1" class="ltx_text" style="background-color:#D2DCFA;">+9.2</span></td>
<td id="S3.T1.13.7.7.9" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.9.1" class="ltx_text" style="background-color:#D2DCFA;">+8.0</span></td>
<td id="S3.T1.13.7.7.10" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.10.1" class="ltx_text" style="background-color:#D2DCFA;">+7.8</span></td>
<td id="S3.T1.13.7.7.11" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.11.1" class="ltx_text" style="background-color:#D2DCFA;">+4.8</span></td>
<td id="S3.T1.13.7.7.12" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.12.1" class="ltx_text" style="background-color:#D2DCFA;">+11.0</span></td>
<td id="S3.T1.13.7.7.13" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.13.1" class="ltx_text" style="background-color:#D2DCFA;">+2.0</span></td>
<td id="S3.T1.13.7.7.14" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.14.1" class="ltx_text" style="background-color:#D2DCFA;">+18.8</span></td>
<td id="S3.T1.13.7.7.15" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T1.13.7.7.15.1" class="ltx_text ltx_font_bold" style="background-color:#D2DCFA;">+10.4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Corpus</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">For mathematical reasoning, we utilize the OpenWebMath (OWM) dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Paster et&nbsp;al.</span>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, which comprises approximately 14B tokens sourced from math-related web pages in the Common Crawl. In the general domain, we combine the SlimPajama <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Daria et&nbsp;al.</span>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> and StarCoderData <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et&nbsp;al.</span>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023a</span></a>)</cite> (both part of the Tinyllama corpus) with OpenWebMath, training on a total of 80 billion tokens with a mix ratio of 6:3:1.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Setting</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">For math pretraining, we continue pretraining on the Tinyllama-1.1B model <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang et&nbsp;al.</span>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and the Mistral-7B model <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jiang et&nbsp;al.</span>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> with learning rates of 8e-5 and 2e-5, respectively.
For general domain, we set the learning rate for Tinyllama-1.1B model to 1e-4.
The batch size is uniformly set to 1M tokens for both domains. Regarding the token selection ratio, we use 60% for the Tinyllama-1.1B model and 70% for the Mistral-7B model.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Baseline Setting</h4>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p1.1" class="ltx_p">We use models that have been continually pretrained (Tinyllama-CT and Mistral-CT) through regular causal language modeling as baselines.
Moreover, we compare <span id="S3.SS1.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span> with well-known and top-performing baselines, including Gemma&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Team et&nbsp;al.</span>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, Qwen1.5&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bai et&nbsp;al.</span>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Phi-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et&nbsp;al.</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023b</span></a>)</cite>, DeepSeekLLM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">DeepSeek-AI</span>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, DeepSeekMath&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et&nbsp;al.</span>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024a</span></a>)</cite>, CodeLlama&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Roziere et&nbsp;al.</span>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Mistral&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jiang et&nbsp;al.</span>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Minerva&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lewkowycz et&nbsp;al.</span>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, Tinyllama&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang et&nbsp;al.</span>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, LLemma&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Azerbayev et&nbsp;al.</span>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, and InternLM2-Math&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ying et&nbsp;al.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.
For fine-tuning results, we also compare with previous best models MAmmoTH<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yue et&nbsp;al.</span>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and ToRA<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gou et&nbsp;al.</span>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation Setup</h4>

<div id="S3.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px5.p1.1" class="ltx_p">To comprehensively evaluate pretrained models, we compare their few-shot capabilities and fine-tuning performance across a variety of tasks.
We adopt the lm-eval-harness<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/EleutherAI/lm-evaluation-harness" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span> for general tasks, and math-eval-harness<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/ZubinGou/math-evaluation-harness" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ZubinGou/math-evaluation-harness</a></span></span></span> for math tasks.
We use vllm (v0.3.2)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kwon et&nbsp;al.</span>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> to speed up inference. Further details on our evaluation can be found in <a href="#A3" title="Appendix C Evalution Details â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;C</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.5.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Tool-integrated reasoning results of math pretraining.</span></figcaption>
<div id="S3.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:234.3pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-50.6pt,27.2pt) scale(0.810840269113284,0.810840269113284) ;">
<table id="S3.T2.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.2.2.3.1" class="ltx_tr">
<td id="S3.T2.2.2.3.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T2.2.2.3.1.2" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S3.T2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.3.1" class="ltx_text ltx_font_bold">Tools</span></td>
<td id="S3.T2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.4.1" class="ltx_text ltx_font_bold">SFT Data</span></td>
<td id="S3.T2.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.5.1" class="ltx_text ltx_font_bold">GSM8k</span></td>
<td id="S3.T2.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.6.1" class="ltx_text ltx_font_bold">MATH</span></td>
<td id="S3.T2.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.7.1" class="ltx_text ltx_font_bold">SVAMP</span></td>
<td id="S3.T2.2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.8.1" class="ltx_text ltx_font_bold">ASDiv</span></td>
<td id="S3.T2.2.2.3.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.9.1" class="ltx_text ltx_font_bold">MAWPS</span></td>
<td id="S3.T2.2.2.3.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.10.1" class="ltx_text ltx_font_bold">TAB</span></td>
<td id="S3.T2.2.2.3.1.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.3.1.11.1" class="ltx_text ltx_font_bold">GSM-H</span></td>
<td id="S3.T2.2.2.3.1.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="2"><span id="S3.T2.2.2.3.1.12.1" class="ltx_text ltx_font_bold">AVG</span></td>
</tr>
<tr id="S3.T2.2.2.4.2" class="ltx_tr">
<td id="S3.T2.2.2.4.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="4"><span id="S3.T2.2.2.4.2.1.1" class="ltx_text ltx_font_bold">Used for SFT?</span></td>
<td id="S3.T2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.2.1" class="ltx_text" style="color:#326400;">âœ“</span></td>
<td id="S3.T2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.3.1" class="ltx_text" style="color:#326400;">âœ“</span></td>
<td id="S3.T2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.4.1" class="ltx_text" style="color:#C80000;">âœ—</span></td>
<td id="S3.T2.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.5.1" class="ltx_text" style="color:#C80000;">âœ—</span></td>
<td id="S3.T2.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.6.1" class="ltx_text" style="color:#C80000;">âœ—</span></td>
<td id="S3.T2.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.7.1" class="ltx_text" style="color:#C80000;">âœ—</span></td>
<td id="S3.T2.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.4.2.8.1" class="ltx_text" style="color:#C80000;">âœ—</span></td>
</tr>
<tr id="S3.T2.2.2.5.3" class="ltx_tr">
<td id="S3.T2.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="12"><span id="S3.T2.2.2.5.3.1.1" class="ltx_text ltx_font_typewriter">Previous Models</span></td>
</tr>
<tr id="S3.T2.2.2.6.4" class="ltx_tr">
<td id="S3.T2.2.2.6.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">GPT4-0314</td>
<td id="S3.T2.2.2.6.4.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.6.4.3.1" class="ltx_text" style="color:#C80000;">âœ—</span></td>
<td id="S3.T2.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">92.0</td>
<td id="S3.T2.2.2.6.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">42.5</td>
<td id="S3.T2.2.2.6.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">93.1</td>
<td id="S3.T2.2.2.6.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">91.3</td>
<td id="S3.T2.2.2.6.4.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">97.6</td>
<td id="S3.T2.2.2.6.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">67.1</td>
<td id="S3.T2.2.2.6.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">64.7</td>
<td id="S3.T2.2.2.6.4.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">78.3</td>
</tr>
<tr id="S3.T2.2.2.7.5" class="ltx_tr">
<td id="S3.T2.2.2.7.5.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">GPT4-0314 (PAL)</td>
<td id="S3.T2.2.2.7.5.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.7.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.7.5.3.1" class="ltx_text" style="color:#326400;">âœ“</span></td>
<td id="S3.T2.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.7.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">94.2</td>
<td id="S3.T2.2.2.7.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">51.8</td>
<td id="S3.T2.2.2.7.5.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">94.8</td>
<td id="S3.T2.2.2.7.5.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">92.6</td>
<td id="S3.T2.2.2.7.5.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">97.7</td>
<td id="S3.T2.2.2.7.5.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">95.9</td>
<td id="S3.T2.2.2.7.5.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">77.6</td>
<td id="S3.T2.2.2.7.5.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">86.4</td>
</tr>
<tr id="S3.T2.2.2.8.6" class="ltx_tr">
<td id="S3.T2.2.2.8.6.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">MAmmoTH</td>
<td id="S3.T2.2.2.8.6.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">70B</td>
<td id="S3.T2.2.2.8.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.8.6.3.1" class="ltx_text" style="color:#326400;">âœ“</span></td>
<td id="S3.T2.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MI-260k</td>
<td id="S3.T2.2.2.8.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
<td id="S3.T2.2.2.8.6.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">41.8</td>
<td id="S3.T2.2.2.8.6.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">82.4</td>
<td id="S3.T2.2.2.8.6.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.8.6.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.8.6.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.8.6.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T2.2.2.8.6.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr id="S3.T2.2.2.9.7" class="ltx_tr">
<td id="S3.T2.2.2.9.7.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA</td>
<td id="S3.T2.2.2.9.7.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td id="S3.T2.2.2.9.7.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.9.7.3.1" class="ltx_text" style="color:#326400;">âœ“</span></td>
<td id="S3.T2.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.9.7.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">68.8</td>
<td id="S3.T2.2.2.9.7.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">40.1</td>
<td id="S3.T2.2.2.9.7.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">68.2</td>
<td id="S3.T2.2.2.9.7.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">73.9</td>
<td id="S3.T2.2.2.9.7.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">88.8</td>
<td id="S3.T2.2.2.9.7.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">42.4</td>
<td id="S3.T2.2.2.9.7.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">54.6</td>
<td id="S3.T2.2.2.9.7.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">62.4</td>
</tr>
<tr id="S3.T2.2.2.10.8" class="ltx_tr">
<td id="S3.T2.2.2.10.8.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA</td>
<td id="S3.T2.2.2.10.8.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">70B</td>
<td id="S3.T2.2.2.10.8.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.10.8.3.1" class="ltx_text" style="color:#326400;">âœ“</span></td>
<td id="S3.T2.2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.10.8.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">84.3</td>
<td id="S3.T2.2.2.10.8.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">49.7</td>
<td id="S3.T2.2.2.10.8.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">82.7</td>
<td id="S3.T2.2.2.10.8.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">86.8</td>
<td id="S3.T2.2.2.10.8.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">93.8</td>
<td id="S3.T2.2.2.10.8.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">74.0</td>
<td id="S3.T2.2.2.10.8.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">67.2</td>
<td id="S3.T2.2.2.10.8.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
</tr>
<tr id="S3.T2.2.2.11.9" class="ltx_tr">
<td id="S3.T2.2.2.11.9.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">DeepSeekMath</td>
<td id="S3.T2.2.2.11.9.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td id="S3.T2.2.2.11.9.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.11.9.3.1" class="ltx_text" style="color:#326400;">âœ“</span></td>
<td id="S3.T2.2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.11.9.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">79.8</td>
<td id="S3.T2.2.2.11.9.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">52.0</td>
<td id="S3.T2.2.2.11.9.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">80.1</td>
<td id="S3.T2.2.2.11.9.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">87.1</td>
<td id="S3.T2.2.2.11.9.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">93.8</td>
<td id="S3.T2.2.2.11.9.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">85.8</td>
<td id="S3.T2.2.2.11.9.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">63.1</td>
<td id="S3.T2.2.2.11.9.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">77.4</td>
</tr>
<tr id="S3.T2.2.2.12.10" class="ltx_tr">
<td id="S3.T2.2.2.12.10.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="12"><span id="S3.T2.2.2.12.10.1.1" class="ltx_text ltx_font_typewriter">Our Pretrained Models</span></td>
</tr>
<tr id="S3.T2.2.2.13.11" class="ltx_tr">
<td id="S3.T2.2.2.13.11.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">TinyLlama-CT</td>
<td id="S3.T2.2.2.13.11.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">1B</td>
<td id="S3.T2.2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.13.11.3.1" class="ltx_text" style="color:#326400;">âœ“</span></td>
<td id="S3.T2.2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">51.4</td>
<td id="S3.T2.2.2.13.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">38.4</td>
<td id="S3.T2.2.2.13.11.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">53.4</td>
<td id="S3.T2.2.2.13.11.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">66.7</td>
<td id="S3.T2.2.2.13.11.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">81.7</td>
<td id="S3.T2.2.2.13.11.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">20.5</td>
<td id="S3.T2.2.2.13.11.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">42.8</td>
<td id="S3.T2.2.2.13.11.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">50.7</td>
</tr>
<tr id="S3.T2.2.2.14.12" class="ltx_tr">
<td id="S3.T2.2.2.14.12.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T2.2.2.14.12.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math</td>
<td id="S3.T2.2.2.14.12.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">1B</td>
<td id="S3.T2.2.2.14.12.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.3.1" class="ltx_text" style="color:#326400;">âœ“</span></td>
<td id="S3.T2.2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.14.12.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.5.1" class="ltx_text" style="color:#BF0040;">59.4</span></td>
<td id="S3.T2.2.2.14.12.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.6.1" class="ltx_text" style="color:#BF0040;">40.6</span></td>
<td id="S3.T2.2.2.14.12.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.7.1" class="ltx_text" style="color:#BF0040;">60.7</span></td>
<td id="S3.T2.2.2.14.12.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.8.1" class="ltx_text" style="color:#BF0040;">74.2</span></td>
<td id="S3.T2.2.2.14.12.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.9.1" class="ltx_text" style="color:#BF0040;">88.6</span></td>
<td id="S3.T2.2.2.14.12.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.10.1" class="ltx_text" style="color:#BF0040;">26.7</span></td>
<td id="S3.T2.2.2.14.12.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.11.1" class="ltx_text" style="color:#BF0040;">48.1</span></td>
<td id="S3.T2.2.2.14.12.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.14.12.12.1" class="ltx_text" style="color:#BF0040;">56.9</span></td>
</tr>
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S3.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T2.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S3.T2.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\Delta</annotation></semantics></math></td>
<td id="S3.T2.1.1.1.2" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.1.1.1.3" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.5.1" class="ltx_text" style="background-color:#D2DCFA;">+8.0</span></td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.6.1" class="ltx_text" style="background-color:#D2DCFA;">+2.2</span></td>
<td id="S3.T2.1.1.1.7" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.7.1" class="ltx_text" style="background-color:#D2DCFA;">+7.3</span></td>
<td id="S3.T2.1.1.1.8" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.8.1" class="ltx_text" style="background-color:#D2DCFA;">+7.5</span></td>
<td id="S3.T2.1.1.1.9" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.9.1" class="ltx_text" style="background-color:#D2DCFA;">+6.9</span></td>
<td id="S3.T2.1.1.1.10" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.10.1" class="ltx_text" style="background-color:#D2DCFA;">+6.2</span></td>
<td id="S3.T2.1.1.1.11" class="ltx_td ltx_align_center ltx_border_r" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.11.1" class="ltx_text" style="background-color:#D2DCFA;">+5.3</span></td>
<td id="S3.T2.1.1.1.12" class="ltx_td ltx_align_center" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.1.1.1.12.1" class="ltx_text ltx_font_bold" style="background-color:#D2DCFA;">+6.2</span></td>
</tr>
<tr id="S3.T2.2.2.15.13" class="ltx_tr">
<td id="S3.T2.2.2.15.13.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Mistral-CT</td>
<td id="S3.T2.2.2.15.13.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td id="S3.T2.2.2.15.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.15.13.3.1" class="ltx_text" style="color:#326400;">âœ“</span></td>
<td id="S3.T2.2.2.15.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.15.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">77.5</td>
<td id="S3.T2.2.2.15.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">48.4</td>
<td id="S3.T2.2.2.15.13.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
<td id="S3.T2.2.2.15.13.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">83.8</td>
<td id="S3.T2.2.2.15.13.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">93.4</td>
<td id="S3.T2.2.2.15.13.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">67.5</td>
<td id="S3.T2.2.2.15.13.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">60.4</td>
<td id="S3.T2.2.2.15.13.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">72.6</td>
</tr>
<tr id="S3.T2.2.2.16.14" class="ltx_tr">
<td id="S3.T2.2.2.16.14.1" class="ltx_td ltx_align_left" style="padding-left:5.0pt;padding-right:5.0pt;">
<span id="S3.T2.2.2.16.14.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math</td>
<td id="S3.T2.2.2.16.14.2" class="ltx_td ltx_align_right" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td id="S3.T2.2.2.16.14.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.3.1" class="ltx_text" style="color:#326400;">âœ“</span></td>
<td id="S3.T2.2.2.16.14.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td id="S3.T2.2.2.16.14.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.5.1" class="ltx_text" style="color:#BF0040;">81.3</span></td>
<td id="S3.T2.2.2.16.14.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.6.1" class="ltx_text" style="color:#BF0040;">51.8</span></td>
<td id="S3.T2.2.2.16.14.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.7.1" class="ltx_text" style="color:#BF0040;">80.8</span></td>
<td id="S3.T2.2.2.16.14.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.8.1" class="ltx_text" style="color:#BF0040;">85.5</span></td>
<td id="S3.T2.2.2.16.14.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.9.1" class="ltx_text" style="color:#BF0040;">94.5</span></td>
<td id="S3.T2.2.2.16.14.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.10.1" class="ltx_text" style="color:#BF0040;">70.1</span></td>
<td id="S3.T2.2.2.16.14.11" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.11.1" class="ltx_text" style="color:#BF0040;">63.1</span></td>
<td id="S3.T2.2.2.16.14.12" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.16.14.12.1" class="ltx_text" style="color:#BF0040;">75.3</span></td>
</tr>
<tr id="S3.T2.2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.2.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><math id="S3.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T2.2.2.2.1.m1.1a"><mi mathvariant="normal" id="S3.T2.2.2.2.1.m1.1.1" xref="S3.T2.2.2.2.1.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.1.m1.1b"><ci id="S3.T2.2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.2.1.m1.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.1.m1.1c">\Delta</annotation></semantics></math></td>
<td id="S3.T2.2.2.2.2" class="ltx_td ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.2.2.2.3" class="ltx_td ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.2.2.2.4" class="ltx_td ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.5.1" class="ltx_text" style="background-color:#D2DCFA;">+3.8</span></td>
<td id="S3.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.6.1" class="ltx_text" style="background-color:#D2DCFA;">+3.4</span></td>
<td id="S3.T2.2.2.2.7" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.7.1" class="ltx_text" style="background-color:#D2DCFA;">+3.9</span></td>
<td id="S3.T2.2.2.2.8" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.8.1" class="ltx_text" style="background-color:#D2DCFA;">+1.7</span></td>
<td id="S3.T2.2.2.2.9" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.9.1" class="ltx_text" style="background-color:#D2DCFA;">+1.1</span></td>
<td id="S3.T2.2.2.2.10" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.10.1" class="ltx_text" style="background-color:#D2DCFA;">+2.6</span></td>
<td id="S3.T2.2.2.2.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.11.1" class="ltx_text" style="background-color:#D2DCFA;">+2.7</span></td>
<td id="S3.T2.2.2.2.12" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T2.2.2.2.12.1" class="ltx_text ltx_font_bold" style="background-color:#D2DCFA;">+2.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Math Pre-training Results</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Few-shot CoT Reasoning Results</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">We evalute base models prompting with few-shot chain-of-thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wei et&nbsp;al.</span>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022a</span></a>)</cite> examples following previous works <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lewkowycz et&nbsp;al.</span>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Azerbayev et&nbsp;al.</span>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Shao et&nbsp;al.</span>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024b</span></a>)</cite>.
As results shown in <a href="#S3.T1" title="Table 1 â€£ Reference Model Training â€£ 3.1 Experimental Setup â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table&nbsp;1</span></a>, in comparison to continue pretraining directly, <span id="S3.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math has achieved the average few-shot accuracy improvement of 16.5% on 1B models and 10.4% on 7B models. Furthermore, after training for multiple epochs on OpenWebMath, we find that <span id="S3.SS2.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">Rho-1</span> could further increase the average few-shot accuracy to 40.9%.
Compared to DeepSeekMath-7B, which pretrained on 500 billion math-related tokens, <span id="S3.SS2.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_smallcaps">Rho-1</span>-7B pretrained on only 15 billion tokens (selecting 10.5 billion tokens) achieved comparable results, demonstrating the efficiency of our approach.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Tool-Integrated Reasoning Results</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">We fine-tune <span id="S3.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span> and baseline models on 69k ToRA corpus <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gou et&nbsp;al.</span>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, consisting of 16k GPT-4-generated trajectories in a tool-integrated reasoning format, and 53k answer-augmented samples using LLaMA.
As presented in <a href="#S3.T2" title="Table 2 â€£ Evaluation Setup â€£ 3.1 Experimental Setup â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table&nbsp;2</span></a>, <span id="S3.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_smallcaps">Rho-1</span>-1B and <span id="S3.SS2.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_smallcaps">Rho-1</span>-7B achieved a state-of-the-art 40.6% and 51.8% on MATH dataset, respectively.
On some unseen tasks (<em id="S3.SS2.SSS0.Px2.p1.1.4" class="ltx_emph ltx_font_italic">e.g.,</em> TabMWP and GSM-Hard), <span id="S3.SS2.SSS0.Px2.p1.1.5" class="ltx_text ltx_font_smallcaps">Rho-1</span> also demonstrates a certain degree of generalizability,
with an average few-shot accuracy improvement of 6.2% on the <span id="S3.SS2.SSS0.Px2.p1.1.6" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math-1B and 2.7% on <span id="S3.SS2.SSS0.Px2.p1.1.7" class="ltx_text ltx_font_smallcaps">Rho-1</span>-Math-7B.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2404.07965/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="256" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.4.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.5.2" class="ltx_text" style="font-size:90%;">
<span id="S3.F5.5.2.1" class="ltx_text ltx_font_bold">General pretraining results.</span>
We continual pretraining Tinyllama-1B on 80G general tokens. Tinyllama-CT is etrained with CLM, while <span id="S3.F5.5.2.2" class="ltx_text ltx_font_smallcaps">Rho-1</span> is trained with our proposed SLM.
</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>General Pre-training Results</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We confirm the efficacy of the SLM in general pretraining by continual training Tinyllama-1.1B on 80 billion tokens.
The results depicted in <a href="#S3.F5" title="Figure 5 â€£ Tool-Integrated Reasoning Results â€£ 3.2 Math Pre-training Results â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;5</span></a> indicate that although Tinyllama has already undergone extensive training on the majority of these tokens, the application of SLM yields an average enhancement of 6.8% across 15 benchmarks compared to direct continual pretraining.
The improvements were especially pronounced in code and math tasks, exceeding 10%.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2404.07965/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="149" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">The dynamics of pretraining loss and downstream loss.<span id="S3.F6.4.2.1" class="ltx_text ltx_font_medium"> (a) and (c) represent the loss of tokens selected/unselected by SLM during pretraining in both SLM and CLM methods, while (b) represents the loss of the SLM and CLM methods on downstream corpora. We tested the above results through the process of pretraining with a total of 4 billion tokens.</span></span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Analysis</h3>

<section id="S3.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Selected Token Loss Aligns Better with Downstream Performance</h4>

<div id="S3.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px1.p1.1" class="ltx_p">We use the reference model to filter tokens and explore the changes in validation loss after training on all/selected tokens, while observing their relationship with downstream loss.
As shown in <a href="#S3.F6" title="Figure 6 â€£ 3.3 General Pre-training Results â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;6</span></a>, we pretrain approximately 4B tokens and displayed the variation curves of loss on different pretraining method and validation sets during the pretraining process.
We can observe that on the tokens selected by the reference model, the decrease in average loss of the <span id="S3.SS4.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span> is more significant compared to regular pretraining. On the contrary, on the unselected tokens, the decrease in average loss of the regular pretraining is more significant.
If we associate fig (a), fig(b) with fig(c), it is not difficult to find that the model trained on the selected tokens has a more significant decrease in downstream loss, while ordinary pretraining, although reducing the average loss of all tokens during the training phase, is difficult to have a significant decrease in downstream loss.
Therefore, we expect that selecting tokens for pretraining is more efficient.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2404.07965/assets/x7.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">The relationship between the selected tokens / unselected tokens loss in SLM and downstream task performance.<span id="S3.F7.4.2.1" class="ltx_text ltx_font_medium"> The y-axis represents the average few-shot accuracy on GSM8k and MATH. The x-axis represents the average loss on selected tokens / unselected tokens at corresponding checkpoint(2B, 5B, 8B, 11B, and 14B).</span></span></figcaption>
</figure>
<div id="S3.SS4.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS4.SSS0.Px1.p2.1" class="ltx_p">Moreover, We relate the selected tokensâ€™ loss to its downstream task performance via a power law in <a href="#S3.F7" title="Figure 7 â€£ Selected Token Loss Aligns Better with Downstream Performance â€£ 3.4 Analysis â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;7</span></a>, which is similar to a concurrent study <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gadre et&nbsp;al.</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>. Observing the curve fitted from the data points in the graph, the average loss of tokens selected by the SLM shows a positive correlation with the performance of downstream tasks, whereas the average loss of tokens not selected exhibits a negative correlation with downstream task performance. Therefore, it is not necessary for the all tokensâ€™ loss to decrease to benefit the modelâ€™s ultimate performance. See <a href="#A4" title="Appendix D Relate the Selected Tokensâ€™ Loss to Downstream Task Performance â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;D</span></a> for more details.</p>
</div>
<figure id="S3.SS4.SSS0.Px1.2" class="ltx_table">
<div id="S3.SS4.SSS0.Px1.2.3" class="ltx_block">
<figure id="S3.F8" class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" style="width:212.5pt;"><img src="/html/2404.07965/assets/x8.png" id="S3.SS4.SSS0.Px1.1.1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="311" alt="Refer to caption">
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">The PPL of tokens selected by different checkpoint.<span id="S3.F8.4.2.1" class="ltx_text ltx_font_medium"> We test the PPL of the tokens selected at 2B, 5B, 8B, 11B, and 14B.
</span></span></figcaption>
</figure>
<figure id="S3.F9" class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" style="width:208.1pt;"><img src="/html/2404.07965/assets/x9.png" id="S3.SS4.SSS0.Px1.2.2.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="347" alt="Refer to caption">
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S3.F9.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Effect of token select ratio.<span id="S3.F9.4.2.1" class="ltx_text ltx_font_medium"> We train 1B LM with SLM objective on 5B tokens.</span></span></figcaption>
</figure>
</div>
</figure>
</section>
<section id="S3.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">What Tokens are Selected with SLM?</h4>

<div id="S3.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px2.p1.1" class="ltx_p">We aim to analyze the tokens selected by the SLM method in pretraining to further explore its working mechanism.
To this end, we visualize the token selection process during the training of <span id="S3.SS4.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">Rho-1</span> using the OpenWebMath.
In <a href="#A5.SS1" title="E.1 Token Selected Examples â€£ Appendix E Examples of Tokens Selected by SLM â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Â§E.1</span></a>, we have highlighted in <span id="S3.SS4.SSS0.Px2.p1.1.2" class="ltx_text" style="color:#1E90FF;">blue</span> the tokens that were retained during actual pretraining.
We observe that the majority of tokens chosen by the SLM method are closely related to mathematics, effectively training the model on the parts of the original corpus that are pertinent to mathematical content.</p>
</div>
<div id="S3.SS4.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS4.SSS0.Px2.p2.1" class="ltx_p">Furthermore, we investigated the differences in token filtering across various checkpoints during the training process and tested the perplexity of these tokens on different checkpoints.
As illustrated in <a href="#S3.F8" title="Figure 8 â€£ Selected Token Loss Aligns Better with Downstream Performance â€£ 3.4 Analysis â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;8</span></a>, we found that the tokens selected by later checkpoints tend to have higher perplexity towards the later stages of training and lower perplexity in the earlier stages.
This may suggest that the model first optimizes tokens with a larger learnable space, thereby increasing learning efficiency.
Moreover, we noticed a sample-wise â€œdouble descentâ€&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Nakkiran et&nbsp;al.</span>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> on the loss of selected tokens, where the select tokenâ€™s perplexity initially increases before decreases.
This might be an effect of selecting tokens based on excess loss, targeting those most in need at each checkpoint.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effect of Token Select Ratio</h4>

<div id="S3.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px3.p1.1" class="ltx_p">We investigate the impact of token selecting ratios of the SLM. Generally, the selecting ratio is defined by heuristic rules, similar to the approach previously employed in the training of Masked Language Models (MLMs)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Devlin et&nbsp;al.</span>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Liu et&nbsp;al.</span>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
As shown in <a href="#S3.F9" title="Figure 9 â€£ Selected Token Loss Aligns Better with Downstream Performance â€£ 3.4 Analysis â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;9</span></a>, the selected tokens is suitable for accounting for about 60% of the original tokens.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Weak-to-Strong Generization</h4>

<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S3.T3.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Weak-to-Strong generization result on math benchmark.<span id="S3.T3.6.2.1" class="ltx_text ltx_font_medium">
</span></span></figcaption>
<div id="S3.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:73.2pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.2pt,2.5pt) scale(0.934457108709658,0.934457108709658) ;">
<table id="S3.T3.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.2.2.2" class="ltx_tr">
<th id="S3.T3.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.3.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S3.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.4.1" class="ltx_text ltx_font_bold">GSM8K</span></th>
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">MATH<sup id="S3.T3.1.1.1.1.1.1" class="ltx_sup"><span id="S3.T3.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium">â€ </span></sup></span></th>
<th id="S3.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.5.1" class="ltx_text ltx_font_bold">SVAMP</span></th>
<th id="S3.T3.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.6.1" class="ltx_text ltx_font_bold">ASDiv</span></th>
<th id="S3.T3.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.7.1" class="ltx_text ltx_font_bold">MAWPS</span></th>
<th id="S3.T3.2.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.8.1" class="ltx_text ltx_font_bold">TAB</span></th>
<th id="S3.T3.2.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.9.1" class="ltx_text ltx_font_bold">MQA</span></th>
<th id="S3.T3.2.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S3.T3.2.2.2.10.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T3.2.2.2.10.1.1" class="ltx_tr">
<td id="S3.T3.2.2.2.10.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.10.1.1.1.1" class="ltx_text ltx_font_bold">MMLU</span></td>
</tr>
<tr id="S3.T3.2.2.2.10.1.2" class="ltx_tr">
<td id="S3.T3.2.2.2.10.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.10.1.2.1.1" class="ltx_text ltx_font_bold">STEM</span></td>
</tr>
</tbody></table>
</th>
<th id="S3.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.2.1" class="ltx_text ltx_font_bold">SAT<sup id="S3.T3.2.2.2.2.1.1" class="ltx_sup"><span id="S3.T3.2.2.2.2.1.1.1" class="ltx_text ltx_font_medium">â€¡</span></sup></span></th>
<th id="S3.T3.2.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S3.T3.2.2.2.11.1" class="ltx_text ltx_font_bold">AVG</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.2.2.3.1" class="ltx_tr">
<th id="S3.T3.2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">Llama-2-7B-CT</th>
<td id="S3.T3.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">28.4</td>
<td id="S3.T3.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">13.6</td>
<td id="S3.T3.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">50.3</td>
<td id="S3.T3.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">62.8</td>
<td id="S3.T3.2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">79.5</td>
<td id="S3.T3.2.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">37.6</td>
<td id="S3.T3.2.2.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">34.1</td>
<td id="S3.T3.2.2.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">41.6</td>
<td id="S3.T3.2.2.3.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">43.5</td>
<td id="S3.T3.2.2.3.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">43.5</td>
</tr>
<tr id="S3.T3.2.2.4.2" class="ltx_tr">
<th id="S3.T3.2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">Llama-2-7B-CT w/ 1B RM</th>
<td id="S3.T3.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">29.8</td>
<td id="S3.T3.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">16.0</td>
<td id="S3.T3.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">55.5</td>
<td id="S3.T3.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">63.7</td>
<td id="S3.T3.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">80.4</td>
<td id="S3.T3.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">37.9</td>
<td id="S3.T3.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">34.3</td>
<td id="S3.T3.2.2.4.2.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">38.2</td>
<td id="S3.T3.2.2.4.2.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">43.8</td>
<td id="S3.T3.2.2.4.2.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">44.4</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS4.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px4.p1.1" class="ltx_p">Apart from the main experiments where we use the same base model for the reference and continual pretraining, we also investigate if a smaller reference model can effectively guide the pretraining of a larger model.
We use Tinyllma-1.1B as reference model and continual pretraining Llama-2-7B on math.
Results presented in <a href="#S3.T3" title="Table 3 â€£ Weak-to-Strong Generization â€£ 3.4 Analysis â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table&nbsp;3</span></a> indicate that, despite the considerable gap between the small and large models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et&nbsp;al.</span>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023c</span></a>)</cite>, employing the small reference model to token selection can still yield benefits to the pre-training of the larger model.
If reference and training models have different vocabularies, one can consider performing token alignment <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wan et&nbsp;al.</span>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et&nbsp;al.</span>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, which we leave for future work.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Works</h2>

<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Data Optimization</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">The objective of optimizing pre-training corpora is to maximize the performance and efficiency of language model training by improving the quality and scale of the pretrain data mixture.
This typically includes data collecting through crawling <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Raffel et&nbsp;al.</span>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> or synthesis <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Polu and Sutskever</span>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Gunasekar et&nbsp;al.</span>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, de-duplication <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lee et&nbsp;al.</span>, <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Kandpal et&nbsp;al.</span>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Tirumala et&nbsp;al.</span>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, filtering and selection <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Albalak et&nbsp;al.</span>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, as well as data composition <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xie et&nbsp;al.</span>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and curriculum <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chen et&nbsp;al.</span>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">MA et&nbsp;al.</span>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Selection</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">Data selection for fine-tuning has been extensively studied, focusing on improving quality <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et&nbsp;al.</span>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023d</span></a>)</cite>, diversity <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Liu et&nbsp;al.</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, and distribution matching <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et&nbsp;al.</span>, <a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023e</span></a>; <span class="ltx_text" style="font-size:90%;">Xia et&nbsp;al.</span>, <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Ni et&nbsp;al.</span>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.
For pretraining, various lightweight filters are utilized <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Albalak et&nbsp;al.</span>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, including heuristic-based (<em id="S4.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> language and item count filtering), classifier-based <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Brown et&nbsp;al.</span>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, and perplexity-based approaches <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wenzek et&nbsp;al.</span>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
The massive public RedPajama-Data-v2 dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Computer</span>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, for example, leverages over 40 quality indicators for data filtering and reweighting. Nevertheless, strict filtering like blocklist <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Raffel et&nbsp;al.</span>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> and Safety API filtering <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Welbl et&nbsp;al.</span>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, have been found to hurt evaluation loss or induce bias <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dodge et&nbsp;al.</span>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.
To our knowledge, we are the first to explore token-level data selection, aimed at enhancing data quality and information density at the most fundamental granularity.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Language Model Training Dynamics</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p">Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Saphra and Lopez</span>, <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, the acquisition of linguistic knowledge <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Choshen et&nbsp;al.</span>, <a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Liu et&nbsp;al.</span>, <a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, and the phenomenon of grokking <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Power et&nbsp;al.</span>, <a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>. The analysis by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xia et&nbsp;al.</span> (<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> is the most related to ours, which examines token-level training trajectories in models of varying sizes.
Our findings, however, diverge from those of <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xia et&nbsp;al.</span> (<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, who posit that tokens with little change in perplexity are â€œalready learnedâ€. We identify a spectrum of token patterns, including â€œeasy tokensâ€ and â€œhard tokensâ€ that resist convergence. Recognizing this, we propose a method of selective language modeling that targets the influential tokens, optimizing the learning process.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scaling Laws</h4>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p1.1" class="ltx_p">Scaling laws guide us in discovering the impact of factors such as parameter count, data size, and compute on language model performance and behavior.
These studies usually focus on predicable scaling though power law <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et&nbsp;al.</span>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Hernandez et&nbsp;al.</span>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, optimal resource allocation <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hoffmann et&nbsp;al.</span>, <a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, downstream tasks <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wei et&nbsp;al.</span>, <a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022b</span></a>; <span class="ltx_text" style="font-size:90%;">Isik et&nbsp;al.</span>, <a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Gadre et&nbsp;al.</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, architectures <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tay et&nbsp;al.</span>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, memorization <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tirumala et&nbsp;al.</span>, <a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Carlini et&nbsp;al.</span>, <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Henighan et&nbsp;al.</span>, <a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Biderman et&nbsp;al.</span>, <a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, and repeating data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hernandez et&nbsp;al.</span>, <a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Muennighoff et&nbsp;al.</span>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Xue et&nbsp;al.</span>, <a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.
Most scaling laws on model performance study cross-entory loss on all training tokens, while we focus on the tokens loss of desired distributions.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Future Work</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Generalization</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">In math continual pretraining, as depicted in <a href="#S3.F6" title="Figure 6 â€£ 3.3 General Pre-training Results â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;6</span></a>, training exclusively with SLM leads to quickly convergence to the domain focused by the reference model, accompanied by a significant rise in the loss of unselected tokens.
Although no adverse effects, like biases, have been observed from the increased loss yet, a general pretraining loss on text and code may prevent overfitting <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Goodhart and Goodhart</span>, <a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1984</span></a>)</cite>, as suggested by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Ouyang et&nbsp;al.</span> (<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Azerbayev et&nbsp;al.</span> (<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
Furthermore, future efforts could broaden the corpus scope of the reference model, and enlarge the pretraining data size, as exemplified by DeepSpeedMath <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et&nbsp;al.</span>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024a</span></a>)</cite>.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scalability</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">Due to budget constraints, we have only verified the effectiveness of our method on smaller models (&lt;=7B parameters) and smaller datasets (&lt;100B tokens). Smaller models benefit significantly from removing the loss of irrelevant tokens and focusing on important ones.
However, itâ€™s possible that very large models trained on extensive corpora may naturally develop this inductive bias to compress useful data (<em id="S5.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">i.e.,</em> compressing everything), although it may sounds inefficient for now.
Therefore, future works should study whether this selective language modeling technique can scale to very large models and data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et&nbsp;al.</span>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Is training a reference model necessary?</h4>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">To score tokens, we need a high-quality reference model. This could be a base model trained with a small amount of high-quality data, or a performant open-source model.
In fact, since we only need input logprobs or perplexity from reference model, we could even utilize more powerful proprietary model APIs. We can input tokens and use the log probabilities of the input returned by the API as reference scores. We leave this for future works.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">How to improve upon SLM?</h4>

<div id="S5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p1.1" class="ltx_p">There are many natural extensions of SLM, <em id="S5.SS0.SSS0.Px4.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> reweighting tokens instead of selecting may improve robustness; using a reference model as a reward model to guide pretraining with reinforcement learning; adopting multiple reference models to reduce overfitting; designing token-level curriculum learning and iterative strategies for continuous improvements, <em id="S5.SS0.SSS0.Px4.p1.1.2" class="ltx_emph ltx_font_italic">etc</em>.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Expanding the Use of SLM</h4>

<div id="S5.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px5.p1.1" class="ltx_p">SLM may be extended to supervised fine-tuning to address the noise and distribution mismatches in many SFT datasets.
Another potential application is alignment, <em id="S5.SS0.SSS0.Px5.p1.1.1" class="ltx_emph ltx_font_italic">e.g.,</em> by training a reference model to emphasize helpfulness, truthfulness, and harmlessness, we may obtain a base model that is natively aligned during the pretraining stage.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Kaplan et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Scaling laws for neural language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2001.08361</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Brown et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">, 33:1877â€“1901, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.4.4.1" class="ltx_text" style="font-size:90%;">OpenAI [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">Gpt-4 technical report, 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Team et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M Dai, Anja Hauth, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Gemini: a family of highly capable multimodal models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.11805</em><span id="bib.bib4.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Wenzek et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco GuzmÃ¡n, Armand Joulin, and Edouard Grave.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Ccnet: Extracting high quality monolingual datasets from web crawl data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.00359</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Welbl et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa&nbsp;Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Challenges in detoxifying language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Findings of the Association for Computational Linguistics: EMNLP 2021</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pages 2447â€“2469, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Muennighoff et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le&nbsp;Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin&nbsp;A Raffel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Scaling data-constrained language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib7.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Dodge et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Jesse Dodge, Maarten Sap, Ana MarasoviÄ‡, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Documenting large webtext corpora: A case study on the colossal clean crawled corpus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em><span id="bib.bib8.11.3" class="ltx_text" style="font-size:90%;">, pages 1286â€“1305, 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Longpre et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">A pretrainerâ€™s guide to training data: Measuring the effects of data age, domain coverage, quality, &amp; toxicity.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.13169</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Tay et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Yi&nbsp;Tay, Mostafa Dehghani, Samira Abnar, Hyung&nbsp;Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh&nbsp;Q Tran, Dani Yogatama, and Donald Metzler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Scaling laws vs model architectures: How does inductive bias influence scaling?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.10551</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Wettig et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Should you mask 15% in masked language modeling?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.9.1" class="ltx_text" style="font-size:90%;">In Andreas Vlachos and Isabelle Augenstein, editors, </span><em id="bib.bib11.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em><span id="bib.bib11.11.3" class="ltx_text" style="font-size:90%;">, pages 2985â€“3000, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.18653/v1/2023.eacl-main.217</span><span id="bib.bib11.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://aclanthology.org/2023.eacl-main.217" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://aclanthology.org/2023.eacl-main.217</a><span id="bib.bib11.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.4.4.1" class="ltx_text" style="font-size:90%;">HÃ¼llermeier and Waegeman [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text" style="font-size:90%;">
Eyke HÃ¼llermeier and Willem Waegeman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib12.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Machine learning</em><span id="bib.bib12.9.2" class="ltx_text" style="font-size:90%;">, 110(3):457â€“506, 2021.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Yu et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Longhui Yu, Weisen Jiang, Han Shi, YU&nbsp;Jincheng, Zhengying Liu, Yu&nbsp;Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Metamath: Bootstrap your own mathematical questions for large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib13.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Huang et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Key-point-driven data synthesis with its enhancement on mathematical reasoning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2403.02333</em><span id="bib.bib14.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Yue et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Xiang Yue, Xingwei Qu, Ge&nbsp;Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu&nbsp;Su, and Wenhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Mammoth: Building math generalist models through hybrid instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Ni et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Exploring the mystery of influential data for mathematical reasoning, 2024.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Ivison et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah&nbsp;A Smith, Iz&nbsp;Beltagy, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Camels in a changing climate: Enhancing lm adaptation with tulu 2.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2311.10702</em><span id="bib.bib17.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.4.4.1" class="ltx_text" style="font-size:90%;">Teknium [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">
Teknium.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://huggingface.co/datasets/teknium/OpenHermes-2.5" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://huggingface.co/datasets/teknium/OpenHermes-2.5</a><span id="bib.bib18.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Lightman et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Letâ€™s verify step by step.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.20050</em><span id="bib.bib19.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Paster et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Keiran Paster, Marco&nbsp;Dos Santos, Zhangir Azerbayev, and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Openwebmath: An open dataset of high-quality mathematical web text, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Daria et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Soboleva Daria, Al-Khateeb Faisal, Myers Robert Steeves&nbsp;Jacob R, Hestness Joel, and Dey Nolan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">SlimPajama: A 627B token cleaned and deduplicated version of RedPajama.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama</a><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.10.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://huggingface.co/datasets/cerebras/SlimPajama-627B" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://huggingface.co/datasets/cerebras/SlimPajama-627B</a><span id="bib.bib21.11.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Li et&nbsp;al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Raymond Li, Loubna&nbsp;Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry&nbsp;Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, JoÃ£o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh&nbsp;Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra&nbsp;Murthy V, Jason Stillerman, Siva&nbsp;Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn&nbsp;Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos&nbsp;MuÃ±oz Ferrandis, Sean Hughes, Thomas
Wolf, Arjun Guha, Leandro von Werra, and Harm de&nbsp;Vries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Starcoder: may the source be with you!
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, abs/2305.06161, 2023a.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Tinyllama: An open-source small language model, 2024.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Jiang et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Mistral 7b.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.06825</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Team et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane RiviÃ¨re, Mihir&nbsp;Sanjay Kale, Juliette Love, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Gemma: Open models based on gemini research and technology.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2403.08295</em><span id="bib.bib25.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Bai et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Qwen technical report.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2309.16609</em><span id="bib.bib26.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Li et&nbsp;al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Yuanzhi Li, SÃ©bastien Bubeck, Ronen Eldan, Allie&nbsp;Del Giorno, Suriya Gunasekar, and Yin&nbsp;Tat Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Textbooks are all you need ii: phi-1.5 technical report, 2023b.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.4.4.1" class="ltx_text" style="font-size:90%;">DeepSeek-AI [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text" style="font-size:90%;">
DeepSeek-AI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">Deepseek llm: Scaling open-source language models with longtermism.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.02954</em><span id="bib.bib28.9.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.10.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://github.com/deepseek-ai/DeepSeek-LLM" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/deepseek-ai/DeepSeek-LLM</a><span id="bib.bib28.11.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Shao et&nbsp;al. [2024a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y.&nbsp;Wu, and Daya Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024a.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/2402.03300" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/2402.03300</a><span id="bib.bib29.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Roziere et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing&nbsp;Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Code llama: Open foundation models for code.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib30.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.12950</em><span id="bib.bib30.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Lewkowycz et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Solving quantitative reasoning problems with language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib31.10.2" class="ltx_text" style="font-size:90%;">, 35:3843â€“3857, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Azerbayev et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco&nbsp;Dos Santos, Stephen McAleer, Albert&nbsp;Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Llemma: An open language model for mathematics.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.10631</em><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Ying et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Internlm-math: Open math large language models toward verifiable reasoning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib33.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.06332</em><span id="bib.bib33.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Gou et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Tora: A tool-integrated reasoning agent for mathematical problem solving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib34.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib34.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Kwon et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Efficient memory management for large language model serving with pagedattention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Wei et&nbsp;al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed&nbsp;Chi, Quoc&nbsp;V Le, Denny Zhou, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Chain-of-thought prompting elicits reasoning in large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib36.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib36.11.3" class="ltx_text" style="font-size:90%;">, volume&nbsp;35, pages 24824â€“24837, 2022a.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Shao et&nbsp;al. [2024b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK&nbsp;Li, Y&nbsp;Wu, and Daya Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib37.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.03300</em><span id="bib.bib37.10.2" class="ltx_text" style="font-size:90%;">, 2024b.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Gadre et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Samir&nbsp;Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros&nbsp;G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Language models scale reliably with over-training and on downstream tasks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib38.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Preprint</em><span id="bib.bib38.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Nakkiran et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Deep double descent: Where bigger models and more data hurt.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib39.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Statistical Mechanics: Theory and Experiment</em><span id="bib.bib39.10.2" class="ltx_text" style="font-size:90%;">, 2021(12):124003, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Devlin et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">BERT: pre-training of deep bidirectional transformers for language understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NAACL-HLT (1)</em><span id="bib.bib40.11.3" class="ltx_text" style="font-size:90%;">, pages 4171â€“4186. Association for Computational Linguistics, 2019.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Liu et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Roberta: A robustly optimized BERT pretraining approach.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib41.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib41.10.2" class="ltx_text" style="font-size:90%;">, abs/1907.11692, 2019.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Li et&nbsp;al. [2023c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Xiang&nbsp;Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Contrastive decoding: Open-ended text generation as optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACL (1)</em><span id="bib.bib42.11.3" class="ltx_text" style="font-size:90%;">, pages 12286â€“12312. Association for Computational Linguistics, 2023c.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Wan et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Knowledge fusion of large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://openreview.net/forum?id=jiDsk12qcz" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://openreview.net/forum?id=jiDsk12qcz</a><span id="bib.bib43.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Fu et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Specializing smaller language models towards multi-step reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib44.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib44.11.3" class="ltx_text" style="font-size:90%;">, pages 10421â€“10430. PMLR, 2023.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Raffel et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Exploring the limits of transfer learning with a unified text-to-text transformer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib45.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of machine learning research</em><span id="bib.bib45.10.2" class="ltx_text" style="font-size:90%;">, 21(140):1â€“67, 2020.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.4.4.1" class="ltx_text" style="font-size:90%;">Polu and Sutskever [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.6.1" class="ltx_text" style="font-size:90%;">
Stanislas Polu and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">Generative language modeling for automated theorem proving.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib46.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.03393</em><span id="bib.bib46.9.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Gunasekar et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Suriya Gunasekar, Yi&nbsp;Zhang, Jyoti Aneja, Caio CÃ©sar&nbsp;Teodoro Mendes, Allie Del&nbsp;Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de&nbsp;Rosa, Olli Saarikivi, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Textbooks are all you need.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib47.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.11644</em><span id="bib.bib47.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Lee et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Deduplicating training data makes language models better.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.06499</em><span id="bib.bib48.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Kandpal et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Nikhil Kandpal, Eric Wallace, and Colin Raffel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Deduplicating training data mitigates privacy risks in language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib49.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib49.11.3" class="ltx_text" style="font-size:90%;">, pages 10697â€“10707. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Tirumala et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">D4: Improving llm pretraining via document de-duplication and diversification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib50.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib50.11.3" class="ltx_text" style="font-size:90%;">, volume&nbsp;36, 2023.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Albalak et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Alon Albalak, Yanai Elazar, Sang&nbsp;Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William&nbsp;Yang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">A survey on data selection for language models, 2024.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Xie et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Sang&nbsp;Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy&nbsp;S Liang, Quoc&nbsp;V Le, Tengyu Ma, and Adams&nbsp;Wei Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Doremi: Optimizing data mixtures speeds up language model pretraining.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib52.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib52.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Chen et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce&nbsp;Zhang, Frederic Sala, and Christopher RÃ©.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">Skill-it! a data-driven skills framework for understanding and training language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib53.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib53.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">MA et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu&nbsp;Jiang, Changjian Wang, and Shanshan Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">At which training stage does code data help LLMs reasoning?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib54.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span id="bib.bib54.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://openreview.net/forum?id=KIPJKST4gw" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://openreview.net/forum?id=KIPJKST4gw</a><span id="bib.bib54.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Li et&nbsp;al. [2023d]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib55.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.12032</em><span id="bib.bib55.10.2" class="ltx_text" style="font-size:90%;">, 2023d.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text" style="font-size:90%;">Liu et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib56.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib56.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Li et&nbsp;al. [2023e]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">One shot learning as instruction data prospector for large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib57.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.10302</em><span id="bib.bib57.10.2" class="ltx_text" style="font-size:90%;">, 2023e.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">Xia et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Less: Selecting influential data for targeted instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib58.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.04333</em><span id="bib.bib58.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.4.4.1" class="ltx_text" style="font-size:90%;">Computer [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.6.1" class="ltx_text" style="font-size:90%;">
Together Computer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">Redpajama: an open dataset for training large language models, 10 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://github.com/togethercomputer/RedPajama-Data" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/togethercomputer/RedPajama-Data</a><span id="bib.bib59.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.4.4.1" class="ltx_text" style="font-size:90%;">Saphra and Lopez [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.6.1" class="ltx_text" style="font-size:90%;">
Naomi Saphra and Adam Lopez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.7.1" class="ltx_text" style="font-size:90%;">Understanding learning dynamics of language models with svcca.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib60.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.00225</em><span id="bib.bib60.9.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.5.5.1" class="ltx_text" style="font-size:90%;">Choshen et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text" style="font-size:90%;">
Leshem Choshen, Guy Hacohen, Daphna Weinshall, and Omri Abend.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.8.1" class="ltx_text" style="font-size:90%;">The grammar-learning trajectories of neural language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib61.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2109.06096</em><span id="bib.bib61.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.5.5.1" class="ltx_text" style="font-size:90%;">Liu et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.7.1" class="ltx_text" style="font-size:90%;">
Leo&nbsp;Z Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah&nbsp;A Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.8.1" class="ltx_text" style="font-size:90%;">Probing across time: What does roberta know and when?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib62.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2104.07885</em><span id="bib.bib62.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.5.5.1" class="ltx_text" style="font-size:90%;">Power et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.7.1" class="ltx_text" style="font-size:90%;">
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.8.1" class="ltx_text" style="font-size:90%;">Grokking: Generalization beyond overfitting on small algorithmic datasets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib63.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.02177</em><span id="bib.bib63.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.5.5.1" class="ltx_text" style="font-size:90%;">Xia et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.7.1" class="ltx_text" style="font-size:90%;">
Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi&nbsp;Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.8.1" class="ltx_text" style="font-size:90%;">Training trajectories of language models across scales.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib64.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.09803</em><span id="bib.bib64.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.5.5.1" class="ltx_text" style="font-size:90%;">Hernandez et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.7.1" class="ltx_text" style="font-size:90%;">
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.8.1" class="ltx_text" style="font-size:90%;">Scaling laws for transfer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib65.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2102.01293</em><span id="bib.bib65.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.5.5.1" class="ltx_text" style="font-size:90%;">Hoffmann et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.7.1" class="ltx_text" style="font-size:90%;">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.8.1" class="ltx_text" style="font-size:90%;">Training compute-optimal large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib66.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.15556</em><span id="bib.bib66.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.5.5.1" class="ltx_text" style="font-size:90%;">Wei et&nbsp;al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.7.1" class="ltx_text" style="font-size:90%;">
Jason Wei, Yi&nbsp;Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.8.1" class="ltx_text" style="font-size:90%;">Emergent abilities of large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib67.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2206.07682</em><span id="bib.bib67.10.2" class="ltx_text" style="font-size:90%;">, 2022b.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.5.5.1" class="ltx_text" style="font-size:90%;">Isik et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.7.1" class="ltx_text" style="font-size:90%;">
Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.8.1" class="ltx_text" style="font-size:90%;">Scaling laws for downstream task performance of large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib68.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.04177</em><span id="bib.bib68.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.5.5.1" class="ltx_text" style="font-size:90%;">Tirumala et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.7.1" class="ltx_text" style="font-size:90%;">
Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.8.1" class="ltx_text" style="font-size:90%;">Memorization without overfitting: Analyzing the training dynamics of large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib69.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib69.10.2" class="ltx_text" style="font-size:90%;">, 35:38274â€“38290, 2022.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.5.5.1" class="ltx_text" style="font-size:90%;">Carlini et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.7.1" class="ltx_text" style="font-size:90%;">
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.8.1" class="ltx_text" style="font-size:90%;">Quantifying memorization across neural language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib70.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2202.07646</em><span id="bib.bib70.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.5.5.1" class="ltx_text" style="font-size:90%;">Henighan et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.7.1" class="ltx_text" style="font-size:90%;">
Tom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort, Nicholas Schiefer, and Christopher Olah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.8.1" class="ltx_text" style="font-size:90%;">Superposition, memorization, and double descent.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib71.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Transformer Circuits Thread</em><span id="bib.bib71.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.5.5.1" class="ltx_text" style="font-size:90%;">Biderman et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.7.1" class="ltx_text" style="font-size:90%;">
Stella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.8.1" class="ltx_text" style="font-size:90%;">Emergent and predictable memorization in large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib72.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib72.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.5.5.1" class="ltx_text" style="font-size:90%;">Hernandez et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.7.1" class="ltx_text" style="font-size:90%;">
Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.8.1" class="ltx_text" style="font-size:90%;">Scaling laws and interpretability of learning from repeated data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib73.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.10487</em><span id="bib.bib73.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.5.5.1" class="ltx_text" style="font-size:90%;">Xue et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.7.1" class="ltx_text" style="font-size:90%;">
Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.8.1" class="ltx_text" style="font-size:90%;">To repeat or not to repeat: Insights from scaling llm under token-crisis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib74.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib74.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.4.4.1" class="ltx_text" style="font-size:90%;">Goodhart and Goodhart [1984]</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.6.1" class="ltx_text" style="font-size:90%;">
Charles&nbsp;AE Goodhart and CAE Goodhart.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib75.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Problems of monetary management: the UK experience</em><span id="bib.bib75.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.9.1" class="ltx_text" style="font-size:90%;">Springer, 1984.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.5.5.1" class="ltx_text" style="font-size:90%;">Ouyang et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.7.1" class="ltx_text" style="font-size:90%;">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.8.1" class="ltx_text" style="font-size:90%;">Training language models to follow instructions with human feedback.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib76.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib76.10.2" class="ltx_text" style="font-size:90%;">, 35:27730â€“27744, 2022.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.5.5.1" class="ltx_text" style="font-size:90%;">Cobbe et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.7.1" class="ltx_text" style="font-size:90%;">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.8.1" class="ltx_text" style="font-size:90%;">Training verifiers to solve math word problems, 2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/2110.14168" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/2110.14168</a><span id="bib.bib77.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.5.5.1" class="ltx_text" style="font-size:90%;">Hendrycks et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.7.1" class="ltx_text" style="font-size:90%;">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.8.1" class="ltx_text" style="font-size:90%;">Measuring mathematical problem solving with the math dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib78.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib78.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.5.5.1" class="ltx_text" style="font-size:90%;">Gao et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.7.1" class="ltx_text" style="font-size:90%;">
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.8.1" class="ltx_text" style="font-size:90%;">Pal: Program-aided language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib79.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2211.10435</em><span id="bib.bib79.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib80.5.5.1" class="ltx_text" style="font-size:90%;">Patel et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib80.7.1" class="ltx_text" style="font-size:90%;">
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.8.1" class="ltx_text" style="font-size:90%;">Are NLP models really able to solve simple math word problems?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib80.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em><span id="bib.bib80.11.3" class="ltx_text" style="font-size:90%;">, pages 2080â€“2094, Online, June 2021. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.18653/v1/2021.naacl-main.168</span><span id="bib.bib80.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://aclanthology.org/2021.naacl-main.168" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://aclanthology.org/2021.naacl-main.168</a><span id="bib.bib80.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib81.5.5.1" class="ltx_text" style="font-size:90%;">Miao et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib81.7.1" class="ltx_text" style="font-size:90%;">
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.8.1" class="ltx_text" style="font-size:90%;">A diverse corpus for evaluating and developing English math word problem solvers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib81.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em><span id="bib.bib81.11.3" class="ltx_text" style="font-size:90%;">, pages 975â€“984, Online, July 2020. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.18653/v1/2020.acl-main.92</span><span id="bib.bib81.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://aclanthology.org/2020.acl-main.92" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://aclanthology.org/2020.acl-main.92</a><span id="bib.bib81.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib82.5.5.1" class="ltx_text" style="font-size:90%;">Koncel-Kedziorski et&nbsp;al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib82.7.1" class="ltx_text" style="font-size:90%;">
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.8.1" class="ltx_text" style="font-size:90%;">MAWPS: A math word problem repository.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib82.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em><span id="bib.bib82.11.3" class="ltx_text" style="font-size:90%;">, pages 1152â€“1157, San Diego, California, June 2016. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.18653/v1/N16-1136</span><span id="bib.bib82.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://aclanthology.org/N16-1136" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://aclanthology.org/N16-1136</a><span id="bib.bib82.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib83.5.5.1" class="ltx_text" style="font-size:90%;">Lu et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib83.7.1" class="ltx_text" style="font-size:90%;">
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying&nbsp;Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.8.1" class="ltx_text" style="font-size:90%;">Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib83.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The Eleventh International Conference on Learning Representations</em><span id="bib.bib83.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://openreview.net/forum?id=DHyHRBwJUTN" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://openreview.net/forum?id=DHyHRBwJUTN</a><span id="bib.bib83.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib84.5.5.1" class="ltx_text" style="font-size:90%;">Amini et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib84.7.1" class="ltx_text" style="font-size:90%;">
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.8.1" class="ltx_text" style="font-size:90%;">Mathqa: Towards interpretable math word problem solving with operation-based formalisms.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib84.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.13319</em><span id="bib.bib84.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib85.5.5.1" class="ltx_text" style="font-size:90%;">Hendrycks et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib85.7.1" class="ltx_text" style="font-size:90%;">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.8.1" class="ltx_text" style="font-size:90%;">Measuring massive multitask language understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib85.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.03300</em><span id="bib.bib85.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib86.5.5.1" class="ltx_text" style="font-size:90%;">Gao et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib86.7.1" class="ltx_text" style="font-size:90%;">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le&nbsp;Noacâ€™h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.8.1" class="ltx_text" style="font-size:90%;">A framework for few-shot language model evaluation, 12 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://zenodo.org/records/10256836" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://zenodo.org/records/10256836</a><span id="bib.bib86.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib87.5.5.1" class="ltx_text" style="font-size:90%;">Suzgun et&nbsp;al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib87.7.1" class="ltx_text" style="font-size:90%;">
Mirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V Le, Ed&nbsp;H Chi, Denny Zhou, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.8.1" class="ltx_text" style="font-size:90%;">Challenging big-bench tasks and whether chain-of-thought can solve them.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib87.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.09261</em><span id="bib.bib87.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib88.5.5.1" class="ltx_text" style="font-size:90%;">Zhong et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib88.7.1" class="ltx_text" style="font-size:90%;">
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.8.1" class="ltx_text" style="font-size:90%;">Agieval: A human-centric benchmark for evaluating foundation models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib88.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.06364</em><span id="bib.bib88.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib89.5.5.1" class="ltx_text" style="font-size:90%;">Clark et&nbsp;al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib89.7.1" class="ltx_text" style="font-size:90%;">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.8.1" class="ltx_text" style="font-size:90%;">Think you have solved question answering? try arc, the ai2 reasoning challenge.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib89.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1803.05457</em><span id="bib.bib89.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib90.5.5.1" class="ltx_text" style="font-size:90%;">Clark et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib90.7.1" class="ltx_text" style="font-size:90%;">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.8.1" class="ltx_text" style="font-size:90%;">Boolq: Exploring the surprising difficulty of natural yes/no questions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib90.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.10044</em><span id="bib.bib90.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib91.5.5.1" class="ltx_text" style="font-size:90%;">Bisk et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib91.7.1" class="ltx_text" style="font-size:90%;">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.8.1" class="ltx_text" style="font-size:90%;">Piqa: Reasoning about physical commonsense in natural language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib91.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</em><span id="bib.bib91.11.3" class="ltx_text" style="font-size:90%;">, volume&nbsp;34, pages 7432â€“7439, 2020.
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib92.5.5.1" class="ltx_text" style="font-size:90%;">Zellers et&nbsp;al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib92.7.1" class="ltx_text" style="font-size:90%;">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.8.1" class="ltx_text" style="font-size:90%;">Hellaswag: Can a machine really finish your sentence?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib92.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.07830</em><span id="bib.bib92.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib93.5.5.1" class="ltx_text" style="font-size:90%;">Sakaguchi et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib93.7.1" class="ltx_text" style="font-size:90%;">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.8.1" class="ltx_text" style="font-size:90%;">Winogrande: An adversarial winograd schema challenge at scale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib93.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Communications of the ACM</em><span id="bib.bib93.10.2" class="ltx_text" style="font-size:90%;">, 64(9):99â€“106, 2021.
</span>
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib94.5.5.1" class="ltx_text" style="font-size:90%;">Mihaylov et&nbsp;al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib94.7.1" class="ltx_text" style="font-size:90%;">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.8.1" class="ltx_text" style="font-size:90%;">Can a suit of armor conduct electricity? a new dataset for open book question answering.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib94.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1809.02789</em><span id="bib.bib94.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib95.5.5.1" class="ltx_text" style="font-size:90%;">Zheng et&nbsp;al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib95.7.1" class="ltx_text" style="font-size:90%;">
Qinkai Zheng, Xiao Xia, Xu&nbsp;Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.8.1" class="ltx_text" style="font-size:90%;">Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib95.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em><span id="bib.bib95.11.3" class="ltx_text" style="font-size:90%;">, pages 5673â€“5684, 2023.
</span>
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib96.5.5.1" class="ltx_text" style="font-size:90%;">Clark et&nbsp;al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib96.7.1" class="ltx_text" style="font-size:90%;">
Jonathan&nbsp;H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib96.8.1" class="ltx_text" style="font-size:90%;">Tydi qa: A benchmark for information-seeking question answering in ty pologically di verse languages.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib96.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics</em><span id="bib.bib96.10.2" class="ltx_text" style="font-size:90%;">, 8:454â€“470, 2020.
</span>
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib97.5.5.1" class="ltx_text" style="font-size:90%;">Austin et&nbsp;al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib97.7.1" class="ltx_text" style="font-size:90%;">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib97.8.1" class="ltx_text" style="font-size:90%;">Program synthesis with large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib97.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2108.07732</em><span id="bib.bib97.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib98.5.5.1" class="ltx_text" style="font-size:90%;">Guo et&nbsp;al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib98.7.1" class="ltx_text" style="font-size:90%;">
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y&nbsp;Wu, YK&nbsp;Li, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib98.8.1" class="ltx_text" style="font-size:90%;">Deepseek-coder: When the large language model meets programmingâ€“the rise of code intelligence.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib98.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.14196</em><span id="bib.bib98.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Author Contributions</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Zhenghao Lin designed and implemented detailed token selection process, conducted extensive preliminary experiments, developed the pre-training and evaluation pipeline, conducted most of the pre-training experiments and analysis, implemented baselines, and significantly contributed to the writing.
Zhibin Gou presented a preliminary proposal, introduced the method of using excess loss for reweighting tokens, compiled high-quality corpora, trained reference models, set up the fine-tuning and evaluation pipelines, designed the experimental analysis, and significantly contributed to the writing.
Yeyun Gong proposed the initial project and co-led the project with Weizhu Chen, they offered extensive advice and guidance on experiments and writing, and oversaw team collaboration and resource management.
Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, and Nan Duan offered research mentorship, coordinated the project, and contributed to the writing.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Analysis and Visualization of Tokens in Pretraining</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>More Details of Four Categories Tokens</h3>

<figure id="A2.F10" class="ltx_figure"><img src="/html/2404.07965/assets/x10.png" id="A2.F10.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="608" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F10.7.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="A2.F10.8.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Sample text containing four categories of tokens.<span id="A2.F10.8.2.1" class="ltx_text ltx_font_medium"> Among them, <span id="A2.F10.8.2.1.1" class="ltx_text" style="color:#1E90FF;">blue</span> represents tokens of categorie Hâ†’L, <span id="A2.F10.8.2.1.2" class="ltx_text" style="color:#228B22;">green</span> indicates tokens of categorie Lâ†’L, <span id="A2.F10.8.2.1.3" class="ltx_text" style="color:#FAAA00;">yellow</span> signifies tokens of categorie Hâ†’H, and <span id="A2.F10.8.2.1.4" class="ltx_text" style="color:#FA8072;">red</span> denotes tokens of categorie Lâ†’H.</span></span></figcaption>
</figure>
<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">We categorize tokens into four categories: Hâ†’H, Lâ†’H, Hâ†’L, Lâ†’L. During the training process, we collected the loss of each token after training on each 1 billion tokens training data. We then used linear fitting and took the difference in loss between the first and last points as evidence of whether the loss decreased during the training process.</p>
</div>
<div id="A2.SS1.p2" class="ltx_para">
<p id="A2.SS1.p2.1" class="ltx_p">Specifically, suppose we have a sequence of tokenâ€™s loss <math id="A2.SS1.p2.1.m1.4" class="ltx_Math" alttext="(l_{0},l_{1},...,l_{n})" display="inline"><semantics id="A2.SS1.p2.1.m1.4a"><mrow id="A2.SS1.p2.1.m1.4.4.3" xref="A2.SS1.p2.1.m1.4.4.4.cmml"><mo stretchy="false" id="A2.SS1.p2.1.m1.4.4.3.4" xref="A2.SS1.p2.1.m1.4.4.4.cmml">(</mo><msub id="A2.SS1.p2.1.m1.2.2.1.1" xref="A2.SS1.p2.1.m1.2.2.1.1.cmml"><mi id="A2.SS1.p2.1.m1.2.2.1.1.2" xref="A2.SS1.p2.1.m1.2.2.1.1.2.cmml">l</mi><mn id="A2.SS1.p2.1.m1.2.2.1.1.3" xref="A2.SS1.p2.1.m1.2.2.1.1.3.cmml">0</mn></msub><mo id="A2.SS1.p2.1.m1.4.4.3.5" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="A2.SS1.p2.1.m1.3.3.2.2" xref="A2.SS1.p2.1.m1.3.3.2.2.cmml"><mi id="A2.SS1.p2.1.m1.3.3.2.2.2" xref="A2.SS1.p2.1.m1.3.3.2.2.2.cmml">l</mi><mn id="A2.SS1.p2.1.m1.3.3.2.2.3" xref="A2.SS1.p2.1.m1.3.3.2.2.3.cmml">1</mn></msub><mo id="A2.SS1.p2.1.m1.4.4.3.6" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><mi mathvariant="normal" id="A2.SS1.p2.1.m1.1.1" xref="A2.SS1.p2.1.m1.1.1.cmml">â€¦</mi><mo id="A2.SS1.p2.1.m1.4.4.3.7" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="A2.SS1.p2.1.m1.4.4.3.3" xref="A2.SS1.p2.1.m1.4.4.3.3.cmml"><mi id="A2.SS1.p2.1.m1.4.4.3.3.2" xref="A2.SS1.p2.1.m1.4.4.3.3.2.cmml">l</mi><mi id="A2.SS1.p2.1.m1.4.4.3.3.3" xref="A2.SS1.p2.1.m1.4.4.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="A2.SS1.p2.1.m1.4.4.3.8" xref="A2.SS1.p2.1.m1.4.4.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.1.m1.4b"><vector id="A2.SS1.p2.1.m1.4.4.4.cmml" xref="A2.SS1.p2.1.m1.4.4.3"><apply id="A2.SS1.p2.1.m1.2.2.1.1.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="A2.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1.2">ğ‘™</ci><cn type="integer" id="A2.SS1.p2.1.m1.2.2.1.1.3.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1.3">0</cn></apply><apply id="A2.SS1.p2.1.m1.3.3.2.2.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.3.3.2.2.1.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="A2.SS1.p2.1.m1.3.3.2.2.2.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2.2">ğ‘™</ci><cn type="integer" id="A2.SS1.p2.1.m1.3.3.2.2.3.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2.3">1</cn></apply><ci id="A2.SS1.p2.1.m1.1.1.cmml" xref="A2.SS1.p2.1.m1.1.1">â€¦</ci><apply id="A2.SS1.p2.1.m1.4.4.3.3.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.4.4.3.3.1.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3">subscript</csymbol><ci id="A2.SS1.p2.1.m1.4.4.3.3.2.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3.2">ğ‘™</ci><ci id="A2.SS1.p2.1.m1.4.4.3.3.3.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3.3">ğ‘›</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.1.m1.4c">(l_{0},l_{1},...,l_{n})</annotation></semantics></math>. Our goal is to minimize the sum of the squares of the differences between each data point and its linear predictive value:</p>
</div>
<div id="A2.SS1.p3" class="ltx_para">
<table id="A2.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A2.E6.m1.3" class="ltx_Math" alttext="f(a,b)=\text{minimize}\sum_{i=0}^{n}(l_{i}-(ax_{i}+b))^{2}," display="block"><semantics id="A2.E6.m1.3a"><mrow id="A2.E6.m1.3.3.1" xref="A2.E6.m1.3.3.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1" xref="A2.E6.m1.3.3.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.3" xref="A2.E6.m1.3.3.1.1.3.cmml"><mi id="A2.E6.m1.3.3.1.1.3.2" xref="A2.E6.m1.3.3.1.1.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="A2.E6.m1.3.3.1.1.3.1" xref="A2.E6.m1.3.3.1.1.3.1.cmml">â€‹</mo><mrow id="A2.E6.m1.3.3.1.1.3.3.2" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml"><mo stretchy="false" id="A2.E6.m1.3.3.1.1.3.3.2.1" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">(</mo><mi id="A2.E6.m1.1.1" xref="A2.E6.m1.1.1.cmml">a</mi><mo id="A2.E6.m1.3.3.1.1.3.3.2.2" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">,</mo><mi id="A2.E6.m1.2.2" xref="A2.E6.m1.2.2.cmml">b</mi><mo stretchy="false" id="A2.E6.m1.3.3.1.1.3.3.2.3" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="A2.E6.m1.3.3.1.1.2" xref="A2.E6.m1.3.3.1.1.2.cmml">=</mo><mrow id="A2.E6.m1.3.3.1.1.1" xref="A2.E6.m1.3.3.1.1.1.cmml"><mtext id="A2.E6.m1.3.3.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.3a.cmml">minimize</mtext><mo lspace="0em" rspace="0em" id="A2.E6.m1.3.3.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.2.cmml">â€‹</mo><mrow id="A2.E6.m1.3.3.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.cmml"><munderover id="A2.E6.m1.3.3.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="A2.E6.m1.3.3.1.1.1.1.2.2.2" xref="A2.E6.m1.3.3.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.2.2.3" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.2.2.3.2" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="A2.E6.m1.3.3.1.1.1.1.2.2.3.1" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="A2.E6.m1.3.3.1.1.1.1.2.2.3.3" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mi id="A2.E6.m1.3.3.1.1.1.1.2.3" xref="A2.E6.m1.3.3.1.1.1.1.2.3.cmml">n</mi></munderover><msup id="A2.E6.m1.3.3.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml"><msub id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml">l</mi><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">âˆ’</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">â€‹</mo><msub id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">x</mi><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">b</mi></mrow><mo stretchy="false" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="A2.E6.m1.3.3.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><mo id="A2.E6.m1.3.3.1.2" xref="A2.E6.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E6.m1.3b"><apply id="A2.E6.m1.3.3.1.1.cmml" xref="A2.E6.m1.3.3.1"><eq id="A2.E6.m1.3.3.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.2"></eq><apply id="A2.E6.m1.3.3.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.3"><times id="A2.E6.m1.3.3.1.1.3.1.cmml" xref="A2.E6.m1.3.3.1.1.3.1"></times><ci id="A2.E6.m1.3.3.1.1.3.2.cmml" xref="A2.E6.m1.3.3.1.1.3.2">ğ‘“</ci><interval closure="open" id="A2.E6.m1.3.3.1.1.3.3.1.cmml" xref="A2.E6.m1.3.3.1.1.3.3.2"><ci id="A2.E6.m1.1.1.cmml" xref="A2.E6.m1.1.1">ğ‘</ci><ci id="A2.E6.m1.2.2.cmml" xref="A2.E6.m1.2.2">ğ‘</ci></interval></apply><apply id="A2.E6.m1.3.3.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1"><times id="A2.E6.m1.3.3.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.2"></times><ci id="A2.E6.m1.3.3.1.1.1.3a.cmml" xref="A2.E6.m1.3.3.1.1.1.3"><mtext id="A2.E6.m1.3.3.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.3">minimize</mtext></ci><apply id="A2.E6.m1.3.3.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1"><apply id="A2.E6.m1.3.3.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2">superscript</csymbol><apply id="A2.E6.m1.3.3.1.1.1.1.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.2.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="A2.E6.m1.3.3.1.1.1.1.2.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.2"></sum><apply id="A2.E6.m1.3.3.1.1.1.1.2.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3"><eq id="A2.E6.m1.3.3.1.1.1.1.2.2.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.1"></eq><ci id="A2.E6.m1.3.3.1.1.1.1.2.2.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="A2.E6.m1.3.3.1.1.1.1.2.2.3.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.3">0</cn></apply></apply><ci id="A2.E6.m1.3.3.1.1.1.1.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.3">ğ‘›</ci></apply><apply id="A2.E6.m1.3.3.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1">superscript</csymbol><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1"><minus id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2"></minus><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2">ğ‘™</ci><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1"><plus id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2"><times id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1"></times><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ‘</ci><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2">ğ‘¥</ci><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3">ğ‘–</ci></apply></apply><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘</ci></apply></apply><cn type="integer" id="A2.E6.m1.3.3.1.1.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E6.m1.3c">f(a,b)=\text{minimize}\sum_{i=0}^{n}(l_{i}-(ax_{i}+b))^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="A2.SS1.p4" class="ltx_para">
<p id="A2.SS1.p4.6" class="ltx_p">where <math id="A2.SS1.p4.1.m1.1" class="ltx_Math" alttext="x_{0}=0" display="inline"><semantics id="A2.SS1.p4.1.m1.1a"><mrow id="A2.SS1.p4.1.m1.1.1" xref="A2.SS1.p4.1.m1.1.1.cmml"><msub id="A2.SS1.p4.1.m1.1.1.2" xref="A2.SS1.p4.1.m1.1.1.2.cmml"><mi id="A2.SS1.p4.1.m1.1.1.2.2" xref="A2.SS1.p4.1.m1.1.1.2.2.cmml">x</mi><mn id="A2.SS1.p4.1.m1.1.1.2.3" xref="A2.SS1.p4.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="A2.SS1.p4.1.m1.1.1.1" xref="A2.SS1.p4.1.m1.1.1.1.cmml">=</mo><mn id="A2.SS1.p4.1.m1.1.1.3" xref="A2.SS1.p4.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.1.m1.1b"><apply id="A2.SS1.p4.1.m1.1.1.cmml" xref="A2.SS1.p4.1.m1.1.1"><eq id="A2.SS1.p4.1.m1.1.1.1.cmml" xref="A2.SS1.p4.1.m1.1.1.1"></eq><apply id="A2.SS1.p4.1.m1.1.1.2.cmml" xref="A2.SS1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.1.m1.1.1.2.1.cmml" xref="A2.SS1.p4.1.m1.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.1.m1.1.1.2.2.cmml" xref="A2.SS1.p4.1.m1.1.1.2.2">ğ‘¥</ci><cn type="integer" id="A2.SS1.p4.1.m1.1.1.2.3.cmml" xref="A2.SS1.p4.1.m1.1.1.2.3">0</cn></apply><cn type="integer" id="A2.SS1.p4.1.m1.1.1.3.cmml" xref="A2.SS1.p4.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.1.m1.1c">x_{0}=0</annotation></semantics></math> is the initial checkpoint and <math id="A2.SS1.p4.2.m2.1" class="ltx_Math" alttext="x_{n}=n" display="inline"><semantics id="A2.SS1.p4.2.m2.1a"><mrow id="A2.SS1.p4.2.m2.1.1" xref="A2.SS1.p4.2.m2.1.1.cmml"><msub id="A2.SS1.p4.2.m2.1.1.2" xref="A2.SS1.p4.2.m2.1.1.2.cmml"><mi id="A2.SS1.p4.2.m2.1.1.2.2" xref="A2.SS1.p4.2.m2.1.1.2.2.cmml">x</mi><mi id="A2.SS1.p4.2.m2.1.1.2.3" xref="A2.SS1.p4.2.m2.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p4.2.m2.1.1.1" xref="A2.SS1.p4.2.m2.1.1.1.cmml">=</mo><mi id="A2.SS1.p4.2.m2.1.1.3" xref="A2.SS1.p4.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.2.m2.1b"><apply id="A2.SS1.p4.2.m2.1.1.cmml" xref="A2.SS1.p4.2.m2.1.1"><eq id="A2.SS1.p4.2.m2.1.1.1.cmml" xref="A2.SS1.p4.2.m2.1.1.1"></eq><apply id="A2.SS1.p4.2.m2.1.1.2.cmml" xref="A2.SS1.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.2.m2.1.1.2.1.cmml" xref="A2.SS1.p4.2.m2.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.2.m2.1.1.2.2.cmml" xref="A2.SS1.p4.2.m2.1.1.2.2">ğ‘¥</ci><ci id="A2.SS1.p4.2.m2.1.1.2.3.cmml" xref="A2.SS1.p4.2.m2.1.1.2.3">ğ‘›</ci></apply><ci id="A2.SS1.p4.2.m2.1.1.3.cmml" xref="A2.SS1.p4.2.m2.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.2.m2.1c">x_{n}=n</annotation></semantics></math> is the final checkpoint. Substituting these into the fitted equation, we can obtain the Loss values at the start and end after fitting: <math id="A2.SS1.p4.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{\text{start}}=b" display="inline"><semantics id="A2.SS1.p4.3.m3.1a"><mrow id="A2.SS1.p4.3.m3.1.1" xref="A2.SS1.p4.3.m3.1.1.cmml"><msub id="A2.SS1.p4.3.m3.1.1.2" xref="A2.SS1.p4.3.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.3.m3.1.1.2.2" xref="A2.SS1.p4.3.m3.1.1.2.2.cmml">â„’</mi><mtext id="A2.SS1.p4.3.m3.1.1.2.3" xref="A2.SS1.p4.3.m3.1.1.2.3a.cmml">start</mtext></msub><mo id="A2.SS1.p4.3.m3.1.1.1" xref="A2.SS1.p4.3.m3.1.1.1.cmml">=</mo><mi id="A2.SS1.p4.3.m3.1.1.3" xref="A2.SS1.p4.3.m3.1.1.3.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.3.m3.1b"><apply id="A2.SS1.p4.3.m3.1.1.cmml" xref="A2.SS1.p4.3.m3.1.1"><eq id="A2.SS1.p4.3.m3.1.1.1.cmml" xref="A2.SS1.p4.3.m3.1.1.1"></eq><apply id="A2.SS1.p4.3.m3.1.1.2.cmml" xref="A2.SS1.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.3.m3.1.1.2.1.cmml" xref="A2.SS1.p4.3.m3.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.3.m3.1.1.2.2.cmml" xref="A2.SS1.p4.3.m3.1.1.2.2">â„’</ci><ci id="A2.SS1.p4.3.m3.1.1.2.3a.cmml" xref="A2.SS1.p4.3.m3.1.1.2.3"><mtext mathsize="70%" id="A2.SS1.p4.3.m3.1.1.2.3.cmml" xref="A2.SS1.p4.3.m3.1.1.2.3">start</mtext></ci></apply><ci id="A2.SS1.p4.3.m3.1.1.3.cmml" xref="A2.SS1.p4.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.3.m3.1c">\mathcal{L}_{\text{start}}=b</annotation></semantics></math> and <math id="A2.SS1.p4.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{\text{end}}=an+b" display="inline"><semantics id="A2.SS1.p4.4.m4.1a"><mrow id="A2.SS1.p4.4.m4.1.1" xref="A2.SS1.p4.4.m4.1.1.cmml"><msub id="A2.SS1.p4.4.m4.1.1.2" xref="A2.SS1.p4.4.m4.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.4.m4.1.1.2.2" xref="A2.SS1.p4.4.m4.1.1.2.2.cmml">â„’</mi><mtext id="A2.SS1.p4.4.m4.1.1.2.3" xref="A2.SS1.p4.4.m4.1.1.2.3a.cmml">end</mtext></msub><mo id="A2.SS1.p4.4.m4.1.1.1" xref="A2.SS1.p4.4.m4.1.1.1.cmml">=</mo><mrow id="A2.SS1.p4.4.m4.1.1.3" xref="A2.SS1.p4.4.m4.1.1.3.cmml"><mrow id="A2.SS1.p4.4.m4.1.1.3.2" xref="A2.SS1.p4.4.m4.1.1.3.2.cmml"><mi id="A2.SS1.p4.4.m4.1.1.3.2.2" xref="A2.SS1.p4.4.m4.1.1.3.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p4.4.m4.1.1.3.2.1" xref="A2.SS1.p4.4.m4.1.1.3.2.1.cmml">â€‹</mo><mi id="A2.SS1.p4.4.m4.1.1.3.2.3" xref="A2.SS1.p4.4.m4.1.1.3.2.3.cmml">n</mi></mrow><mo id="A2.SS1.p4.4.m4.1.1.3.1" xref="A2.SS1.p4.4.m4.1.1.3.1.cmml">+</mo><mi id="A2.SS1.p4.4.m4.1.1.3.3" xref="A2.SS1.p4.4.m4.1.1.3.3.cmml">b</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.4.m4.1b"><apply id="A2.SS1.p4.4.m4.1.1.cmml" xref="A2.SS1.p4.4.m4.1.1"><eq id="A2.SS1.p4.4.m4.1.1.1.cmml" xref="A2.SS1.p4.4.m4.1.1.1"></eq><apply id="A2.SS1.p4.4.m4.1.1.2.cmml" xref="A2.SS1.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.4.m4.1.1.2.1.cmml" xref="A2.SS1.p4.4.m4.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.4.m4.1.1.2.2.cmml" xref="A2.SS1.p4.4.m4.1.1.2.2">â„’</ci><ci id="A2.SS1.p4.4.m4.1.1.2.3a.cmml" xref="A2.SS1.p4.4.m4.1.1.2.3"><mtext mathsize="70%" id="A2.SS1.p4.4.m4.1.1.2.3.cmml" xref="A2.SS1.p4.4.m4.1.1.2.3">end</mtext></ci></apply><apply id="A2.SS1.p4.4.m4.1.1.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3"><plus id="A2.SS1.p4.4.m4.1.1.3.1.cmml" xref="A2.SS1.p4.4.m4.1.1.3.1"></plus><apply id="A2.SS1.p4.4.m4.1.1.3.2.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2"><times id="A2.SS1.p4.4.m4.1.1.3.2.1.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.1"></times><ci id="A2.SS1.p4.4.m4.1.1.3.2.2.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.2">ğ‘</ci><ci id="A2.SS1.p4.4.m4.1.1.3.2.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.3">ğ‘›</ci></apply><ci id="A2.SS1.p4.4.m4.1.1.3.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.4.m4.1c">\mathcal{L}_{\text{end}}=an+b</annotation></semantics></math>. The change in loss can then be expressed as: <math id="A2.SS1.p4.5.m5.1" class="ltx_Math" alttext="\Delta\mathcal{L}=\mathcal{L}_{\text{end}}-\mathcal{L}_{\text{start}}" display="inline"><semantics id="A2.SS1.p4.5.m5.1a"><mrow id="A2.SS1.p4.5.m5.1.1" xref="A2.SS1.p4.5.m5.1.1.cmml"><mrow id="A2.SS1.p4.5.m5.1.1.2" xref="A2.SS1.p4.5.m5.1.1.2.cmml"><mi mathvariant="normal" id="A2.SS1.p4.5.m5.1.1.2.2" xref="A2.SS1.p4.5.m5.1.1.2.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p4.5.m5.1.1.2.1" xref="A2.SS1.p4.5.m5.1.1.2.1.cmml">â€‹</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.2.3" xref="A2.SS1.p4.5.m5.1.1.2.3.cmml">â„’</mi></mrow><mo id="A2.SS1.p4.5.m5.1.1.1" xref="A2.SS1.p4.5.m5.1.1.1.cmml">=</mo><mrow id="A2.SS1.p4.5.m5.1.1.3" xref="A2.SS1.p4.5.m5.1.1.3.cmml"><msub id="A2.SS1.p4.5.m5.1.1.3.2" xref="A2.SS1.p4.5.m5.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.3.2.2" xref="A2.SS1.p4.5.m5.1.1.3.2.2.cmml">â„’</mi><mtext id="A2.SS1.p4.5.m5.1.1.3.2.3" xref="A2.SS1.p4.5.m5.1.1.3.2.3a.cmml">end</mtext></msub><mo id="A2.SS1.p4.5.m5.1.1.3.1" xref="A2.SS1.p4.5.m5.1.1.3.1.cmml">âˆ’</mo><msub id="A2.SS1.p4.5.m5.1.1.3.3" xref="A2.SS1.p4.5.m5.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.3.3.2" xref="A2.SS1.p4.5.m5.1.1.3.3.2.cmml">â„’</mi><mtext id="A2.SS1.p4.5.m5.1.1.3.3.3" xref="A2.SS1.p4.5.m5.1.1.3.3.3a.cmml">start</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.5.m5.1b"><apply id="A2.SS1.p4.5.m5.1.1.cmml" xref="A2.SS1.p4.5.m5.1.1"><eq id="A2.SS1.p4.5.m5.1.1.1.cmml" xref="A2.SS1.p4.5.m5.1.1.1"></eq><apply id="A2.SS1.p4.5.m5.1.1.2.cmml" xref="A2.SS1.p4.5.m5.1.1.2"><times id="A2.SS1.p4.5.m5.1.1.2.1.cmml" xref="A2.SS1.p4.5.m5.1.1.2.1"></times><ci id="A2.SS1.p4.5.m5.1.1.2.2.cmml" xref="A2.SS1.p4.5.m5.1.1.2.2">Î”</ci><ci id="A2.SS1.p4.5.m5.1.1.2.3.cmml" xref="A2.SS1.p4.5.m5.1.1.2.3">â„’</ci></apply><apply id="A2.SS1.p4.5.m5.1.1.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3"><minus id="A2.SS1.p4.5.m5.1.1.3.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.1"></minus><apply id="A2.SS1.p4.5.m5.1.1.3.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="A2.SS1.p4.5.m5.1.1.3.2.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2">subscript</csymbol><ci id="A2.SS1.p4.5.m5.1.1.3.2.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2.2">â„’</ci><ci id="A2.SS1.p4.5.m5.1.1.3.2.3a.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2.3"><mtext mathsize="70%" id="A2.SS1.p4.5.m5.1.1.3.2.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2.3">end</mtext></ci></apply><apply id="A2.SS1.p4.5.m5.1.1.3.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="A2.SS1.p4.5.m5.1.1.3.3.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3">subscript</csymbol><ci id="A2.SS1.p4.5.m5.1.1.3.3.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3.2">â„’</ci><ci id="A2.SS1.p4.5.m5.1.1.3.3.3a.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3.3"><mtext mathsize="70%" id="A2.SS1.p4.5.m5.1.1.3.3.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3.3">start</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.5.m5.1c">\Delta\mathcal{L}=\mathcal{L}_{\text{end}}-\mathcal{L}_{\text{start}}</annotation></semantics></math>. Meanwhile, we represent the average Loss of the last checkpoint as <math id="A2.SS1.p4.6.m6.1" class="ltx_Math" alttext="\mathcal{L}_{\text{mean}}" display="inline"><semantics id="A2.SS1.p4.6.m6.1a"><msub id="A2.SS1.p4.6.m6.1.1" xref="A2.SS1.p4.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.6.m6.1.1.2" xref="A2.SS1.p4.6.m6.1.1.2.cmml">â„’</mi><mtext id="A2.SS1.p4.6.m6.1.1.3" xref="A2.SS1.p4.6.m6.1.1.3a.cmml">mean</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.6.m6.1b"><apply id="A2.SS1.p4.6.m6.1.1.cmml" xref="A2.SS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="A2.SS1.p4.6.m6.1.1.1.cmml" xref="A2.SS1.p4.6.m6.1.1">subscript</csymbol><ci id="A2.SS1.p4.6.m6.1.1.2.cmml" xref="A2.SS1.p4.6.m6.1.1.2">â„’</ci><ci id="A2.SS1.p4.6.m6.1.1.3a.cmml" xref="A2.SS1.p4.6.m6.1.1.3"><mtext mathsize="70%" id="A2.SS1.p4.6.m6.1.1.3.cmml" xref="A2.SS1.p4.6.m6.1.1.3">mean</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.6.m6.1c">\mathcal{L}_{\text{mean}}</annotation></semantics></math>.</p>
</div>
<div id="A2.SS1.p5" class="ltx_para">
<p id="A2.SS1.p5.7" class="ltx_p">Next, we can classify the tokens based on <math id="A2.SS1.p5.1.m1.1" class="ltx_Math" alttext="\Delta\mathcal{L}" display="inline"><semantics id="A2.SS1.p5.1.m1.1a"><mrow id="A2.SS1.p5.1.m1.1.1" xref="A2.SS1.p5.1.m1.1.1.cmml"><mi mathvariant="normal" id="A2.SS1.p5.1.m1.1.1.2" xref="A2.SS1.p5.1.m1.1.1.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p5.1.m1.1.1.1" xref="A2.SS1.p5.1.m1.1.1.1.cmml">â€‹</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.1.m1.1.1.3" xref="A2.SS1.p5.1.m1.1.1.3.cmml">â„’</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.1.m1.1b"><apply id="A2.SS1.p5.1.m1.1.1.cmml" xref="A2.SS1.p5.1.m1.1.1"><times id="A2.SS1.p5.1.m1.1.1.1.cmml" xref="A2.SS1.p5.1.m1.1.1.1"></times><ci id="A2.SS1.p5.1.m1.1.1.2.cmml" xref="A2.SS1.p5.1.m1.1.1.2">Î”</ci><ci id="A2.SS1.p5.1.m1.1.1.3.cmml" xref="A2.SS1.p5.1.m1.1.1.3">â„’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.1.m1.1c">\Delta\mathcal{L}</annotation></semantics></math> and the <math id="A2.SS1.p5.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\text{mean}}" display="inline"><semantics id="A2.SS1.p5.2.m2.1a"><msub id="A2.SS1.p5.2.m2.1.1" xref="A2.SS1.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.2.m2.1.1.2" xref="A2.SS1.p5.2.m2.1.1.2.cmml">â„’</mi><mtext id="A2.SS1.p5.2.m2.1.1.3" xref="A2.SS1.p5.2.m2.1.1.3a.cmml">mean</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.2.m2.1b"><apply id="A2.SS1.p5.2.m2.1.1.cmml" xref="A2.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS1.p5.2.m2.1.1.1.cmml" xref="A2.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="A2.SS1.p5.2.m2.1.1.2.cmml" xref="A2.SS1.p5.2.m2.1.1.2">â„’</ci><ci id="A2.SS1.p5.2.m2.1.1.3a.cmml" xref="A2.SS1.p5.2.m2.1.1.3"><mtext mathsize="70%" id="A2.SS1.p5.2.m2.1.1.3.cmml" xref="A2.SS1.p5.2.m2.1.1.3">mean</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.2.m2.1c">\mathcal{L}_{\text{mean}}</annotation></semantics></math>. We categorize tokens with <math id="A2.SS1.p5.3.m3.1" class="ltx_Math" alttext="\Delta\mathcal{L}<-0.2" display="inline"><semantics id="A2.SS1.p5.3.m3.1a"><mrow id="A2.SS1.p5.3.m3.1.1" xref="A2.SS1.p5.3.m3.1.1.cmml"><mrow id="A2.SS1.p5.3.m3.1.1.2" xref="A2.SS1.p5.3.m3.1.1.2.cmml"><mi mathvariant="normal" id="A2.SS1.p5.3.m3.1.1.2.2" xref="A2.SS1.p5.3.m3.1.1.2.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p5.3.m3.1.1.2.1" xref="A2.SS1.p5.3.m3.1.1.2.1.cmml">â€‹</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.3.m3.1.1.2.3" xref="A2.SS1.p5.3.m3.1.1.2.3.cmml">â„’</mi></mrow><mo id="A2.SS1.p5.3.m3.1.1.1" xref="A2.SS1.p5.3.m3.1.1.1.cmml">&lt;</mo><mrow id="A2.SS1.p5.3.m3.1.1.3" xref="A2.SS1.p5.3.m3.1.1.3.cmml"><mo id="A2.SS1.p5.3.m3.1.1.3a" xref="A2.SS1.p5.3.m3.1.1.3.cmml">âˆ’</mo><mn id="A2.SS1.p5.3.m3.1.1.3.2" xref="A2.SS1.p5.3.m3.1.1.3.2.cmml">0.2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.3.m3.1b"><apply id="A2.SS1.p5.3.m3.1.1.cmml" xref="A2.SS1.p5.3.m3.1.1"><lt id="A2.SS1.p5.3.m3.1.1.1.cmml" xref="A2.SS1.p5.3.m3.1.1.1"></lt><apply id="A2.SS1.p5.3.m3.1.1.2.cmml" xref="A2.SS1.p5.3.m3.1.1.2"><times id="A2.SS1.p5.3.m3.1.1.2.1.cmml" xref="A2.SS1.p5.3.m3.1.1.2.1"></times><ci id="A2.SS1.p5.3.m3.1.1.2.2.cmml" xref="A2.SS1.p5.3.m3.1.1.2.2">Î”</ci><ci id="A2.SS1.p5.3.m3.1.1.2.3.cmml" xref="A2.SS1.p5.3.m3.1.1.2.3">â„’</ci></apply><apply id="A2.SS1.p5.3.m3.1.1.3.cmml" xref="A2.SS1.p5.3.m3.1.1.3"><minus id="A2.SS1.p5.3.m3.1.1.3.1.cmml" xref="A2.SS1.p5.3.m3.1.1.3"></minus><cn type="float" id="A2.SS1.p5.3.m3.1.1.3.2.cmml" xref="A2.SS1.p5.3.m3.1.1.3.2">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.3.m3.1c">\Delta\mathcal{L}&lt;-0.2</annotation></semantics></math> as Hâ†’L (loss decreases from high to low) category tokens, and tokens with <math id="A2.SS1.p5.4.m4.1" class="ltx_Math" alttext="\Delta\mathcal{L}>0.2" display="inline"><semantics id="A2.SS1.p5.4.m4.1a"><mrow id="A2.SS1.p5.4.m4.1.1" xref="A2.SS1.p5.4.m4.1.1.cmml"><mrow id="A2.SS1.p5.4.m4.1.1.2" xref="A2.SS1.p5.4.m4.1.1.2.cmml"><mi mathvariant="normal" id="A2.SS1.p5.4.m4.1.1.2.2" xref="A2.SS1.p5.4.m4.1.1.2.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p5.4.m4.1.1.2.1" xref="A2.SS1.p5.4.m4.1.1.2.1.cmml">â€‹</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.4.m4.1.1.2.3" xref="A2.SS1.p5.4.m4.1.1.2.3.cmml">â„’</mi></mrow><mo id="A2.SS1.p5.4.m4.1.1.1" xref="A2.SS1.p5.4.m4.1.1.1.cmml">&gt;</mo><mn id="A2.SS1.p5.4.m4.1.1.3" xref="A2.SS1.p5.4.m4.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.4.m4.1b"><apply id="A2.SS1.p5.4.m4.1.1.cmml" xref="A2.SS1.p5.4.m4.1.1"><gt id="A2.SS1.p5.4.m4.1.1.1.cmml" xref="A2.SS1.p5.4.m4.1.1.1"></gt><apply id="A2.SS1.p5.4.m4.1.1.2.cmml" xref="A2.SS1.p5.4.m4.1.1.2"><times id="A2.SS1.p5.4.m4.1.1.2.1.cmml" xref="A2.SS1.p5.4.m4.1.1.2.1"></times><ci id="A2.SS1.p5.4.m4.1.1.2.2.cmml" xref="A2.SS1.p5.4.m4.1.1.2.2">Î”</ci><ci id="A2.SS1.p5.4.m4.1.1.2.3.cmml" xref="A2.SS1.p5.4.m4.1.1.2.3">â„’</ci></apply><cn type="float" id="A2.SS1.p5.4.m4.1.1.3.cmml" xref="A2.SS1.p5.4.m4.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.4.m4.1c">\Delta\mathcal{L}&gt;0.2</annotation></semantics></math> as Lâ†’H (loss increases from low to high) category tokens. If <math id="A2.SS1.p5.5.m5.1" class="ltx_Math" alttext="-0.2\leq\Delta\mathcal{L}\leq 0.2" display="inline"><semantics id="A2.SS1.p5.5.m5.1a"><mrow id="A2.SS1.p5.5.m5.1.1" xref="A2.SS1.p5.5.m5.1.1.cmml"><mrow id="A2.SS1.p5.5.m5.1.1.2" xref="A2.SS1.p5.5.m5.1.1.2.cmml"><mo id="A2.SS1.p5.5.m5.1.1.2a" xref="A2.SS1.p5.5.m5.1.1.2.cmml">âˆ’</mo><mn id="A2.SS1.p5.5.m5.1.1.2.2" xref="A2.SS1.p5.5.m5.1.1.2.2.cmml">0.2</mn></mrow><mo id="A2.SS1.p5.5.m5.1.1.3" xref="A2.SS1.p5.5.m5.1.1.3.cmml">â‰¤</mo><mrow id="A2.SS1.p5.5.m5.1.1.4" xref="A2.SS1.p5.5.m5.1.1.4.cmml"><mi mathvariant="normal" id="A2.SS1.p5.5.m5.1.1.4.2" xref="A2.SS1.p5.5.m5.1.1.4.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="A2.SS1.p5.5.m5.1.1.4.1" xref="A2.SS1.p5.5.m5.1.1.4.1.cmml">â€‹</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.5.m5.1.1.4.3" xref="A2.SS1.p5.5.m5.1.1.4.3.cmml">â„’</mi></mrow><mo id="A2.SS1.p5.5.m5.1.1.5" xref="A2.SS1.p5.5.m5.1.1.5.cmml">â‰¤</mo><mn id="A2.SS1.p5.5.m5.1.1.6" xref="A2.SS1.p5.5.m5.1.1.6.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.5.m5.1b"><apply id="A2.SS1.p5.5.m5.1.1.cmml" xref="A2.SS1.p5.5.m5.1.1"><and id="A2.SS1.p5.5.m5.1.1a.cmml" xref="A2.SS1.p5.5.m5.1.1"></and><apply id="A2.SS1.p5.5.m5.1.1b.cmml" xref="A2.SS1.p5.5.m5.1.1"><leq id="A2.SS1.p5.5.m5.1.1.3.cmml" xref="A2.SS1.p5.5.m5.1.1.3"></leq><apply id="A2.SS1.p5.5.m5.1.1.2.cmml" xref="A2.SS1.p5.5.m5.1.1.2"><minus id="A2.SS1.p5.5.m5.1.1.2.1.cmml" xref="A2.SS1.p5.5.m5.1.1.2"></minus><cn type="float" id="A2.SS1.p5.5.m5.1.1.2.2.cmml" xref="A2.SS1.p5.5.m5.1.1.2.2">0.2</cn></apply><apply id="A2.SS1.p5.5.m5.1.1.4.cmml" xref="A2.SS1.p5.5.m5.1.1.4"><times id="A2.SS1.p5.5.m5.1.1.4.1.cmml" xref="A2.SS1.p5.5.m5.1.1.4.1"></times><ci id="A2.SS1.p5.5.m5.1.1.4.2.cmml" xref="A2.SS1.p5.5.m5.1.1.4.2">Î”</ci><ci id="A2.SS1.p5.5.m5.1.1.4.3.cmml" xref="A2.SS1.p5.5.m5.1.1.4.3">â„’</ci></apply></apply><apply id="A2.SS1.p5.5.m5.1.1c.cmml" xref="A2.SS1.p5.5.m5.1.1"><leq id="A2.SS1.p5.5.m5.1.1.5.cmml" xref="A2.SS1.p5.5.m5.1.1.5"></leq><share href="#A2.SS1.p5.5.m5.1.1.4.cmml" id="A2.SS1.p5.5.m5.1.1d.cmml" xref="A2.SS1.p5.5.m5.1.1"></share><cn type="float" id="A2.SS1.p5.5.m5.1.1.6.cmml" xref="A2.SS1.p5.5.m5.1.1.6">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.5.m5.1c">-0.2\leq\Delta\mathcal{L}\leq 0.2</annotation></semantics></math> and <math id="A2.SS1.p5.6.m6.1" class="ltx_Math" alttext="l_{n}\leq\mathcal{L}_{\text{mean}}" display="inline"><semantics id="A2.SS1.p5.6.m6.1a"><mrow id="A2.SS1.p5.6.m6.1.1" xref="A2.SS1.p5.6.m6.1.1.cmml"><msub id="A2.SS1.p5.6.m6.1.1.2" xref="A2.SS1.p5.6.m6.1.1.2.cmml"><mi id="A2.SS1.p5.6.m6.1.1.2.2" xref="A2.SS1.p5.6.m6.1.1.2.2.cmml">l</mi><mi id="A2.SS1.p5.6.m6.1.1.2.3" xref="A2.SS1.p5.6.m6.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p5.6.m6.1.1.1" xref="A2.SS1.p5.6.m6.1.1.1.cmml">â‰¤</mo><msub id="A2.SS1.p5.6.m6.1.1.3" xref="A2.SS1.p5.6.m6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.6.m6.1.1.3.2" xref="A2.SS1.p5.6.m6.1.1.3.2.cmml">â„’</mi><mtext id="A2.SS1.p5.6.m6.1.1.3.3" xref="A2.SS1.p5.6.m6.1.1.3.3a.cmml">mean</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.6.m6.1b"><apply id="A2.SS1.p5.6.m6.1.1.cmml" xref="A2.SS1.p5.6.m6.1.1"><leq id="A2.SS1.p5.6.m6.1.1.1.cmml" xref="A2.SS1.p5.6.m6.1.1.1"></leq><apply id="A2.SS1.p5.6.m6.1.1.2.cmml" xref="A2.SS1.p5.6.m6.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p5.6.m6.1.1.2.1.cmml" xref="A2.SS1.p5.6.m6.1.1.2">subscript</csymbol><ci id="A2.SS1.p5.6.m6.1.1.2.2.cmml" xref="A2.SS1.p5.6.m6.1.1.2.2">ğ‘™</ci><ci id="A2.SS1.p5.6.m6.1.1.2.3.cmml" xref="A2.SS1.p5.6.m6.1.1.2.3">ğ‘›</ci></apply><apply id="A2.SS1.p5.6.m6.1.1.3.cmml" xref="A2.SS1.p5.6.m6.1.1.3"><csymbol cd="ambiguous" id="A2.SS1.p5.6.m6.1.1.3.1.cmml" xref="A2.SS1.p5.6.m6.1.1.3">subscript</csymbol><ci id="A2.SS1.p5.6.m6.1.1.3.2.cmml" xref="A2.SS1.p5.6.m6.1.1.3.2">â„’</ci><ci id="A2.SS1.p5.6.m6.1.1.3.3a.cmml" xref="A2.SS1.p5.6.m6.1.1.3.3"><mtext mathsize="70%" id="A2.SS1.p5.6.m6.1.1.3.3.cmml" xref="A2.SS1.p5.6.m6.1.1.3.3">mean</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.6.m6.1c">l_{n}\leq\mathcal{L}_{\text{mean}}</annotation></semantics></math>, then tokens are classified as Lâ†’L (loss remains low); if <math id="A2.SS1.p5.7.m7.1" class="ltx_Math" alttext="l_{n}>\mathcal{L}_{\text{mean}}" display="inline"><semantics id="A2.SS1.p5.7.m7.1a"><mrow id="A2.SS1.p5.7.m7.1.1" xref="A2.SS1.p5.7.m7.1.1.cmml"><msub id="A2.SS1.p5.7.m7.1.1.2" xref="A2.SS1.p5.7.m7.1.1.2.cmml"><mi id="A2.SS1.p5.7.m7.1.1.2.2" xref="A2.SS1.p5.7.m7.1.1.2.2.cmml">l</mi><mi id="A2.SS1.p5.7.m7.1.1.2.3" xref="A2.SS1.p5.7.m7.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p5.7.m7.1.1.1" xref="A2.SS1.p5.7.m7.1.1.1.cmml">&gt;</mo><msub id="A2.SS1.p5.7.m7.1.1.3" xref="A2.SS1.p5.7.m7.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.7.m7.1.1.3.2" xref="A2.SS1.p5.7.m7.1.1.3.2.cmml">â„’</mi><mtext id="A2.SS1.p5.7.m7.1.1.3.3" xref="A2.SS1.p5.7.m7.1.1.3.3a.cmml">mean</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.7.m7.1b"><apply id="A2.SS1.p5.7.m7.1.1.cmml" xref="A2.SS1.p5.7.m7.1.1"><gt id="A2.SS1.p5.7.m7.1.1.1.cmml" xref="A2.SS1.p5.7.m7.1.1.1"></gt><apply id="A2.SS1.p5.7.m7.1.1.2.cmml" xref="A2.SS1.p5.7.m7.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p5.7.m7.1.1.2.1.cmml" xref="A2.SS1.p5.7.m7.1.1.2">subscript</csymbol><ci id="A2.SS1.p5.7.m7.1.1.2.2.cmml" xref="A2.SS1.p5.7.m7.1.1.2.2">ğ‘™</ci><ci id="A2.SS1.p5.7.m7.1.1.2.3.cmml" xref="A2.SS1.p5.7.m7.1.1.2.3">ğ‘›</ci></apply><apply id="A2.SS1.p5.7.m7.1.1.3.cmml" xref="A2.SS1.p5.7.m7.1.1.3"><csymbol cd="ambiguous" id="A2.SS1.p5.7.m7.1.1.3.1.cmml" xref="A2.SS1.p5.7.m7.1.1.3">subscript</csymbol><ci id="A2.SS1.p5.7.m7.1.1.3.2.cmml" xref="A2.SS1.p5.7.m7.1.1.3.2">â„’</ci><ci id="A2.SS1.p5.7.m7.1.1.3.3a.cmml" xref="A2.SS1.p5.7.m7.1.1.3.3"><mtext mathsize="70%" id="A2.SS1.p5.7.m7.1.1.3.3.cmml" xref="A2.SS1.p5.7.m7.1.1.3.3">mean</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.7.m7.1c">l_{n}&gt;\mathcal{L}_{\text{mean}}</annotation></semantics></math>, they are classified as Hâ†’H (loss remains high). In <a href="#A2.F10" title="Figure 10 â€£ B.1 More Details of Four Categories Tokens â€£ Appendix B Analysis and Visualization of Tokens in Pretraining â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;10</span></a>, we visualize examples of the four categories of tokens in actual text.</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Non-Converging Tokens in Pretrainig</h3>

<figure id="A2.F11" class="ltx_figure"><img src="/html/2404.07965/assets/x11.png" id="A2.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="511" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F11.4.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="A2.F11.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">An example of an abnormal state of token perplexity during pretrainig process.<span id="A2.F11.5.2.1" class="ltx_text ltx_font_medium"> The tokens highlighted in <span id="A2.F11.5.2.1.1" class="ltx_text" style="color:#FF8C00;">orange</span> represent tokens that were significant abnormalities during the pretraining process.</span></span></figcaption>
</figure>
<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">In <a href="#S2.SS1" title="2.1 Not All Tokens Are Equal: Training Dynamics of Token Loss â€£ 2 Selective Language Modeling â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Â§2.1</span></a>, we mentioned that during the training process, only a minority of tokens belong to the Hâ†’L category. Among the remaining categories of Hâ†’H and Lâ†’L tokens, there are tokens that exhibit significant fluctuations during training. Furthermore, there are instances where Hâ†’L tokens are not effectively learned. Therefore, in our analysis, we specifically select those tokens from these categories that demonstrate considerable variability and distinct loss.
We visualize these tokens that exhibit abnormal behavior during the training process. As illustrated in <a href="#A2.F11" title="Figure 11 â€£ B.2 Non-Converging Tokens in Pretrainig â€£ Appendix B Analysis and Visualization of Tokens in Pretraining â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;11</span></a>, we find that the majority of these tokens originate from rather chaotic corpora. For instance, the corpora may include a mix of custom symbols, unintelligible gibberish, and information such as timetables and bibliographic references. Within a segment of normal text, there may also be fluctuations in the usage of common conjunctions, word suffixes, and punctuation marks. The latter may not necessarily be disastrous for training; in fact, it could represent a normal occurrence. However, if we can effectively mitigate the losses caused by the former, it might lead to more stable and efficient model training.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Evalution Details</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Math Evalution</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">We conducted a comprehensive evaluation of the model across various math reasoning benchmarks, encompassing a range of difficulties from elementary to university level, multiple mathematical domains, and diverse question types including multiple-choice and open-ended questions.
Our benchmarks include GSM8k&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Cobbe et&nbsp;al.</span>, <a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, MATH <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et&nbsp;al.</span>, <a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, GSM-Hard <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gao et&nbsp;al.</span>, <a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, SVAMP&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Patel et&nbsp;al.</span>, <a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, ASDIV <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Miao et&nbsp;al.</span>, <a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, MAWPS <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Koncel-Kedziorski et&nbsp;al.</span>, <a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>, TabMWP (TAB) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lu et&nbsp;al.</span>, <a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, MathQA (MQA) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Amini et&nbsp;al.</span>, <a href="#bib.bib84" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, MMLU-STEM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et&nbsp;al.</span>, <a href="#bib.bib85" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, and SAT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Azerbayev et&nbsp;al.</span>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>General Evalution</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p">In the evaluation of general domain, we followed the lm-evaluation-harness&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gao et&nbsp;al.</span>, <a href="#bib.bib86" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> and evalute model on MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et&nbsp;al.</span>, <a href="#bib.bib85" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, BBH&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Suzgun et&nbsp;al.</span>, <a href="#bib.bib87" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, AGIEval&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et&nbsp;al.</span>, <a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, ARC-Easy and ARC-Challenge&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et&nbsp;al.</span>, <a href="#bib.bib89" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, BoolQ&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et&nbsp;al.</span>, <a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, PIQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bisk et&nbsp;al.</span>, <a href="#bib.bib91" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, Hellaswag&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zellers et&nbsp;al.</span>, <a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, WinoGrande&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sakaguchi et&nbsp;al.</span>, <a href="#bib.bib93" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, OpenBookQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Mihaylov et&nbsp;al.</span>, <a href="#bib.bib94" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>.
On HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zheng et&nbsp;al.</span>, <a href="#bib.bib95" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> and TydiQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et&nbsp;al.</span>, <a href="#bib.bib96" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, we follow the evaluation pipeline of open-instrcut&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ivison et&nbsp;al.</span>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> and report Pass@1 and Pass@10 for HumanEval and F1 for TydiQA. For MBPP&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Austin et&nbsp;al.</span>, <a href="#bib.bib97" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> benchmark, we follow the evaluation pipeline of DeepSeek-Coder&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Guo et&nbsp;al.</span>, <a href="#bib.bib98" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, and report Pass@1 and Pass@10.</p>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Relate the Selected Tokensâ€™ Loss to Downstream Task Performance</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">In this section, we declare the details about correlating the loss of selected tokens with the performance of downstream tasks. Concurrent study has explored similar methods to study the impact of scaling laws with the performance of models in downstream tasks&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gadre et&nbsp;al.</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>. Our analysis here differs in that it aims to elucidate the relationship between the decrease/increase in loss for selected/unselected tokens and the modelâ€™s performance on downstream tasks.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.1" class="ltx_p">We use the average accuracy of MATH and GSM8K as the standard for measuring downstream tasks performance of model. Based on the trend of data points in <a href="#S3.F7" title="Figure 7 â€£ Selected Token Loss Aligns Better with Downstream Performance â€£ 3.4 Analysis â€£ 3 Experiments â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;7</span></a>, we propose the relationship between the average accuracy of downstream tasks and selected/unselected tokensâ€™ loss,</p>
</div>
<div id="A4.p3" class="ltx_para">
<table id="A4.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="A4.E7.m1.3" class="ltx_Math" alttext="Acc(\mathcal{L})=\log(a*\mathcal{L}+c)" display="block"><semantics id="A4.E7.m1.3a"><mrow id="A4.E7.m1.3.3" xref="A4.E7.m1.3.3.cmml"><mrow id="A4.E7.m1.3.3.3" xref="A4.E7.m1.3.3.3.cmml"><mi id="A4.E7.m1.3.3.3.2" xref="A4.E7.m1.3.3.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="A4.E7.m1.3.3.3.1" xref="A4.E7.m1.3.3.3.1.cmml">â€‹</mo><mi id="A4.E7.m1.3.3.3.3" xref="A4.E7.m1.3.3.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A4.E7.m1.3.3.3.1a" xref="A4.E7.m1.3.3.3.1.cmml">â€‹</mo><mi id="A4.E7.m1.3.3.3.4" xref="A4.E7.m1.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="A4.E7.m1.3.3.3.1b" xref="A4.E7.m1.3.3.3.1.cmml">â€‹</mo><mrow id="A4.E7.m1.3.3.3.5.2" xref="A4.E7.m1.3.3.3.cmml"><mo stretchy="false" id="A4.E7.m1.3.3.3.5.2.1" xref="A4.E7.m1.3.3.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="A4.E7.m1.1.1" xref="A4.E7.m1.1.1.cmml">â„’</mi><mo stretchy="false" id="A4.E7.m1.3.3.3.5.2.2" xref="A4.E7.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="A4.E7.m1.3.3.2" xref="A4.E7.m1.3.3.2.cmml">=</mo><mrow id="A4.E7.m1.3.3.1.1" xref="A4.E7.m1.3.3.1.2.cmml"><mi id="A4.E7.m1.2.2" xref="A4.E7.m1.2.2.cmml">log</mi><mo id="A4.E7.m1.3.3.1.1a" xref="A4.E7.m1.3.3.1.2.cmml">â¡</mo><mrow id="A4.E7.m1.3.3.1.1.1" xref="A4.E7.m1.3.3.1.2.cmml"><mo stretchy="false" id="A4.E7.m1.3.3.1.1.1.2" xref="A4.E7.m1.3.3.1.2.cmml">(</mo><mrow id="A4.E7.m1.3.3.1.1.1.1" xref="A4.E7.m1.3.3.1.1.1.1.cmml"><mrow id="A4.E7.m1.3.3.1.1.1.1.2" xref="A4.E7.m1.3.3.1.1.1.1.2.cmml"><mi id="A4.E7.m1.3.3.1.1.1.1.2.2" xref="A4.E7.m1.3.3.1.1.1.1.2.2.cmml">a</mi><mo lspace="0.222em" rspace="0.222em" id="A4.E7.m1.3.3.1.1.1.1.2.1" xref="A4.E7.m1.3.3.1.1.1.1.2.1.cmml">âˆ—</mo><mi class="ltx_font_mathcaligraphic" id="A4.E7.m1.3.3.1.1.1.1.2.3" xref="A4.E7.m1.3.3.1.1.1.1.2.3.cmml">â„’</mi></mrow><mo id="A4.E7.m1.3.3.1.1.1.1.1" xref="A4.E7.m1.3.3.1.1.1.1.1.cmml">+</mo><mi id="A4.E7.m1.3.3.1.1.1.1.3" xref="A4.E7.m1.3.3.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="A4.E7.m1.3.3.1.1.1.3" xref="A4.E7.m1.3.3.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.E7.m1.3b"><apply id="A4.E7.m1.3.3.cmml" xref="A4.E7.m1.3.3"><eq id="A4.E7.m1.3.3.2.cmml" xref="A4.E7.m1.3.3.2"></eq><apply id="A4.E7.m1.3.3.3.cmml" xref="A4.E7.m1.3.3.3"><times id="A4.E7.m1.3.3.3.1.cmml" xref="A4.E7.m1.3.3.3.1"></times><ci id="A4.E7.m1.3.3.3.2.cmml" xref="A4.E7.m1.3.3.3.2">ğ´</ci><ci id="A4.E7.m1.3.3.3.3.cmml" xref="A4.E7.m1.3.3.3.3">ğ‘</ci><ci id="A4.E7.m1.3.3.3.4.cmml" xref="A4.E7.m1.3.3.3.4">ğ‘</ci><ci id="A4.E7.m1.1.1.cmml" xref="A4.E7.m1.1.1">â„’</ci></apply><apply id="A4.E7.m1.3.3.1.2.cmml" xref="A4.E7.m1.3.3.1.1"><log id="A4.E7.m1.2.2.cmml" xref="A4.E7.m1.2.2"></log><apply id="A4.E7.m1.3.3.1.1.1.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1"><plus id="A4.E7.m1.3.3.1.1.1.1.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1.1"></plus><apply id="A4.E7.m1.3.3.1.1.1.1.2.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2"><times id="A4.E7.m1.3.3.1.1.1.1.2.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.1"></times><ci id="A4.E7.m1.3.3.1.1.1.1.2.2.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.2">ğ‘</ci><ci id="A4.E7.m1.3.3.1.1.1.1.2.3.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.3">â„’</ci></apply><ci id="A4.E7.m1.3.3.1.1.1.1.3.cmml" xref="A4.E7.m1.3.3.1.1.1.1.3">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.E7.m1.3c">Acc(\mathcal{L})=\log(a*\mathcal{L}+c)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div id="A4.p4" class="ltx_para">
<p id="A4.p4.6" class="ltx_p">The parameters <math id="A4.p4.1.m1.1" class="ltx_Math" alttext="a" display="inline"><semantics id="A4.p4.1.m1.1a"><mi id="A4.p4.1.m1.1.1" xref="A4.p4.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="A4.p4.1.m1.1b"><ci id="A4.p4.1.m1.1.1.cmml" xref="A4.p4.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.1.m1.1c">a</annotation></semantics></math> and <math id="A4.p4.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="A4.p4.2.m2.1a"><mi id="A4.p4.2.m2.1.1" xref="A4.p4.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="A4.p4.2.m2.1b"><ci id="A4.p4.2.m2.1.1.cmml" xref="A4.p4.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.2.m2.1c">c</annotation></semantics></math> are fitted from the data. If the loss of selected tokens <math id="A4.p4.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{s}" display="inline"><semantics id="A4.p4.3.m3.1a"><msub id="A4.p4.3.m3.1.1" xref="A4.p4.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.p4.3.m3.1.1.2" xref="A4.p4.3.m3.1.1.2.cmml">â„’</mi><mi id="A4.p4.3.m3.1.1.3" xref="A4.p4.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p4.3.m3.1b"><apply id="A4.p4.3.m3.1.1.cmml" xref="A4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="A4.p4.3.m3.1.1.1.cmml" xref="A4.p4.3.m3.1.1">subscript</csymbol><ci id="A4.p4.3.m3.1.1.2.cmml" xref="A4.p4.3.m3.1.1.2">â„’</ci><ci id="A4.p4.3.m3.1.1.3.cmml" xref="A4.p4.3.m3.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.3.m3.1c">\mathcal{L}_{s}</annotation></semantics></math> is used for fitting, then <math id="A4.p4.4.m4.1" class="ltx_Math" alttext="a>0" display="inline"><semantics id="A4.p4.4.m4.1a"><mrow id="A4.p4.4.m4.1.1" xref="A4.p4.4.m4.1.1.cmml"><mi id="A4.p4.4.m4.1.1.2" xref="A4.p4.4.m4.1.1.2.cmml">a</mi><mo id="A4.p4.4.m4.1.1.1" xref="A4.p4.4.m4.1.1.1.cmml">&gt;</mo><mn id="A4.p4.4.m4.1.1.3" xref="A4.p4.4.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.4.m4.1b"><apply id="A4.p4.4.m4.1.1.cmml" xref="A4.p4.4.m4.1.1"><gt id="A4.p4.4.m4.1.1.1.cmml" xref="A4.p4.4.m4.1.1.1"></gt><ci id="A4.p4.4.m4.1.1.2.cmml" xref="A4.p4.4.m4.1.1.2">ğ‘</ci><cn type="integer" id="A4.p4.4.m4.1.1.3.cmml" xref="A4.p4.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.4.m4.1c">a&gt;0</annotation></semantics></math>. Conversely, if the loss of unselected tokens <math id="A4.p4.5.m5.1" class="ltx_Math" alttext="\mathcal{L}_{us}" display="inline"><semantics id="A4.p4.5.m5.1a"><msub id="A4.p4.5.m5.1.1" xref="A4.p4.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.p4.5.m5.1.1.2" xref="A4.p4.5.m5.1.1.2.cmml">â„’</mi><mrow id="A4.p4.5.m5.1.1.3" xref="A4.p4.5.m5.1.1.3.cmml"><mi id="A4.p4.5.m5.1.1.3.2" xref="A4.p4.5.m5.1.1.3.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="A4.p4.5.m5.1.1.3.1" xref="A4.p4.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="A4.p4.5.m5.1.1.3.3" xref="A4.p4.5.m5.1.1.3.3.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.p4.5.m5.1b"><apply id="A4.p4.5.m5.1.1.cmml" xref="A4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="A4.p4.5.m5.1.1.1.cmml" xref="A4.p4.5.m5.1.1">subscript</csymbol><ci id="A4.p4.5.m5.1.1.2.cmml" xref="A4.p4.5.m5.1.1.2">â„’</ci><apply id="A4.p4.5.m5.1.1.3.cmml" xref="A4.p4.5.m5.1.1.3"><times id="A4.p4.5.m5.1.1.3.1.cmml" xref="A4.p4.5.m5.1.1.3.1"></times><ci id="A4.p4.5.m5.1.1.3.2.cmml" xref="A4.p4.5.m5.1.1.3.2">ğ‘¢</ci><ci id="A4.p4.5.m5.1.1.3.3.cmml" xref="A4.p4.5.m5.1.1.3.3">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.5.m5.1c">\mathcal{L}_{us}</annotation></semantics></math> is used for fitting, then <math id="A4.p4.6.m6.1" class="ltx_Math" alttext="a<0" display="inline"><semantics id="A4.p4.6.m6.1a"><mrow id="A4.p4.6.m6.1.1" xref="A4.p4.6.m6.1.1.cmml"><mi id="A4.p4.6.m6.1.1.2" xref="A4.p4.6.m6.1.1.2.cmml">a</mi><mo id="A4.p4.6.m6.1.1.1" xref="A4.p4.6.m6.1.1.1.cmml">&lt;</mo><mn id="A4.p4.6.m6.1.1.3" xref="A4.p4.6.m6.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.6.m6.1b"><apply id="A4.p4.6.m6.1.1.cmml" xref="A4.p4.6.m6.1.1"><lt id="A4.p4.6.m6.1.1.1.cmml" xref="A4.p4.6.m6.1.1.1"></lt><ci id="A4.p4.6.m6.1.1.2.cmml" xref="A4.p4.6.m6.1.1.2">ğ‘</ci><cn type="integer" id="A4.p4.6.m6.1.1.3.cmml" xref="A4.p4.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.6.m6.1c">a&lt;0</annotation></semantics></math>. Therefore, we believe that training the model on selected tokens can effectively improve its performance on downstream tasks, while unselected tokens may have a detrimental effect on the modelâ€™s performance in downstream tasks.</p>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Examples of Tokens Selected by SLM</h2>

<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Token Selected Examples</h3>

<figure id="A5.F12" class="ltx_figure"><img src="/html/2404.07965/assets/x12.png" id="A5.F12.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="617" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F12.4.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="A5.F12.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Specific examples of selecting tokens during the ToT training process of the <span id="A5.F12.5.2.1" class="ltx_text ltx_font_smallcaps">Rho-1</span>.<span id="A5.F12.5.2.2" class="ltx_text ltx_font_medium"> The tokens marked in <span id="A5.F12.5.2.2.1" class="ltx_text" style="color:#1E90FF;">blue</span> represent the actual tokens trained during the ToT training process, while the remaining black tokens are not trained during the ToT training process.</span></span></figcaption>
</figure>
<div id="A5.SS1.p1" class="ltx_para">
<p id="A5.SS1.p1.1" class="ltx_p">In <a href="#A5.F12" title="Figure 12 â€£ E.1 Token Selected Examples â€£ Appendix E Examples of Tokens Selected by SLM â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;12</span></a>, we present several examples of tokens selected by the SLM method, with content marked in <span id="A5.SS1.p1.1.1" class="ltx_text" style="color:#1E90FF;">blue</span> indicating the tokens actually chosen during the pretraining process.</p>
</div>
</section>
<section id="A5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Dynamic Token Selected</h3>

<figure id="A5.F13" class="ltx_figure"><img src="/html/2404.07965/assets/x13.png" id="A5.F13.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="706" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F13.7.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="A5.F13.8.2" class="ltx_text ltx_font_bold" style="font-size:90%;">An example of dynamic token selection changes during the training process<span id="A5.F13.8.2.1" class="ltx_text ltx_font_medium">, which illustrated with five different score levels represented by <span id="A5.F13.8.2.1.1" class="ltx_text" style="color:#0000FF;">deep blue</span>, <span id="A5.F13.8.2.1.2" class="ltx_text" style="color:#1E90FF;">light blue</span>, black, <span id="A5.F13.8.2.1.3" class="ltx_text" style="color:#FFB496;">light orange</span>, and <span id="A5.F13.8.2.1.4" class="ltx_text" style="color:#FF6400;">dark orange</span>. The bluer the color indicates a higher tendency for the token to be selected, while the more orange the color suggests a lower tendency for the token to be selected.</span></span></figcaption>
</figure>
<div id="A5.SS2.p1" class="ltx_para">
<p id="A5.SS2.p1.1" class="ltx_p">In <a href="#A5.F13" title="Figure 13 â€£ E.2 Dynamic Token Selected â€£ Appendix E Examples of Tokens Selected by SLM â€£ Rho-1: Not All Tokens Are What You Need" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure&nbsp;13</span></a>, we display the dynamic changes in token selection tendencies throughout the SLM training process. We chose four checkpoints during the training process (0%, 33%, 66%, and 100%) to analyze the current tendencies in token selection. The preferences for token selection are indicated by different colors, ranging from high to low preference, typically represented as <span id="A5.SS2.p1.1.1" class="ltx_text" style="color:#0000FF;">deep blue</span>, <span id="A5.SS2.p1.1.2" class="ltx_text" style="color:#1E90FF;">blue</span>, black, <span id="A5.SS2.p1.1.3" class="ltx_text" style="color:#FFB496;">orange</span>, and <span id="A5.SS2.p1.1.4" class="ltx_text" style="color:#FF6400;">dark orange</span>, respectively.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.07964" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.07965" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2404.07965">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.07965" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.07967" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 19:41:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Rho-1: Not All Tokens Are What You Need</title>
<!--Generated on Thu Apr 11 15:02:20 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.07965v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2404.07965v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2404.07965v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2404.07965v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S1" title="1 Introduction ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2" title="2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Selective Language Modeling</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2.SS1" title="2.1 Not All Tokens Are Equal: Training Dynamics of Token Loss ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Not All Tokens Are Equal: Training Dynamics of Token Loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2.SS2" title="2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Selective Language Modeling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2.SS2.SSS0.Px1" title="Overview ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2.SS2.SSS0.Px2" title="Reference Modeling ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Reference Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S2.SS2.SSS0.Px3" title="Selective Pretraining ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Selective Pretraining</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3" title="3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1" title="3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1.SSS0.Px1" title="Reference Model Training ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Reference Model Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1.SSS0.Px2" title="Pretraining Corpus ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Pretraining Corpus</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1.SSS0.Px3" title="Pretraining Setting ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Pretraining Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1.SSS0.Px4" title="Baseline Setting ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Baseline Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS1.SSS0.Px5" title="Evaluation Setup ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Evaluation Setup</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS2" title="3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Math Pre-training Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS2.SSS0.Px1" title="Few-shot CoT Reasoning Results ‣ 3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Few-shot CoT Reasoning Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS2.SSS0.Px2" title="Tool-Integrated Reasoning Results ‣ 3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Tool-Integrated Reasoning Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS3" title="3.3 General Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>General Pre-training Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS4" title="3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS4.SSS0.Px1" title="Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Selected Token Loss Aligns Better with Downstream Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS4.SSS0.Px2" title="What Tokens are Selected with SLM? ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">What Tokens are Selected with SLM?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS4.SSS0.Px3" title="Effect of Token Select Ratio ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Effect of Token Select Ratio</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S3.SS4.SSS0.Px4" title="Weak-to-Strong Generization ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Weak-to-Strong Generization</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S4" title="4 Related Works ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S4.SS0.SSS0.Px1" title="Pretraining Data Optimization ‣ 4 Related Works ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Pretraining Data Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S4.SS0.SSS0.Px2" title="Data Selection ‣ 4 Related Works ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Data Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S4.SS0.SSS0.Px3" title="Language Model Training Dynamics ‣ 4 Related Works ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Language Model Training Dynamics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S4.SS0.SSS0.Px4" title="Scaling Laws ‣ 4 Related Works ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Scaling Laws</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5" title="5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5.SS0.SSS0.Px1" title="Generalization ‣ 5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Generalization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5.SS0.SSS0.Px2" title="Scalability ‣ 5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Scalability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5.SS0.SSS0.Px3" title="Is training a reference model necessary? ‣ 5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Is training a reference model necessary?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5.SS0.SSS0.Px4" title="How to improve upon SLM? ‣ 5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">How to improve upon SLM?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#S5.SS0.SSS0.Px5" title="Expanding the Use of SLM ‣ 5 Discussion and Future Work ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title">Expanding the Use of SLM</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A1" title="Appendix A Author Contributions ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Author Contributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A2" title="Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Analysis and Visualization of Tokens in Pretraining</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A2.SS1" title="B.1 More Details of Four Categories Tokens ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>More Details of Four Categories Tokens</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A2.SS2" title="B.2 Non-Converging Tokens in Pretrainig ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Non-Converging Tokens in Pretrainig</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A3" title="Appendix C Evalution Details ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Evalution Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A3.SS1" title="C.1 Math Evalution ‣ Appendix C Evalution Details ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Math Evalution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A3.SS2" title="C.2 General Evalution ‣ Appendix C Evalution Details ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>General Evalution</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A4" title="Appendix D Relate the Selected Tokens’ Loss to Downstream Task Performance ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Relate the Selected Tokens’ Loss to Downstream Task Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A5" title="Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Examples of Tokens Selected by SLM</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A5.SS1" title="E.1 Token Selected Examples ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.1 </span>Token Selected Examples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#A5.SS2" title="E.2 Dynamic Token Selected ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.2 </span>Dynamic Token Selected</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: fontawesome</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2404.07965v1 [cs.CL] 11 Apr 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_smallcaps" id="id15.id1">Rho-1</span>: Not All Tokens Are What You Need</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenghao Lin<math alttext="~{}~{}^{\chi\phi}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mrow id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml"><mi id="id1.1.m1.1.1.1.2" xref="id1.1.m1.1.1.1.2.cmml">χ</mi><mo id="id1.1.m1.1.1.1.1" xref="id1.1.m1.1.1.1.1.cmml">⁢</mo><mi id="id1.1.m1.1.1.1.3" xref="id1.1.m1.1.1.1.3.cmml">ϕ</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><apply id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"><times id="id1.1.m1.1.1.1.1.cmml" xref="id1.1.m1.1.1.1.1"></times><ci id="id1.1.m1.1.1.1.2.cmml" xref="id1.1.m1.1.1.1.2">𝜒</ci><ci id="id1.1.m1.1.1.1.3.cmml" xref="id1.1.m1.1.1.1.3">italic-ϕ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">~{}~{}^{\chi\phi}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT italic_χ italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Zhibin Gou<math alttext="{}^{\star\pi\phi}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mrow id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml"><mi id="id2.2.m2.1.1.1.2" xref="id2.2.m2.1.1.1.2.cmml"></mi><mo id="id2.2.m2.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="id2.2.m2.1.1.1.1.cmml">⋆</mo><mrow id="id2.2.m2.1.1.1.3" xref="id2.2.m2.1.1.1.3.cmml"><mi id="id2.2.m2.1.1.1.3.2" xref="id2.2.m2.1.1.1.3.2.cmml">π</mi><mo id="id2.2.m2.1.1.1.3.1" xref="id2.2.m2.1.1.1.3.1.cmml">⁢</mo><mi id="id2.2.m2.1.1.1.3.3" xref="id2.2.m2.1.1.1.3.3.cmml">ϕ</mi></mrow></mrow></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><apply id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1"><ci id="id2.2.m2.1.1.1.1.cmml" xref="id2.2.m2.1.1.1.1">⋆</ci><csymbol cd="latexml" id="id2.2.m2.1.1.1.2.cmml" xref="id2.2.m2.1.1.1.2">absent</csymbol><apply id="id2.2.m2.1.1.1.3.cmml" xref="id2.2.m2.1.1.1.3"><times id="id2.2.m2.1.1.1.3.1.cmml" xref="id2.2.m2.1.1.1.3.1"></times><ci id="id2.2.m2.1.1.1.3.2.cmml" xref="id2.2.m2.1.1.1.3.2">𝜋</ci><ci id="id2.2.m2.1.1.1.3.3.cmml" xref="id2.2.m2.1.1.1.3.3">italic-ϕ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{\star\pi\phi}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT ⋆ italic_π italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Yeyun Gong<math alttext="~{}~{}^{\phi}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mi id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><ci id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">~{}~{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Xiao Liu<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><msup id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mi id="id4.4.m4.1.1a" xref="id4.4.m4.1.1.cmml"></mi><mi id="id4.4.m4.1.1.1" xref="id4.4.m4.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><ci id="id4.4.m4.1.1.1.cmml" xref="id4.4.m4.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Yelong Shen<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id5.5.m5.1"><semantics id="id5.5.m5.1a"><msup id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml"><mi id="id5.5.m5.1.1a" xref="id5.5.m5.1.1.cmml"></mi><mi id="id5.5.m5.1.1.1" xref="id5.5.m5.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><apply id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1"><ci id="id5.5.m5.1.1.1.cmml" xref="id5.5.m5.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id5.5.m5.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id11.11.6">
Ruochen Xu<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id6.6.1.m1.1"><semantics id="id6.6.1.m1.1a"><msup id="id6.6.1.m1.1.1" xref="id6.6.1.m1.1.1.cmml"><mi id="id6.6.1.m1.1.1a" xref="id6.6.1.m1.1.1.cmml"></mi><mi id="id6.6.1.m1.1.1.1" xref="id6.6.1.m1.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id6.6.1.m1.1b"><apply id="id6.6.1.m1.1.1.cmml" xref="id6.6.1.m1.1.1"><ci id="id6.6.1.m1.1.1.1.cmml" xref="id6.6.1.m1.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.1.m1.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id6.6.1.m1.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Chen Lin<math alttext="{}^{\diamond\chi}" class="ltx_Math" display="inline" id="id7.7.2.m2.1"><semantics id="id7.7.2.m2.1a"><msup id="id7.7.2.m2.1.1" xref="id7.7.2.m2.1.1.cmml"><mi id="id7.7.2.m2.1.1a" xref="id7.7.2.m2.1.1.cmml"></mi><mrow id="id7.7.2.m2.1.1.1" xref="id7.7.2.m2.1.1.1.cmml"><mo id="id7.7.2.m2.1.1.1a" mathvariant="normal" rspace="0em" xref="id7.7.2.m2.1.1.1.cmml">⋄</mo><mi id="id7.7.2.m2.1.1.1.2" xref="id7.7.2.m2.1.1.1.2.cmml">χ</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id7.7.2.m2.1b"><apply id="id7.7.2.m2.1.1.cmml" xref="id7.7.2.m2.1.1"><apply id="id7.7.2.m2.1.1.1.cmml" xref="id7.7.2.m2.1.1.1"><ci id="id7.7.2.m2.1.1.1.1.cmml" xref="id7.7.2.m2.1.1.1">normal-⋄</ci><ci id="id7.7.2.m2.1.1.1.2.cmml" xref="id7.7.2.m2.1.1.1.2">𝜒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.2.m2.1c">{}^{\diamond\chi}</annotation><annotation encoding="application/x-llamapun" id="id7.7.2.m2.1d">start_FLOATSUPERSCRIPT ⋄ italic_χ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Yujiu Yang<math alttext="{}^{\diamond\pi}" class="ltx_Math" display="inline" id="id8.8.3.m3.1"><semantics id="id8.8.3.m3.1a"><msup id="id8.8.3.m3.1.1" xref="id8.8.3.m3.1.1.cmml"><mi id="id8.8.3.m3.1.1a" xref="id8.8.3.m3.1.1.cmml"></mi><mrow id="id8.8.3.m3.1.1.1" xref="id8.8.3.m3.1.1.1.cmml"><mo id="id8.8.3.m3.1.1.1a" mathvariant="normal" rspace="0em" xref="id8.8.3.m3.1.1.1.cmml">⋄</mo><mi id="id8.8.3.m3.1.1.1.2" xref="id8.8.3.m3.1.1.1.2.cmml">π</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id8.8.3.m3.1b"><apply id="id8.8.3.m3.1.1.cmml" xref="id8.8.3.m3.1.1"><apply id="id8.8.3.m3.1.1.1.cmml" xref="id8.8.3.m3.1.1.1"><ci id="id8.8.3.m3.1.1.1.1.cmml" xref="id8.8.3.m3.1.1.1">normal-⋄</ci><ci id="id8.8.3.m3.1.1.1.2.cmml" xref="id8.8.3.m3.1.1.1.2">𝜋</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.8.3.m3.1c">{}^{\diamond\pi}</annotation><annotation encoding="application/x-llamapun" id="id8.8.3.m3.1d">start_FLOATSUPERSCRIPT ⋄ italic_π end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Jian Jiao<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id9.9.4.m4.1"><semantics id="id9.9.4.m4.1a"><msup id="id9.9.4.m4.1.1" xref="id9.9.4.m4.1.1.cmml"><mi id="id9.9.4.m4.1.1a" xref="id9.9.4.m4.1.1.cmml"></mi><mi id="id9.9.4.m4.1.1.1" xref="id9.9.4.m4.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id9.9.4.m4.1b"><apply id="id9.9.4.m4.1.1.cmml" xref="id9.9.4.m4.1.1"><ci id="id9.9.4.m4.1.1.1.cmml" xref="id9.9.4.m4.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.9.4.m4.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id9.9.4.m4.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Nan Duan<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id10.10.5.m5.1"><semantics id="id10.10.5.m5.1a"><msup id="id10.10.5.m5.1.1" xref="id10.10.5.m5.1.1.cmml"><mi id="id10.10.5.m5.1.1a" xref="id10.10.5.m5.1.1.cmml"></mi><mi id="id10.10.5.m5.1.1.1" xref="id10.10.5.m5.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id10.10.5.m5.1b"><apply id="id10.10.5.m5.1.1.cmml" xref="id10.10.5.m5.1.1"><ci id="id10.10.5.m5.1.1.1.cmml" xref="id10.10.5.m5.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.10.5.m5.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id10.10.5.m5.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>&nbsp;&nbsp;&nbsp;&nbsp;
Weizhu Chen<math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id11.11.6.m6.1"><semantics id="id11.11.6.m6.1a"><msup id="id11.11.6.m6.1.1" xref="id11.11.6.m6.1.1.cmml"><mi id="id11.11.6.m6.1.1a" xref="id11.11.6.m6.1.1.cmml"></mi><mi id="id11.11.6.m6.1.1.1" xref="id11.11.6.m6.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id11.11.6.m6.1b"><apply id="id11.11.6.m6.1.1.cmml" xref="id11.11.6.m6.1.1"><ci id="id11.11.6.m6.1.1.1.cmml" xref="id11.11.6.m6.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.11.6.m6.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id11.11.6.m6.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<br class="ltx_break"><math alttext="{}^{\chi}" class="ltx_Math" display="inline" id="id12.12.m6.1"><semantics id="id12.12.m6.1a"><msup id="id12.12.m6.1.1" xref="id12.12.m6.1.1.cmml"><mi id="id12.12.m6.1.1a" xref="id12.12.m6.1.1.cmml"></mi><mi id="id12.12.m6.1.1.1" xref="id12.12.m6.1.1.1.cmml">χ</mi></msup><annotation-xml encoding="MathML-Content" id="id12.12.m6.1b"><apply id="id12.12.m6.1.1.cmml" xref="id12.12.m6.1.1"><ci id="id12.12.m6.1.1.1.cmml" xref="id12.12.m6.1.1.1">𝜒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.12.m6.1c">{}^{\chi}</annotation><annotation encoding="application/x-llamapun" id="id12.12.m6.1d">start_FLOATSUPERSCRIPT italic_χ end_FLOATSUPERSCRIPT</annotation></semantics></math>Xiamen University <math alttext="{}^{\pi}" class="ltx_Math" display="inline" id="id13.13.m7.1"><semantics id="id13.13.m7.1a"><msup id="id13.13.m7.1.1" xref="id13.13.m7.1.1.cmml"><mi id="id13.13.m7.1.1a" xref="id13.13.m7.1.1.cmml"></mi><mi id="id13.13.m7.1.1.1" xref="id13.13.m7.1.1.1.cmml">π</mi></msup><annotation-xml encoding="MathML-Content" id="id13.13.m7.1b"><apply id="id13.13.m7.1.1.cmml" xref="id13.13.m7.1.1"><ci id="id13.13.m7.1.1.1.cmml" xref="id13.13.m7.1.1.1">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id13.13.m7.1c">{}^{\pi}</annotation><annotation encoding="application/x-llamapun" id="id13.13.m7.1d">start_FLOATSUPERSCRIPT italic_π end_FLOATSUPERSCRIPT</annotation></semantics></math>Tsinghua University <math alttext="{}^{\phi}" class="ltx_Math" display="inline" id="id14.14.m8.1"><semantics id="id14.14.m8.1a"><msup id="id14.14.m8.1.1" xref="id14.14.m8.1.1.cmml"><mi id="id14.14.m8.1.1a" xref="id14.14.m8.1.1.cmml"></mi><mi id="id14.14.m8.1.1.1" xref="id14.14.m8.1.1.1.cmml">ϕ</mi></msup><annotation-xml encoding="MathML-Content" id="id14.14.m8.1b"><apply id="id14.14.m8.1.1.cmml" xref="id14.14.m8.1.1"><ci id="id14.14.m8.1.1.1.cmml" xref="id14.14.m8.1.1.1">italic-ϕ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id14.14.m8.1c">{}^{\phi}</annotation><annotation encoding="application/x-llamapun" id="id14.14.m8.1d">start_FLOATSUPERSCRIPT italic_ϕ end_FLOATSUPERSCRIPT</annotation></semantics></math>Microsoft 
<br class="ltx_break"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aka.ms/rho" title="">https://aka.ms/rho</a>
</span><span class="ltx_author_notes">Equal contribution. See author contributions for details. Work done during their internships at Microsoft Research Asia. 🖂:&nbsp;<span class="ltx_text ltx_font_typewriter" id="id16.15.id1">zhenghaolin@stu.xmu.edu.cn</span>;&nbsp;&nbsp;<span class="ltx_text ltx_font_typewriter" id="id17.16.id2">zebgou@gmail.com</span>Correspondence authors.</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id18.id1">Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens.
Challenging this norm, we posit that <span class="ltx_text ltx_font_italic" id="id18.id1.1">“Not all tokens in a corpus are equally important for language model training”</span>.
Our initial analysis delves into token-level training dynamics of language model, revealing distinct loss patterns for different tokens.
Leveraging these insights, we introduce a new language model called <span class="ltx_text ltx_font_smallcaps" id="id18.id1.2">Rho-1</span>. Unlike traditional LMs that learn to predict every next token in a corpus, <span class="ltx_text ltx_font_smallcaps" id="id18.id1.3">Rho-1</span> employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution.
This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher excess loss.
When continual pretraining on 15B OpenWebMath corpus, <span class="ltx_text ltx_font_smallcaps" id="id18.id1.4">Rho-1</span> yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks.
After fine-tuning, <span class="ltx_text ltx_font_smallcaps" id="id18.id1.5">Rho-1</span>-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively — matching DeepSeekMath with only 3% of the pretraining tokens.
Furthermore, when pretraining on 80B general tokens, <span class="ltx_text ltx_font_smallcaps" id="id18.id1.6">Rho-1</span> achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="338" id="S0.F1.g1" src="x1.png" width="789">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S0.F1.4.2" style="font-size:90%;">
We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. <span class="ltx_text ltx_font_smallcaps" id="S0.F1.4.2.1">Rho-1</span> is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling.
SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5-10x faster.
</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S0.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="320" id="S0.F2.g1" src="x2.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F2.5.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S0.F2.6.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S0.F2.6.2.1">Upper:</span> Even an extensively filtered pretraining corpus contains token-level noise.
<span class="ltx_text ltx_font_bold" id="S0.F2.6.2.2">Left:</span> Previous Causal Language Modeling (CLM) trains on all tokens.
<span class="ltx_text ltx_font_bold" id="S0.F2.6.2.3">Right:</span> Our proposed Selective Language Modeling (SLM) selectively applies loss on those useful and clean tokens.
</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Scaling up model parameters and dataset size has consistently elevated the next-token prediction accuracy in large language models, yielding significant advancements in artificial intelligence <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Brown et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">OpenAI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib3" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Team et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib4" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
However, training on all available data is not always optimal or feasible. As a result, the practice of data filtering has become crucial, using various heuristics and classifiers <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Brown et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Wenzek et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> to select training documents.
These techniques significantly improve data quality and boost model performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, despite thorough document-level filtering, high-quality datasets still contain many noisy tokens that can negatively affect training, as illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S0.F2" title="Figure 2 ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;2</span></a> (Upper).
Removing such tokens might alter the text’s meaning, while overly strict filtering could exclude useful data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Welbl et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Muennighoff et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and lead to biases <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dodge et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Longpre et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib9" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
Furthermore, research indicates that the distribution of web data does not inherently align with the ideal distribution for downstream applications&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tay et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Wettig et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib11" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
For example, common corpus at the token level may include undesirable content like hallucinations or highly ambiguous tokens that are hard to predict.
Applying the same loss to all tokens can result in wasted computation on non-beneficial tokens, possibly limiting LLM’s potential to merely mediocre intelligence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining.
In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.SS1" title="2.1 Not All Tokens Are Equal: Training Dynamics of Token Loss ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.1</span></a>, we evaluated the model’s token perplexity at different checkpoints and categorized tokens into different types.
Our findings reveal that significant loss reduction is limited to a select group of tokens during training.
Many tokens are “easy tokens” that are already learned, and some are “hard tokens” that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Based on these analyses, we introduce <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">Rho-1</span> models trained with a novel Selective Language Modeling (SLM) objective.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S0.F2" title="Figure 2 ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;2</span></a> (Right), this approach inputs the full sequence into the model and selectively removes the loss of undesired tokens.
The detailed pipeline is depicted in&nbsp;<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.F4" title="Figure 4 ‣ Reference Modeling ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;4</span></a>:
First, SLM trains a reference language model on high-quality corpora. This model establishes utility metrics to score tokens according to the desired distribution, naturally filtering out unclean and irrelevant tokens.
Second, SLM uses the reference model to score each token in a corpus using its loss (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.SS2.SSS0.Px2" title="Reference Modeling ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.2</span></a>).
Finally, we train a language model only on those tokens that exhibit a high excess loss between the reference and the training model, selectively learning the tokens that best benefit downstream applications (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.SS2" title="2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.2</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We show through comprehensive experiments that SLM significantly enhances token efficiency during pretraining and improves performance on downstream tasks. Furthermore, our findings indicate that SLM effectively identifies tokens relevant to the target distribution, resulting in improved perplexity scores on benchmarks for models trained with the selected tokens.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.SS2" title="3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§3.2</span></a> shows the effectiveness of SLM on math continual pretraining: both 1B and 7B <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.1">Rho-1</span> outperform CLM-trained baselines by over 16% on the GSM8k and MATH datasets. SLM reaches baseline accuracy up to 10x faster, as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S0.F1" title="Figure 1 ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;1</span></a>.
Remarkably, <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.2">Rho-1</span>-7B matches the state-of-the-art performance of DeepSeekMath-7B using only 15B tokens, compared to the 500B tokens required by DeepSeekMath.
Upon fine-tuning, <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.3">Rho-1</span>-1B and 7B achieve 40.6% and 51.8% on MATH, respectively.
Notably, <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.4">Rho-1</span>-1B is the first 1B LM to exceed 40% accuracy, nearing the early GPT-4’s CoT performance of 42.5%.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.SS3" title="3.3 General Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§3.3</span></a> confirms the efficacy of SLM in general pretraining:
Training Tinyllama-1B on 80B tokens with SLM improves 6.8% on average across 15 benchmarks, with gains over 10% in code and math tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="239" id="S1.F3.g1" src="x3.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F3.4.2" style="font-size:90%;">The loss of four categories of tokens during pretraining.<span class="ltx_text ltx_font_medium" id="S1.F3.4.2.1"> (a) shows the loss of H→H, L→H, H→L, and L→L tokens during pretraining. (b) and (c) show three cases of fluctuating tokens’ loss in L→L and H→H during pretraining, respectively.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Selective Language Modeling</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Not All Tokens Are Equal: Training Dynamics of Token Loss</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Our investigation begins with a critical look at how individual tokens’ losses evolve during standard pre-training. We continue pre-training Tinyllama-1B with 15B tokens from OpenWebMath, saving checkpoints after every 1B tokens. We then evaluate token-level loss at these intervals using the validation set of approximately 320,000 tokens.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;3</span></a>(a) reveals a striking pattern: tokens fall into four categories based on their loss trajectory—persistent high loss (H→H), increasing loss (L→H), decreasing loss (H→L), and consistent low loss (L→L). For further details on these categories, see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A2.SS1" title="B.1 More Details of Four Categories Tokens ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§B.1</span></a>.
Our analysis uncovers that a mere 26% of tokens show a notable loss reduction (H→L), while the majority (51%) remain in the L→L category, indicating they have already been learned.
Interestingly, 11% of the tokens are persistently challenging (H→H), likely due to high aleatoric uncertainty <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hüllermeier and Waegeman</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib12" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.
Additionally, 12% of tokens experience an unexpected loss increase (L→H) during training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Our second observation is that a significant number of token losses exhibit persistent fluctuations, and resist convergence. The loss of many L→L and H→H tokens, as depicted in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;3</span></a> (b) and (c), show high variance during training.
In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A2.SS2" title="B.2 Non-Converging Tokens in Pretrainig ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§B.2</span></a>, we visualize and analyze the content of these tokens and find that many of them are noisy, which is consistent with our hypothesis.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Consequently, we learn that the loss associated with each token during training does not decrease smoothly like the overall loss; instead, there is a complex training dynamic among different tokens.
If we can select the appropriate tokens for the model to focus on during training, we may be able to stabilize the trajectory of the model’s training and enhance its efficiency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Selective Language Modeling</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Overview</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">Inspired by the practice of reference model in document-level filtering, we propose a simple pipeline of token-level data selection, termed “Selective Language Modeling (SLM)”.
Our method comprises three steps, as depicted in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.F4" title="Figure 4 ‣ Reference Modeling ‣ 2.2 Selective Language Modeling ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;4</span></a>.
We begin by training a reference model on a curated, high-quality dataset. This model then assesses the loss of each token within the pretraining corpus.
In the final phase, we train the language model selectively, focusing on tokens with high excess loss between the training and reference model.
The intuition is that tokens with high excess loss are more learnable and better aligned with the desired distribution, naturally excluding tokens that are either irrelevant or of low quality.
Below, we provide a detailed description of each step.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Reference Modeling</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.2">We begin by curating a high-quality dataset that reflects the desired data distribution.
We train a reference model (RM) using standard cross-entropy loss on the curated data. The resulting RM is then used to assess the token loss within a larger pretraining corpus.
We compute the reference loss (<math alttext="\mathcal{L}_{\text{ref}}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p1.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3a.cmml">ref</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3a.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3"><mtext id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3">ref</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.1.m1.1c">\mathcal{L}_{\text{ref}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT</annotation></semantics></math>) of a token <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS2.SSS0.Px2.p1.2.m2.1a"><msub id="S2.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.2.m2.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> based on the probability that the RM assigns to this token. The calculation is formalized as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{ref}}(x_{i})=-\log P(x_{i}|x{<i})" class="ltx_Math" display="block" id="S2.E1.m1.2"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.3" xref="S2.E1.m1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.3.2.cmml">ℒ</mi><mtext id="S2.E1.m1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.3.3a.cmml">ref</mtext></msub><mo id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">=</mo><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><mo id="S2.E1.m1.2.2.2a" rspace="0.167em" xref="S2.E1.m1.2.2.2.cmml">−</mo><mrow id="S2.E1.m1.2.2.2.1" xref="S2.E1.m1.2.2.2.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.3" xref="S2.E1.m1.2.2.2.1.3.cmml"><mi id="S2.E1.m1.2.2.2.1.3.1" xref="S2.E1.m1.2.2.2.1.3.1.cmml">log</mi><mo id="S2.E1.m1.2.2.2.1.3a" lspace="0.167em" xref="S2.E1.m1.2.2.2.1.3.cmml">⁡</mo><mi id="S2.E1.m1.2.2.2.1.3.2" xref="S2.E1.m1.2.2.2.1.3.2.cmml">P</mi></mrow><mo id="S2.E1.m1.2.2.2.1.2" xref="S2.E1.m1.2.2.2.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.2.2.2.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mo id="S2.E1.m1.2.2.2.1.1.1.2" stretchy="false" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml"><mrow id="S2.E1.m1.2.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.cmml"><msub id="S2.E1.m1.2.2.2.1.1.1.1.2.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml"><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.2.2" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.2.cmml">x</mi><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.2.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.3.cmml">i</mi></msub><mo fence="false" id="S2.E1.m1.2.2.2.1.1.1.1.2.1" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1.cmml">|</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.2.3" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml">x</mi></mrow><mo id="S2.E1.m1.2.2.2.1.1.1.1.1" xref="S2.E1.m1.2.2.2.1.1.1.1.1.cmml">&lt;</mo><mi id="S2.E1.m1.2.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.2.1.1.1.1.3.cmml">i</mi></mrow><mo id="S2.E1.m1.2.2.2.1.1.1.3" stretchy="false" xref="S2.E1.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3"></eq><apply id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><times id="S2.E1.m1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.2"></times><apply id="S2.E1.m1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.3.2">ℒ</ci><ci id="S2.E1.m1.1.1.1.3.3a.cmml" xref="S2.E1.m1.1.1.1.3.3"><mtext id="S2.E1.m1.1.1.1.3.3.cmml" mathsize="70%" xref="S2.E1.m1.1.1.1.3.3">ref</mtext></ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><minus id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2"></minus><apply id="S2.E1.m1.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1"><times id="S2.E1.m1.2.2.2.1.2.cmml" xref="S2.E1.m1.2.2.2.1.2"></times><apply id="S2.E1.m1.2.2.2.1.3.cmml" xref="S2.E1.m1.2.2.2.1.3"><log id="S2.E1.m1.2.2.2.1.3.1.cmml" xref="S2.E1.m1.2.2.2.1.3.1"></log><ci id="S2.E1.m1.2.2.2.1.3.2.cmml" xref="S2.E1.m1.2.2.2.1.3.2">𝑃</ci></apply><apply id="S2.E1.m1.2.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1"><lt id="S2.E1.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.1"></lt><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2"><csymbol cd="latexml" id="S2.E1.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.1">conditional</csymbol><apply id="S2.E1.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.2">𝑥</ci><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.2.3">𝑖</ci></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.2.3">𝑥</ci></apply><ci id="S2.E1.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.2.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\mathcal{L}_{\text{ref}}(x_{i})=-\log P(x_{i}|x{&lt;i})</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.2d">caligraphic_L start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = - roman_log italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x &lt; italic_i )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p3.1">By evaluating <math alttext="\mathcal{L}_{\text{ref}}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p3.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p3.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p3.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3a.cmml">ref</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3a.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3"><mtext id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3.cmml" mathsize="70%" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.3">ref</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.1.m1.1c">\mathcal{L}_{\text{ref}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT</annotation></semantics></math> for each token, we establish the reference loss for selective pretraining, allowing us to focus on the most influential tokens in language modeling.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="146" id="S2.F4.g1" src="x4.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S2.F4.4.2" style="font-size:90%;">The pipeline of Selective Language Modeling (SLM).<span class="ltx_text ltx_font_medium" id="S2.F4.4.2.1">
SLM optimizes language model performance by concentrating on valuable, clean tokens during pre-training.
It involves three steps:
(Step 1) Initially, train a reference model on high-quality data.
(Step 2) Then, score each token’s loss in a corpus using the reference model.
(Step 3) Finally, train the language model selectively on tokens that show higher excess loss compared to the reference loss.
</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Selective Pretraining</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p1.1">Note that causal language modeling (CLM) employs the cross-entropy loss:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{CLM}}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}\log P(x_{i}|x_{<i}%
;\theta)" class="ltx_Math" display="block" id="S2.E2.m1.3"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml"><mrow id="S2.E2.m1.3.3.3" xref="S2.E2.m1.3.3.3.cmml"><msub id="S2.E2.m1.3.3.3.2" xref="S2.E2.m1.3.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.3.3.3.2.2" xref="S2.E2.m1.3.3.3.2.2.cmml">ℒ</mi><mtext id="S2.E2.m1.3.3.3.2.3" xref="S2.E2.m1.3.3.3.2.3a.cmml">CLM</mtext></msub><mo id="S2.E2.m1.3.3.3.1" xref="S2.E2.m1.3.3.3.1.cmml">⁢</mo><mrow id="S2.E2.m1.3.3.3.3.2" xref="S2.E2.m1.3.3.3.cmml"><mo id="S2.E2.m1.3.3.3.3.2.1" stretchy="false" xref="S2.E2.m1.3.3.3.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">θ</mi><mo id="S2.E2.m1.3.3.3.3.2.2" stretchy="false" xref="S2.E2.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.3.3.2" xref="S2.E2.m1.3.3.2.cmml">=</mo><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.cmml"><mo id="S2.E2.m1.3.3.1a" xref="S2.E2.m1.3.3.1.cmml">−</mo><mrow id="S2.E2.m1.3.3.1.1" xref="S2.E2.m1.3.3.1.1.cmml"><mfrac id="S2.E2.m1.3.3.1.1.3" xref="S2.E2.m1.3.3.1.1.3.cmml"><mn id="S2.E2.m1.3.3.1.1.3.2" xref="S2.E2.m1.3.3.1.1.3.2.cmml">1</mn><mi id="S2.E2.m1.3.3.1.1.3.3" xref="S2.E2.m1.3.3.1.1.3.3.cmml">N</mi></mfrac><mo id="S2.E2.m1.3.3.1.1.2" xref="S2.E2.m1.3.3.1.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.1.cmml"><munderover id="S2.E2.m1.3.3.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.2.cmml"><mo id="S2.E2.m1.3.3.1.1.1.2.2.2" movablelimits="false" xref="S2.E2.m1.3.3.1.1.1.2.2.2.cmml">∑</mo><mrow id="S2.E2.m1.3.3.1.1.1.2.2.3" xref="S2.E2.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.2.2.3.2" xref="S2.E2.m1.3.3.1.1.1.2.2.3.2.cmml">i</mi><mo id="S2.E2.m1.3.3.1.1.1.2.2.3.1" xref="S2.E2.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E2.m1.3.3.1.1.1.2.2.3.3" xref="S2.E2.m1.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m1.3.3.1.1.1.2.3" xref="S2.E2.m1.3.3.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S2.E2.m1.3.3.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><mrow id="S2.E2.m1.3.3.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.3.1" xref="S2.E2.m1.3.3.1.1.1.1.3.1.cmml">log</mi><mo id="S2.E2.m1.3.3.1.1.1.1.3a" lspace="0.167em" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml">⁡</mo><mi id="S2.E2.m1.3.3.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.3.2.cmml">P</mi></mrow><mo id="S2.E2.m1.3.3.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">x</mi><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">θ</mi></mrow></mrow><mo id="S2.E2.m1.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3"><eq id="S2.E2.m1.3.3.2.cmml" xref="S2.E2.m1.3.3.2"></eq><apply id="S2.E2.m1.3.3.3.cmml" xref="S2.E2.m1.3.3.3"><times id="S2.E2.m1.3.3.3.1.cmml" xref="S2.E2.m1.3.3.3.1"></times><apply id="S2.E2.m1.3.3.3.2.cmml" xref="S2.E2.m1.3.3.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.2.1.cmml" xref="S2.E2.m1.3.3.3.2">subscript</csymbol><ci id="S2.E2.m1.3.3.3.2.2.cmml" xref="S2.E2.m1.3.3.3.2.2">ℒ</ci><ci id="S2.E2.m1.3.3.3.2.3a.cmml" xref="S2.E2.m1.3.3.3.2.3"><mtext id="S2.E2.m1.3.3.3.2.3.cmml" mathsize="70%" xref="S2.E2.m1.3.3.3.2.3">CLM</mtext></ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝜃</ci></apply><apply id="S2.E2.m1.3.3.1.cmml" xref="S2.E2.m1.3.3.1"><minus id="S2.E2.m1.3.3.1.2.cmml" xref="S2.E2.m1.3.3.1"></minus><apply id="S2.E2.m1.3.3.1.1.cmml" xref="S2.E2.m1.3.3.1.1"><times id="S2.E2.m1.3.3.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.3"><divide id="S2.E2.m1.3.3.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.3"></divide><cn id="S2.E2.m1.3.3.1.1.3.2.cmml" type="integer" xref="S2.E2.m1.3.3.1.1.3.2">1</cn><ci id="S2.E2.m1.3.3.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.3.3">𝑁</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1"><apply id="S2.E2.m1.3.3.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S2.E2.m1.3.3.1.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.2.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2">subscript</csymbol><sum id="S2.E2.m1.3.3.1.1.1.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.2"></sum><apply id="S2.E2.m1.3.3.1.1.1.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3"><eq id="S2.E2.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S2.E2.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.2.2.3.2">𝑖</ci><cn id="S2.E2.m1.3.3.1.1.1.2.2.3.3.cmml" type="integer" xref="S2.E2.m1.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.3.3.1.1.1.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.2.3">𝑁</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1"><times id="S2.E2.m1.3.3.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2"></times><apply id="S2.E2.m1.3.3.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3"><log id="S2.E2.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.1"></log><ci id="S2.E2.m1.3.3.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.2">𝑃</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.2">𝑥</ci><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.3.3">𝑖</ci></apply><list id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3"><lt id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝜃</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">\mathcal{L}_{\text{CLM}}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}\log P(x_{i}|x_{&lt;i}%
;\theta)</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.3d">caligraphic_L start_POSTSUBSCRIPT CLM end_POSTSUBSCRIPT ( italic_θ ) = - divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_log italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ; italic_θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p3">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p3.10">Here, <math alttext="\mathcal{L_{\text{CLM}}}(\theta)" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.1.m1.1"><semantics id="S2.SS2.SSS0.Px3.p3.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p3.1.m1.1.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml"><msub id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2.cmml">ℒ</mi><mtext id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3a.cmml">CLM</mtext></msub><mo id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml"><mo id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2.1" stretchy="false" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml">(</mo><mi id="S2.SS2.SSS0.Px3.p3.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.1.cmml">θ</mi><mo id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.3.2.2" stretchy="false" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2"><times id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.1"></times><apply id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.2">ℒ</ci><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3a.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3"><mtext id="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3.cmml" mathsize="70%" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.2.2.3">CLM</mtext></ci></apply><ci id="S2.SS2.SSS0.Px3.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.1.m1.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.1.m1.1c">\mathcal{L_{\text{CLM}}}(\theta)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT CLM end_POSTSUBSCRIPT ( italic_θ )</annotation></semantics></math> represents the loss function parameterized by model <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.2.m2.1"><semantics id="S2.SS2.SSS0.Px3.p3.2.m2.1a"><mi id="S2.SS2.SSS0.Px3.p3.2.m2.1.1" xref="S2.SS2.SSS0.Px3.p3.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.2.m2.1b"><ci id="S2.SS2.SSS0.Px3.p3.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.2.m2.1d">italic_θ</annotation></semantics></math>. <math alttext="N" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.3.m3.1"><semantics id="S2.SS2.SSS0.Px3.p3.3.m3.1a"><mi id="S2.SS2.SSS0.Px3.p3.3.m3.1.1" xref="S2.SS2.SSS0.Px3.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.3.m3.1b"><ci id="S2.SS2.SSS0.Px3.p3.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.3.m3.1d">italic_N</annotation></semantics></math> is the length of the sequence, <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.4.m4.1"><semantics id="S2.SS2.SSS0.Px3.p3.4.m4.1a"><msub id="S2.SS2.SSS0.Px3.p3.4.m4.1.1" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.4.m4.1b"><apply id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.4.m4.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.5.m5.1"><semantics id="S2.SS2.SSS0.Px3.p3.5.m5.1a"><mi id="S2.SS2.SSS0.Px3.p3.5.m5.1.1" xref="S2.SS2.SSS0.Px3.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.5.m5.1b"><ci id="S2.SS2.SSS0.Px3.p3.5.m5.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.5.m5.1d">italic_i</annotation></semantics></math>-th token in the sequence, and <math alttext="x_{<i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.6.m6.1"><semantics id="S2.SS2.SSS0.Px3.p3.6.m6.1a"><msub id="S2.SS2.SSS0.Px3.p3.6.m6.1.1" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2.cmml">x</mi><mrow id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2.cmml"></mi><mo id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1.cmml">&lt;</mo><mi id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.6.m6.1b"><apply id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.2">𝑥</ci><apply id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3"><lt id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.1"></lt><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.2">absent</csymbol><ci id="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3.cmml" xref="S2.SS2.SSS0.Px3.p3.6.m6.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.6.m6.1c">x_{&lt;i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.6.m6.1d">italic_x start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents all tokens before the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.7.m7.1"><semantics id="S2.SS2.SSS0.Px3.p3.7.m7.1a"><mi id="S2.SS2.SSS0.Px3.p3.7.m7.1.1" xref="S2.SS2.SSS0.Px3.p3.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.7.m7.1b"><ci id="S2.SS2.SSS0.Px3.p3.7.m7.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.7.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.7.m7.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.7.m7.1d">italic_i</annotation></semantics></math>-th token.
In contrast, Selective Language Modeling (SLM) trains the language model with a focus on tokens that exhibit a high excess loss when compared to the reference model.
The excess loss (<math alttext="\mathcal{L}_{\Delta}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.8.m8.1"><semantics id="S2.SS2.SSS0.Px3.p3.8.m8.1a"><msub id="S2.SS2.SSS0.Px3.p3.8.m8.1.1" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2.cmml">ℒ</mi><mi id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3" mathvariant="normal" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3.cmml">Δ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.8.m8.1b"><apply id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.8.m8.1.1.3">Δ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.8.m8.1c">\mathcal{L}_{\Delta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.8.m8.1d">caligraphic_L start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT</annotation></semantics></math>) for a token <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.9.m9.1"><semantics id="S2.SS2.SSS0.Px3.p3.9.m9.1a"><msub id="S2.SS2.SSS0.Px3.p3.9.m9.1.1" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.9.m9.1b"><apply id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.9.m9.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.9.m9.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is defined as the difference between the current training model loss (<math alttext="\mathcal{L}_{\theta}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p3.10.m10.1"><semantics id="S2.SS2.SSS0.Px3.p3.10.m10.1a"><msub id="S2.SS2.SSS0.Px3.p3.10.m10.1.1" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2.cmml">ℒ</mi><mi id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p3.10.m10.1b"><apply id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.2">ℒ</ci><ci id="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p3.10.m10.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p3.10.m10.1c">\mathcal{L}_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p3.10.m10.1d">caligraphic_L start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>) and the reference loss:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p4">
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\Delta}(x_{i})=\mathcal{L}_{\theta}(x_{i})-\mathcal{L}_{\text{ref%
}}(x_{i})" class="ltx_Math" display="block" id="S2.E3.m1.3"><semantics id="S2.E3.m1.3a"><mrow id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.3" xref="S2.E3.m1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.3.2.cmml">ℒ</mi><mi id="S2.E3.m1.1.1.1.3.3" mathvariant="normal" xref="S2.E3.m1.1.1.1.3.3.cmml">Δ</mi></msub><mo id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mo id="S2.E3.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E3.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.3.3.4" xref="S2.E3.m1.3.3.4.cmml">=</mo><mrow id="S2.E3.m1.3.3.3" xref="S2.E3.m1.3.3.3.cmml"><mrow id="S2.E3.m1.2.2.2.1" xref="S2.E3.m1.2.2.2.1.cmml"><msub id="S2.E3.m1.2.2.2.1.3" xref="S2.E3.m1.2.2.2.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.2.2.2.1.3.2" xref="S2.E3.m1.2.2.2.1.3.2.cmml">ℒ</mi><mi id="S2.E3.m1.2.2.2.1.3.3" xref="S2.E3.m1.2.2.2.1.3.3.cmml">θ</mi></msub><mo id="S2.E3.m1.2.2.2.1.2" xref="S2.E3.m1.2.2.2.1.2.cmml">⁢</mo><mrow id="S2.E3.m1.2.2.2.1.1.1" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml"><mo id="S2.E3.m1.2.2.2.1.1.1.2" stretchy="false" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml">(</mo><msub id="S2.E3.m1.2.2.2.1.1.1.1" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml"><mi id="S2.E3.m1.2.2.2.1.1.1.1.2" xref="S2.E3.m1.2.2.2.1.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.2.2.2.1.1.1.1.3" xref="S2.E3.m1.2.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E3.m1.2.2.2.1.1.1.3" stretchy="false" xref="S2.E3.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.3.3.3.3" xref="S2.E3.m1.3.3.3.3.cmml">−</mo><mrow id="S2.E3.m1.3.3.3.2" xref="S2.E3.m1.3.3.3.2.cmml"><msub id="S2.E3.m1.3.3.3.2.3" xref="S2.E3.m1.3.3.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.3.3.3.2.3.2" xref="S2.E3.m1.3.3.3.2.3.2.cmml">ℒ</mi><mtext id="S2.E3.m1.3.3.3.2.3.3" xref="S2.E3.m1.3.3.3.2.3.3a.cmml">ref</mtext></msub><mo id="S2.E3.m1.3.3.3.2.2" xref="S2.E3.m1.3.3.3.2.2.cmml">⁢</mo><mrow id="S2.E3.m1.3.3.3.2.1.1" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml"><mo id="S2.E3.m1.3.3.3.2.1.1.2" stretchy="false" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml">(</mo><msub id="S2.E3.m1.3.3.3.2.1.1.1" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml"><mi id="S2.E3.m1.3.3.3.2.1.1.1.2" xref="S2.E3.m1.3.3.3.2.1.1.1.2.cmml">x</mi><mi id="S2.E3.m1.3.3.3.2.1.1.1.3" xref="S2.E3.m1.3.3.3.2.1.1.1.3.cmml">i</mi></msub><mo id="S2.E3.m1.3.3.3.2.1.1.3" stretchy="false" xref="S2.E3.m1.3.3.3.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.3b"><apply id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3"><eq id="S2.E3.m1.3.3.4.cmml" xref="S2.E3.m1.3.3.4"></eq><apply id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><times id="S2.E3.m1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.3.2">ℒ</ci><ci id="S2.E3.m1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.3.3">Δ</ci></apply><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E3.m1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E3.m1.3.3.3.cmml" xref="S2.E3.m1.3.3.3"><minus id="S2.E3.m1.3.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3"></minus><apply id="S2.E3.m1.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.1"><times id="S2.E3.m1.2.2.2.1.2.cmml" xref="S2.E3.m1.2.2.2.1.2"></times><apply id="S2.E3.m1.2.2.2.1.3.cmml" xref="S2.E3.m1.2.2.2.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.1.3.1.cmml" xref="S2.E3.m1.2.2.2.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.2.1.3.2.cmml" xref="S2.E3.m1.2.2.2.1.3.2">ℒ</ci><ci id="S2.E3.m1.2.2.2.1.3.3.cmml" xref="S2.E3.m1.2.2.2.1.3.3">𝜃</ci></apply><apply id="S2.E3.m1.2.2.2.1.1.1.1.cmml" xref="S2.E3.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.2.1.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.2.1.1.1.1.2">𝑥</ci><ci id="S2.E3.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.2.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E3.m1.3.3.3.2.cmml" xref="S2.E3.m1.3.3.3.2"><times id="S2.E3.m1.3.3.3.2.2.cmml" xref="S2.E3.m1.3.3.3.2.2"></times><apply id="S2.E3.m1.3.3.3.2.3.cmml" xref="S2.E3.m1.3.3.3.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.2.3.1.cmml" xref="S2.E3.m1.3.3.3.2.3">subscript</csymbol><ci id="S2.E3.m1.3.3.3.2.3.2.cmml" xref="S2.E3.m1.3.3.3.2.3.2">ℒ</ci><ci id="S2.E3.m1.3.3.3.2.3.3a.cmml" xref="S2.E3.m1.3.3.3.2.3.3"><mtext id="S2.E3.m1.3.3.3.2.3.3.cmml" mathsize="70%" xref="S2.E3.m1.3.3.3.2.3.3">ref</mtext></ci></apply><apply id="S2.E3.m1.3.3.3.2.1.1.1.cmml" xref="S2.E3.m1.3.3.3.2.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.2.1.1.1.1.cmml" xref="S2.E3.m1.3.3.3.2.1.1">subscript</csymbol><ci id="S2.E3.m1.3.3.3.2.1.1.1.2.cmml" xref="S2.E3.m1.3.3.3.2.1.1.1.2">𝑥</ci><ci id="S2.E3.m1.3.3.3.2.1.1.1.3.cmml" xref="S2.E3.m1.3.3.3.2.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.3c">\mathcal{L}_{\Delta}(x_{i})=\mathcal{L}_{\theta}(x_{i})-\mathcal{L}_{\text{ref%
}}(x_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.3d">caligraphic_L start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = caligraphic_L start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - caligraphic_L start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p5">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p5.1">We introduce a token selection ratio <math alttext="k\%" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p5.1.m1.1"><semantics id="S2.SS2.SSS0.Px3.p5.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p5.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p5.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p5.1.m1.1.1.2">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p5.1.m1.1c">k\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p5.1.m1.1d">italic_k %</annotation></semantics></math>, which determines the proportion of tokens to be included based on their excess loss.
The cross-entropy loss for the selected tokens is computed as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p6">
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{SLM}}(\theta)=-\frac{1}{N*k\%}\sum_{i=1}^{N}I_{k\%}(x_{i})%
\cdot\log P(x_{i}|x_{<i};\theta)" class="ltx_Math" display="block" id="S2.E4.m1.4"><semantics id="S2.E4.m1.4a"><mrow id="S2.E4.m1.4.4" xref="S2.E4.m1.4.4.cmml"><mrow id="S2.E4.m1.4.4.4" xref="S2.E4.m1.4.4.4.cmml"><msub id="S2.E4.m1.4.4.4.2" xref="S2.E4.m1.4.4.4.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.4.4.4.2.2" xref="S2.E4.m1.4.4.4.2.2.cmml">ℒ</mi><mtext id="S2.E4.m1.4.4.4.2.3" xref="S2.E4.m1.4.4.4.2.3a.cmml">SLM</mtext></msub><mo id="S2.E4.m1.4.4.4.1" xref="S2.E4.m1.4.4.4.1.cmml">⁢</mo><mrow id="S2.E4.m1.4.4.4.3.2" xref="S2.E4.m1.4.4.4.cmml"><mo id="S2.E4.m1.4.4.4.3.2.1" stretchy="false" xref="S2.E4.m1.4.4.4.cmml">(</mo><mi id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">θ</mi><mo id="S2.E4.m1.4.4.4.3.2.2" stretchy="false" xref="S2.E4.m1.4.4.4.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.4.4.3" xref="S2.E4.m1.4.4.3.cmml">=</mo><mrow id="S2.E4.m1.4.4.2" xref="S2.E4.m1.4.4.2.cmml"><mo id="S2.E4.m1.4.4.2a" xref="S2.E4.m1.4.4.2.cmml">−</mo><mrow id="S2.E4.m1.4.4.2.2" xref="S2.E4.m1.4.4.2.2.cmml"><mfrac id="S2.E4.m1.4.4.2.2.4" xref="S2.E4.m1.4.4.2.2.4.cmml"><mn id="S2.E4.m1.4.4.2.2.4.2" xref="S2.E4.m1.4.4.2.2.4.2.cmml">1</mn><mrow id="S2.E4.m1.4.4.2.2.4.3" xref="S2.E4.m1.4.4.2.2.4.3.cmml"><mi id="S2.E4.m1.4.4.2.2.4.3.2" xref="S2.E4.m1.4.4.2.2.4.3.2.cmml">N</mi><mo id="S2.E4.m1.4.4.2.2.4.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E4.m1.4.4.2.2.4.3.1.cmml">*</mo><mrow id="S2.E4.m1.4.4.2.2.4.3.3" xref="S2.E4.m1.4.4.2.2.4.3.3.cmml"><mi id="S2.E4.m1.4.4.2.2.4.3.3.2" xref="S2.E4.m1.4.4.2.2.4.3.3.2.cmml">k</mi><mo id="S2.E4.m1.4.4.2.2.4.3.3.1" xref="S2.E4.m1.4.4.2.2.4.3.3.1.cmml">%</mo></mrow></mrow></mfrac><mo id="S2.E4.m1.4.4.2.2.3" xref="S2.E4.m1.4.4.2.2.3.cmml">⁢</mo><mrow id="S2.E4.m1.4.4.2.2.2" xref="S2.E4.m1.4.4.2.2.2.cmml"><munderover id="S2.E4.m1.4.4.2.2.2.3" xref="S2.E4.m1.4.4.2.2.2.3.cmml"><mo id="S2.E4.m1.4.4.2.2.2.3.2.2" movablelimits="false" xref="S2.E4.m1.4.4.2.2.2.3.2.2.cmml">∑</mo><mrow id="S2.E4.m1.4.4.2.2.2.3.2.3" xref="S2.E4.m1.4.4.2.2.2.3.2.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.3.2.3.2" xref="S2.E4.m1.4.4.2.2.2.3.2.3.2.cmml">i</mi><mo id="S2.E4.m1.4.4.2.2.2.3.2.3.1" xref="S2.E4.m1.4.4.2.2.2.3.2.3.1.cmml">=</mo><mn id="S2.E4.m1.4.4.2.2.2.3.2.3.3" xref="S2.E4.m1.4.4.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S2.E4.m1.4.4.2.2.2.3.3" xref="S2.E4.m1.4.4.2.2.2.3.3.cmml">N</mi></munderover><mrow id="S2.E4.m1.4.4.2.2.2.2" xref="S2.E4.m1.4.4.2.2.2.2.cmml"><mrow id="S2.E4.m1.3.3.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.cmml"><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S2.E4.m1.3.3.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml">I</mi><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2.cmml">k</mi><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1.cmml">%</mo></mrow></msub><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.3.3.1.1.1.1.1.2" rspace="0.222em" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml">⋅</mo><mrow id="S2.E4.m1.3.3.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.3.1" xref="S2.E4.m1.3.3.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.E4.m1.3.3.1.1.1.1.1.3a" lspace="0.167em" xref="S2.E4.m1.3.3.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.E4.m1.3.3.1.1.1.1.1.3.2" xref="S2.E4.m1.3.3.1.1.1.1.1.3.2.cmml">P</mi></mrow></mrow><mo id="S2.E4.m1.4.4.2.2.2.2.3" xref="S2.E4.m1.4.4.2.2.2.2.3.cmml">⁢</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml"><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.2" stretchy="false" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml">(</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml"><msub id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2.cmml">x</mi><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.2.cmml">|</mo><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml"><msub id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2.cmml">x</mi><mrow id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.2" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml">;</mo><mi id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">θ</mi></mrow></mrow><mo id="S2.E4.m1.4.4.2.2.2.2.2.1.3" stretchy="false" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.4b"><apply id="S2.E4.m1.4.4.cmml" xref="S2.E4.m1.4.4"><eq id="S2.E4.m1.4.4.3.cmml" xref="S2.E4.m1.4.4.3"></eq><apply id="S2.E4.m1.4.4.4.cmml" xref="S2.E4.m1.4.4.4"><times id="S2.E4.m1.4.4.4.1.cmml" xref="S2.E4.m1.4.4.4.1"></times><apply id="S2.E4.m1.4.4.4.2.cmml" xref="S2.E4.m1.4.4.4.2"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.4.2.1.cmml" xref="S2.E4.m1.4.4.4.2">subscript</csymbol><ci id="S2.E4.m1.4.4.4.2.2.cmml" xref="S2.E4.m1.4.4.4.2.2">ℒ</ci><ci id="S2.E4.m1.4.4.4.2.3a.cmml" xref="S2.E4.m1.4.4.4.2.3"><mtext id="S2.E4.m1.4.4.4.2.3.cmml" mathsize="70%" xref="S2.E4.m1.4.4.4.2.3">SLM</mtext></ci></apply><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">𝜃</ci></apply><apply id="S2.E4.m1.4.4.2.cmml" xref="S2.E4.m1.4.4.2"><minus id="S2.E4.m1.4.4.2.3.cmml" xref="S2.E4.m1.4.4.2"></minus><apply id="S2.E4.m1.4.4.2.2.cmml" xref="S2.E4.m1.4.4.2.2"><times id="S2.E4.m1.4.4.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.3"></times><apply id="S2.E4.m1.4.4.2.2.4.cmml" xref="S2.E4.m1.4.4.2.2.4"><divide id="S2.E4.m1.4.4.2.2.4.1.cmml" xref="S2.E4.m1.4.4.2.2.4"></divide><cn id="S2.E4.m1.4.4.2.2.4.2.cmml" type="integer" xref="S2.E4.m1.4.4.2.2.4.2">1</cn><apply id="S2.E4.m1.4.4.2.2.4.3.cmml" xref="S2.E4.m1.4.4.2.2.4.3"><times id="S2.E4.m1.4.4.2.2.4.3.1.cmml" xref="S2.E4.m1.4.4.2.2.4.3.1"></times><ci id="S2.E4.m1.4.4.2.2.4.3.2.cmml" xref="S2.E4.m1.4.4.2.2.4.3.2">𝑁</ci><apply id="S2.E4.m1.4.4.2.2.4.3.3.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3"><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.4.3.3.1.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3.1">percent</csymbol><ci id="S2.E4.m1.4.4.2.2.4.3.3.2.cmml" xref="S2.E4.m1.4.4.2.2.4.3.3.2">𝑘</ci></apply></apply></apply><apply id="S2.E4.m1.4.4.2.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2"><apply id="S2.E4.m1.4.4.2.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3">superscript</csymbol><apply id="S2.E4.m1.4.4.2.2.2.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.3.2.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3">subscript</csymbol><sum id="S2.E4.m1.4.4.2.2.2.3.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.2"></sum><apply id="S2.E4.m1.4.4.2.2.2.3.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3"><eq id="S2.E4.m1.4.4.2.2.2.3.2.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3.1"></eq><ci id="S2.E4.m1.4.4.2.2.2.3.2.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.3.2.3.2">𝑖</ci><cn id="S2.E4.m1.4.4.2.2.2.3.2.3.3.cmml" type="integer" xref="S2.E4.m1.4.4.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.4.4.2.2.2.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.3.3">𝑁</ci></apply><apply id="S2.E4.m1.4.4.2.2.2.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2"><times id="S2.E4.m1.4.4.2.2.2.2.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.3"></times><apply id="S2.E4.m1.3.3.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1"><ci id="S2.E4.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.2">⋅</ci><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1"><times id="S2.E4.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.2"></times><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.2">𝐼</ci><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3"><csymbol cd="latexml" id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.1">percent</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.3.3.2">𝑘</ci></apply></apply><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E4.m1.3.3.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3"><log id="S2.E4.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3.1"></log><ci id="S2.E4.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.3.2">𝑃</ci></apply></apply><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1"><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.2">conditional</csymbol><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3">subscript</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.2">𝑥</ci><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.3.3">𝑖</ci></apply><list id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1"><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.2">𝑥</ci><apply id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3"><lt id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.4.4.2.2.2.2.2.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">𝜃</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.4c">\mathcal{L}_{\text{SLM}}(\theta)=-\frac{1}{N*k\%}\sum_{i=1}^{N}I_{k\%}(x_{i})%
\cdot\log P(x_{i}|x_{&lt;i};\theta)</annotation><annotation encoding="application/x-llamapun" id="S2.E4.m1.4d">caligraphic_L start_POSTSUBSCRIPT SLM end_POSTSUBSCRIPT ( italic_θ ) = - divide start_ARG 1 end_ARG start_ARG italic_N * italic_k % end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT italic_k % end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ⋅ roman_log italic_P ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ; italic_θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p7">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p7.3">Here, <math alttext="N*k\%" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p7.1.m1.1"><semantics id="S2.SS2.SSS0.Px3.p7.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p7.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2.cmml">N</mi><mo id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1.cmml">*</mo><mrow id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1"><times id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.1"></times><ci id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.2">𝑁</ci><apply id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.1.m1.1.1.3.2">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.1.m1.1c">N*k\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p7.1.m1.1d">italic_N * italic_k %</annotation></semantics></math> defines the number of tokens that fall within the top <math alttext="k\%" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p7.2.m2.1"><semantics id="S2.SS2.SSS0.Px3.p7.2.m2.1a"><mrow id="S2.SS2.SSS0.Px3.p7.2.m2.1.1" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.2.m2.1b"><apply id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.2.m2.1.1.2">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.2.m2.1c">k\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p7.2.m2.1d">italic_k %</annotation></semantics></math> of excess loss. The indicator function <math alttext="I_{k\%}(x_{i})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p7.3.m3.1"><semantics id="S2.SS2.SSS0.Px3.p7.3.m3.1a"><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.cmml"><msub id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2.cmml">I</mi><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1.cmml">%</mo></mrow></msub><mo id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2.cmml">⁢</mo><mrow id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml"><mo id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.2" stretchy="false" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.3" stretchy="false" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p7.3.m3.1b"><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1"><times id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.2"></times><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.2">𝐼</ci><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.3.3.2">𝑘</ci></apply></apply><apply id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p7.3.m3.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p7.3.m3.1c">I_{k\%}(x_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p7.3.m3.1d">italic_I start_POSTSUBSCRIPT italic_k % end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> is defined as:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p8">
<table class="ltx_equation ltx_eqn_table" id="S2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I_{k\%}(x_{i})=\begin{cases}1&amp;\text{if }x_{i}\text{ is in the top }k\%\text{ %
of }\mathcal{L}_{\Delta}\\
0&amp;\text{otherwise}\end{cases}" class="ltx_Math" display="block" id="S2.E5.m1.5"><semantics id="S2.E5.m1.5a"><mrow id="S2.E5.m1.5.5" xref="S2.E5.m1.5.5.cmml"><mrow id="S2.E5.m1.5.5.1" xref="S2.E5.m1.5.5.1.cmml"><msub id="S2.E5.m1.5.5.1.3" xref="S2.E5.m1.5.5.1.3.cmml"><mi id="S2.E5.m1.5.5.1.3.2" xref="S2.E5.m1.5.5.1.3.2.cmml">I</mi><mrow id="S2.E5.m1.5.5.1.3.3" xref="S2.E5.m1.5.5.1.3.3.cmml"><mi id="S2.E5.m1.5.5.1.3.3.2" xref="S2.E5.m1.5.5.1.3.3.2.cmml">k</mi><mo id="S2.E5.m1.5.5.1.3.3.1" xref="S2.E5.m1.5.5.1.3.3.1.cmml">%</mo></mrow></msub><mo id="S2.E5.m1.5.5.1.2" xref="S2.E5.m1.5.5.1.2.cmml">⁢</mo><mrow id="S2.E5.m1.5.5.1.1.1" xref="S2.E5.m1.5.5.1.1.1.1.cmml"><mo id="S2.E5.m1.5.5.1.1.1.2" stretchy="false" xref="S2.E5.m1.5.5.1.1.1.1.cmml">(</mo><msub id="S2.E5.m1.5.5.1.1.1.1" xref="S2.E5.m1.5.5.1.1.1.1.cmml"><mi id="S2.E5.m1.5.5.1.1.1.1.2" xref="S2.E5.m1.5.5.1.1.1.1.2.cmml">x</mi><mi id="S2.E5.m1.5.5.1.1.1.1.3" xref="S2.E5.m1.5.5.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E5.m1.5.5.1.1.1.3" stretchy="false" xref="S2.E5.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.5.5.2" xref="S2.E5.m1.5.5.2.cmml">=</mo><mrow id="S2.E5.m1.4.4" xref="S2.E5.m1.5.5.3.1.cmml"><mo id="S2.E5.m1.4.4.5" xref="S2.E5.m1.5.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E5.m1.4.4.4" rowspacing="0pt" xref="S2.E5.m1.5.5.3.1.cmml"><mtr id="S2.E5.m1.4.4.4a" xref="S2.E5.m1.5.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4b" xref="S2.E5.m1.5.5.3.1.cmml"><mn id="S2.E5.m1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4c" xref="S2.E5.m1.5.5.3.1.cmml"><mrow id="S2.E5.m1.2.2.2.2.2.1" xref="S2.E5.m1.2.2.2.2.2.1.cmml"><mtext id="S2.E5.m1.2.2.2.2.2.1.2" xref="S2.E5.m1.2.2.2.2.2.1.2a.cmml">if&nbsp;</mtext><mo id="S2.E5.m1.2.2.2.2.2.1.1" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">⁢</mo><msub id="S2.E5.m1.2.2.2.2.2.1.3" xref="S2.E5.m1.2.2.2.2.2.1.3.cmml"><mi id="S2.E5.m1.2.2.2.2.2.1.3.2" xref="S2.E5.m1.2.2.2.2.2.1.3.2.cmml">x</mi><mi id="S2.E5.m1.2.2.2.2.2.1.3.3" xref="S2.E5.m1.2.2.2.2.2.1.3.3.cmml">i</mi></msub><mo id="S2.E5.m1.2.2.2.2.2.1.1a" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">⁢</mo><mtext id="S2.E5.m1.2.2.2.2.2.1.4" xref="S2.E5.m1.2.2.2.2.2.1.4a.cmml">&nbsp;is in the top&nbsp;</mtext><mo id="S2.E5.m1.2.2.2.2.2.1.1b" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">⁢</mo><mrow id="S2.E5.m1.2.2.2.2.2.1.5" xref="S2.E5.m1.2.2.2.2.2.1.5.cmml"><mi id="S2.E5.m1.2.2.2.2.2.1.5.2" xref="S2.E5.m1.2.2.2.2.2.1.5.2.cmml">k</mi><mo id="S2.E5.m1.2.2.2.2.2.1.5.1" xref="S2.E5.m1.2.2.2.2.2.1.5.1.cmml">%</mo></mrow><mo id="S2.E5.m1.2.2.2.2.2.1.1c" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">⁢</mo><mtext id="S2.E5.m1.2.2.2.2.2.1.6" xref="S2.E5.m1.2.2.2.2.2.1.6a.cmml">&nbsp;of&nbsp;</mtext><mo id="S2.E5.m1.2.2.2.2.2.1.1d" xref="S2.E5.m1.2.2.2.2.2.1.1.cmml">⁢</mo><msub id="S2.E5.m1.2.2.2.2.2.1.7" xref="S2.E5.m1.2.2.2.2.2.1.7.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E5.m1.2.2.2.2.2.1.7.2" xref="S2.E5.m1.2.2.2.2.2.1.7.2.cmml">ℒ</mi><mi id="S2.E5.m1.2.2.2.2.2.1.7.3" mathvariant="normal" xref="S2.E5.m1.2.2.2.2.2.1.7.3.cmml">Δ</mi></msub></mrow></mtd></mtr><mtr id="S2.E5.m1.4.4.4d" xref="S2.E5.m1.5.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4e" xref="S2.E5.m1.5.5.3.1.cmml"><mn id="S2.E5.m1.3.3.3.3.1.1" xref="S2.E5.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E5.m1.4.4.4f" xref="S2.E5.m1.5.5.3.1.cmml"><mtext id="S2.E5.m1.4.4.4.4.2.1" xref="S2.E5.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.5b"><apply id="S2.E5.m1.5.5.cmml" xref="S2.E5.m1.5.5"><eq id="S2.E5.m1.5.5.2.cmml" xref="S2.E5.m1.5.5.2"></eq><apply id="S2.E5.m1.5.5.1.cmml" xref="S2.E5.m1.5.5.1"><times id="S2.E5.m1.5.5.1.2.cmml" xref="S2.E5.m1.5.5.1.2"></times><apply id="S2.E5.m1.5.5.1.3.cmml" xref="S2.E5.m1.5.5.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.1.3.1.cmml" xref="S2.E5.m1.5.5.1.3">subscript</csymbol><ci id="S2.E5.m1.5.5.1.3.2.cmml" xref="S2.E5.m1.5.5.1.3.2">𝐼</ci><apply id="S2.E5.m1.5.5.1.3.3.cmml" xref="S2.E5.m1.5.5.1.3.3"><csymbol cd="latexml" id="S2.E5.m1.5.5.1.3.3.1.cmml" xref="S2.E5.m1.5.5.1.3.3.1">percent</csymbol><ci id="S2.E5.m1.5.5.1.3.3.2.cmml" xref="S2.E5.m1.5.5.1.3.3.2">𝑘</ci></apply></apply><apply id="S2.E5.m1.5.5.1.1.1.1.cmml" xref="S2.E5.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.5.5.1.1.1.1.1.cmml" xref="S2.E5.m1.5.5.1.1.1">subscript</csymbol><ci id="S2.E5.m1.5.5.1.1.1.1.2.cmml" xref="S2.E5.m1.5.5.1.1.1.1.2">𝑥</ci><ci id="S2.E5.m1.5.5.1.1.1.1.3.cmml" xref="S2.E5.m1.5.5.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S2.E5.m1.5.5.3.1.cmml" xref="S2.E5.m1.4.4"><csymbol cd="latexml" id="S2.E5.m1.5.5.3.1.1.cmml" xref="S2.E5.m1.4.4.5">cases</csymbol><cn id="S2.E5.m1.1.1.1.1.1.1.cmml" type="integer" xref="S2.E5.m1.1.1.1.1.1.1">1</cn><apply id="S2.E5.m1.2.2.2.2.2.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1"><times id="S2.E5.m1.2.2.2.2.2.1.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.1"></times><ci id="S2.E5.m1.2.2.2.2.2.1.2a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.2"><mtext id="S2.E5.m1.2.2.2.2.2.1.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.2">if&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.2.2.2.1.3.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3">subscript</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.3.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3.2">𝑥</ci><ci id="S2.E5.m1.2.2.2.2.2.1.3.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.3.3">𝑖</ci></apply><ci id="S2.E5.m1.2.2.2.2.2.1.4a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.4"><mtext id="S2.E5.m1.2.2.2.2.2.1.4.cmml" xref="S2.E5.m1.2.2.2.2.2.1.4">&nbsp;is in the top&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.5.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5"><csymbol cd="latexml" id="S2.E5.m1.2.2.2.2.2.1.5.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5.1">percent</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.5.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.5.2">𝑘</ci></apply><ci id="S2.E5.m1.2.2.2.2.2.1.6a.cmml" xref="S2.E5.m1.2.2.2.2.2.1.6"><mtext id="S2.E5.m1.2.2.2.2.2.1.6.cmml" xref="S2.E5.m1.2.2.2.2.2.1.6">&nbsp;of&nbsp;</mtext></ci><apply id="S2.E5.m1.2.2.2.2.2.1.7.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.2.2.2.1.7.1.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7">subscript</csymbol><ci id="S2.E5.m1.2.2.2.2.2.1.7.2.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7.2">ℒ</ci><ci id="S2.E5.m1.2.2.2.2.2.1.7.3.cmml" xref="S2.E5.m1.2.2.2.2.2.1.7.3">Δ</ci></apply></apply><cn id="S2.E5.m1.3.3.3.3.1.1.cmml" type="integer" xref="S2.E5.m1.3.3.3.3.1.1">0</cn><ci id="S2.E5.m1.4.4.4.4.2.1a.cmml" xref="S2.E5.m1.4.4.4.4.2.1"><mtext id="S2.E5.m1.4.4.4.4.2.1.cmml" xref="S2.E5.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.5c">I_{k\%}(x_{i})=\begin{cases}1&amp;\text{if }x_{i}\text{ is in the top }k\%\text{ %
of }\mathcal{L}_{\Delta}\\
0&amp;\text{otherwise}\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S2.E5.m1.5d">italic_I start_POSTSUBSCRIPT italic_k % end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = { start_ROW start_CELL 1 end_CELL start_CELL if italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is in the top italic_k % of caligraphic_L start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL otherwise end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p9">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p9.1">This ensures that the loss is applied only to the tokens that are deemed most beneficial for the language model to learn from.
In practice, token selection can be implemented by ranking the tokens in a batch according to their excess loss and using only the top <math alttext="k\%" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p9.1.m1.1"><semantics id="S2.SS2.SSS0.Px3.p9.1.m1.1a"><mrow id="S2.SS2.SSS0.Px3.p9.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2.cmml">k</mi><mo id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p9.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.1">percent</csymbol><ci id="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p9.1.m1.1.1.2">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p9.1.m1.1c">k\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p9.1.m1.1d">italic_k %</annotation></semantics></math> of tokens for training.
This process eliminates the loss for undesired tokens without incurring additional costs during pretraining, making our approach both efficient and easily integrated.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We continually pretrained models in both mathematical and general domain and designed ablation and analysis experiments to understand the effectiveness of SLM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Setup</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Reference Model Training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">To train our mathematical reference model, we gathered a dataset of 0.5B high-quality, math-related tokens. This dataset is a blend of synthetic data from GPT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib13" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Huang et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib14" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and manually curated data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yue et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Ni et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.
For the general reference model, we compiled a corpus of 1.9B tokens from open-source datasets, such as Tulu-v2 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ivison et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> and OpenHermes-2.5 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Teknium</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
We trained the reference models for 3 epochs. The maximum learning rate was set at 5e-5 for 1B models and 1e-5 for 7B models, applying a cosine decay schedule.
We set the maximum sequence lengths to 2048 for 1B models and 4096 for 7B models, packing multiple samples into these lengths for model input.
In all main experiments, we initialized the continual pretraining model and the reference model with the <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.1">same</em> base model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.19.4.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.6.3" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.T1.6.3.1">Few-shot CoT reasoning results of math pretraining.</span> All models are tested with few-shot prompting. Previous best results are highlighted in <span class="ltx_text" id="S3.T1.6.3.2" style="color:#1E90FF;">blue</span>, while our best results are in <span class="ltx_text" id="S3.T1.6.3.3" style="color:#BF0040;">purple</span>. <math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T1.4.1.m1.1"><semantics id="S3.T1.4.1.m1.1b"><msup id="S3.T1.4.1.m1.1.1" xref="S3.T1.4.1.m1.1.1.cmml"><mi id="S3.T1.4.1.m1.1.1b" xref="S3.T1.4.1.m1.1.1.cmml"></mi><mo id="S3.T1.4.1.m1.1.1.1" xref="S3.T1.4.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.4.1.m1.1c"><apply id="S3.T1.4.1.m1.1.1.cmml" xref="S3.T1.4.1.m1.1.1"><times id="S3.T1.4.1.m1.1.1.1.cmml" xref="S3.T1.4.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.1.m1.1d">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.1.m1.1e">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>Only unique math-related tokens are calculated. For <span class="ltx_text ltx_font_smallcaps" id="S3.T1.6.3.4">Rho-1</span>, we calculate only the selected tokens that are used for training. <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.5.2.m2.1"><semantics id="S3.T1.5.2.m2.1b"><msup id="S3.T1.5.2.m2.1.1" xref="S3.T1.5.2.m2.1.1.cmml"><mi id="S3.T1.5.2.m2.1.1b" xref="S3.T1.5.2.m2.1.1.cmml"></mi><mo id="S3.T1.5.2.m2.1.1.1" xref="S3.T1.5.2.m2.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.5.2.m2.1c"><apply id="S3.T1.5.2.m2.1.1.cmml" xref="S3.T1.5.2.m2.1.1"><ci id="S3.T1.5.2.m2.1.1.1.cmml" xref="S3.T1.5.2.m2.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.2.m2.1d">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.2.m2.1e">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>We use OpenAI’s MATH subset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lightman et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib19" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> for evaluation, since some original test samples have been used in public training sets such as PRM800k. <math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T1.6.3.m3.1"><semantics id="S3.T1.6.3.m3.1b"><msup id="S3.T1.6.3.m3.1.1" xref="S3.T1.6.3.m3.1.1.cmml"><mi id="S3.T1.6.3.m3.1.1b" xref="S3.T1.6.3.m3.1.1.cmml"></mi><mo id="S3.T1.6.3.m3.1.1.1" xref="S3.T1.6.3.m3.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.6.3.m3.1c"><apply id="S3.T1.6.3.m3.1.1.cmml" xref="S3.T1.6.3.m3.1.1"><ci id="S3.T1.6.3.m3.1.1.1.cmml" xref="S3.T1.6.3.m3.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.3.m3.1d">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.3.m3.1e">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math>The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.
</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.13" style="width:433.6pt;height:484.6pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.6pt,21.9pt) scale(0.917124534204067,0.917124534204067) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.13.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.10.4.4">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.10.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.5.1">Model</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S3.T1.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="|\bm{\theta}|" class="ltx_Math" display="inline" id="S3.T1.7.1.1.1.m1.1"><semantics id="S3.T1.7.1.1.1.m1.1a"><mrow id="S3.T1.7.1.1.1.m1.1.2.2" xref="S3.T1.7.1.1.1.m1.1.2.1.cmml"><mo id="S3.T1.7.1.1.1.m1.1.2.2.1" stretchy="false" xref="S3.T1.7.1.1.1.m1.1.2.1.1.cmml">|</mo><mi id="S3.T1.7.1.1.1.m1.1.1" xref="S3.T1.7.1.1.1.m1.1.1.cmml">𝜽</mi><mo id="S3.T1.7.1.1.1.m1.1.2.2.2" stretchy="false" xref="S3.T1.7.1.1.1.m1.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.1.1.1.m1.1b"><apply id="S3.T1.7.1.1.1.m1.1.2.1.cmml" xref="S3.T1.7.1.1.1.m1.1.2.2"><abs id="S3.T1.7.1.1.1.m1.1.2.1.1.cmml" xref="S3.T1.7.1.1.1.m1.1.2.2.1"></abs><ci id="S3.T1.7.1.1.1.m1.1.1.cmml" xref="S3.T1.7.1.1.1.m1.1.1">𝜽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.1.1.1.m1.1c">|\bm{\theta}|</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.1.1.1.m1.1d">| bold_italic_θ |</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.6.1">Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.8.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.8.2.2.2.1">
<tbody><tr class="ltx_tr" id="S3.T1.8.2.2.2.1.2">
<td class="ltx_td ltx_align_center" id="S3.T1.8.2.2.2.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.8.2.2.2.1.2.1.1">Uniq.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.8.2.2.2.1.1">
<td class="ltx_td ltx_align_center" id="S3.T1.8.2.2.2.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.8.2.2.2.1.1.1.1">Toks</span><math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T1.8.2.2.2.1.1.1.m1.1"><semantics id="S3.T1.8.2.2.2.1.1.1.m1.1a"><msup id="S3.T1.8.2.2.2.1.1.1.m1.1.1" xref="S3.T1.8.2.2.2.1.1.1.m1.1.1.cmml"><mi id="S3.T1.8.2.2.2.1.1.1.m1.1.1a" xref="S3.T1.8.2.2.2.1.1.1.m1.1.1.cmml"></mi><mo id="S3.T1.8.2.2.2.1.1.1.m1.1.1.1" xref="S3.T1.8.2.2.2.1.1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.8.2.2.2.1.1.1.m1.1b"><apply id="S3.T1.8.2.2.2.1.1.1.m1.1.1.cmml" xref="S3.T1.8.2.2.2.1.1.1.m1.1.1"><times id="S3.T1.8.2.2.2.1.1.1.m1.1.1.1.cmml" xref="S3.T1.8.2.2.2.1.1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.2.2.2.1.1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.2.2.2.1.1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.10.4.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.10.4.4.7.1">
<tbody><tr class="ltx_tr" id="S3.T1.10.4.4.7.1.1">
<td class="ltx_td ltx_align_center" id="S3.T1.10.4.4.7.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.7.1.1.1.1">Train</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.10.4.4.7.1.2">
<td class="ltx_td ltx_align_center" id="S3.T1.10.4.4.7.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.7.1.2.1.1">Toks</span></td>
</tr>
</tbody></table></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.8.1">GSM8K</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.9.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.9.3.3.3.1">MATH<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.9.3.3.3.1.m1.1"><semantics id="S3.T1.9.3.3.3.1.m1.1a"><msup id="S3.T1.9.3.3.3.1.m1.1.1" xref="S3.T1.9.3.3.3.1.m1.1.1.cmml"><mi id="S3.T1.9.3.3.3.1.m1.1.1a" xref="S3.T1.9.3.3.3.1.m1.1.1.cmml"></mi><mo id="S3.T1.9.3.3.3.1.m1.1.1.1" mathvariant="normal" xref="S3.T1.9.3.3.3.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.9.3.3.3.1.m1.1b"><apply id="S3.T1.9.3.3.3.1.m1.1.1.cmml" xref="S3.T1.9.3.3.3.1.m1.1.1"><ci id="S3.T1.9.3.3.3.1.m1.1.1.1.cmml" xref="S3.T1.9.3.3.3.1.m1.1.1.1">normal-†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.3.3.3.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.3.3.3.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.9.1">SVAMP</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.10.1">ASDiv</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.11.1">MAWPS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.12.1">TAB</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.13.1">MQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.14" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.10.4.4.14.1">
<tbody><tr class="ltx_tr" id="S3.T1.10.4.4.14.1.1">
<td class="ltx_td ltx_align_center" id="S3.T1.10.4.4.14.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.14.1.1.1.1">MMLU</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.10.4.4.14.1.2">
<td class="ltx_td ltx_align_center" id="S3.T1.10.4.4.14.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.14.1.2.1.1">STEM</span></td>
</tr>
</tbody></table></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.10.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.4.1">SAT<math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T1.10.4.4.4.1.m1.1"><semantics id="S3.T1.10.4.4.4.1.m1.1a"><msup id="S3.T1.10.4.4.4.1.m1.1.1" xref="S3.T1.10.4.4.4.1.m1.1.1.cmml"><mi id="S3.T1.10.4.4.4.1.m1.1.1a" xref="S3.T1.10.4.4.4.1.m1.1.1.cmml"></mi><mo id="S3.T1.10.4.4.4.1.m1.1.1.1" mathvariant="normal" xref="S3.T1.10.4.4.4.1.m1.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.10.4.4.4.1.m1.1b"><apply id="S3.T1.10.4.4.4.1.m1.1.1.cmml" xref="S3.T1.10.4.4.4.1.m1.1.1"><ci id="S3.T1.10.4.4.4.1.m1.1.1.1.cmml" xref="S3.T1.10.4.4.4.1.m1.1.1.1">normal-‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.4.4.4.1.m1.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.10.4.4.4.1.m1.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.10.4.4.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.10.4.4.15.1">AVG</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.8.1">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="15" id="S3.T1.13.7.8.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_typewriter" id="S3.T1.13.7.8.1.1.1">1-2B Base Models</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.9.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.13.7.9.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/Tinyllama/Tinyllama-1.1B-intermediate-step-1431k-3T" title="">Tinyllama</a></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.9.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.9.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.13.7.9.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">2.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">3.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">11.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">18.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">20.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.11" style="padding-left:2.0pt;padding-right:2.0pt;">12.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.12" style="padding-left:2.0pt;padding-right:2.0pt;">14.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.13" style="padding-left:2.0pt;padding-right:2.0pt;">16.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.13.7.9.2.14" style="padding-left:2.0pt;padding-right:2.0pt;">21.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.9.2.15" style="padding-left:2.0pt;padding-right:2.0pt;">13.4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.10.3">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.10.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/microsoft/phi-1_5" title="">Phi-1.5</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.10.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.10.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.10.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">32.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">4.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">43.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">53.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.10" style="padding-left:2.0pt;padding-right:2.0pt;">66.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.11" style="padding-left:2.0pt;padding-right:2.0pt;">24.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.12" style="padding-left:2.0pt;padding-right:2.0pt;">14.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.13" style="padding-left:2.0pt;padding-right:2.0pt;">21.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.10.3.14" style="padding-left:2.0pt;padding-right:2.0pt;">18.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.10.3.15" style="padding-left:2.0pt;padding-right:2.0pt;">31.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.11.4">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.11.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/Qwen/Qwen1.5-1.8B" title="">Qwen1.5</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.11.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.8B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.11.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.11.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.11.4.6.1" style="color:#1E90FF;">36.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">6.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.11.4.8.1" style="color:#1E90FF;">48.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.11.4.9.1" style="color:#1E90FF;">63.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.11.4.10.1" style="color:#1E90FF;">79.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.11" style="padding-left:2.0pt;padding-right:2.0pt;">29.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.12" style="padding-left:2.0pt;padding-right:2.0pt;">25.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.13" style="padding-left:2.0pt;padding-right:2.0pt;">31.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.11.4.14" style="padding-left:2.0pt;padding-right:2.0pt;">40.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.11.4.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.11.4.15.1" style="color:#1E90FF;">40.0</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.12.5">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.12.5.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/google/gemma-2b" title="">Gemma</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.12.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">2.0B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.12.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.12.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">18.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">11.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">38.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.9" style="padding-left:2.0pt;padding-right:2.0pt;">56.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.10" style="padding-left:2.0pt;padding-right:2.0pt;">72.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.12.5.11.1" style="color:#1E90FF;">36.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.12.5.12.1" style="color:#1E90FF;">26.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.12.5.13.1" style="color:#1E90FF;">34.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.12.5.14" style="padding-left:2.0pt;padding-right:2.0pt;">50.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.12.5.15" style="padding-left:2.0pt;padding-right:2.0pt;">38.4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.13.6">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.13.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">DeepSeekLLM</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.13.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.13.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.13.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">150B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">11.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">8.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.13" style="padding-left:2.0pt;padding-right:2.0pt;">29.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.13.6.14" style="padding-left:2.0pt;padding-right:2.0pt;">31.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.13.6.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.14.7">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.14.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">DeepSeekMath</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.14.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.3B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.14.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">120B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.14.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">150B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.6" style="padding-left:2.0pt;padding-right:2.0pt;">23.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.14.7.7.1" style="color:#1E90FF;">13.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.13" style="padding-left:2.0pt;padding-right:2.0pt;">33.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.14.7.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.14.7.14.1" style="color:#1E90FF;">56.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.14.7.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.15.8">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="15" id="S3.T1.13.7.15.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_typewriter" id="S3.T1.13.7.15.8.1.1">Continual Pretraining on Tinyllama-1B</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.16.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.13.7.16.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">Tinyllama-CT</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.16.9.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.16.9.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.13.7.16.9.5" style="padding-left:2.0pt;padding-right:2.0pt;">15B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.6" style="padding-left:2.0pt;padding-right:2.0pt;">6.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.7" style="padding-left:2.0pt;padding-right:2.0pt;">2.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.8" style="padding-left:2.0pt;padding-right:2.0pt;">21.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.9" style="padding-left:2.0pt;padding-right:2.0pt;">36.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.10" style="padding-left:2.0pt;padding-right:2.0pt;">47.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.11" style="padding-left:2.0pt;padding-right:2.0pt;">17.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.12" style="padding-left:2.0pt;padding-right:2.0pt;">13.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.13" style="padding-left:2.0pt;padding-right:2.0pt;">23.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.13.7.16.9.14" style="padding-left:2.0pt;padding-right:2.0pt;">25.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.16.9.15" style="padding-left:2.0pt;padding-right:2.0pt;">21.6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.17.10">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.17.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_smallcaps" id="S3.T1.13.7.17.10.1.1">Rho-1</span>-Math</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.17.10.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.17.10.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.17.10.5" style="padding-left:2.0pt;padding-right:2.0pt;">9B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.6" style="padding-left:2.0pt;padding-right:2.0pt;">29.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.7" style="padding-left:2.0pt;padding-right:2.0pt;">14.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.8" style="padding-left:2.0pt;padding-right:2.0pt;">49.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.9" style="padding-left:2.0pt;padding-right:2.0pt;">61.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.10" style="padding-left:2.0pt;padding-right:2.0pt;">79.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.11" style="padding-left:2.0pt;padding-right:2.0pt;">25.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.12" style="padding-left:2.0pt;padding-right:2.0pt;">30.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.17.10.13.1" style="color:#BF0040;">24.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.17.10.14" style="padding-left:2.0pt;padding-right:2.0pt;">28.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.17.10.15" style="padding-left:2.0pt;padding-right:2.0pt;">38.1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.11.5.5">
<td class="ltx_td ltx_align_left" id="S3.T1.11.5.5.1" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T1.11.5.5.1.m1.1"><semantics id="S3.T1.11.5.5.1.m1.1a"><mi id="S3.T1.11.5.5.1.m1.1.1" mathvariant="normal" xref="S3.T1.11.5.5.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T1.11.5.5.1.m1.1b"><ci id="S3.T1.11.5.5.1.m1.1.1.cmml" xref="S3.T1.11.5.5.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.5.5.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T1.11.5.5.1.m1.1d">roman_Δ</annotation></semantics></math></td>
<td class="ltx_td" id="S3.T1.11.5.5.2" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T1.11.5.5.3" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S3.T1.11.5.5.4" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.11.5.5.5" style="background-color:#E6E6E6;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.5.1" style="background-color:#E6E6E6;">-40%</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.6" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.6.1" style="background-color:#D2DCFA;">+23.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.7" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.7.1" style="background-color:#D2DCFA;">+11.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.8" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.8.1" style="background-color:#D2DCFA;">+27.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.9" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.9.1" style="background-color:#D2DCFA;">+24.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.10" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.10.1" style="background-color:#D2DCFA;">+32.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.11" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.11.1" style="background-color:#D2DCFA;">+7.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.12" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.12.1" style="background-color:#D2DCFA;">+16.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.13" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.13.1" style="background-color:#D2DCFA;">+1.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.11.5.5.14" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.11.5.5.14.1" style="background-color:#D2DCFA;">+3.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.11.5.5.15" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.11.5.5.15.1" style="background-color:#D2DCFA;">+16.5</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.18.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.13.7.18.11.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_smallcaps" id="S3.T1.13.7.18.11.1.1">Rho-1</span>-Math</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.18.11.2" style="padding-left:2.0pt;padding-right:2.0pt;">1.1B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.18.11.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.13.7.18.11.5" style="padding-left:2.0pt;padding-right:2.0pt;">30B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.6.1" style="color:#BF0040;">36.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.7.1" style="color:#BF0040;">15.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.8.1" style="color:#BF0040;">52.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.9.1" style="color:#BF0040;">67.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.10.1" style="color:#BF0040;">83.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.11.1" style="color:#BF0040;">29.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.12.1" style="color:#BF0040;">32.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.13" style="padding-left:2.0pt;padding-right:2.0pt;">23.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.13.7.18.11.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.14.1" style="color:#BF0040;">28.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.18.11.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.18.11.15.1" style="color:#BF0040;">40.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.12.6.6">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="15" id="S3.T1.12.6.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<math alttext="\geq" class="ltx_Math" display="inline" id="S3.T1.12.6.6.1.m1.1"><semantics id="S3.T1.12.6.6.1.m1.1a"><mo id="S3.T1.12.6.6.1.m1.1.1" xref="S3.T1.12.6.6.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S3.T1.12.6.6.1.m1.1b"><geq id="S3.T1.12.6.6.1.m1.1.1.cmml" xref="S3.T1.12.6.6.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.6.6.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S3.T1.12.6.6.1.m1.1d">≥</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S3.T1.12.6.6.1.1"> 7B Base Models</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.19.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.13.7.19.12.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/meta-llama/Llama-2-7b-hf" title="">LLaMA-2</a></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.19.12.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_border_t" id="S3.T1.13.7.19.12.3" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.19.12.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.13.7.19.12.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.6" style="padding-left:2.0pt;padding-right:2.0pt;">14.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.7" style="padding-left:2.0pt;padding-right:2.0pt;">3.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.8" style="padding-left:2.0pt;padding-right:2.0pt;">39.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.9" style="padding-left:2.0pt;padding-right:2.0pt;">51.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.10" style="padding-left:2.0pt;padding-right:2.0pt;">63.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.11" style="padding-left:2.0pt;padding-right:2.0pt;">30.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.12" style="padding-left:2.0pt;padding-right:2.0pt;">12.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.13" style="padding-left:2.0pt;padding-right:2.0pt;">32.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.13.7.19.12.14" style="padding-left:2.0pt;padding-right:2.0pt;">34.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.19.12.15" style="padding-left:2.0pt;padding-right:2.0pt;">31.4</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.20.13">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.20.13.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/mistralai/Mistral-7B-v0.1" title="">Mistral</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.20.13.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td" id="S3.T1.13.7.20.13.3" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.20.13.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.20.13.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.6" style="padding-left:2.0pt;padding-right:2.0pt;">41.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.7" style="padding-left:2.0pt;padding-right:2.0pt;">11.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.8" style="padding-left:2.0pt;padding-right:2.0pt;">64.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.9" style="padding-left:2.0pt;padding-right:2.0pt;">68.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.10" style="padding-left:2.0pt;padding-right:2.0pt;">87.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.11" style="padding-left:2.0pt;padding-right:2.0pt;">52.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.12" style="padding-left:2.0pt;padding-right:2.0pt;">33.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.13" style="padding-left:2.0pt;padding-right:2.0pt;">49.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.20.13.14" style="padding-left:2.0pt;padding-right:2.0pt;">59.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.20.13.15" style="padding-left:2.0pt;padding-right:2.0pt;">52.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.21.14">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.21.14.1" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.21.14.2" style="padding-left:2.0pt;padding-right:2.0pt;">8B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.21.14.4" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.21.14.5" style="padding-left:2.0pt;padding-right:2.0pt;">164B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.6" style="padding-left:2.0pt;padding-right:2.0pt;">16.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.7" style="padding-left:2.0pt;padding-right:2.0pt;">14.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.13" style="padding-left:2.0pt;padding-right:2.0pt;">35.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.21.14.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.21.14.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.22.15">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.22.15.1" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.22.15.2" style="padding-left:2.0pt;padding-right:2.0pt;">62B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.22.15.4" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.22.15.5" style="padding-left:2.0pt;padding-right:2.0pt;">109B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.6" style="padding-left:2.0pt;padding-right:2.0pt;">52.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.7" style="padding-left:2.0pt;padding-right:2.0pt;">27.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.13" style="padding-left:2.0pt;padding-right:2.0pt;">53.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.22.15.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.22.15.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.23.16">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.23.16.1" style="padding-left:2.0pt;padding-right:2.0pt;">Minerva</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.23.16.2" style="padding-left:2.0pt;padding-right:2.0pt;">540B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.23.16.4" style="padding-left:2.0pt;padding-right:2.0pt;">39B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.23.16.5" style="padding-left:2.0pt;padding-right:2.0pt;">26B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.6" style="padding-left:2.0pt;padding-right:2.0pt;">58.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.7" style="padding-left:2.0pt;padding-right:2.0pt;">33.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.23.16.13.1" style="color:#1E90FF;">63.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.23.16.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.23.16.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.24.17">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.24.17.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/EleutherAI/llemma_7b" title="">LLemma</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.24.17.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.3" style="padding-left:2.0pt;padding-right:2.0pt;">PPile</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.24.17.4" style="padding-left:2.0pt;padding-right:2.0pt;">55B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.24.17.5" style="padding-left:2.0pt;padding-right:2.0pt;">200B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.6" style="padding-left:2.0pt;padding-right:2.0pt;">38.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.7" style="padding-left:2.0pt;padding-right:2.0pt;">17.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.8" style="padding-left:2.0pt;padding-right:2.0pt;">56.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.9" style="padding-left:2.0pt;padding-right:2.0pt;">69.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.10" style="padding-left:2.0pt;padding-right:2.0pt;">82.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.11" style="padding-left:2.0pt;padding-right:2.0pt;">48.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.12" style="padding-left:2.0pt;padding-right:2.0pt;">41.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.13" style="padding-left:2.0pt;padding-right:2.0pt;">45.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.24.17.14" style="padding-left:2.0pt;padding-right:2.0pt;">59.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.24.17.15" style="padding-left:2.0pt;padding-right:2.0pt;">50.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.25.18">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.25.18.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/EleutherAI/llemma_34b" title="">LLemma</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.25.18.2" style="padding-left:2.0pt;padding-right:2.0pt;">34B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.3" style="padding-left:2.0pt;padding-right:2.0pt;">PPile</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.25.18.4" style="padding-left:2.0pt;padding-right:2.0pt;">55B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.25.18.5" style="padding-left:2.0pt;padding-right:2.0pt;">50B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.6" style="padding-left:2.0pt;padding-right:2.0pt;">54.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.7" style="padding-left:2.0pt;padding-right:2.0pt;">23.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.8" style="padding-left:2.0pt;padding-right:2.0pt;">67.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.9" style="padding-left:2.0pt;padding-right:2.0pt;">75.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.10" style="padding-left:2.0pt;padding-right:2.0pt;">90.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.11" style="padding-left:2.0pt;padding-right:2.0pt;">57.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.12" style="padding-left:2.0pt;padding-right:2.0pt;">49.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.13" style="padding-left:2.0pt;padding-right:2.0pt;">54.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.25.18.14" style="padding-left:2.0pt;padding-right:2.0pt;">68.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.25.18.15" style="padding-left:2.0pt;padding-right:2.0pt;">60.1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.26.19">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.26.19.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/internlm/internlm2-math-base-7b" title="">Intern-Math</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.26.19.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.26.19.4" style="padding-left:2.0pt;padding-right:2.0pt;">31B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.26.19.5" style="padding-left:2.0pt;padding-right:2.0pt;">125B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.6" style="padding-left:2.0pt;padding-right:2.0pt;">41.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.7" style="padding-left:2.0pt;padding-right:2.0pt;">14.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.8" style="padding-left:2.0pt;padding-right:2.0pt;">61.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.9" style="padding-left:2.0pt;padding-right:2.0pt;">66.8</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.10" style="padding-left:2.0pt;padding-right:2.0pt;">83.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.11" style="padding-left:2.0pt;padding-right:2.0pt;">50.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.12" style="padding-left:2.0pt;padding-right:2.0pt;">57.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.13" style="padding-left:2.0pt;padding-right:2.0pt;">24.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.26.19.14" style="padding-left:2.0pt;padding-right:2.0pt;">37.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.26.19.15" style="padding-left:2.0pt;padding-right:2.0pt;">48.7</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.27.20">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.27.20.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/internlm/internlm2-math-base-20b" title="">Intern-Math</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.27.20.2" style="padding-left:2.0pt;padding-right:2.0pt;">20B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.27.20.4" style="padding-left:2.0pt;padding-right:2.0pt;">31B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.27.20.5" style="padding-left:2.0pt;padding-right:2.0pt;">125B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.27.20.6.1" style="color:#1E90FF;">65.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.7" style="padding-left:2.0pt;padding-right:2.0pt;">30.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.27.20.8.1" style="color:#1E90FF;">75.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.9" style="padding-left:2.0pt;padding-right:2.0pt;">79.3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.27.20.10.1" style="color:#1E90FF;">94.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.11" style="padding-left:2.0pt;padding-right:2.0pt;">50.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.12" style="padding-left:2.0pt;padding-right:2.0pt;">38.5</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.13" style="padding-left:2.0pt;padding-right:2.0pt;">53.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.27.20.14" style="padding-left:2.0pt;padding-right:2.0pt;">71.9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.27.20.15" style="padding-left:2.0pt;padding-right:2.0pt;">62.1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.28.21">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.28.21.1" style="padding-left:2.0pt;padding-right:2.0pt;"><a class="ltx_ref ltx_href" href="https://huggingface.co/deepseek-ai/deepseek-math-7b-base" title="">DeepSeekMath</a></td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.28.21.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.28.21.4" style="padding-left:2.0pt;padding-right:2.0pt;">120B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.28.21.5" style="padding-left:2.0pt;padding-right:2.0pt;">500B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.6" style="padding-left:2.0pt;padding-right:2.0pt;">64.1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.7.1" style="color:#1E90FF;">34.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.8" style="padding-left:2.0pt;padding-right:2.0pt;">74.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.9.1" style="color:#1E90FF;">83.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.10" style="padding-left:2.0pt;padding-right:2.0pt;">92.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.11.1" style="color:#1E90FF;">63.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.12.1" style="color:#1E90FF;">62.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.13" style="padding-left:2.0pt;padding-right:2.0pt;">56.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.28.21.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.14.1" style="color:#1E90FF;">84.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.28.21.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.28.21.15.1" style="color:#1E90FF;">68.4</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.29.22">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="15" id="S3.T1.13.7.29.22.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_typewriter" id="S3.T1.13.7.29.22.1.1">Continual Pretraining on Mistral-7B</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.30.23">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.13.7.30.23.1" style="padding-left:2.0pt;padding-right:2.0pt;">Mistral-CT</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.30.23.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.13.7.30.23.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.13.7.30.23.5" style="padding-left:2.0pt;padding-right:2.0pt;">15B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.6" style="padding-left:2.0pt;padding-right:2.0pt;">42.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.7" style="padding-left:2.0pt;padding-right:2.0pt;">22.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.8" style="padding-left:2.0pt;padding-right:2.0pt;">68.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.9" style="padding-left:2.0pt;padding-right:2.0pt;">71.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.10" style="padding-left:2.0pt;padding-right:2.0pt;">86.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.11" style="padding-left:2.0pt;padding-right:2.0pt;">45.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.12" style="padding-left:2.0pt;padding-right:2.0pt;">47.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.13" style="padding-left:2.0pt;padding-right:2.0pt;">52.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.13.7.30.23.14" style="padding-left:2.0pt;padding-right:2.0pt;">65.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.13.7.30.23.15" style="padding-left:2.0pt;padding-right:2.0pt;">55.8</td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.31.24">
<td class="ltx_td ltx_align_left" id="S3.T1.13.7.31.24.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_smallcaps" id="S3.T1.13.7.31.24.1.1">Rho-1</span>-Math</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.31.24.2" style="padding-left:2.0pt;padding-right:2.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.3" style="padding-left:2.0pt;padding-right:2.0pt;">OWM</td>
<td class="ltx_td ltx_align_right" id="S3.T1.13.7.31.24.4" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.13.7.31.24.5" style="padding-left:2.0pt;padding-right:2.0pt;">10.5B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.6.1" style="color:#BF0040;">66.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.7.1" style="color:#BF0040;">31.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.8.1" style="color:#BF0040;">77.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.9.1" style="color:#BF0040;">79.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.10.1" style="color:#BF0040;">93.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.11.1" style="color:#BF0040;">49.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.12" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.12.1" style="color:#BF0040;">58.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.13" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.13.1" style="color:#BF0040;">54.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.13.7.31.24.14" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.14.1" style="color:#BF0040;">84.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.13.7.31.24.15" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.31.24.15.1" style="color:#BF0040;">66.2</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.13.7.7.1" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T1.13.7.7.1.m1.1"><semantics id="S3.T1.13.7.7.1.m1.1a"><mi id="S3.T1.13.7.7.1.m1.1.1" mathvariant="normal" xref="S3.T1.13.7.7.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T1.13.7.7.1.m1.1b"><ci id="S3.T1.13.7.7.1.m1.1.1.cmml" xref="S3.T1.13.7.7.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.7.7.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T1.13.7.7.1.m1.1d">roman_Δ</annotation></semantics></math></td>
<td class="ltx_td ltx_border_bb" id="S3.T1.13.7.7.2" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_bb" id="S3.T1.13.7.7.3" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_border_bb" id="S3.T1.13.7.7.4" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S3.T1.13.7.7.5" style="background-color:#E6E6E6;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.5.1" style="background-color:#E6E6E6;">-30%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.6" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.6.1" style="background-color:#D2DCFA;">+24.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.7" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.7.1" style="background-color:#D2DCFA;">+8.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.8" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.8.1" style="background-color:#D2DCFA;">+9.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.9" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.9.1" style="background-color:#D2DCFA;">+8.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.10" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.10.1" style="background-color:#D2DCFA;">+7.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.11" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.11.1" style="background-color:#D2DCFA;">+4.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.12" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.12.1" style="background-color:#D2DCFA;">+11.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.13" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.13.1" style="background-color:#D2DCFA;">+2.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.13.7.7.14" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S3.T1.13.7.7.14.1" style="background-color:#D2DCFA;">+18.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.13.7.7.15" style="background-color:#D2DCFA;padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.13.7.7.15.1" style="background-color:#D2DCFA;">+10.4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Corpus</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">For mathematical reasoning, we utilize the OpenWebMath (OWM) dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Paster et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib20" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, which comprises approximately 14B tokens sourced from math-related web pages in the Common Crawl. In the general domain, we combine the SlimPajama <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Daria et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib21" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> and StarCoderData <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib22" title=""><span class="ltx_text" style="font-size:90%;">2023a</span></a>)</cite> (both part of the Tinyllama corpus) with OpenWebMath, training on a total of 80 billion tokens with a mix ratio of 6:3:1.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Setting</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">For math pretraining, we continue pretraining on the Tinyllama-1.1B model <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and the Mistral-7B model <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jiang et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> with learning rates of 8e-5 and 2e-5, respectively.
For general domain, we set the learning rate for Tinyllama-1.1B model to 1e-4.
The batch size is uniformly set to 1M tokens for both domains. Regarding the token selection ratio, we use 60% for the Tinyllama-1.1B model and 70% for the Mistral-7B model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Baseline Setting</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">We use models that have been continually pretrained (Tinyllama-CT and Mistral-CT) through regular causal language modeling as baselines.
Moreover, we compare <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.SSS0.Px4.p1.1.1">Rho-1</span> with well-known and top-performing baselines, including Gemma&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Team et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib25" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, Qwen1.5&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bai et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib26" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Phi-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib27" title=""><span class="ltx_text" style="font-size:90%;">2023b</span></a>)</cite>, DeepSeekLLM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">DeepSeek-AI</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib28" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, DeepSeekMath&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">2024a</span></a>)</cite>, CodeLlama&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Roziere et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Mistral&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jiang et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, Minerva&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lewkowycz et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, Tinyllama&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib23" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, LLemma&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Azerbayev et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, and InternLM2-Math&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ying et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib33" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.
For fine-tuning results, we also compare with previous best models MAmmoTH<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Yue et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib15" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and ToRA<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gou et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Evaluation Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px5.p1.1">To comprehensively evaluate pretrained models, we compare their few-shot capabilities and fine-tuning performance across a variety of tasks.
We adopt the lm-eval-harness<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EleutherAI/lm-evaluation-harness" title="">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span> for general tasks, and math-eval-harness<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ZubinGou/math-evaluation-harness" title="">https://github.com/ZubinGou/math-evaluation-harness</a></span></span></span> for math tasks.
We use vllm (v0.3.2)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kwon et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib35" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> to speed up inference. Further details on our evaluation can be found in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A3" title="Appendix C Evalution Details ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;C</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.5.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.T2.6.2" style="font-size:90%;">Tool-integrated reasoning results of math pretraining.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.2" style="width:433.6pt;height:298.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(7.8pt,-5.3pt) scale(1.03709079480337,1.03709079480337) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.2.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.2.2.3.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S3.T2.2.2.3.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.2.1">Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.3.1">Tools</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.2.2.3.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.4.1">SFT Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.5.1">GSM8k</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.6.1">MATH</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.7.1">SVAMP</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.8.1">ASDiv</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.9" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.9.1">MAWPS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.10" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.10.1">TAB</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.2.2.3.1.11" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.11.1">GSM-H</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.3.1.12" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.3.1.12.1">AVG</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.4.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" colspan="4" id="S3.T2.2.2.4.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T2.2.2.4.2.1.1">Used for SFT?</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.2.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.4.1" style="color:#C80000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.5.1" style="color:#C80000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.6.1" style="color:#C80000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.4.2.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.7.1" style="color:#C80000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.4.2.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.4.2.8.1" style="color:#C80000;">✗</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="12" id="S3.T2.2.2.5.3.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_typewriter" id="S3.T2.2.2.5.3.1.1">Previous Models</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.6.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.6.4.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT4-0314</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.2.2.6.4.2" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.6.4.3.1" style="color:#C80000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.6.4.4" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.5" style="padding-left:5.0pt;padding-right:5.0pt;">92.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.6" style="padding-left:5.0pt;padding-right:5.0pt;">42.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.7" style="padding-left:5.0pt;padding-right:5.0pt;">93.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.8" style="padding-left:5.0pt;padding-right:5.0pt;">91.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.9" style="padding-left:5.0pt;padding-right:5.0pt;">97.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.10" style="padding-left:5.0pt;padding-right:5.0pt;">67.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.6.4.11" style="padding-left:5.0pt;padding-right:5.0pt;">64.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.6.4.12" style="padding-left:5.0pt;padding-right:5.0pt;">78.3</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.7.5">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.7.5.1" style="padding-left:5.0pt;padding-right:5.0pt;">GPT4-0314 (PAL)</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.7.5.2" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.7.5.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.7.5.4" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.5" style="padding-left:5.0pt;padding-right:5.0pt;">94.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.6" style="padding-left:5.0pt;padding-right:5.0pt;">51.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.7" style="padding-left:5.0pt;padding-right:5.0pt;">94.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.8" style="padding-left:5.0pt;padding-right:5.0pt;">92.6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.9" style="padding-left:5.0pt;padding-right:5.0pt;">97.7</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.10" style="padding-left:5.0pt;padding-right:5.0pt;">95.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.7.5.11" style="padding-left:5.0pt;padding-right:5.0pt;">77.6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.7.5.12" style="padding-left:5.0pt;padding-right:5.0pt;">86.4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.8.6">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.8.6.1" style="padding-left:5.0pt;padding-right:5.0pt;">MAmmoTH</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.8.6.2" style="padding-left:5.0pt;padding-right:5.0pt;">70B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.8.6.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.8.6.4" style="padding-left:5.0pt;padding-right:5.0pt;">MI-260k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.5" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.6" style="padding-left:5.0pt;padding-right:5.0pt;">41.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.7" style="padding-left:5.0pt;padding-right:5.0pt;">82.4</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.8" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.9" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.10" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.8.6.11" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.8.6.12" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.9.7">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.9.7.1" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.9.7.2" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.9.7.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.9.7.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.5" style="padding-left:5.0pt;padding-right:5.0pt;">68.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.6" style="padding-left:5.0pt;padding-right:5.0pt;">40.1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.7" style="padding-left:5.0pt;padding-right:5.0pt;">68.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.8" style="padding-left:5.0pt;padding-right:5.0pt;">73.9</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.9" style="padding-left:5.0pt;padding-right:5.0pt;">88.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.10" style="padding-left:5.0pt;padding-right:5.0pt;">42.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.9.7.11" style="padding-left:5.0pt;padding-right:5.0pt;">54.6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.9.7.12" style="padding-left:5.0pt;padding-right:5.0pt;">62.4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.10.8">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.10.8.1" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.10.8.2" style="padding-left:5.0pt;padding-right:5.0pt;">70B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.10.8.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.10.8.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.5" style="padding-left:5.0pt;padding-right:5.0pt;">84.3</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.6" style="padding-left:5.0pt;padding-right:5.0pt;">49.7</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.7" style="padding-left:5.0pt;padding-right:5.0pt;">82.7</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.8" style="padding-left:5.0pt;padding-right:5.0pt;">86.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.9" style="padding-left:5.0pt;padding-right:5.0pt;">93.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.10" style="padding-left:5.0pt;padding-right:5.0pt;">74.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.10.8.11" style="padding-left:5.0pt;padding-right:5.0pt;">67.2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.10.8.12" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.11.9">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.11.9.1" style="padding-left:5.0pt;padding-right:5.0pt;">DeepSeekMath</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.11.9.2" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.11.9.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.11.9.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.5" style="padding-left:5.0pt;padding-right:5.0pt;">79.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.6" style="padding-left:5.0pt;padding-right:5.0pt;">52.0</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.7" style="padding-left:5.0pt;padding-right:5.0pt;">80.1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.8" style="padding-left:5.0pt;padding-right:5.0pt;">87.1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.9" style="padding-left:5.0pt;padding-right:5.0pt;">93.8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.10" style="padding-left:5.0pt;padding-right:5.0pt;">85.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.11.9.11" style="padding-left:5.0pt;padding-right:5.0pt;">63.1</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.11.9.12" style="padding-left:5.0pt;padding-right:5.0pt;">77.4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="12" id="S3.T2.2.2.12.10.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_typewriter" id="S3.T2.2.2.12.10.1.1">Our Pretrained Models</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.13.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.13.11.1" style="padding-left:5.0pt;padding-right:5.0pt;">TinyLlama-CT</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.2.2.13.11.2" style="padding-left:5.0pt;padding-right:5.0pt;">1B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.13.11.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.13.11.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.5" style="padding-left:5.0pt;padding-right:5.0pt;">51.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.6" style="padding-left:5.0pt;padding-right:5.0pt;">38.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.7" style="padding-left:5.0pt;padding-right:5.0pt;">53.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.8" style="padding-left:5.0pt;padding-right:5.0pt;">66.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.9" style="padding-left:5.0pt;padding-right:5.0pt;">81.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.10" style="padding-left:5.0pt;padding-right:5.0pt;">20.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.13.11.11" style="padding-left:5.0pt;padding-right:5.0pt;">42.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.13.11.12" style="padding-left:5.0pt;padding-right:5.0pt;">50.7</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.14.12">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.14.12.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_smallcaps" id="S3.T2.2.2.14.12.1.1">Rho-1</span>-Math</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.14.12.2" style="padding-left:5.0pt;padding-right:5.0pt;">1B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.14.12.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.5.1" style="color:#BF0040;">59.4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.6.1" style="color:#BF0040;">40.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.7.1" style="color:#BF0040;">60.7</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.8.1" style="color:#BF0040;">74.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.9.1" style="color:#BF0040;">88.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.10" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.10.1" style="color:#BF0040;">26.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.14.12.11" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.11.1" style="color:#BF0040;">48.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.14.12.12" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.14.12.12.1" style="color:#BF0040;">56.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T2.1.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.1.m1.1a"><mi id="S3.T2.1.1.1.1.m1.1.1" mathvariant="normal" xref="S3.T2.1.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.1.m1.1d">roman_Δ</annotation></semantics></math></td>
<td class="ltx_td" id="S3.T2.1.1.1.2" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td" id="S3.T2.1.1.1.3" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" id="S3.T2.1.1.1.4" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.5" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.5.1" style="background-color:#D2DCFA;">+8.0</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.6" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.6.1" style="background-color:#D2DCFA;">+2.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.7" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.7.1" style="background-color:#D2DCFA;">+7.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.8" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.8.1" style="background-color:#D2DCFA;">+7.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.9" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.9.1" style="background-color:#D2DCFA;">+6.9</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.10" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.10.1" style="background-color:#D2DCFA;">+6.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.1.1.11" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.1.1.1.11.1" style="background-color:#D2DCFA;">+5.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.12" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.12.1" style="background-color:#D2DCFA;">+6.2</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.15.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.2.2.15.13.1" style="padding-left:5.0pt;padding-right:5.0pt;">Mistral-CT</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.2.2.15.13.2" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.15.13.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.15.13.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.5" style="padding-left:5.0pt;padding-right:5.0pt;">77.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.6" style="padding-left:5.0pt;padding-right:5.0pt;">48.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.7" style="padding-left:5.0pt;padding-right:5.0pt;">76.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.8" style="padding-left:5.0pt;padding-right:5.0pt;">83.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.9" style="padding-left:5.0pt;padding-right:5.0pt;">93.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.10" style="padding-left:5.0pt;padding-right:5.0pt;">67.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.2.2.15.13.11" style="padding-left:5.0pt;padding-right:5.0pt;">60.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.15.13.12" style="padding-left:5.0pt;padding-right:5.0pt;">72.6</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.16.14">
<td class="ltx_td ltx_align_left" id="S3.T2.2.2.16.14.1" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_smallcaps" id="S3.T2.2.2.16.14.1.1">Rho-1</span>-Math</td>
<td class="ltx_td ltx_align_right" id="S3.T2.2.2.16.14.2" style="padding-left:5.0pt;padding-right:5.0pt;">7B</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.3.1" style="color:#326400;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.16.14.4" style="padding-left:5.0pt;padding-right:5.0pt;">ToRA-69k</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.5.1" style="color:#BF0040;">81.3</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.6.1" style="color:#BF0040;">51.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.7.1" style="color:#BF0040;">80.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.8.1" style="color:#BF0040;">85.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.9.1" style="color:#BF0040;">94.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.10" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.10.1" style="color:#BF0040;">70.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.2.2.16.14.11" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.11.1" style="color:#BF0040;">63.1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.2.16.14.12" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.16.14.12.1" style="color:#BF0040;">75.3</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.2.2.2.1" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T2.2.2.2.1.m1.1"><semantics id="S3.T2.2.2.2.1.m1.1a"><mi id="S3.T2.2.2.2.1.m1.1.1" mathvariant="normal" xref="S3.T2.2.2.2.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.1.m1.1b"><ci id="S3.T2.2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.2.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.2.1.m1.1d">roman_Δ</annotation></semantics></math></td>
<td class="ltx_td ltx_border_bb" id="S3.T2.2.2.2.2" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_bb" id="S3.T2.2.2.2.3" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_bb ltx_border_r" id="S3.T2.2.2.2.4" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.5" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.5.1" style="background-color:#D2DCFA;">+3.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.6" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.6.1" style="background-color:#D2DCFA;">+3.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.7" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.7.1" style="background-color:#D2DCFA;">+3.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.8" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.8.1" style="background-color:#D2DCFA;">+1.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.9" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.9.1" style="background-color:#D2DCFA;">+1.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.10" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.10.1" style="background-color:#D2DCFA;">+2.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.2.2.2.11" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text" id="S3.T2.2.2.2.11.1" style="background-color:#D2DCFA;">+2.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.2.2.12" style="background-color:#D2DCFA;padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.2.12.1" style="background-color:#D2DCFA;">+2.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Math Pre-training Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Few-shot CoT Reasoning Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">We evalute base models prompting with few-shot chain-of-thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wei et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib36" title=""><span class="ltx_text" style="font-size:90%;">2022a</span></a>)</cite> examples following previous works <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lewkowycz et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib31" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Azerbayev et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Shao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib37" title=""><span class="ltx_text" style="font-size:90%;">2024b</span></a>)</cite>.
As results shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.T1" title="Table 1 ‣ Reference Model Training ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Table&nbsp;1</span></a>, in comparison to continue pretraining directly, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.1">Rho-1</span>-Math has achieved the average few-shot accuracy improvement of 16.5% on 1B models and 10.4% on 7B models. Furthermore, after training for multiple epochs on OpenWebMath, we find that <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.2">Rho-1</span> could further increase the average few-shot accuracy to 40.9%.
Compared to DeepSeekMath-7B, which pretrained on 500 billion math-related tokens, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px1.p1.1.3">Rho-1</span>-7B pretrained on only 15 billion tokens (selecting 10.5 billion tokens) achieved comparable results, demonstrating the efficiency of our approach.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Tool-Integrated Reasoning Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">We fine-tune <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.1">Rho-1</span> and baseline models on 69k ToRA corpus <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gou et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib34" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, consisting of 16k GPT-4-generated trajectories in a tool-integrated reasoning format, and 53k answer-augmented samples using LLaMA.
As presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.T2" title="Table 2 ‣ Evaluation Setup ‣ 3.1 Experimental Setup ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Table&nbsp;2</span></a>, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.2">Rho-1</span>-1B and <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.3">Rho-1</span>-7B achieved a state-of-the-art 40.6% and 51.8% on MATH dataset, respectively.
On some unseen tasks (<em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.4">e.g.,</em> TabMWP and GSM-Hard), <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.5">Rho-1</span> also demonstrates a certain degree of generalizability,
with an average few-shot accuracy improvement of 6.2% on the <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.6">Rho-1</span>-Math-1B and 2.7% on <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px2.p1.1.7">Rho-1</span>-Math-7B.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="462" id="S3.F5.g1" src="x5.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.4.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.5.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.F5.5.2.1">General pretraining results.</span>
We continual pretraining Tinyllama-1B on 80G general tokens. Tinyllama-CT is etrained with CLM, while <span class="ltx_text ltx_font_smallcaps" id="S3.F5.5.2.2">Rho-1</span> is trained with our proposed SLM.
</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>General Pre-training Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We confirm the efficacy of the SLM in general pretraining by continual training Tinyllama-1.1B on 80 billion tokens.
The results depicted in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F5" title="Figure 5 ‣ Tool-Integrated Reasoning Results ‣ 3.2 Math Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;5</span></a> indicate that although Tinyllama has already undergone extensive training on the majority of these tokens, the application of SLM yields an average enhancement of 6.8% across 15 benchmarks compared to direct continual pretraining.
The improvements were especially pronounced in code and math tasks, exceeding 10%.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S3.F6.g1" src="x6.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.3.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F6.4.2" style="font-size:90%;">The dynamics of pretraining loss and downstream loss.<span class="ltx_text ltx_font_medium" id="S3.F6.4.2.1"> (a) and (c) represent the loss of tokens selected/unselected by SLM during pretraining in both SLM and CLM methods, while (b) represents the loss of the SLM and CLM methods on downstream corpora. We tested the above results through the process of pretraining with a total of 4 billion tokens.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Selected Token Loss Aligns Better with Downstream Performance</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">We use the reference model to filter tokens and explore the changes in validation loss after training on all/selected tokens, while observing their relationship with downstream loss.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F6" title="Figure 6 ‣ 3.3 General Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;6</span></a>, we pretrain approximately 4B tokens and displayed the variation curves of loss on different pretraining method and validation sets during the pretraining process.
We can observe that on the tokens selected by the reference model, the decrease in average loss of the <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px1.p1.1.1">Rho-1</span> is more significant compared to regular pretraining. On the contrary, on the unselected tokens, the decrease in average loss of the regular pretraining is more significant.
If we associate fig (a), fig(b) with fig(c), it is not difficult to find that the model trained on the selected tokens has a more significant decrease in downstream loss, while ordinary pretraining, although reducing the average loss of all tokens during the training phase, is difficult to have a significant decrease in downstream loss.
Therefore, we expect that selecting tokens for pretraining is more efficient.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="296" id="S3.F7.g1" src="x7.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.3.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F7.4.2" style="font-size:90%;">The relationship between the selected tokens / unselected tokens loss in SLM and downstream task performance.<span class="ltx_text ltx_font_medium" id="S3.F7.4.2.1"> The y-axis represents the average few-shot accuracy on GSM8k and MATH. The x-axis represents the average loss on selected tokens / unselected tokens at corresponding checkpoint(2B, 5B, 8B, 11B, and 14B).</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p2.1">Moreover, We relate the selected tokens’ loss to its downstream task performance via a power law in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F7" title="Figure 7 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;7</span></a>, which is similar to a concurrent study <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gadre et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>. Observing the curve fitted from the data points in the graph, the average loss of tokens selected by the SLM shows a positive correlation with the performance of downstream tasks, whereas the average loss of tokens not selected exhibits a negative correlation with downstream task performance. Therefore, it is not necessary for the all tokens’ loss to decrease to benefit the model’s ultimate performance. See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A4" title="Appendix D Relate the Selected Tokens’ Loss to Downstream Task Performance ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Appendix&nbsp;D</span></a> for more details.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.SS4.SSS0.Px1.2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2"><span class="ltx_inline-para ltx_minipage ltx_flex_size_2 ltx_align_middle" id="S3.SS4.SSS0.Px1.1.1" style="width:212.5pt;">
<span class="ltx_para ltx_align_center" id="S3.SS4.SSS0.Px1.1.1.p1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="559" id="S3.SS4.SSS0.Px1.1.1.p1.g1" src="x8.png" width="829">
</span>
<span class="ltx_figure ltx_align_center" id="S3.F8">
<span class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F8.3.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F8.4.2" style="font-size:90%;">The PPL of tokens selected by different checkpoint.<span class="ltx_text ltx_font_medium" id="S3.F8.4.2.1"> We test the PPL of the tokens selected at 2B, 5B, 8B, 11B, and 14B.
</span></span></span>
</span></span></div>
<div class="ltx_flex_cell ltx_flex_size_2"><span class="ltx_inline-para ltx_minipage ltx_flex_size_2 ltx_align_middle" id="S3.SS4.SSS0.Px1.2.2" style="width:208.1pt;">
<span class="ltx_para ltx_align_center" id="S3.SS4.SSS0.Px1.2.2.p1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="625" id="S3.SS4.SSS0.Px1.2.2.p1.g1" src="x9.png" width="830">
</span>
<span class="ltx_figure ltx_align_center" id="S3.F9">
<span class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F9.3.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F9.4.2" style="font-size:90%;">Effect of token select ratio.<span class="ltx_text ltx_font_medium" id="S3.F9.4.2.1"> We train 1B LM with SLM objective on 5B tokens.</span></span></span>
</span></span></div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">What Tokens are Selected with SLM?</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.1">We aim to analyze the tokens selected by the SLM method in pretraining to further explore its working mechanism.
To this end, we visualize the token selection process during the training of <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px2.p1.1.1">Rho-1</span> using the OpenWebMath.
In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A5.SS1" title="E.1 Token Selected Examples ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§E.1</span></a>, we have highlighted in <span class="ltx_text" id="S3.SS4.SSS0.Px2.p1.1.2" style="color:#1E90FF;">blue</span> the tokens that were retained during actual pretraining.
We observe that the majority of tokens chosen by the SLM method are closely related to mathematics, effectively training the model on the parts of the original corpus that are pertinent to mathematical content.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p2.1">Furthermore, we investigated the differences in token filtering across various checkpoints during the training process and tested the perplexity of these tokens on different checkpoints.
As illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F8" title="Figure 8 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;8</span></a>, we found that the tokens selected by later checkpoints tend to have higher perplexity towards the later stages of training and lower perplexity in the earlier stages.
This may suggest that the model first optimizes tokens with a larger learnable space, thereby increasing learning efficiency.
Moreover, we noticed a sample-wise “double descent”&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Nakkiran et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib39" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> on the loss of selected tokens, where the select token’s perplexity initially increases before decreases.
This might be an effect of selecting tokens based on excess loss, targeting those most in need at each checkpoint.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Effect of Token Select Ratio</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px3.p1.1">We investigate the impact of token selecting ratios of the SLM. Generally, the selecting ratio is defined by heuristic rules, similar to the approach previously employed in the training of Masked Language Models (MLMs)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Devlin et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib40" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Liu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib41" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F9" title="Figure 9 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;9</span></a>, the selected tokens is suitable for accounting for about 60% of the original tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Weak-to-Strong Generization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.5.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text ltx_font_bold" id="S3.T3.6.2" style="font-size:90%;">Weak-to-Strong generization result on math benchmark.<span class="ltx_text ltx_font_medium" id="S3.T3.6.2.1">
</span></span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.2" style="width:433.6pt;height:79.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(6.1pt,-1.1pt) scale(1.02907802348152,1.02907802348152) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.2.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.3.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.4.1">GSM8K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1">MATH<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.1.m1.1a"><msup id="S3.T3.1.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T3.1.1.1.1.1.m1.1.1a" xref="S3.T3.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="S3.T3.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="S3.T3.1.1.1.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.m1.1.1"><ci id="S3.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.1.1.m1.1.1.1">normal-†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.5.1">SVAMP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.6.1">ASDiv</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.7.1">MAWPS</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.8.1">TAB</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.9.1">MQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.2.2.2.10.1">
<tbody><tr class="ltx_tr" id="S3.T3.2.2.2.10.1.1">
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.10.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.10.1.1.1.1">MMLU</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.2.10.1.2">
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2.10.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.10.1.2.1.1">STEM</span></td>
</tr>
</tbody></table></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T3.2.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.2.1">SAT<math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T3.2.2.2.2.1.m1.1"><semantics id="S3.T3.2.2.2.2.1.m1.1a"><msup id="S3.T3.2.2.2.2.1.m1.1.1" xref="S3.T3.2.2.2.2.1.m1.1.1.cmml"><mi id="S3.T3.2.2.2.2.1.m1.1.1a" xref="S3.T3.2.2.2.2.1.m1.1.1.cmml"></mi><mo id="S3.T3.2.2.2.2.1.m1.1.1.1" mathvariant="normal" xref="S3.T3.2.2.2.2.1.m1.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.2.1.m1.1b"><apply id="S3.T3.2.2.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.2.2.1.m1.1.1"><ci id="S3.T3.2.2.2.2.1.m1.1.1.1.cmml" xref="S3.T3.2.2.2.2.1.m1.1.1.1">normal-‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.2.1.m1.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.2.1.m1.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.11.1">AVG</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.2.2.3.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">Llama-2-7B-CT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">28.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">13.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">50.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">62.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">79.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">37.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">34.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">41.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.2.2.3.1.10" style="padding-left:2.0pt;padding-right:2.0pt;">43.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3.1.11" style="padding-left:2.0pt;padding-right:2.0pt;">43.5</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T3.2.2.4.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">Llama-2-7B-CT w/ 1B RM</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">29.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">16.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">55.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">63.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">80.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">37.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">34.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">38.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T3.2.2.4.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">43.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.2.4.2.11" style="padding-left:2.0pt;padding-right:2.0pt;">44.4</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px4.p1.1">Apart from the main experiments where we use the same base model for the reference and continual pretraining, we also investigate if a smaller reference model can effectively guide the pretraining of a larger model.
We use Tinyllma-1.1B as reference model and continual pretraining Llama-2-7B on math.
Results presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.T3" title="Table 3 ‣ Weak-to-Strong Generization ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Table&nbsp;3</span></a> indicate that, despite the considerable gap between the small and large models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib42" title=""><span class="ltx_text" style="font-size:90%;">2023c</span></a>)</cite>, employing the small reference model to token selection can still yield benefits to the pre-training of the larger model.
If reference and training models have different vocabularies, one can consider performing token alignment <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wan et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib43" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Fu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib44" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, which we leave for future work.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Works</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Pretraining Data Optimization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">The objective of optimizing pre-training corpora is to maximize the performance and efficiency of language model training by improving the quality and scale of the pretrain data mixture.
This typically includes data collecting through crawling <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Raffel et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> or synthesis <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Polu and Sutskever</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib46" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Gunasekar et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib47" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, de-duplication <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lee et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib48" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Kandpal et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib49" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Tirumala et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib50" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, filtering and selection <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Albalak et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, as well as data composition <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Xie et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib52" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite> and curriculum <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chen et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib53" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">MA et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib54" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Data Selection</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">Data selection for fine-tuning has been extensively studied, focusing on improving quality <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib55" title=""><span class="ltx_text" style="font-size:90%;">2023d</span></a>)</cite>, diversity <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Liu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib56" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, and distribution matching <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Li et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib57" title=""><span class="ltx_text" style="font-size:90%;">2023e</span></a>; <span class="ltx_text" style="font-size:90%;">Xia et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib58" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Ni et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib16" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.
For pretraining, various lightweight filters are utilized <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Albalak et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib51" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, including heuristic-based (<em class="ltx_emph ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.1.1">e.g.,</em> language and item count filtering), classifier-based <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Brown et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib2" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, and perplexity-based approaches <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wenzek et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
The massive public RedPajama-Data-v2 dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Computer</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib59" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, for example, leverages over 40 quality indicators for data filtering and reweighting. Nevertheless, strict filtering like blocklist <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Raffel et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib45" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> and Safety API filtering <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Welbl et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, have been found to hurt evaluation loss or induce bias <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dodge et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.
To our knowledge, we are the first to explore token-level data selection, aimed at enhancing data quality and information density at the most fundamental granularity.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Language Model Training Dynamics</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Saphra and Lopez</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib60" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, the acquisition of linguistic knowledge <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Choshen et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib61" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Liu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib62" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, and the phenomenon of grokking <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Power et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib63" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>. The analysis by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xia et&nbsp;al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> is the most related to ours, which examines token-level training trajectories in models of varying sizes.
Our findings, however, diverge from those of <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xia et&nbsp;al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib64" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, who posit that tokens with little change in perplexity are “already learned”. We identify a spectrum of token patterns, including “easy tokens” and “hard tokens” that resist convergence. Recognizing this, we propose a method of selective language modeling that targets the influential tokens, optimizing the learning process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Scaling Laws</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px4.p1.1">Scaling laws guide us in discovering the impact of factors such as parameter count, data size, and compute on language model performance and behavior.
These studies usually focus on predicable scaling though power law <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Hernandez et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib65" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, optimal resource allocation <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hoffmann et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib66" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, downstream tasks <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wei et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">2022b</span></a>; <span class="ltx_text" style="font-size:90%;">Isik et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib68" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Gadre et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, architectures <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tay et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib10" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, memorization <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tirumala et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib69" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Carlini et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib70" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Henighan et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib71" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>; <span class="ltx_text" style="font-size:90%;">Biderman et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib72" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, and repeating data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hernandez et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib73" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>; <span class="ltx_text" style="font-size:90%;">Muennighoff et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib7" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>; <span class="ltx_text" style="font-size:90%;">Xue et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib74" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>.
Most scaling laws on model performance study cross-entory loss on all training tokens, while we focus on the tokens loss of desired distributions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Generalization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">In math continual pretraining, as depicted in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F6" title="Figure 6 ‣ 3.3 General Pre-training Results ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;6</span></a>, training exclusively with SLM leads to quickly convergence to the domain focused by the reference model, accompanied by a significant rise in the loss of unselected tokens.
Although no adverse effects, like biases, have been observed from the increased loss yet, a general pretraining loss on text and code may prevent overfitting <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Goodhart and Goodhart</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib75" title=""><span class="ltx_text" style="font-size:90%;">1984</span></a>)</cite>, as suggested by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Ouyang et&nbsp;al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib76" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Azerbayev et&nbsp;al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.
Furthermore, future efforts could broaden the corpus scope of the reference model, and enlarge the pretraining data size, as exemplified by DeepSpeedMath <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib29" title=""><span class="ltx_text" style="font-size:90%;">2024a</span></a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Scalability</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">Due to budget constraints, we have only verified the effectiveness of our method on smaller models (&lt;=7B parameters) and smaller datasets (&lt;100B tokens). Smaller models benefit significantly from removing the loss of irrelevant tokens and focusing on important ones.
However, it’s possible that very large models trained on extensive corpora may naturally develop this inductive bias to compress useful data (<em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px2.p1.1.1">i.e.,</em> compressing everything), although it may sounds inefficient for now.
Therefore, future works should study whether this selective language modeling technique can scale to very large models and data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaplan et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib1" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Is training a reference model necessary?</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">To score tokens, we need a high-quality reference model. This could be a base model trained with a small amount of high-quality data, or a performant open-source model.
In fact, since we only need input logprobs or perplexity from reference model, we could even utilize more powerful proprietary model APIs. We can input tokens and use the log probabilities of the input returned by the API as reference scores. We leave this for future works.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">How to improve upon SLM?</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px4.p1.1">There are many natural extensions of SLM, <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px4.p1.1.1">e.g.,</em> reweighting tokens instead of selecting may improve robustness; using a reference model as a reward model to guide pretraining with reinforcement learning; adopting multiple reference models to reduce overfitting; designing token-level curriculum learning and iterative strategies for continuous improvements, <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px4.p1.1.2">etc</em>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Expanding the Use of SLM</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px5.p1.1">SLM may be extended to supervised fine-tuning to address the noise and distribution mismatches in many SFT datasets.
Another potential application is alignment, <em class="ltx_emph ltx_font_italic" id="S5.SS0.SSS0.Px5.p1.1.1">e.g.,</em> by training a reference model to emphasize helpfulness, truthfulness, and harmlessness, we may obtain a base model that is natively aligned during the pretraining stage.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib1.5.5.1" style="font-size:90%;">Kaplan et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.7.1" style="font-size:90%;">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.8.1" style="font-size:90%;">Scaling laws for neural language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.9.1" style="font-size:90%;">arXiv preprint arXiv:2001.08361</em><span class="ltx_text" id="bib.bib1.10.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib2.5.5.1" style="font-size:90%;">Brown et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.7.1" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.8.1" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.9.1" style="font-size:90%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib2.10.2" style="font-size:90%;">, 33:1877–1901, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib3.4.4.1" style="font-size:90%;">OpenAI [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.6.1" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.7.1" style="font-size:90%;">Gpt-4 technical report, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib4.5.5.1" style="font-size:90%;">Team et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.7.1" style="font-size:90%;">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M Dai, Anja Hauth, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.8.1" style="font-size:90%;">Gemini: a family of highly capable multimodal models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.9.1" style="font-size:90%;">arXiv preprint arXiv:2312.11805</em><span class="ltx_text" id="bib.bib4.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib5.5.5.1" style="font-size:90%;">Wenzek et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.7.1" style="font-size:90%;">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.8.1" style="font-size:90%;">Ccnet: Extracting high quality monolingual datasets from web crawl data.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.9.1" style="font-size:90%;">arXiv preprint arXiv:1911.00359</em><span class="ltx_text" id="bib.bib5.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib6.5.5.1" style="font-size:90%;">Welbl et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.7.1" style="font-size:90%;">
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa&nbsp;Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.8.1" style="font-size:90%;">Challenges in detoxifying language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib6.10.2" style="font-size:90%;">Findings of the Association for Computational Linguistics: EMNLP 2021</em><span class="ltx_text" id="bib.bib6.11.3" style="font-size:90%;">, pages 2447–2469, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib7.5.5.1" style="font-size:90%;">Muennighoff et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.7.1" style="font-size:90%;">
Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le&nbsp;Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin&nbsp;A Raffel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.8.1" style="font-size:90%;">Scaling data-constrained language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib7.10.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib8.5.5.1" style="font-size:90%;">Dodge et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.7.1" style="font-size:90%;">
Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.8.1" style="font-size:90%;">Documenting large webtext corpora: A case study on the colossal clean crawled corpus.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib8.10.2" style="font-size:90%;">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text" id="bib.bib8.11.3" style="font-size:90%;">, pages 1286–1305, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib9.5.5.1" style="font-size:90%;">Longpre et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.7.1" style="font-size:90%;">
Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.8.1" style="font-size:90%;">A pretrainer’s guide to training data: Measuring the effects of data age, domain coverage, quality, &amp; toxicity.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.13169</em><span class="ltx_text" id="bib.bib9.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib10.5.5.1" style="font-size:90%;">Tay et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.7.1" style="font-size:90%;">
Yi&nbsp;Tay, Mostafa Dehghani, Samira Abnar, Hyung&nbsp;Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh&nbsp;Q Tran, Dani Yogatama, and Donald Metzler.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.8.1" style="font-size:90%;">Scaling laws vs model architectures: How does inductive bias influence scaling?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.9.1" style="font-size:90%;">arXiv preprint arXiv:2207.10551</em><span class="ltx_text" id="bib.bib10.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib11.5.5.1" style="font-size:90%;">Wettig et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.7.1" style="font-size:90%;">
Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.8.1" style="font-size:90%;">Should you mask 15% in masked language modeling?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.9.1" style="font-size:90%;">In Andreas Vlachos and Isabelle Augenstein, editors, </span><em class="ltx_emph ltx_font_italic" id="bib.bib11.10.2" style="font-size:90%;">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</em><span class="ltx_text" id="bib.bib11.11.3" style="font-size:90%;">, pages 2985–3000, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.12.1" style="font-size:90%;">doi: </span><a class="ltx_ref ltx_Url" href="10.18653/v1/2023.eacl-main.217" style="font-size:90%;" title="">10.18653/v1/2023.eacl-main.217</a><span class="ltx_text" id="bib.bib11.13.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.14.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.eacl-main.217" style="font-size:90%;" title="">https://aclanthology.org/2023.eacl-main.217</a><span class="ltx_text" id="bib.bib11.15.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib12.4.4.1" style="font-size:90%;">Hüllermeier and Waegeman [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.6.1" style="font-size:90%;">
Eyke Hüllermeier and Willem Waegeman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.7.1" style="font-size:90%;">Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.8.1" style="font-size:90%;">Machine learning</em><span class="ltx_text" id="bib.bib12.9.2" style="font-size:90%;">, 110(3):457–506, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib13.5.5.1" style="font-size:90%;">Yu et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.7.1" style="font-size:90%;">
Longhui Yu, Weisen Jiang, Han Shi, YU&nbsp;Jincheng, Zhengying Liu, Yu&nbsp;Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.8.1" style="font-size:90%;">Metamath: Bootstrap your own mathematical questions for large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib13.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib13.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib14.5.5.1" style="font-size:90%;">Huang et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib14.8.2" style="font-size:90%;">Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.9.1" style="font-size:90%;">Key-point-driven data synthesis with its enhancement on mathematical reasoning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.10.1" style="font-size:90%;">arXiv preprint arXiv:2403.02333</em><span class="ltx_text" id="bib.bib14.11.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib15.5.5.1" style="font-size:90%;">Yue et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.7.1" style="font-size:90%;">
Xiang Yue, Xingwei Qu, Ge&nbsp;Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu&nbsp;Su, and Wenhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.8.1" style="font-size:90%;">Mammoth: Building math generalist models through hybrid instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib15.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib15.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib16.5.5.1" style="font-size:90%;">Ni et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.7.1" style="font-size:90%;">
Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.8.1" style="font-size:90%;">Exploring the mystery of influential data for mathematical reasoning, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib17.5.5.1" style="font-size:90%;">Ivison et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.7.1" style="font-size:90%;">
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah&nbsp;A Smith, Iz&nbsp;Beltagy, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.8.1" style="font-size:90%;">Camels in a changing climate: Enhancing lm adaptation with tulu 2.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.9.1" style="font-size:90%;">arXiv preprint arXiv:2311.10702</em><span class="ltx_text" id="bib.bib17.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib18.4.4.1" style="font-size:90%;">Teknium [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.6.1" style="font-size:90%;">
Teknium.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.7.1" style="font-size:90%;">Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.8.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/teknium/OpenHermes-2.5" style="font-size:90%;" title="">https://huggingface.co/datasets/teknium/OpenHermes-2.5</a><span class="ltx_text" id="bib.bib18.9.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib19.5.5.1" style="font-size:90%;">Lightman et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.7.1" style="font-size:90%;">
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.8.1" style="font-size:90%;">Let’s verify step by step.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.9.1" style="font-size:90%;">arXiv preprint arXiv:2305.20050</em><span class="ltx_text" id="bib.bib19.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib20.5.5.1" style="font-size:90%;">Paster et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.7.1" style="font-size:90%;">
Keiran Paster, Marco&nbsp;Dos Santos, Zhangir Azerbayev, and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.8.1" style="font-size:90%;">Openwebmath: An open dataset of high-quality mathematical web text, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib21.5.5.1" style="font-size:90%;">Daria et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.7.1" style="font-size:90%;">
Soboleva Daria, Al-Khateeb Faisal, Myers Robert Steeves&nbsp;Jacob R, Hestness Joel, and Dey Nolan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.8.1" style="font-size:90%;">SlimPajama: A 627B token cleaned and deduplicated version of RedPajama.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama" style="font-size:90%;" title="">https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama</a><span class="ltx_text" id="bib.bib21.9.1" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.10.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/cerebras/SlimPajama-627B" style="font-size:90%;" title="">https://huggingface.co/datasets/cerebras/SlimPajama-627B</a><span class="ltx_text" id="bib.bib21.11.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib22.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.7.1" style="font-size:90%;">
Raymond Li, Loubna&nbsp;Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry&nbsp;Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh&nbsp;Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra&nbsp;Murthy V, Jason Stillerman, Siva&nbsp;Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn&nbsp;Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos&nbsp;Muñoz Ferrandis, Sean Hughes, Thomas
Wolf, Arjun Guha, Leandro von Werra, and Harm de&nbsp;Vries.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.8.1" style="font-size:90%;">Starcoder: may the source be with you!
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.9.1" style="font-size:90%;">CoRR</em><span class="ltx_text" id="bib.bib22.10.2" style="font-size:90%;">, abs/2305.06161, 2023a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib23.5.5.1" style="font-size:90%;">Zhang et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.7.1" style="font-size:90%;">
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.8.1" style="font-size:90%;">Tinyllama: An open-source small language model, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib24.5.5.1" style="font-size:90%;">Jiang et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.7.1" style="font-size:90%;">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.8.1" style="font-size:90%;">Mistral 7b.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.9.1" style="font-size:90%;">arXiv preprint arXiv:2310.06825</em><span class="ltx_text" id="bib.bib24.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib25.5.5.1" style="font-size:90%;">Team et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.7.1" style="font-size:90%;">
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir&nbsp;Sanjay Kale, Juliette Love, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.8.1" style="font-size:90%;">Gemma: Open models based on gemini research and technology.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.9.1" style="font-size:90%;">arXiv preprint arXiv:2403.08295</em><span class="ltx_text" id="bib.bib25.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib26.5.5.1" style="font-size:90%;">Bai et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.7.1" style="font-size:90%;">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.8.1" style="font-size:90%;">Qwen technical report.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.9.1" style="font-size:90%;">arXiv preprint arXiv:2309.16609</em><span class="ltx_text" id="bib.bib26.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib27.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.7.1" style="font-size:90%;">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie&nbsp;Del Giorno, Suriya Gunasekar, and Yin&nbsp;Tat Lee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.8.1" style="font-size:90%;">Textbooks are all you need ii: phi-1.5 technical report, 2023b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib28.4.4.1" style="font-size:90%;">DeepSeek-AI [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.6.1" style="font-size:90%;">
DeepSeek-AI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.7.1" style="font-size:90%;">Deepseek llm: Scaling open-source language models with longtermism.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.8.1" style="font-size:90%;">arXiv preprint arXiv:2401.02954</em><span class="ltx_text" id="bib.bib28.9.2" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.10.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/deepseek-ai/DeepSeek-LLM" style="font-size:90%;" title="">https://github.com/deepseek-ai/DeepSeek-LLM</a><span class="ltx_text" id="bib.bib28.11.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib29.5.5.1" style="font-size:90%;">Shao et&nbsp;al. [2024a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.7.1" style="font-size:90%;">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y.&nbsp;Wu, and Daya Guo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.8.1" style="font-size:90%;">Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024a.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.9.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.03300" style="font-size:90%;" title="">https://arxiv.org/abs/2402.03300</a><span class="ltx_text" id="bib.bib29.10.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib30.5.5.1" style="font-size:90%;">Roziere et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.7.1" style="font-size:90%;">
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing&nbsp;Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.8.1" style="font-size:90%;">Code llama: Open foundation models for code.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.9.1" style="font-size:90%;">arXiv preprint arXiv:2308.12950</em><span class="ltx_text" id="bib.bib30.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib31.5.5.1" style="font-size:90%;">Lewkowycz et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.7.1" style="font-size:90%;">
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.8.1" style="font-size:90%;">Solving quantitative reasoning problems with language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib31.10.2" style="font-size:90%;">, 35:3843–3857, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib32.5.5.1" style="font-size:90%;">Azerbayev et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.7.1" style="font-size:90%;">
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco&nbsp;Dos Santos, Stephen McAleer, Albert&nbsp;Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.8.1" style="font-size:90%;">Llemma: An open language model for mathematics.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.9.1" style="font-size:90%;">arXiv preprint arXiv:2310.10631</em><span class="ltx_text" id="bib.bib32.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib33.5.5.1" style="font-size:90%;">Ying et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.7.1" style="font-size:90%;">
Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.8.1" style="font-size:90%;">Internlm-math: Open math large language models toward verifiable reasoning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.06332</em><span class="ltx_text" id="bib.bib33.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib34.5.5.1" style="font-size:90%;">Gou et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.7.1" style="font-size:90%;">
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.8.1" style="font-size:90%;">Tora: A tool-integrated reasoning agent for mathematical problem solving.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib34.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib34.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib35.5.5.1" style="font-size:90%;">Kwon et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.7.1" style="font-size:90%;">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.8.1" style="font-size:90%;">Efficient memory management for large language model serving with pagedattention.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib35.10.2" style="font-size:90%;">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em><span class="ltx_text" id="bib.bib35.11.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib36.5.5.1" style="font-size:90%;">Wei et&nbsp;al. [2022a]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.7.1" style="font-size:90%;">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed&nbsp;Chi, Quoc&nbsp;V Le, Denny Zhou, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.8.1" style="font-size:90%;">Chain-of-thought prompting elicits reasoning in large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib36.10.2" style="font-size:90%;">NIPS</em><span class="ltx_text" id="bib.bib36.11.3" style="font-size:90%;">, volume&nbsp;35, pages 24824–24837, 2022a.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib37.5.5.1" style="font-size:90%;">Shao et&nbsp;al. [2024b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.7.1" style="font-size:90%;">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK&nbsp;Li, Y&nbsp;Wu, and Daya Guo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.8.1" style="font-size:90%;">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.03300</em><span class="ltx_text" id="bib.bib37.10.2" style="font-size:90%;">, 2024b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib38.5.5.1" style="font-size:90%;">Gadre et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.7.1" style="font-size:90%;">
Samir&nbsp;Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros&nbsp;G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.8.1" style="font-size:90%;">Language models scale reliably with over-training and on downstream tasks.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.9.1" style="font-size:90%;">Preprint</em><span class="ltx_text" id="bib.bib38.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib39.5.5.1" style="font-size:90%;">Nakkiran et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.7.1" style="font-size:90%;">
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.8.1" style="font-size:90%;">Deep double descent: Where bigger models and more data hurt.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.9.1" style="font-size:90%;">Journal of Statistical Mechanics: Theory and Experiment</em><span class="ltx_text" id="bib.bib39.10.2" style="font-size:90%;">, 2021(12):124003, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib40.5.5.1" style="font-size:90%;">Devlin et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.7.1" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.8.1" style="font-size:90%;">BERT: pre-training of deep bidirectional transformers for language understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib40.10.2" style="font-size:90%;">NAACL-HLT (1)</em><span class="ltx_text" id="bib.bib40.11.3" style="font-size:90%;">, pages 4171–4186. Association for Computational Linguistics, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib41.5.5.1" style="font-size:90%;">Liu et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.7.1" style="font-size:90%;">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.8.1" style="font-size:90%;">Roberta: A robustly optimized BERT pretraining approach.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.9.1" style="font-size:90%;">CoRR</em><span class="ltx_text" id="bib.bib41.10.2" style="font-size:90%;">, abs/1907.11692, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib42.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023c]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.7.1" style="font-size:90%;">
Xiang&nbsp;Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.8.1" style="font-size:90%;">Contrastive decoding: Open-ended text generation as optimization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib42.10.2" style="font-size:90%;">ACL (1)</em><span class="ltx_text" id="bib.bib42.11.3" style="font-size:90%;">, pages 12286–12312. Association for Computational Linguistics, 2023c.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib43.5.5.1" style="font-size:90%;">Wan et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.7.1" style="font-size:90%;">
Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.8.1" style="font-size:90%;">Knowledge fusion of large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib43.10.2" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span class="ltx_text" id="bib.bib43.11.3" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.12.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=jiDsk12qcz" style="font-size:90%;" title="">https://openreview.net/forum?id=jiDsk12qcz</a><span class="ltx_text" id="bib.bib43.13.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib44.5.5.1" style="font-size:90%;">Fu et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib44.8.2" style="font-size:90%;">Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.9.1" style="font-size:90%;">Specializing smaller language models towards multi-step reasoning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.10.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib44.11.2" style="font-size:90%;">International Conference on Machine Learning</em><span class="ltx_text" id="bib.bib44.12.3" style="font-size:90%;">, pages 10421–10430. PMLR, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib45.5.5.1" style="font-size:90%;">Raffel et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.7.1" style="font-size:90%;">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.8.1" style="font-size:90%;">Exploring the limits of transfer learning with a unified text-to-text transformer.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.9.1" style="font-size:90%;">Journal of machine learning research</em><span class="ltx_text" id="bib.bib45.10.2" style="font-size:90%;">, 21(140):1–67, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib46.4.4.1" style="font-size:90%;">Polu and Sutskever [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.6.1" style="font-size:90%;">
Stanislas Polu and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.7.1" style="font-size:90%;">Generative language modeling for automated theorem proving.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.8.1" style="font-size:90%;">arXiv preprint arXiv:2009.03393</em><span class="ltx_text" id="bib.bib46.9.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib47.5.5.1" style="font-size:90%;">Gunasekar et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.7.1" style="font-size:90%;">
Suriya Gunasekar, Yi&nbsp;Zhang, Jyoti Aneja, Caio César&nbsp;Teodoro Mendes, Allie Del&nbsp;Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de&nbsp;Rosa, Olli Saarikivi, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.8.1" style="font-size:90%;">Textbooks are all you need.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.9.1" style="font-size:90%;">arXiv preprint arXiv:2306.11644</em><span class="ltx_text" id="bib.bib47.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib48.5.5.1" style="font-size:90%;">Lee et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.7.1" style="font-size:90%;">
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.8.1" style="font-size:90%;">Deduplicating training data makes language models better.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.9.1" style="font-size:90%;">arXiv preprint arXiv:2107.06499</em><span class="ltx_text" id="bib.bib48.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib49.5.5.1" style="font-size:90%;">Kandpal et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib49.8.2" style="font-size:90%;">Nikhil Kandpal, Eric Wallace, and Colin Raffel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.9.1" style="font-size:90%;">Deduplicating training data mitigates privacy risks in language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.10.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib49.11.2" style="font-size:90%;">International Conference on Machine Learning</em><span class="ltx_text" id="bib.bib49.12.3" style="font-size:90%;">, pages 10697–10707. PMLR, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib50.5.5.1" style="font-size:90%;">Tirumala et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.7.1" style="font-size:90%;">
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.8.1" style="font-size:90%;">D4: Improving llm pretraining via document de-duplication and diversification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib50.10.2" style="font-size:90%;">NIPS</em><span class="ltx_text" id="bib.bib50.11.3" style="font-size:90%;">, volume&nbsp;36, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib51.5.5.1" style="font-size:90%;">Albalak et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.7.1" style="font-size:90%;">
Alon Albalak, Yanai Elazar, Sang&nbsp;Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William&nbsp;Yang Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.8.1" style="font-size:90%;">A survey on data selection for language models, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib52.5.5.1" style="font-size:90%;">Xie et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.7.1" style="font-size:90%;">
Sang&nbsp;Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy&nbsp;S Liang, Quoc&nbsp;V Le, Tengyu Ma, and Adams&nbsp;Wei Yu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.8.1" style="font-size:90%;">Doremi: Optimizing data mixtures speeds up language model pretraining.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib52.10.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib53.5.5.1" style="font-size:90%;">Chen et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.7.1" style="font-size:90%;">
Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce&nbsp;Zhang, Frederic Sala, and Christopher Ré.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.8.1" style="font-size:90%;">Skill-it! a data-driven skills framework for understanding and training language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib53.10.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib54.5.5.1" style="font-size:90%;">MA et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.7.1" style="font-size:90%;">
YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu&nbsp;Jiang, Changjian Wang, and Shanshan Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.8.1" style="font-size:90%;">At which training stage does code data help LLMs reasoning?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib54.10.2" style="font-size:90%;">The Twelfth International Conference on Learning Representations</em><span class="ltx_text" id="bib.bib54.11.3" style="font-size:90%;">, 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.12.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=KIPJKST4gw" style="font-size:90%;" title="">https://openreview.net/forum?id=KIPJKST4gw</a><span class="ltx_text" id="bib.bib54.13.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib55.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023d]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.7.1" style="font-size:90%;">
Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.8.1" style="font-size:90%;">From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.9.1" style="font-size:90%;">arXiv preprint arXiv:2308.12032</em><span class="ltx_text" id="bib.bib55.10.2" style="font-size:90%;">, 2023d.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib56.5.5.1" style="font-size:90%;">Liu et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.7.1" style="font-size:90%;">
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.8.1" style="font-size:90%;">What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib56.10.2" style="font-size:90%;">ICLR</em><span class="ltx_text" id="bib.bib56.11.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib57.5.5.1" style="font-size:90%;">Li et&nbsp;al. [2023e]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.7.1" style="font-size:90%;">
Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.8.1" style="font-size:90%;">One shot learning as instruction data prospector for large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.9.1" style="font-size:90%;">arXiv preprint arXiv:2312.10302</em><span class="ltx_text" id="bib.bib57.10.2" style="font-size:90%;">, 2023e.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib58.5.5.1" style="font-size:90%;">Xia et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.7.1" style="font-size:90%;">
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.8.1" style="font-size:90%;">Less: Selecting influential data for targeted instruction tuning.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.04333</em><span class="ltx_text" id="bib.bib58.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib59.4.4.1" style="font-size:90%;">Computer [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.6.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib59.7.2" style="font-size:90%;">Together Computer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.8.1" style="font-size:90%;">Redpajama: an open dataset for training large language models, 10 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.9.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/togethercomputer/RedPajama-Data" style="font-size:90%;" title="">https://github.com/togethercomputer/RedPajama-Data</a><span class="ltx_text" id="bib.bib59.10.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib60.4.4.1" style="font-size:90%;">Saphra and Lopez [2018]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.6.1" style="font-size:90%;">
Naomi Saphra and Adam Lopez.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.7.1" style="font-size:90%;">Understanding learning dynamics of language models with svcca.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.8.1" style="font-size:90%;">arXiv preprint arXiv:1811.00225</em><span class="ltx_text" id="bib.bib60.9.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib61.5.5.1" style="font-size:90%;">Choshen et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.7.1" style="font-size:90%;">
Leshem Choshen, Guy Hacohen, Daphna Weinshall, and Omri Abend.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.8.1" style="font-size:90%;">The grammar-learning trajectories of neural language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.9.1" style="font-size:90%;">arXiv preprint arXiv:2109.06096</em><span class="ltx_text" id="bib.bib61.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib62.5.5.1" style="font-size:90%;">Liu et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.7.1" style="font-size:90%;">
Leo&nbsp;Z Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah&nbsp;A Smith.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.8.1" style="font-size:90%;">Probing across time: What does roberta know and when?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.9.1" style="font-size:90%;">arXiv preprint arXiv:2104.07885</em><span class="ltx_text" id="bib.bib62.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib63.5.5.1" style="font-size:90%;">Power et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.7.1" style="font-size:90%;">
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.8.1" style="font-size:90%;">Grokking: Generalization beyond overfitting on small algorithmic datasets.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.9.1" style="font-size:90%;">arXiv preprint arXiv:2201.02177</em><span class="ltx_text" id="bib.bib63.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib64.5.5.1" style="font-size:90%;">Xia et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib64.8.2" style="font-size:90%;">Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi&nbsp;Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.9.1" style="font-size:90%;">Training trajectories of language models across scales.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.10.1" style="font-size:90%;">arXiv preprint arXiv:2212.09803</em><span class="ltx_text" id="bib.bib64.11.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib65.5.5.1" style="font-size:90%;">Hernandez et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.7.1" style="font-size:90%;">
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.8.1" style="font-size:90%;">Scaling laws for transfer.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.9.1" style="font-size:90%;">arXiv preprint arXiv:2102.01293</em><span class="ltx_text" id="bib.bib65.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib66.5.5.1" style="font-size:90%;">Hoffmann et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.7.1" style="font-size:90%;">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.8.1" style="font-size:90%;">Training compute-optimal large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.9.1" style="font-size:90%;">arXiv preprint arXiv:2203.15556</em><span class="ltx_text" id="bib.bib66.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib67.5.5.1" style="font-size:90%;">Wei et&nbsp;al. [2022b]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.7.1" style="font-size:90%;">
Jason Wei, Yi&nbsp;Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.8.1" style="font-size:90%;">Emergent abilities of large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.9.1" style="font-size:90%;">arXiv preprint arXiv:2206.07682</em><span class="ltx_text" id="bib.bib67.10.2" style="font-size:90%;">, 2022b.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib68.5.5.1" style="font-size:90%;">Isik et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.7.1" style="font-size:90%;">
Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.8.1" style="font-size:90%;">Scaling laws for downstream task performance of large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.9.1" style="font-size:90%;">arXiv preprint arXiv:2402.04177</em><span class="ltx_text" id="bib.bib68.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib69.5.5.1" style="font-size:90%;">Tirumala et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib69.8.2" style="font-size:90%;">Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.9.1" style="font-size:90%;">Memorization without overfitting: Analyzing the training dynamics of large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.10.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib69.11.2" style="font-size:90%;">, 35:38274–38290, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib70.5.5.1" style="font-size:90%;">Carlini et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.7.1" style="font-size:90%;">
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.8.1" style="font-size:90%;">Quantifying memorization across neural language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.9.1" style="font-size:90%;">arXiv preprint arXiv:2202.07646</em><span class="ltx_text" id="bib.bib70.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib71.5.5.1" style="font-size:90%;">Henighan et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.7.1" style="font-size:90%;">
Tom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort, Nicholas Schiefer, and Christopher Olah.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.8.1" style="font-size:90%;">Superposition, memorization, and double descent.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.9.1" style="font-size:90%;">Transformer Circuits Thread</em><span class="ltx_text" id="bib.bib71.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib72.5.5.1" style="font-size:90%;">Biderman et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.7.1" style="font-size:90%;">
Stella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.8.1" style="font-size:90%;">Emergent and predictable memorization in large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.9.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib72.10.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib73.5.5.1" style="font-size:90%;">Hernandez et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.7.1" style="font-size:90%;">
Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.8.1" style="font-size:90%;">Scaling laws and interpretability of learning from repeated data.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.9.1" style="font-size:90%;">arXiv preprint arXiv:2205.10487</em><span class="ltx_text" id="bib.bib73.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib74.5.5.1" style="font-size:90%;">Xue et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib74.8.2" style="font-size:90%;">Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.9.1" style="font-size:90%;">To repeat or not to repeat: Insights from scaling llm under token-crisis.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.10.1" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span class="ltx_text" id="bib.bib74.11.2" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib75.4.4.1" style="font-size:90%;">Goodhart and Goodhart [1984]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.6.1" style="font-size:90%;">
Charles&nbsp;AE Goodhart and CAE Goodhart.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.7.1" style="font-size:90%;">Problems of monetary management: the UK experience</em><span class="ltx_text" id="bib.bib75.8.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.9.1" style="font-size:90%;">Springer, 1984.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib76.5.5.1" style="font-size:90%;">Ouyang et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.7.1" style="font-size:90%;">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.8.1" style="font-size:90%;">Training language models to follow instructions with human feedback.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.9.1" style="font-size:90%;">Advances in neural information processing systems</em><span class="ltx_text" id="bib.bib76.10.2" style="font-size:90%;">, 35:27730–27744, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib77.5.5.1" style="font-size:90%;">Cobbe et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.7.1" style="font-size:90%;">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.8.1" style="font-size:90%;">Training verifiers to solve math word problems, 2021.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.9.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2110.14168" style="font-size:90%;" title="">https://arxiv.org/abs/2110.14168</a><span class="ltx_text" id="bib.bib77.10.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib78.5.5.1" style="font-size:90%;">Hendrycks et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.7.1" style="font-size:90%;">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.8.1" style="font-size:90%;">Measuring mathematical problem solving with the math dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib78.10.2" style="font-size:90%;">NIPS</em><span class="ltx_text" id="bib.bib78.11.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib79.5.5.1" style="font-size:90%;">Gao et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.7.1" style="font-size:90%;">
</span><span class="ltx_text" id="bib.bib79.8.2" style="font-size:90%;">Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.9.1" style="font-size:90%;">Pal: Program-aided language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.10.1" style="font-size:90%;">arXiv preprint arXiv:2211.10435</em><span class="ltx_text" id="bib.bib79.11.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib80.5.5.1" style="font-size:90%;">Patel et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.7.1" style="font-size:90%;">
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.8.1" style="font-size:90%;">Are NLP models really able to solve simple math word problems?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib80.10.2" style="font-size:90%;">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em><span class="ltx_text" id="bib.bib80.11.3" style="font-size:90%;">, pages 2080–2094, Online, June 2021. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.12.1" style="font-size:90%;">doi: </span><a class="ltx_ref ltx_Url" href="10.18653/v1/2021.naacl-main.168" style="font-size:90%;" title="">10.18653/v1/2021.naacl-main.168</a><span class="ltx_text" id="bib.bib80.13.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.14.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.naacl-main.168" style="font-size:90%;" title="">https://aclanthology.org/2021.naacl-main.168</a><span class="ltx_text" id="bib.bib80.15.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib81.5.5.1" style="font-size:90%;">Miao et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.7.1" style="font-size:90%;">
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.8.1" style="font-size:90%;">A diverse corpus for evaluating and developing English math word problem solvers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib81.10.2" style="font-size:90%;">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em><span class="ltx_text" id="bib.bib81.11.3" style="font-size:90%;">, pages 975–984, Online, July 2020. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.12.1" style="font-size:90%;">doi: </span><a class="ltx_ref ltx_Url" href="10.18653/v1/2020.acl-main.92" style="font-size:90%;" title="">10.18653/v1/2020.acl-main.92</a><span class="ltx_text" id="bib.bib81.13.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.14.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.92" style="font-size:90%;" title="">https://aclanthology.org/2020.acl-main.92</a><span class="ltx_text" id="bib.bib81.15.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib82.5.5.1" style="font-size:90%;">Koncel-Kedziorski et&nbsp;al. [2016]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.7.1" style="font-size:90%;">
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.8.1" style="font-size:90%;">MAWPS: A math word problem repository.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib82.10.2" style="font-size:90%;">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em><span class="ltx_text" id="bib.bib82.11.3" style="font-size:90%;">, pages 1152–1157, San Diego, California, June 2016. Association for Computational Linguistics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.12.1" style="font-size:90%;">doi: </span><a class="ltx_ref ltx_Url" href="10.18653/v1/N16-1136" style="font-size:90%;" title="">10.18653/v1/N16-1136</a><span class="ltx_text" id="bib.bib82.13.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.14.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N16-1136" style="font-size:90%;" title="">https://aclanthology.org/N16-1136</a><span class="ltx_text" id="bib.bib82.15.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib83.5.5.1" style="font-size:90%;">Lu et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.7.1" style="font-size:90%;">
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying&nbsp;Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.8.1" style="font-size:90%;">Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib83.10.2" style="font-size:90%;">The Eleventh International Conference on Learning Representations</em><span class="ltx_text" id="bib.bib83.11.3" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.12.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=DHyHRBwJUTN" style="font-size:90%;" title="">https://openreview.net/forum?id=DHyHRBwJUTN</a><span class="ltx_text" id="bib.bib83.13.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib84.5.5.1" style="font-size:90%;">Amini et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.7.1" style="font-size:90%;">
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.8.1" style="font-size:90%;">Mathqa: Towards interpretable math word problem solving with operation-based formalisms.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.9.1" style="font-size:90%;">arXiv preprint arXiv:1905.13319</em><span class="ltx_text" id="bib.bib84.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib85.5.5.1" style="font-size:90%;">Hendrycks et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.7.1" style="font-size:90%;">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.8.1" style="font-size:90%;">Measuring massive multitask language understanding.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.9.1" style="font-size:90%;">arXiv preprint arXiv:2009.03300</em><span class="ltx_text" id="bib.bib85.10.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib86.5.5.1" style="font-size:90%;">Gao et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.7.1" style="font-size:90%;">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le&nbsp;Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.8.1" style="font-size:90%;">A framework for few-shot language model evaluation, 12 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.9.1" style="font-size:90%;">URL </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/10256836" style="font-size:90%;" title="">https://zenodo.org/records/10256836</a><span class="ltx_text" id="bib.bib86.10.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib87.5.5.1" style="font-size:90%;">Suzgun et&nbsp;al. [2022]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.7.1" style="font-size:90%;">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V Le, Ed&nbsp;H Chi, Denny Zhou, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.8.1" style="font-size:90%;">Challenging big-bench tasks and whether chain-of-thought can solve them.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.9.1" style="font-size:90%;">arXiv preprint arXiv:2210.09261</em><span class="ltx_text" id="bib.bib87.10.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib88.5.5.1" style="font-size:90%;">Zhong et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.7.1" style="font-size:90%;">
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.8.1" style="font-size:90%;">Agieval: A human-centric benchmark for evaluating foundation models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.9.1" style="font-size:90%;">arXiv preprint arXiv:2304.06364</em><span class="ltx_text" id="bib.bib88.10.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib89.5.5.1" style="font-size:90%;">Clark et&nbsp;al. [2018]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.7.1" style="font-size:90%;">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.8.1" style="font-size:90%;">Think you have solved question answering? try arc, the ai2 reasoning challenge.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.9.1" style="font-size:90%;">arXiv preprint arXiv:1803.05457</em><span class="ltx_text" id="bib.bib89.10.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib90.5.5.1" style="font-size:90%;">Clark et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.7.1" style="font-size:90%;">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.8.1" style="font-size:90%;">Boolq: Exploring the surprising difficulty of natural yes/no questions.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.9.1" style="font-size:90%;">arXiv preprint arXiv:1905.10044</em><span class="ltx_text" id="bib.bib90.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib91.5.5.1" style="font-size:90%;">Bisk et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.7.1" style="font-size:90%;">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.8.1" style="font-size:90%;">Piqa: Reasoning about physical commonsense in natural language.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib91.10.2" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</em><span class="ltx_text" id="bib.bib91.11.3" style="font-size:90%;">, volume&nbsp;34, pages 7432–7439, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib92.5.5.1" style="font-size:90%;">Zellers et&nbsp;al. [2019]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.7.1" style="font-size:90%;">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.8.1" style="font-size:90%;">Hellaswag: Can a machine really finish your sentence?
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.9.1" style="font-size:90%;">arXiv preprint arXiv:1905.07830</em><span class="ltx_text" id="bib.bib92.10.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib93.5.5.1" style="font-size:90%;">Sakaguchi et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib93.7.1" style="font-size:90%;">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib93.8.1" style="font-size:90%;">Winogrande: An adversarial winograd schema challenge at scale.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.9.1" style="font-size:90%;">Communications of the ACM</em><span class="ltx_text" id="bib.bib93.10.2" style="font-size:90%;">, 64(9):99–106, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib94.5.5.1" style="font-size:90%;">Mihaylov et&nbsp;al. [2018]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.7.1" style="font-size:90%;">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.8.1" style="font-size:90%;">Can a suit of armor conduct electricity? a new dataset for open book question answering.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.9.1" style="font-size:90%;">arXiv preprint arXiv:1809.02789</em><span class="ltx_text" id="bib.bib94.10.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib95.5.5.1" style="font-size:90%;">Zheng et&nbsp;al. [2023]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.7.1" style="font-size:90%;">
Qinkai Zheng, Xiao Xia, Xu&nbsp;Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.8.1" style="font-size:90%;">Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.9.1" style="font-size:90%;">In </span><em class="ltx_emph ltx_font_italic" id="bib.bib95.10.2" style="font-size:90%;">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em><span class="ltx_text" id="bib.bib95.11.3" style="font-size:90%;">, pages 5673–5684, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib96.5.5.1" style="font-size:90%;">Clark et&nbsp;al. [2020]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib96.7.1" style="font-size:90%;">
Jonathan&nbsp;H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib96.8.1" style="font-size:90%;">Tydi qa: A benchmark for information-seeking question answering in ty pologically di verse languages.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.9.1" style="font-size:90%;">Transactions of the Association for Computational Linguistics</em><span class="ltx_text" id="bib.bib96.10.2" style="font-size:90%;">, 8:454–470, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib97.5.5.1" style="font-size:90%;">Austin et&nbsp;al. [2021]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib97.7.1" style="font-size:90%;">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib97.8.1" style="font-size:90%;">Program synthesis with large language models.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.9.1" style="font-size:90%;">arXiv preprint arXiv:2108.07732</em><span class="ltx_text" id="bib.bib97.10.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text" id="bib.bib98.5.5.1" style="font-size:90%;">Guo et&nbsp;al. [2024]</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.7.1" style="font-size:90%;">
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y&nbsp;Wu, YK&nbsp;Li, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.8.1" style="font-size:90%;">Deepseek-coder: When the large language model meets programming–the rise of code intelligence.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.9.1" style="font-size:90%;">arXiv preprint arXiv:2401.14196</em><span class="ltx_text" id="bib.bib98.10.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Author Contributions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Zhenghao Lin designed and implemented detailed token selection process, conducted extensive preliminary experiments, developed the pre-training and evaluation pipeline, conducted most of the pre-training experiments and analysis, implemented baselines, and significantly contributed to the writing.
Zhibin Gou presented a preliminary proposal, introduced the method of using excess loss for reweighting tokens, compiled high-quality corpora, trained reference models, set up the fine-tuning and evaluation pipelines, designed the experimental analysis, and significantly contributed to the writing.
Yeyun Gong proposed the initial project and co-led the project with Weizhu Chen, they offered extensive advice and guidance on experiments and writing, and oversaw team collaboration and resource management.
Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, and Nan Duan offered research mentorship, coordinated the project, and contributed to the writing.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Analysis and Visualization of Tokens in Pretraining</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>More Details of Four Categories Tokens</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A2.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1094" id="A2.F10.g1" src="x10.png" width="831">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F10.7.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text ltx_font_bold" id="A2.F10.8.2" style="font-size:90%;">Sample text containing four categories of tokens.<span class="ltx_text ltx_font_medium" id="A2.F10.8.2.1"> Among them, <span class="ltx_text" id="A2.F10.8.2.1.1" style="color:#1E90FF;">blue</span> represents tokens of categorie H→L, <span class="ltx_text" id="A2.F10.8.2.1.2" style="color:#228B22;">green</span> indicates tokens of categorie L→L, <span class="ltx_text" id="A2.F10.8.2.1.3" style="color:#FAAA00;">yellow</span> signifies tokens of categorie H→H, and <span class="ltx_text" id="A2.F10.8.2.1.4" style="color:#FA8072;">red</span> denotes tokens of categorie L→H.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">We categorize tokens into four categories: H→H, L→H, H→L, L→L. During the training process, we collected the loss of each token after training on each 1 billion tokens training data. We then used linear fitting and took the difference in loss between the first and last points as evidence of whether the loss decreased during the training process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1">Specifically, suppose we have a sequence of token’s loss <math alttext="(l_{0},l_{1},...,l_{n})" class="ltx_Math" display="inline" id="A2.SS1.p2.1.m1.4"><semantics id="A2.SS1.p2.1.m1.4a"><mrow id="A2.SS1.p2.1.m1.4.4.3" xref="A2.SS1.p2.1.m1.4.4.4.cmml"><mo id="A2.SS1.p2.1.m1.4.4.3.4" stretchy="false" xref="A2.SS1.p2.1.m1.4.4.4.cmml">(</mo><msub id="A2.SS1.p2.1.m1.2.2.1.1" xref="A2.SS1.p2.1.m1.2.2.1.1.cmml"><mi id="A2.SS1.p2.1.m1.2.2.1.1.2" xref="A2.SS1.p2.1.m1.2.2.1.1.2.cmml">l</mi><mn id="A2.SS1.p2.1.m1.2.2.1.1.3" xref="A2.SS1.p2.1.m1.2.2.1.1.3.cmml">0</mn></msub><mo id="A2.SS1.p2.1.m1.4.4.3.5" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="A2.SS1.p2.1.m1.3.3.2.2" xref="A2.SS1.p2.1.m1.3.3.2.2.cmml"><mi id="A2.SS1.p2.1.m1.3.3.2.2.2" xref="A2.SS1.p2.1.m1.3.3.2.2.2.cmml">l</mi><mn id="A2.SS1.p2.1.m1.3.3.2.2.3" xref="A2.SS1.p2.1.m1.3.3.2.2.3.cmml">1</mn></msub><mo id="A2.SS1.p2.1.m1.4.4.3.6" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><mi id="A2.SS1.p2.1.m1.1.1" mathvariant="normal" xref="A2.SS1.p2.1.m1.1.1.cmml">…</mi><mo id="A2.SS1.p2.1.m1.4.4.3.7" xref="A2.SS1.p2.1.m1.4.4.4.cmml">,</mo><msub id="A2.SS1.p2.1.m1.4.4.3.3" xref="A2.SS1.p2.1.m1.4.4.3.3.cmml"><mi id="A2.SS1.p2.1.m1.4.4.3.3.2" xref="A2.SS1.p2.1.m1.4.4.3.3.2.cmml">l</mi><mi id="A2.SS1.p2.1.m1.4.4.3.3.3" xref="A2.SS1.p2.1.m1.4.4.3.3.3.cmml">n</mi></msub><mo id="A2.SS1.p2.1.m1.4.4.3.8" stretchy="false" xref="A2.SS1.p2.1.m1.4.4.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.1.m1.4b"><vector id="A2.SS1.p2.1.m1.4.4.4.cmml" xref="A2.SS1.p2.1.m1.4.4.3"><apply id="A2.SS1.p2.1.m1.2.2.1.1.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="A2.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="A2.SS1.p2.1.m1.2.2.1.1.2">𝑙</ci><cn id="A2.SS1.p2.1.m1.2.2.1.1.3.cmml" type="integer" xref="A2.SS1.p2.1.m1.2.2.1.1.3">0</cn></apply><apply id="A2.SS1.p2.1.m1.3.3.2.2.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.3.3.2.2.1.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="A2.SS1.p2.1.m1.3.3.2.2.2.cmml" xref="A2.SS1.p2.1.m1.3.3.2.2.2">𝑙</ci><cn id="A2.SS1.p2.1.m1.3.3.2.2.3.cmml" type="integer" xref="A2.SS1.p2.1.m1.3.3.2.2.3">1</cn></apply><ci id="A2.SS1.p2.1.m1.1.1.cmml" xref="A2.SS1.p2.1.m1.1.1">…</ci><apply id="A2.SS1.p2.1.m1.4.4.3.3.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.4.4.3.3.1.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3">subscript</csymbol><ci id="A2.SS1.p2.1.m1.4.4.3.3.2.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3.2">𝑙</ci><ci id="A2.SS1.p2.1.m1.4.4.3.3.3.cmml" xref="A2.SS1.p2.1.m1.4.4.3.3.3">𝑛</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.1.m1.4c">(l_{0},l_{1},...,l_{n})</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.1.m1.4d">( italic_l start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_l start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>. Our goal is to minimize the sum of the squares of the differences between each data point and its linear predictive value:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.SS1.p3">
<table class="ltx_equation ltx_eqn_table" id="A2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f(a,b)=\text{minimize}\sum_{i=0}^{n}(l_{i}-(ax_{i}+b))^{2}," class="ltx_Math" display="block" id="A2.E6.m1.3"><semantics id="A2.E6.m1.3a"><mrow id="A2.E6.m1.3.3.1" xref="A2.E6.m1.3.3.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1" xref="A2.E6.m1.3.3.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.3" xref="A2.E6.m1.3.3.1.1.3.cmml"><mi id="A2.E6.m1.3.3.1.1.3.2" xref="A2.E6.m1.3.3.1.1.3.2.cmml">f</mi><mo id="A2.E6.m1.3.3.1.1.3.1" xref="A2.E6.m1.3.3.1.1.3.1.cmml">⁢</mo><mrow id="A2.E6.m1.3.3.1.1.3.3.2" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml"><mo id="A2.E6.m1.3.3.1.1.3.3.2.1" stretchy="false" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">(</mo><mi id="A2.E6.m1.1.1" xref="A2.E6.m1.1.1.cmml">a</mi><mo id="A2.E6.m1.3.3.1.1.3.3.2.2" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">,</mo><mi id="A2.E6.m1.2.2" xref="A2.E6.m1.2.2.cmml">b</mi><mo id="A2.E6.m1.3.3.1.1.3.3.2.3" stretchy="false" xref="A2.E6.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="A2.E6.m1.3.3.1.1.2" xref="A2.E6.m1.3.3.1.1.2.cmml">=</mo><mrow id="A2.E6.m1.3.3.1.1.1" xref="A2.E6.m1.3.3.1.1.1.cmml"><mtext id="A2.E6.m1.3.3.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.3a.cmml">minimize</mtext><mo id="A2.E6.m1.3.3.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="A2.E6.m1.3.3.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.cmml"><munderover id="A2.E6.m1.3.3.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.2.cmml"><mo id="A2.E6.m1.3.3.1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="A2.E6.m1.3.3.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.2.2.3" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.2.2.3.2" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="A2.E6.m1.3.3.1.1.1.1.2.2.3.1" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="A2.E6.m1.3.3.1.1.1.1.2.2.3.3" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mi id="A2.E6.m1.3.3.1.1.1.1.2.3" xref="A2.E6.m1.3.3.1.1.1.1.2.3.cmml">n</mi></munderover><msup id="A2.E6.m1.3.3.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.2" stretchy="false" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml"><msub id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml">l</mi><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">a</mi><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">x</mi><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">b</mi></mrow><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A2.E6.m1.3.3.1.1.1.1.1.1.1.3" stretchy="false" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="A2.E6.m1.3.3.1.1.1.1.1.3" xref="A2.E6.m1.3.3.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><mo id="A2.E6.m1.3.3.1.2" xref="A2.E6.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E6.m1.3b"><apply id="A2.E6.m1.3.3.1.1.cmml" xref="A2.E6.m1.3.3.1"><eq id="A2.E6.m1.3.3.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.2"></eq><apply id="A2.E6.m1.3.3.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.3"><times id="A2.E6.m1.3.3.1.1.3.1.cmml" xref="A2.E6.m1.3.3.1.1.3.1"></times><ci id="A2.E6.m1.3.3.1.1.3.2.cmml" xref="A2.E6.m1.3.3.1.1.3.2">𝑓</ci><interval closure="open" id="A2.E6.m1.3.3.1.1.3.3.1.cmml" xref="A2.E6.m1.3.3.1.1.3.3.2"><ci id="A2.E6.m1.1.1.cmml" xref="A2.E6.m1.1.1">𝑎</ci><ci id="A2.E6.m1.2.2.cmml" xref="A2.E6.m1.2.2">𝑏</ci></interval></apply><apply id="A2.E6.m1.3.3.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1"><times id="A2.E6.m1.3.3.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.2"></times><ci id="A2.E6.m1.3.3.1.1.1.3a.cmml" xref="A2.E6.m1.3.3.1.1.1.3"><mtext id="A2.E6.m1.3.3.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.3">minimize</mtext></ci><apply id="A2.E6.m1.3.3.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1"><apply id="A2.E6.m1.3.3.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2">superscript</csymbol><apply id="A2.E6.m1.3.3.1.1.1.1.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.2.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="A2.E6.m1.3.3.1.1.1.1.2.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.2"></sum><apply id="A2.E6.m1.3.3.1.1.1.1.2.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3"><eq id="A2.E6.m1.3.3.1.1.1.1.2.2.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.1"></eq><ci id="A2.E6.m1.3.3.1.1.1.1.2.2.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.2">𝑖</ci><cn id="A2.E6.m1.3.3.1.1.1.1.2.2.3.3.cmml" type="integer" xref="A2.E6.m1.3.3.1.1.1.1.2.2.3.3">0</cn></apply></apply><ci id="A2.E6.m1.3.3.1.1.1.1.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.2.3">𝑛</ci></apply><apply id="A2.E6.m1.3.3.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1">superscript</csymbol><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1"><minus id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.2"></minus><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.2">𝑙</ci><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1"><plus id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2"><times id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1"></times><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑎</ci><apply id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.2">𝑥</ci><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><ci id="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3">𝑏</ci></apply></apply><cn id="A2.E6.m1.3.3.1.1.1.1.1.3.cmml" type="integer" xref="A2.E6.m1.3.3.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E6.m1.3c">f(a,b)=\text{minimize}\sum_{i=0}^{n}(l_{i}-(ax_{i}+b))^{2},</annotation><annotation encoding="application/x-llamapun" id="A2.E6.m1.3d">italic_f ( italic_a , italic_b ) = minimize ∑ start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - ( italic_a italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_b ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A2.SS1.p4">
<p class="ltx_p" id="A2.SS1.p4.6">where <math alttext="x_{0}=0" class="ltx_Math" display="inline" id="A2.SS1.p4.1.m1.1"><semantics id="A2.SS1.p4.1.m1.1a"><mrow id="A2.SS1.p4.1.m1.1.1" xref="A2.SS1.p4.1.m1.1.1.cmml"><msub id="A2.SS1.p4.1.m1.1.1.2" xref="A2.SS1.p4.1.m1.1.1.2.cmml"><mi id="A2.SS1.p4.1.m1.1.1.2.2" xref="A2.SS1.p4.1.m1.1.1.2.2.cmml">x</mi><mn id="A2.SS1.p4.1.m1.1.1.2.3" xref="A2.SS1.p4.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="A2.SS1.p4.1.m1.1.1.1" xref="A2.SS1.p4.1.m1.1.1.1.cmml">=</mo><mn id="A2.SS1.p4.1.m1.1.1.3" xref="A2.SS1.p4.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.1.m1.1b"><apply id="A2.SS1.p4.1.m1.1.1.cmml" xref="A2.SS1.p4.1.m1.1.1"><eq id="A2.SS1.p4.1.m1.1.1.1.cmml" xref="A2.SS1.p4.1.m1.1.1.1"></eq><apply id="A2.SS1.p4.1.m1.1.1.2.cmml" xref="A2.SS1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.1.m1.1.1.2.1.cmml" xref="A2.SS1.p4.1.m1.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.1.m1.1.1.2.2.cmml" xref="A2.SS1.p4.1.m1.1.1.2.2">𝑥</ci><cn id="A2.SS1.p4.1.m1.1.1.2.3.cmml" type="integer" xref="A2.SS1.p4.1.m1.1.1.2.3">0</cn></apply><cn id="A2.SS1.p4.1.m1.1.1.3.cmml" type="integer" xref="A2.SS1.p4.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.1.m1.1c">x_{0}=0</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.1.m1.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0</annotation></semantics></math> is the initial checkpoint and <math alttext="x_{n}=n" class="ltx_Math" display="inline" id="A2.SS1.p4.2.m2.1"><semantics id="A2.SS1.p4.2.m2.1a"><mrow id="A2.SS1.p4.2.m2.1.1" xref="A2.SS1.p4.2.m2.1.1.cmml"><msub id="A2.SS1.p4.2.m2.1.1.2" xref="A2.SS1.p4.2.m2.1.1.2.cmml"><mi id="A2.SS1.p4.2.m2.1.1.2.2" xref="A2.SS1.p4.2.m2.1.1.2.2.cmml">x</mi><mi id="A2.SS1.p4.2.m2.1.1.2.3" xref="A2.SS1.p4.2.m2.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p4.2.m2.1.1.1" xref="A2.SS1.p4.2.m2.1.1.1.cmml">=</mo><mi id="A2.SS1.p4.2.m2.1.1.3" xref="A2.SS1.p4.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.2.m2.1b"><apply id="A2.SS1.p4.2.m2.1.1.cmml" xref="A2.SS1.p4.2.m2.1.1"><eq id="A2.SS1.p4.2.m2.1.1.1.cmml" xref="A2.SS1.p4.2.m2.1.1.1"></eq><apply id="A2.SS1.p4.2.m2.1.1.2.cmml" xref="A2.SS1.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.2.m2.1.1.2.1.cmml" xref="A2.SS1.p4.2.m2.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.2.m2.1.1.2.2.cmml" xref="A2.SS1.p4.2.m2.1.1.2.2">𝑥</ci><ci id="A2.SS1.p4.2.m2.1.1.2.3.cmml" xref="A2.SS1.p4.2.m2.1.1.2.3">𝑛</ci></apply><ci id="A2.SS1.p4.2.m2.1.1.3.cmml" xref="A2.SS1.p4.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.2.m2.1c">x_{n}=n</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_n</annotation></semantics></math> is the final checkpoint. Substituting these into the fitted equation, we can obtain the Loss values at the start and end after fitting: <math alttext="\mathcal{L}_{\text{start}}=b" class="ltx_Math" display="inline" id="A2.SS1.p4.3.m3.1"><semantics id="A2.SS1.p4.3.m3.1a"><mrow id="A2.SS1.p4.3.m3.1.1" xref="A2.SS1.p4.3.m3.1.1.cmml"><msub id="A2.SS1.p4.3.m3.1.1.2" xref="A2.SS1.p4.3.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.3.m3.1.1.2.2" xref="A2.SS1.p4.3.m3.1.1.2.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.3.m3.1.1.2.3" xref="A2.SS1.p4.3.m3.1.1.2.3a.cmml">start</mtext></msub><mo id="A2.SS1.p4.3.m3.1.1.1" xref="A2.SS1.p4.3.m3.1.1.1.cmml">=</mo><mi id="A2.SS1.p4.3.m3.1.1.3" xref="A2.SS1.p4.3.m3.1.1.3.cmml">b</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.3.m3.1b"><apply id="A2.SS1.p4.3.m3.1.1.cmml" xref="A2.SS1.p4.3.m3.1.1"><eq id="A2.SS1.p4.3.m3.1.1.1.cmml" xref="A2.SS1.p4.3.m3.1.1.1"></eq><apply id="A2.SS1.p4.3.m3.1.1.2.cmml" xref="A2.SS1.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.3.m3.1.1.2.1.cmml" xref="A2.SS1.p4.3.m3.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.3.m3.1.1.2.2.cmml" xref="A2.SS1.p4.3.m3.1.1.2.2">ℒ</ci><ci id="A2.SS1.p4.3.m3.1.1.2.3a.cmml" xref="A2.SS1.p4.3.m3.1.1.2.3"><mtext id="A2.SS1.p4.3.m3.1.1.2.3.cmml" mathsize="70%" xref="A2.SS1.p4.3.m3.1.1.2.3">start</mtext></ci></apply><ci id="A2.SS1.p4.3.m3.1.1.3.cmml" xref="A2.SS1.p4.3.m3.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.3.m3.1c">\mathcal{L}_{\text{start}}=b</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT start end_POSTSUBSCRIPT = italic_b</annotation></semantics></math> and <math alttext="\mathcal{L}_{\text{end}}=an+b" class="ltx_Math" display="inline" id="A2.SS1.p4.4.m4.1"><semantics id="A2.SS1.p4.4.m4.1a"><mrow id="A2.SS1.p4.4.m4.1.1" xref="A2.SS1.p4.4.m4.1.1.cmml"><msub id="A2.SS1.p4.4.m4.1.1.2" xref="A2.SS1.p4.4.m4.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.4.m4.1.1.2.2" xref="A2.SS1.p4.4.m4.1.1.2.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.4.m4.1.1.2.3" xref="A2.SS1.p4.4.m4.1.1.2.3a.cmml">end</mtext></msub><mo id="A2.SS1.p4.4.m4.1.1.1" xref="A2.SS1.p4.4.m4.1.1.1.cmml">=</mo><mrow id="A2.SS1.p4.4.m4.1.1.3" xref="A2.SS1.p4.4.m4.1.1.3.cmml"><mrow id="A2.SS1.p4.4.m4.1.1.3.2" xref="A2.SS1.p4.4.m4.1.1.3.2.cmml"><mi id="A2.SS1.p4.4.m4.1.1.3.2.2" xref="A2.SS1.p4.4.m4.1.1.3.2.2.cmml">a</mi><mo id="A2.SS1.p4.4.m4.1.1.3.2.1" xref="A2.SS1.p4.4.m4.1.1.3.2.1.cmml">⁢</mo><mi id="A2.SS1.p4.4.m4.1.1.3.2.3" xref="A2.SS1.p4.4.m4.1.1.3.2.3.cmml">n</mi></mrow><mo id="A2.SS1.p4.4.m4.1.1.3.1" xref="A2.SS1.p4.4.m4.1.1.3.1.cmml">+</mo><mi id="A2.SS1.p4.4.m4.1.1.3.3" xref="A2.SS1.p4.4.m4.1.1.3.3.cmml">b</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.4.m4.1b"><apply id="A2.SS1.p4.4.m4.1.1.cmml" xref="A2.SS1.p4.4.m4.1.1"><eq id="A2.SS1.p4.4.m4.1.1.1.cmml" xref="A2.SS1.p4.4.m4.1.1.1"></eq><apply id="A2.SS1.p4.4.m4.1.1.2.cmml" xref="A2.SS1.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p4.4.m4.1.1.2.1.cmml" xref="A2.SS1.p4.4.m4.1.1.2">subscript</csymbol><ci id="A2.SS1.p4.4.m4.1.1.2.2.cmml" xref="A2.SS1.p4.4.m4.1.1.2.2">ℒ</ci><ci id="A2.SS1.p4.4.m4.1.1.2.3a.cmml" xref="A2.SS1.p4.4.m4.1.1.2.3"><mtext id="A2.SS1.p4.4.m4.1.1.2.3.cmml" mathsize="70%" xref="A2.SS1.p4.4.m4.1.1.2.3">end</mtext></ci></apply><apply id="A2.SS1.p4.4.m4.1.1.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3"><plus id="A2.SS1.p4.4.m4.1.1.3.1.cmml" xref="A2.SS1.p4.4.m4.1.1.3.1"></plus><apply id="A2.SS1.p4.4.m4.1.1.3.2.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2"><times id="A2.SS1.p4.4.m4.1.1.3.2.1.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.1"></times><ci id="A2.SS1.p4.4.m4.1.1.3.2.2.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.2">𝑎</ci><ci id="A2.SS1.p4.4.m4.1.1.3.2.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3.2.3">𝑛</ci></apply><ci id="A2.SS1.p4.4.m4.1.1.3.3.cmml" xref="A2.SS1.p4.4.m4.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.4.m4.1c">\mathcal{L}_{\text{end}}=an+b</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.4.m4.1d">caligraphic_L start_POSTSUBSCRIPT end end_POSTSUBSCRIPT = italic_a italic_n + italic_b</annotation></semantics></math>. The change in loss can then be expressed as: <math alttext="\Delta\mathcal{L}=\mathcal{L}_{\text{end}}-\mathcal{L}_{\text{start}}" class="ltx_Math" display="inline" id="A2.SS1.p4.5.m5.1"><semantics id="A2.SS1.p4.5.m5.1a"><mrow id="A2.SS1.p4.5.m5.1.1" xref="A2.SS1.p4.5.m5.1.1.cmml"><mrow id="A2.SS1.p4.5.m5.1.1.2" xref="A2.SS1.p4.5.m5.1.1.2.cmml"><mi id="A2.SS1.p4.5.m5.1.1.2.2" mathvariant="normal" xref="A2.SS1.p4.5.m5.1.1.2.2.cmml">Δ</mi><mo id="A2.SS1.p4.5.m5.1.1.2.1" xref="A2.SS1.p4.5.m5.1.1.2.1.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.2.3" xref="A2.SS1.p4.5.m5.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p4.5.m5.1.1.1" xref="A2.SS1.p4.5.m5.1.1.1.cmml">=</mo><mrow id="A2.SS1.p4.5.m5.1.1.3" xref="A2.SS1.p4.5.m5.1.1.3.cmml"><msub id="A2.SS1.p4.5.m5.1.1.3.2" xref="A2.SS1.p4.5.m5.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.3.2.2" xref="A2.SS1.p4.5.m5.1.1.3.2.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.5.m5.1.1.3.2.3" xref="A2.SS1.p4.5.m5.1.1.3.2.3a.cmml">end</mtext></msub><mo id="A2.SS1.p4.5.m5.1.1.3.1" xref="A2.SS1.p4.5.m5.1.1.3.1.cmml">−</mo><msub id="A2.SS1.p4.5.m5.1.1.3.3" xref="A2.SS1.p4.5.m5.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.5.m5.1.1.3.3.2" xref="A2.SS1.p4.5.m5.1.1.3.3.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.5.m5.1.1.3.3.3" xref="A2.SS1.p4.5.m5.1.1.3.3.3a.cmml">start</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.5.m5.1b"><apply id="A2.SS1.p4.5.m5.1.1.cmml" xref="A2.SS1.p4.5.m5.1.1"><eq id="A2.SS1.p4.5.m5.1.1.1.cmml" xref="A2.SS1.p4.5.m5.1.1.1"></eq><apply id="A2.SS1.p4.5.m5.1.1.2.cmml" xref="A2.SS1.p4.5.m5.1.1.2"><times id="A2.SS1.p4.5.m5.1.1.2.1.cmml" xref="A2.SS1.p4.5.m5.1.1.2.1"></times><ci id="A2.SS1.p4.5.m5.1.1.2.2.cmml" xref="A2.SS1.p4.5.m5.1.1.2.2">Δ</ci><ci id="A2.SS1.p4.5.m5.1.1.2.3.cmml" xref="A2.SS1.p4.5.m5.1.1.2.3">ℒ</ci></apply><apply id="A2.SS1.p4.5.m5.1.1.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3"><minus id="A2.SS1.p4.5.m5.1.1.3.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.1"></minus><apply id="A2.SS1.p4.5.m5.1.1.3.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="A2.SS1.p4.5.m5.1.1.3.2.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2">subscript</csymbol><ci id="A2.SS1.p4.5.m5.1.1.3.2.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2.2">ℒ</ci><ci id="A2.SS1.p4.5.m5.1.1.3.2.3a.cmml" xref="A2.SS1.p4.5.m5.1.1.3.2.3"><mtext id="A2.SS1.p4.5.m5.1.1.3.2.3.cmml" mathsize="70%" xref="A2.SS1.p4.5.m5.1.1.3.2.3">end</mtext></ci></apply><apply id="A2.SS1.p4.5.m5.1.1.3.3.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="A2.SS1.p4.5.m5.1.1.3.3.1.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3">subscript</csymbol><ci id="A2.SS1.p4.5.m5.1.1.3.3.2.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3.2">ℒ</ci><ci id="A2.SS1.p4.5.m5.1.1.3.3.3a.cmml" xref="A2.SS1.p4.5.m5.1.1.3.3.3"><mtext id="A2.SS1.p4.5.m5.1.1.3.3.3.cmml" mathsize="70%" xref="A2.SS1.p4.5.m5.1.1.3.3.3">start</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.5.m5.1c">\Delta\mathcal{L}=\mathcal{L}_{\text{end}}-\mathcal{L}_{\text{start}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.5.m5.1d">roman_Δ caligraphic_L = caligraphic_L start_POSTSUBSCRIPT end end_POSTSUBSCRIPT - caligraphic_L start_POSTSUBSCRIPT start end_POSTSUBSCRIPT</annotation></semantics></math>. Meanwhile, we represent the average Loss of the last checkpoint as <math alttext="\mathcal{L}_{\text{mean}}" class="ltx_Math" display="inline" id="A2.SS1.p4.6.m6.1"><semantics id="A2.SS1.p4.6.m6.1a"><msub id="A2.SS1.p4.6.m6.1.1" xref="A2.SS1.p4.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p4.6.m6.1.1.2" xref="A2.SS1.p4.6.m6.1.1.2.cmml">ℒ</mi><mtext id="A2.SS1.p4.6.m6.1.1.3" xref="A2.SS1.p4.6.m6.1.1.3a.cmml">mean</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p4.6.m6.1b"><apply id="A2.SS1.p4.6.m6.1.1.cmml" xref="A2.SS1.p4.6.m6.1.1"><csymbol cd="ambiguous" id="A2.SS1.p4.6.m6.1.1.1.cmml" xref="A2.SS1.p4.6.m6.1.1">subscript</csymbol><ci id="A2.SS1.p4.6.m6.1.1.2.cmml" xref="A2.SS1.p4.6.m6.1.1.2">ℒ</ci><ci id="A2.SS1.p4.6.m6.1.1.3a.cmml" xref="A2.SS1.p4.6.m6.1.1.3"><mtext id="A2.SS1.p4.6.m6.1.1.3.cmml" mathsize="70%" xref="A2.SS1.p4.6.m6.1.1.3">mean</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p4.6.m6.1c">\mathcal{L}_{\text{mean}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p4.6.m6.1d">caligraphic_L start_POSTSUBSCRIPT mean end_POSTSUBSCRIPT</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.SS1.p5">
<p class="ltx_p" id="A2.SS1.p5.7">Next, we can classify the tokens based on <math alttext="\Delta\mathcal{L}" class="ltx_Math" display="inline" id="A2.SS1.p5.1.m1.1"><semantics id="A2.SS1.p5.1.m1.1a"><mrow id="A2.SS1.p5.1.m1.1.1" xref="A2.SS1.p5.1.m1.1.1.cmml"><mi id="A2.SS1.p5.1.m1.1.1.2" mathvariant="normal" xref="A2.SS1.p5.1.m1.1.1.2.cmml">Δ</mi><mo id="A2.SS1.p5.1.m1.1.1.1" xref="A2.SS1.p5.1.m1.1.1.1.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.1.m1.1.1.3" xref="A2.SS1.p5.1.m1.1.1.3.cmml">ℒ</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.1.m1.1b"><apply id="A2.SS1.p5.1.m1.1.1.cmml" xref="A2.SS1.p5.1.m1.1.1"><times id="A2.SS1.p5.1.m1.1.1.1.cmml" xref="A2.SS1.p5.1.m1.1.1.1"></times><ci id="A2.SS1.p5.1.m1.1.1.2.cmml" xref="A2.SS1.p5.1.m1.1.1.2">Δ</ci><ci id="A2.SS1.p5.1.m1.1.1.3.cmml" xref="A2.SS1.p5.1.m1.1.1.3">ℒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.1.m1.1c">\Delta\mathcal{L}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.1.m1.1d">roman_Δ caligraphic_L</annotation></semantics></math> and the <math alttext="\mathcal{L}_{\text{mean}}" class="ltx_Math" display="inline" id="A2.SS1.p5.2.m2.1"><semantics id="A2.SS1.p5.2.m2.1a"><msub id="A2.SS1.p5.2.m2.1.1" xref="A2.SS1.p5.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.2.m2.1.1.2" xref="A2.SS1.p5.2.m2.1.1.2.cmml">ℒ</mi><mtext id="A2.SS1.p5.2.m2.1.1.3" xref="A2.SS1.p5.2.m2.1.1.3a.cmml">mean</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.2.m2.1b"><apply id="A2.SS1.p5.2.m2.1.1.cmml" xref="A2.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS1.p5.2.m2.1.1.1.cmml" xref="A2.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="A2.SS1.p5.2.m2.1.1.2.cmml" xref="A2.SS1.p5.2.m2.1.1.2">ℒ</ci><ci id="A2.SS1.p5.2.m2.1.1.3a.cmml" xref="A2.SS1.p5.2.m2.1.1.3"><mtext id="A2.SS1.p5.2.m2.1.1.3.cmml" mathsize="70%" xref="A2.SS1.p5.2.m2.1.1.3">mean</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.2.m2.1c">\mathcal{L}_{\text{mean}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT mean end_POSTSUBSCRIPT</annotation></semantics></math>. We categorize tokens with <math alttext="\Delta\mathcal{L}<-0.2" class="ltx_Math" display="inline" id="A2.SS1.p5.3.m3.1"><semantics id="A2.SS1.p5.3.m3.1a"><mrow id="A2.SS1.p5.3.m3.1.1" xref="A2.SS1.p5.3.m3.1.1.cmml"><mrow id="A2.SS1.p5.3.m3.1.1.2" xref="A2.SS1.p5.3.m3.1.1.2.cmml"><mi id="A2.SS1.p5.3.m3.1.1.2.2" mathvariant="normal" xref="A2.SS1.p5.3.m3.1.1.2.2.cmml">Δ</mi><mo id="A2.SS1.p5.3.m3.1.1.2.1" xref="A2.SS1.p5.3.m3.1.1.2.1.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.3.m3.1.1.2.3" xref="A2.SS1.p5.3.m3.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p5.3.m3.1.1.1" xref="A2.SS1.p5.3.m3.1.1.1.cmml">&lt;</mo><mrow id="A2.SS1.p5.3.m3.1.1.3" xref="A2.SS1.p5.3.m3.1.1.3.cmml"><mo id="A2.SS1.p5.3.m3.1.1.3a" xref="A2.SS1.p5.3.m3.1.1.3.cmml">−</mo><mn id="A2.SS1.p5.3.m3.1.1.3.2" xref="A2.SS1.p5.3.m3.1.1.3.2.cmml">0.2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.3.m3.1b"><apply id="A2.SS1.p5.3.m3.1.1.cmml" xref="A2.SS1.p5.3.m3.1.1"><lt id="A2.SS1.p5.3.m3.1.1.1.cmml" xref="A2.SS1.p5.3.m3.1.1.1"></lt><apply id="A2.SS1.p5.3.m3.1.1.2.cmml" xref="A2.SS1.p5.3.m3.1.1.2"><times id="A2.SS1.p5.3.m3.1.1.2.1.cmml" xref="A2.SS1.p5.3.m3.1.1.2.1"></times><ci id="A2.SS1.p5.3.m3.1.1.2.2.cmml" xref="A2.SS1.p5.3.m3.1.1.2.2">Δ</ci><ci id="A2.SS1.p5.3.m3.1.1.2.3.cmml" xref="A2.SS1.p5.3.m3.1.1.2.3">ℒ</ci></apply><apply id="A2.SS1.p5.3.m3.1.1.3.cmml" xref="A2.SS1.p5.3.m3.1.1.3"><minus id="A2.SS1.p5.3.m3.1.1.3.1.cmml" xref="A2.SS1.p5.3.m3.1.1.3"></minus><cn id="A2.SS1.p5.3.m3.1.1.3.2.cmml" type="float" xref="A2.SS1.p5.3.m3.1.1.3.2">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.3.m3.1c">\Delta\mathcal{L}&lt;-0.2</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.3.m3.1d">roman_Δ caligraphic_L &lt; - 0.2</annotation></semantics></math> as H→L (loss decreases from high to low) category tokens, and tokens with <math alttext="\Delta\mathcal{L}>0.2" class="ltx_Math" display="inline" id="A2.SS1.p5.4.m4.1"><semantics id="A2.SS1.p5.4.m4.1a"><mrow id="A2.SS1.p5.4.m4.1.1" xref="A2.SS1.p5.4.m4.1.1.cmml"><mrow id="A2.SS1.p5.4.m4.1.1.2" xref="A2.SS1.p5.4.m4.1.1.2.cmml"><mi id="A2.SS1.p5.4.m4.1.1.2.2" mathvariant="normal" xref="A2.SS1.p5.4.m4.1.1.2.2.cmml">Δ</mi><mo id="A2.SS1.p5.4.m4.1.1.2.1" xref="A2.SS1.p5.4.m4.1.1.2.1.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.4.m4.1.1.2.3" xref="A2.SS1.p5.4.m4.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p5.4.m4.1.1.1" xref="A2.SS1.p5.4.m4.1.1.1.cmml">&gt;</mo><mn id="A2.SS1.p5.4.m4.1.1.3" xref="A2.SS1.p5.4.m4.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.4.m4.1b"><apply id="A2.SS1.p5.4.m4.1.1.cmml" xref="A2.SS1.p5.4.m4.1.1"><gt id="A2.SS1.p5.4.m4.1.1.1.cmml" xref="A2.SS1.p5.4.m4.1.1.1"></gt><apply id="A2.SS1.p5.4.m4.1.1.2.cmml" xref="A2.SS1.p5.4.m4.1.1.2"><times id="A2.SS1.p5.4.m4.1.1.2.1.cmml" xref="A2.SS1.p5.4.m4.1.1.2.1"></times><ci id="A2.SS1.p5.4.m4.1.1.2.2.cmml" xref="A2.SS1.p5.4.m4.1.1.2.2">Δ</ci><ci id="A2.SS1.p5.4.m4.1.1.2.3.cmml" xref="A2.SS1.p5.4.m4.1.1.2.3">ℒ</ci></apply><cn id="A2.SS1.p5.4.m4.1.1.3.cmml" type="float" xref="A2.SS1.p5.4.m4.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.4.m4.1c">\Delta\mathcal{L}&gt;0.2</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.4.m4.1d">roman_Δ caligraphic_L &gt; 0.2</annotation></semantics></math> as L→H (loss increases from low to high) category tokens. If <math alttext="-0.2\leq\Delta\mathcal{L}\leq 0.2" class="ltx_Math" display="inline" id="A2.SS1.p5.5.m5.1"><semantics id="A2.SS1.p5.5.m5.1a"><mrow id="A2.SS1.p5.5.m5.1.1" xref="A2.SS1.p5.5.m5.1.1.cmml"><mrow id="A2.SS1.p5.5.m5.1.1.2" xref="A2.SS1.p5.5.m5.1.1.2.cmml"><mo id="A2.SS1.p5.5.m5.1.1.2a" xref="A2.SS1.p5.5.m5.1.1.2.cmml">−</mo><mn id="A2.SS1.p5.5.m5.1.1.2.2" xref="A2.SS1.p5.5.m5.1.1.2.2.cmml">0.2</mn></mrow><mo id="A2.SS1.p5.5.m5.1.1.3" xref="A2.SS1.p5.5.m5.1.1.3.cmml">≤</mo><mrow id="A2.SS1.p5.5.m5.1.1.4" xref="A2.SS1.p5.5.m5.1.1.4.cmml"><mi id="A2.SS1.p5.5.m5.1.1.4.2" mathvariant="normal" xref="A2.SS1.p5.5.m5.1.1.4.2.cmml">Δ</mi><mo id="A2.SS1.p5.5.m5.1.1.4.1" xref="A2.SS1.p5.5.m5.1.1.4.1.cmml">⁢</mo><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.5.m5.1.1.4.3" xref="A2.SS1.p5.5.m5.1.1.4.3.cmml">ℒ</mi></mrow><mo id="A2.SS1.p5.5.m5.1.1.5" xref="A2.SS1.p5.5.m5.1.1.5.cmml">≤</mo><mn id="A2.SS1.p5.5.m5.1.1.6" xref="A2.SS1.p5.5.m5.1.1.6.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.5.m5.1b"><apply id="A2.SS1.p5.5.m5.1.1.cmml" xref="A2.SS1.p5.5.m5.1.1"><and id="A2.SS1.p5.5.m5.1.1a.cmml" xref="A2.SS1.p5.5.m5.1.1"></and><apply id="A2.SS1.p5.5.m5.1.1b.cmml" xref="A2.SS1.p5.5.m5.1.1"><leq id="A2.SS1.p5.5.m5.1.1.3.cmml" xref="A2.SS1.p5.5.m5.1.1.3"></leq><apply id="A2.SS1.p5.5.m5.1.1.2.cmml" xref="A2.SS1.p5.5.m5.1.1.2"><minus id="A2.SS1.p5.5.m5.1.1.2.1.cmml" xref="A2.SS1.p5.5.m5.1.1.2"></minus><cn id="A2.SS1.p5.5.m5.1.1.2.2.cmml" type="float" xref="A2.SS1.p5.5.m5.1.1.2.2">0.2</cn></apply><apply id="A2.SS1.p5.5.m5.1.1.4.cmml" xref="A2.SS1.p5.5.m5.1.1.4"><times id="A2.SS1.p5.5.m5.1.1.4.1.cmml" xref="A2.SS1.p5.5.m5.1.1.4.1"></times><ci id="A2.SS1.p5.5.m5.1.1.4.2.cmml" xref="A2.SS1.p5.5.m5.1.1.4.2">Δ</ci><ci id="A2.SS1.p5.5.m5.1.1.4.3.cmml" xref="A2.SS1.p5.5.m5.1.1.4.3">ℒ</ci></apply></apply><apply id="A2.SS1.p5.5.m5.1.1c.cmml" xref="A2.SS1.p5.5.m5.1.1"><leq id="A2.SS1.p5.5.m5.1.1.5.cmml" xref="A2.SS1.p5.5.m5.1.1.5"></leq><share href="#A2.SS1.p5.5.m5.1.1.4.cmml" id="A2.SS1.p5.5.m5.1.1d.cmml" xref="A2.SS1.p5.5.m5.1.1"></share><cn id="A2.SS1.p5.5.m5.1.1.6.cmml" type="float" xref="A2.SS1.p5.5.m5.1.1.6">0.2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.5.m5.1c">-0.2\leq\Delta\mathcal{L}\leq 0.2</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.5.m5.1d">- 0.2 ≤ roman_Δ caligraphic_L ≤ 0.2</annotation></semantics></math> and <math alttext="l_{n}\leq\mathcal{L}_{\text{mean}}" class="ltx_Math" display="inline" id="A2.SS1.p5.6.m6.1"><semantics id="A2.SS1.p5.6.m6.1a"><mrow id="A2.SS1.p5.6.m6.1.1" xref="A2.SS1.p5.6.m6.1.1.cmml"><msub id="A2.SS1.p5.6.m6.1.1.2" xref="A2.SS1.p5.6.m6.1.1.2.cmml"><mi id="A2.SS1.p5.6.m6.1.1.2.2" xref="A2.SS1.p5.6.m6.1.1.2.2.cmml">l</mi><mi id="A2.SS1.p5.6.m6.1.1.2.3" xref="A2.SS1.p5.6.m6.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p5.6.m6.1.1.1" xref="A2.SS1.p5.6.m6.1.1.1.cmml">≤</mo><msub id="A2.SS1.p5.6.m6.1.1.3" xref="A2.SS1.p5.6.m6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.6.m6.1.1.3.2" xref="A2.SS1.p5.6.m6.1.1.3.2.cmml">ℒ</mi><mtext id="A2.SS1.p5.6.m6.1.1.3.3" xref="A2.SS1.p5.6.m6.1.1.3.3a.cmml">mean</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.6.m6.1b"><apply id="A2.SS1.p5.6.m6.1.1.cmml" xref="A2.SS1.p5.6.m6.1.1"><leq id="A2.SS1.p5.6.m6.1.1.1.cmml" xref="A2.SS1.p5.6.m6.1.1.1"></leq><apply id="A2.SS1.p5.6.m6.1.1.2.cmml" xref="A2.SS1.p5.6.m6.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p5.6.m6.1.1.2.1.cmml" xref="A2.SS1.p5.6.m6.1.1.2">subscript</csymbol><ci id="A2.SS1.p5.6.m6.1.1.2.2.cmml" xref="A2.SS1.p5.6.m6.1.1.2.2">𝑙</ci><ci id="A2.SS1.p5.6.m6.1.1.2.3.cmml" xref="A2.SS1.p5.6.m6.1.1.2.3">𝑛</ci></apply><apply id="A2.SS1.p5.6.m6.1.1.3.cmml" xref="A2.SS1.p5.6.m6.1.1.3"><csymbol cd="ambiguous" id="A2.SS1.p5.6.m6.1.1.3.1.cmml" xref="A2.SS1.p5.6.m6.1.1.3">subscript</csymbol><ci id="A2.SS1.p5.6.m6.1.1.3.2.cmml" xref="A2.SS1.p5.6.m6.1.1.3.2">ℒ</ci><ci id="A2.SS1.p5.6.m6.1.1.3.3a.cmml" xref="A2.SS1.p5.6.m6.1.1.3.3"><mtext id="A2.SS1.p5.6.m6.1.1.3.3.cmml" mathsize="70%" xref="A2.SS1.p5.6.m6.1.1.3.3">mean</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.6.m6.1c">l_{n}\leq\mathcal{L}_{\text{mean}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.6.m6.1d">italic_l start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ≤ caligraphic_L start_POSTSUBSCRIPT mean end_POSTSUBSCRIPT</annotation></semantics></math>, then tokens are classified as L→L (loss remains low); if <math alttext="l_{n}>\mathcal{L}_{\text{mean}}" class="ltx_Math" display="inline" id="A2.SS1.p5.7.m7.1"><semantics id="A2.SS1.p5.7.m7.1a"><mrow id="A2.SS1.p5.7.m7.1.1" xref="A2.SS1.p5.7.m7.1.1.cmml"><msub id="A2.SS1.p5.7.m7.1.1.2" xref="A2.SS1.p5.7.m7.1.1.2.cmml"><mi id="A2.SS1.p5.7.m7.1.1.2.2" xref="A2.SS1.p5.7.m7.1.1.2.2.cmml">l</mi><mi id="A2.SS1.p5.7.m7.1.1.2.3" xref="A2.SS1.p5.7.m7.1.1.2.3.cmml">n</mi></msub><mo id="A2.SS1.p5.7.m7.1.1.1" xref="A2.SS1.p5.7.m7.1.1.1.cmml">&gt;</mo><msub id="A2.SS1.p5.7.m7.1.1.3" xref="A2.SS1.p5.7.m7.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p5.7.m7.1.1.3.2" xref="A2.SS1.p5.7.m7.1.1.3.2.cmml">ℒ</mi><mtext id="A2.SS1.p5.7.m7.1.1.3.3" xref="A2.SS1.p5.7.m7.1.1.3.3a.cmml">mean</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p5.7.m7.1b"><apply id="A2.SS1.p5.7.m7.1.1.cmml" xref="A2.SS1.p5.7.m7.1.1"><gt id="A2.SS1.p5.7.m7.1.1.1.cmml" xref="A2.SS1.p5.7.m7.1.1.1"></gt><apply id="A2.SS1.p5.7.m7.1.1.2.cmml" xref="A2.SS1.p5.7.m7.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p5.7.m7.1.1.2.1.cmml" xref="A2.SS1.p5.7.m7.1.1.2">subscript</csymbol><ci id="A2.SS1.p5.7.m7.1.1.2.2.cmml" xref="A2.SS1.p5.7.m7.1.1.2.2">𝑙</ci><ci id="A2.SS1.p5.7.m7.1.1.2.3.cmml" xref="A2.SS1.p5.7.m7.1.1.2.3">𝑛</ci></apply><apply id="A2.SS1.p5.7.m7.1.1.3.cmml" xref="A2.SS1.p5.7.m7.1.1.3"><csymbol cd="ambiguous" id="A2.SS1.p5.7.m7.1.1.3.1.cmml" xref="A2.SS1.p5.7.m7.1.1.3">subscript</csymbol><ci id="A2.SS1.p5.7.m7.1.1.3.2.cmml" xref="A2.SS1.p5.7.m7.1.1.3.2">ℒ</ci><ci id="A2.SS1.p5.7.m7.1.1.3.3a.cmml" xref="A2.SS1.p5.7.m7.1.1.3.3"><mtext id="A2.SS1.p5.7.m7.1.1.3.3.cmml" mathsize="70%" xref="A2.SS1.p5.7.m7.1.1.3.3">mean</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p5.7.m7.1c">l_{n}&gt;\mathcal{L}_{\text{mean}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p5.7.m7.1d">italic_l start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT &gt; caligraphic_L start_POSTSUBSCRIPT mean end_POSTSUBSCRIPT</annotation></semantics></math>, they are classified as H→H (loss remains high). In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A2.F10" title="Figure 10 ‣ B.1 More Details of Four Categories Tokens ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;10</span></a>, we visualize examples of the four categories of tokens in actual text.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Non-Converging Tokens in Pretrainig</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A2.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="921" id="A2.F11.g1" src="x11.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F11.4.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text ltx_font_bold" id="A2.F11.5.2" style="font-size:90%;">An example of an abnormal state of token perplexity during pretrainig process.<span class="ltx_text ltx_font_medium" id="A2.F11.5.2.1"> The tokens highlighted in <span class="ltx_text" id="A2.F11.5.2.1.1" style="color:#FF8C00;">orange</span> represent tokens that were significant abnormalities during the pretraining process.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S2.SS1" title="2.1 Not All Tokens Are Equal: Training Dynamics of Token Loss ‣ 2 Selective Language Modeling ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">§2.1</span></a>, we mentioned that during the training process, only a minority of tokens belong to the H→L category. Among the remaining categories of H→H and L→L tokens, there are tokens that exhibit significant fluctuations during training. Furthermore, there are instances where H→L tokens are not effectively learned. Therefore, in our analysis, we specifically select those tokens from these categories that demonstrate considerable variability and distinct loss.
We visualize these tokens that exhibit abnormal behavior during the training process. As illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A2.F11" title="Figure 11 ‣ B.2 Non-Converging Tokens in Pretrainig ‣ Appendix B Analysis and Visualization of Tokens in Pretraining ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;11</span></a>, we find that the majority of these tokens originate from rather chaotic corpora. For instance, the corpora may include a mix of custom symbols, unintelligible gibberish, and information such as timetables and bibliographic references. Within a segment of normal text, there may also be fluctuations in the usage of common conjunctions, word suffixes, and punctuation marks. The latter may not necessarily be disastrous for training; in fact, it could represent a normal occurrence. However, if we can effectively mitigate the losses caused by the former, it might lead to more stable and efficient model training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Evalution Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Math Evalution</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">We conducted a comprehensive evaluation of the model across various math reasoning benchmarks, encompassing a range of difficulties from elementary to university level, multiple mathematical domains, and diverse question types including multiple-choice and open-ended questions.
Our benchmarks include GSM8k&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Cobbe et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib77" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, MATH <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib78" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, GSM-Hard <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib79" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, SVAMP&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Patel et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib80" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, ASDIV <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Miao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib81" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, MAWPS <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Koncel-Kedziorski et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib82" title=""><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>, TabMWP (TAB) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Lu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib83" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, MathQA (MQA) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Amini et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib84" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, MMLU-STEM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, and SAT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Azerbayev et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib32" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>General Evalution</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">In the evaluation of general domain, we followed the lm-evaluation-harness&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib86" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> and evalute model on MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hendrycks et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib85" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, BBH&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Suzgun et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib87" title=""><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, AGIEval&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhong et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib88" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite>, ARC-Easy and ARC-Challenge&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib89" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, BoolQ&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib90" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, PIQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bisk et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib91" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, Hellaswag&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zellers et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib92" title=""><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, WinoGrande&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Sakaguchi et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib93" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, OpenBookQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Mihaylov et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib94" title=""><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>.
On HumanEval&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zheng et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib95" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> and TydiQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Clark et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib96" title=""><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>, we follow the evaluation pipeline of open-instrcut&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Ivison et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib17" title=""><span class="ltx_text" style="font-size:90%;">2023</span></a>)</cite> and report Pass@1 and Pass@10 for HumanEval and F1 for TydiQA. For MBPP&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Austin et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib97" title=""><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> benchmark, we follow the evaluation pipeline of DeepSeek-Coder&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Guo et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib98" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>, and report Pass@1 and Pass@10.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Relate the Selected Tokens’ Loss to Downstream Task Performance</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">In this section, we declare the details about correlating the loss of selected tokens with the performance of downstream tasks. Concurrent study has explored similar methods to study the impact of scaling laws with the performance of models in downstream tasks&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Gadre et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.07965v1#bib.bib38" title=""><span class="ltx_text" style="font-size:90%;">2024</span></a>)</cite>. Our analysis here differs in that it aims to elucidate the relationship between the decrease/increase in loss for selected/unselected tokens and the model’s performance on downstream tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">We use the average accuracy of MATH and GSM8K as the standard for measuring downstream tasks performance of model. Based on the trend of data points in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#S3.F7" title="Figure 7 ‣ Selected Token Loss Aligns Better with Downstream Performance ‣ 3.4 Analysis ‣ 3 Experiments ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;7</span></a>, we propose the relationship between the average accuracy of downstream tasks and selected/unselected tokens’ loss,</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A4.p3">
<table class="ltx_equation ltx_eqn_table" id="A4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Acc(\mathcal{L})=\log(a*\mathcal{L}+c)" class="ltx_Math" display="block" id="A4.E7.m1.3"><semantics id="A4.E7.m1.3a"><mrow id="A4.E7.m1.3.3" xref="A4.E7.m1.3.3.cmml"><mrow id="A4.E7.m1.3.3.3" xref="A4.E7.m1.3.3.3.cmml"><mi id="A4.E7.m1.3.3.3.2" xref="A4.E7.m1.3.3.3.2.cmml">A</mi><mo id="A4.E7.m1.3.3.3.1" xref="A4.E7.m1.3.3.3.1.cmml">⁢</mo><mi id="A4.E7.m1.3.3.3.3" xref="A4.E7.m1.3.3.3.3.cmml">c</mi><mo id="A4.E7.m1.3.3.3.1a" xref="A4.E7.m1.3.3.3.1.cmml">⁢</mo><mi id="A4.E7.m1.3.3.3.4" xref="A4.E7.m1.3.3.3.4.cmml">c</mi><mo id="A4.E7.m1.3.3.3.1b" xref="A4.E7.m1.3.3.3.1.cmml">⁢</mo><mrow id="A4.E7.m1.3.3.3.5.2" xref="A4.E7.m1.3.3.3.cmml"><mo id="A4.E7.m1.3.3.3.5.2.1" stretchy="false" xref="A4.E7.m1.3.3.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="A4.E7.m1.1.1" xref="A4.E7.m1.1.1.cmml">ℒ</mi><mo id="A4.E7.m1.3.3.3.5.2.2" stretchy="false" xref="A4.E7.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="A4.E7.m1.3.3.2" xref="A4.E7.m1.3.3.2.cmml">=</mo><mrow id="A4.E7.m1.3.3.1.1" xref="A4.E7.m1.3.3.1.2.cmml"><mi id="A4.E7.m1.2.2" xref="A4.E7.m1.2.2.cmml">log</mi><mo id="A4.E7.m1.3.3.1.1a" xref="A4.E7.m1.3.3.1.2.cmml">⁡</mo><mrow id="A4.E7.m1.3.3.1.1.1" xref="A4.E7.m1.3.3.1.2.cmml"><mo id="A4.E7.m1.3.3.1.1.1.2" stretchy="false" xref="A4.E7.m1.3.3.1.2.cmml">(</mo><mrow id="A4.E7.m1.3.3.1.1.1.1" xref="A4.E7.m1.3.3.1.1.1.1.cmml"><mrow id="A4.E7.m1.3.3.1.1.1.1.2" xref="A4.E7.m1.3.3.1.1.1.1.2.cmml"><mi id="A4.E7.m1.3.3.1.1.1.1.2.2" xref="A4.E7.m1.3.3.1.1.1.1.2.2.cmml">a</mi><mo id="A4.E7.m1.3.3.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="A4.E7.m1.3.3.1.1.1.1.2.1.cmml">*</mo><mi class="ltx_font_mathcaligraphic" id="A4.E7.m1.3.3.1.1.1.1.2.3" xref="A4.E7.m1.3.3.1.1.1.1.2.3.cmml">ℒ</mi></mrow><mo id="A4.E7.m1.3.3.1.1.1.1.1" xref="A4.E7.m1.3.3.1.1.1.1.1.cmml">+</mo><mi id="A4.E7.m1.3.3.1.1.1.1.3" xref="A4.E7.m1.3.3.1.1.1.1.3.cmml">c</mi></mrow><mo id="A4.E7.m1.3.3.1.1.1.3" stretchy="false" xref="A4.E7.m1.3.3.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.E7.m1.3b"><apply id="A4.E7.m1.3.3.cmml" xref="A4.E7.m1.3.3"><eq id="A4.E7.m1.3.3.2.cmml" xref="A4.E7.m1.3.3.2"></eq><apply id="A4.E7.m1.3.3.3.cmml" xref="A4.E7.m1.3.3.3"><times id="A4.E7.m1.3.3.3.1.cmml" xref="A4.E7.m1.3.3.3.1"></times><ci id="A4.E7.m1.3.3.3.2.cmml" xref="A4.E7.m1.3.3.3.2">𝐴</ci><ci id="A4.E7.m1.3.3.3.3.cmml" xref="A4.E7.m1.3.3.3.3">𝑐</ci><ci id="A4.E7.m1.3.3.3.4.cmml" xref="A4.E7.m1.3.3.3.4">𝑐</ci><ci id="A4.E7.m1.1.1.cmml" xref="A4.E7.m1.1.1">ℒ</ci></apply><apply id="A4.E7.m1.3.3.1.2.cmml" xref="A4.E7.m1.3.3.1.1"><log id="A4.E7.m1.2.2.cmml" xref="A4.E7.m1.2.2"></log><apply id="A4.E7.m1.3.3.1.1.1.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1"><plus id="A4.E7.m1.3.3.1.1.1.1.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1.1"></plus><apply id="A4.E7.m1.3.3.1.1.1.1.2.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2"><times id="A4.E7.m1.3.3.1.1.1.1.2.1.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.1"></times><ci id="A4.E7.m1.3.3.1.1.1.1.2.2.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.2">𝑎</ci><ci id="A4.E7.m1.3.3.1.1.1.1.2.3.cmml" xref="A4.E7.m1.3.3.1.1.1.1.2.3">ℒ</ci></apply><ci id="A4.E7.m1.3.3.1.1.1.1.3.cmml" xref="A4.E7.m1.3.3.1.1.1.1.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.E7.m1.3c">Acc(\mathcal{L})=\log(a*\mathcal{L}+c)</annotation><annotation encoding="application/x-llamapun" id="A4.E7.m1.3d">italic_A italic_c italic_c ( caligraphic_L ) = roman_log ( italic_a * caligraphic_L + italic_c )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="A4.p4">
<p class="ltx_p" id="A4.p4.6">The parameters <math alttext="a" class="ltx_Math" display="inline" id="A4.p4.1.m1.1"><semantics id="A4.p4.1.m1.1a"><mi id="A4.p4.1.m1.1.1" xref="A4.p4.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="A4.p4.1.m1.1b"><ci id="A4.p4.1.m1.1.1.cmml" xref="A4.p4.1.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.1.m1.1c">a</annotation><annotation encoding="application/x-llamapun" id="A4.p4.1.m1.1d">italic_a</annotation></semantics></math> and <math alttext="c" class="ltx_Math" display="inline" id="A4.p4.2.m2.1"><semantics id="A4.p4.2.m2.1a"><mi id="A4.p4.2.m2.1.1" xref="A4.p4.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="A4.p4.2.m2.1b"><ci id="A4.p4.2.m2.1.1.cmml" xref="A4.p4.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.2.m2.1c">c</annotation><annotation encoding="application/x-llamapun" id="A4.p4.2.m2.1d">italic_c</annotation></semantics></math> are fitted from the data. If the loss of selected tokens <math alttext="\mathcal{L}_{s}" class="ltx_Math" display="inline" id="A4.p4.3.m3.1"><semantics id="A4.p4.3.m3.1a"><msub id="A4.p4.3.m3.1.1" xref="A4.p4.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.p4.3.m3.1.1.2" xref="A4.p4.3.m3.1.1.2.cmml">ℒ</mi><mi id="A4.p4.3.m3.1.1.3" xref="A4.p4.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p4.3.m3.1b"><apply id="A4.p4.3.m3.1.1.cmml" xref="A4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="A4.p4.3.m3.1.1.1.cmml" xref="A4.p4.3.m3.1.1">subscript</csymbol><ci id="A4.p4.3.m3.1.1.2.cmml" xref="A4.p4.3.m3.1.1.2">ℒ</ci><ci id="A4.p4.3.m3.1.1.3.cmml" xref="A4.p4.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.3.m3.1c">\mathcal{L}_{s}</annotation><annotation encoding="application/x-llamapun" id="A4.p4.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> is used for fitting, then <math alttext="a>0" class="ltx_Math" display="inline" id="A4.p4.4.m4.1"><semantics id="A4.p4.4.m4.1a"><mrow id="A4.p4.4.m4.1.1" xref="A4.p4.4.m4.1.1.cmml"><mi id="A4.p4.4.m4.1.1.2" xref="A4.p4.4.m4.1.1.2.cmml">a</mi><mo id="A4.p4.4.m4.1.1.1" xref="A4.p4.4.m4.1.1.1.cmml">&gt;</mo><mn id="A4.p4.4.m4.1.1.3" xref="A4.p4.4.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.4.m4.1b"><apply id="A4.p4.4.m4.1.1.cmml" xref="A4.p4.4.m4.1.1"><gt id="A4.p4.4.m4.1.1.1.cmml" xref="A4.p4.4.m4.1.1.1"></gt><ci id="A4.p4.4.m4.1.1.2.cmml" xref="A4.p4.4.m4.1.1.2">𝑎</ci><cn id="A4.p4.4.m4.1.1.3.cmml" type="integer" xref="A4.p4.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.4.m4.1c">a&gt;0</annotation><annotation encoding="application/x-llamapun" id="A4.p4.4.m4.1d">italic_a &gt; 0</annotation></semantics></math>. Conversely, if the loss of unselected tokens <math alttext="\mathcal{L}_{us}" class="ltx_Math" display="inline" id="A4.p4.5.m5.1"><semantics id="A4.p4.5.m5.1a"><msub id="A4.p4.5.m5.1.1" xref="A4.p4.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.p4.5.m5.1.1.2" xref="A4.p4.5.m5.1.1.2.cmml">ℒ</mi><mrow id="A4.p4.5.m5.1.1.3" xref="A4.p4.5.m5.1.1.3.cmml"><mi id="A4.p4.5.m5.1.1.3.2" xref="A4.p4.5.m5.1.1.3.2.cmml">u</mi><mo id="A4.p4.5.m5.1.1.3.1" xref="A4.p4.5.m5.1.1.3.1.cmml">⁢</mo><mi id="A4.p4.5.m5.1.1.3.3" xref="A4.p4.5.m5.1.1.3.3.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.p4.5.m5.1b"><apply id="A4.p4.5.m5.1.1.cmml" xref="A4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="A4.p4.5.m5.1.1.1.cmml" xref="A4.p4.5.m5.1.1">subscript</csymbol><ci id="A4.p4.5.m5.1.1.2.cmml" xref="A4.p4.5.m5.1.1.2">ℒ</ci><apply id="A4.p4.5.m5.1.1.3.cmml" xref="A4.p4.5.m5.1.1.3"><times id="A4.p4.5.m5.1.1.3.1.cmml" xref="A4.p4.5.m5.1.1.3.1"></times><ci id="A4.p4.5.m5.1.1.3.2.cmml" xref="A4.p4.5.m5.1.1.3.2">𝑢</ci><ci id="A4.p4.5.m5.1.1.3.3.cmml" xref="A4.p4.5.m5.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.5.m5.1c">\mathcal{L}_{us}</annotation><annotation encoding="application/x-llamapun" id="A4.p4.5.m5.1d">caligraphic_L start_POSTSUBSCRIPT italic_u italic_s end_POSTSUBSCRIPT</annotation></semantics></math> is used for fitting, then <math alttext="a<0" class="ltx_Math" display="inline" id="A4.p4.6.m6.1"><semantics id="A4.p4.6.m6.1a"><mrow id="A4.p4.6.m6.1.1" xref="A4.p4.6.m6.1.1.cmml"><mi id="A4.p4.6.m6.1.1.2" xref="A4.p4.6.m6.1.1.2.cmml">a</mi><mo id="A4.p4.6.m6.1.1.1" xref="A4.p4.6.m6.1.1.1.cmml">&lt;</mo><mn id="A4.p4.6.m6.1.1.3" xref="A4.p4.6.m6.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p4.6.m6.1b"><apply id="A4.p4.6.m6.1.1.cmml" xref="A4.p4.6.m6.1.1"><lt id="A4.p4.6.m6.1.1.1.cmml" xref="A4.p4.6.m6.1.1.1"></lt><ci id="A4.p4.6.m6.1.1.2.cmml" xref="A4.p4.6.m6.1.1.2">𝑎</ci><cn id="A4.p4.6.m6.1.1.3.cmml" type="integer" xref="A4.p4.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p4.6.m6.1c">a&lt;0</annotation><annotation encoding="application/x-llamapun" id="A4.p4.6.m6.1d">italic_a &lt; 0</annotation></semantics></math>. Therefore, we believe that training the model on selected tokens can effectively improve its performance on downstream tasks, while unselected tokens may have a detrimental effect on the model’s performance in downstream tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Examples of Tokens Selected by SLM</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Token Selected Examples</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A5.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1111" id="A5.F12.g1" src="x12.png" width="829">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F12.4.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text ltx_font_bold" id="A5.F12.5.2" style="font-size:90%;">Specific examples of selecting tokens during the ToT training process of the <span class="ltx_text ltx_font_smallcaps" id="A5.F12.5.2.1">Rho-1</span>.<span class="ltx_text ltx_font_medium" id="A5.F12.5.2.2"> The tokens marked in <span class="ltx_text" id="A5.F12.5.2.2.1" style="color:#1E90FF;">blue</span> represent the actual tokens trained during the ToT training process, while the remaining black tokens are not trained during the ToT training process.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A5.SS1.p1">
<p class="ltx_p" id="A5.SS1.p1.1">In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A5.F12" title="Figure 12 ‣ E.1 Token Selected Examples ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;12</span></a>, we present several examples of tokens selected by the SLM method, with content marked in <span class="ltx_text" id="A5.SS1.p1.1.1" style="color:#1E90FF;">blue</span> indicating the tokens actually chosen during the pretraining process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Dynamic Token Selected</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A5.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1271" id="A5.F13.g1" src="x13.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A5.F13.7.1.1" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text ltx_font_bold" id="A5.F13.8.2" style="font-size:90%;">An example of dynamic token selection changes during the training process<span class="ltx_text ltx_font_medium" id="A5.F13.8.2.1">, which illustrated with five different score levels represented by <span class="ltx_text" id="A5.F13.8.2.1.1" style="color:#0000FF;">deep blue</span>, <span class="ltx_text" id="A5.F13.8.2.1.2" style="color:#1E90FF;">light blue</span>, black, <span class="ltx_text" id="A5.F13.8.2.1.3" style="color:#FFB496;">light orange</span>, and <span class="ltx_text" id="A5.F13.8.2.1.4" style="color:#FF6400;">dark orange</span>. The bluer the color indicates a higher tendency for the token to be selected, while the more orange the color suggests a lower tendency for the token to be selected.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A5.SS2.p1">
<p class="ltx_p" id="A5.SS2.p1.1">In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.07965v1#A5.F13" title="Figure 13 ‣ E.2 Dynamic Token Selected ‣ Appendix E Examples of Tokens Selected by SLM ‣ Rho-1: Not All Tokens Are What You Need"><span class="ltx_text ltx_ref_tag">Figure&nbsp;13</span></a>, we display the dynamic changes in token selection tendencies throughout the SLM training process. We chose four checkpoints during the training process (0%, 33%, 66%, and 100%) to analyze the current tendencies in token selection. The preferences for token selection are indicated by different colors, ranging from high to low preference, typically represented as <span class="ltx_text" id="A5.SS2.p1.1.1" style="color:#0000FF;">deep blue</span>, <span class="ltx_text" id="A5.SS2.p1.1.2" style="color:#1E90FF;">blue</span>, black, <span class="ltx_text" id="A5.SS2.p1.1.3" style="color:#FFB496;">orange</span>, and <span class="ltx_text" id="A5.SS2.p1.1.4" style="color:#FF6400;">dark orange</span>, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
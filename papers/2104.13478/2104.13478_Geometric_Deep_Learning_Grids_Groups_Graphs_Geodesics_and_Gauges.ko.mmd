# Geometric Deep Learning

그리드, 그룹, 그래프,

Geodesics, Gauges

마이클 M. Bronstein\({}^{1}\), Joan Bruna\({}^{2}\), Taco Cohen\({}^{3}\), Petar Velickovic\({}^{4}\)

임페리얼 칼리지 런던 / USI IDSIA / Twitter

뉴욕대학교

\({}^{3}\)Qualcomm AI Research. 퀄컴 AI 리서치는 퀄컴 테크놀로지스의 이니셔티브이다.

\({}^{4}\)DeepMind

[MISSING_PAGE_EMPTY:2]

###### Contents

* 1 소개
* 2 High Dimension의 학습
	* 2.1 함수 규칙성을 통한 유도성 바이어스
	* 2.2 차원성의 저주
* 3 기하학적 프리저
	* 3.1 대칭, 표현 및 불변
	* 3.2 Isomorphisms 및 Automorphisms
	* 3.3 변형 안정성
	* 3.4 스케일 분리
	* 3.5 기하 딥러닝의 청사진
* 4 Geometric Domains: 5 Gs
	* 4.1 그래프 및 집합
	* 4.2 그리드 및 유클리드 공간
	* 4.3 그룹 및 균질한 공간
*5*4*4.4 Geodesics and Manifolds
* 4.5 Gauges and Bundles
* 4.6 지오메트리 그래프 및 메쉬
* 5 기하학적 딥러닝 모델
	* 5.1 합성곱 신경망
	* 5.2 Group-equivariant CNNs
	* 5.3 그래프 신경망
	* 5.4 딥셋, 트랜스포머 및 잠재 그래프 추론
	* 5.5 등분산 메시지 전달 네트워크
	* 5.6 내재적 메쉬 CNNs
	* 5.7 순환 신경망
	* 5.8 장단기 메모리 네트워크
*6 문제점 및 적용예
*7 역사적 관점

## Preface

유클리드의 _원소_ 이후 거의 2천 년 동안 '기하학'이라는 단어는 다른 유형의 기하학이 존재하지 않았기 때문에 _유클리드 기하학_과 동의어였다. 유클리드의 독점은 19세기에 끝났으며, 로바체베스키, 볼라이, 가우스, 리만에 의해 구성된 비유클리드 기하학의 예를 들 수 있다. 그 세기가 끝날 무렵, 수학자들과 철학자들이 "하나의 진정한 기하학"의 본질뿐만 아니라 이러한 기하학의 타당성과 관계들에 대해 토론하면서, 이 연구들은 서로 다른 분야로 갈라졌다.

이 피클에서 벗어나는 길은 1872년에 작은 바이에른 엘랑겐 대학의 교수로 임명된 젊은 수학자 펠릭스 클라인(Felix Klein)에 의해 보여졌습니다. Klein은 수학 연보를 _Erlangen Programme_로 입력한 연구 설명서에서, 기하학의 _대칭_이라고 불리는 일부 종류의 변환 하에서 변하지 않는 성질, 즉 _불변들_에 대한 연구로 기하학에 접근하는 것을 제안했다. 이 접근법은 당시 알려진 다양한 기하학이 군 이론의 언어를 사용하여 공식화된 대칭 변환의 적절한 선택에 의해 정의될 수 있음을 보여줌으로써 명확성을 만들었다. 예를 들어, 유클리드 기하학은 길이와 각도에 관한 것인데, 이러한 성질은 유클리드 변환(회전과 변환) 그룹에 의해 보존되는 반면, 아핀 기하학은 아핀 변환 그룹에 의해 보존되는 평행성을 연구하기 때문이다. 유클리드 그룹은 아핀 그룹의 하위 그룹이고, 이는 차례로 사영 변환 그룹의 하위 그룹이기 때문에 이러한 기하학 사이의 관계는 각 그룹을 고려할 때 즉시 명백하다.

기하학에 대한 Erlangen 프로그램의 영향은 매우 심오했다. 게다가, 그것은 다른 분야, 특히 물리학에 유출되었는데, 여기서 대칭 원리는 대칭의 첫 번째 원리에서 보존 법칙을 도출할 수 있게 했고(Noether의 정리로 알려진 놀라운 결과), 심지어 기본 입자를 대칭군의 환원 불가능한 표현으로 분류할 수 있게 했다. _ 현재 순수 수학에 널리 퍼져 있는 범주 이론_은 창작자 사무엘 아일렌버와 손더스 맥 레인의 말처럼 "변환 그룹이 있는 기하학적 공간이 매핑의 대수가 있는 범주로 일반화된 의미에서 클라인 엘랑겐 프로그램의 연속으로 간주될 수 있다.

글을 쓸 당시, 딥 러닝 분야의 상태는 19세기 기하학 분야의 어느 정도 미미하다. 다양한 종류의 데이터에 대한 신경망 아키텍처의 진정한 동물원은 있지만 원칙을 통합하는 것은 거의 없다. 과거와 마찬가지로 이것은 다양한 방법 간의 관계를 이해하기 어렵게 만들고, 필연적으로 다른 응용 영역에서 동일한 개념의 재발명과 재브랜딩을 초래한다. 그 분야를 배우려는 초보자에게, 엄청난 양의 중복된 아이디어를 흡수하는 것은 진정한 악몽이다.

이 텍스트에서 우리는 이 분야의 시스템화를 얻고 '점 연결'을 궁극적인 목표로 딥 러닝의 영역에 에를랑겐 프로그램 마인드를 적용하려는 겸손한 시도를 한다. 우리는 이 기하학 시도를 '기하학적 딥 러닝'이라고 부르며, 펠릭스 클라인의 정신에 충실하며 대칭과 불변성의 첫 번째 원리에서 서로 다른 귀납적 편향과 이를 구현하는 네트워크 아키텍처를 도출할 것을 제안한다. 특히, 구조화되지 않은 집합, 격자, 그래프, 매니폴드를 분석하기 위해 설계된 대규모 신경망에 초점을 맞추고, 이러한 도메인의 구조와 대칭을 존중하는 방법으로 통합적으로 이해할 수 있음을 보여준다.

우리는 이 텍스트가 광범위한 심층 학습 연구자, 실무자 및 애호가에게 어필할 것이라고 믿습니다. 초보자는 이를 기하 딥러닝의 개요 및 소개로 활용할 수 있다. 노련한 딥 러닝 전문가는 기본 원리와 몇 가지 놀라운 연결에서 친숙한 아키텍처를 도출하는 새로운 방법을 발견할 수 있다. 실천가는 각 분야의 문제를 해결하는 방법에 대한 새로운 통찰력을 얻을 수 있다.

현대 기계 학습처럼 빠르게 진행되는 분야라면, 이런 글을 쓸 위험은 낮의 빛을 보기 전에 쓸모없고 무관해지기 때문이다. 기초에 초점을 맞춘 우리의 희망은 우리가 논의하는 핵심 개념이 그들의 구체적인 현실을 초월할 것이라는 것입니다. 혹은 클로드 아드리앵 헬베티우스가 말한 것처럼, "라 코나상스 드 특정스 원칙들은 라 코나상스 드 특정스 페이트를 쉽게 수용할 수 있습니다."

## Chapter 0 Notation

## 1 Introduction

지난 10년 동안 데이터 과학 및 기계 학습에서 딥 러닝 방법으로 대표되는 실험적 혁명을 목격했다. 실제로, 컴퓨터 비전, 바둑 연주 또는 단백질 접기와 같이 이전에는 도달할 수 없는 것으로 생각되었던 많은 고차원 학습 과제는 실제로 적절한 계산 규모로 실현 가능하다. 놀랍게도, 딥 러닝의 본질은 두 가지 간단한 알고리즘 원리로부터 구축된다: 첫째, 표현 또는 특징 학습의 개념이며, 이에 의해 적응되고 종종 계층적인 특징은 각 태스크에 대한 규칙성의 적절한 개념을 포착하고, 둘째, 일반적으로 _역전파_로 구현되는 로컬 그래디언트-하강에 의한 학습이다.

고차원에서 일반 함수를 학습하는 것은 저주받은 추정 문제이지만 대부분의 관심 과제는 일반적이지 않으며 물리적 세계의 근본적인 저차원성과 구조에서 발생하는 필수 미리 정의된 규칙성을 포함한다. 이 텍스트는 광범위한 응용 분야에 걸쳐 적용될 수 있는 통일된 기하학적 원리를 통해 이러한 규칙성을 노출시키는 것과 관련이 있다.

큰 계의 알려진 대칭을 이용하는 것은 차원성의 저주에 대항하는 강력하고 고전적인 해결책이며, 대부분의 물리적 이론의 기초를 형성한다. 딥러닝 시스템도 예외는 아니며, 초기 연구자들은 이미지의 그리드, 시계열의 시퀀스 또는 분자의 위치 및 운동량, 병진 또는 회전과 같은 관련 대칭과 같은 물리적 측정에서 발생하는 저차원 기하학을 활용하기 위해 신경망을 채택했다. 우리의 박람회 전반에 걸쳐 우리는 이러한 모델과 다른 많은 모델을 기하 규칙성의 동일한 기본 원리의 자연 사례로 설명할 것이다.

Erlangen 프로그램의 정신에서 이러한 '기하학적 통일' 노력은 이중 목적을 제공한다: 한편으로 CNN, RNN, GNN 및 트랜스포머와 같은 가장 성공적인 신경망 아키텍처를 연구하기 위한 공통 수학적 프레임워크를 제공한다. 한편, 이전의 물리적 지식을 신경망 구조에 통합하기 위한 건설적인 절차를 제공하고, 아직 발명되지 않은 미래의 아키텍처를 구축하기 위한 원칙적인 방법을 제공한다.

계속하기 전에, 우리의 작업은 _표현 학습 아키텍처_와 그 안의 데이터의 대칭을 활용하는 것에 관한 것이라는 점에 주목할 필요가 있다. 이러한 표현이 사용될 수 있는 많은 흥미로운 _파이프라인_(예: 자체 지도 학습, 생성 모델링 또는 강화 학습)은 우리의 중심 초점이 아닙니다. 따라서 우리는 변분 오토인코더(Kingma and Welling, 2013), 생성적 적대적 네트워크(Goodfellow et al., 2014), 정상화 흐름(Rezende and Mohamed, 2015), 심층 Q-네트워크(Mnih et al., 2015), 근접 정책 최적화(Schulman et al., 2017) 또는 심층 상호 정보 최대화(Hjelm et al., 2019)와 같은 심층 영향력 있는 신경 파이프라인을 검토하지 않을 것이다. 즉, 우리는 우리가 초점을 맞출 원칙이 이 모든 분야에서 매우 중요하다고 믿습니다.

또한, 기하학 청사진의 힘을 설명하기 위해 합리적으로 넓은 그물을 던지려고 시도했지만, 우리의 작업은 기하학 딥 러닝에 대한 기존의 풍부한 연구를 정확하게 요약하려고 시도하지 않는다. 오히려, 우리는 원리를 입증하고 기존 연구에서 근거하기 위해 잘 알려진 몇 가지 아키텍처를 심층적으로 연구하며, 독자가 이러한 원리를 접하거나 고안하는 미래의 기하학적 심층 아키텍처에 의미 있게 적용할 수 있는 충분한 참조를 남겼기를 바란다.

## 2 High Dimensions에서 학습

지도 기계 학습은 가장 간단한 형식화에서 \(N\) 관측치 \(\mathcal{D}=\{(x_{i},y_{i})\}_{i=1}^{N}\)를 고려하여 \(\mathcal{X}\times\mathcal{Y}\)에 정의된 기본 데이터 분포 \(P\)에서 _i.i.d._를 도출한다. 여기서 \(\mathcal{X}\)와 \(\mathcal{Y}\)는 각각 데이터와 레이블 도메인이다. 이 설정에서 정의하는 특징은 \(\mathcal{X}\)가 _고차원 공간_이라는 것이다: 일반적으로 \(\mathcal{X}=\mathbb{R}^{d}\)를 큰 차원의 유클리드 공간으로 가정한다 \(d\).

더 나아가 레이블 \(y\)이 미지의 함수 \(f\)에 의해 생성된다고 가정하자 \(y_{i}=f(x_{i})\), 학습 문제는 매개변수 함수 클래스 \(\mathcal{F}=\{f_{\mathbf{\theta}\in\theta}\}\)를 사용하여 함수 \(f\)를 추정하는 것으로 줄어든다. 신경망은 이러한 매개변수 함수 클래스의 일반적인 구현이며, 이 경우 \(\mathbf{\theta}\in\theta\)는 네트워크 가중치에 해당한다. 이 이상적인 설정에서는 레이블에 노이즈가 없으며, 현대 딥 러닝 시스템은 일반적으로 소위 _인터폴레이팅 체제_에서 작동하며, 여기서 추정된 \(\tilde{f}\in\mathcal{F}\)는 모든 \(i=1,\ldots,N\)에 대해 \(\tilde{f}(x_{i})=f(x_{i})\)를 만족한다. 학습 알고리즘의 성능은 \(P\)에서 추출한 새로운 샘플에 대해 일부 _손실_\(L(\cdot,\cdot)\)를 사용하여 _예상 성능_으로 측정된다.

\[\mathcal{R}(\tilde{f}):=\mathbb{E}_{P}\ L(\tilde{f}(x),f(x)),\]

(L(y,y^{\prime})=\frac{1}{2}|y-y^{\prime}|^{2}\).

따라서 성공적인 학습 스킴은 함수 클래스 \(\mathcal{F}\)의 구성 및 정규화의 사용을 통해 부과되는 \(f\)에 대한 규칙성 또는 귀납적 편향의 적절한 개념을 인코딩해야 한다. 우리는 다음 절에서 이 개념을 간략하게 소개한다.

### 함수 정규성을 통한 유도 편향

현대 기계 학습은 대용량 고품질 데이터 세트를 사용하여 작동하며, 이는 적절한 계산 자원과 함께 이러한 대용량 데이터를 보간할 수 있는 용량으로 풍부한 함수 클래스 \(\mathcal{F}\)의 설계에 동기를 부여한다. 이 사고방식은 신경망과 잘 어울립니다. 가장 간단한 아키텍처 선택도 _밀집된_ 함수 클래스를 산출하기 때문입니다. 거의 임의의 함수에 근사하는 능력은 다양한 _유니버설 근사 이론의 주제이다; 그러한 몇 가지 결과는 응용 수학자와 컴퓨터 과학자에 의해 1990년대에 증명되고 대중화되었다(예를 들어, Cybenko (1989), Hornik (1991), Barron (1993), Leshno 등 (1993), Maiorov (1999), Pinkus (1999).

그러나 보편적 근사화는 귀납적 편향이 없음을 의미하지 않는다. 범용 근사를 갖는 가설 공간 \(\mathcal{F}\)이 주어지면 우리는 복잡도 측정 \(c:\mathcal{F}\to\mathbb{R}_{+}\)을 정의하고 보간 문제를 다음과 같이 재정의할 수 있다.

\[\tilde{f}\in\arg\min_{g\in\mathcal{F}}c(g)\quad\mathrm{s.t.}\quad g(x_{i})=f(x_{i})\quad\mathrm{for}\;\;i=1,\ldots,N,\

즉, 우리는 가설 클래스 내에서 가장 규칙적인 함수를 찾고 있다. 표준 함수 공간의 경우, 이 복잡도 척도는 \(\mathcal{F}\)를 _Banach 공간_으로 만들고 함수 분석에서 많은 이론적 결과를 활용할 수 있도록 하는 _norm_으로 정의할 수 있다. 저차원에서 스플라인은 함수 근사를 위한 작업마입니다. 2차 미분 \(\int_{-\infty}^{+\infty}|f^{\prime\prime}(x)|^{2}\mathrm{d}x\)과 같은 고전적인 평활도 개념을 포착하는 규범으로 위와 같이 공식화될 수 있다.

신경망의 경우 복잡도 측정 \(c\)은 네트워크 가중치로 표현될 수 있다. 즉 \(c(f_{\mathbf{\theta}})=c(\mathbf{\theta})\). 네트워크 가중치의 \(L_{2}\)-norm, 즉 _weight decay_ 또는 소위 _path-norm_(Neyshabur et al., 2015)은 딥 러닝 문헌에서 인기 있는 선택이다. 베이지안 관점에서 이러한 복잡도 측정은 관심 함수에 대한 이전의 음의 로그로도 해석될 수 있다. 보다 일반적으로 이러한 복잡성은 특정 최적화 계획의 결과로 경험적 손실(이른바 구조적 위험 최소화 결과) 또는 명시적으로)에 통합함으로써 명시적으로 시행될 수 있습니다. 예를 들어, 미결정 최소자승 목표에서의 기울기 하강은 최소 \(L_{2}\) norm을 갖는 보간해를 선택할 것이라는 것은 잘 알려져 있다. 이러한 암묵적 정규화 결과의 현대 신경망으로의 확장은 현재 연구의 주제이다(예: Blanc 등(2020), Shamir와 Vardi(2020), Razin과 Cohen(2020), Gunasekar 등(2017). 대체로, 자연적인 질문이 발생한다: 실제 예측 작업의 예상되는 규칙성과 복잡성을 포착하는 효과적인 전과를 어떻게 정의할 것인가?

그림 1: 가장 단순한 피드포워드 신경망인 다층 퍼셉트론(Rosenblatt, 1958)은 보편적인 근사기이다: 단 하나의 은닉층으로, 이들은 스텝 함수의 조합을 나타낼 수 있어 임의의 정밀도로 임의의 연속 함수를 근사할 수 있다.

### 차원성의 저주

저차원( \(d=1,2\) 또는 \(3\))에서의 보간은 점점 더 정교한 규칙성 클래스(스플라인 보간, 웨이블릿, 곡선 또는 능선)를 사용하여 추정 오차를 매우 정밀하게 수학적으로 제어하는 고전적인 신호 처리 작업이지만, 고차원 문제에 대한 상황은 완전히 다르다.

이 아이디어의 본질을 전달하기 위해, 1-Lipschitz 함수 \(f:\mathcal{X}\to\mathbb{R}\), 즉 모든 \(x,x^{\prime}\in\mathcal{X}\)에 대해 \(|f(x)-f(x^{\prime})|\leq\|x-x^{\prime}\|\)를 만족시키는 함수 \(|f(x)-f(x^{\prime}\|\)로 쉽게 확장될 수 있는 고전적인 규칙성 개념을 생각해 보자. 이 가설은 목표 함수가 _지역적으로_ 매끄럽도록 요청합니다. 즉, 입력 \(x\)를 약간 교란하면(정규 \(\|x-x^{\prime}\|\)로 측정됨), 출력 \(f(x)\)는 크게 변경되지 않습니다. 목표함수 \(f\)에 대한 우리의 지식만이 1-Lipschitz라고 한다면, 우리의 추정치 \(\tilde{f}\)가 \(f\)에 가까울 것을 보장하기 위해 얼마나 많은 관측치가 필요할 것으로 예상하는가? 그림 2는 일반적인 답이 차원 \(d\)에서 반드시 지수임을 보여주며, 입력 차원이 증가함에 따라 립시츠 클래스가 '너무 빨리' 성장한다는 것을 보여준다. 작은 차원 \(d\)을 갖는 많은 응용에서 샘플의 수는 우주의 원자 수보다 클 것이다. Lipschitz 클래스를 Sobolev 클래스 \(\mathcal{H}^{s}(\Omega_{d})\)와 같은 전역 평활도 가설로 대체하는 경우 상황은 더 좋지 않다. 실제로, 고전적인 결과(Tsybakov, 2008)는 \(\epsilon^{-d/s}\) 차수의 소볼레프 클래스에 대한 근사화 및 학습의 최소 비율을 설정하여, \(f\)에 대한 추가적인 평활성 가정이 실제 비현실적인 가정인 \(s\propto d\)일 때만 통계적 그림을 개선한다는 것을 보여준다.

완전 연결 신경망은 가중치에 복잡도 함수 \(c\)를 고려하여 보다 유연한 규칙성 개념을 가능하게 하는 함수 공간을 정의한다. 특히, 희소성을 촉진하는 규칙화를 선택함으로써, 그들은 차원성의 저주를 깰 수 있는 능력을 가지고 있다. 그러나 이는 \(f\)가 입력의 저차원 투영의 집합에 의존한다는 것과 같이 목표 함수의 성질 \(f\)에 대한 강력한 가정을 희생시키면서 온다(그림 3 참조). 대부분의 실제 응용(컴퓨터 비전, 음성 분석, 물리학 또는 화학 등)에서 관심 기능은 저차원 투영으로 표현할 수 없는 복잡한 장거리 상관 관계를 나타내는 경향이 있어(그림 3), 이 가설을 비현실적으로 만든다. 따라서 다음 섹션 3에서 설명하는 것처럼 물리적 영역의 공간 구조와 \(f\)의 기하학적 전치를 활용하여 규칙성의 대체 소스를 정의할 필요가 있다.

## 3 Geometric Priors

현대 데이터 분석은 고차원 학습과 동의어이다. 2.1절의 간단한 주장은 차원의 저주로 인해 일반적인 고차원 데이터에서 배우는 것이 불가능하다는 것을 보여주지만, 물리적 구조 데이터에 대한 희망이 있으며, 여기서 우리는 두 가지 기본 원리인 _대칭_ 및 _스케일 분리_를 사용할 수 있다. 이 텍스트에서 고려되는 설정에서 이 추가 구조는 일반적으로 입력 신호의 기초가 되는 도메인의 구조에서 비롯됩니다. 우리는 기계 학습 시스템이 일부 도메인 \(\Omega\)의 _신호_(함수)로 작동한다고 가정할 것입니다. 많은 경우에 \(\Omega\) 상의 점들의 선형 조합은 잘 정의되지 않지만, 우리는 그 상의 신호들을 선형 조합할 수 있다. 즉, 신호들의 공간은 벡터 공간을 형성한다. 게다가 우리는 신호 사이의 내적을 정의할 수 있기 때문에 이 공간은 _힐버트 공간_이다.

그림 2: 우리는 립시츠 함수 \(f(x)=\sum_{j=1}^{2^{d}}z_{j}\phi(x-x_{j})\)를 고려한다. 여기서 \(z_{j}=\pm 1\), \(x_{j}\in\mathbb{R}^{d}\)는 각 사분면에 배치되고, \(\phi\)는 국부적으로 지원되는 립시츠 '범프'이다. 우리가 대부분의 \(2^{d}\) 사분면에서 함수를 관찰하지 않는 한, 우리는 그것을 예측하는 데 일정한 오류가 발생할 것이다. 이 간단한 기하학적 논증은 립시츠 클래스에 대해 \(\kappa(d)=\mathbb{E}_{x,x^{\prime}}\sup_{f\in\operatorname{Lip}(1)}\left| \frac{1}{N}\sum_{l}f(x_{l})-\frac{1}{N}\sum_{l}f(x_{l}^{\prime})\right|\simeq N ^{-1/d}\로 정의된 _Maximum Disrepancy_(von Luxburg and Bousquet, 2004) 개념을 통해 공식화될 수 있으며, 이는 두 개의 독립적인 \(N\)-표본 기대치 사이의 가장 큰 예상 불일치를 측정한다. \(\kappa(d)\simeq\epsilon\)가 \(N=\theta(\epsilon^{-d})\)를 필요로 하는지 확인 합니다. 해당 샘플 \(\{x_{l}\}_{l}\)은 도메인의 \(\epsilon\)-net을 정의 합니다. 직경의 \(d\)차원 유클리드 영역의 크기는 \(\epsilon^{-d}\)로 기하급수적으로 증가한다.

그림 3: 미지의 함수 \(f\)가 \(k\ll d\)와 함께 미지의 일부 \(\mathbf{A}\in\mathbb{R}^{k\times d}\)에 대해 \(f(\mathbf{x})\approx g(\mathbf{A}\mathbf{x})\)로 잘 근사되는 것으로 추정되면, 얕은 신경망은 이 유도성 바이어스를 포착할 수 있으며, 예를 들어 바흐(2017)를 참조한다. 전형적인 응용들에서, 이러한 저차원 투영에 대한 의존성은, 이 예에서 예시된 바와 같이, 비현실적이다: 저-패스 필터는 입력 이미지들을 저-차원 서브공간으로 투영하고; 그것은 대부분의 의미론들을 전달하지만, 실질적인 정보는 손실된다.

[MISSING_PAGE_EMPTY:15]

### 대칭, 표현 및 분산

비공식적으로, 객체 또는 시스템의 _대칭_은 상기 객체 또는 시스템의 특정 속성을 변경되지 않거나 _불변_하게 하는 변환이다. 이러한 변환은 매끄럽거나 연속적이거나 이산적일 수 있다. 대칭은 많은 기계 학습 작업에서 어디에나 있습니다. 예를 들어, 컴퓨터 비전에서 객체 카테고리는 시프트에 의해 변하지 않으므로, 시프트는 시각적 객체 분류의 문제에서 대칭이다. 계산 화학에서 분자의 특성을 공간에서의 방향과 독립적으로 예측하는 작업은 _회전 불변_ 을 필요로 한다. 이산 대칭은 입자가 정준적 순서를 갖지 않으므로 임의적으로 순열될 수 있는 입자 계를 설명할 때 자연적으로 나타나며, 시간-역 대칭(상세한 균형에 있는 계 또는 뉴턴의 제2 운동 법칙과 같은)을 통해 많은 역학 계뿐만 아니라 임의적으로 순열될 수 있다. 4.1절에서 볼 수 있듯이 순열 대칭도 그래프 구조 데이터 분석의 중심이다.

대칭 그룹 객체의 대칭 집합은 여러 속성을 충족합니다. 먼저, 새로운 대칭을 얻기 위해 대칭이 결합될 수 있다: \(\mathfrak{g}\) 및 \(\mathfrak{h}\)가 두 개의 대칭이면, 이들의 조성 \(\mathfrak{g}\circ\mathfrak{h}\) 및 \(\mathfrak{h}\circ\mathfrak{g}\)도 대칭이다. 그 이유는 두 변환이 모두 대상을 불변으로 떠난다면 변환의 구성도 마찬가지이므로 그 구성 역시 대칭이기 때문이다. 더욱이 대칭은 항상 가역적이며, 역도 대칭이다. 이것은 모든 대칭의 집합이 _그룹_으로 알려진 대수적 대상을 형성한다는 것을 보여준다. 이러한 개체는 기하학 딥러닝의 수학적 모델의 중심이 될 것이기 때문에 공식적인 정의와 자세한 논의가 필요하다: _그룹_은 다음 공리를 충족하는 _구성_이라고 하는 이진 연산과 함께 집합 \(\mathfrak{G}\) \(\circ:\mathfrak{G}\times\mathfrak{G}\to\mathfrak{G}\) \(\circ:\mathfrak{g}\circ\mathfrak{h}=\mathfrak{gh}\))이다.

_연관성: 모든 \(\mathfrak{g},\mathfrak{h},\mathfrak{t}\in\mathfrak{G}\)에 대해_\((\mathfrak{gh})\mathfrak{t}=\mathfrak{g}(\mathfrak{h}\mathfrak{t})\).

_Identity:_ 모든 \(\mathfrak{g}\in\mathfrak{G}\)에 대해 \(\mathfrak{eq}=\mathfrak{ge}=\mathfrak{g}\)를 만족하는 고유한 \(\mathfrak{e}\in\mathfrak{G}\)가 존재한다.

_Inverse:_ 각 \(\mathfrak{g}\in\mathfrak{G}\)에 대해 고유한 역 \(\mathfrak{g}^{-1}\in\mathfrak{G}\)이 있으므로 \(\mathfrak{gg}^{-1}=\mathfrak{g}=\mathfrak{e}\).

_Closure:_ 그룹은 구성 아래 닫혀 있습니다. 즉, 모든 \(\mathfrak{g},\mathfrak{h}\in\mathfrak{G}\), \(\mathfrak{gh}\in\mathfrak{G}\).

교환성은 이 정의의 일부가 아니다. 즉, 우리는 \(\mathfrak{gh}\neq\mathfrak{hg}\)를 가질 수 있다.

모든 \(\mathfrak{g},\mathfrak{h}\in\mathfrak{G}\)에 대해 \(\mathfrak{gh}=\mathfrak{hg}\)가 있는 그룹을 교환 또는 _Abelian_이라고 합니다.

일부 그룹은 매우 크고 심지어 무한할 수 있지만 종종 _그룹 생성기_라고 하는 몇 가지 요소의 구성에서 발생한다. 형식적으로 \(\mathfrak{G}\)는 모든 요소 \(\mathfrak{g}\in\mathfrak{G}\)가 \(S\)의 원소와 그 역수의 유한한 합으로 작성될 수 있는 경우 부분집합 \(S\subseteq\mathfrak{G}\)(그룹 _생성자_라고 함)에 의해 생성된다고 한다. 예를 들어, 정삼각형(이면체군 \(\mathrm{D}_{3}\))의 대칭군은 \(60^{\circ}\) 회전과 반사에 의해 생성된다(그림 4). 아래에서 자세히 논의할 1D _번역 그룹_ 은 극소 변위에 의해 생성되며, 이것은 미분 가능한 대칭의 _거짓말 그룹_ 의 예입니다.

여기에서 그룹 요소가 _are_(예: 일부 도메인의 변환)인 것은 말할 것도 없고 그룹이 _구성하는 방법만 말하면서 그룹을 추상 객체로 정의했습니다. 따라서, 매우 다른 종류의 물체들은 동일한 대칭 그룹을 가질 수 있다. 예를 들어, 앞서 언급한 삼각형의 회전 및 반사 대칭 그룹은 세 요소의 시퀀스의 순열 그룹과 동일하다(우리는 회전 및 반사를 사용하여 삼각형의 모서리를 어떤 식으로든 순열할 수 있다 - 그림 4 참조).

그룹 액션 및 그룹 표현 그룹을 추상적 엔터티로 간주하는 대신 그룹이 데이터에서 _액션_ 하는 방법에 주로 관심이 있습니다. 우리는 데이타의 기저에 어떤 도메인 \(\Omega\)이 있다고 가정했기 때문에, 이 그룹이 \(\Omega\)(예: 평면의 점들의 변환)에 어떻게 작용하는지 연구할 것이며, 그로부터 신호 공간 \(\mathcal{X}(\Omega)\)(예: 평면 이미지 및 특징 맵의 변환)에 동일한 그룹의 작용을 얻을 것이다.

기술적으로 여기에서 정의하는 것은 _왼쪽_ 그룹 작업입니다. 집합 \(\Omega\)에 대한 \(\mathfrak{G}\)의 _그룹 동작_은 그룹 요소 \(\mathfrak{g},u)\mapsto\mathfrak{g}.u\)를 그룹 작업과 호환되는 방식으로 \(\mathfrak{g}\in\mathfrak{G}\) 및 점 \(u\in\Omega\)을 다른 점 \(\Omega\)에 연결하는 매핑 \((\mathfrak{g},u)\로 정의된다. (\mathfrak{h}.u)=(\mathfrak{gh}).u\) 모든 \(\mathfrak{g},\mathfrak{h}\in\mathfrak{G}\) 및 \(u\in\Omega\). 우리는 다음 섹션에서 그룹 액션의 수많은 사례를 볼 것이다. 예를 들어, 평면에서 _유클리드 그룹_\(\mathrm{E}(2)\)은 유클리드 거리를 보존하는 \(\mathbb{R}^{2}\)의 변환 그룹이며, 변환, 회전 및 반사로 구성된다. 그러나 동일한 그룹은 평면에서 _이미지의 공간(픽셀의 격자를 병진, 회전 및 뒤집음으로써)뿐만 아니라 신경망에 의해 학습된 표현 공간에도 작용할 수 있다. 보다 정확하게는, \(\mathfrak{G}\)가 \(\Omega\)에 작용한다면, 우리는 공간 \(\mathcal{X}(\Omega\)에 대한 \(\mathfrak{G}\)의 작용을 자동적으로 얻는다:

\[(\mathfrak{g}.x)(u)=x(\mathfrak{g}^{-1}u). \tag{3}\]

\(\mathfrak{g}\)에 대한 역행렬로 인해, 우리는 \((\mathfrak{g})를 가지고 있다는 점에서, 이것은 실제로 유효한 그룹 액션이다. (\mathfrak{h}.x))(u)=((\mathfrak{gh}).x)(u)\).

이 텍스트 전체에서 반복적으로 마주칠 가장 중요한 그룹 액션의 종류는 _그룹 표현_이라고도 하는 _선형_ 그룹 액션입니다. 식 (3)의 신호에 대한 작용은 실제로 선형이며,

그림 4: 왼쪽: 모서리가 \(1,2,3\)로 표시된 정삼각형과 삼각형의 모든 가능한 회전과 반사. 삼각형의 회전/반사 대칭의 그룹 \(\mathrm{D}_{3}\)은 두 개의 원소(R과 반사 F에 의한 회전)에 의해 생성되며 세 원소의 순열의 그룹 \(\Sigma_{3}\)과 동일하다. 오른쪽: 군의 곱셈표 \(\mathrm{D}_{3}\). 행 \(\mathfrak{g}\) 및 열 \(\mathfrak{h}\)의 요소는 요소 \(\mathfrak{gh}\)에 대응한다.

sense that

\[\mathfrak{g}.(\alpha x+\beta x^{\prime})=\alpha(\mathfrak{g}.x)+\beta(\mathfrak{g}. x^{\prime})\]

모든 스칼라에 대해 \(\alpha,\beta\) 및 신호 \(x,x^{\prime}\in\mathcal{X}(\Omega)\). 우리는 선형행동을 지도 \((\mathfrak{g},x)\mapsto\mathfrak{g}.x\)로 설명할 수 있으며, 이와 동등하게 커레링을 통해 각 그룹요소 \(\mathfrak{g}\to\mathbb{R}^{n\times n}\)에 (역행렬) \(\rho(\mathfrak{g})\)를 할당하는 지도 \(\rho\mathfrak{g})\로 설명할 수 있다. 행렬의 차원 \(n\)은 일반적으로 임의적이며, 반드시 그룹의 차원 또는 \(\Omega\)의 차원과 관련이 있는 것은 아니지만, 딥 러닝의 응용에서 \(n\)은 일반적으로 그룹이 작용하는 특징 공간의 차원일 것이다. 예를 들어, 우리는 \(n\) 픽셀을 가진 이미지의 공간에 작용하는 2D 변환 그룹을 가질 수 있다.

일반적인 그룹 액션과 마찬가지로, 그룹 요소에 행렬의 할당은 그룹 액션과 양립할 수 있어야 한다. 보다 구체적으로, 복합 그룹 요소 \(\mathfrak{g}\mathfrak{h}\)를 나타내는 행렬은 \(\mathfrak{g}\)와 \(\mathfrak{h}\)의 표현의 행렬 곱과 같아야 한다:

그룹 \(\mathfrak{G}\)의 차원 실수 _표현_은 맵 \(\rho:\mathfrak{G}\to\mathbb{R}^{n\times n}\)이고, 각 \(\mathfrak{g}\in\mathfrak{G}\)에 반전행렬 \(\rho(\mathfrak{g})\)을 할당하고, 모든 \(\mathfrak{g},\mathfrak{h}\in\mathfrak{G}\)에 대해 조건 \(\mathfrak{g}\mathfrak{h})=\rho(\mathfrak{g})\rho(\mathfrak{h})\)를 만족한다. 행렬 \(\rho(\mathfrak{g})\)이 모든 \(\mathfrak{g}\in\mathfrak{G}\)에 대해 단일 또는 직교인 경우 표현을 _단위_ 또는 _직교_라고 합니다.

\begin{tabular}{l l} \hline \hline \multicolumn{1}{c}{
\begin{tabular}{l} Similarly, a complex \\ representation is a map \\ \(\rho:\mathfrak{G}\to\mathbb{C}^{n\times n}\) satisfying the \\ same equation. \\ \end{tabular} } \\ \hline \hline \end{tabular}

그룹 표현 언어로 쓰여진 신호 \(x\in\mathcal{X}(\Omega)\)에 대한 \(\mathfrak{G}\)의 작용은 \(\rho(\mathfrak{g})x(u)=x(\mathfrak{g}^{-1}u)\로 정의된다. 다시 한번 확인하겠습니다

\[(\rho(\mathfrak{g})(\rho(\mathfrak{h})x))(u)=(\rho(\mathfrak{g}\mathfrak{h}) x)(u).\]

불변함수와 등변함수는 신호 \(\mathcal{X}(\Omega)\)의 기저에 있는 도메인 \(\Omega\)의 대칭성이 이러한 신호에 정의된 함수 \(f\)에 구조를 부여한다. 이는 귀납적 편향이 강한 것으로 밝혀졌으며, 대칭 전치를 만족하는 보간체 \(\mathcal{F}(\mathcal{X}(\Omega))\)의 공간을 줄임으로써 학습 효율을 향상시켰다. 이 텍스트에서 탐구할 두 가지 중요한 사례는 _불변_ 및 _equivariant_ 함수입니다.

함수 \(f:\mathcal{X}(\Omega)\to\mathcal{Y}\)는 모든 \(\mathfrak{g}\in\mathfrak{G}\) 및 \(x\in\mathcal{X}(\Omega)\)에 대해 \(f(\rho(\mathfrak{g})x)=f(x)\)인 경우 _\(\mathfrak{G}\)-불변_ 즉, 출력에 대한 그룹 액션의 영향을 받지 않는다.

신호 처리 책들은 시프트-불변, 예를 들어 선형 시프트-불변 시스템들을 지칭하는 '시프트-불변'이라는 용어를 일상적으로 사용한다. 불변성의 고전적인 예는 이미지 분류와 같은 컴퓨터 비전 및 패턴 인식 애플리케이션에서 발생하는 _시프트 불변_이다. 이 경우의 함수 \(f\)는(대표적으로 Convolutional Neural Network로 구현됨) 이미지를 입력하고, 이미지가 어떤 클래스(예: 고양이 또는 개)로부터의 객체를 포함할 확률을 출력한다. 종종 분류 결과는 이미지 내의 객체의 위치에 영향을 받지 않아야 하며, 즉 함수 \(f\)는 시프트 불변적이어야 한다고 합리적으로 가정된다. 매끄러운 기능에 근사할 수 있는 다층 퍼셉트론은 이러한 특성을 가지고 있지 않다 - 1970년대 패턴 인식 문제에 이러한 아키텍처를 적용하려는 초기 시도가 실패한 이유 중 하나이다. 컨볼루션 신경망에 의해 전칭되는 지역 가중치 공유를 갖는 신경망 아키텍처의 개발은 다른 이유들 중에서도 시프트-불변 객체 분류의 필요성에 의해 동기가 부여되었다.

그러나 CNN의 컨볼루션 계층을 자세히 살펴보면 시프트 불변이 아니라 _shift-equivariant_임을 알 수 있습니다. 즉, 컨볼루션 계층에 대한 입력의 시프트는 출력 특징 맵에서 동일한 양만큼 이동을 생성합니다.

일반적으로 입력공간과 출력공간이 다른 \(f:\mathcal{X}(\Omega)\to\mathcal{X}(\Omega^{\prime})\)와 같은 그룹 \(\mathfrak{G}\)의 표현 \(\rho\), \(\rho^{\prime}\)을 가질 수 있다. 이 경우, 등분성은 \(f(\rho(\mathfrak{g})x)=\rho^{\prime}(\mathfrak{g})f(x)\로 정의된다.

함수 \(f:\mathcal{X}(\Omega)\to\mathcal{X}(\Omega)\)는 모든 \(\mathfrak{g}\in\mathfrak{G}\)에 대해 \(f(\rho(\mathfrak{g})x)=\rho(\mathfrak{g})f(x)\)인 경우 _\(\mathfrak{G}\)-equivariant_, 즉 입력에 대한 그룹 액션이 출력에 동일한 방식으로 영향을 미친다.

컴퓨터 비전에 다시 의지하기, 원형의 응용 프로그램

그림 5: 기하학 딥러닝에서 관심 있는 세 공간: (물리적) _도메인_\(\Omega\), _신호_\(\mathcal{X}(\Omega)\), _가설_ 클래스 \(\mathcal{F}(\mathcal{X}(\Omega))\). 도메인의 대칭성 \(\Omega\)(그룹에 의해 포착됨 \(\mathfrak{G}\))은 그룹 표현을 통해 신호 \(x\in\mathcal{X}(\Omega)\)에 작용하고, 그러한 신호에 작용하는 함수 \(f\in\mathcal{F}(\mathcal{X}(\Omega))\)에 구조를 부과한다.

shift-equivariance는 이미지 분할이며, 여기서 \(f\)의 출력은 픽셀-와이즈 이미지 마스크이다. 명백히, 분할 마스크는 입력 이미지에서 시프트들을 따라야 한다. 이 예에서 입력과 출력의 도메인은 동일하지만 출력은 클래스당 하나의 채널을 갖는 반면 입력은 세 개의 컬러 채널을 갖기 때문에, 표현 \((\rho,\mathcal{X}(\Omega,\mathcal{C}))\) 및 \((\rho^{\prime},\mathcal{X}(\Omega,\mathcal{C}^{\prime))\)는 다소 다르다.

그러나, 이미지 분류의 이전 사용 사례조차도 일반적으로 컨볼루션(shift-equivariant) 계층의 시퀀스로 구현되고, 이어서 글로벌 풀링(shift-invariant)이 뒤따른다. 우리가 섹션 3.5에서 볼 수 있듯이, 이것은 CNN 및 그래프 신경망(GNN)을 포함한 대부분의 딥 러닝 아키텍처의 일반적인 청사진이다.

### Isomorphisms 및 Automorphisms

앞서 언급했듯이 구조의 부분군 및 수준 대칭은 어떤 성질이나 구조를 보존하는 변환이며, 주어진 구조에 대한 그러한 모든 변환의 집합은 대칭군을 형성한다. 하나의 구조가 아닌 여러 개의 관심 구조가 있는 경우가 자주 발생하므로 도메인 \(\Omega\)에서 여러 _구조 수준_ 을 고려할 수 있습니다. 따라서 대칭으로 간주되는 것은 고려 중인 구조에 따라 다르지만 모든 경우에 대칭은 이 구조를 존중하는 가역 지도이다.

가장 기본적인 수준에서 도메인 \(\Omega\)은 _set_이며, 이는 최소한의 구조를 갖는다: 우리가 말할 수 있는 것은 집합이 약간의 _cardinality_를 갖는다는 것이다. 이 구조를 보존하는 자체 지도는 _bijections_(역방향 지도)이며, 이는 집합 수준의 대칭으로 간주할 수 있습니다. 공리를 확인하면 이것이 그룹임을 쉽게 확인할 수 있다: 두 개의 비젝션의 구성은 비젝션(닫힘)이고, 연관성은 함수 구성의 연관성에서 비롯되며, 지도 \(\tau(u)=u\)는 동일 원소이고, 모든 \(\tau\)에 대해 정의상 역이 존재하여 \((\tau\circ\tau^{-1})(u)=(\tau^{-1}\circ\tau)(u)=u\를 만족한다.

애플리케이션에 따라, 추가적인 레벨의 구조가 존재할 수 있다. 예를 들어, \(\Omega\)가 위상 공간인 경우, 우리는 _연속성_을 보존하는 지도를 고려할 수 있는데, 이러한 지도는 _호모프리즘_이라고 하며, 집합 사이의 단순 절사 외에도 연속적이고 연속적인 역수를 갖는다. 직관적으로 연속함수는 잘 수행되며, 점 \(u\)을 중심으로 한 이웃(열린 집합)에서 점 \(\tau(u)\)을 중심으로 한 이웃에 점을 매핑한다.

하나는 맵과 그 역이 (연속적으로) _ 미분가능하다고 더 요구할 수 있다. 즉, 맵과 그 역은 모든 지점에서 도함수를 갖는다(그리고 도함수도 연속적이다). 이는 미분 가능한 다양체와 함께 제공되는 더 많은 미분 가능한 구조를 필요로 하며, 여기서 이러한 맵은 _diffeomorphisms_라고 하고 \(\operatorname{Diff}(\Omega)\)로 표시된다. 우리가 마주칠 구조의 추가 예에는 _거리_ 또는 _메트릭_(이를 보존하는 지도를 _아이소메트리_라고 함) 또는 _배향_이 포함됩니다(알고 있는 범위 내에서는 방향 보존 지도는 일반적인 그리스 이름이 없습니다).

_metric_ 또는 _distance_는 모든 \(u,v,w\in\Omega\)에 대해 만족하는 함수 \(d:\Omega\times\Omega\to[0,\infty)\)입니다.

_Identity of indiscernibles:_\(d(u,v)=0\) iff \(u=v\).

_Symmetry:_\(d(u,v)=d(v,u)\).

_Triangle inequality:_\(d(u,v)\leq d(u,w)+d(w,v)\.

메트릭 \((\Omega,d)\)이 장착된 공간을 _메트릭 공간_ 이라고 합니다.

고려해야 할 적절한 구조의 수준은 문제에 달려 있다. 예를 들어, 조직병리학 슬라이드 이미지를 분할할 때, 우리는 이미지의 뒤집힌 버전을 동등한 것으로 고려하기를 원할 수 있지만(현미경 아래에 놓으면 샘플이 뒤집힐 수 있기 때문에), 도로 표지판을 분류하려고 한다면, 방향 보존 변환을 대칭으로 고려하기를 원할 것이다(반사가 기호의 의미를 변경할 수 있기 때문에).

보존할 구조의 수준을 더하면 대칭군은 작아질 것이다. 실제로, 구조를 추가하는 것은 _부분군_을 선택하는 것과 동일하며, 이는 그 자체로 그룹의 공리를 만족하는 더 큰 그룹의 하위 집합이다:

\((\mathfrak{G},\circ)\)는 그룹이고 \(\mathfrak{H}\subseteq\mathfrak{G}\)는 서브세트라고 하자. \ (\mathfrak{H}\)는 \((\mathfrak{H},\circ)\)가 동일한 연산으로 그룹을 구성하면 \(\mathfrak{G}\)의 _하위 그룹_이라고 합니다.

예를 들어, Euclidean isometries의 그룹 \(\mathrm{E}(2)\)은 평면형 디페오모프들의 그룹 \(\operatorname{Diff}(2)\)의 하위 그룹이고, 차례로 배향 보존 isometries의 그룹 \(\mathrm{SE}(2)\)은 평면형 디페오모프들의 그룹 \(\mathrm{E}(2)\)의 하위 그룹이다. 이 구조의 위계는 서문에 요약된 에를랑겐 프로그램 철학을 따르며, 클라인의 구성에서 프로젝티브, 어파인 및 유클리드 기하학은 점점 더 불변하고 점진적으로 더 작은 그룹에 해당한다.

동형 및 오토모피즘 우리는 대칭을 구조 보존 및 반전 가능한 맵으로 설명했다. 이러한 지도는 _automorphisms_라고도 하며 객체가 자신과 동등한 방식을 설명합니다. 그러나 똑같이 중요한 지도 클래스는 두 개의 동일하지 않은 객체 간에 동등성을 나타내는 소위 _동형_이다. 이러한 개념은 종종 통합되지만 다음 논의에 대한 명확성을 만들기 위해서는 이를 구별하는 것이 필요하다.

차이를 이해하려면 집합 \(\Omega=\{0,1,2\}\)을 고려하십시오. 집합 \(\Omega\)의 오토모피즘은 순환 시프트 \(\tau(u)=u+1\mod 3\)와 같은 바이젝션 \(\tau:\Omega\to\Omega\)이다. 이러한 맵은 카디널리티 속성을 보존하고, \(\Omega\)를 자체에 매핑한다. 동일한 수의 원소를 갖는 다른 집합 \(\Omega^{\prime}=\{a,b,c\}\)이 있다면, \(\eta(0)=a\), \(\eta(1)=b\), \(\eta(2)=c\)와 같은 bijection \(\eta:\Omega\to\Omega^{\prime}\)은 _set isomorphism_이다.

그래프 4.1절에서 볼 수 있듯이 구조의 개념에는 노드의 수뿐만 아니라 연결성도 포함된다. 따라서 두 그래프 사이의 동형화 \(\eta:\mathcal{V}\to\mathcal{V}^{\prime}\) \(\mathcal{G}=(\mathcal{V},\mathcal{E})\)와 \(\mathcal{G}^{\prime}=(\mathcal{V}^{\prime},\mathcal{E}^{\prime)\)는 연결된 노드 쌍을 연결된 노드 쌍에 매핑하는 노드 사이의 바이젝션이고, 마찬가지로 연결되지 않은 노드 쌍에 대한 바이젝션이다. 따라서 두 개의 동형 그래프는 구조적으로 동일하며 노드의 순서만 다르다. 반면에 그래프 오토모픽 또는 대칭은 연결성을 유지하면서 그래프의 노드를 다시 자신에게 매핑하는 맵 \(\tau:\mathcal{V}\to\mathcal{V}\)이다. 비 자명한 오토모픽을 갖는 그래프(즉, \(\tau\neq\mathrm{id}\))는 대칭성을 나타낸다.

### 3.3 변형 안정성

섹션 3.1-3.2에서 소개된 대칭 형식론은 어떤 변환이 대칭으로 간주될지 정확히 알고 있는 이상적인 세계를 포착하며, 우리는 이러한 대칭을 정확하게 존중하기를 원한다. 예를 들어 컴퓨터 비전에서 우리는 평면 병진이 정확한 대칭이라고 가정할 수 있다. 그러나 현실 세계는 시끄럽고 이 모델은 두 가지 면에서 부족합니다.

첫째, 이 간단한 그룹들은 도메인 \(\Omega\)의 _global_ 대칭을 이해할 수 있는 방법을 제공하지만 (그리고 이를 확장하여, \(\mathcal{X}(\Omega)\)) 그들은 _local_ 대칭을 잘 포착하지 못한다. 예를 들어, 각각이 다른 방향을 따라 움직이는 여러 객체가 있는 비디오 장면을 고려한다. 후속 프레임들에서, 결과적인 장면은 대략 동일한 시맨틱 정보를 포함할 것이지만, 어느 전역 번역도 한 프레임으로부터 다른 프레임으로의 변환을 설명하지 못한다. 카메라에 의해 보여지는 변형가능한 3D 객체와 같은 다른 경우들에서, 객체 동일성을 보존하는 변환들의 그룹을 기술하는 것은 단순히 매우 어렵다. 이러한 예는 실제로 우리가 글로벌하고 정확한 불변성이 로컬하고 부정확한 변형으로 대체되는 훨씬 더 큰 변형 세트에 더 관심이 있음을 보여준다. 논의에서 우리는 도메인 \(\Omega\)이 고정된 설정과 신호 \(x\in\mathcal{X}(\Omega)\)가 변형되고 있는 설정과 도메인 \(\Omega\) 자체가 변형될 수 있는 설정의 두 시나리오를 구별할 것이다.

신호 변형에 대한 안정성 많은 응용에서 신호 \(x\)의 작은 변형이 \(f(x)\의 출력을 변경하지 않아야 한다는 선험적인 사실을 알고 있으므로 이러한 변형을 대칭으로 간주하는 것이 유혹적이다. 예를 들어, 우리는 작은 차이 형태 \(\tau\in\operatorname{Diff}(\Omega)\) 또는 작은 돌기를 대칭으로 볼 수 있다. 그러나 작은 변형은 큰 변형을 형성하기 위해 구성될 수 있으므로 "작은 변형"은 그룹을 형성하지 않으며 작은 변형에만 불변이나 등분성을 요구할 수 없다. 큰 변형은 실제로 입력의 의미 내용을 실질적으로 변화시킬 수 있기 때문에, 전체 그룹 \(\operatorname{Diff}(\Omega)\)을 대칭 그룹으로 사용하는 것도 좋은 생각이 아니다.

더 나은 접근법은 주어진 \(\tau\in\operatorname{Diff}(\Omega)\)가 주어진 대칭 부분군 \(\mathfrak{G}\subset\operatorname{Diff}(\Omega)\)(예를 들어, 복잡도 측정 \(c(\tau)\)을 갖는 변환들 \(c(\tau)=0\)로부터 얼마나 멀리 떨어져 있는지를 정량화하는 것이다. 이제 그룹 작업에서 정확한 불변성과 등분성에 대한 이전 정의를 '변형 안정성'의 '근사 불변성' 개념으로 대체할 수 있다.

\[\|f(\rho(\tau)x)-f(x)\|\leq Cc(\tau)\|x\|,\,\\forall x\in\mathcal{X}(\Omega) \tag{4}\]

여기서 \(\rho(\tau)x(u)=x(\tau^{-1}u)\)는 이전과 같고, 여기서 \(C\)는 신호 \(x\)와 무관한 일부 상수이다. 상기 식을 만족하는 함수 \(f\in\mathcal{F}(\mathcal{X}(\Omega))\)는 _기하학적으로 안정하다_라고 한다. 우리는 다음 섹션 3.4에서 그러한 함수의 예를 볼 것이다.

\(\tau\in\mathfrak{G}\)에 대한 \(c(\tau)=0\)이므로, 이 정의는 위에서 정의한 \(\mathfrak{G}\)-불변 특성을 일반화한다. 응용에서 그 유용성은 적절한 변형 비용을 도입하는 데 달려 있다. 연속적인 유클리드 평면상에서 정의되는 영상의 경우, \(c^{2}(\tau):=\int_{\Omega}\|\nabla\tau(u)\|^{2}\mathrm{d}u\)를 선택하는데, 이는 \(\tau\)의 '탄성' 즉, 상수 벡터장에 의한 변위와 얼마나 다른지를 측정한다. 이 변형 비용은 실제로 종종 _디리클레 에너지_라고 하는 규범이며 \(\tau\)가 번역 그룹에서 얼마나 멀리 떨어져 있는지 정량화하는 데 사용할 수 있다.

도메인 변형에 대한 안정성 많은 응용에서 변형되는 객체는 신호가 아니라 기하학적 도메인 \(\Omega\) 자체이다. 이것의 정규 인스턴스들은 그래프 및 다양체를 다루는 애플리케이션들이다: 그래프는 약간 상이한 사회적 관계들을 포함하는 상이한 시간의 인스턴스에서 소셜 네트워크를 모델링할 수 있거나(다음 그래프), 또는 다양체는 비-강성 변형을 겪는 3D 오브젝트를 모델링할 수 있다. 이러한 변형은 정량화될 수 있다.

그림 6: \(\Omega\)에서 자체로의 모든 양형 매핑 집합은 _집합 오토모피즘 그룹_\(\mathrm{Aut}(\Omega)\)을 형성하며, 이 중 대칭 그룹 \(\mathfrak{G}\)(원으로 표시됨)은 부분군이다. 기하학적 안정성은 \(\mathfrak{G}\)-불변성과 등분성의 개념을'\(\mathfrak{G}\) 주변의 변환'(회색 고리로 표시)으로 확장하고 변환 사이의 몇 가지 메트릭의 의미에서 정량화한다. 이 예에서, 이미지의 매끄러운 왜곡은 시프트에 가깝다.

다음과 같다. 만약 \(\mathcal{D}\)가 모든 가능한 가변 도메인의 공간(예: 모든 그래프의 공간 또는 Riemannian manifolds의 공간)을 나타내면, \(\Omega,\tilde{\Omega}\in\mathcal{D}\)에 대해 적절한 메트릭(거리) \(d(\Omega,\tilde{\Omega})\)을 만족하는 \(d(\Omega,\tilde{\Omega})\)를 정의할 수 있다. \(\Omega\)와 \(\tilde{\Omega}\)가 같은 의미라면, 예를 들어 그래프 편집 거리는 그래프가 동형일 때 사라지고, 측지선 거리가 장착된 Riemannian manifolds 사이의 Gromov-Hausdorff 거리는 두 manifolds가 등척일 때 사라진다.

그래프 편집 거리는 그래프 편집 작업의 시퀀스에 의해 두 그래프를 동형으로 만드는 최소 비용을 측정한다. Gromov-Hausdorff 거리는 두 메트릭 공간 사이의 대응의 가능한 가장 작은 메트릭 왜곡을 측정하며, Gromov (1981)를 참조한다.

이러한 도메인 간 거리의 일반적인 구성은 해당 구조가 가장 잘 보존되는 방식으로 도메인을 '정렬'하려는 역변환 매핑 \(\eta:\Omega\to\tilde{\Omega}\) 계열에 의존한다. 예를 들어, 그래프 또는 리만 매니폴드의 경우(측지선 거리를 갖는 메트릭 공간으로 간주됨), 이 정렬은 쌍별 인접성 또는 거리 구조(각각 \(d\) 및 \(\tilde{d}\)를 비교할 수 있고,

\[d_{\mathcal{D}}(\Omega,\tilde{\Omega})=\inf_{\eta\in\mathfrak{G}}\|d-\tilde{d }\circ(\eta\times\eta)\|\]

여기서 \(\mathfrak{G}\)는 bijections 또는 isometries와 같은 동형들의 그룹이고, 표준은 생성물 공간 \(\Omega\times\Omega\)에 걸쳐 정의된다. 즉, \(\Omega,\tilde{\Omega}\)의 원소들 사이의 거리는 내부 구조를 보존하는 모든 가능한 정렬을 고려함으로써 도메인들 사이의 거리로 '상승'된다. 신호 \(x\in\mathcal{X}(\Omega)\)와 변형된 도메인 \(\tilde{\Omega}\)이 주어지면, 변형된 신호 \(\tilde{x}=x\circ\eta^{-1}\in\mathcal{X}(\tilde{\Omega})\)를 고려할 수 있다.

이 표기법을 약간 악용하여, 우리는 \(\mathcal{X}(\mathcal{D})=\{(\mathcal{X}(\Omega),\Omega)\,\,\Omega\in\mathcal{D}\}\)를 다양한 영역에서 정의된 가능한 입력 신호의 앙상블로 정의한다. 함수 \(f:\mathcal{X}(\mathcal{D})\to\mathcal{Y}\)는 도메인 변형에 안정적이지만

\[\|f(x,\Omega)-f(\tilde{x},\tilde{\Omega})\|\leq C\|x\|d_{\mathcal{D}}(\Omega,\tilde{\Omega}) \tag{5}\]

모든 \(\Omega,\tilde{\Omega}\in\mathcal{D}\), \(x\in\mathcal{X}(\Omega)\). 우리는 등각 변형이 중요한 역할을 하는 섹션 4.4-4.6에서 다양체의 맥락에서 안정성 개념에 대해 논의할 것이다. 또한, 영역 변형에 대한 안정성은 Gama 등(2019)의 체적 형태의 변형 관점에서 후자를 봄으로써 신호 변형에 대한 안정성의 자연스러운 일반화임을 보여줄 수 있다.

### Scale Separation

변형 안정성은 전역 대칭 전치를 실질적으로 강화하지만 차원성의 저주를 극복하는 것 자체로는 충분하지 않지만, 비공식적으로 말하면 정의역의 크기가 커짐에 따라 (4)를 존중하는 "너무 많은" 함수가 여전히 존재한다는 의미에서는 그렇다. 이 저주를 극복하기 위한 핵심 통찰은 물리적 작업의 멀티스케일 구조를 이용하는 것이다. 멀티스케일 표현을 설명하기 전에, 우리는 규모보다는 주파수에 의존하는 푸리에 변환의 주요 요소들을 소개할 필요가 있다.

푸리에 변환과 전역 불변 중에서 가장 유명한 신호 분해는 하모닉 분석의 초석인 _푸리에 변환_이다. 고전적인 1차원 푸리에 변환

\[\hat{x}(\xi)=\int_{-\infty}^{+\infty}x(u)e^{-\mathrm{i}\xi u}\mathrm{d}u\]

\(\Omega=\mathbb{R}\) 영역에서 함수 \(x(u)\in L^{2}(\Omega)\)를 직교진동함수 \(\varphi_{\xi}(u)=e^{\mathrm{i}\xi u}\)의 선형조합으로 표현한다. 주파수로의 그러한 조직은 신호에 대한 중요한 정보, 예를 들어 그것의 매끄러움 및 지역화를 드러낸다. 푸리에 기저 자체는 깊은 기하학적 기초를 가지며, 그 기하학적 구조와 관련된 도메인의 자연 진동으로 해석될 수 있다(예를 들어, 베르거(2012) 참조).

푸리에 변환은 _convolution_ 의 이중 공식을 제공하므로 신호 처리에 중요한 역할을 합니다.

\[(x\star\theta)(u)=\int_{-\infty}^{+\infty}x(v)\theta(u-v)\mathrm{d}v\]

선형 신호 필터링의 표준 모델(여기서는 \(x\)는 신호를 나타내고 \(\theta\)는 필터를 나타냄)입니다. 다음에서 보여주겠지만, 컨볼루션 연산자는 푸리에 기저로 대각화되어, 컨볼루션이 각각의 푸리에 변환의 곱으로 표현될 수 있게 하고,

\[\widehat{(x\star\theta)}(\xi)=\hat{x}(\xi)\cdot\hat{\theta}(\xi),\]

신호처리에서 컨볼루션 정리로 알려진 사실.

알고 보니, 라플라시안과 같은 많은 근본적인 미분 연산자들은 유클리드 영역에 대한 컨볼루션으로 기술되어 있다. 이러한 미분 연산자는 매우 일반적인 기하학에 대해 본질적으로 정의될 수 있기 때문에 그래프, 그룹 및 다양체를 포함하여 유클리드 영역을 넘어 푸리에 변환을 확장하는 공식 절차를 제공한다. 이에 대해서는 4.4절에서 자세히 논의하기로 한다.

푸리에 변환의 필수 측면은 평활도 또는 컨덕턴스와 같은 신호와 도메인의 _글로벌_ 속성을 드러낸다는 것이다. 이러한 전역적 행동은 번역과 같은 도메인의 전역적 대칭이 있는 경우 편리하지만 보다 일반적인 차이를 연구하지는 않는다. 이것은 우리가 다음에 보는 바와 같이 공간 및 주파수 지역화를 거래하는 표현을 필요로 한다.

다중 스케일 표현 로컬 불변성의 개념은 푸리에 주파수 기반 표현에서 _웨이블릿_과 같은 다중 스케일 분해 방법의 초석인 _스케일 기반_ 표현으로 전환함으로써 표현될 수 있다. 다중 척도 방법의 본질적인 통찰은 도메인 \(\Omega\)에 정의된 함수를 공간과 주파수 모두에 국한된 기본 함수로 분해하는 것이다. 웨이블릿의 경우, 이는 변환 및 확장 필터(_mother wavelet_) \(\psi\)를 상관시켜 _연속 웨이블릿 변환_이라고 하는 결합된 공간-주파수 표현을 생성함으로써 달성된다.

\[(W_{\psi}x)(u,\xi)=\xi^{-1/2}\int_{-\infty}^{+\infty}\psi\left(\frac{v-u}{\xi} \right)x(v)\mathrm{d}v.\]

병진 및 확장된 필터는 _웨이블릿 원자_라고 하며, 이들의 공간적 위치 및 확장은 웨이블릿 변환의 좌표 \(u\) 및 \(xi\)에 대응한다. 이러한 좌표는 일반적으로 쌍방향(\(\xi=2^{-j}\) 및 \(u=2^{-j}k\))으로 샘플링되며 \(j\)는 _스케일_이라고 한다. 다중 스케일 신호 표현은 부분적 평활도와 같은 전역적 평활도를 넘어 규칙성 특성을 포착하는 측면에서 중요한 이점을 가져오며, 이는 90년대 신호 및 이미지 처리 및 수치 분석에서 인기 있는 도구로 만들었다.

다중스케일 표현의 변형 안정성: 푸리에 분해보다 다중스케일 국부 웨이블릿 분해의 이점은 기본 대칭 그룹에 가까운 작은 변형의 영향을 고려할 때 드러난다. 유클리드 영역과 번역 그룹에서 이 중요한 개념을 설명해보자. 푸리에 표현은 시프트 연산자를 대각화하므로(4.2절에서 더 자세히 볼 수 있듯이 컨볼루션으로 생각할 수 있음), 번역 변환을 위한 효율적인 표현이다. 그러나, 푸리에 분해는 고주파 변형 하에서 불안정하다. 반면 웨이블릿 분해는 이러한 경우에 안정적인 표현을 제공한다.

[MISSING_PAGE_EMPTY:29]

그리드, 그래프 및 다양체를 포함합니다. 비공식적으로 조대화는 가까운 점 \(u,u^{\prime}\in\Omega\)을 함께 동화시키므로 도메인에서 적절한 _metric_ 개념만 필요하다. \(\mathcal{X}_{j}(\Omega_{j},\mathcal{C}_{j}):=\{x_{j}:\Omega_{j}\to\mathcal{C}_{j}\}\)는 조대화 도메인 \(\Omega_{j}\)에 정의된 신호를 나타내는 함수 \(f:\mathcal{X}(\Omega)\to\mathcal{Y}\)가 크기의 인수분해를 허용하면 크기 \(j\)에서 국소적으로 안정하다고 비공식적으로 말한다. 여기서 \(P_{j}:\mathcal{X}(\Omega)\to\mathcal{X}_{j}(\Omega_{j})\)는 비선형 _굵은 결정립이고 \(f_{j}:\mathcal{X}_{j}(\Omega_{j})\to\mathcal{Y}\). 즉, 대상 함수 \(f\)는 전체 도메인에 걸쳐 기능 간의 복잡한 장거리 상호 작용에 의존할 수 있지만 국부적으로 안정적인 함수에서는 먼저 국부화된 상호 작용에 초점을 맞추고 거친 척도를 향해 전파함으로써 척도 간의 상호 작용을 _분리_할 수 있다.

이러한 원리는 소위 재규격화 그룹의 통계 물리학에서 나타나거나 빠른 다중극 방법과 같은 중요한 수치 알고리즘에서 활용되는 것처럼 물리학과 수학의 많은 영역에서 근본적으로 중요하다. 기계 학습에서 멀티스케일 표현과 로컬 불변성은 합성곱 신경망과 그래프 신경망의 효율성을 뒷받침하는 근본적인 수학적 원리이며 일반적으로 _로컬 풀링_ 형태로 구현된다. 향후 연구에서는 이러한 원리를 기하 영역에 걸쳐 통합하는 계산 조화 분석에서 도구를 추가로 개발하고 스케일 분리의 통계적 학습 이점을 밝힐 것이다.

### 3.5 Geometric Deep Learning의 Blueprint

섹션 3.1-3.4에서 논의된 대칭, 기하학적 안정성 및 스케일 분리의 기하학적 원리는 고차원 데이터의 안정적인 표현을 학습하기 위한 보편적인 청사진을 제공하기 위해 결합될 수 있다. 이러한 표현들은 대칭군 \(\mathfrak{G}\)이 부여된 도메인 \(\Omega\)에 정의된 신호 \(\mathcal{X}(\Omega,\mathcal{C})\)에서 동작하는 함수 \(f\)에 의해 생성될 것이다.

지금까지 설명한 기하학적 사전은 그러한 표현을 구축하기 위한 특정 _건축물_ 을 규정하지 않고 오히려 일련의 필요한 조건을 규정한다. 그러나, 그들은 이러한 기하학적 전적을 증명 가능하게 만족시키는 공리적 구성을 암시하는 한편, 그러한 전적을 만족시키는 임의의 목표 함수를 근사화할 수 있는 고도로 표현적 표현을 보장한다.

간단한 초기 관찰은 고도로 표현적인 표현을 얻기 위해, \(f\)가 선형이고 \(\mathfrak{G}\)-불변인 경우 모든 \(x\in\mathcal{X}(\Omega)\)에 대해 비선형 요소를 도입해야 하기 때문에,

\[f(x)=\frac{1}{\mu(\mathfrak{G})}\int_{\mathfrak{G}}f(\mathfrak{g}.x)\mathrm{d} \mu(\mathfrak{g})=f\left(\frac{1}{\mu(\mathfrak{G})}\int_{\mathfrak{G}}( \mathfrak{g}.x)\mathrm{d}\mu(\mathfrak{g})\right),\]

이는 \(\mathfrak{G}\)_-average_\(Ax=\frac{1}{\mu(\mathfrak{G})}\int_{\mathfrak{G}}(\mathfrak{g}.x)\mathrm{d} \mu(\mathfrak{g})\). 이미지 및 번역의 경우, 이는 입력의 평균 RGB 색상만을 사용하는 것을 수반할 것이다!

이 추론은 _선형 불변량_ 패밀리가 매우 풍부한 객체가 아님을 보여주지만, _선형 등분산_ 패밀리는 이제 설명할 것처럼 적절한 비선형 지도로 구성하여 풍부하고 안정적인 피쳐를 구성할 수 있기 때문에 훨씬 강력한 도구를 제공합니다. 실제로, \(B:\mathcal{X}(\Omega,\mathcal{C})\to\mathcal{X}(\Omega,\mathcal{C}^{\prime})\)이 임의의 \((\)non-linear\() 맵인 경우, \(\mathfrak{G}\)와 \(\mathfrak{g}.x)=\mathfrak{g}.B(x)\)를 만족시키는 \(\mathfrak{G}\)-equivariant 또한 \(\mathfrak{G}\)-equivariant이며, 여기서 \(\boldsymbol{\sigma}:\mathcal{X}(\Omega,\mathcal{C}^{\prime})\to\mathcal{X}(\Omega,\mathcal{C}^{\prime\prime\)\)는 \((\boldsymbol{\sigma}(x))(u):=\sigma(x(u))\)로 주어진 \(\sigma\의 원소별 인스턴스화임을 쉽게 확인할 수

이 간단한 성질은 \(\mathfrak{G}\)-불변의 매우 일반적인 계열을 정의할 수 있게 해주며, \(U\)를 그룹 평균 \(A\circ U:\mathcal{X}(\Omega,\mathcal{C})\to\mathcal{C}^{\prime\prime}\)과 합성한다. 따라서 \(\mathfrak{G}\)-불변함수가 \(B\) 및 \(\sigma\)의 적절한 선택에 대해 이러한 모델에 의해 임의의 정밀도로 근사될 수 있는지 여부가 자연스러운 질문이다. 그룹 평균을 일반적인 비선형 불변으로 적절하게 일반화함으로써 얕은 '기하학적' 네트워크도 보편적인 근사기임을 보여주기 위해 구조화되지 않은 벡터 입력에서 표준 범용 근사 정리를 적용하는 것은 어렵지 않다. 그러나 푸리에 대 웨이블릿 불변량의 경우 이미 설명한 바와 같이 얕은 전역 불변량과 변형 안정성 사이에는 근본적인 장력이 있다. 이것은 대신 _국소화된_ 등분산 지도를 고려하는 대체 표현에 동기를 부여합니다. \(\Omega\)에 거리 메트릭 \(d\)이 추가로 장착되어 있다고 가정하면, \((Ux)(u)\)이 \(\mathcal{N}_{u}=\{v:d(u,v)\leq r\}\의 값에만 의존하는 경우 로컬화된 등분산 맵 \(U\)을 호출합니다. \(\mathcal{N}_{u}\)는 일부 작은 반지름 \(r\)에 대해 \(x(v)\)의 값에만 의존하며, 후자의 집합 \(\mathcal{N}_{u}\)는 _수용 필드_라고 합니다.

한 층의 국부적 등분산 지도 \(U\)는 원거리 상호작용으로 함수를 근사할 수 없지만, 여러 개의 국부적 등분산 지도 \(U_{J}\circ U_{J-1}\cdots\circ U_{1}\)의 구성은 국부적 등분산 지도의 안정성 특성을 유지하면서 수용장을 증가시킨다. 수용 필드는 도메인을 조대화하는 다운샘플링 연산자를 인터리빙함으로써(메트릭 구조를 다시 가정하여), 다중해상도 분석(MRA, 예를 들어, Mallat(1999) 참조)으로 병렬화를 완료함으로써 더욱 증가된다.

요약하면, 기본 대칭 그룹에 대한 지식을 가진 입력 도메인의 기하학적 구조는 (i) 로컬 등분산 맵, (ii) 전역 불변 맵 및 (iii) 조대화 연산자의 세 가지 주요 구성 요소를 제공한다.

그림 8: 그래프에 예시된 기하학적 딥러닝 설계도. 전형적인 그래프 신경망 아키텍처는 순열 등분산 계층(컴퓨팅 노드-와이즈 특징), 로컬 풀링(그래프 조대화) 및 순열-불변 글로벌 풀링 계층(판독 계층)을 포함할 수 있다.

이러한 빌딩 블록들은 우리가 _기하학적 딥 러닝 청사진_이라고 지칭하는 스킴에서 이들을 함께 조합함으로써 규정된 불변 및 안정성 속성들을 갖는 풍부한 함수 근사 공간을 제공한다(그림 8).

Geometric Deep LearningOne의 다른 설정은 도메인 \(\Omega\)이 _고정_으로 가정되고 도메인에서 정의된 다양한 입력 신호에만 관심이 있거나 도메인이 정의된 신호와 함께 _변경_으로 입력의 일부일 때 설정을 중요한 구별을 할 수 있습니다. 이전 사례의 고전적인 사례는 이미지가 고정된 도메인(그리드)에 정의된 것으로 가정되는 컴퓨터 비전 애플리케이션에서 마주친다. 그래프 분류는 후자의 설정의 예로서, 그래프의 구조뿐만 아니라 그것에 정의된 신호(예를 들어, 노드 특징들)가 모두 중요하다. 도메인이 변하는 경우, 기하학적 안정성(\(\Omega\)의 변형에 둔감한 의미)은 기하학적 딥러닝 구조에서 중요한 역할을 한다.

이 설계도는 광범위한 기하학 영역에 걸쳐 사용할 수 있는 적절한 수준의 일반성을 가지고 있다. 따라서 상이한 기하학적 딥 러닝 방법들은 도메인, 대칭 그룹, 및 전술한 빌딩 블록들의 특정 구현 세부사항들의 선택에서 상이하다. 다음에서 볼 수 있듯이 현재 사용 중인 대규모 딥러닝 아키텍처는 이 스킴에 속하며 따라서 공통 기하 원리에서 파생될 수 있다.

다음 섹션(4.1-4.6)에서는 '5G'에 초점을 맞춘 다양한 기하 영역을 설명하고 섹션 5.1-5.8에서는 이러한 도메인에 대한 기하 딥러닝의 특정 구현을 설명한다.

\begin{tabular}{l l l}
**Architecture** & **Domain**\(\Omega\) & **Symmetry group**\(\mathfrak{G}\) \\ _CNN_ & Grid & Translation \\ _Spherical CNN_ & Sphere / SO(3) & Rotation SO(3) \\ _Intrinsic / Mesh CNN_ & Manifold & Isometry Iso(\(\Omega\)) / \\  & & Gauge symmetry SO(2) \\ _GNN_ & Graph & Permutation \(\Sigma_{n}\) \\ _Deep Sets_ & Set & Permutation \(\Sigma_{n}\) \\ _Transformer_ & Complete Graph & Permutation \(\Sigma_{n}\) \\ _LSTM_ & 1D Grid & Time warping \\ \end{tabular}

## 4 Geometric Domains: 5 Gs

우리 텍스트의 주요 초점은 그래프, 그리드, 그룹, 측지선 및 게이지에 있습니다. 이러한 맥락에서 '그룹'은 균질한 공간에서의 전역 대칭 변환, 다양체의 '지오데식스' 메트릭 구조, 접선 번들(및 일반적으로 벡터 번들)에 정의된 '게이즈' 로컬 참조 프레임을 의미한다. 이러한 개념들은 추후 보다 상세히 설명될 것이다. 다음 절에서는 공통의 주요 요소와 이러한 구조 사이의 주요 구별 특징에 대해 자세히 논의하고 이와 관련된 대칭 그룹을 설명한다. 우리의 설명은 일반성의 순서가 아니라, 사실 그리드는 그래프의 특정 사례이다 - 기하학적 딥러닝 청사진의 기초가 되는 중요한 개념을 강조하는 방법이다.

### Graph 및 Sets

사회학에서 입자 물리학에 이르기까지 과학의 여러 분야에서 그래프는 관계와 상호 작용 시스템의 모델로 사용된다. 우리의 관점에서 그래프는 순열 그룹으로 모델링된 매우 기본적인 유형의 불변성을 발생시킨다. 더욱이, 격자 및 집합과 같은 우리에게 관심있는 다른 객체들은 그래프의 특정한 경우로서 획득될 수 있다.

그래프_\(\mathcal{G}=(\mathcal{V},\mathcal{E})\)는 노드 쌍 사이의 노드_\(\mathcal{V}\)와 에지_\(\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}\)의 집합이다. 다음 논의의 목적을 위해, 우리는 모든 \(u\in\mathcal{V}\)에 대해 \(\mathbf{x}_{u}\)로 표시된 \(s\)-차원 _노드 특징_이 부여될 노드를 추가로 가정할 것이다. 소셜 네트워크는 아마도 가장 일반적으로 연구되는 그래프의 예 중 하나이며, 여기서 노드는 사용자를 나타내고, 에지는 그들 사이의 우정 관계에 해당하며, 노드는 나이, 프로필 사진 등과 같은 모델 사용자 속성을 특징으로 한다. 또한 간선 또는 전체 그래프에 기능을 부여하는 것도 종종 가능하지만 이것이 이 섹션의 주요 결과를 변경하지 않으므로 향후 작업으로 논의를 연기할 것이다.

도 9: Geometric Deep Learning의 5G: 전역 대칭을 갖는 그리드, 그룹 및 균질 공간, 그래프, 매니폴드 상의 지오데식스 및 메트릭, 및 게이지(접선 또는 특징 공간에 대한 프레임)를 포함한다.

그래프들의 주요 구조적 성질은 \(\mathcal{V}\)의 노드들이 특정한 순서로 제공되는 것으로 가정되지 않기 때문에 그래프들에 대해 수행되는 모든 연산들은 노드들의 순서에 의존하지 않아야 한다는 것이다. 그래프에 작용하는 함수가 만족해야 하는 바람직한 속성은 순열 불변이며, 이는 두 개의 동형 그래프에 대해 이러한 함수의 결과가 동일함을 의미한다. 우리는 이것을 우리의 청사진의 특정한 설정으로 볼 수 있는데, 여기서 도메인 \(\Omega=\mathcal{G}\)과 공간 \(\mathcal{X}(\mathcal{G},\mathbb{R}^{d})\)은 \(d\)-차원 노드별 신호의 설정이다. 우리가 고려하는 대칭성은 _순열 그룹_\(\mathfrak{G}=\Sigma_{n}\)에 의해 주어지며, 그 원소들은 모두 노드 인덱스 집합 \(\{1,\ldots,n\}\)의 가능한 순서들이다.

먼저 에지가 없는 그래프의 특별한 경우인 \(\mathcal{E}=\emptyset\)에 대한 순열 불변의 개념을 설명한다. 노드 특징을 \(n\times d\) 행렬의 행으로 쌓음으로써 \(\mathbf{X}=(\mathbf{x}_{1},\ldots,\mathbf{x}_{n})^{\top}\) 노드의 순서를 효과적으로 지정할 수 있다. 노드 집합에 대한 순열 \(\mathfrak{g}\in\Sigma_{n}\)의 작용은 \(\mathbf{X}\) 행의 재정렬에 해당하며, 이는 \(n\times n\)_순열 행렬_\(\rho(\mathfrak{g})=\mathbf{P}\)로 나타낼 수 있으며, 여기서 각 행과 열은 정확히 하나의 \(1\)을 포함하고 다른 모든 엔트리는 0이다.

그런 다음 이 집합에서 작동하는 함수 \(f\)는 순열 불변이라고 합니다. 이러한 순열 행렬 \(\mathbf{P}\)에 대해 \(f(\mathbf{P}\mathbf{X})=f(\mathbf{X})\라고 가정하면 _순열 불변_입니다. 하나의 간단한 그러한 기능은

\[f(\mathbf{X})=\phi\left(\sum_{u\in\mathcal{V}}\psi\left(\mathbf{x}_{u}\right) \right)\, \tag{6}\]

여기서 함수 \(\psi\)는 모든 노드의 기능에 독립적으로 적용되며 \(\phi\)는 _sum-aggregated_ 출력에 적용됩니다. 합은 입력이 제공되는 순서와 무관하므로 이러한 함수는 노드 집합의 순열에 대해 불변하므로 노드가 어떻게 순열되더라도 항상 동일한 출력을 반환하도록 보장됩니다.

위와 같은 함수는 '글로벌' 그래프 단위 출력을 제공하지만, 매우 자주 우리는 노드 단위 방식으로 '로컬'로 작동하는 함수에 관심을 가질 것이다. 예를 들어 모든 노드의 피쳐를 _업데이트_ 하는 데 일부 함수를 적용 하 여 _잠재 노드 피쳐 집합을 얻을 수 있습니다. 이러한 잠재 특징을 행렬로 쌓으면 \(\mathbf{H}=\mathbf{F}(\mathbf{X})\)는 더 이상 순열 불변이다: \(\mathbf{H}\)의 행의 순서는 \(\mathbf{X}\)의 행의 순서에 _tied_ 되어야 하므로 어떤 출력 노드 특징이 어떤 입력 노드에 해당하는지를 알 수 있다. 대신 입력의 순열에 "커밋"하면 결과 개체를 일관되게 순열한다고 명시하는 더 세밀한 _순열 등분산_ 개념이 필요하다. 형식적으로, \(\mathbf{F}(\mathbf{X})\)는 임의의 순열 행렬에 대해 \(\mathbf{P}\mathbf{F}(\mathbf{P}\mathbf{X})=\mathbf{P}\mathbff{F}(\mathbf{X})\라고 가정하면 _순열 등변수_ 함수이다. 공유 노드별 선형 변환

\[\mathbf{F}_{\mathbf{\Theta}}(\mathbf{X})=\mathbf{X}\mathbf{\Theta} \tag{7}\]

가중치 행렬로 지정된 \(\mathbf{\theta}\in\mathbb{R}^{d\times d^{\prime}}\)은 이러한 순열 등분산 함수의 한 가지 가능한 구성이며, 본 예제에서는 \(\mathbf{h}_{u}=\mathbf{\theta}^{\top}\mathbf{x}_{u}\) 형태의 잠재 특징을 생성한다.

이 구조는 기하학 딥 러닝 청사진에서 자연스럽게 발생합니다. 첫째, 선형 등분산 함수(형태 \(\mathbf{P}\mathbf{P}\mathbf{X}=\mathbf{P}\mathbf{F}\mathbf{X}\))를 특성화할 수 있는데, 이 함수는 두 개의 생성자(형태 \(\mathbf{F}_{1}\mathbf{X}=\mathbf{X}\)와 평균 \(\mathbf{F}_{2}\mathbf{X}=\frac{1}{n}\mathbf{1}\mathbf{1}^{\top}\mathbf{X}=\frac{1}{n}\sum_{u=1}^{n}\mathbf{x}_{u}\)의 선형 조합으로 작성될 수 있음을 쉽게 확인할 수 있다. 섹션 5.4에서 설명되는 바와 같이, 인기 있는 딥 세트(Zaheer et al., 2017) 아키텍처는 정확하게 이 청사진을 따른다.

우리는 이제 집합에서 그래프로의 순열 불변성과 등분성의 개념을 일반화할 수 있다. 일반 설정 \(\mathcal{E}\neq\emptyset\)에서 그래프 연결성은 \(n\times n\)_adjacency matrix_\(\mathbf{A}\)로 정의할 수 있습니다.

\[a_{uv}=\begin{cases}1&(u,v)\in\mathcal{E}\\ 0&\text{otherwise}.\end{cases} \tag{8}\]

이제 인접성 및 특징 행렬 \(\mathbf{A}\) 및 \(\mathbf{X}\)은 "동기화"된다는 점에 유의한다. \(a_{uv}\)는 \(\mathbf{X}\)의 \(u\)번째 및 \(v\)번째 행에 의해 기술된 노드들 사이의 인접성 정보를 지정한다는 의미에서. 따라서, 노드 특징에 순열 행렬 \(\mathbf{P}\)을 적용하는 것은 \(\mathbf{A}\)의 행과 열, \(\mathbf{P}\mathbf{A}\mathbf{P}^{\top}\)에 자동으로 적용하는 것을 의미한다. 우리는 (그래프 방향 함수) \(f\)가 _순열 불변인 경우라고 한다.

\[f(\mathbf{P}\mathbf{X},\mathbf{P}\mathbf{A}\mathbf{P}^{\top})=f(\mathbf{X},\mathbf{A}) \tag{9}\]

and (a node-wise function) \(\mathbf{F}\) is _permutation equivariant_ if

\[\mathbf{F}(\mathbf{P}\mathbf{X},\mathbf{P}\mathbf{A}\mathbf{P}^{\top})= \mathbf{P}\mathbff{F}(\mathbf{X},\mathbf{A}) \tag{10}\

임의의 순열 행렬 \(\mathbf{P}\).

여기서 다시, 우리는 먼저 선형 등분산 함수들을 특성화할 수 있다. Maron et al. (2018)에 의해 관찰된 바와 같이, 식 (10)을 만족하는 임의의 선형 \(\mathbf{F}\)은 15개의 선형 생성기의 선형 조합으로 표현될 수 있다. 놀랍게도, 이 생성기 계열은 \(n\)_에 독립적이다. 이러한 생성기 중에서 우리의 청사진은 특히 _로컬_인 것, 즉 노드 \(u\)의 출력이 그래프의 이웃 노드에 직접 의존하는 것을 옹호한다. 우리는 노드가 다른 노드에 이웃한다는 것이 무엇을 의미하는지 정의함으로써 모델 구성에서 이 제약을 명시적으로 공식화할 수 있다.

노드 \(u\)의 (무지향) _근접_ 은 때때로 _1-hop_ 로도 정의됩니다.

\[\mathcal{N}_{u}=\{v:(u,v)\in\mathcal{E}\operatorname{or}\left(v,u\right)\in\mathcal{E}\} \tag{11}\]

그리고 멀티세트로서 _근접 지형지물_

\[\mathbf{X}_{\mathcal{N}_{u}}=\{\!\{\mathbf{x}_{v}:v\in\mathcal{N}_{u}\}\!\}. \tag{12}\]

1-홉 이웃에서 작동하는 것은 청사진의 _지역성_ 측면과 잘 일치합니다. 즉, 그래프 위의 메트릭을 \(\mathcal{E}\)의 에지를 사용하여 노드 간의 _가장 짧은 경로 거리로 정의합니다.

따라서 GDL 설계도는 노드 및 그 이웃의 특성 \(\phi(\mathbf{x}_{u},\mathbf{X}_{\mathcal{N}_{u}})\)에 대해 동작하는 _local_ 함수 \(\phi\)를 지정함으로써 그래프에서 순열 등분산 함수를 구성하는 일반적인 레시피를 산출한다. 그런 다음, 격리된 모든 노드의 이웃에 \(\phi\)를 적용하여 순열 등분산 함수 \(\mathbf{F}\)를 구성할 수 있다(도 10 참조):

\[\mathbf{F}(\mathbf{X},\mathbf{A})=\left[\begin{array}{ccc}\longrightarrow& \phi(\mathbf{x}_{1},\mathbf{X}_{\mathcal{N}_{1}})&\longrightarrow\\\longrightarrow&\phi(\mathbf{x}_{2},\mathbf{X}_{\mathcal{N}_{2}})&\longrightarrow\\ &\vdots&\\\longrightarrow&\phi(\mathbf{x}_{n},\mathbf{X}_{\mathcal{N}_{n}})&\longrightarrow \end{array}\right] \tag{13}\

\(\mathbf{F}\)는 각 노드에 공유 함수 \(\phi\)를 로컬로 적용하여 구성되므로, 그 순열 등분성은 \(\phi\)의 출력에 의존하며, \(\mathcal{N}_{u}\)의 노드 순서와 무관하다. 따라서, \(\phi\)가 순열 불변으로 구축되면, 이 속성은 만족된다. 향후 연구에서 살펴보겠지만, \(\phi\)의 선택은 이러한 도식의 표현력에 중요한 역할을 한다. \(\phi\)가 주입형일 때, 두 그래프가 반복적인 색상 정제 절차에 의해 동형이 되기 위한 필수 조건을 제공하는 그래프 이론에서 고전적인 알고리즘인 _Weisfeiler-Lehman 그래프 동형 테스트_의 한 단계와 동일하다.

이 예에서 집합에 정의된 함수와 보다 일반적인 그래프 사이의 차이는 후자의 경우 도메인의 구조를 명시적으로 설명할 필요가 있다는 점도 주목할 가치가 있다. 결과적으로 그래프는 기계 학습 문제에서 도메인이 입력의 일부가 된다는 점에서 두각을 나타내지만 집합과 그리드(그래프의 특정 경우 모두)를 다룰 때 피쳐만 지정하고 도메인을 _고정_으로 가정할 수 있다. 이러한 구분은 우리의 논의에서 반복되는 모티브가 될 것이다. 그 결과, 그래프에 대한 대부분의 학습 문제에서 기하학적 안정성(영역 변형에 대한 불변) 개념이 매우 중요하다. 순열 불변 및 등분산 함수가 동형(위상-등가) 그래프에서 동일한 출력을 생성한다는 것은 우리의 구성에서 쉽게 따른다. 이러한 결과는 대략 동형 그래프로 일반화될 수 있으며, 그래프 섭동 하에서 안정성에 대한 몇 가지 결과가 존재한다(Levie et al., 2018). 우리는 다양체에 대한 논의에서 이 중요한 지점으로 돌아갈 것이며, 우리는 그러한 불변성을 더 자세히 연구하기 위한 수단으로 사용할 것이다.

둘째, 그래프와 그리드는 추가적인 구조로 인해 집합과 달리 단순하지 않은 방식으로 조대화될 수 있어 다양한 풀링 연산이 발생한다.

### 4.2 그리드 및 유클리드 공간

우리가 고려하는 두 번째 유형의 객체는 그리드입니다. 딥러닝의 영향은 컴퓨터 비전, 자연어 처리, 음성 인식 등에서 특히 극적이었다고 해도 과언이 아니다. 이러한 응용 프로그램은 모두 기본 그리드 구조라는 기하학적 공통 분모를 공유합니다. 이미

그림 10: 모든 이웃에 순열 불변 함수 \(\phi\)를 적용하여 그래프 위에 순열-등변수 함수를 구성하는 예제입니다. 이 경우, \(\phi\)는 노드 \(b\)의 특성 \(\mathbf{x}_{b}\) 뿐만 아니라 이웃 특성의 다중 집합인 \(\mathbf{X}_{\mathcal{N}_{b}}=\{\!\!\{\mathbf{x}_{a},\mathbf{x}_{b},\mathbf{x} _{c},\mathbf{x}_{d},\mathbf{x}_{e}\}\!\}\!\}\. 모든 노드의 이웃에 이러한 방식으로 \(\phi\)를 적용하면 잠재자질의 결과 행렬의 행이 복원된다 \(\mathbf{H}=\mathbf{F}(\mathbf{X},\mathbf{A})\).

[MISSING_PAGE_FAIL:40]

(\mathbf{C}(\mathbf{\theta})\mathbf{x}=\mathbf{x}\star\mathbf{\theta}\). \(\mathbf{\theta}=(0,1,0,\dots,0)^{\top}\)의 특정한 선택은 벡터들을 한 위치만큼 오른쪽으로 이동시키는 특별한 순환행렬을 산출한다. 이 행렬을 (오른쪽) _shift_ 또는 _번역 연산자_라고 하며 \(\mathbf{S}\)로 나타냅니다.

순환 행렬의 곱은 순환 행렬의 _교환성_ 특성에 의해 특징지어질 수 있다. 즉, 순환 행렬의 곱은 임의의 \(\mathbf{\theta}\) 및 \(\mathbf{\eta}\)에 대해 \(\mathbf{C}(\mathbf{\theta})\mathbf{C}(\mathbf{\eta})=\mathbf{C}(\mathbf{\eta})\mathbf{C}(\mathbf{\theta})\)이다. 시프트는 순환 행렬이므로 컨볼루션 연산자의 익숙한 _번역_ 또는 _시프트 등분산_ 을 얻는다.

\[\mathbf{SC}(\mathbf{\theta})\mathbf{x}=\mathbf{C}(\mathbf{\theta})\mathbf{S}\mathbf{x}.\]

근본적인 대칭군(번역군)이 아벨리아인이기 때문에 이러한 교환성 성질은 놀라운 일이 아닐 것이다. 더욱이, 반대 방향도 참인 것으로 보이며, 즉 매트릭스는 시프트와 함께 통근하면 순환한다. 이것은 차례로 컨벌루션을 번역 등분산 선형 연산으로 _정의할 수 있게 해주며 기하학적 전수의 힘과 기하 ML의 전반적인 철학을 잘 보여준다: 컨벌루션은 번역 대칭의 첫 번째 원리에서 나온다.

집합 및 그래프의 상황과 달리 선형 독립 시프트-등분 함수(컨볼루션)의 수는 도메인 크기에 따라 증가한다(순환 행렬의 각 대각선에 하나의 자유도가 있기 때문이다). 그러나, 스케일 분리 사전 보장 필터는 _로컬_일 수 있고, 결과적으로 층당 동일한 \(\theta(1)\)-파라미터 복잡도를 초래할 수 있으며, 이는 컨볼루션 신경망 아키텍처의 구현에서 이러한 원리들의 사용에 대해 논의할 때 섹션 5.1에서 검증할 것이다.

이산 푸리에 변환의 유도 우리는 이미 푸리에 변환과 컨볼루션에 대한 연결을 언급했는데, 푸리에 변환이 컨볼루션 연산을 대각화한다는 사실은 푸리에 변환의 요소별 곱으로서 주파수 영역에서 컨벌루션을 수행하기 위해 신호 처리에 사용되는 중요한 속성이다. 그러나 교과서에서는 푸리에 변환이 어디에서 왔으며 푸리에 기초에 대해 그렇게 특별한 것을 설명하는 경우는 거의 없는 이 사실만 명시하고 있다. 여기서 우리는 그것을 보여줄 수 있는데, 어떻게 기초적인 것이 대칭의 기본 원리인지 다시 한번 보여준다.

이를 위해 선형 대수학에서 (대각선이 가능한) 행렬이 상호 통근하는 경우 _공동 대각선이 가능한_ 사실을 떠올려라. 즉, 모든 순환 행렬에 공통 고유 기준이 존재하며, 고유 값만 다르다. 따라서 우리는 하나의 순환행렬을 선택하고 그것의 고유벡터를 계산할 수 있다--우리는 이것들이 다른 모든 순환행렬의 고유벡터가 될 것이라고 확신한다. 고유 벡터가 이산 푸리에 기저인 시프트 연산자를 선택하는 것이 편리하다.

\[\mathbf{\varphi}_{k}=\frac{1}{\sqrt{n}}\left(1,e^{\frac{2\pi\mathrm{ik}}{n}},e^{ \frac{4\pi\mathrm{ik}}{n}},\ldots,e^{\frac{2\pi\mathrm{i}(n-1)k}{n}}\right)^{ \top},\hskip 14.226378ptk=0,1,\ldots,n-1,\

우리는 \(n\times n\) 푸리에 행렬 \(\mathbf{\Phi}=(\mathbf{\varphi}_{0},\ldots,\mathbf{\varphi}_{n-1})\)로 배열할 수 있다. \(\mathbf{\Phi}^{*}\)에 의한 곱셈은 이산 푸리에 변환(DFT)을 제공하고, \(\mathbf{\Phi}\)에 의한 역 DFT를 제공하며,

\[\hat{x}_{k}=\frac{1}{\sqrt{n}}\sum_{u=0}^{n-1}x_{u}e^{-\frac{2\pi\mathrm{ik}u}{n}}\hskip 42.679134ptx_{u}=\frac{1}{\sqrt{n}}\sum_{k=0}^{n-1}\hat{x}_{k}e^{ +\frac{2\pi\mathrm{ik}u}{n}}}.\

모든 순환 행렬은 공동으로 대각화할 수 있기 때문에 푸리에 변환에 의해 대각화되며 고유값만 다르다. 순환 행렬의 고유값 \(\mathbf{C}(\mathbf{\theta})\)은 필터의 푸리에 변환이므로(예: Bamieh (2018 참조)), \(\mathbf{\hat{\theta}}=\mathbf{\Phi}^{*}\mathbf{\theta}\) 우리는 컨볼루션 정리를 얻는다:

\[\mathbf{C}(\mathbf{\theta})\mathbf{x}=\mathbf{\Phi}\left[\begin{array}{ccc}\hat{ \theta}_{0}&&\\ &\ddots&&\\ &&\hat{\theta}_{n-1}\end{array}\right]\mathbf{\Phi}^{*}\mathbf{x}=\mathbf{\Phi}(\mathbf{ \hat{\theta}}\odot\hat{\mathbf{x}}})\]

푸리에 행렬 \(\mathbf{\Phi}\)은 특별한 대수적 구조를 가지므로, FFT 알고리즘을 이용하여 \(\mathbf{\Phi}^{*}\mathbf{x}\)와 \(\mathbf{\Phi}\mathbf{x}\)의 곱을 \(\mathcal{O}(n\log n)\)의 복잡도로 계산할 수 있다. 이것은 주파수 영역 필터링이 신호 처리 분야에서 널리 사용되는 이유 중 하나이며, 필터는 일반적으로 주파수 영역에서 직접 설계되므로 푸리에 변환 \(\mathbf{\hat{\theta}}\)은 명시적으로 계산되지 않는다.

여기에서 수행한 푸리에 변환 및 컨볼루션의 도출의 교훈적 값 외에도 이러한 개념을 그래프로 일반화하는 스킴을 제공한다. 고리 그래프의 인접 행렬이 정확히 시프트 연산자임을 깨닫고, 인접 행렬의 고유 벡터를 계산함으로써 그래프 푸리에 변환 및 컨볼루션 연산자의 유추를 개발할 수 있다(예를 들어, Sandryhaila 및 Moura (2013) 참조). 때때로 '스펙트럴 GNN'이라고 불리는 CNN에 유추하여 그래프 신경망을 개발하려는 초기 시도는 이 정확한 청사진을 활용했다. 우리는 섹션 4.4-4.6에서 이 비유가 몇 가지 중요한 한계를 가지고 있음을 볼 것이다. 첫 번째 한계는 격자가 고정되어 있으므로 그 위의 모든 신호를 동일한 푸리에 기반으로 나타낼 수 있다는 사실에서 비롯된다. 이에 비해 일반적인 그래프에서는 푸리에 기저가 그래프의 구조에 따라 달라진다. 따라서 우리는 두 개의 다른 그래프에서 푸리에 변환을 직접 비교할 수 없습니다. 이 문제는 기계 학습 문제의 일반화 부족으로 번역됩니다. 둘째, 1차원 격자의 텐서 곱으로 구성된 다차원 격자는 기본 구조를 유지한다: 푸리에 기저 요소와 그에 대응하는 주파수(고유값)는 다차원으로 구성될 수 있다. 예를 들어 이미지에서는 수평 및 수직 주파수에 대해 자연스럽게 이야기할 수 있으며 필터는 _방향_ 개념을 가지고 있습니다. 그래프에서 푸리에 영역의 구조는 푸리에 기저 함수를 해당 주파수의 크기만큼만 정리할 수 있기 때문에 1차원이다. 결과적으로, 그래프 필터는 방향 또는 _등방성_ 을 망각한다.

완전성을 위해 연속 푸리에 변환의 유도 및 다음 논의의 세그웨이로서 연속 설정에서 분석을 반복한다. 섹션 3.4에서와 같이, \(\Omega=\mathbb{R}\) 및 번역 연산자 \((S_{v}f)(u)=f(u-v)\) 시프트 \(f\)에 정의된 함수를 일부 위치 \(v\)만큼 고려한다. 푸리에 기저 함수 \(\varphi_{\xi}(u)=e^{\mathrm{i}\xi u}\)에 \(S_{v}\)를 적용하면, 지수의 연관성에 의해,

\[S_{v}e^{\mathrm{i}\xi u}=e^{\mathrm{i}\xi(u-v)}=e^{-\mathrm{i}\xi v}e^{\mathrm{i}\xi u},\]

즉, \(\varphi u_{\xi}(u)\)는 \(S_{v}\)의 복소 고유 벡터로 복소 고유 값 \(e^{-\mathrm{i}\xi v}\) - 이산 설정에서 우리가 겪었던 상황을 정확히 반영한다. \(S_{v}\)는 임의의 \(p\) 및 \(x\in L_{p}(\mathbb{R})\)에 대해 단일 연산자(즉, \(\|S_{v}x\|_{p}=\|x\|_{p}\))이기 때문에, 임의의 고유값 \(\lambda\)은 \(|\lambda|=1\)을 만족해야 하며, 이는 위에서 발견된 고유값 \(e^{-\mathrm{i}\xi v}\)에 정확하게 대응한다. 또한 변환 연산자의 스펙트럼은 _단순_하여 동일한 고유값을 공유하는 두 함수가 반드시 공선적이어야 함을 의미한다. 실제로, 일부 \(\xi_{0}\)에 대해 \(S_{v}f=e^{-\mathrm{i}\xi_{0}v}f\)라고 가정한다. 양쪽의 푸리에 변환을 취하면, 우리는

\[\forall\;\xi\;,\;e^{-\mathrm{i}\xi v}\hat{f}(\xi)=e^{-\mathrm{i}\xi_{0}v}\hat{f}(\xi)\;,\]

이는 \(\xi\neq\xi_{0}\)에 대해 \(\hat{f}(\xi)=0\)이므로 \(f=\alpha\varphi_{\xi_{0}}\)임을 의미한다.

일반 선형 연산자 \(C\)의 경우 변환 등분산(\(S_{v}C=CS_{v}\))이 있습니다.

\[S_{v}Ce^{\mathrm{i}\xi u}=CS_{v}e^{\mathrm{i}\xi u}=e^{-\mathrm{i}\xi v}Ce^{ \mathrm{i}\xi u},\]

또한 \(Ce^{\mathrm{i}\xi u}\)는 고유값 \(e^{-\mathrm{i}\xi v}\)을 갖는 \(S_{v}\)의 고유함수이며, 여기에서 \(Ce^{\mathrm{i}\xi u}=\beta\varphi_{\xi}(u)\)의 스펙트럼의 단순함에서 비롯된다; 즉, 푸리에 기저는 모든 변환 등분산 연산자의 고유함수가 된다. 결과적으로, \(C\)는 푸리에 영역에서 _diagonal_이고, \(Ce^{\mathrm{i}\xi u}=\hat{p}_{C}(\xi)e^{\mathrm{i}\xi u}\로 표현될 수 있으며, 여기서 \(\hat{p}_{C}(\xi)\)는 서로 다른 주파수에 작용하는 _전달 함수_이다. \(\xi\). 마지막으로 임의의 함수 \(x(u)\에 대해 선형성에 의해,

\[(Cx)(u) = C\int_{-\infty}^{+\infty}\hat{x}(\xi)e^{\mathrm{i}\xi u}\mathrm{ d}\xi=\int_{-\infty}^{+\infty}\hat{x}(\xi)\hat{x}(\xi)\hat{p}_{C}(\xi)e^{\mathrm{i}\xi u} \mathrm{d}\xi\] \[= \int_{-\infty}^{+\infty}p_{C}(v)x(u-v)\mathrm{d}v\ = (x\star p_{C})(u),\]

여기서, \(p_{C}(u)\)는 \(\hat{p}_{C}(\xi)\)의 역 푸리에 변환이다. 따라서 모든 선형 변환 등분산 연산자는 컨볼루션이다.

### 그룹 및 동질 공간

그리드에 대한 우리의 논의는 시프트와 컨벌루션이 밀접하게 연결되는 방식을 강조했는데, 컨벌루션은 선형 시프트-컨벌루션 연산이고 그 반대의 경우 시프트-컨벌루션 선형 연산자는 컨벌루션이다. 또한, 시프트 연산자는 퓨리에 변환에 의해 공동으로 대각화될 수 있다. 알고 보니, 이것은 훨씬 더 큰 이야기의 일부입니다: 컨볼루션과 푸리에 변환은 모두 우리가 합하거나 적분할 수 있는 모든 대칭 그룹에 대해 _정의될 수 있습니다.

유클리드 도메인 \(\Omega=\mathbb{R}\)을 고려한다. 우리는 컨벌루션을 패턴 매칭 연산으로 이해할 수 있다: 우리는 필터 \(\theta(u)\)의 시프트된 복사본을 입력 신호 \(x(u)\)와 매칭한다. 점 \(u\)에서의 콘볼루션 \((x\star\theta)(u)\)의 값은 필터 _shifted by \(u\)_를 갖는 신호 \(x\)의 내적이고,

\[(x\star\theta)(u)=\langle x,S_{u}\theta\rangle=\int_{\mathbb{R}}x(v)\theta(u+v)\mathrm{d}v.\]

이 경우 \(u\)는 도메인에서 _a 지점 \(\Omega=\mathbb{R}\)_ 이며 도메인 자체에서 식별할 수 있는 번역 그룹의 요소 \(\mathfrak{G}=\mathbb{R}\)입니다. 이제 우리는 번역 그룹을 다른 그룹 \(\mathfrak{G}\)으로 대체함으로써 \(\Omega\)에 작용하는 이 구조를 일반화하는 방법을 보여줄 것이다.

3절에서 논의한 그룹 컨볼루션은 영역 \(\Omega\)에 대한 그룹 \(\mathfrak{G}\)의 작용은 신호 공간 \(\mathcal{X}(\Omega)\)에 대한 표현 \(\rho\)을 \(\rho(\mathfrak{g})x(u)=x(\mathfrak{g}^{-1}u)\를 통해 유도한다. 위의 예에서, \(\mathfrak{G}\)는 좌표, \(u+v\)를 이동시킴으로써 요소가 작용하는 변환 그룹인 반면, \(\rho(\mathfrak{g})\)는 \((S_{v}x)(u)=x(u-v)\)로서 신호에 작용하는 시프트 연산자이다. 마지막으로, 신호에 필터를 적용하기 위해, 우리는 \(\mathcal{X}(\Omega)\)가 힐버트 공간이라는 가정을 내적과 함께 호출한다.

\[\langle x,\theta\rangle=\int_{\Omega}x(u)\theta(u)\mathrm{d}u,\]

여기에서 우리는 단순화를 위해 스칼라 값 신호 \(\mathcal{X}(\Omega,\mathbb{R})\); 일반적으로 내부 곱은 식 (2)의 형태를 갖는다고 가정했다.

따라서 신호를 변환하고 필터와 일치시키는 방법을 정의하면 \(\Omega\)의 신호에 대한 _그룹 컨볼루션_ 을 정의할 수 있으며,

\[(x\star\theta)(\mathfrak{g})=\langle x,\rho(\mathfrak{g})\theta\rangle=\int_{ \Omega}x(u)\theta(\mathfrak{g}^{-1}u)\mathrm{d}u. \tag{14}\]

\(x\star\theta\)는 도메인 \(\mega\)의 점이 아닌 그룹 \(\mathfrak{G}\)_의 _요소 \(\mathfrak{g}\)에서 값을 취합니다. 따라서 \(x\star\theta\)를 입력으로 하는 다음 레이어는 그룹 \(\mathfrak{G}\)_에 정의된 신호에 작용해야 합니다.

전통적인 유클리드 합성곱이 shift-equivariant인 것처럼, 더 일반적인 그룹 합성곱은 _\(\mathfrak{G}\)-equivariant_이다. 주요 관찰은 신호 \(x\)와 \(\mathfrak{g}\)-변환된 필터 \(\rho(\mathfrak{g})\theta\)를 일치시키는 것은 역변환된 신호 \(\rho(\mathfrak{g}^{-1})x\)와 변환되지 않은 필터 \(\theta\)를 일치시키는 것과 동일하다는 것이다. 수학적으로, 이것은 \(\langle x,\rho(\mathfrak{g})\theta\rangle=\langle\rho(\mathfrak{g}^{-1})x,\theta\rangle\로 표현될 수 있다. 이러한 통찰을 통해, 그룹 컨볼루션(14)의 \(\mathfrak{G}\)-등분성은 그룹 표현의 정의 및 정의 속성으로부터 즉시 뒤따른다 \(\rho(\mathfrak{h}^{-1})\rho(\mathfrak{g})=\rho(\mathfrak{h}^{-1}\mathfrak{g})\)

\[(\rho(\mathfrak{h})x\star\theta)(\mathfrak{g})=\langle\rho(\mathfrak{h})x, \rho(\mathfrak{g})\theta\rangle=\langle x,\rho(\mathfrak{h}^{-1}\mathfrak{g}) \theta\rangle=\rho(\mathfrak{h})(x\star\theta)(\mathfrak{g}).\]

몇 가지 예를 살펴보겠습니다. 1차원 격자의 경우는 \(\Omega=\mathbb{Z}_{n}=\{0,\ldots,n-1\}\)과 순환이동군 \(\mathfrak{G}=\mathbb{Z}_{n}\)을 선택하여 구한다. 이 경우 그룹요소는 지수들의 순환이동으로, 즉 요소 \(\mathfrak{g}\in\mathfrak{G}\)는 \(\mathfrak{g}.v=v-u\operatorname{mod}n\)와 같은 일부 \(u=0,\ldots,n-1\)로 식별될 수 있는 반면, 역요소는 \(\mathfrak{g}^{-1}.v=v+u\operatorname{mod}n\)이다. 중요한 것은, 이 예에서 _group_(shifts)의 엘리먼트들은 또한 _domain_(indices)의 엘리먼트들이다. 따라서 우리는 표기법의 일부 \(\mathbb{S}^{2}\) 남용을 통해 두 구조(즉, \(\Omega=\mathfrak{G}\)); 이 경우 그룹 컨볼루션에 대한 우리의 표현을 식별할 수 있다.

\[(x\star\theta)(\mathfrak{g})=\sum_{v=0}^{n-1}x_{v}\,\theta_{\mathfrak{g}^{-1}v},\]

익숙한 컨볼루션 \((x\star\theta)_{u}=\sum_{v=0}^{n-1}x_{v}\,\theta_{v+u\bmod n}\).

구형 콘볼루션은 2차원 구 \(\Omega=\mathbb{S}^{2}\)와 회전군, _특수직교군_\(\mathfrak{G}=\operatorname{SO}(3)\)를 고려한다. 교육학적 이유로 선택되었지만, 이 예는 실제로 매우 실용적이며 수많은 응용 분야에서 발생한다. 예를 들어, 천체 물리학에서 관측 데이터는 종종 자연적으로 구면 기하학을 가지고 있다. 또한, 구형 대칭은 분자를 모델링하고 예를 들어 가상 약물 스크리닝의 목적으로 이들의 특성을 예측하려고 할 때 화학 분야에서 응용에서 매우 중요하다.

구상의 한 점을 3차원 단위 벡터 \(\mathbf{u}:\|\mathbf{u}\|=1\)로 표현하면, 그룹의 작용은 \(\det(\mathbf{R})=1\)로 \(3\times 3\) 직교 행렬 \(\mathbf{R}\)로 표현될 수 있다. 구면 컨볼루션은 따라서 신호와 회전된 필터 사이의 내적으로서 기입될 수 있고,

\[(x\star\theta)(\mathbf{R})=\int_{\mathbb{S}^{2}}x(\mathbf{u})\theta(\mathbf{ R}^{-1}\mathbf{u})\mathrm{d}\mathbf{u}.\]

가장 먼저 주목해야 할 것은 지금보다 그룹이 도메인과 동일하지 않다는 것이다. 그룹 \(\operatorname{SO}(3)\)은 사실상 3차원 다양체인 거짓말 그룹인 반면, \(\mathbb{S}^{2}\)은 2차원 다양체인 것이다. 결과적으로 이 경우 이전 예제와 달리 컨볼루션은 on_\(\오메가\)가 아닌 함수 _on_\(\operatorname{SO}(3)\)_이다.

이것은 중요한 실제적인 결과를 가져온다: 우리의 기하학적 딥 러닝 설계도에서, 우리는 이전의 출력에 후속 연산자를 적용함으로써 다수의 등분산 맵들("딥 러닝 전문용어에서 층들")을 연결한다. 변환의 경우 출력이 모두 동일한 도메인 \(\Omega\)에 정의되어 있기 때문에 여러 개의 콘볼루션을 순차적으로 적용할 수 있다. 일반 설정에서 \(x\star\theta\)는 \(\Omega\)가 아닌 \(\mathfrak{G}\)에 대한 함수이기 때문에 후속적으로 정확히 동일한 연산을 사용할 수 없습니다. 즉, 다음 연산은 \(\mathfrak{G}\)_의 _신호, 즉 \(x\in\mathcal{X}(\mathfrak{G})\)를 처리해야 합니다. 그룹 컨볼루션을 정의하면 다음과 같다 : 도메인 \(\Omega=\mathfrak{G}\)이 그룹 액션을 통해 \(\mathfrak{G}\) 자체에 작용하는 영역 \(\mathfrak{g},\mathfrak{h})\mapsto\mathfrak{gh}\)이 \(\mathfrak{G}\)의 합성 연산으로 정의된다. 이를 통해 \(x\in\mathcal{X}(\mathfrak{G})\)에 작용하는 \(\rho(\mathfrak{g})\)를 \((\rho(\mathfrak{g})x)(\mathfrak{h})=x(\mathfrak{g}^{-1}\mathfrak{h})\)로 표현할 수 있다. \(\mathfrak{G}\) 자체에 정의된 함수에 작용하는 \(\mathfrak{G}\)의 표현을 \(\mathfrak{G}\)의 _정규 표현_이라고 한다.

컨볼루션은 도메인 \(\Omega\)을 통해 적분해야 하는 내적 곱을 포함하기 때문에, 우리는 작은 도메인 \(\Omega\)에서만 사용할 수 있다(이산형인 경우) 또는 저차원인 도메인 \(\Omega\)에서만 사용할 수 있다. 예를 들어, 우리는 평면상의 콘볼루션 \(\mathbb{R}^{2}\)(2차원) 또는 특수 직교군 \(\mathrm{SE}(3)\)(3차원) 또는 그래프의 유한한 노드 집합(\(n\)-차원)에 대해 콘볼루션을 사용할 수 있지만, 실제로는 \(n!\)을 갖는 치환군 \(\Sigma_{n}\)에 대해 콘볼루션을 수행할 수 없다. 요소로 이루어진 군에서 선택된 어느 하나인 것을 특징으로 하는 반도체 소자의 제조방법. 마찬가지로, 아핀 그룹과 같은 고차원 그룹(번역, 회전, 전단 및 스케일링을 포함, 총 6차원)을 통합하는 것은 실제로 실현 가능하지 않다. 그럼에도 불구하고, 섹션 5.3에서 보았듯이, 우리는 여전히 \(\mathfrak{G}\)이 작용하는 저차원 공간 \(\Omega\)에 정의된 신호로 작업함으로써 큰 그룹 \(\mathfrak{G}\)에 대한 등분산 공선을 구축할 수 있다. 실제로, 두 도메인 사이의 임의의 등분산 선형 맵 \(f:\mathcal{X}(\Omega)\to\mathcal{X}(\Omega^{\prime})\) \(\Omega,\Omega^{\prime}\)은 여기에서 논의된 그룹 컨볼루션과 유사한 일반화된 컨볼루션으로 기록될 수 있음을 보여줄 수 있다.

둘째, 컨볼루션의 shift-equivariance 특성으로부터 이전 섹션에서 도출한 푸리에 변환은 대칭군의 기약적 표현의 행렬 요소에 신호를 투영함으로써 보다 일반적인 경우로 확장될 수 있다는 점에 주목한다. 우리는 향후 작업에서 이에 대해 논의할 것입니다. 여기에서 연구된 \(\mathrm{SO}(3)\)의 경우, 이것은 양자역학 및 화학에서 광범위한 응용을 찾는 _구형 조화_ 및 _위그너 D-함수_를 발생시킨다.

마지막으로, \(\Omega\)가 격자이든 평면이든 구이든, 우리는 모든 점을 다른 점으로 변환할 수 있으며, 직관적으로 도메인의 모든 점이 "똑같아 보인다"는 것을 의미한다. 이러한 속성을 가진 도메인 \(\Omega\)을 _균질 공간_이라고 하며, 여기서 임의의 \(u,v\in\Omega\)에 대해 \(\mathfrak{g}\in\mathfrak{G}\)가 존재하여 \(\mathfrak{g}.u=v\)가 존재한다. 다음 절에서 우리는 이 가정을 완화하려고 노력할 것이다.

은 여기서 암묵적으로 추정된다.

### 지오데릭 및 매니폴드

우리의 마지막 예에서, 구 \(\mathbb{S}^{2}\)는 균질한 구조로 인해 전역 대칭군을 가진 특별한 것이기는 하지만 _다양체_였다. 불행히도, 일반적으로 전역 대칭을 갖지 않는 대부분의 다양체는 그렇지 않다. 이 경우, \(\mathfrak{G}\) 상의 신호 공간에 대한 \(\Omega\)의 동작을 직접 정의하고, 이를 이용하여 컨볼루션(convolution)을 고전적 구조의 직접적인 일반화로 정의할 수 없다. 그럼에도 불구하고 다양체는 메트릭 구조를 보존하는 변환과 로컬 참조 프레임 변경의 두 가지 유형의 불변성을 가지고 있다.

많은 기계 학습 독자들의 경우 다양체는 다소 이국적인 대상으로 나타날 수 있지만 실제로 다양한 과학 영역에서 매우 일반적이다. 물리학에서 다양체는 우리 우주의 모델로서 중심적인 역할을 한다. 아인슈타인의 일반상대성이론에 따르면 중력은 유사 리만니안 다양체로 모델링된 시공간의 곡률에서 발생한다. 컴퓨터 그래픽 및 비전과 같은 보다 '전문적인' 분야에서 매니폴드는 3D 도형의 일반적인 수학적 모델이다. 이러한 모델의 광범위한 응용 범위는 가상 및 증강 현실과 '모션 캡처'를 통해 얻은 특수 효과에서 3D 퍼즐 조각처럼 서로 붙어 있는 단백질 상호 작용을 다루는 구조 생물학에 이르기까지 다양하다. 이러한 응용의 공통 분모는 일부 3D 객체의 경계면을 표현하기 위해 매니폴드를 사용하는 것이다.

이러한 모델이 편리한 이유는 여러 가지가 있습니다. 첫째, 그들은 3D 객체에 대한 컴팩트한 설명을 제공하여 그리드 기반 표현에서 요구되는 것처럼 '빈 공간'에 메모리를 할당할 필요가 없다. 둘째, 객체의 내부 구조를 무시할 수 있도록 한다. 이것은 예를 들어 단백질 분자의 내부 접힘이 분자 표면에서 발생하는 상호 작용과 종종 관련이 없는 구조 생물학에서 편리한 특성이다. 셋째, 가장 중요한 것은 비강성 변형을 겪는 _변형 가능한 객체_ 를 처리해야 하는 경우가 많다는 것이다. 우리 자신의 몸은 그러한 예 중 하나이며, 앞서 언급한 모션 캡처 및 가상 아바타와 같은 컴퓨터 그래픽 및 비전의 많은 응용 프로그램은 _변형 불변_ 을 필요로 한다. 이러한 변형은 매니폴드가 주변 공간에 매립되는 방식에 관계없이 (리만니안) 매니폴드의 고유한 구조, 즉 매니폴드를 따라 측정된 점 사이의 거리를 보존하기 위해 매우 잘 모델링될 수 있다.

우리는 다양체가 기하 딥러닝 설계도에서 _변화하는 도메인_ 설정에 해당하며 이러한 의미에서 그래프와 유사하다는 점을 강조해야 한다. 우리는 3.3절에서 우리가 '기하학적 안정성'이라고 부르는 영역 변형에 대한 불변 개념의 중요성을 강조할 것이다. 미분기하학은 아마도 기계 학습 청중에게 덜 친숙하기 때문에, 우리는 논의에 필요한 기본 개념을 소개하고 독자들에게 그들의 상세한 설명을 펜로즈(2005)에게 참조할 것이다.

리만 다양체의 형식적 정의는 다소 관련되어 있기 때문에 우리는 약간의 정밀도를 희생시키면서 직관적인 그림을 제공하는 것을 선호한다. 이러한 맥락에서, 우리는 ( 미분가능하거나 매끄러운) 매니폴드를 지역적으로 유클리드인 매끄러운 다차원 곡면으로 생각할 수 있는데, 어떤 점 주위의 작은 이웃이 \(\mathbb{R}^{s}\)의 이웃으로 변형될 수 있다는 의미에서, 이 경우 매니폴드는 _\(s\)-차원_이라고 한다. 이를 통해 점 \(u\) 주변의 다양체를 _접선 공간_\(T_{u}\Omega\)을 통해 국소적으로 근사화할 수 있다. 후자는 원형 2차원 다양체인 구를 생각하고 한 점에 평면을 붙임으로써 시각화할 수 있다: 충분한 줌으로 구면은 평면처럼 보일 것이다(그림 11). 모든 접선 공간의 집합은 \(T\Omega\)로 표시된 _접선 번들_이라고 하며, 4.5절에서 번들의 개념에 대해 더 자세히 설명한다.

우리가 \(X\in T_{u}\Omega\)로 나타내는 _접선 벡터_는 점 \(u\)로부터의 국소 변위로 생각할 수 있다. 접선벡터의 _길이_와 이들 사이의 _각도_를 측정하기 위해서는 \(u\)에 따라 양정의 쌍선형 함수 \(g_{u}:T_{u}\Omega\times T_{u}\Omega\to\mathbb{R}\)로 표현되는 추가적인 구조를 접선공간에 갖추어야 한다. 이러한 함수는 1856년에 이 개념을 도입한 Bernhardt Riemann의 이름을 따서 _Riemannian metric_이라고 부르며, 접선 공간상의 내적인 \(\langle X,Y\rangle_{u}=g_{u}(X,Y)\), 즉 임의의 두 접선 벡터 사이의 각도를 나타내는 \(X,Y\in T_{u}\Omega\)로 생각할 수 있다. 이 메트릭은 또한 벡터의 길이를 국부적으로 측정할 수 있는 norm \(\|X\|_{u}=g_{u}^{1/2}(X,X)\)을 유도한다.

우리는 접 벡터가 자신의 오른쪽에 존재하고 좌표가 없는 추상적인 기하학적 실체임을 강조해야 한다. 우리가 접선 벡터 \(X\)를 숫자의 배열로 수치적으로 표현한다면, 우리는 단지 좌표 \(\mathbf{x}=(x_{1},\ldots,x_{s})\)_의 목록으로 나타낼 수 있을 뿐이다. 마찬가지로 메트릭은 원소 \(g_{ij}=g_{u}(X_{i},X_{j})\)를 기준으로 \(s\times s\) 행렬 \(\mathbf{G}\)로 나타낼 수 있다. 우리는 섹션 4.5의 이 지점으로 돌아갈 것이다.

메트릭이 장착된 매니폴드를 _리만니안 매니폴드_라고 하며, 메트릭으로 완전히 표현할 수 있는 속성은 _내재적_이라고 한다. 이것은 우리의 템플릿에 따르면, 우리는 다양체의 국소 구조에 영향을 미치지 않고 다양체를 변형시키는 메트릭 보존 변환에 불변인 \(\Omega\)에 정의된 신호에 작용하는 함수를 구성하려고 할 것이다. 그러한 함수들이 고유량들의 관점에서 표현될 수 있다면, 그것들은 자동적으로 등척성-불변인 것으로 보장되고 따라서 등척성 변형들에 의해 영향을 받지 않는다. 이러한 결과는 대략적인 등측량을 다루는 것으로 더 확장될 수 있으며, 따라서 이것은 우리의 설계도에서 논의된 기하학적 안정성(도메인 변형)의 사례이다.

우리가 언급했듯이, 리만 다양체의 정의는 어떤 공간에서도 기하학적 실현을 필요로 하지 않지만, 임의의 매끄러운 리만 다양체는 리만 계량법을 유도하기 위해 유클리드 공간의 구조를 사용함으로써 충분히 고차원의 유클리드 공간의 부분집합으로 실현될 수 있다는 것이 밝혀졌다. 그러나 이러한 임베딩은 반드시 고유한 것은 아니다 - 우리가 볼 수 있듯이 리만 메트릭의 두 가지 다른 등척성 구현이 가능하다.

스칼라 및 벡터 필드는 \(\Omega\)에 정의된 신호에 관심이 있기 때문에 다양체 상에서 스칼라 및 벡터 값 함수에 대한 적절한 개념을 제공할 필요가 있다. A(smooth) _scalar field_는 \(x:\Omega\to\mathbb{R}\) 형태의 함수이다. Scalar fields form a vector space \(\mathcal{X}(\Omega,\mathbb{R})\) can be equipped with inner product

\[\angle x,y\rangle=\int_{\Omega}x(u)y(u)\mathrm{d}u, \tag{15}\]

여기서 \(\mathrm{d}u\)는 Riemannian metric에 의해 유도된 부피 요소이다. (매끄러운) _접선 벡터 필드_는 \(X:\Omega\to T\Omega\) 형태가 각 접선 공간에 있는 접선 벡터를 각 점에 할당하는 함수입니다. \(u\mapsto X(u)\in T_{u}\Omega\). 벡터 필드는 또한 Riemannian 메트릭을 통해 정의된 내적과 벡터 공간 \(\mathcal{X}(\Omega,T\Omega)\)을 형성하고,

\[\angle X,Y\rangle=\int_{\Omega}g_{u}(X(u),Y(u))\mathrm{d}u. \tag{16}\]

고유 구배 벡터 필드(그리고 실제로 _정의_)를 생각하는 또 다른 방법은 도함수의 일반화된 개념으로 간주된다. 고전적 미적분학에서 한 함수는 _미분_\(\mathrm{d}x(u)=x(u+\mathrm{d}u)-x(u)\를 통해 국소적으로 선형화할 수 있으며, 이는 무한소 변위의 결과로 점 \(u\)에서 함수 값 \(x\)의 변화를 제공한다 \(\mathrm{d}u\). 그러나 이 정의의 경우 전역 벡터 공간 구조의 부재로 인해 "\(u+\mathrm{d}u\" 형태의 표현은 다양체 상에서 무의미하기 때문에 이 정의의 순진한 사용은 불가능하다.

해는 접벡터를 국소 무한소 변위의 모델로 사용하는 것이다. 부드러운 스칼라 필드 \(x\in\mathcal{X}(\Omega,\mathbb{R})\)가 주어지면, 우리는 임의의 상수 \(c\)에 대하여 \(Y(c)=0\)의 성질을 만족하는 직선 맵 \(Y:\mathcal{X}(\Omega,\mathbb{R})\)으로 생각할 수 있다. (상수함수가 소실미분, \(Y(x+z)=Y(x)+Y(z)\) (선형성), \(Y(xz)=Y(x)z+xY(z)\) (_곱_ 또는 _Leibniz 규칙_)\) 임의의 평활 스칼라 필드에 대하여 \(x,z\in\mathcal{X}(\Omega,\mathbb{R})\). 벡터 필드를 공리적으로 정의하기 위해 이러한 속성을 사용할 수 있음을 보여줄 수 있다. 미분 \(\mathrm{d}x(Y)=Y(x)\)는 연산자 \((u,Y)\mapsto Y(x)\)로 볼 수 있으며 다음과 같이 해석될 수 있다. \(u\)점에서 변위 \(Y\in T_{u}\Omega\)의 결과에 따른 \(x\)의 변화는 \(\mathrm{d}_{u}x(Y)\)로 주어진다. 따라서 이는 _방향 도함수_라는 고전적 개념의 확장이다. 또는, 각 점 \(u\)에서 미분은 접선 벡터 \(X\in T_{u}\Omega\)에 작용하는 _선형 함수_\(\mathrm{d}x_{u}:T_{u}\Omega\to\mathbb{R}\)로 간주될 수 있다. 벡터 공간상의 선형 함수들은 _dual vectors_ 또는 _covectors_라고 불리며, 만약 여기에 더하여 내적(리만너 메트릭)이 주어진다면, 이중 벡터는 항상 다음과 같이 표현될 수 있다.

\[\mathrm{d}x_{u}(X)=g_{u}(\nabla x(u),X).\]

점 \(u\)에서의 미분 표현은 \(x\)의 (intrinsic) _gradient_라고 불리는 접선 벡터 \(\nabla x(u)\in T_{u}\Omega\)이며, 고전 미적분학의 그래디언트와 유사하게 \(x\)의 가장 가파른 증가의 방향으로 생각할 수 있다. 연산자_\(\nabla:\mathcal{X}(\Omega,\mathbb{R})\to\mathcal{X}(\Omega,T\Omega)\)로 간주되는 기울기는 각 점 \(x(u)\mapsto\nabla x(u)\in T_{u}\Omega\)에서 할당되며, 따라서 스칼라 필드 \(x\)의 기울기는 벡터 필드 \(\nabla x\)이다.

Geodesics 이제 끝점이 \(u=\gamma(0)\) 및 \(v=\gamma(T)\)인 매니폴드의 부드러운 곡선 \(\gamma:[0,T]\to\Omega\)을 고려합니다. 점 \(t\)에서 곡선의 도함수는 속도 벡터라고 불리는 접선 벡터 \(\gamma^{\prime}(t)\in T_{\gamma(t)}\Omega\이다. 우리는 \(u\)와 \(v\)를 연결하는 모든 곡선 중에서 _최소 길이_ 즉, 길이 함수를 최소화하는 \(\gamma\)를 찾고 있다.

\[\ell(\gamma)=\int_{0}^{T}\|\gamma^{\prime}(t)\|_{\gamma(t)}\mathrm{d}t=\int_{ 0}^{T}g_{\gamma(t)}^{1/2}(\gamma^{\prime}(t),\gamma^{\prime}(t))\mathrm{d}t.\]

이러한 곡선은 _geodesics_ (그리스 \(\gamma\)e\(\delta\)x\(\alpha\), 문자 그대로 '지구의 분할'로 불리며 미분 기하학에서 중요한 역할을 한다. 결정적으로, 우리의 논의에서, 우리가 측지학을 정의한 방식은 (길이 함수를 통해) 리만 미터법에만 의존하기 때문에 내재적이다.

미분 기하학에 정통한 독자들은 측지학이 더 일반적인 개념이며 실제로 측지학의 정의가 반드시 리만 미터법이 아니라 _연결_(벡터 및 텐서 필드에 대한 도함수 개념을 일반화하기 때문에 _공변량 도함수라고도 함)을 필요로 한다는 것을 상기할 수 있으며, 이는 미분의 구성과 유사하게 공리적으로 정의된다. 리만 메트릭이 주어지면 리만 기하학에서 종종 암묵적으로 가정되는 _레비-시비타 연결_이라는 독특한 특수 연결이 존재한다. 이 연결에서 발생하는 측지선은 위에서 정의한 길이 최소화 곡선이다.

다음으로 측지학을 사용하여 다양체 상의 접선 벡터를 전송하는 방법을 정의하고(평행 전송), 다양체에서 접선 공간으로 로컬 고유 지도를 작성하고(지수 맵), 거리를 정의하는 방법(측지 메트릭)을 보여 줄 것이다. 이를 통해 접선 공간에서 국소적으로 필터를 적용하여 컨볼루션과 같은 연산을 구성할 수 있다.

병렬 수송 다양체를 다룰 때 우리가 이미 접한 문제 중 하나는 두 점을 직접 가감할 수 없다는 것이다 \(u,v\in\Omega\). 다른 점에서 접벡터를 비교하려고 할 때 같은 문제가 발생한다. 같은 차원을 갖지만 서로 다른 공간, 예를 들어 \(X\in T_{u}\Omega\) 및 \(Y\in T_{v}\Omega\)에 속하므로 직접 비교할 수 없다. 측지학은 다음과 같은 방법으로 벡터들을 한 지점에서 다른 지점으로 이동시키는 메커니즘을 제공한다 : \(\gamma\)를 측지 연결점이라고 하자 \(u=\gamma(0)\)와 \(v=\gamma(T)\) 그리고 \(X\in T_{u}\Omega\). 우리는 측지선, \(X(t)\in T_{\gamma(t)}\Omega\)를 따라 접선 벡터의 새로운 집합을 정의할 수 있는데, 이 집합은 \(X(t)\)의 길이와 곡선의 속도 벡터 사이의 각도(Riemannian metric을 통해 표현됨)가 일정하도록,

\[g_{\gamma(t)}(X(t),\gamma^{\prime}(t))=g_{u}(X,\gamma^{\prime}(0))=\mathrm{ const},\qquad\|X(t)\|_{\gamma(t)}=\|X\|_{u}=\mathrm{const}.\]

그 결과, 종점 \(v\)에서 고유한 벡터 \(X(T)\in T_{v}\Omega\)를 얻는다.

이러한 표기법을 이용하여 지도 \(\Gamma_{u\to v}(X):T_{u}\Omega\to T_{u}\Omega\)와 \(\Gamma_{u\to v}(X)=X(T)\로 정의된 \(\T_{v}\Omega\)를 평행이동 또는 연결이라 한다. 후자의 용어는 접선공간 \(T_{u}\Omega\)과 \(T_{v}\Omega\) 사이에 '연결'하는 메커니즘임을 의미한다. 각 및 길이 보존 조건으로 인해 평행 수송은 벡터의 회전에만 해당하므로 특수 직교 그룹 \(\mathrm{SO}(s)\)(접선 다발의 _구조 그룹_이라고 함)의 요소와 연관될 수 있으며, \(\mathfrak{g}_{u\to v}\)로 나타내고 4.5절에서 더 자세히 논의한다.

앞서 언급했듯이 연결은 리만 미터법과 공리적으로 독립적으로 정의될 수 있으며, 따라서 부드러운 곡선을 따라 병렬 수송에 대한 추상적인 개념을 제공한다. 그러나 이러한 운송의 결과는 취한 경로에 따라 다르다.

지수적 지도(exponential mapLocally around a point \(u\))는 주어진 방향(X\in T_{u}\Omega\), 즉 \(\gamma(0)=u\)와 \(\gamma^{\prime}(0)=X\)에서 항상 고유한 측지선을 정의할 수 있다. 모든 \(t\geq 0\)에 대해 \(\gamma_{X}(t)\)를 정의하면(즉, 원하는 만큼 점 \(u\)에서 측지선을 촬영할 수 있음), 매니폴드는 _geodesically complete_라고 하며 지수 맵은 전체 접선 공간에서 정의된다. 컴팩트 매니폴드는 측지학적으로 완전하기 때문에 우리는 이 편리한 성질을 암묵적으로 가정할 수 있다.

이러한 측지선의 정의는 (T_{u}\Omega\)에서 \(\exponential map_\(\exp:B_{r}(0)\subset T_{u}\Omega\to\Omega\)로 이어지는 접선 공간 \(T_{u}\Omega\)에서 \(X\), 즉 \(\exp_{u}(X)=\gamma_{X}(1)\로 정의되는 자연스러운 매핑을 제공한다. 지수지도 \(\exp_{u}\)는 \(T_{u}\Omega\)에서 원점의 근점인 \(B_{r}(0)\)(공 또는 반지름 \(r\))을 \(u\)의 근점으로 변형시키는 국부적 차이형태이다. 반대로, 지수 지도는 접선 공간으로의 다양체의 고유한 국부 변형('평탄화')으로 간주할 수도 있다.

Geodesic distancesHopf-Rinow 정리로 알려진 결과는 Geodesically complete manifolds도 _완전한 메트릭 공간_임을 보장하며, 이 공간에서는 임의의 점 쌍 \(u,v\) 사이의 거리( _geodesic distance_ 또는 _metric_이라고 함)를 그들 사이의 최단 경로의 길이로 실현할 수 있습니다.

\[d_{g}(u,v)=\min_{\gamma}\ell(\gamma)\qquad\text{s.t.}\qquad\gamma(0)=u,\ \gamma(T)=v,\]

가 존재하는 경우(즉, 최소값에 도달함).

이제 등량계는 우리의 다양체 \(\Omega\)가 다른 다양체 \(\tilde{\Omega}\)로 변형되는 것을 리만 메트릭 \(h\)으로 간주하며, 우리는 다양체 사이의 이형성 \(\eta:(\Omega,g)\to(\tilde{\Omega},h)\)으로 가정한다. 그 미분 \(\mathrm{d}\eta:T\Omega\to T\tilde{\Omega}\)은 각 접선다발들 사이의 지도를 정의하여, 점 \(u\)에서, 점 \(\mathrm{d}\eta_{u}:T_{u}\Omega\to T_{\eta(u)}\tilde{\Omega}\)을 이전과 같이 해석한다: 점 \(u\)에서 접선벡터 \(X\in T_{u}\Omega\)로 작은 변위를 만들면, 지도 \(\eta\)는 점 \(\eta(u)\)에서 접선벡터 \(\mathrm{d}\eta_{u}(X)\in T_{\eta(u)}\tilde{\Omega}\)로 변하게 된다.

푸시포워드는 두 매니폴드에 접선 벡터를 연결하는 메커니즘을 제공하기 때문에 메트릭 \(h\)을 \(\tilde{\Omega}\)에서 \(\Omega\)로 풀백할 수 있습니다.

\[(\eta^{*}h)_{u}(X,Y)=h_{\eta(u)}(\mathrm{d}\eta_{u}(X),\mathrm{d}\eta_{u}(Y))\]

풀백 메트릭이 모든 지점에서 \(\Omega\), 즉 \(g=\eta^{*}h\)의 메트릭과 일치하면 맵 \(\eta\)을 (리만니안) _아이소메트리_라고 합니다. 2차원 매니폴드(표면)의 경우, 등축계는 매니폴드를 '찢어짐'이나 '찢어짐' 없이 변형시키는 비탄성 변형으로 직관적으로 이해될 수 있다.

그들의 정의 덕분에 등측량은 지오데식 거리와 같은 고유 구조를 보존하며, 이는 전적으로 리만 미터법으로 표현된다. 따라서 우리는 거리 보존 지도('metric isometries')가 Metric space_\(\eta:(\Omega,d_{g})\to(\tilde{\Omega},d_{h})\)라는 의미에서 Metric geometry의 위치로부터 isometries를 이해할 수도 있다.

\[d_{g}(u,v)=d_{h}(\eta(u),\eta(v))\]

모든 \(u,v\in\Omega\) 또는 더 컴팩트하게 \(d_{g}=d_{h}\circ(\eta\times\eta)\)에 대해. 즉, 리만형 등량계도 미터법 등량계이다. 연결된 다양체에서, 그 반대도 참이다: 모든 메트릭 등식은 리만 등식이기도 하다.

이 결과는 마이어스-스티엔로드 정리로 알려져 있다. 우리는 암묵적으로 우리의 다양체가 연결되어 있다고 가정한다.

우리의 기하 딥러닝 설계도에서 \(\eta\)는 도메인 변형의 모델이다. \(\eta\)가 등량론일 때, 임의의 고유량은 그러한 변형에 영향을 받지 않는다. _metric dilation_ 개념을 통해 정확한 (metric) 등측량을 일반화할 수 있습니다.

\[\mathrm{dil}(\eta)=\sup_{u\neq v\in\Omega}\frac{d_{h}(\eta(u),\eta(v))}{d_{g}( u,v)}\]

또는 _metric distortion_

\[\mathrm{dis}(\eta)=\sup_{u,v\in\Omega}|d_{h}(\eta(u),\eta(v))-d_{g}(u,v)|,\]

이는 \(\eta\)에서 측지선 거리의 상대적 변화와 절대적 변화를 각각 포착한다. 도메인 변형 하에서 함수 \(f\in\mathcal{F}(\mathcal{X}(\Omega))\)의 안정성에 대한 조건 (5)는 이 경우에 다시 쓰여질 수 있다.

\[\|f(x,\Omega)-f(x\circ\eta^{-1},\tilde{\Omega})\|\leq C\|x\|\mathrm{dis}(\eta).\]

고유 대칭 위의 특정 경우는 도메인 자체의 차이 형태(섹션 3.2에서 _자동 형태_라고 함)이며, 이는 \(\tau\in\mathrm{Diff}(\Omega)\)로 나타낼 것이다. 풀백 메트릭이 \(\tau^{*}g=g\)를 만족하면 리만형(self-isometry)이라 하고, \(d_{g}=d_{g}\circ(\tau\times\tau)\)를 만족하면 자기-isometry라 한다. 물론 isometries는 \(\mathrm{Iso}(\Omega)\)로 표기되고 _isometry group_이라고 불리는 합성 연산자와 그룹을 형성하는데, 아이덴티티 요소는 map \(\tau(u)=u\)이고 역은 항상 존재한다 (diffeomorphism으로서 \(\tau\의 정의에 의해). 따라서 자기 동척은 다양체의 _내재적 대칭_이다.

매니폴드에 대한 푸리에 분석 이제 우리는 구성에 의해 등척성 변형에 불변하는 매니폴드에 대한 고유 컨볼루션 유사 연산을 구성하는 방법을 보여줄 것이다. 이를 위해 두 가지 옵션이 있다: 하나는 푸리에 변환의 유추를 사용하는 것이고, 컨볼루션은 푸리에 도메인에서 곱으로 정의하는 것이다. 다른 하나는 필터를 신호와 국부적으로 상관시킴으로써, 콘볼루션을 공간적으로 정의하는 것이다. 스펙트럼 접근법에 대해 먼저 논의해 봅시다.

우리는 유클리드 영역에서 퓨리에 변환이 순환 행렬의 고유 벡터로 얻어지며, 이 고유 벡터는 교환도로 인해 공동으로 대각화할 수 있음을 상기한다. 따라서, 임의의 순환 매트릭스, 특히 미분 연산자는 일반 도메인 상의 푸리에 변환의 유추를 정의하기 위해 사용될 수 있다. 리만 기하학에서는 라플라시안 연산자의 직교 고유 기저를 사용하는 것이 일반적이며, 여기서 정의하기로 한다.

이를 위해 고유 기울기 연산자 \(\nabla:\mathcal{X}(\Omega,\mathbb{R})\to\mathcal{X}(\Omega,T\Omega)\에 대한 정의를 상기하여 다양체 상의 스칼라 필드의 가장 가파른 증가의 국지적 방향을 나타내는 접선 벡터 필드를 생성한다. 유사한 방식으로, 우리는 _발산 연산자_\(\nabla^{*}:\mathcal{X}(\Omega,T\Omega)\to\mathcal{X}(\Omega,\mathbb{R})\)를 정의할 수 있다. 우리가 접 벡터장을 다양체 위의 흐름으로 생각한다면, 발산은 한 점에서 장의 순 흐름을 측정하여 장의 '소스'와 '싱크'를 구별할 수 있게 한다. 우리는 표기법 \(\nabla^{*}\)(일반적인 \(\mathrm{div}\)와 반대로)을 사용하여 두 연산자가 인접하다는 것을 강조하고,

\[\angle X,\nabla x\rangle=\langle\nabla^{*}X,x\rangle,\]

여기서 우리는 스칼라장과 벡터장 사이의 내적(15)과 내적(16)을 사용한다.

미분 기하학에서 라플라스 연산자(Laplace-Beltrami 연산자라고도 함)는 \(\Delta=\nabla^{*}\nabla\)로 정의되는 \(\mathcal{X}(\Omega)\) 상의 연산자로, 점 주위의 무한소 구에서의 함수의 평균과 점 자체의 함수 값의 차이로 해석할 수 있다. 이것은 열 확산, 양자 진동 및 파동 전파와 같은 다양한 현상을 설명하는 데 사용되는 수리 물리학에서 가장 중요한 연산자 중 하나이다. 중요한 것은 우리의 맥락에서 라플라시안(Laplacian)은 내재적이며, 따라서 \(\Omega\)의 등량 하에서 불변이다.

라플라시안(Laplacian)이 자기조인트('대칭적')인 것을 쉽게 알 수 있고,

\[\langle\nabla x,\nabla x\rangle=\langle x,\Delta x\rangle=\langle\Delta x,x\rangle.\] 상기 표현에서 좌측의 2차 형태는 실제로 이미 익숙한 디리클레 에너지이고,

\[c^{2}(x)=\|\nabla x\|^{2}=\langle\nabla x,\nabla x\rangle=\int_{\Omega}\|\nabla x(u)\|_{u}^{2}\mathrm{d}u=\int_{\Omega}g_{u}(\nabla x(u),\nabla x(u))\mathrm{d}u\

\(x\)의 평활도를 측정하는 단계를 포함하는 것을 특징으로 하는 방법.

라플라시안 연산자는 자기분해를 인정한다.

\[\Delta\varphi_{k}=\lambda_{k}\varphi_{k},\qquad k=0,1,\ldots\]

만약 매니폴드가 콤팩트한 경우(우리가 암묵적으로 가정한 경우), 직교 고유함수 \(\langle\varphi_{k},\varphi_{l}\rangle=\delta_{kl}\)는 \(\Delta\)의 자기조인트성에 기인한다. 라플라시안 고유바시스는 또한 디리클레 에너지의 직교 최소치들의 세트로서 구성될 수 있고,

\[\varphi_{k+1}=\arg\min_{\varphi}\|\nabla\varphi\|^{2}\qquad\text{s.t.}\qquad \|\varphi\|=1\;\;\text{and}\;\;\langle\varphi,\varphi_{j}\rangle=0\]

\(j=0,\ldots,k\)의 경우 \(\Omega\)에서 가장 부드러운 직교 기준으로 해석할 수 있습니다. 고유함수 \(\varphi_{0},\varphi_{1},\ldots\)와 그에 상응하는 고유값 \(0=\lambda_{0}\leq\lambda_{1}\leq\ldots\)은 고전 푸리에 변환에서 원자와 주파수의 유사성으로 해석될 수 있다.

실제로 \(e^{\mathrm{i}\xi u}\)는 유클리드 라플라시안 \(\frac{\mathrm{d}^{2}}{\mathrm{d}u^{2}}\)의 고유함수이다.

이 직교 기저를 사용하면 \(\Omega\)에서 제곱 적분 함수를 _푸리에 급수_로 확장할 수 있습니다.

\[x(u)=\sum_{k\geq 0}\langle x,\varphi_{k}\rangle\varphi_{k}(u)\]

여기서 \(\hat{x}_{k}=\langle x,\varphi_{k}\rangle\)는 _푸리에 계수_ 또는 \(x\)의 (일반화된) 푸리에 변환으로 지칭된다. Truncating the Fourier series results in approximation error that can be bounded (Aflalo and Kimmel, 2013) by

\[\left\|x-\sum_{k=0}^{N}\langle x,\varphi_{k}\rangle\varphi_{k}\right\|^{2} \leq\frac{\|\nabla x\|^{2}}{\lambda_{N+1}}}.\]

Aflalo 등(2015)은 다른 어떤 근거도 더 나은 오류를 달성하지 못하여 라플라시안 고유바시스가 다양체의 부드러운 신호를 나타내는 데 최적이 됨을 추가로 보여주었다.

매니폴드 상의 스펙트럼 컨볼루션_스펙트럼 컨볼루션_은 신호 \(x\) 및 필터 \(\theta\)의 푸리에 변환들의 곱으로 정의될 수 있고,

\[(x\star\theta)(u)=\sum_{k\geq 0}(\hat{x}_{k}\cdot\hat{\theta}_{k})\varphi_{k}(u). \tag{17}\]

여기서 우리는 비유클리드 컨벌루션을 _정의하는 방법으로 고전적 푸리에 변환(컨벌루션 정리)의 _속성_ 을 사용한다는 점에 유의한다. 그 구성 덕분에 스펙트럼 컨벌루션은 고유하고 따라서 등량론에 불변한다. 또한 라플라시안 연산자는 등방성이기 때문에 방향 감각이 없으며, 이러한 의미에서 이웃 집합의 순열 불변성으로 인해 섹션 4.1의 그래프에서 얻은 상황과 유사하다.

실제로, (17)의 직접 계산은 라플라시안 대각화의 필요성으로 인해 엄청나게 비싼 것으로 보인다. 더 나쁜 것은 기하학적으로 불안정하다는 것이다: 라플라시안 고유함수는 영역 \(\Omega\)의 작은 근등각 섭동의 결과로 극적으로 변할 수 있다(그림 12 참조). 필터가 \(\hat{p}(\Delta)\) 형태의 _스펙트럴 전달 함수_로 실현되어 보다 안정적인 솔루션이 제공되며,

\[(\hat{p}(\Delta)x)(u) = \sum_{k\geq 0}\hat{p}(\lambda_{k})\langle x,\varphi_{k}\rangle \varphi_{k}(u) \tag{18}\] \[= \int_{\Omega}x(v)\,\sum_{k\geq 0}\hat{p}(\lambda_{k})\varphi_{k}(u)\,\mathrm{d}v \tag{19}\]

이는 스펙트럼 필터(18)로 해석할 수 있으며, 여기에서 \(\hat{\theta}_{k}=\hat{p}(\lambda_{k})\)를 식별하거나 위치 종속 커널 \(\theta(u,v)=\sum_{k\geq 0}\hat{p}(\lambda_{k})\varphi_{k}(v)\varphi_{k}(u)\)를 갖는 공간 필터(19)로 해석할 수 있다. 이것의 장점은

도 12: 도메인 섭동 하에서 스펙트럼 필터의 불안정성. 왼쪽: 그물망 위의 신호 \(\mathbf{x}\) \(\Omega\). 중간: 라플라시안 \(\Delta\)의 고유 기저에서 스펙트럼 필터링 결과 \(\Omega\). 오른쪽: 라플라시안 \(\tilde{\Delta}\)의 고유벡터 \(\tilde{\Omega}\)에 동일한 스펙트럼 필터를 적용하면 매우 다른 결과를 얻을 수 있다.

제형은 \(\hat{p}(\lambda)\)는 소수의 계수에 의해 매개변수화될 수 있으며 다항식과 같은 매개변수 함수를 선택하면 \(\hat{p}(\lambda)=\sum_{l=0}^{r}\alpha_{l}\lambda^{l}\)와 같이 필터를 효율적으로 계산할 수 있다.

\[(\hat{p}(\Delta)x)(u)=\sum_{k\geq 0}\sum_{l=0}^{r}\alpha_{l}\lambda_{k}^{l}\left<x, \varphi_{k}\right>\varphi_{k}(u)=\sum_{l=0}^{r}\alpha_{l}(\Delta^{l}x)(u),\]

스펙트럼 분해를 완전히 피합니다. 우리는 섹션 4.6에서 이 구성에 대해 더 자세히 논의할 것이다.

매니폴드에 대한 공간 컨벌루션 두 번째 대안은 매니폴드에 대한 컨벌루션을 정의하기 위해 식 (14)에서처럼 서로 다른 지점에서 필터를 매칭하는 것이다.

\[(x\star\theta)(u)=\int_{T_{u}\Omega}x(\exp_{u}Y)\theta_{u}(Y)\mathrm{d}Y, \tag{20}\]

여기서 우리는 스칼라 필드 \(x\)의 값을 접선 공간에서 접근하기 위해 지수 맵을 사용해야 하며, 필터 \(\theta_{u}\)는 각 점의 접선 공간에서 정의되므로 위치에 따라 달라진다. 필터가 본질적으로 정의된다면, 그러한 컨볼루션은 우리가 많은 컴퓨터 비전 및 그래픽 애플리케이션에서 중요하다고 언급한 특성인 등각 불변일 것이다.

그러나, 우리는 4.2-4.3절에서 우리의 이전 구조와 몇 가지 실질적인 차이점을 주목할 필요가 있다. 첫째, 다양체는 일반적으로 균질한 공간이 아니기 때문에, 우리는 한 지점에서 정의된 식 (20)의 \(\theta_{u}\)이 아닌 모든 \(u\)에서 동일한 \(\theta\)의 공유 필터를 가질 수 있는 글로벌 그룹 구조를 더 이상 갖지 않는다. 이 연산과 매니폴드에 대한 유사성은 병렬 수송을 필요로 하며, \(T_{u}\Omega\)에 함수로서 정의된 공유 \(\theta\)를 다른 \(T_{v}\Omega\)에 적용할 수 있다. 그러나, 우리가 보았듯이, 이것은 일반적으로 \(u\)와 \(v\ 사이의 경로에 따라 달라지기 때문에 _물질을 중심으로 필터를 이동하는 방식_이다. 셋째, 지수 맵을 국부적으로만 사용할 수 있기 때문에 필터는 주입 반경에 의해 경계가 지정된 _로컬_이어야 한다. 넷째, 가장 중요한 것은, \(\theta(X)\)는 추상적인 기하학적 객체이기 때문에 작업할 수 없다. \(X\)가 계산에 사용되기 위해서는 좌표의 \(s\)차원 배열로서 \(\mathbf{x}=\omega_{u}^{-1}(X)\)를 어떤 국소 기저에 대해 _상대적으로_ 표현해야 한다. \(\omega_{u}:\mathbb{R}^{s}\to T_{u}\Omega\) 이를 통해 컨볼루션(20)을 다음과 같이 다시 쓸 수 있다.

\[(x\star\theta)(u)=\int_{[0,1]^{s}}x(\exp_{u}(\omega_{u}\mathbf{y}))\theta( \mathbf{y})\mathrm{d}\mathbf{y}, \tag{21}\] 필터를 단위 큐브에 정의합니다. 지수 맵은 (측지선의 정의를 통해) 고유하기 때문에, 결과 컨볼루션은 등변-불변이다.

그러나, 이 암묵적으로 우리는 프레임 \(\omega_{u}\)을 다른 다양체, 즉 \(\omega^{\prime}_{u}=\mathrm{d}\eta_{u}\circ\omega_{u}\)로 운반할 수 있다고 가정했다. 일관된 방식으로 다양체 \(\Omega\)만 주어진 그러한 프레임(또는 물리학 용어로 _게이지_)을 얻는 것은 여전히 어려움으로 가득하다. 첫째, 매끄러운 전역 게이지가 존재하지 않을 수 있습니다. 이것은 _평행화할 수 없는 다양체의 상황이며, 이 경우 매끄럽지 않은 탄젠트 벡터 필드를 정의할 수 없습니다. 둘째, 우리는 다양체에 정준 게이지가 없기 때문에 이 선택은 임의적이다; 우리의 컨볼루션은 \(\omega\)에 의존하기 때문에, 만약 다른 것을 선택한다면, 우리는 다른 결과를 얻을 것이다.

우리는 실천이 이론에서 발산하는 경우라는 점에 주목해야 한다. 실제로, 다양체 위의 일부 고유 스칼라 장의 고유 기울기를 취함으로써 제한된 수의 특이점으로 대부분 매끄러운 프레임을 구축하는 것이 가능하다. 더욱이, 이러한 구성은 안정적이며, 즉, 이러한 방식으로 구성된 프레임은 등척성 매니폴드 상에서 동일하고 대략 등척성 매니폴드 상에서 유사할 것이다. 이러한 접근법은 실제로 다양체에 대한 딥 러닝에 대한 초기 작업에서 사용되었다(Masci et al., 2015; Monti et al., 2017).

그럼에도 불구하고, 이 솔루션은 거의 특이점들, 필터 배향(게이지에 대해 고정된 방식으로 정의됨)이 거칠게 변할 것이기 때문에 완전히 만족스럽지는 않으며, 이는 입력 신호 및 필터가 매끄럽더라도 매끄럽지 않은 특징 맵으로 이어진다. 또한, 어떤 점 \(u\)에서 주어진 방향이 완전히 다른 점 \(v\)에서 다른 방향과 동등한 것으로 간주되어야 하는 명확한 이유는 없다. 따라서 _실용적인_ 대안에도 불구하고 다음으로 게이지 선택에 완전히 독립적인 보다 _이론적으로 잘 기반을 둔 접근법을 찾을 것이다.

### Guges 및 Bundles

우리가 접선 공간에 대한 프레임으로 정의한 게이지의 개념은 물리학에서 훨씬 더 일반적입니다. 이것은 접선 번들뿐만 아니라 모든 _벡터 번들_ 에 대한 프레임을 의미할 수 있습니다. 비공식적으로, 벡터 번들은 다른 공간에 의해 매개되는 벡터 공간의 패밀리를 설명하고, 각 위치에 부착된 동일한 벡터 공간 \(\mathbb{V}\)(\fibre_라고 함) \(u\in\Omega\)(접선 번들의 경우 이들은 접선 공간 \(T_{u}\Omega\))을 갖는 _베이스 공간_\(\Omega\)으로 구성된다. 대략적으로 번들은 \(u\) 주위에 로컬로 제품 \(\Omega\times\mathbb{V}\)으로 보이지만 전역적으로 'twisted'될 수 있으며 전체적으로 다른 구조를 가질 수 있습니다. 기하학적 딥 러닝에서 섬유는 다양체 \(\Omega\)의 각 지점에서 특징 공간을 모델링하는 데 사용될 수 있으며 섬유의 차원은 특징 채널의 수와 같다. 이러한 맥락에서, 게이지 대칭이라고 불리는 새롭고 매혹적인 종류의 대칭이 나타날 수 있다.

접선 번들 \(T\Omega\)과 벡터 필드 \(X\Omega\to T\Omega\)를 갖는 \(s\)-차원 매니폴드 \(\Omega\)를 다시 생각해 보자(이 용어에서 접선 번들의 _섹션_ 이라고 함). 접선 번들에 대한 게이지 \(\omega\)에 비해 \(X\)는 함수 \(\mathbf{x}:\Omega\to\mathbb{R}^{s}\)로 표시된다. 그러나 우리가 정말로 관심을 갖는 것은 함수 \(\mathbf{x}\in\mathcal{X}(\Omega,\mathbb{R}^{s})\)_의 표상이 게이지 \(\omega\)_의 선택에 의존한다는 것을 깨닫는 것이 중요하다. 게이지가 변경되면, 표현되는 기본 벡터 필드를 보존하기 위해 \(\mathbf{x}\)도 변경해야 한다.

접선 번들 및 구조 그룹 우리가 게이지를 변경할 때, 우리는 각 지점에서 이전 게이지와 새로운 게이지를 매핑하는 역행렬을 적용해야 한다. 이 행렬은 각 점의 모든 게이지 쌍에 대해 고유하지만 다른 점에서는 다를 수 있습니다. 즉, _게이지 변환_은 매핑 \(\mathbf{g}:\Omega\to\mathrm{GL}(s)\)이고, 여기서 \(\mathrm{GL}(s)\)는 가역 \(s\times s\) 행렬의 _일반 선형 그룹_입니다. 게이지 \(\omega_{u}:\mathbb{R}^{s}\to T_{u}\Omega\)에 작용하여 새로운 게이지 \(\omega_{u}^{\prime}=\omega_{u}\circ\mathbf{g}_{u}:\mathbb{R}^{s}\to T_{u}\Omega\)를 생성한다. 게이지 변환은 \(\mathbf{x}^{\prime}(u)=\mathbf{g}_{u}^{-1}\mathbf{x}(u)\)를 통해 각 지점의 좌표 벡터장에 작용하여 새로운 게이지에 대한 \(X\)의 좌표 표현 \(\mathbf{x}^{\prime}\)을 생성한다. 기본 벡터 필드는 변경되지 않은 채로 남아 있다:

\[X(u)=\omega_{u}^{\prime}(\mathbf{x}^{\prime}(u))=\omega_{u}(\mathbf{g}_{u} \mathbf{g}_{u}^{-1}\mathbf{x}(u))=\omega_{u}(\mathbf{x}(u))=X(u),\]

그것은 정확히 우리가 원하는 속성입니다. 보다 일반적으로, \(\mathrm{GL}(s)\)의 표현에 따라 변환되는 기하학적 양의 필드, 예를 들어, \(\)matrices\()\)\(\mathbf{A}(u)\in\mathbb{R}^{s\times s}\)의 필드, \(\mathbf{A}^{\prime}(u)=\rho_{2}(\mathbf{g}_{u}^{-1})\mathbf{A}(u)=\rho_{1}(\mathbf{g}_{u})\mathbf{A}(u)\rho_{1}(\mathbf{g}_{u}^{-1})\mathbf{A}(u)\rho_{1}(\mathbf{g}_{u}^{-1})\를 가질 수 있다. 이 경우 게이지 변환 \(\mathbf{g}_{u}\)은 \(\rho(\mathbf{g}_{u})\)를 통해 작동합니다.

때때로 우리는 직교 프레임, 오른손잡이 프레임 등과 같은 특정 속성을 가진 프레임에 주의를 제한하고자 할 수 있다. 놀랄 것도 없이, 우리는 그룹을 형성하는 몇 가지 재산 보존 변형 세트에 관심이 있다. 예를 들어, 직교성을 보존하는 그룹은 직교 그룹 \(\mathrm{O}(s)\)(회전 및 반사)이고, 방향 또는 '손잡이'를 추가로 보존하는 그룹은 \(\mathrm{SO}(s)\)(순수 회전)이다. 따라서 일반적으로 번들의 _구조 그룹_이라고 하는 그룹 \(\mathfrak{G}\)이 있고 게이지 변환은 맵 \(\mathfrak{g}:\Omega\to\mathfrak{G}\)이다. 중요한 관찰은 주어진 특성을 가진 모든 경우에 주어진 지점에서 임의의 두 프레임에 대해 이들을 관련시키는 게이지 변환이 정확히 하나 존재한다는 것이다.

앞서 언급했듯이 게이지 이론은 접선 번들을 넘어 확장되며, 일반적으로 우리는 구조와 차원이 반드시 기본 공간 \(\Omega\)과 관련이 없는 벡터 공간의 번들을 고려할 수 있다. 예를 들어, 컬러 이미지 픽셀은 2D 그리드 상의 위치 \(u\in\Omega=\mathbb{Z}^{2}\)와 RGB 공간 내의 값 \(\mathbf{x}(u)\in\mathbb{R}^{3}\)을 가지므로, 픽셀들의 공간은 베이스 공간 \(\mathbb{Z}^{2}\)과 각 지점에 부착된 섬유 \(\mathbb{R}^{3}\)를 갖는 벡터 다발로 볼 수 있다. R, G, B에 대한 기저 벡터를 갖는 게이지에 상대적인 RGB 이미지를 표현하는 것이 관례이며, 따라서 이미지의 좌표 표현은 \(\mathbf{x}(u)=(r(u),g(u),b(u))^{\top}\)와 같다. 그러나 우리는 각 지점에서 사용 중인 프레임(채널 순서)을 기억하는 한, 각 위치에서 기저 벡터(컬러 채널)를 독립적으로 균등하게 순열할 수 있다. 계산 작업으로서 이것은 다소 무의미하지만 곧 보게 될 것처럼 RGB 색상의 공간에 대한 게이지 변환에 대해 생각하는 것이 개념적으로 유용합니다. 왜냐하면 게이지 대칭을 표현할 수 있기 때문입니다 - 이 경우 색상 간의 동등성 - 그리고 이미지에 정의된 함수가 이러한 대칭을 존중하도록 합니다(각 색상을 동등하게 처리).

다양체 상의 벡터 필드의 경우와 같이, RGB 게이지 변환은 이미지의 수치 표현(각 픽셀에서 독립적으로 RGB 값을 퍼뮤팅함)을 변경하지만 기본 이미지는 변경하지 않는다. 기계 학습 응용 분야에서 우리는 신경망의 계층으로 구현된 이미지 분류 또는 분할을 수행하기 위해 이러한 이미지에 \(f\in\mathcal{F}(\mathcal{X}(\Omega))\) 함수를 구성하는 데 관심이 있다. 그 결과, 어떤 이유에서든 게이지 변환을 우리의 이미지에 적용한다면, 그 의미를 보존하기 위해 함수 \(f\)(네트워크 레이어)도 변경해야 할 것이다. 단순화를 위해 \(1\times 1\) 컨볼루션, 즉 RGB 픽셀 \(\mathbf{x}(u)\in\mathbb{R}^{3}\)을 특징 벡터 \(\mathbf{y}(u)\in\mathbb{R}^{C}\)에 취하는 맵을 고려한다. 우리의 기하학적 딥러닝 설계도에 따르면, 출력은 그룹 표현 \(\rho_{\text{out}}\), 이 경우 구조 그룹 \(\mathfrak{G}=\Sigma_{3}\)(RGB 채널 순열)의 \(\rho_{\text{in}}(\mathfrak{g})=\mathfrak{g}\)과 연관된다. 그런 다음, 입력에 게이지 변환을 적용하면 선형 맵 \((1\times 1\) convolution) \(f:\mathbb{R}^{3}\to\mathbb{R}^{C}\)을 \(f^{\prime}=\rho_{\text{out}}^{-1}(\mathfrak{g})\circ f\circ\rho_{\text{in}}(\mathfrak{g})\)로 변경하여 출력 특징 벡터 \(\mathbf{y}(u)=f(\mathbf{x}(u))\)를 모든 지점에서 \(\mathbf{y}^{\prime}(u)=\rho_{\text{out}(\mathfrak{g}_{u})\mathbf{y}(u)\)와 같이 변환해야 한다. 실제로 우리는 검증합니다.

\[\mathbf{y}^{\prime}=f^{\prime}(\mathbf{x}^{\prime})=\rho_{\text{out}}^{-1}( \mathfrak{g})f(\rho_{\text{in}(\mathfrak{g})\rho_{\text{in}}^{-1}(\mathfrak{g})\mathbf{x})=\rho_{\text{out}}^{-1}(\mathfrak{g})f(\mathbf{x}).\]

게이지 대칭 우리가 게이지 변환을 대칭으로 간주한다고 말하는 것은 게이지 변환에 의해 관련된 모든 두 게이지가 동등한 것으로 간주된다는 것이다. 예를 들어, \(\mathfrak{G}=\operatorname{SO}(d)\)를 취하면, 임의의 두 개의 오른손잡이 직교 프레임이 동등한 것으로 간주되는데, 이는 임의의 그러한 프레임을 회전에 의해 임의의 다른 그러한 프레임에 매핑할 수 있기 때문이다. 즉, "위"나 "오른쪽"과 같이 구별되는 지역적 방향이 존재하지 않는다. 마찬가지로, \(\mathfrak{G}=\operatorname{O}(d)\)(직교 그룹)이면, 임의의 왼손잡이와 오른손잡이의 직교 프레임이 동등한 것으로 간주된다. 이 경우, 바람직한 배향도 없다. 일반적으로 그룹 \(\mathfrak{G}\)과 모든 점의 프레임 집합 \(u\)을 고려하여 두 개의 프레임에 대해 하나의 프레임을 다른 프레임에 매핑하는 고유한 \(\mathfrak{g}(u)\in\mathfrak{G}\)이 있다.

기하학적 딥 러닝 설계도에서 게이지 변환을 대칭으로 하는 것과 관련하여, 우리는 \(\Omega\)에 정의된 신호에 작용하는 함수 \(f\)와 게이지에 대해 표현되는 함수 \(f\)가 그러한 변환과 동등해야 하는 것에 관심이 있다. 구체적으로, 이것은 우리가 입력에 게이지 변환을 적용한다면, 출력은 동일한 변환(아마도 \(\mathfrak{G}\)의 다른 표현을 통해 작용)을 겪어야 한다는 것을 의미한다. 우리는 이전에 게이지를 변경할 때 함수 \(f\)도 변경해야 하지만 게이지 등분산 지도의 경우 게이지 변경이 매핑 불변에서 벗어나는 경우가 아니라는 점에 주목했다. 이를 보기 위해 RGB 색공간 예를 다시 생각해 본다. 지도 \(f:\mathbb{R}^{3}\to\mathbb{R}^{C}\)는 \(f\circ\rho_{\text{in}}(\mathfrak{g})=\rho_{\text{out}}(\mathfrak{g})\circ f\이면 등식이지만 이 경우 \(f\)에 적용된 게이지 변환은 효과가 없습니다: \(\rho_{\text{out}}^{-1}(\mathfrak{g})\circ f\circ\rho_{\text{in}}(\mathfrak{g})=f\. 즉, 게이지 등분산 지도의 좌표 표현은 게이지와 무관하며, 그래프의 경우 입력 노드가 어떻게 순열되었는지에 관계없이 동일한 함수를 적용하는 것과 같다. 그러나 지금까지 다룬 그래프 및 다른 예제의 경우와 달리 게이지 변환은 _on_\(\Omega\)이 아니라 개별적으로 _on_각 \(u\in\Omega\)에 대한 변환 \(\mathfrak{g}(u)\in\mathfrak{G}\)에 의해 각 특징 벡터_\(\mathbf{x}(u)\)에 작용한다.

더 큰 공간 지원을 가진 다양체의 필터를 볼 때 더 많은 고려 사항이 그림에 들어간다. 먼저 \(s\)차원 매니폴드의 스칼라 필드에서 스칼라 필드로의 매핑 \(f:\mathcal{X}(\Omega,\mathbb{R})\to\mathcal{X}(\Omega,\mathbb{R})\)의 간단한 예를 살펴보자. \(s\)차원 매니폴드의 스칼라 필드에서 스칼라 필드로의 매핑 \(f\mathcal{X}(\Omega,\mathbb{R})\) 벡터 및 기타 기하량과 달리 스칼라에는 방향이 없으므로 스칼라 필드 \(x\in\mathcal{X}(\Omega,\mathbb{R})\)는 _불변_하여 변환(사소한 표현에 따른 변환 \(\rho(\mathfrak{g})=1\))을 측정합니다. 따라서 스칼라 필드에서 스칼라 필드로의 선형 맵은 _게이지 등분산(또는 이 경우 동일)입니다. 예를 들어, 우리는 위치 종속 필터 \(\theta:\Omega\times\Omega\to\mathbb{R}\)를 갖는 컨볼루션과 같은 연산으로서 (19)와 유사하게 \(f\)를 쓸 수 있다.

\[(x\star\theta)(u)=\int_{\Omega}\theta(u,v)x(v)\mathrm{d}v. \tag{22}\]

이는 각 지점에서 잠재적으로 다른 필터 \(\theta_{u}=\theta(u,\cdot)\)를 갖는다는 것을 의미한다. 즉, 게이지 대칭만으로는 제공되지 않는 공간 가중치 공유가 없다.

이제 벡터 필드에서 벡터 필드로의 매핑 \(f:\mathcal{X}(\Omega,T\Omega)\to\mathcal{X}(\Omega,T\Omega)\)의 보다 흥미로운 경우를 생각해보자. 게이지에 비해 입력 및 출력 벡터 필드 \(X,Y\in\mathcal{X}(\Omega,T\Omega)\)는 벡터 값 함수 \(\mathbf{x},\mathbf{y}\in\mathcal{X}(\Omega,\mathbb{R}^{s})\)이다. 이러한 함수들 사이의 일반적인 선형 맵은 스칼라 커널을 행렬 값인 \(\mathbf{\theta}:\Omega\times\Omega\to\mathbb{R}^{s\times s}\)로 대체하는 스칼라(22)에 사용된 것과 동일한 방정식을 사용하여 작성될 수 있다. 행렬 \(\mathbf{\theta}(u,v)\)은 \(T_{v}\Omega\)의 접선벡터를 \(T_{u}\Omega\)의 접선벡터에 매핑해야 하지만, 이 점들은 서로 다른 게이지를 가지고 있기 때문에 임의적이고 독립적으로 변화할 수 있다. 즉, 모든 \(u,v\in\Omega\에 대해 \(\mathbf{\theta}(u,v)=\rho^{-1}(\mathfrak{g}(u))\mathbf{\theta}(u,v)\rho(\mathfrak{g}(v))\)를 만족시켜야 하는데, 여기서 \(\rho\는 \(s\times s\) 회전 행렬에 의해 주어지는 벡터에 대한 \(\mathfrak{G}\)의 작용을 나타낸다. \(\mathfrak{g}(u)\) 및 \(\mathfrak{g}(v)\)를 자유롭게 선택할 수 있기 때문에, 이것은 필터에 대한 지나치게 강한 제약이다.

실제로 \(\mathbf{\theta}\) 이 경우 0이어야 합니다.

더 나은 접근법은 먼저 연결을 통해 벡터를 공통 접선 공간으로 수송한 다음 한 점에서만 게이지 등분산 wrt 단일 게이지 변환을 부과하는 것이다. (22) 대신에, 우리는 그 다음에 벡터 필드들 사이에서 다음의 맵을 정의할 수 있고,

\[(\mathbf{x}\star\mathbf{\theta})(u)=\int_{\Omega}\mathbf{\theta}(u,v)\rho( \mathfrak{g}_{v\to u})\mathbf{x}(v)\mathrm{d}v, \tag{23}\]

여기서 \(\mathfrak{g}_{v\to u}\in\mathfrak{G}\)는 이 두 점을 연결하는 측지선을 따라 \(v\)에서 \(u\)로의 평행 수송을 나타내며, 그 표현 \(\rho(\mathfrak{g}_{v\to u})\)는 점 사이를 이동할 때 벡터를 회전시키는 \(s\times s\) 회전 행렬이다. 이 측지선은 고유한 것으로 가정되며, 이는 로컬에서만 해당하므로 필터가 로컬 지원을 가져야 합니다. 게이지 변환 \(\mathfrak{g}_{u}\하에서 이 요소는 \(\mathfrak{g}_{u\to v}\mapsto\mathfrak{g}_{u}^{-1}\mathfrak{g}_{u\to v} \mathfrak{g}_{v}\로 변환되고 필드 자체는 \(\mathbf{x}(v)\mapsto\rho(\mathfrak{g}_{u})\mathbf{x}(v)\로 변환된다. 필터가 구조 그룹 표현 \(\mathbf{\theta}(u,v)\rho(\mathfrak{g}_{u})=\rho(\mathfrak{g}_{u})\mathbf{ \theta}(u,v)\로 통근하는 경우 방정식 (23)은 _게이지-등변수 컨볼루션_ 을 정의하며, 이는 다음과 같이 변환된다.

\[(\mathbf{x}^{\prime}\star\mathbf{\theta})(u)=\rho^{-1}(\mathfrak{g}_{u})( \mathbf{x}\star\mathbf{\theta})(u).\] 앞서 언급한 변신에 따라.

### 4.6 지오메트릭 그래프 및 메시

우리는 _기하학적 그래프_ (즉, 일부 기하학적 공간에서 실현될 수 있는 그래프) 및 _메쉬_ 를 사용하여 다양한 기하학적 도메인에 대한 논의를 마무리할 것이다. 기하 영역의 '5G'에서 메쉬는 그래프와 다양체 사이의 어딘가에 속하는데, 많은 면에서 그래프와 유사하지만, 그들의 추가적인 구조는 연속적인 물체들과 유사하게 취급할 수 있다. 이러한 이유로 우리는 메쉬를 스킴에서 독립 객체로 간주하지 않으며 실제로 메쉬에 대해 이 섹션에서 파생된 많은 구성이 일반 그래프에도 직접 적용할 수 있음을 강조할 것이다.

우리가 4.4절에서 이미 언급했듯이, 2차원 다양체(표면)는 3차원 물체(또는 더 좋게 말하면 그러한 물체의 경계면)를 모델링하는 일반적인 방법이다. 컴퓨터 그래픽 및 비전 응용 프로그램에서 이러한 표면은 종종 _삼각형 메쉬_로 이산화되며, 이는 삼각형을 가장자리를 따라 함께 접착하여 얻은 표면의 단편적 평면 근사치로 대략 생각할 수 있다. 따라서 메쉬는 추가적인 구조를 가진 (무지향) _그래프입니다. 노드 및 에지 외에도 메쉬 \(\mathcal{T}=(\mathcal{V},\mathcal{E},\mathcal{F})\) 또한 삼각형 면들_\(\mathcal{F}=\{(u,v,q):u,v,q\in\mathcal{V}\text{ 및 }(u,v),(u,q),(q,v)\in\mathcal{E}\}\)을 형성하는 노드들의 트리플렛을 정렬했습니다.

또한, 각 모서리는 정확히 두 개의 삼각형에 의해 공유되고, 각 노드에 입사하는 모든 삼각형의 경계는 모서리의 단일 루프를 형성한다고 가정한다. 이 조건은 각 노드 주변의 1홉 이웃이 디스크 모양이고 메쉬가 _이산 매니폴드_ 를 구성한다는 것을 보장합니다. 이러한 메쉬를 _매니폴드 메쉬_ 라고 합니다. 리만 다양체와 유사하게 메쉬에서 _메트릭_을 정의할 수 있습니다. 가장 간단한 예에서, 메쉬 노드 \(\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\)의 임베딩으로부터 유도될 수 있고, 에지의 유클리드 길이 \(\ell_{uv}=\|\mathbf{x}_{u}-\mathbf{x}_{v}\|\)를 통해 표현될 수 있다. 이렇게 정의된 메트릭은 _삼각형 부등식_과 같은 속성, 즉 임의의 \((u,v,q)\in\mathcal{F}\) 및 에지의 조합에 대해 \(\ell_{uv}\leq\ell_{uq}+\ell_{vq}\) 형태의 식을 자동으로 만족한다. \(\ell\)만으로 표현할 수 있는 모든 성질은 _intrinsic_이고, 메시 보존 \(\ell\)의 변형은 정확히 하나의 삼각형에 속하는 _isometry edge_이다.

라플라시안 행렬은 그래프의 처리와 유사하게, 각각 \(d\)차원 특징 벡터와 연관된 \(n\) 노드가 있는 (다양체) 메쉬를 가정하자. 이 메쉬는 임의의 순서를 가정하여 \(n\times d\) 매트릭스 \(\mathbf{X}\)로 배열할 수 있다. 특징들은 컬러들, 정규들 등과 같은 추가적인 특성들뿐만 아니라 노드들의 기하학적 좌표들, 또는 기하학적 그래프들이 분자들을 모델링하는 화학과 같은 특정 응용들에서, 원자 번호와 같은 특성들을 나타낼 수 있다.

독자들에게 상기시키는 메쉬상의 스펙트럼 콘볼루션(17)은 라플라시안 연산자에서 비롯된 것임을 먼저 살펴보자. 메쉬를 기본 연속 표면의 이산화로 간주하면 라플라시아를 이산화할 수 있다.

\[(\boldsymbol{\Delta}\mathbf{X})_{u}=\sum_{v\in\mathcal{N}_{u}}w_{uv}(\mathbf{x}_{u}-\mathbf{x}_{v}), \tag{24}\]

또는 행렬-벡터 표기법에서, \(n\times n\) 대칭 행렬 \(\boldsymbol{\Delta}=\mathbf{D}-\mathbf{W}\)로서, 여기서 \(\mathbf{D}=\operatorname{diag}(d_{1},\ldots,d_{n})\)를 _차수 행렬_ 및 \(d_{u}=\sum_{v}w_{uv}\) 노드의 _차수_라고 한다. (\phi(\mathbf{x}_{u},\mathbf{X}_{\mathcal{N}_{u}})=d_{u}\mathbf{x}_{u}-\sum_{ v\in\mathcal{N}_{u}}w_{uv}\mathbf{x}_{v}\)와, \(\mathbf{F}(\mathbf{X})=\boldsymbol{\Delta}\mathbf{X}\)는 실제로 그래프에서 순열-등변수 함수를 구성하기 위한 일반적인 청사진(13)의 예이다.

(24)의 라플라시안 정의에는 메쉬에 대한 _특유한 것이 없습니다. 사실, 이 구성은 인접 행렬로 식별되는 에지 가중치, \(\mathbf{W}=\mathbf{A}\), 즉 \((u,v)\in\mathcal{E}\) 및 그렇지 않으면 \(w_{uv}=1\)인 임의의 그래프에도 유효합니다. 이러한 방식으로 구성된 라플라시안들은 종종 그래프의 연결 구조를 포착한다는 사실을 반영하기 위해 _조합자_라고 불린다. 기하 그래프(메쉬의 추가적인 구조를 가질 필요는 없지만, 에지 길이의 형태로 메트릭을 유도하는 공간 좌표를 갖는 노드)의 경우, 예를 들어 \(w_{uv}\propto e^{-\ell_{uv}}\)와 반비례하는 가중치를 사용하는 것이 일반적이다.

메쉬 상에서, 우리는 면들이 제공하는 추가적인 구조를 이용할 수 있고, _코탄젠트 수식_(Pinkall and Polthier, 1993; Meyer et al., 2003)을 사용하여 식 (24)에서 에지 가중치들을 정의할 수 있다.

\[w_{uv}=\frac{\cot\angle_{uqv}+\cot\angle_{upv}}{2a_{u}} \tag{25}\]

여기서 \(\angle_{uqv}\)와 \(\angle_{upv}\)는 공유 에지 \((u,v)\)에 대향하는 삼각형 \((u,q,v)\)과 \((u,p,v)\)의 두 각도이고, \(a_{u}\)는 국소 영역 요소이며, 일반적으로 \(u,p,q)\) 노드를 공유하고 \(a_{u}=\frac{1}{3}\sum_{v,q:(u,v,q)\in\mathcal{F}}}a_{uvq}\로 주어진다.

코탄젠트 라플라시안(cotangent Laplacian)은 여러 개의 편리한 성질을 갖는다는 것을 알 수 있다 (예를 들어, Wardetzky et al. (2007) 참조): 이것은 _positive-semidefinite_ matrix, \(\mathbf{\Delta}\succcurlyeq 0\)이고, 따라서 비음수 고유값 \(\lambda_{1}\leq\ldots\leq\lambda_{n}\)을 가지며, 주파수의 유추로 간주될 수 있고, 대칭이고 직교 고유벡터를 가지며, _local_ (즉, \(\mathbf{\Delta}\mathbf{X})_{u}\)의 값은 1홉 이웃, \(\mathcal{N}_{u}\)에만 의존한다). 아마도 가장 중요한 특성은 코탄젠트 메쉬 라플라시안 행렬 \(\mathbf{\Delta}\)이 연속 연산자 \(\Delta\)에 수렴하는 것이다. 따라서 식 (25)는 4.4절의 리만 다양체에 정의된 라플라시안 연산자의 적절한 이산화를 구성한다.

하나는 라플라시안(Laplacian)이 내재적일 것으로 예상하지만, 이것은 식 (25)에서 그다지 분명하지 않으며, 코탄젠트 가중치를 이산 미터법 \(\ell\)으로 완전히 표현하려면 약간의 노력이 필요하다.

\[w_{uv}=\frac{-\ell_{uv}^{2}+\ell_{vq}^{2}+\ell_{uq}^{2}}{8a_{uvq}}+\frac{- \ell_{uv}^{2}+\ell_{vp}^{2}+\ell_{up}^{2}}{8a_{uvp}}\]

여기서 삼각형의 넓이 \(a_{ijk}\)는 다음과 같이 주어진다.

\[a_{uvq}=\sqrt{s_{uvq}(s_{uvq}-\ell_{uv})(s_{uvq}-\ell_{vq})(s_{uvq}-\ell_{uq})}\]

(s_{uvq}=\frac{1}{2}(\ell_{uv}+\ell_{uq}+\ell_{vq})\). 이것은 라플라시안(및 고유 벡터 및 고유 값과 같은 모든 양)에 기하학 처리 및 컴퓨터 그래픽에서 매우 사랑받는 속성인 _등각 불변_ 을 부여합니다. (왕 및 솔로몬(2019)의 우수한 검토 참조): 메트릭 \(\ell\)에 영향을 미치지 않는 메쉬의 변형 \(\ell\) (메쉬의 가장자리를 '쾅쾅'하거나 '쾅쾅'하지 않음)은 라플라시안(라플라시안)을 변경하지 않습니다.

마지막으로, 라플라시안(25)의 정의는 \(\mathcal{N}_{u}\)에서 노드의 순열에 불변하며, 이는 합산의 형태로 집계를 포함하기 때문이다. 일반적인 그래프에서는 이웃의 정준적 순서가 없기 때문에 필요악이지만 메쉬에서는 어떤 방향(예: 시계 방향)에 따라 1홉 이웃을 주문할 수 있으며 유일한 모호성은 첫 번째 노드의 선택이다. 따라서 4.5절에서 논의한 \(\mathrm{SO}(2)\) 게이지 변환에서 발생하는 모호성에 직관적으로 대응하는 _순환 시프트_(회전) 대신 _이방성 라플라시안_을 정의할 수 있다. 고정 게이지의 경우 지역 방향에 민감하고 메트릭 또는 가중치를 변경하는 양에 해당하는 _이방성 라플라시안_을 정의할 수 있다 \(w_{uv}\). 이러한 종류의 구성은 Andreux 등(2014); Boscaini 등(2016b) 및 Boscaini 등(2016a)의 초기 기하 딥러닝 아키텍처에서 형상 기술자를 설계하는 데 사용되었다.

메쉬에 대한 스펙트럼 분석 직교 고유벡터 \(\mathbf{\Phi}=(\boldsymbol{\varphi}_{1},\ldots,\boldsymbol{\varphi}_{n})\) 라플라시안 행렬의 대각선화 \((\boldsymbol{\Delta}=\mathbf{\Phi}\boldsymbol{\Lambda}\mathbf{\Phi}^{\top}\) 여기서 \(\boldsymbol{\Delta}=\operatorname{diag}(\lambda_{1},\ldots,\lambda_{n})\)는 라플라시안 고유값의 대각선 행렬이며, 각각의 푸리에 변환의 곱으로서 메쉬에 대한 스펙트럼 콘볼루션을 수행할 수 있는 비유클리드 유사로서 사용된다.

\[\mathbf{X}\star\boldsymbol{\theta}=\mathbf{\Phi}\operatorname{diag}(\mathbf{ \Phi}^{\top}\boldsymbol{\theta})(\mathbf{\Phi}^{\top}\mathbf{X})=\mathbf{\Phi}\operatorname{diag}(\hat{\boldsymbol{\theta}})\hat{\mathbf{X}},\

여기서 필터 \(\hat{\boldsymbol{\theta}}\)는 푸리에 영역에서 직접 설계된다. 다시 말하지만, 이 공식의 어떤 것도 메쉬에 특정되지 않으며 일반(무지향) 그래프의 라플라시안 행렬을 사용할 수 있다. CNN을 그래프로 일반화하기 위해 콘볼루션의 이러한 스펙트럼 정의를 이용하는 것은 유혹적이며, 이는 실제로 이 텍스트의 저자 중 한 명인 Bruna 등(2013)에 의해 수행되었다. 그러나 비유클리드 푸리에 변환은 기본 메쉬 또는 그래프의 사소한 섭동에도 매우 민감한 것으로 보이며(섹션 4.4의 그림 12 참조), 따라서 _고정_ 도메인에서 다른 신호를 처리해야 할 때만 사용할 수 있지만 _다른 도메인에서 일반화하려는 경우에는 사용할 수 없다. 운 좋게도, 많은 컴퓨터 그래픽 및 비전 문제가 후자의 범주에 속하는데, 여기서 하나는 3D 형상(메쉬)의 한 세트에서 신경망을 훈련시키고 다른 세트에서 테스트하여 푸리에 변환 기반 접근법을 부적절하게 만든다.

섹션 4.4에서 언급된 바와 같이, 라플라시안 매트릭스에 일부 전달 함수 \(\hat{p}(\lambda)\)를 적용하는 형태 (18)의 스펙트럼 필터를 사용하는 것이 바람직하고,

\[\hat{p}(\boldsymbol{\Delta})\mathbf{X}=\mathbf{\Phi}\hat{p}(\boldsymbol{ \Lambda})\mathbf{\Phi}^{\top}\mathbf{X}=\mathbf{\Phi}\operatorname{diag}(\hat{p}(\lambda_{1}),\ldots,\hat{p}(\lambda_{n}))\hat{\mathbf{X}}}.\]

\(\hat{p}\)를 행렬-벡터 곱으로 표현할 수 있을 때, \(n\times n\) 행렬 \(\boldsymbol{\Delta}\)의 고유분해는 전혀 피할 수 있다. 예를 들어, Defferrard 등(2016)은 차수 \(r\)의 _다항식_ 을 필터 함수로 사용하였다.

\[\hat{p}(\boldsymbol{\Delta})\mathbf{X}=\sum_{k=0}^{r}\alpha_{k}\boldsymbol{ \Delta}^{k}\mathbf{X}=\alpha_{0}\mathbf{X}+\alpha_{1}\boldsymbol{\Delta} \mathbf{X}+\ldots+\alpha_{r}\boldsymbol{\Delta}^{r}\mathbf{X},\

## Chapter 4 Geometric Domains: 5 Gs

5Gs(\(n\times n\) 라플라시안 행렬)은 _유클리드_ 행렬 \(r\)회이다. Laplacian은 일반적으로 희박하기 때문에 ( \(\mathcal{O}(|\mathcal{E}|)\) 0이 아닌 원소를 가지고) 이 연산은 \(\mathcal{O}(|\mathcal{E}|dr)\sim\mathcal{O}(|\mathcal{E}|)\의 낮은 복잡도를 갖는다. 또한 라플라시안(Laplacian)은 로컬(local)이기 때문에 차수(degree)의 다항식 필터(polynomial filter)가 \(r\)-홉(hop) 이웃에 위치된다.

그러나 필터의 실제 지원(즉, 커버하는 반지름)이 메쉬의 _해상도_ 에 따라 달라지기 때문에 이러한 정확한 속성은 메쉬를 처리할 때 불리하게 된다. 하나의 메쉬는 어떤 연속적인 표면의 이산화로부터 발생한다는 것을 명심해야 하며, 하나의 메쉬는 _동일한 물체_ 를 나타내는 두 개의 서로 다른 메쉬 \(\mathcal{T}\)와 \(\mathcal{T}^{\prime}\)를 가질 수 있다. 더 미세한 메쉬에서는 더 거친 메쉬보다 더 큰 이웃(따라서 필터의 더 큰 차수 \(r\))을 사용해야 할 수 있다.

이러한 이유로 컴퓨터 그래픽 응용 프로그램에서는 해상도에 독립적이기 때문에 _합리적인 필터_ 를 사용하는 것이 더 일반적이다. 이러한 필터를 정의하는 방법에는 여러 가지가 있다(예를 들어, Patane (2020) 참조). 가장 일반적인 것은 \(\frac{\lambda-1}{\lambda+1}\). 보다 일반적으로 실수선을 복소 평면의 단위 원에 매핑하는 _Cayley transform_\(\frac{\lambda-\mathrm{i}}{\lambda+\mathrm{i}}\)와 같은 복소 함수를 사용할 수 있다. Levie et al.(2018)은 _Cayley 다항식으로 표현된 스펙트럼 필터, 복소 계수를 갖는 실수 유리 함수 \(\alpha_{l}\in\mathbb{C}\),

\[\hat{p}(\lambda)=\mathrm{Re}\left(\sum_{l=0}^{r}\alpha_{l}\left(\frac{\lambda- \mathrm{i}}{\lambda+\mathrm{i}}\right)^{l}\right).\]

행렬들에 적용될 때, 상기 케일리 다항식의 계산은 행렬 역산을 필요로 하고,

\[\hat{p}(\boldsymbol{\Delta})=\mathrm{Re}\left(\sum_{l=0}^{r}\alpha_{l}( \boldsymbol{\Delta}-\mathrm{i}\mathbf{I})^{l}(\boldsymbol{\Delta}+\mathrm{i} \mathbf{I})^{-l}\right),\]

이는 선형 복잡도로 대략적으로 수행될 수 있다. 다항식 필터와 달리, 유리형 필터는 국부적인 지지를 갖지 않지만, 지수 감쇠를 갖는다(Levie et al., 2018). 푸리에 변환의 직접 계산과 비교하여 중요한 차이점은 다항식 및 유리 필터가 기본 그래프 또는 메쉬의 근사 등척성 변형 하에서 안정하다는 것이다. 이러한 종류의 다양한 결과가 Levie 등(2018, 2019), Gama 등(2020), Kenlay 등(2021)에 의해 나타났다.

연산자로서의 메시와 기능 지도 기능 지도의 패러다임은 메시를 _연산자_로 생각하는 것을 제안한다. 우리가 보여주듯이, 이것은 메쉬의 추가 구조를 이용하는 보다 흥미로운 유형의 불변성을 얻을 수 있게 한다. 논의의 목적을 위해, 메쉬 \(\mathcal{T}\)가 좌표 \(\mathbf{X}\)를 갖는 임베디드 노드에 구축되었다고 가정한다. 라플라시안과 같은 고유 연산자를 구성하면 메쉬의 구조를 완전히 인코딩하고 메쉬를 복구할 수 있음을 보여줄 수 있다(Zeng 등(2012). 이것은 또한 일부 다른 연산자(예를 들어, Boscaini 등(2015); Corman 등(2017); Chern 등(2018))에 대해서도 마찬가지이므로, 우리는 일반적인 연산자, 또는 \(n\times n\) 행렬 \(\mathbf{Q}(\mathcal{T},\mathbf{X})\)를 메쉬의 표현으로 가정할 것이다.

이 견해에서, 형태 \(f(\mathbf{X},\mathcal{T})\)의 학습 기능에 대한 섹션 4.1의 논의는 형태 \(f(\mathbf{Q})\)의 학습 기능으로 다시 표현될 수 있다. 그래프들 및 세트들과 유사하게, 메쉬들의 노드들은 또한 정준 순서화(canonical ordering)를 갖지 않으며, 즉 메쉬들 상의 함수들은 순열 불변 또는 등분산 조건들을 만족시켜야 하고,

\[f(\mathbf{Q}) = f(\mathbf{P}\mathbf{Q}\mathbf{P}^{\top})\] \[\mathbf{P}\mathbf{F}(\mathbf{Q}) = \mathbf{F}(\mathbf{P}\mathbf{Q}\mathbf{P}^{\top})\]

임의의 순열 행렬 \(\mathbf{P}\). 그러나 일반적인 그래프에 비해 우리는 더 많은 구조를 가지고 있다: 우리는 메쉬가 어떤 기본 연속 표면 \(\Omega\)의 이산화로부터 발생한다고 가정할 수 있다. 따라서 \(\mathcal{T}^{\prime}=(\mathcal{V}^{\prime},\mathcal{E}^{\prime},\mathcal{F}^ {\prime})\)와 \(n^{\prime}\) 노드 및 좌표 \(\mathbf{X}^{\prime}\)는 \(\mathcal{T}\)와 동일한 대상을 나타내는 다른 메쉬 \(\mathcal{T}\)를 가질 수 있다. 중요하게도, 메쉬 \(\mathcal{T}\)와 \(\mathcal{T}^{\prime\)는 서로 다른 연결 구조를 가질 수 있고 심지어 노드 수 (\(n^{\prime\neq n\))도 서로 다를 수 있다. 따라서 우리는 이러한 메쉬를 단순히 노드의 재정렬만으로 동형 그래프로 생각할 수 없으며, 순열 행렬 \(\mathbf{P}\)을 그들 사이의 대응으로 간주할 수 없다.

기능 지도는 Ovsjanikov 등(2012)에 의해 이러한 설정에 대한 대응 개념의 일반화로 도입되었으며, 두 도메인(지도 \(\eta:\Omega\to\Omega^{\prime}\))의 _포인트 간의 대응 관계를 _함수_(지도 \(\mathbf{C}:\mathcal{X}(\Omega)\to\mathcal{X}(\Omega^{\prime})\) 간의 대응 관계로 대체했다. 함수 맵은 행렬 \(n^{\prime}\times n\)로 표시되는 선형 연산자 \(\mathbf{C}\)로서 각 도메인에서 신호 \(\mathbf{x}^{\prime}\)와 \(\mathbf{x}\) 사이의 대응 관계를 설정합니다.

\[\mathbf{x}^{\prime}=\mathbf{C}\mathbf{x}.\]

Rustamov et al.(2013)은 _영역 보존_ 매핑을 보장하기 위해서는 함수 맵이 직교, \(\mathbf{C}^{\top}\mathbf{C}=\mathbf{I}\), 즉 직교 그룹 \(\mathbf{C}\in\mathrm{O}(n)\)의 요소가 되어야 함을 보였다. 이 경우 \(\mathbf{C}^{-1}=\mathbf{C}^{\top}\)을 이용하여 지도를 반전시킬 수 있다.

상기 기능 맵은 또한 메쉬들의 오퍼레이터 표현 사이의 관계를 확립하고,

\[\mathbf{Q}^{\prime}=\mathbf{C}\mathbf{Q}\mathbf{C}^{\top},\qquad\mathbf{Q}= \mathbf{C}^{\top}\mathbf{Q}^{\prime}\mathbf{C},\]

우리는 \(\mathcal{T}\)의 연산자 표현 \(\mathbf{Q}\)과 함수 맵 \(\mathbf{C}\)이 주어지면 \(\mathcal{T}^{\prime}\)의 연산자 표현 \(\mathbf{Q}^{\prime}\)을 구성할 수 있다. \(\mathcal{T}\)에서 \(\mathcal{T}\)로 신호를 먼저 매핑하여 \(\mathbf{C}^{\top}\)를 사용하고 연산자 \(\mathbf{Q}\)를 적용한 후 다시 \(\mathcal{T}^{\prime}\)로 매핑하여 \(\mathbf{C}\)를 사용할 수 있다.

\[f(\mathbf{Q}) = f(\mathbf{C}\mathbf{Q}\mathbf{C}^{\top})=f(\mathbf{Q}^{\prime})\] \[\mathbf{C}\mathbf{F}(\mathbf{Q}\mathbf{C}^{\top})=\mathbf{F}( \mathbf{Q}^{\prime})\]

for any \(\mathbf{C}\in\mathrm{O}(n)\). 앞의 순열 불변성과 등분성의 설정이 특정한 경우임을 쉽게 알 수 있는데, 이는 마디의 순서만 바뀌는 사소한 재해석으로 생각할 수 있다.

Wang et al.(2019a)은 연산자 \(\mathbf{Q}=\mathbf{V}\mathbf{A}\mathbf{V}^{\top}\), 임의의 재메싱 불변 함수(또는 equivariant)는 \(f(\mathbf{Q})=f(\mathbf{\Lambda})\) 및 \(\mathbf{F}(\mathbf{Q})=\mathbf{V}\mathbf{F}(\mathbf{\Lambda})\)로 표현될 수 있거나, 다시 말해서 재메싱 불변 함수들은 \(\mathbf{Q}\)의 스펙트럼에만 관여한다. 실제로, 라플라시안 고유값들의 함수들은 표면 이산화 및 섭동에 강인하다는 것이 실제로 입증되어, 그래프 상의 딥 러닝뿐만 아니라 컴퓨터 그래픽에서 라플라시안들에 기초한 스펙트럼 구성들의 인기를 설명한다(Defferrard et al., 2016; Levie et al., 2018). 이 결과는 일반적인 연산자 \(\mathbf{Q}\)를 참조하기 때문에, 유비쿼터스 라플라시안 외에 다중 선택이 가능하다 - 주목할 만한 예는 Dirac(Liu et al., 2017; Kostrikov et al., 2018) 또는 Steklov(Wang et al., 2018) 연산자와 학습 가능한 파라메트릭 연산자(Wang et al., 2019a)를 포함한다.

도 13: 포인트 와이즈 맵(왼쪽) 대 기능 맵(오른쪽)

## 5 지오메트릭 딥러닝 모델

기하학적 딥 러닝 청사진의 다양한 인스턴스화(도메인, 대칭 그룹 및 지역 개념의 다양한 선택에 대해)를 철저히 연구한 후, 이러한 처방을 시행하는 것이 가장 인기 있는 딥 러닝 아키텍처 중 일부를 산출할 수 있는 방법에 대해 논의할 준비가 되어 있습니다.

우리의 박람회는 다시 한번 일반성의 엄격한 순서가 아닐 것이다. 우리는 처음에 구현이 우리의 앞선 논의로부터 거의 직접 따르는 세 가지 아키텍처, 즉 컨볼루션 신경망(CNN), 그룹-등변 CNN 및 그래프 신경망(GNN)을 다룬다.

그런 다음 그래프 구조가 미리 알려져 있지 않은 경우(즉, 정렬되지 않은 집합)에 대한 GNN의 변형을 자세히 살펴보고 토론을 통해 인기 있는 딥셋 및 트랜스포머 아키텍처를 GNN의 인스턴스로 설명한다.

기하 그래프와 메쉬에 대한 논의에 이어, 먼저 명시적인 기하 대칭을 GNN 계산에 도입하는 등분산 메시지 전달 네트워크를 설명한다. 그런 다음, 지오데식 및 게이지 대칭 이론이 딥 러닝 내에서 구체화될 수 있는 방법을 보여줌으로써, 고유 메시 CNN(Geodesic CNN, MoNet 및 게이지-equivariant mesh CNN 포함)의 패밀리를 복구한다.

마지막으로, 우리는 _시간적_ 각도에서 그리드 영역을 돌아봅니다. 이 논의는 우리를 순환 신경망(RNN)으로 이끌 것이다. 우리는 RNN이 시간 격자에 대해 번역 불변인 방식을 보여주지만 시간 와핑 변환에 대한 안정성을 연구할 것이다. 이 특성은 장거리 종속성을 적절하게 처리하는데 매우 바람직하며, 그러한 변환들에 대한 클래스 불변성을 강제하는 것은 (LSTM 또는 GRU와 같은 인기 있는 RNN 모델들을 포함하는) 게이티드 RNN들의 클래스를 정확하게 산출한다.

위의 캔버스가 작성 시 사용되는 대부분의 주요 딥 러닝 아키텍처를 활용하기를 희망하지만, 우리는 새로운 신경망 인스턴스가 매일 제안된다는 것을 잘 알고 있다. 따라서, 가능한 모든 아키텍처를 포괄하는 것을 목표로 하기보다는 다음 섹션이 충분히 예시적이어서 독자가 불변과 대칭의 렌즈를 사용하여 미래의 기하 딥러닝 개발을 쉽게 분류할 수 있기를 바란다.

## 5 GEOMETRIC DEEP LEARNING 모델

### 컨볼루션 신경망

Convolutional Neural Networks는 3.5절에서 설명한 Geometric Deep Learning의 설계도를 따르는 딥 러닝 아키텍처의 가장 초기이자 가장 잘 알려진 예일 것이다. 4.2절에서는 Convolutions \(\mathbf{C}(\boldsymbol{\theta})\mathbf{x}=\mathbf{x}\star\boldsymbol{\theta}\)와 localised filter \(\boldsymbol{\theta}\)로 주어진 선형 및 국소 번역 등분산 연산자의 클래스를 완전히 특성화했다. 먼저 스칼라 값('단일 채널' 또는 '그레이스케일')이 이산화 된 이미지에 초점을 맞추자. 여기서 도메인은 \(\mathbf{u}=(u_{1},u_{2})\) 및 \(\mathbf{x}\in\mathcal{X}(\Omega,\mathbb{R})\)로 격자 \(\Omega=[H]\times[W]\)이다.

크기 \(H^{f}\times W^{f}\)의 컴팩트하게 지원되는 필터와의 컨볼루션은 예를 들어 단위 피크 \(\boldsymbol{\theta}_{vw}(u_{1},u_{2})=\delta(u_{1}-v,u_{2}-w)\로 주어진 생성기 \(\boldsymbol{\theta}_{1,1},\ldots,\boldsymbol{\theta}_{H^{f},W^{f}}\)의 선형 조합으로 기록될 수 있다. 따라서 임의의 국소 선형 등분산 맵은 다음과 같이 표현할 수 있다.

\[\mathbf{F}(\mathbf{x})=\sum_{v=1}^{H^{f}}\sum_{w=1}^{W^{f}}\alpha_{vw}\mathbf{ C}(\boldsymbol{\theta}_{vw})\mathbf{x}\;, \tag{26}\]

이는, 좌표들에서, 친숙한 2D 컨볼루션에 대응한다(개요를 위해 도 14 참조):

\[\mathbf{F}(\mathbf{x})_{uv}=\sum_{a=1}^{H^{f}}\sum_{b=1}^{W^{f}}\alpha_{ab}x_{ u+a,v+b}\;. \tag{27}\]

그림 14: 이미지를 필터로 컨볼빙하는 과정 \(\mathbf{x}\) \(\mathbf{C}(\boldsymbol{\theta})\) 필터 파라미터 \(\boldsymbol{\theta}\)는 발생기 \(\boldsymbol{\theta}_{vw}\)의 선형 결합으로 표현될 수 있다.

기저 \(\mathbf{\theta}_{vw}\)의 다른 선택도 가능하며 (잠재적으로 다른 \(\alpha_{vw}\)의 선택에 대해) 동등한 연산을 산출할 것이다. 대표적인 예로 방향도함수 \(\mathbf{\theta}_{vw}(u_{1},u_{2})=\delta(u_{1},u_{2})-\delta(u_{1}-v,u_{2}-w),(v,w) \neq(0,0)\)를 지역평균 \(\mathbf{\theta}_{0}(u_{1},u_{2})=\frac{1}{H_{f}W_{f}}\)과 함께 취한다. 사실, 방향성 도함수는 그래프에서 확산 과정의 격자별 유사체로 간주될 수 있으며, 각 픽셀을 그리드의 인접한 픽셀에 연결된 노드로 가정하면 복구된다.

스칼라 입력 채널이 다수의 채널들(예를 들어, RGB 컬러들, 또는 더 일반적으로 임의의 수의 _특징 맵들_)로 대체될 때, 컨볼루션 필터는 입력 특징들의 임의의 선형 조합들을 출력 특징 맵들로 표현하는 _컨볼루션 텐서_가 된다. 좌표에서, 이는 다음과 같이 표현될 수 있다.

\[\mathbf{F}(\mathbf{x})_{uvj}=\sum_{a=1}^{H^{f}}\sum_{b=1}^{W^{f}}\sum_{c=1}^{M} \alpha_{jabc}x_{u+a,v+b,c}\;,\;j\in[N]\;, \tag{28}\]

여기서, \(M\) 및 \(N\)는 각각 입력 채널 및 출력 채널의 수이다. 이 기본 작업은 다음 섹션에서 보여주겠지만 컴퓨터 비전, 신호 처리 및 그 이상의 많은 영역에 걸쳐 심오한 영향을 미친 광범위한 종류의 신경망 아키텍처를 포함한다. 여기에서 CNN의 무수히 많은 가능한 아키텍처 변형을 해부하기보다는 광범위한 사용을 가능하게 한 필수 혁신 중 일부에 초점을 맞추는 것을 선호한다.

일반적인 대칭을 위한 GDL 템플릿에서 논의된 바와 같이, 컨볼루션 연산자 \(\mathbf{F}\)에서 변환 불변 특징을 추출하는 효율적인 멀티스케일 계산은 비선형 단계를 필요로 한다. 컨벌루션 피쳐는 비선형 _활성화 함수_\(\sigma\)를 통해 처리되며, 입력 요소별로 작용합니다. 즉, \(\sigma:\mathcal{X}(\Omega)\rightarrow\mathcal{X}(\Omega)\), \(\sigma(\mathbf{x})(u)=\sigma(\mathbf{x}(u))\). 아마도 쓰기 당시 가장 인기 있는 예는 Rectified Linear Unit(ReLU): \(\sigma(x)=\max(x,0)\이다. 이 비선형성은 신호를 효과적으로 _수정_하여 에너지를 더 낮은 주파수로 밀어내고 구성을 반복함으로써 규모에 걸쳐 고차 상호 작용의 계산을 가능하게 한다.

이미 후쿠시마와 미야케(1982)와 LeCun 등(1998)의 초기 연구에서 CNN과 유사한 아키텍처는 다중스케일 구조를 가졌으며, 각 컨볼루션 레이어(28) 후에 그리드 조대화 \(\mathbf{P}:\mathcal{X}(\Omega)\rightarrow\mathcal{X}(\Omega^{\prime})\)를 수행하고 그리드 \(\Omega^{\prime}\)는 \(\Omega\)보다 더 거친 해상도를 갖는다. 이는 수용 필드를 효과적으로 증가시키면서 스케일당 일정한 수의 파라미터를 유지하는 멀티스케일 필터를 가능하게 한다. 몇 가지 신호 조대화 전략 \(\mathbf{P}\)( _pooling_이라고 함)이 사용될 수 있으며, 가장 일반적인 것은 저역 통과 안티앨리어싱 필터(예: 로컬 평균)를 적용한 후 그리드 다운샘플링 또는 비선형 맥스 풀링이다.

요약하면, '바닐라' CNN 레이어는 우리의 기하학적 딥 러닝 청사진에 이미 소개된 기본 객체들의 구성으로 표현될 수 있다:

\[\mathbf{h}=\mathbf{P}(\sigma(\mathbf{F}(\mathbf{x})))\, \tag{29}\]

i.e. equivariant linear layer \(\mathbf{F}\), coarsening operation \(\mathbf{P}\), non-linearity \(\sigma\). 또한 CNN 내에서 번역 불변 _global_ 풀링 작업을 수행할 수 있습니다. 직관적으로, 이것은 여러 컨벌루션 후에 이미지의 최종 표현을 요약하는 _패치_를 포함하는 각 픽셀을 포함하며, 최종 선택은 이러한 제안의 집합 형태에 의해 안내된다. 여기에서 인기 있는 선택은 평균 함수인데, 그 출력은 이미지 크기에 관계없이 유사한 크기를 유지하기 때문이다(Springenberg et al., 2014).

이 CNN 청사진에 이어지는 두드러진 예(그 중 일부는 다음에 논의할 것임)가 그림 15에 표시되어 있다.

따라서 딥 앤 레지듀얼 NetworksA CNN 구조는 가장 간단한 형태로 하이퍼파라미터 \((H_{k}^{f},W_{k}^{f},N_{k},p_{k})_{k\leq K}\)로 지정되며, \(M_{k+1}=N_{k}\)와 \(p_{k}=0,1\)는 그리드 조대화 수행 여부를 나타낸다. 이 모든 하이퍼파라미터가 실제로 중요하지만, 특히 중요한 질문은 CNN 구조에서 깊이 \(K\)의 역할과 특히 필터 크기 \((H_{k}^{f},W_{k}^{f})\와 관련하여 이러한 주요 하이퍼파라미터를 선택하는 데 관련된 근본적인 트레이드오프가 무엇인지 이해하는 것이다.

이 질문에 대한 엄격한 답은 여전히 파악하기 어렵지만 최근 몇 년 동안 수집된 경험적 증거의 증가는 더 깊은(큰 \(K\))이지만 더 얇은(작은 \((H_{k}^{f},W_{k}^{f})\)) 모델에 유리한 절충안을 제안한다. 이러한 맥락에서 He 등(2016)의 중요한 통찰력은 일반적인 비선형 변환이 아닌 이전 기능의 _교란_ 을 모델링하기 위해 각 콘볼루션 계층을 재파라메타라이즈하는 것이었다:

\[\mathbf{h}=\mathbf{P}\left(\mathbf{x}+\sigma(\mathbf{F}(\mathbf{x}))\right). \tag{30}\]

결과 _잔여_ 네트워크는 이전 공식보다 몇 가지 주요 이점을 제공합니다. 본질적으로, 잔여 파라미터화는 일관된 도 15: CNN 아키텍처들의 두드러진 예시들이다. **Top-to-bottom**: LeNet(LeCun 등, 1998), AlexNet(Krizhevsky 등, 2012), ResNet(He 등, 2016) 및 U-Net(Ronneberger 등, 2015). PlotNeuralNet 패키지(Iqbal, 2018)를 사용하여 그립니다.

심층 네트워크는 상미분 방정식(ODE)으로 모델링된 기본 연속 역학 시스템의 이산화라는 관점에서. 결정적으로, 속도를 모델링하여 동적 시스템을 학습하는 것은 위치를 직접 학습하는 것보다 훨씬 쉬운 것으로 밝혀졌다. 우리의 학습 설정에서 이것은 더 유리한 기하학을 가진 최적화 지형으로 변환되어 이전보다 훨씬 더 깊은 아키텍처를 훈련할 수 있는 능력으로 이어진다. 향후 연구에서 논의될 바와 같이, 심층 신경망을 사용한 학습은 비-볼록 최적화 문제를 정의하며, 이는 특정 단순화 체제 하에서 구배-하강 방법을 사용하여 효율적으로 해결될 수 있다. ResNet 매개변수의 주요 이점은 간단한 시나리오(Hardt and Ma, 2016)에서 엄격하게 분석되었으며 이론적 조사의 활발한 영역으로 남아 있다. 마지막으로, 신경망 ODE(Chen et al., 2018)는 ODE \(\dot{\mathbf{x}}=\sigma(\mathbf{F}(\mathbf{x}))\)의 매개 변수를 직접 학습하고 표준 수치 적분에 의존함으로써 ODE와의 유추를 더욱 밀어붙이는 최근의 인기 있는 아키텍처이다.

정규화 CNN의 경험적 성능을 크게 향상시킨 또 다른 중요한 알고리즘 혁신은 _정규화_ 개념입니다. 신경 활동의 초기 모델에서 뉴런은 국소적인 '이득 제어'의 일부 형태를 수행하는 것으로 가정되었으며, 여기서 층 계수 \(\mathbf{x}_{k}\)는 \(\tilde{\mathbf{x}_{k}=\sigma_{k}^{-1}\odot(\mathbf{x}_{k}-\mu_{k})\)로 대체된다. 여기서, \(\mu_{k}\)와 \(\sigma_{k}\)는 각각 \(\mathbf{x}_{k}\)의 1차 모멘트 정보와 2차 모멘트 정보를 부호화한다. 또한, 그것들은 전역적으로 또는 국부적으로 계산될 수 있다.

딥 러닝의 맥락에서, 이 원칙은 _배치 정규화_ 계층(Ioffe and Szegedy, 2015)을 통해 널리 채택되었고, 그 다음 몇 가지 변형(Ba 등, 2016; Salimans and Kingma, 2016; Ulyanov 등, 2016; Cooijmans 등, 2016; Wu and He, 2018)이 뒤따랐다. 더 나은 조건화된 최적화 풍경의 관점에서 정상화의 이점을 엄격하게 설명하려는 일부 시도에도 불구하고(산투르카르 등, 2018), 지침 원칙을 제공할 수 있는 일반 이론은 작성 당시 여전히 누락되어 있다.

데이터 증강 CNN은 번역 불변 및 스케일 분리와 관련된 기하학적 전적을 인코딩하지만, 번개 또는 색상 변화, 또는 작은 회전 및 확장과 같은 의미 정보를 보존하는 다른 알려진 변환을 명시적으로 설명하지 않는다. 이러한 이전을 최소한의 아키텍처 변경으로 통합하기 위한 실용적인 접근 방식은 _데이터 증강_ 을 수행하는 것이며, 여기서 수동으로 입력 이미지에 대한 변환을 수행하고 이를 훈련 세트에 추가합니다.

데이터 증강은 경험적으로 성공적이었고, 최첨단 비전 아키텍처를 훈련하는 것뿐만 아니라, 자기-감독 및 인과적 표현 학습에서의 여러 발전을 지원하는 데 널리 사용된다(Chen et al., 2020; Grill et al., 2020; Mitrovic et al., 2020). 그러나 샘플 복잡성(Mei et al., 2021) 측면에서 입증할 수 있을 정도로 최적이 아니며, 다음으로 논의하는 것처럼 더 효율적인 전략은 대신 더 풍부한 불변 그룹을 가진 아키텍처를 고려한다.

### Group-equivariant CNNs

섹션 4.3에서 논의한 바와 같이, 우리는 유클리드 공간의 신호에서 그룹 \(\mathfrak{G}\)에 의해 작용하는 모든 _균질 공간_\(\Omega\)의 신호로 컨볼루션 연산을 일반화할 수 있다. 그룹 컨벌루션의 아이디어는 _번역된_ 필터가 신호와 일치하는 유클리드 컨벌루션과 유추하여 그룹 액션을 사용하여 예를 들어 회전 및 병진으로 필터를 도메인 주위에 이동시키는 것이다. 그룹 액션의 _전이성_ 덕분에 우리는 필터를 \(\Omega\) 상의 임의의 위치로 이동시킬 수 있다. 이 절에서는 구현 측면과 아키텍처 선택을 포함하여 그룹 컨볼루션의 일반적인 아이디어에 대한 몇 가지 구체적인 예를 논의할 것이다.

이산 그룹 컨볼루션은 영역 \(\Omega\)과 그룹 \(\mathfrak{G}\)이 이산인 경우를 고려하여 시작한다. 첫 번째 예로, 이산 이동 및 회전 대칭을 갖는 3D 그리드 상의 신호로 표현되는 의료 체적 이미지를 고려한다. 도메인은 3차원 입체격자 \(\Omega=\mathbb{Z}^{3}\)이고, 이미지(예: MRI 또는 CT 3D 스캔)는 함수 \(x:\mathbb{Z}^{3}\to\mathbb{R}\), 즉 \(x\in\mathcal{X}(\Omega)\)로 모델링된다. 실제적으로 이러한 이미지들은 유한 큐보이드 \([W]\times[H]\times[D]\subset\mathbb{Z}^{3}\)에 대한 지지를 갖지만, 우리는 대신에 적절한 제로 패딩이 있는 \(\mathbb{Z}^{3}\)에 대한 함수로 보는 것을 선호한다. 우리의 대칭성으로서, 우리는 \(\mathfrak{G}=\mathbb{Z}^{3}\rtimes O_{h}\)에서 거리 및 방향 보존 변환의 그룹 \(\mathbb{Z}^{3}\)을 고려한다. 이 그룹은 세 축에 대한 \(90\)도 회전에 의해 생성된 병진 \((\mathbb{Z}^{3})\) 및 이산 회전 \(O_{h}\)로 구성된다(도 16 참조).

두 번째 예로 C, G, A 및 T의 네 글자로 구성된 DNA 서열을 고려한다. 수열은 1D 격자 \(\Omega=\mathbb{Z}\)에서 신호 \(x:\mathbb{Z}\to\mathbb{R}^{4}\)로 표현될 수 있으며, 여기서 각 문자는 \(\mathbb{R}^{4}\)에서 원-핫 코딩된다. 당연히 우리는 그리드에 이산적인 1D 번역 대칭을 가지고 있지만 DNA 서열은 추가적인 흥미로운 대칭을 가지고 있다. 이 대칭은 DNA가 물리적으로 이중 나선으로 구현되는 방식과 세포의 분자 기계에 의해 읽혀지는 방식에서 발생한다. 이중나선의 각 가닥은 \(5^{\prime}\)-말단으로 시작하고 \(3^{\prime}\)-말단으로 끝납니다. 한 가닥의 경우 \(5^{\prime}\)가 다른 가닥의 경우 \(3^{\prime}\)로 보완됩니다. 즉, 두 가닥은 반대 방향을 갖는다. DNA 분자는 항상 \(5^{\prime}\)-end에서 시작하여 판독되지만 ACCCTGG와 같은 서열은 보체인 CCAGGGT로 각 문자가 대체된 역 서열과 동일하다는 것을 알 수 없다. 이를 문자 시퀀스의 _역보완 대칭_ 이라고 합니다. 따라서 우리는 2원소 그룹 \(\mathbb{Z}_{2}=\{0,1\}\)이 아이덴티티 \(0\)와 역보완 변환 \(1\)(및 구성 \(1+1=0\mod 2\))에 해당한다. 전체 그룹은 번역과 역보완 변환을 결합한다.

우리의 경우 4.3절에서 정의한 그룹 콘볼루션(14)은 다음과 같이 주어진다.

\[(x\star\theta)(\mathfrak{g})=\sum_{u\in\Omega}x_{u}\rho(\mathfrak{g})\theta_{u}, \tag{31}\]

\(\rho(\mathfrak{g})\theta_{u}=\theta_{\mathfrak{g}^{-1}u}\)를 통해 \(\mathfrak{g}\in\mathfrak{G}\)에 의해 변환된 (단일 채널) 입력 신호 \(x\)와 필터 \(\theta\) 사이의 내부 곱과 출력 \(x\star\theta\)는 함수이다.

그림 16: A \(3\times 3\) 필터, 이산 회전 그룹 \(O_{h}\)의 모든 \(24\) 요소에 의해 회전, 세로 축에 대한 \(90\)-도 회전(빨간색 화살표) 및 대각선 축에 대한 \(120\)-도 회전에 의해 생성됩니다.

on \(\mathfrak{G}\). \(\Omega\)는 이산적이므로 우리는 식 (14)의 적분을 합으로 대체했다.

Transform+Convolve 접근법을 통해 그룹 컨볼루션이 필터 변환 단계와 변환 컨볼루션 단계의 두 단계로 구현될 수 있음을 보일 것이다. 필터 변환 단계는 기본 필터의 회전된(또는 역-보완 변환된) 복사본을 생성하는 것으로 구성되는 반면, 병진 컨볼루션은 표준 CNN에서와 동일하고 따라서 GPU와 같은 하드웨어 상에서 효율적으로 계산가능하다. 이를 보기 위해 두 예 모두에서 일반적인 변환 \(\mathfrak{g}\in\mathfrak{G}\)을 변환 \(\mathfrak{h}\in\mathfrak{H}\)(예: 회전 또는 역보체 변환)에 이어 변환 \(\mathfrak{k}\in\mathbb{Z}^{d}\), 즉 \(\mathfrak{g}=\mathfrak{h}\mathfrak{h}\)(그룹 원소의 구성을 나타내는 병치 \(\mathfrak{k}\)와 \(\mathfrak{h}\)를 쓸 수 있습니다. 그룹 표현의 특성에 따라 \(\rho(\mathfrak{g})=\rho(\mathfrak{h}\mathfrak{h})=\rho(\mathfrak{k})\rho(\mathfrak{h})\를 갖는다. 따라서,

\[\begin{split}(x\star\theta)(\mathfrak{h}\mathfrak{h})& =\sum_{u\in\Omega}x_{u}\rho(\mathfrak{k})\rho(\mathfrak{h}) \theta_{u}\\ &=\sum_{u\in\Omega}x_{u}(\rho(\mathfrak{h})\theta)_{u-\mathfrak{k}}\end{split} \tag{32}\

우리는 마지막 방정식을 신호 \(x\)와 변환된 필터 \(\rho(\mathfrak{h})\theta\의 표준(평면 유클리드) 컨볼루션으로 인식한다. 따라서, 이들 그룹에 대한 그룹 컨볼루션을 구현하기 위해, 표준 필터 \(\theta\)를 취하고, 각 \(\mathfrak{h}\in\mathfrak{H}\)에 대해 변환된 복사본 \(\theta_{\mathfrak{h}}=\rho(\mathfrak{h})\theta\)를 생성한다(예를 들어, 각 회전 \(\mathfrak{h}\in O_{h}\) 또는 역보완 DNA 대칭 \(\mathfrak{h}\in\mathbb{Z}_{2}\)) 다음, 이들 각각의 필터로 \((x\star\theta)(\mathfrak{h}\mathfrak{h})=(x\star\theta_{\mathfrak{h}})(\mathfrak{k})\). 우리의 두 예 모두에 대해 대칭은 이산 회전의 경우 그림 16과 같이 필터 계수를 단순히 순열하여 필터에 작용한다. 따라서, 이러한 연산들은 미리 계산된 인덱스들을 갖는 인덱싱 연산을 사용하여 효율적으로 구현될 수 있다.

그룹 컨벌루션으로 출력되는 특징맵을 \(\mathfrak{G}\)에 대한 함수로 정의하였으나, \(\mathfrak{g}\)를 \(\mathfrak{h}\)와 \(\mathfrak{k}\)로 분할할 수 있다는 것은 필터 변환/배향당 하나의 특징맵을 갖는 유클리드 특징맵(때로는 _배향 채널_이라고 함)의 스택으로 생각할 수도 있음을 의미한다. 예를 들어, 첫 번째 예에서는 회전된 필터를 컨볼빙(전통적인 병진 의미에서)하여 얻은 특징 맵을 각 필터 회전(그림 16의 각 노드)에 연관시킨다. 따라서 이러한 특징 맵은 여전히 \(W\times H\times C\) 배열로 저장될 수 있으며, 여기서 채널 수 \(C\)는 변환 수 \(\mathfrak{h}\in\mathfrak{H}\)에 대한 독립 필터 수와 같다.

섹션 4.3에 도시된 바와 같이, 그룹 컨볼루션은 등분산이다: \((\rho(\mathfrak{g})x)\star\theta=\rho(\mathfrak{g})(x\star\theta)\). 이것이 방향 채널 측면에서 의미하는 바는 \(\mathfrak{h}\)의 작용 하에서 각 방향 채널이 변환되고 방향 채널 자체가 순열된다는 것이다. 예를 들어, 그림 16에서 변환당 하나의 방향 채널을 연관시키고 z축(빨간색 화살표에 해당)에 대해 \(90\)도만큼 회전을 적용하면 특징 맵은 빨간색 화살표로 표시된 대로 순열된다. 이러한 설명은 그룹 컨볼루션 신경망이 전통적인 CNN과 많은 유사성을 갖는다는 것을 분명히 한다. 따라서 잔차 네트워크와 같은 섹션 5.1에서 논의된 많은 네트워크 설계 패턴은 그룹 컨벌루션에도 사용할 수 있다.

푸리에 영역의 구형 CNN 4.3절에서 본 구의 연속 대칭군에 대해 적절한 푸리에 변환을 사용하여 스펙트럼 영역에서 컨벌루션을 구현할 수 있다(우리는 판독기에게 \(\mathbb{S}^{2}\) 상의 컨벌루션이 \(\mathrm{SO}(3)\) 상의 함수임을 상기시킨다). 따라서 다층 구형 CNN을 구현하기 위해서는 이 두 도메인에서 푸리에 변환을 정의해야 한다. _ 구면 고조파_는 복소 지수의 고전적인 푸리에 기저와 유사한 2D 구면의 직교 기저이다. 특수 직교 그룹에서 푸리에 기저는 _위그너 D-함수_로 알려져 있다. 두 경우 모두 푸리에 변환(계수)은 기저함수와 내적 곱으로 계산되며, 콘볼루션 정리의 유추는 푸리에 변환의 요소별 곱으로 푸리에 도메인에서 콘볼루션을 계산할 수 있다. 또한 \(\mathbb{S}^{2}\)와 \(\mathrm{SO}(3)\)에 대한 푸리에 변환의 효율적인 계산을 위해 FFT 유사 알고리즘이 존재한다. 자세한 내용은 Cohen 등(2018)을 참조한다.

### 5.3 그래프 신경망

그래프 신경망(GNN)은 순열 그룹의 특성을 활용하는 그래프에 대한 기하학적 딥 러닝 청사진을 실현하는 것이다. GNN은 현재 존재하는 딥 러닝 아키텍처의 가장 일반적인 클래스 중 하나이며, 이 텍스트에서 볼 수 있듯이, 대부분의 다른 딥 러닝 아키텍처는 추가적인 기하학적 구조를 갖는 GNN의 특수한 경우로 이해될 수 있다.

섹션 4.1의 논의에 따라, 우리는 인접 행렬 \(\mathbf{A}\) 및 노드 특징 \(\mathbf{X}\)로 지정되는 그래프를 고려한다. 본 논문에서는 지역 이웃에 공유된 순열 불변 함수 \(\phi(\mathbf{x}_{u},\mathbf{X}_{\mathcal{N}_{u})\)를 적용하여 구성된 순열 등변 함수 \(\mathbf{F}(\mathbf{X},\mathbf{A})\)인 GNN 구조를 연구한다. 다양한 가명 하에서, 이 로컬 함수 \(\phi\)는 "확산", "전파", 또는 "메시지 통과"로 지칭될 수 있고, 이러한 \(\mathbf{F}\)의 전체 계산은 "GNN 층"으로 지칭될 수 있다.

GNN 레이어의 설계 및 연구는 글쓰기 당시 딥러닝의 가장 활발한 영역 중 하나로 탐색하기 어려운 풍경이다. 다행히도, 우리는 대다수의 문헌이 여기서 제시할 GNN 층(그림 17)의 세 가지 "맛"에서만 파생될 수 있음을 발견한다. 이러한 풍미는 \(\phi\)가 이웃 특징을 변환하는 정도를 제어하여 그래프 전반에 걸친 상호작용을 모델링할 때 다양한 정도의 복잡성을 허용한다.

세 가지 향미 모두에서, 순열 불변성은 \(\mathbf{X}_{\mathcal{N}_{u}}\)(일부 함수에 의해 잠재적으로 변환됨 \(\psi\))에서 일부 순열 불변 함수 \(\bigoplus\)로 _aggregating_한 다음 노드 \(u\)의 특성을 일부 함수 \(\phi\)에 의해 _updating_함으로써 보장된다. 일반적으로 \(\psi\)와 \(\phi\)는 학습할 수 있는 반면, \(\bigoplus\)는 합, 평균 또는 최대와 같은 비모수 연산으로 실현되지만 순환 신경망을 사용하여 구성할 수도 있다(Murphy et al., 2018).

그림 17: GNN 층의 세 가지 맛에 대한 데이터 흐름의 시각화 \(g\). 이를 설명하기 위해 그림 10의 노드 \(b\)의 이웃을 이용한다. 좌우: **convolutional** 에서 송신 노드 기능이 상수, \(c_{uv}\); **attentional** 에서 이 승수는 송신자에 대 한 수신자의 주의 메커니즘을 통해 _암묵적으로_ 계산 됩니다. \(\alpha_{uv}=a(\mathbf{x}_{u},\mathbf{x}_{v})\); 및 **message-passing** 에서 벡터 기반 메시지가 송신자와 수신자 모두에 따라 계산 됩니다. \(\mathbf{m}_{uv}=\psi(\mathbf{x}_{u},\mathbf{x}_{v})\).

**컨볼루션** 풍미(Kipf and Welling, 2016; Defferrard et al., 2016; Wu et al., 2019)에서, 이웃 노드의 특징은 고정된 가중치로 직접 집계되고,

\[\mathbf{h}_{u}=\phi\left(\mathbf{x}_{u},\bigoplus_{v\in\mathcal{N}_{u}}c_{uv} \psi(\mathbf{x}_{v})\right). \tag{33}\

여기서, \(c_{uv}\)는 노드 \(v\)에서 노드 \(u\)의 표현에 대한 _중요도_를 지정합니다. 그것은 종종 그래프의 구조를 나타내는 \(\mathbf{A}\)의 엔트리에 직접적으로 의존하는 상수이다. 집계 연산자 \(\bigoplus\)가 합산으로 선택될 때, 이는 컨볼루션의 일반화인 선형 확산 또는 위치 의존적 선형 필터링으로 간주될 수 있다는 점에 유의한다. 특히, 섹션 4.4 및 섹션 4.6에서 본 스펙트럼 필터는 노드별 신호에 고정된 로컬 연산자(예: 라플라시안 행렬)를 적용하는 것에 해당하기 때문에 이 범주에 속한다.

**주의** 풍미(Velickovic et al., 2018; Monti et al., 2017; Zhang et al., 2018)에서 상호 작용은 암시적입니다.

\[\mathbf{h}_{u}=\phi\left(\mathbf{x}_{u},\bigoplus_{v\in\mathcal{N}_{u}}a( \mathbf{x}_{u},\mathbf{x}_{v})\psi(\mathbf{x}_{v})\right). \tag{34}\

여기서, \(a\)는 중요도 계수 \(\alpha_{uv}=a(\mathbf{x}_{u},\mathbf{x}_{v})\)를 암묵적으로 계산하는 학습 가능한 _자기 주의 메커니즘_이다. 그들은 종종 모든 이웃에 걸쳐 소프트맥스 정규화된다. \(\bigoplus\)가 합일 때, 집합은 여전히 이웃 노드 특징의 선형 조합이지만, 이제 가중치는 특징 의존적이다.

마지막으로, **메시지 전달** 향미(Gilmer et al., 2017; Battaglia et al., 2018)는 에지에 걸쳐 임의의 벡터("메시지")를 계산하는 것에 해당하고,

\[\mathbf{h}_{u}=\phi\left(\mathbf{x}_{u},\bigoplus_{v\in\mathcal{N}_{u}}\psi( \mathbf{x}_{u},\mathbf{x}_{v})\right). \tag{35}\

여기서, \(\psi\)는 학습 가능한 _메시지 함수_이며, \(u\)로 전송된 \(v\)의 벡터를 계산하며, 집계는 그래프 상에 전달되는 메시지의 형태로 간주될 수 있다.

한 가지 중요한 점은 이러한 접근법 간의 표현적 봉쇄입니다. _convolution \(\subseteq\) attention \(\subseteq\) message-passing_. 실제로, 어텐션 GNN은 룩업 테이블로 구현된 어텐션 메커니즘에 의해 컨볼루션 GNN을 나타낼 수 있다 \(a(\mathbf{x}_{u},\mathbf{x}_{v})=c_{uv}\)이고, 컨볼루션 GNN과 어텐션 GNN은 모두 메시지 전달의 특수한 경우로서, 메시지가 송신자 노드의 특징일 뿐이다. 컨볼루션 GNN의 경우 \(\psi(\mathbf{x}_{u},\mathbf{x}_{v})=c_{uv}\psi(\mathbf{x}_{v})\)이고 어텐션 GNN의 경우 \(\psi(\mathbf{x}_{u},\mathbf{x}_{v})=a(\mathbf{x}_{u},\mathbf{x}_{v})\psi(\mathbf{x}_{v})\).

이것은 GNN을 전달하는 메시지가 항상 가장 유용한 변형이라는 것을 의미하지 않으며, 에지들을 가로질러 벡터 값 메시지들을 계산해야 하기 때문에, 이들은 전형적으로 트레이닝하기가 더 어렵고 다루기 힘든 양의 메모리를 필요로 한다. 또한, 광범위한 자연 발생 그래프에서, 그래프의 에지는 다운스트림 클래스 유사성(즉, 에지 \((u,v)\)에 대해 인코딩된다는 것은 \(u\)와 \(v\)가 동일한 출력을 가질 가능성이 있음을 의미한다. 이러한 그래프(종종 _호모필루스_라고 함)의 경우, 이웃 간의 컨벌루션 집계는 규칙화 및 확장성 측면에서 훨씬 더 나은 선택인 경우가 많습니다. 주목하는 GNN은 "중간 범위"를 제공하므로 이웃 내에서 복잡한 상호 작용을 모델링할 수 있으며 가장자리를 가로질러 스칼라 값 양만 계산하므로 메시지 전달보다 확장성이 높다.

여기에 제시된 "3가지 맛" 분류는 간결함을 염두에 두고 제공되며 불가피하게 GNN 모델에 대한 풍부한 뉘앙스, 통찰력, 일반화 및 역사적 맥락을 무시한다. 중요한 것은 Weisfeiler-Lehman 계층에 기반한 고차원 GNN과 그래프 푸리에 변환의 명시적 계산에 의존하는 스펙트럼 GNN을 제외한다는 것이다.

### 심층 집합, 변환기 및 잠재 그래프 추론

순열되지 않은 집합의 표현을 학습하기 위한 순열-등분 신경망 아키텍처에 대해 언급함으로써 GNN에 대한 논의를 마무리한다. 이 텍스트에서 논의한 도메인 중 집합은 구조가 가장 적지만 최근 트랜스포머(Vaswani et al., 2017) 및 딥셋(Zaheer et al., 2017)과 같은 인기 있는 아키텍처에 의해 중요성이 강조되었다. 섹션 4.1의 언어에서 우리는 노드 특징의 행렬인 \(\mathbf{X}\)이 주어지지만 노드 사이의 지정된 인접성 또는 순서 정보가 없다고 가정한다. 특정 아키텍처는 노드 간의 _상호작용_ 을 모델링 하는 정도를 결정 하 여 발생 합니다.

빈 에지 집합 무질서 집합은 추가 구조나 기하학 없이 제공되며, 따라서 이들을 처리하는 가장 자연스러운 방법은 각 집합 요소를 완전히 독립적으로 처리하는 것이라고 주장할 수 있다. 이것은 이러한 입력에 대한 순열 등분산 함수로 변환되며, 이는 섹션 4.1에서 이미 소개되었으며, 공유 변환은 격리된 모든 노드에 적용되었다. GNN을 설명할 때와 동일한 표기를 가정하면(섹션 5.3), 이러한 모델은 다음과 같이 나타낼 수 있다.

\[\mathbf{h}_{u}=\psi(\mathbf{x}_{u}),\]

여기서 \(\psi\)는 학습 가능한 변환입니다. 이것은 \(\mathcal{N}_{u}=\{u\}\)--또는 등가적으로 \(\mathbf{A}=\mathbf{I}\)를 갖는 컨볼루션 GNN의 특수한 경우임을 관찰할 수 있다. 이러한 아키텍처는 이러한 아키텍처의 여러 범용 근사 특성을 이론적으로 입증한 Zaheer 등(2017)의 작업을 인식하여 일반적으로 Deep Sets라고 한다. 정렬되지 않은 세트를 처리할 필요성은 _포인트 클라우드_ 를 처리할 때 컴퓨터 비전 및 그래픽에서 일반적으로 발생한다는 점에 유의해야 하며, 그 안에서 이러한 모델은 PointNets(Qi 등, 2017)로 알려져 있다.

완전한 에지 집합 빈 에지 집합이 정렬되지 않은 집합 위에 함수를 구축하는 데 매우 효율적인 구성이라고 가정하지만, 종종 집합의 요소가 어떤 형태의 관계 구조를 나타내는 것, 즉 노드 사이에 _잠재 그래프_가 존재한다고 예상할 수 있다. 설정 \(\mathbf{A}=\mathbf{I}\)은 그러한 구조를 폐기하고, 차선책의 성능을 산출할 수 있다. 반대로, 다른 사전 지식이 없는 경우 노드 간의 가능한 링크를 미리 배제할 수 없다고 가정할 수 있다. 이 방법에서는 _완전 그래프, \(\mathbf{A}=\mathbf{1}\mathbf{1}^{\top}\); 등가적으로 \(\mathcal{N}_{u}=\mathcal{V}\)를 가정한다. 상호 작용 계수에 대한 액세스를 가정하지 않으므로 이러한 그래프를 통해 _컨볼루션_ 유형 GNN을 실행하는 것은 다음과 같습니다.

\[\mathbf{h}_{u}=\phi\left(\mathbf{x}_{u},\bigoplus_{v\in\mathcal{V}}\psi( \mathbf{x}_{v})\right),\]

여기서 두 번째 입력인 \(\bigoplus_{v\in\mathcal{V}}\psi(\mathbf{x}_{v})\)는 모든 노드 \(u\)에 대해 _동일_하며, 따라서 모델은 그 입력을 완전히 무시하는 것, 즉 위에서 언급한 \(\mathbf{A}=\mathbf{I}\) 경우를 등가적으로 표현한다.

이것은 더 표현력 있는 GNN 맛인 주의력을 사용하도록 동기를 부여하며,

\[\mathbf{h}_{u}=\phi\left(\mathbf{x}_{u},\bigoplus_{v\in\mathcal{V}}a(\mathbf{ x}_{u},\mathbf{x}_{v})\psi(\mathbf{x}_{v})\right) \tag{36}\

이는 트랜스포머 아키텍처의 핵심인 _self-attention_ 연산자를 산출한다(Vaswani et al., 2017). 어텐션 계수(예를 들어, softmax)에 대한 정규화를 가정하면, 모든 스칼라 \(a(\mathbf{x}_{u},\mathbf{x}_{v})\)가 범위 \([0,1]\)에 있도록 제한할 수 있다. 따라서, 자기 주의는 일부 다운스트림 태스크에 대한 기울기 기반 최적화의 부산물로 _소프트 인접 행렬_, \(a_{uv}=a(\mathbf{x}_{u},\mathbf{x}_{v})\)를 추론하는 것으로 생각할 수 있다.

메시지 전달 맛을 적용하는 것도 적절합니다.

물리 시뮬레이션 및 관계 추론(예를 들어, Battaglia et al. (2016); Santoro et al. (2017))에 인기 있는 반면, 이들은 트랜스포머만큼 널리 사용되지 않았다. 이는 완전한 그래프를 통해 벡터 메시지를 컴퓨팅하는 것과 관련된 메모리 문제 또는 벡터 기반 메시지가 자기 주의에 의해 제공되는 "소프트 인접성"보다 덜 해석 가능하다는 사실 때문일 수 있다.

노드의 자연스러운 순서가 존재하지 않는 그래프에서 이러한 위치 인코딩에 대한 여러 대안이 제시되었다. 나중에 이러한 대안을 논의하는 것을 연기하지만, 우리는 트랜스포머에서 사용되는 위치 인코딩이 이산 푸리에 변환(DFT)과 직접 관련될 수 있고 따라서 "원형 그리드"의 그래프 라플라시안 고유 벡터와 관련될 수 있다는 실현을 포함하는 한 가지 유망한 방향에 주목한다. 따라서 트랜스포머의 위치 인코딩은 입력 노드가 격자로 연결되어 있다는 가정을 암시적으로 나타낸다. 보다 일반적인 그래프 구조의 경우, 경험적으로 강력한 그래프 트랜스포머 모델 내에서 드비베디와 브레슨(2020)에 의해 이용된 관측치인 (가정된) 그래프의 라플라시안 고유 벡터를 간단히 사용할 수 있다.

마지막으로 추론된 에지 집합은 잠재 관계 구조를 학습하여 \(\mathbf{I}\)도 아니고 \(\mathbf{1}\mathbf{1}^{\top}\)도 아닌 일반적인 \(\mathbf{A}\)로 이어질 수 있다. GNN이 사용할 잠재 인접 행렬 \(\mathbf{A}\)을 추론하는 문제(흔히 _잠재 그래프 추론_이라고 함)는 그래프 표현 학습에 대한 관심이 높다. 이는 \(\mathbf{A}=\mathbf{I}\)을 가정하는 것이 표현적으로 열등할 수 있고, \(\mathbf{A}=\mathbf{1}\mathbf{1}^{\top}\)을 가정하는 것은 메모리 요구 사항과 큰 이웃으로 인해 구현하기 어려울 수 있기 때문이다. 또한, "참" 문제에 가장 가깝다: 인접 행렬 \(\mathbf{A}\)은 \(\mathbf{X}\)의 행 사이의 유용한 구조를 탐지하는 것을 의미하며, 이는 변수 간의 인과 관계와 같은 가설을 공식화하는 데 도움이 될 수 있다.

불행히도, 그러한 프레이밍은 모델링 복잡성에서 반드시 단계를 유도한다. 특히, 그래프가 사용되는 모든 다운스트림 태스크와 구조 학습 목표(이산되어 기울기 기반 최적화에 어려움이 있음)의 균형을 적절하게 유지해야 합니다. 이것은 잠재 그래프 추론을 매우 도전적이고 복잡한 문제로 만든다.

### 5.5 Equivariant Message Passing Networks

그래프 신경망의 많은 응용에서 노드 특징(또는 그 부분)은 단순한 임의의 벡터가 아니라 기하학적 개체의 _좌표_이다. 이것은 예를 들어 분자 그래프를 다룰 때이다: 원자를 나타내는 노드는 원자 타입뿐만 아니라 그것의 3D 공간 좌표에 대한 정보를 포함할 수 있다. 분자가 공간상에서 변환되는 것과 동일한 방식으로, 다시 말해, 이전에 논의된 표준 순열 등분 외에 강체 운동(회전, 병진 및 반사)의 유클리드 그룹 \(\mathrm{E}(3)\)과 등분하는 방식으로 특징의 후부를 처리하는 것이 바람직하다.

이를 위해 노드 _features_\(\mathbf{f}_{u}\in\mathbb{R}^{d}\)와 노드 _공간좌표_\(\mathbf{x}_{u}\in\mathbb{R}^{3}\)를 구분한다. 이 설정에서 등분산 계층은 이 두 입력을 개별적으로 명시적으로 변환하여 수정된 노드 특징 \(\mathbf{f}_{u}^{\prime}\) 및 좌표 \(\mathbf{x}_{u}^{\prime}\)을 생성한다.

이제 기하학적 딥러닝 청사진을 따라 바람직한 등분산 특성을 설명할 수 있습니다. 입력의 공간 성분을 \(\mathfrak{g}\in\mathrm{E}(3)\) ( \(\rho(\mathfrak{g})\mathbf{x}=\mathbf{R}\mathbf{x}+\mathbf{b}\)로 나타내면, 여기서 \(\mathbf{R}\)는 직교 행렬 모델링 회전 및 반사이고, \(\mathbf{b}\)는 변환 벡터인 경우, 출력의 공간 성분은 동일한 방식으로 변환된다(\(\mathbf{x}_{u}^{\prime}\mapsto\mathbf{R}\mathbf{x}_{u}^{\prime}+\mathbf{b}\)). 반면, \(\mathbf{f}_{u}^{\prime}\)는 불변으로 유지된다.

일반 그래프의 맥락에서 이전에 논의한 순열 등분산 함수의 공간과 마찬가지로 위의 제약 조건을 충족할 수 있는 방대한 양의 \(\mathrm{E}(3)\)-등분산 계층이 존재하지만 이 모든 계층이 기하학적으로 안정적이거나 구현이 쉬운 것은 아니다. 사실, 실질적으로 유용한 등분산 층의 공간은 공간 GNN 층의 "세 가지 맛"과 달리 간단한 분류로 쉽게 설명될 수 있다. Satorras 등(2021)은 한 가지 우아한 솔루션을 _equivariant message passing_ 형식으로 제안했다. 그들의 모델은 다음과 같이 작동합니다.

\[\mathbf{f}^{\prime}_{u} = \phi\left(\mathbf{f}_{u},\mathbf{f}_{v},\|\mathbf{x}_{u}-\mathbff{x}_{v}\|^{2 })\right),\] \[\mathbf{x}^{\prime}_{u} = \mathbf{x}_{u}+\sum_{v\neq u}(\mathbf{x}_{u}-\mathbf{x}_{v})\psi_{\mathrm{c}}(\mathbf{f}_{u},\mathbf{f}_{v},\|\mathbf{x}_{u}-\mathbf{x}_{v}\|^{2})\]

여기서 \(\psi_{\mathrm{f}}\)와 \(\psi_{\mathrm{c}}\)는 두 개의 구별되는(학습 가능한) 함수이다. 이러한 집합은 공간 좌표의 유클리드 변환 하에서 등분산임을 보여줄 수 있다. 이것은 \(\mathbf{f}^{\prime}_{u}\)에 대한 \(\mathbf{x}_{u}\)의 유일한 의존성은 \(\|\mathbf{x}_{u}-\mathbf{x}_{v}\|^{2}\)의 거리를 통해서이고, \(\mathrm{E}(3)\)의 작용은 노드 사이의 거리를 반드시 변화시키지 않기 때문이다. 또한, 이러한 계층의 계산은 "메시지 전달" GNN 풍미의 특정 사례로 볼 수 있으므로 구현하기에 효율적이다.

요약하자면, 일반적인 GNN과 대조적으로, Satorras et al.(2021)은 그래프의 각 점에 대한 '좌표'의 올바른 처리를 가능하게 한다. 그들은 이제 \(\mathrm{E}(3)\) 그룹의 멤버로 취급되며, 이는 네트워크의 출력이 입력의 회전, 반사 및 변환 하에서 올바르게 동작한다는 것을 의미한다. 그러나 \(\mathbf{f}_{u}\)는 채널 단위로 처리되며 이러한 변환에서는 변하지 않는 _scalars_ 로 간주됩니다. 이는 그러한 프레임워크 내에서 캡처될 수 있는 공간 정보의 유형을 제한한다. 예를 들어, 일부 피처들은 그러한 변환들 하에서 방향을 변경해야 하는 _벡터_ - 예를 들어, 포인트 속도들 - 로 인코딩되는 것이 바람직할 수도 있다. Satorras et al.(2021)은 이들 아키텍처의 한 변형에서 속도 개념을 도입함으로써 이 문제를 부분적으로 완화한다. Velocities는 적절하게 회전하는 각 점의 3차원 벡터 특성이다. 그러나 이것은 \(\mathrm{E}(3)\) 등분산 네트워크로 학습될 수 있는 일반적인 표현들의 작은 부분 공간일 뿐이다. 일반적으로 노드 피쳐는 \(\mathrm{E}(3)\)에 따라 여전히 변환되는 임의 차원의 _텐서_를 잘 정의된 방식으로 인코딩할 수 있습니다.

따라서 위에서 논의한 아키텍처는 이미 많은 실제 입력 표현에 대해 우아한 등분산 솔루션을 제시하지만, 일부 경우에는 등분산 특성을 충족하는 함수의 광범위한 모음을 탐색하는 것이 바람직할 수 있다. 이러한 설정을 다루는 기존 방법은 두 가지 클래스로 분류할 수 있습니다. _환원 가능한 표현_ (이 중 이전에 언급한 계층은 단순화된 인스턴스임) 및 _정규 표현_입니다. 우리는 여기서 그들을 간략하게 조사하며 자세한 논의는 향후 작업에 맡긴다.

[MISSING_PAGE_EMPTY:89]

추론하는 기하학. 섹션 4.6에서 논의된 바와 같이, _메쉬_는 연속적인 표면의 이산화로 이해될 수 있는 기하학적 그래프의 특별한 예이다. 우리는 다음으로 메쉬별 등분산 신경망을 연구할 것이다.

### 내부 Mesh CNN

특히 삼각형 메쉬는 컴퓨터 그래픽의 '빵과 버터'이며 아마도 3D 객체를 모델링하는 가장 일반적인 방법이다. 일반적으로 딥러닝과 특히 컴퓨터 비전에서 CNN의 놀라운 성공은 메쉬 데이터에 대한 유사한 아키텍처를 구성하기 위해 2010년대 중반 전후 그래픽 및 기하 처리 커뮤니티에 큰 관심을 불러일으켰다.

지오데식 패치 메쉬에 대한 딥 러닝을 위한 대부분의 아키텍처는 지수 맵을 이산화하거나 근사화하고 접선 평면의 좌표계에 필터를 표현함으로써 형태(21)의 컨볼루션 필터를 구현한다. 측지선 \(\gamma:[0,T]\to\Omega\)을 한 점 \(u=\gamma(0)\)에서 가까운 점 \(v=\gamma(T)\)까지 쏘는 것은 _geodesic 극좌표_\((r(u,v),\vartheta(u,v))\)의 국소계를 정의하는데, 여기서 \(r\)은 \(u\)와 \(v\) 사이의 측지선 거리(측지선 \(\gamma\)의 길이)이고 \(\vartheta\)는 \(\gamma^{\prime}(0)\)와 일부 국소 기준 방향 사이의 각도이다. 이를 통해 _geodesic patch_\(x(u,r,\vartheta)=x(\exp_{u}\tilde{\omega}(r,\vartheta))\)를 정의할 수 있으며, 여기서 \(\tilde{\omega}_{u}:[0,R]\times[0,2\pi)\to T_{u}\Omega\)는 국소 극성 프레임이다.

메쉬로 이산화 된 표면에서 측지선은 삼각형 면을 가로지르는 폴리선입니다. 전통적으로 측지학은 매질에서의 파동 전파의 물리적 모델에서 마주치는 _eikonal 방정식_이라고 하는 비선형 PDE의 효율적인 수치 근사치인 Fast Marching 알고리즘 Kimmel and Sethian(1998)을 사용하여 계산되었다. 이 기법은 Kokkinos et al. (2012)에 의해 로컬 지오데식 패치의 계산을 위해 적응되었고 나중에 Masci et al. (2015)에 의해 메쉬 상의 최초의 고유 CNN-유사 아키텍처인 _Geodesic CNNs_의 구축을 위해 재사용되었다.

등방성 필터는 중요한데, 측지 패치의 정의에서 참조 방향과 패치 방향의 선택에 모호성이 있다. 이것은 정확히 게이지 선택의 모호성이며, 우리의 국소 좌표계는 임의의 회전(또는 각도 좌표의 이동, \(x(u,r,\vartheta+\vartheta_{0})\))까지 정의되며, 이는 모든 노드에서 다를 수 있다. Perhapsthe 가장 간단한 해결책은 이웃 특징들의 방향 독립적인 집계를 수행하는 형태 \(\theta(r)\)의 등방성 필터들을 사용하는 것이고,

\[(x\star\theta)(u)=\int_{0}^{R}\int_{0}^{2\pi}x(u,r,\vartheta)\theta(r)\mathrm{d}r \mathrm{d}\vartheta.\]

섹션 4.4-4.6에서 논의된 스펙트럼 필터는 이 범주에 속하며, 이는 등방성인 라플라시안 연산자를 기반으로 한다. 그러나, 이러한 방법은 중요한 방향성 정보를 폐기하고, 에지-유사 특징을 추출하는데 실패할 수 있다.

고정 게이지 4.4절에서 이미 언급한 대안은 일부 게이지를 _고정하는 것입니다. 몬티 등(2017)은 주요 곡률 방향을 사용했는데, 이러한 선택은 고유하지 않고 평평한 지점(곡률이 사라지는 곳) 또는 균일한 곡률(완전한 구와 같은)에서 모호할 수 있지만, 저자는 변형 가능한 인체 형상을 다루는 것이 대략 단편적으로 엄격한 것으로 합리적인 것으로 나타났다. 이후 연구인 Melzi et al. (2019)은 고유 함수의 (고유) 구배로 계산된 메쉬 상의 게이지의 신뢰할 수 있는 고유 구성을 보여주었다. 이러한 접선 필드는 특이점을 가질 수 있지만(즉, 일부 지점에서 사라짐), 전체 절차는 노이즈 및 리메싱에 매우 강력하다.

각 풀링 _angular max pooling_ 이라고 하는 또 다른 접근법은 Masci 등 (2015)에 의해 사용되었다. 이 경우 필터 \(\theta(r,\vartheta)\)는 이방성이지만 함수와의 매칭은 _모든 가능한 회전들_ 에 걸쳐 수행되며, 그 다음 집합됩니다:

\[(x\star\theta)(u)=\max_{\vartheta_{0}\in[0,2\pi)}\ \int_{0}^{R}\int_{0}^{2\pi}x(u,r, \vartheta)\theta(r,\vartheta+\vartheta_{0})\mathrm{d}r\mathrm{d}\vartheta.\]

개념적으로 이것은 측지선 패치를 회전 필터와 상관시키고 가장 강한 응답을 수집하는 것으로 시각화할 수 있다.

메시에서 연속 적분은 _패치 연산자_라고 하는 구성을 사용하여 이산화할 수 있습니다(Masci 등, 2015). 노드 \(u\) 주변의 측지 패치에서 이웃 노드 \(\mathcal{N}_{u}\), 국소 극좌표로 표시되는 \((r_{uv},\vartheta_{uv})\), 가중 함수 \(w_{1}(r,\vartheta),\ldots,w_{K}(r,\vartheta)\)(도 18에 도시되고 '소프트 픽셀'로 작용함)에 의해 가중되고 집계되며,

\[(x\star\theta)_{u}=\frac{\sum_{k=1}^{K}w_{k}\sum_{v\in\mathcal{N}_{u}}(r_{uv}, \vartheta_{uv})x_{v}\,\theta_{k}}{\sum_{k=1}^{K}w_{k}\sum_{v\in\mathcal{N}_{u}}(r_{uv},\vartheta_{uv})\theta_{k}}}](여기서 \(\theta_{1},\ldots,\theta_{K}\)는 필터의 학습 가능한 계수이다. 다중 채널 기능은 적절한 필터 패밀리로 채널별로 처리됩니다. Masci et al. (2015); Boscaini et al. (2016a)는 미리 정의된 가중치 함수 \(w\)를 사용했지만 Monti et al. (2017)은 추가로 학습할 수 있도록 허용했다.

게이지-등방성 필터와 각도 최대 풀링은 모두 게이지 변환을 위해 _불변_인 특징으로 이어지며, 이는 사소한 표현에 따라 변환된다 \(\rho(\mathfrak{g})=1\)(여기서 \(\mathfrak{g}\in\mathrm{SO}(2)\)는 로컬 좌표 프레임의 회전이다. 이 관점은 Cohen et al. (2019); de Haan et al. (2020)에 의해 제안되고 섹션 4.5에서 논의되는 또 다른 접근법을 제안하며, 여기서 네트워크에 의해 계산된 특징들은 구조 그룹의 임의의 표현 \(\rho\) \(\mathfrak{G}\)(예: \(\mathrm{SO}(2)\) 또는 \(\mathrm{O}(2)\)의 회전 또는 회전+좌표 프레임의 반사와 각각 연관된다. 접선 벡터는 표준 표현 \(\rho(\mathfrak{g})=\mathfrak{g}\)에 따라 변환된다. 다른 예로서, 동일한 필터의 \(n\)개의 회전된 복사본들을 게이지의 회전들 하에서 순환 시프트들에 의해 매칭시킴으로써 획득된 특징 벡터는 변환된다; 이것은 순환 그룹 \(C_{n}\)의 정규 표현으로 알려져 있다.

섹션 4.5에서 논의된 바와 같이, 그러한 기하학적 특징들을 다룰 때(비-사소한 표현과 연관될 때), 우리는 먼저 필터를 적용하기 전에 그것들을 동일한 벡터 공간으로 병렬 수송해야 한다. 메쉬 상에서, 이것은 de Haan 등(2020)에 의해 설명된 다음의 메시지 전달 메커니즘을 통해 구현될 수 있다. \(\mathbf{x}_{u}\in\mathbb{R}^{d}\)를 메쉬 노드 \(u\)에서 \(d\)-차원 입력 피쳐라고 하자. 이 특징은 \(u\에서 게이지의 (임의) 선택에 대해 표현되며, 표현 \(\rho_{\text{in}}}\)에 따라 변환되는 것으로 가정된다.

도 18: 좌-우: Geodesic CNN(Masci et al., 2015), Anisotropic CNN(Boscaini et al., 2016b) 및 MoNet(Monti et al., 2017)에서 사용되는 패치 연산자의 예 \(w_{k}(r,\vartheta)\)의 레벨 세트가 빨간색으로 표시된다.

\(\mathfrak{G}=\mathrm{SO}(2)\)는 게이지의 회전 하에 있다. 마찬가지로, 메쉬 컨벌루션의 출력 특징 \(\mathbf{h}_{u}\)은 \(d^{\prime}\) 차원이며, \(\rho_{\mathrm{out}}\)에 따라 변환해야 한다(네트워크 설계자가 임의로 선택할 수 있음).

그래프 신경망에 유추하여, 우리는 \(u\)의 이웃 \(\mathcal{N}_{u}\)으로부터 \(u\)의 (및 \(u\) 자체로부터 \(u\)로 메시지를 전송함으로써 메쉬 상에서 게이지-등가 컨볼루션(23)을 구현할 수 있다:

\[\mathbf{h}_{u}=\mathbf{\theta}_{\text{self}}\,\mathbf{x}_{u}+\sum_{v\in \mathcal{N}_{u}}\mathbf{\theta}_{\text{neigh}}(\vartheta_{uv})\rho(\mathfrak{ g}_{v\to u})\mathbf{x}_{v}, \tag{37}\

여기서 \(\mathbf{\theta}_{\text{self}},\mathbf{\theta}_{\text{neigh}}(\vartheta_{uv}) \in\mathbb{R}^{d^{\prime}\times d}\)는 학습된 필터 행렬이다. 구조 그룹 요소 \(\mathfrak{g}_{v\to u}\in\mathrm{SO}(2)\)는 \(v\)에서 \(u\)로의 병렬 전송 효과를 나타내며, \(u\) 및 \(v\)에서의 게이지에 대해 표현되며, 각 메시에 대해 사전 계산될 수 있다. 이 동작은 _transporter matrix_\(\rho(\mathfrak{g}_{v\to u})\in\mathbb{R}^{d\times d}\)에 의해 인코딩된다. 행렬 \(\mathbf{\theta}_{\text{neigh}}(\vartheta_{uv})\)은 이웃의 각도 \(\vartheta_{uv}\)에서 기준 방향(예: 프레임의 첫 번째 축)에 의존하므로 이 커널은 이방성이다: 서로 다른 이웃은 다르게 처리된다.

4.5절에서 설명한 바와 같이, \(\mathbf{h}(u)\)가 잘 정의된 기하학적 양이 되기 위해서는 게이지 변환 하에서 \(\mathbf{h}(u)\mapsto\rho_{\mathrm{out}}(\mathfrak{g}^{-1}(u))\mathbf{h}(u)\)로 변환해야 한다. 이는 모든 \(\vartheta\in\mathrm{SO}(2)\)와 \(\mathbf{\theta}_{\text{neigh}}(\vartheta_{uv}-\vartheta)\rho_{\text{in}}( \vartheta)=\rho_{\mathrm{out}}(\vartheta)\mathbf{\theta}_{\text{neigh}}( \vartheta_{uv})\rho_{\text{in}}( \vartheta)=\rho_{\mathrm{out}}(\vartheta)\mathbf{\theta}_{\text{neigh}}( \vartheta_{uv})\의 경우이다. 이러한 제약조건은 선형이므로 행렬의 공간 \(\mathbf{\theta}_{\text{self}}\)과 행렬값 함수 \(\mathbf{\theta}_{\text{neigh}}\)은 선형 부분공간이므로 학습 가능한 계수를 갖는 기저 커널의 선형 조합으로 매개변수화할 수 있다 : \(\mathbf{\theta}_{\text{self}}=\sum_{i}\alpha_{i}\mathbf{\theta}_{\text{self}}^{i}\)와 \(\mathbf{\theta}_{\text{neigh}}}=\sum_{i}\beta_{i}\mathbf{\theta}_{\text{neigh}}^{i}\).

### 5.7 순환 신경망

우리의 논의는 지금까지 항상 입력이 주어진 도메인에 걸쳐 전적으로 _공간_이라고 가정했다. 그러나, 많은 일반적인 사용 사례들에서, 입력들은 또한 _순차적인_ 것으로 간주될 수 있다(예를 들어, 비디오, 텍스트 또는 스피치). 이 경우, 우리는 입력이 임의로 많은 _단계_로 구성되어 있다고 가정하며, 여기서 각 단계 \(t\)에서 입력 신호가 제공되며, 이를 \(\mathbf{X}^{(t)}\in\mathcal{X}(\Omega^{(t)})\로 표현한다.

일반적으로 도메인은 그 위에 있는 신호와 함께 시간에 따라 진화할 수 있지만, 일반적으로 도메인은 모든 \(t\), 즉 \(\Omega^{(t)}=\Omega\에 걸쳐 고정된 상태로 유지된다고 가정한다. 여기서는 이 경우를 배타적으로 중심으로 살펴보겠지만 예외는 흔하다는 점에 유의한다. 소셜 네트워크는 새로운 링크가 정기적으로 생성되고 삭제되기 때문에 시간이 지남에 따라 변화하는 도메인에 대해 종종 설명해야 하는 예이다. 이 설정의 도메인은 종종 _동적 그래프_(Xu 등, 2020; Rossi 등, 2020)로 지칭된다.

종종, 개별 \(\mathbf{X}^{(t)}\) 입력은 유용한 대칭을 나타내므로 이전에 논의된 아키텍처에 의해 비공개적으로 처리될 수 있다. 몇 가지 일반적인 예는 다음과 같습니다. _videos_ (\(\Omega\)는 고정된 그리드이고 신호는 _프레임_의 시퀀스입니다); _fMRI 스캔_ (\(\Omega\)는 뇌 피질의 기하학을 나타내는 고정된 _mesh_입니다. 여기서 서로 다른 영역은 제시된 자극에 대한 반응으로 서로 다른 시간에 활성화됩니다); 및 _교통 흐름 네트워크_ (\(\Omega\)는 도로 네트워크를 나타내는 고정된 _그래프_입니다.

인코더 함수 \(f(\mathbf{X}^{(t)})\)가 문제에 적합하고 입력 도메인의 대칭성을 존중하는 입도의 수준에서 잠재 표현을 제공한다고 가정하자. 예를 들어, 비디오 프레임을 처리하는 것을 고려하십시오. 즉, 각 타이밍에서 \(n\times d\) 행렬 \(\mathbf{X}^{(t)}\)로 표시되는 _그리드 구조의 입력_이 주어지며, 여기서 \(n\)는 픽셀 수(시간적으로 고정됨)이고 \(d\)는 입력 채널 수(예: RGB 프레임에 대해 \(d=3\)입니다. 또한, 전체 프레임 수준에서 분석에 관심이 있는데, 이 경우 \(f\)를 변환 불변 CNN으로 구현하여 시간 단계 \(t\)에서 프레임의 \(k\)-차원 표현 \(\mathbf{z}^{(t)}=f(\mathbf{X}^{(t)})\)을 출력하는 것이 적합하다.

우리는 이제 모든 단계에 걸쳐 벡터 \(\mathbf{z}^{(t)}\)_의 시퀀스를 적절하게 _요약_하는 작업을 맡는다. 입력의 시간적 진행을 존중하고 새로운 데이터 포인트의 온라인 도착을 쉽게 허용하는 방식으로 이 정보를 동적으로 집계하는 표준 방법은 RNN(Recurrent Neural Network)을 사용하는 것입니다. 여기서 우리가 보여줄 것은 RNN이 입력 \(\mathbf{z}^{(t)}\)에 걸쳐 다소 특이한 형태의 대칭을 구현하기 때문에 그 자체로 연구하기에 흥미로운 기하학적 구조라는 것이다.

SimpleRNNs 각 단계에서 순환 신경망은 \(m\)-차원 _요약_ 벡터 \(\mathbf{h}^{(t)}\)를 계산한다. 이 (부분) 요약은 공유 _업데이트_ 함수 \(R:\mathbb{R}^{k}\times\mathbb{R}^{m}\rightarrow\mathbb{R}^{m}\)를 통해 현재 단계의 특징과 이전 단계의 요약에 대해 다음과 같이 조건부 계산된다(요약을 위해 도 19 참조):

\[\mathbf{h}^{(t)}=R(\mathbf{z}^{(t)},\mathbf{h}^{(t-1)}) \tag{38}\]

[MISSING_PAGE_EMPTY:95]

및, \(\mathbf{z}^{(t)}\) 및 \(\mathbf{h}^{(t-1)}\) 둘 다 _flat_ 벡터 표현들로서, \(R\)은 단일 완전-연결된 신경망 층으로서 가장 쉽게 표현될 수 있다(종종 _SimpleRNN_으로 알려져 있다; 엘만(1990) 참조; Jordan(1997)):

\[\mathbf{h}^{(t)}=\sigma(\mathbf{W}\mathbf{z}^{(t)}+\mathbff{U}\mathbff{h}^{(t-1)}+\mathbf{b}) \tag{39}\

여기서, \(\mathbf{W}\in\mathbb{R}^{k\times m}\), \(\mathbf{U}\in\mathbb{R}^{m\times m}\) 및 \(\mathbf{b}\in\mathbb{R}^{m}\)는 학습 가능한 파라미터이고, \(\sigma\)는 활성화 함수이다. 이것은 네트워크의 계산 그래프에서 _루프_ 를 도입하지만, 실제로 네트워크는 적절한 수의 단계들에 대해 언롤링되어, 시간_ 을 통한 _역전파(Robinson and Fallside, 1987; Werbos, 1988; Mozer, 1989)가 적용될 수 있게 한다.

그런 다음 요약 벡터를 다운스트림 작업에 적절하게 활용할 수 있습니다. 시퀀스의 모든 단계에서 예측이 필요한 경우 공유 예측자가 각 \(\mathbf{h}^{(t)}\)에 개별적으로 적용될 수 있습니다. 전체 서열을 분류하기 위해 일반적으로 최종 요약인 \(\mathbf{h}^{(T)}\)가 분류기로 전달된다. 여기서, \(T\)는 시퀀스의 길이이다.

특히, 초기 요약 벡터는 일반적으로 제로 벡터, 즉 \(\mathbf{h}^{(0)}=\mathbf{0}\)로 설정되거나 학습 가능하게 된다. 초기 요약 벡터가 설정되는 방식을 분석하면 RNN이 나타내는 흥미로운 형태의 _번역 등분산_을 추론할 수도 있다.

RNNs에서 변환 등분성 개별 단계 \(t\)를 이산 시간 단계 \(\mathbf{z}^{(t)}\)로 해석하기 때문에 입력 벡터 \(\mathbf{z}^{(t)}\)는 시간 단계의 1차원 격자 위에 사는 것으로 볼 수 있다. 여기에서 CNN에서 번역 등분산 분석을 확장하려는 시도는 매력적일 수 있지만 사소한 방식으로 수행할 수는 없다.

이유를 알아보기 위해, 우리가 수열을 한 단계 왼쪽으로 이동함으로써 새로운 수열 \(\mathbf{z}^{\prime(t)}=\mathbf{z}^{(t+1)}\)을 생성했다고 가정하자. 번역 공분산이 예상되는 것처럼 \(\mathbf{h}^{\prime(t)}=\mathbf{h}^{(t+1)}\)를 보여주려는 유혹이 있을 수 있지만 일반적으로 성립하지는 않는다. \(t=1\)를 고려 합니다. 업데이트 함수를 직접 적용 하 고 확장 하면 다음을 복구 합니다.

\[\mathbf{h}^{\prime(1)} =R(\mathbf{z}^{\prime(1)},\mathbf{h}^{(0)})=R(\mathbf{z}^{(2)}, \mathbf{h}^{(0)}) \tag{40}\] \[\mathbf{h}^{(2)} =R(\mathbf{z}^{(2)},\mathbf{h}^{(1)})=R(\mathbff{z}^{(2)},R( \mathbff{z}^{(1)},\mathbf{h}^{(0)})) \tag{41}\]

따라서, \(\mathbf{h}^{(0)}=R(\mathbf{z}^{(1)},\mathbf{h}^{(0)})\를 보장할 수 없다면, 우리는 번역 등분성을 회복하지 못할 것이다. 그런 다음 단계 \(t>1\)에 대해서도 유사한 분석을 수행할 수 있습니다.

[MISSING_PAGE_EMPTY:97]

효과적으로, 순환 연산의 반복된 적용으로 인해, 단일 RNN "계층"조차도 입력 단계들의 수와 동일한 깊이 _를 갖는다.

이것은 종종 RNN을 최적화할 때 독특하게 도전적인 학습 역학을 도입하는데, 각각의 트레이닝 예가 업데이트 네트워크의 _공유_ 파라미터에 대한 많은 그래디언트 업데이트를 유도하기 때문이다. 여기에서 우리는 깊이 및 매개변수 공유를 고려할 때 RNN에서 특히 문제가 되는 _사라짐_ 및 _폭발_ 기울기(Bengio 등, 1994)의 가장 두드러진 문제에 초점을 맞출 것이다. 또한, 그것은 단독으로 RNN에 대한 가장 영향력 있는 연구 중 일부에 박차를 가했다. 보다 상세한 개요를 위해, 우리는 RNN의 훈련 동학을 매우 상세하게 연구한 Pascanu et al.(2013)을 참조하고, 이러한 과제를 해석적, 기하학적, 동적 시스템의 렌즈와 같은 다양한 관점에서 노출시켰다.

소실구배를 설명하기 위해 S자형 활성화 함수 \(\sigma\)를 갖는 SimpleRNN을 고려하며, 그 미분 크기 \(|\sigma^{\prime}|\)는 항상 \(0\)와 \(1\) 사이에 있다. 이러한 값을 많이 곱하면 빠르게 0이 되는 경향이 있는 기울기가 발생하여 입력 시퀀스의 초기 단계가 네트워크 매개변수를 업데이트하는 데 전혀 영향을 미치지 않을 수 있음을 의미한다.

예를 들어 다음 단어 예측 태스크(예: 예측 키보드에서 흔히 볼 수 있음)를 고려하고 입력 텍스트 _"페타는 세르비아인입니다. 그는...[긴 단락]...페타는 현재..._에 살고 있습니다. 여기에서 다음 단어를 "세르비아"로 예측하는 것은 단락의 시작 부분을 고려하여 합리적으로 결론지을 수 있지만 이 입력 단계에 도달할 때까지 기울기가 사라져서 그러한 예제를 배우는 것이 매우 어려울 수 있다.

딥 피드포워드 신경망은 ReLU 활성화(정확하게 0 또는 1과 같은 기울기를 가지므로 소실 기울기 문제를 해결함)가 발명될 때까지 소실 기울기 문제에도 시달렸다. 그러나 RNN에서 ReLUs를 사용하면 업데이트 함수의 출력 공간이 이제 _unbounded_이므로 _exploding_gradients로 쉽게 이어질 수 있으며 Gradient descent는 모든 입력 단계에 대해 셀을 한 번 업데이트하여 업데이트의 규모를 빠르게 구축합니다. 역사적으로, 소실 구배 현상은 일찍이 순환 네트워크 사용의 중요한 장애물로 인식되었다. 이 문제에 대처하는 것은 우리가 다음에 설명하는 보다 정교한 RNN 층의 개발에 동기를 부여했다.

### 5.8 Long Short-Term Memory 네트워크

RNN에서 기울기 소실 효과를 크게 줄인 핵심 발명은 네트워크가 데이터 기반 방식으로 정보를 선택적으로 _덮어쓰기_ 할 수 있도록 하는 _게이팅 메커니즘_의 효과입니다. 이러한 _게이트된 RNNs_의 두드러진 예는 _Long Short-Term Memory_(LSTM; Hochreiter and Schmidhuber(1997)) 및 _Gated Recurrent Unit_(GRU; Cho 등(2014))을 포함한다. 여기서는 이러한 모델의 작동을 설명하기 위해 주로 LSTM, 특히 그레이브스(2013)가 제시한 변형에 대해 논의할 것이다. LSTM의 개념은 다른 게이트형 RNN으로 쉽게 넘어간다.

이 섹션 전반에 걸쳐, 우리가 텍스트로 논의할 모든 LSTM 동작을 예시하는 그림 20을 참조하는 것이 유용할 것이다.

LSTM은 계산 단계 사이에 _보존된_ 셀 상태 벡터 \(\mathbf{c}^{(t)}\in\mathbb{R}^{m}\)를 저장하는 _메모리 셀_을 도입하여 반복 계산을 증가시킨다. LSTM은 \(\mathbf{c}^{(t)}\), \(\mathbf{c}^{(t)}\)에 직접 기초하여 요약 벡터인 \(\mathbf{h}^{(t)}\를 계산하고, \(\mathbf{c}^{(t)}\)는 차례로 \(\mathbf{z}^{(t)}\), \(\mathbf{h}^{(t-1)}\) 및 \(\mathbf{c}^{(t-1)}\를 사용하여 계산된다. 비판적으로 셀은 \(\mathbf{z}^{(t)}\) 및 \(\mathbf{h}^{(t-1)}\)를 기반으로 **완전히 덮어쓰기** 되지 않으므로 SimpleRNN과 동일한 문제에 네트워크가 노출 됩니다. 대신에, 이전 셀 상태의 특정 양이 _보유_ 및 _-및

그림 20: 구성 요소 및 메모리 셀 \((M)\)이 명확하게 강조 표시된 장기 단기 메모리(LSTM)의 데이터 흐름입니다. 현재 입력 \(\mathbf{z}^{(t)}\), 이전 요약 \(\mathbf{h}^{(t-1)}\) 및 이전 셀 상태 \(\mathbf{c}^{(t-1)}\)에 기초하여, LSTM은 업데이트된 셀 상태 \(\mathbf{c}^{(t)}\) 및 요약 \(\mathbf{h}^{(t)}\를 예측한다.

이것이 발생하는 비율은 데이터에서 명시적으로 _학습된_ 것입니다.

SimpleRNN에서와 마찬가지로, 현재 입력 단계 및 이전 요약에 걸쳐 단일 완전 연결 신경망 계층을 사용하여 특징을 계산한다:

\[\widetilde{\mathbf{c}}^{(t)}=\tanh(\mathbf{W}_{c}\mathbf{z}^{(t)}+\mathbff{U}_{c}\mathbf{h}^{(t-1)}+\mathbf{b}_{c}) \tag{43}\

그러나 언급된 바와 같이, 우리는 이 벡터의 _all_이 세포에 들어가는 것을 허용하지 않는데, 그래서 우리는 이것을 _candidate_ 특징의 벡터라고 부르고 그것을 \(\widetilde{\mathbf{c}}^{(t)}\)로 나타낸다. 대신 LSTM은 범위 \([0,1]\)의 실수 벡터인 _게이팅 벡터_를 직접 학습하고, 메모리 셀에 신호가 얼마나 들어가고, 나가고, 덮어쓰도록 허용되어야 하는지를 결정한다.

이러한 세 개의 게이트는 모두 \(\mathbf{z}^{(t)}\)와 \(\mathbf{h}^{(t-1)}\)에 기초하여 계산된다: 셀에 들어갈 수 있는 후보 벡터의 비율을 계산하는 _입력 게이트_\(\mathbf{i}^{(t)}\; 보유될 이전 셀 상태의 비율을 계산하는 _포젯 게이트_\(\mathbf{f}^{(t)}\; 및 최종 요약 벡터에 사용될 새로운 셀 상태의 비율을 계산하는 _출력 게이트_\(\mathbf{o}^{(t)}\). 일반적으로 이 모든 게이트는 _logistic sigmoid_ 활성화 \(\mathrm{logistic}(x)=\frac{1}{1+\exp(-x)}\)이지만 출력이 \([0,1]\) 범위에 있음을 보장하기 위해 단일 완전 연결 계층을 사용하여 유도됩니다.

\[\mathbf{i}^{(t)} =\mathrm{logistic}(\mathbf{W}_{\mathbf{i}}\mathbf{z}^{(t)}+\mathbff{U}_{\mathbf{f}}\mathbf{z}^{(t)}+\mathbf{b}_{\mathbf{i}})\tag{44}\] \[\mathbf{f}^{(t)} =\mathrm{logistic}(\mathbf{W}_{\mathbf{f}}\mathbf{z}^{(t)}+\mathbf{U}_{\mathbf{f}}_{\mathbf{f}})\tag{46}\] (45) \[\mathbf{o}^{(t)} =\mathrm{logistic}(\mathbf{W}_{\mathbff{o}}}\mathbf{z}^{(t)}+\mathbf{h}^{

마지막으로, 이 게이트들은 _new_ 셀 상태, \(\mathbf{c}^{(t)}\)를 디코딩하기 위해 적절하게 적용되며, 그 다음 출력 게이트에 의해 변조되어 다음과 같이 요약 벡터 \(\mathbf{h}^{(t)}\)를 생성한다:

\[\mathbf{c}^{(t)} =\mathbf{i}^{(t)}\odot\widetilde{\mathbf{c}}^{(t)}+\mathbf{f}^{(t)}\odot\mathbff{c}^{(t-1)} \tag{47}\] \[\mathbf{h}^{(t)} =\mathbf{o}^{(t)}\odot\tanh(\mathbf{c}^{(t)}) \tag{48}\

여기서 \(\odot\)는 요소별 벡터 곱셈이다. 함께 적용되면 식 (43)-(48)은 LSTM에 대한 _업데이트 규칙_을 완전히 지정하며, 이는 이제 셀 벡터 \(\mathbf{c}^{(t)}\)도 고려한다:

\[(\mathbf{h}^{(t)},\mathbf{c}^{(t)})=R(\mathbf{z}^{(t)},(\mathbf{h}^{(t-1)}, \mathbf{c}^{(t-1)}))\]

(\mathbf{f}^{(t)}\)의 값은 \(\mathbf{z}^{(t)}\)와 \(\mathbf{h}^{(t-1)}\)에서 파생되므로 LSTM은 데이터에서 직접 _학습할 수 있습니다. 따라서 LSTM은 과거의 경험을 적절하게 잊는 방법을 효과적으로 학습합니다. 실제로 모든 LSTM 매개변수 \((\mathbf{f}^{(t)}\)에 대한 역전파 업데이트에서 \((\mathbf{W}_{*},\mathbf{U}_{*},\mathbf{b}_{*})\) 값이 직접 나타나 네트워크가 데이터 기반 방식으로 명시적으로 _제어_할 수 있도록 하며 시간 단계에 걸쳐 기울기에 대한 소실 정도를 나타냅니다.

사라지는 기울기 문제를 정면으로 해결하는 것 외에도 게이티드 RNN은 SimpleRNN의 손이 닿지 않는 _시간 워핑_ 변환에 대한 매우 유용한 형태의 불변도 여는 것으로 나타났다.

게이트 RNN의 시간 워핑 불변성은 _연속 시간_ 설정에서 이러한 변환에 대한 불변성을 달성하기 위해 _워핑 시간_이 의미하는 것과 순환 모델에 필요한 것을 설명하는 것으로 시작할 것이다. 우리의 박람회는 처음에 이 현상을 설명한 탤렉과 올리비에(2018)의 작업을 주로 따를 것이며 실제로 불변 렌즈에서 RNN을 실제로 연구한 최초의 참가자 중 하나였다.

RNN을 적용하고자 하는 연속적인 시간영역 신호 \(z(t)\)를 가정하자. 요약 벡터 \(\mathbf{h}^{(t)}\)에 대한 RNN의 이산 시간 계산을 연속 도메인인 \(h(t)\의 유사체와 정렬하기 위해, 우리는 그것의 선형 테일러 확장을 관찰할 것이다:

\[h(t+\delta)\approx h(t)+\delta\frac{\mathrm{d}h(t)}{\mathrm{d}t} \tag{49}\]

그리고 \(\delta=1\)을 설정하면, 우리는 \(h(t)\)와 \(h(t+1)\ 사이의 관계를 복구하는데, 이것은 정확히 RNN 갱신 함수 \(R\)(식 38)가 계산하는 것이다. 즉, RNN 갱신 함수는 다음의 미분 방정식을 만족한다:

\[\frac{\mathrm{d}h(t)}{\mathrm{d}t}=h(t+1)-h(t)=R(z(t+1),h(t))-h(t) \tag{50}\]

우리는 RNN이 (예를 들어, 측정의 시간 단위를 변경함으로써) 신호가 샘플링되는 방식에 탄력적이기를 바란다. 형식적으로는 시간 간의 미분 가능한 매핑이 단조적으로 증가하는 _시간 워핑_ 연산 \(\tau:\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}\)을 나타냅니다. 표기 \(\tau\)는 시간 워핑이 시간의 _automorphism_ 을 나타내기 때문에 선택 됩니다.

또한, 클래스의 모든 모델과 그러한 \(\tau\)에 대해 워핑되지 않은 경우에 원래 모델과 동일한 방식으로 워핑된 데이터를 처리하는 클래스에서 다른(아마도 동일한) 모델이 존재하는 경우 모델 클래스는 타임 워핑에 대해 _불변_이라고 한다.

우리는 \(h(t)\)는 시간에서 연속 신호를 나타내고 \(t\), \(\mathbf{h}^{(t)}\)는 시간 단계 \(t\)에서 이산 신호를 나타낼 것이다.

우리는 \(h(t)\)는 시간에서 연속 신호를 나타내고 \(t\), \(\mathbf{h}^{(t)}\)는 시간 단계 \(t\)에서 이산 신호를 나타낼 것이다.

우리는 \(h(t)\)는 시간에서 연속 신호 \(t\)를 나타내고 \(\mathbf{h}^{(t)}\)는 시간 단계 \(t\)에서 이산 신호를 나타내기를 원한다.

우리는 RNN이 (예를 들어, 측정의 시간 단위를 변경함으로써) 신호가 샘플링되는 방식에 탄력적이기를 바란다. 형식적으로는 시간 간의 미분 가능한 매핑이 단조적으로 증가하는 _시간 워핑_ 연산 \(\tau:\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}\)을 나타냅니다. 표기 \(\tau\)는 시간 워핑이 시간의 _automorphism_ 을 나타내기 때문에 선택 됩니다.

또한, 클래스의 모든 모델과 그러한 \(\tau\)에 대해 워핑되지 않은 경우에 원래 모델과 동일한 방식으로 워핑된 데이터를 처리하는 클래스에서 다른(아마도 동일한) 모델이 존재하는 경우 모델 클래스는 타임 워핑에 대해 _불변_이라고 한다.

이것은 잠재적으로 매우 유용한 속성입니다. 단기 종속성을 잘 모델링할 수 있는 RNN 클래스가 있고, 또한 이 클래스가 시간 와핑에 불변한다는 것을 보여줄 수 있다면, (단기 종속성을 갖는 신호의 시간 확장 와핑에 대응할 것이기 때문에) 장기 종속성도 유용하게 캡처할 방식으로 그러한 모델을 트레이닝하는 것이 가능하다는 것을 알 수 있다. 곧 살펴보겠지만, LSTM과 같은 _gated_ RNN 모델이 장거리 종속성을 모델링하기 위해 제안된 것은 우연이 아니다. 시간 왜곡 불변성을 달성하는 것은 LSTM의 입력/잊음/출력 게이트와 같은 게이팅 메커니즘의 존재와 밀접하게 결합된다.

시간이 \(\tau\)로 워핑될 때, 시간 \(t\)에서 RNN에 의해 관측된 신호는 \(z(\tau(t))\)이고, 이러한 워핑에 불변하기 위해서는 등가적으로 워핑된 요약 함수 \(h(\tau(t))\)를 예측해야 한다. 테일러 확장 인수를 다시 한번 사용하여, 우리는 RNN 업데이트 \(R\)가 만족해야 하는 와핑된 시간에 대한 수학식 50의 형태를 유도한다:

\[\frac{\mathrm{d}h(\tau(t))}{\mathrm{d}\tau(t)}=R(z(\tau(t+1)),h(\tau(t)))-h( \tau(t)) \tag{51}\]

그러나, 위 도함수는 와핑된 시간 \(\tau(t)\)에 대해 계산되므로, 원래 신호를 고려하지 않는다. 워핑 변환을 명시적으로 고려하기 위해서는 \(t\)와 관련하여 워핑된 요약 함수를 미분해야 한다. 체인 규칙을 적용하면 다음과 같은 미분 방정식이 산출된다:

\[\frac{\mathrm{d}h(\tau(t))}{\mathrm{d}t}=\frac{\mathrm{d}h(\tau(t))}{\mathrm{d}t}=\frac{\mathrm{d}h(\tau(t))}{\mathrm{d}t}=\frac{\mathrm{d}\tau(t)}{ \mathrm{d}t}R(z(\tau(t+1)),h(\tau(t)))-\frac{\mathrm{d}\tau(t)}{\mathrm{d}t}h( \tau(t)) \tag{52}\

그리고 우리의 (연속시간) RNN이 _any_ 시간 와핑 \(\tau(t)\)에 불변으로 유지되기 위해서는 미리 알려져 있지 않은 도함수 \(\frac{\mathrm{d}\tau(t)}{\mathrm{d}t}\)를 명시적으로 나타낼 수 있어야 한다. 이 도함수를 근사화하는 _학습 가능_ 함수 \(\Gamma\)를 도입해야 합니다. 예를 들어 \(\Gamma\)는 \(z(t+1)\) 및 \(h(t)\)를 고려하고 스칼라 출력을 예측하는 신경망일 수 있습니다.

이제 타임 워핑 하에서 _이산_ RNN 모델의 관점에서, 그것의 입력 \(\mathbf{z}^{(t)}\)은 \(z(\tau(t))\)에 대응할 것이고, 그것의 요약 \(\mathbf{h}^{(t)}\)은 \(h(\tau(t))\)에 대응할 것이라는 점에 유의하라. \(\mathbf{h}^{(t)}\) 대 \(\mathbf{h}^{(t+1)}\)의 필요 관계를 얻기 위해 시간 와핑에 불변하기 위해 우리는 \(h(\tau(t))\)의 1단계 테일러 확장을 사용할 것이다:

\[h(\tau(t+\delta))\approx h(\tau(t))+\delta\frac{\mathrm{d}h(\tau(t))}{\mathrm{d}t}\] 및, 다시 한번 \(\delta=1\)를 설정하고 수학식 52를 대입한 후 이산화하는 단계:

\mathbf{h}^{(t+1)} =\mathbf{h}^{(t)}+\frac{\mathrm{d}\tau(t)}}{\mathrm{d}t}\mathbf{h}^{(t)}\] \[=\frac{\mathrm{d}\tau(t)}{\mathrm{d}t}R(\mathbf{z}^{(t+1)}, \mathbf{h}^{(t)})+\left(1-\frac{\mathrm{d}\tau(t)}{\mathrm{h}^{(t)}\] \[=\frac{\mathrm{d}\tau(t)}{\mathrm{d}t}R(\mathbf{z}^{(t+1)}, \mathbf{h}^{(t)})+\left(1-\frac{\mathrm{d}\tau(t)}{\mathrm{h}^{(t)}\]\mathbf{h}^{(t)}

마지막으로 \(\frac{\mathrm{d}\tau(t)}{\mathrm{d}t}\)을 앞서 언급한 학습 가능한 함수인 \(\Gamma\)와 스왑한다. 이렇게 하면 시간 왜곡 불변 RNN에 필요한 양식을 얻을 수 있습니다.

\[\mathbf{h}^{(t+1)}=\Gamma(\mathbf{z}^{(t+1)},\mathbf{h}^{(t)})R(\mathbf{z}^{(t+1)},\mathbf{h}^{(t)})+(1-\Gamma(\mathbf{z}^{(t+1)},\mathbf{h}^{(t)}))\mathbf{ h}^{(t)} \tag{53}\]

우리는 SimpleRNNs (식 39)가 식 53의 두 번째 항이 특징적이지 않다는 점을 감안할 때 __ 시간 와핑 불변임을 빠르게 추론할 수 있다. 대신 \(\mathbf{h}^{(t)}\)를 \(R(\mathbf{z}^{(t+1)},\mathbf{h}^{(t)})\로 완전히 덮어쓰는데, 이는 시간 와핑이 전혀 없다고 가정하는 것에 해당한다; \(\frac{\mathrm{d}\tau(t)}{\mathrm{d}t}=1\), 즉 \(\tau(t)=t\).

또한, \(R\)에 기초한 연속시간 RNN과 이산 RNN 사이의 연결은 시간 워핑 도함수가 너무 크지 않은 경우, 즉 \(\frac{\mathrm{d}\tau(t)}{\mathrm{d}t}\lesssim 1\)에만 유지되는 테일러 근사법의 정확도에 달려 있다. 이에 대한 직관적인 설명은 다음과 같습니다. 시간 워핑 작업이 중간 데이터 변경이 샘플링되지 않을 만큼 충분히 큰 시간 증가 \((t\to t+1)\)를 만드는 방식으로 시간을 _계약하면 모델은 시간 워핑된 입력을 원래 입력과 동일한 방식으로 처리할 수 없습니다. 단순히 동일한 정보에 액세스할 수 없습니다. 반대로, 모든 형태의 시간 _확장_(이산적 용어로 입력 시계열에 0이 있는 분산에 해당)은 프레임워크 내에서 완벽하게 허용된다.

\(\tau\)(\(\frac{\mathrm{d}\tau(t)}{\mathrm{d}t}>0\)), \(\Gamma\)의 출력 공간을 \(0<\Gamma(\mathbf{z}^{(t+1)},\mathbf{h}^{(t)})<1\)로 묶을 수 있다.

\[\Gamma(\mathbf{z}^{(t+1)},\mathbf{h}^{(t)})=\mathrm{logistic}(\mathbf{W}_{ \Gamma}\mathbf{z}^{(t+1)}+\mathbf{U}_{\Gamma}\mathbff{h}^{(t)}+\mathbf{b}_{ \Gamma})\]

_exactly_ 매칭하는 LSTM 게이팅 방정식들(예를 들어, 수학식 44). 주요 차이점은 LSTM이 게이팅 _벡터_를 계산하는 반면 방정식 53은 \(\Gamma\)가 스칼라 출력을 함축한다는 것입니다. 벡터화된 게이트(Hochreiter, 1991)는 \(\mathbf{h}^{(t)}\)의 모든 차원에 서로 다른 워핑 도함수를 맞출 수 있게 하여 여러 시간 지평선에 대해 동시에 추론할 수 있게 한다.

우리가 한 일을 요약하기 위해 여기서 잠시 멈출 가치가 있다. 우리의 RNN 클래스가 (비파괴적) 시간 와핑에 불변임을 요구함으로써, 우리는 그것이 가져야 하는 필요한 형태를 도출했고(식 53), 그것이 정확히 _게이트_ RNN의 클래스에 해당함을 보여주었다. 이러한 관점에서 게이트의 주요 역할은 워핑 변환의 _도함수_\(\frac{\mathrm{d}\tau(t)}{\mathrm{d}t}\)를 정확하게 맞추는 것이다.

클래스 불변성의 개념은 이전에 연구한 불변성과 다소 다르다. 즉, \(\tau_{1}(t)\)로 시간 왜곡된 입력에서 게이트 RNN을 훈련시키면 일반적으로 다른 \(\tau_{2}(t)\)에 의해 왜곡된 신호로 제로 샷을 전달할 수 없다. 오히려, 클래스 불변성은 게이트된 RNN들이 이들 신호들 모두를 동일한 방식으로 적합시킬 수 있을 만큼 충분히 강력하지만, 잠재적으로 매우 상이한 모델 파라미터들을 갖는다는 것을 보장할 뿐이다. 즉, 효과적인 게이팅 메커니즘이 워핑 유도체를 맞추는 것과 밀접하게 관련되어 있다는 깨달음은 이제 간략하게 보여주듯이 게이팅된 RNN 최적화에 대한 유용한 처방을 산출할 수 있다.

예를 들어, 신호 내에서 추적에 관심이 있는 종속성의 범위는 \([T_{l},T_{h}]\) 시간 단계 범위일 것이라고 종종 가정할 수 있다.

방정식 52에 대한 해석해를 분석한 결과, 게이트 RNN에 의한 \(\mathbf{h}^{(t)}\)의 특성 _forgetting time_은 \(\frac{1}{\Gamma(\mathbf{z}^{(t+1)},\mathbf{h}^{(t)})}\에 비례함을 알 수 있었다. 따라서 우리는 가정된 범위 내에서 정보를 효과적으로 기억하기 위해 게이팅 값이 \(\left[\frac{1}{T_{h}},\frac{1}{T_{m}}\right]\) 사이에 놓이기를 원한다.

또한, \(\mathbf{z}^{(t)}\)와 \(\mathbf{h}^{(t)}\)가 대략 0-중심이라고 가정하면, 층 정규화(Ba et al., 2016)와 같은 변환을 적용하는 일반적인 부산물인 \(\mathbb{E}[\Gamma(\mathbf{z}^{(t+1)},\mathbf{h}^{(t)})]\approx\mathrm{ logistic}(\mathbf{b}_{\Gamma})\라고 가정할 수 있다. 따라서 게이팅 메커니즘의 _bias_ 벡터를 제어하는 것은 효과적인 게이트 값을 제어하는 매우 강력한 방법이다.

두 관측치를 합하면 \(\mathbf{b}_{\Gamma}\sim-\log(\mathcal{U}(T_{l},T_{h})-1)\)를 초기화하여 적절한 범위의 게이팅 값을 얻을 수 있으며, 여기서 \(\mathcal{U}\)는 균일한 실수 분포이다. 이러한 권장 사항은 Tallec 및 Ollivier(2018)에 의해 _chrono 초기화_로 명명되었으며 게이티드 RNN의 장거리 종속 모델링을 개선하는 것으로 경험적으로 나타났다.

RNNs를 사용한 시퀀스 대 시퀀스 학습 RNN 지원 계산을 사용하는 대표적인 역사적 예는 자연 언어의 기계 번역과 같은 _시퀀스 대 시퀀스_ 번역 작업이다. Sutskever 등(2014)에 의한 pioneering _seq2seq_ 작업은 요약 벡터를 통과시킴으로써 이를 달성하였고,

[MISSING_PAGE_EMPTY:105]

입력 내용, 상당한 작업도 입력에 주의를 기울이는 더 많은 명시적인 방법을 배웠다. 이렇게 하는 강력한 알고리즘 기반 방법은 Vinyals 등(2015)의 _포인터 네트워크_이며, 이는 _가변 크기_ 입력의 요소를 가리킬 수 있도록 반복 주의의 간단한 수정을 제안한다. 그런 다음 이러한 발견은 포인터 네트워크 지원 LSTM에서 지원하는 정렬되지 않은 집합에 seq2seq 모델을 일반화하는 _set2set_ 아키텍처(Vinyals 등, 2016)로 일반화되었다.

## 6 문제점 및 응용 프로그램

불변과 대칭은 모두 실제 세계에서 발생하는 데이터에 걸쳐 너무 흔하게 발생한다. 따라서 21세기 기계 학습의 가장 인기 있는 응용 프로그램 중 일부가 기하학 딥 러닝의 직접적인 부산물로 발생했다는 것은 놀라운 일이 아니며, 아마도 때로는 이 사실을 완전히 깨닫지 못한 채 발생한다. 우리는 기하학 딥러닝에서 영향력 있는 작품과 흥미롭고 유망한 새로운 응용 프로그램에 대한 개요를 독자들에게 제공하고 싶습니다. 우리의 동기는 두 가지이다: 5개의 기하학 영역이 일반적으로 발생하는 과학 및 산업 문제의 특정 사례를 보여주고 기하학 딥러닝 원리와 아키텍처에 대한 추가 연구를 위한 추가 동기를 제공한다.

그래프에 대한 표현 학습의 가장 유망한 응용 프로그램 중 하나는 계산 화학 및 약물 개발이다. 전통적인 약물은 질병과 관련된 화학 과정을 활성화하거나 방해하기 위해 일반적으로 단백질인 일부 표적 분자에 화학적으로 부착('결합')하도록 설계된 작은 분자이다. 불행히도, 약물 개발은 매우 길고 비용이 많이 드는 과정입니다: 신약을 시장에 출시할 때 일반적으로 10년 이상이 걸리고 10억 달러 이상의 비용이 듭니다. 그 이유 중 하나는 다양한 단계에서 많은 약물이 실패하는 테스트 비용이다 - 후보의 5% 미만이 마지막 단계로 도달한다(예를 들어, 가우데렛 등(2020) 참조).

화학적으로 합성 가능한 분자의 공간은 매우 크기 때문에 (10^{60}\) 부근에서 추정됨), 표적 결합 친화도, 낮은 독성, 용해도 등과 같은 특성이 적절히 조합된 후보 분자를 탐색한다. 실험적으로 수행할 수 없으며 _가상_ 또는 _인 실리코 스크리닝_ (즉, 유망한 분자를 식별하기 위한 계산 기술의 사용)이 사용된다. 기계 학습 기술은 이 작업에서 점점 더 두드러진 역할을 한다. 가상 약물 스크리닝을 위한 기하 딥러닝 사용의 두드러진 예는 최근 스톡스 등(2020)이 후보 분자가 모델 박테리아 _대장균_에서 성장을 억제하는지 여부를 예측하도록 훈련된 그래프 신경망을 사용하여 보여주었으며, 이는 원래 당뇨병 치료를 위해 표시된 분자인 _할리신_이 항생제 내성이 알려져 있는 박테리아 균주에 대해서도 매우 강력한 항생제임을 효과적으로 발견할 수 있었다. 이 발견은 과학 및 대중 언론에서 널리 다뤄졌다.

보다 넓게 말하면, 그래프로 모델링된 분자에 대한 그래프 신경망의 적용은 매우 활발한 분야였으며, 물리학에서 영감을 받고, 예를 들어 회전 및 변환에 대한 등분성을 통합하는 다수의 전문화된 아키텍처가 최근에 제안되었다(예를 들어 Thomas 등(2018); Anderson 등(2019); Fuchs 등(2020); Satorras 등(2021). 또한, Bapst 등(2020)은 이전에 이용 가능한 물리 기반 모델을 능가하는 방식으로 유리의 역학을 예측적으로 모델링하기 위한 GNN의 유용성을 성공적으로 입증했다. 역사적으로 계산 화학의 많은 작업은 많은 공통 특성을 공유하는 현대 그래프 신경망 구조의 전구체였다.

약물 재배치 완전히 새로운 약물 후보를 생성하는 것은 잠재적으로 실행 가능한 접근법이지만 새로운 치료법을 개발하기 위한 더 빠르고 저렴한 방법은 새로운 목적으로 이미 승인된 약물(단독 또는 조합)을 평가하려는 _약물 재배치_이다. 이것은 종종 약물을 시장에 출시하는 데 필요한 임상 평가의 양을 상당히 감소시킨다. 어느 정도 추상화 수준에서, 신체 생화학에 대한 약물의 작용 및 서로와 다른 생체 분자 사이의 상호 작용은 그래프로 모델링될 수 있으며, 이는 저명한 네트워크 과학자 알버트-라즐로 바라바시가 만든 '네트워크 의학'의 개념을 생성하고 새로운 치료법을 개발하기 위한 생물학적 네트워크(단백질-단백질 상호작용 및 대사 경로와 같은)의 사용을 옹호한다(바라바시 외, 2011).

기하학적 딥 러닝은 이러한 종류의 접근법에 대한 현대적인 견해를 제공한다. 눈에 띄는 초기 예는 약물-약물 상호작용 그래프에서 에지 예측으로 공식화된 _조합 요법_ 또는 _다약제_로 알려진 약물 재배치 형태로 부작용을 예측하기 위해 그래프 신경망을 사용한 Zitnik 등(2018)의 작업이다. 본문 작성 당시 크게 진행 중인 신종 코로나바이러스 팬데믹은 코로나19에 대해 이러한 접근 방식을 적용하려는 시도에 특별한 관심을 불러일으켰다(Gysi et al., 2020). 마지막으로, 약물 재배치가 반드시 합성 분자로 제한되는 것은 아니라는 점에 유의해야 한다: 베셀코프 등(2019)은 식품에 포함된 약물 유사 분자에 유사한 접근법을 적용했다(언급했듯이 많은 식물성 식품에는 종양 치료에 사용되는 화합물의 생물학적 유사체가 포함되어 있기 때문이다). 이 텍스트의 저자 중 한 명은 이러한 약물 같은 분자가 풍부한 '하이퍼푸드' 성분을 기반으로 흥미로운 레시피를 설계하는 분자 셰프와 협력함으로써 이 연구에 창의적인 반전을 추가하는 협업에 참여한다.

단백질 생물학은 이미 단백질을 약물 표적으로 언급했기 때문에 이 주제에 대해 몇 가지 시간을 더 보낼 수 있습니다. 단백질은 병원체(항체)에 대한 보호, 피부에 구조 부여(콜라겐), 세포로 산소 수송(헴글로빈), 화학 반응(효소) 촉매, 신호 전달(많은 호르몬은 단백질)을 포함하여 우리 몸에서 무수히 많은 기능을 가진 가장 중요한 생체 분자 중 하나이다. 화학적으로 말하면 단백질은 생물고분자 또는 정전기력의 영향을 받아 복잡한 3D 구조로 접히는 _아미노산_이라고 하는 작은 빌딩 블록의 사슬이다. 단백질에 기능을 부여하는 것은 이 구조이기 때문에 단백질이 어떻게 작용하고 무엇을 하는지 이해하는 데 중요하다. 단백질은 약물 요법의 일반적인 표적이기 때문에 제약 산업은 이 분야에 대한 관심이 높다.

단백질 생물정보학에서 문제의 전형적인 계층은 단백질 _서열_ (20개의 다른 아미노산의 알파벳 위에 있는 1D 문자열)에서 3D _구조_ ('단백질 접힘'으로 알려진 문제)에서 _기능_ ('단백질 기능 예측')으로 가는 것이다. 시니어 등의 딥마인드 알파폴드(2020)와 같은 최근 접근법은 단백질 구조를 나타내기 위해 _접촉 그래프_를 사용했다. Gligorijevic et al.(2020)은 이러한 그래프에 그래프 신경망을 적용하면 순전히 시퀀스 기반 방법을 사용하는 것보다 더 나은 함수 예측을 달성할 수 있음을 보여주었다.

Gainza et al.(2020)은 MaSIF라는 Geometric Deep Learning 파이프라인을 개발하여 이들의 3D 구조에서 단백질 간의 상호작용을 예측하였다. MaSIF는 단백질을 메쉬로 이산화한 분자 표면으로 모델링하며, 이 표현이 내부 접힘 구조를 추상화할 수 있기 때문에 상호작용을 다룰 때 유리하다고 주장한다. 이 구조는 작은 지역 측지 패치에서 미리 계산된 화학적 및 기하학적 특징으로 작동하는 메쉬 합성곱 신경망을 기반으로 했다. 네트워크는 인터페이스 예측, 리간드 분류 및 도킹을 포함한 여러 작업을 다루기 위해 단백질 데이터 은행의 수천 개의 공결정 단백질 3D 구조를 사용하여 훈련되었으며 원칙적으로 암에 대한 생물학적 면역 치료 약물로 작용할 수 있는 단백질의 _de novo_('처음부터') 설계를 허용했다. 이러한 단백질은 프로그래밍된 세포 사멸 단백질 복합체(PD-1/PD-L1)의 부분 사이의 단백질-단백질 상호작용(PPI)을 억제하고 면역계에 종양 세포를 공격하는 능력을 제공하도록 설계되었다.

추천 시스템 및 소셜 네트워크 그래프 표현 학습의 첫 번째 대중화된 대규모 응용 프로그램은 주로 _추천 시스템_의 맥락에서 _소셜 네트워크_ 내에서 발생했다. 추천인은 잠재적으로 서비스에 대한 이전 상호 작용 이력에 따라 사용자에게 제공할 콘텐츠를 결정하는 작업을 수행합니다. 이것은 일반적으로 _링크 예측_ 목표를 통해 실현됩니다: 다양한 노드(콘텐츠의 조각)의 임베딩을 감독하여 _관련된 것으로 간주되는 경우(예: 일반적으로 함께 보기) 함께 가깝게 유지되도록 합니다. 그런 다음 두 개의 임베딩(예: 내부 곱)의 _근접성_ 은 콘텐츠 그래프의 간선에 의해 링크될 확률로 해석될 수 있으며, 따라서 사용자가 쿼리하는 모든 콘텐츠에 대해 하나의 접근법은 임베딩 공간에서 \(k\) 가장 가까운 이웃을 제공할 수 있다.

이 방법론의 선구자 중에는 미국 이미지 공유 및 소셜 미디어 회사 핀터레스트가 있다: 생산에서 GNN의 첫 번째 성공적인 배포 중 하나를 제시하는 것 외에도, 그들의 방법, 핀세이지(PinSage)는 수백만 개의 노드 및 수십억 개의 에지의 그래프에 대해 그래프 표현 학습을 _스케일러블_ 성공적으로 만들었다(Ying et al., 2018). 특히 제품 권장 사항의 공간에서 관련 응용 프로그램이 곧 뒤따랐다. 현재 생산에 투입되고 있는 인기 GNN 지원 추천으로는 Alibaba사의 Aligraph(Zhu et al., 2019)와 Amazon사의 P-Companion(Hao et al., 2020) 등이 있다. 이러한 방식으로, 그래프 딥 러닝은 매일 수백만 명의 사람들에게 영향을 미치고 있다.

소셜 네트워크에 대한 콘텐츠 분석의 맥락에서 또 다른 주목할 만한 노력은 (2019년, 트위터에 의해) 첫 번째 GNN 기반 스타트업 중 하나인 파불라 AI이다. 텍스트의 저자 중 한 명과 그의 팀이 설립한 이 스타트업은 소셜 네트워크 상에서 잘못된 정보를 검출하기 위한 새로운 기술을 개발했다(Monti et al., 2019). 파불라의 솔루션은 특정 뉴스 아이템의 확산을 공유한 사용자들의 네트워크에 의해 모델링하는 것으로 구성된다. 사용자는 한 사람이 다른 사람으로부터 정보를 다시 공유하면 연결되지만 소셜 네트워크에서 서로 팔로우하면 연결됩니다. 그런 다음 이 그래프는 그래프 신경망에 공급되어 전체 그래프를 사실 확인 본체 간의 일치를 기반으로 하는 레이블이 있는 '참' 또는 '가짜' 내용으로 분류한다. 뉴스 확산 후 몇 시간 이내에 빠르게 안정되는 강력한 예측력을 입증하는 것 외에도 개별 사용자 노드의 임베딩을 분석한 결과 잘 알려진 _'echo chamber'_ 효과를 예시하여 잘못된 정보를 공유하는 경향이 있는 사용자의 명확한 군집링이 나타났다.

교통량 예측 교통 네트워크는 기하학 딥러닝 기술이 이미 전 세계적으로 수십억 명의 사용자에게 실행 가능한 영향을 미치고 있는 또 다른 영역이다. 예를 들어, 도로 네트워크에서, 우리는 교차로들을 노드들로서 관찰할 수 있고, 도로 세그먼트들을 이들을 연결하는 에지들로서 관찰할 수 있는데, 이러한 에지들은 그 다음 도로 길이, 그들의 세그먼트를 따른 현재 또는 과거 속도들 등에 의해 특징지어질 수 있다.

이 공간의 표준 예측 문제 중 하나는 주어진 후보 경로에 대해 도착 예상 시간(ETA)을 예측하여 이를 횡단하는 데 필요한 예상 이동 시간을 제공하는 것입니다. 이러한 문제는 사용자 대면 트래픽 추천 앱뿐만 아니라 자체 운영 내에서 이러한 예측을 활용하는 기업(음식 배달 또는 승차 공유 서비스와 같은)의 경우에도 이 공간에서 필수적이다.

그래프 신경망은 이 공간에서도 엄청난 가능성을 보여주었습니다. 예를 들어 도로 네트워크의 관련 하위 그래프(효과적으로 _그래프 회귀_ 작업)에 대한 ETA를 직접 예측하는 데 사용할 수 있습니다. 이러한 접근법은 딥마인드에 의해 성공적으로 활용되어 현재 Google Maps(Derrow-Pinion 등, 2021)에서 생산에 배포되는 GNN 기반 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 예상 유사한 수익률이 바이두 지도 팀에 의해 관찰되었으며, 여기서 이동 시간 예측은 현재 ConSTGAT 모델에 의해 제공되며, 이는 그 자체가 그래프 어텐션 네트워크 모델의 시공간 변형에 기초한다(Fang et al., 2020).

객체 인식 컴퓨터 비전의 기계 학습 기술에 대한 주요 벤치마크는 제공된 이미지 내에서 중심 객체를 _분류하는 기능입니다. ImageNet 대규모 시각 인식 챌린지(Russakovsky et al., 2015, ILSVRC)는 기하 딥러닝의 초기 개발의 많은 부분을 추진한 연간 객체 분류 챌린지였다. ImageNet은 웹에서 긁어낸 사실적인 이미지를 1000개의 카테고리 중 하나로 분류하기 위해 모델을 필요로 한다: 그러한 카테고리는 동시에 다양하며(생물과 무생물 모두를 포함함), 특정적이다(다양한 고양이와 개의 품종을 구별하는 데 중점을 둔 많은 클래스와 함께). 따라서 ImageNet에 대한 우수한 성능은 종종 일반 사진에서 견고한 수준의 특징 추출을 의미하며, 이는 사전 훈련된 ImageNet 모델에서 다양한 _전이 학습_ 설정의 기초를 형성했다.

ImageNet에서 합성곱 신경망의 성공, 특히 ILSVRC 2012를 큰 마진으로 휩쓸었던 Krizhevsky 등의 AlexNet 모델(2012)은 학계와 산업계 모두에서 딥 러닝의 채택을 크게 주도했다. 이후 CNN은 VGG-16(Simonyan and Zisserman, 2014), Inception(Szegedy et al., 2015) 및 ResNets(He et al., 2016)와 같은 많은 인기 있는 아키텍처를 산란시키면서 ILSVRC의 상위권에 지속적으로 올랐으며, 이는 이 작업에서 인간 수준의 성능을 성공적으로 능가했다. 이러한 아키텍처(예: 정류된 선형 활성화(Nair and Hinton, 2010), 드롭아웃(Srivastava et al., 2014), 스킵 연결(He et al., 2016) 및 배치 정규화(Ioffe and Szegedy, 2015))에 의해 사용되는 설계 결정 및 정규화 기술은 오늘날 사용되는 많은 효과적인 CNN 모델의 백본을 형성한다.

객체 분류와 동시에 객체 _탐지_ 즉, 이미지 내에서 관심 있는 모든 객체를 분리하고 특정 클래스로 태그 지정에 상당한 진전이 있었다. 이러한 작업은 이미지 캡셔닝에서부터 자율 주행 차량에 이르기까지 다양한 다운스트림 문제와 관련이 있다. 예측이 _국소화되어야 하기 때문에 더 세분화된 접근이 필요하며, 따라서 종종 번역 불변 모델이 이 영역에서 가치를 입증했다. 이 공간에서 한 가지 영향력 있는 예는 모델의 R-CNN 계열(Girshick et al., 2014; Girshick, 2015; Ren et al., 2015; He et al., 2017)을 포함하는 반면, _의미 분할_ 관련 분야에서 Badrinarayanan et al.(2017)의 SegNet 모델은 VGG-16 백본에 의존하는 인코더-디코더 아키텍처로 영향력이 있는 것으로 입증되었다.

게임 플레이 컨볼루션 신경망은 또한 관찰된 상태가 그리드 도메인에서 표현될 수 있을 때마다 _강화 학습_ (RL) 환경에서 번역 불변 특징 추출기로서 두드러진 역할을 한다; 예를 들어, 이는 픽셀로부터 비디오 게임을 플레이하도록 학습하는 경우이다. 이 경우, CNN은 입력을 플랫 벡터 표현으로 줄이는 역할을 하며, 이는 RL 에이전트의 행동을 유도하는 _정책_ 또는 _값 함수_를 유도하는 데 사용된다. 강화 학습의 세부 사항은 이 섹션의 초점이 아니지만, 우리는 지난 10년 동안 딥 러닝의 가장 영향력 있는 결과 중 일부가 CNN 지원 강화 학습을 통해 발생했다는 점에 주목한다.

여기서 확실히 언급할 가치가 있는 한 가지 구체적인 예는 DeepMind의 _AlphaGo_(Silver et al., 2016)이다. 배치된 돌의 현재 위치를 나타내는 \(19\times 19\) 격자에 CNN을 적용하여 바둑 게임 내의 현재 상태를 인코딩한다. 이어 이전 전문가 동작 학습과 몬테카를로 트리 탐색, 셀프 플레이의 조합을 통해 전 세계적으로 널리 알려진 5라운드 챌린지 매치에서 역대 바둑 최강자 중 한 명인 이세돌을 능가할 정도로 바둑 숙달 수준에 성공적으로 도달했다.

이것은 이미 바둑이 체스보다 훨씬 더 복잡한 상태 공간을 갖는 광범위한 인공지능의 중요한 이정표를 나타냈지만 알파고의 발전은 거기서 멈추지 않았다. 저자들은 아키텍쳐에서 점점 더 많은 Go-specific bias를 제거하였으며, 알파고 제로(AlphaGo Zero)는 인간의 bias를 제거하고 순수하게 self-play(Silver et al., 2017)를 통해 최적화한다. 알파제로(AlphaZero)는 이 알고리즘을 체스(Chess)와 쇼기(Shogi)와 같은 관련 2인용 게임으로 확장한다. 마지막으로, _MuZero_(Schrittwieser et al., 2020)는 규칙의 사전 지식 없이 아타리 2600 콘솔과 바둑, 체스, 쇼기(Shogi)에서 강력한 성능에 도달할 수 있는 게임 규칙을 즉시 학습할 수 있는 모델을 통합한다. 이러한 모든 개발 전반에 걸쳐 CNN은 이러한 모델의 입력 표현 뒤에 백본으로 남아 있었다.

수년에 걸쳐 아타리 2600 플랫폼에 대해 여러 고성능 RL 에이전트가 제안되었지만(Mnih 등, 2015, 2016; Schulman 등, 2017), 오랫동안 제공된 57개 게임의 _all_에서 인간 수준의 성능에 도달할 수 없었다. 이 장벽은 마침내 에이전트57(Badia et al., 2020)과 함께 깨졌는데, 에이전트57은 강력한 탐색적 정책에서 순수하게 착취적 정책에 이르기까지 파라메트릭 정책 패밀리를 사용하고 훈련의 여러 단계에서 다른 방식으로 우선순위를 매겼다. 또한, 그것은 비디오 게임의 프레임 버퍼에 적용된 CNN에 의해 그것의 계산의 대부분을 구동한다.

텍스트와 음성 합성 외에도 (자연스럽게 _2차원 그리드에 매핑되는) 몇 가지 (기하학적) 딥 러닝의 가장 강력한 성공은 1차원 그리드에서 발생했다. 이것의 자연적인 예는 자연어 처리 및 디지털 신호 처리와 같은 다양한 영역 내에서 기하학 딥 러닝 청사진을 접는 _텍스트_ 및 _음성_이다.

이 공간에서 가장 널리 적용되고 널리 퍼진 작업 중 일부는 _합성_ 에 중점을 둡니다: 무조건적으로 또는 특정 _prompt_ 를 조건으로 음성 또는 텍스트를 생성할 수 있습니다. 이러한 설정은 _text-to-speech_ (TTS), 예측 텍스트 완료 및 기계 번역과 같은 많은 유용한 작업을 지원할 수 있습니다. 텍스트 및 음성 생성을 위한 다양한 신경망 구조가 지난 10년 동안 제안되었으며, 초기에는 대부분 _recurrent_ 신경망(예를 들어, 전술한 seq2seq 모델(Sutskever et al., 2014)) 또는 반복 주의(Bahdanau et al., 2014))를 기반으로 한다. 그러나 최근에는 합성곱 신경망과 트랜스포머 기반 아키텍처로 점차 대체되고 있다.

이 설정에서 간단한 1D 컨벌루션의 한 가지 특별한 제한은 선형적으로 성장하는 _수용 필드_이며, 지금까지 생성된 시퀀스를 커버하기 위해 많은 계층이 필요하다. _ 대신 Dilated_convolutions는 등가 수의 매개 변수를 사용하여 _지수적으로 증가하는 수용 필드를 제공합니다. 이로 인해, 그들은 기계 번역(Kalchbrenner et al., 2016)에서 RNN과 경쟁하면서 모든 입력 위치에 대한 병렬성으로 인해 계산 복잡도를 크게 줄이는 매우 강력한 대안을 입증했다. 확장된 컨볼루션의 가장 잘 알려진 적용은 van den Oord 등(2016)의 _WaveNet_ 모델이다. WaveNets는 확장을 사용하여 _raw 파형_ 수준(일반적으로 초당 16,000개 이상의 샘플)에서 음성을 합성할 수 있음을 보여주었으며, 최고의 이전 텍스트 음성 변환(TTS) 시스템보다 훨씬 더 "인간과 유사한" 음성 샘플을 생성했다. 그 후, WaveNets의 계산은 훨씬 더 간단한 모델인 _WaveRNN_(Kalchbrenner et al., 2018)에서 증류될 수 있으며, 이 모델은 산업 규모에서 이 기술을 효과적으로 배치할 수 있음을 추가로 입증했다. 이를 통해 Google Assistant와 같은 서비스에 대한 대규모 음성 생성을 위한 배포뿐만 아니라 종단 간 암호화를 사용하는 Google Duo와 같은 효율적인 온-디바이스 계산을 가능하게 했다.

트랜스포머(바스와니 등, 2017)는 순환 및 컨볼루션 아키텍처 모두의 한계를 능가하여 _자기 주의_가 기계 번역에서 최첨단 성능을 달성하는 데 충분함을 보여준다. 그 후, 그들은 자연어 처리에 혁명을 일으켰다. BERT(Devlin et al., 2018)와 같은 모델에 의해 제공되는 사전 훈련된 임베딩을 통해, 트랜스포머 계산은 자연어 처리의 많은 양의 다운스트림 애플리케이션에 대해 가능하게 되었다. 예를 들어, 구글은 검색 엔진에 전력을 공급하기 위해 BERT 임베딩을 사용한다.

지난 몇 년 동안 가장 널리 퍼진 트랜스포머의 적용은 주로 OpenAI의 모델 계열인 _Generative Pre-trained Transformer_(GPT, Radford et al. (2018, 2019); Brown et al. (2020))에 의해 자극된 텍스트 생성이다. 특히, GPT-3(Brown et al., 2020)은 스크래핑된 텍스트 말뭉치의 웹 스케일 양에 대한 다음 단어 예측에 대해 훈련된 1,750억 개의 학습 가능한 파라미터로 언어 모델 학습을 성공적으로 스케일링하였다. 이를 통해 다양한 언어 기반 과제에서 높은 잠재력을 가진 소수 샷 학습자가 될 수 있을 뿐만 아니라 일관되고 인간다운 텍스트 조각을 생성할 수 있는 능력을 갖춘 텍스트 생성기가 될 수 있었다. 이 기능은 많은 양의 다운스트림 응용 프로그램을 암시할 뿐만 아니라 방대한 미디어 보도를 유도했다.

의료 분야의 의료 응용 분야는 기하학 딥 러닝의 또 다른 유망한 분야이다. 이러한 방법들이 사용되고 있는 방법은 여러 가지가 있다. 먼저, CNN과 같은 보다 전통적인 아키텍처는 예를 들어, 중환자실에서의 체류 기간의 예측(Rocheteau et al., 2020), 또는 망막 스캔으로부터 시력을 위협하는 질병의 진단을 위해 그리드-구조화된 데이터에 적용되었다(De Fauw et al., 2018). Winkels and Cohen (2019)은 3D 로토-번역 그룹 컨볼루션 네트워크를 사용하는 것이 기존의 CNN에 비해 폐 결절 검출의 정확도를 향상시킨다는 것을 보여주었다.

둘째, 기하학적 표면으로 장기를 모델링하는 메쉬 합성곱 신경망은 유전학 관련 정보(Mahdi et al., 2020)에서 얼굴 구조를 재구성하는 것(Cucurull et al., 2018)에서 피질 표면 구조로부터 인구통계학적 특성을 회귀하는 것(Besson et al., 2020)까지 다양한 작업을 다룰 수 있는 것으로 나타났다. 후자의 예는 뇌를 고도로 비유클리드 구조를 생성하는 복잡한 주름의 표면으로 간주하려는 신경과학의 증가 추세를 나타낸다.

동시에 신경과학자들은 종종 어떤 인지 기능을 수행할 때 함께 활성화되는 뇌의 다양한 영역을 나타내는 뇌의 _기능적 네트워크_를 구성하고 분석하려고 시도하며, 이러한 네트워크는 종종 뇌의 어떤 영역이 혈액을 더 많이 소비하는지 실시간으로 보여주는 기능적 자기 공명 영상(fMRI)을 사용하여 구성된다. 이러한 기능적 네트워크는 환자의 인구통계학적(예를 들어, 남성과 여성을 구별하여, Arslan 등(2018))을 드러낼 수 있을 뿐만 아니라 신경병리학적 진단에 사용될 수 있으며, 이는 우리가 여기에서 강조하고자 하는 의학에서 기하 딥러닝의 세 번째 응용 분야이다. 이러한 맥락에서 Ktena et al.(2017)은 자폐 스펙트럼 장애와 같은 신경학적 상태의 예측을 위한 그래프 신경망의 사용을 개척하였다. 뇌의 기하학적, 기능적 구조는 밀접한 관련이 있는 것으로 보이며, 최근 이란과 타누(2021)는 신경 질환 분석에서 이들을 공동으로 활용하는 것의 이점을 지적했다.

넷째, ML 기반 의료 진단에서 _환자 네트워크_가 더욱 두드러지고 있다. 이러한 방법의 근거는 환자 인구통계학적, 유전형 및 표현형 유사성의 정보가 질병 예측을 개선할 수 있다는 것이다. Parisot 등(2018)은 신경학적 질환 진단을 위해 인구통계학적 특징으로부터 생성된 환자의 네트워크에 그래프 신경망을 적용하여, 그래프의 사용이 예측 결과를 향상시킨다는 것을 보여주었다. Cosmo 등(2020)은 이 설정에서 잠재 그래프 학습의 이점(네트워크가 미지의 환자 그래프를 _학습_ 함)을 보여주었다. 후자의 작업은 뇌 이미징을 포함한 의료 데이터의 대규모 수집인 영국 바이오뱅크의 데이터를 사용했다(Miller et al., 2016).

병원 환자에 대한 풍부한 데이터는 _전자 건강 기록_ (EHR)에서 찾을 수 있습니다. 환자의 진행에 대한 포괄적인 관점을 제공하는 것 외에도 EHR 분석을 통해 유사한 환자를 함께 _연관시킬 수 있다. 이는 진단에 일반적으로 사용되는 _패턴 인식 방법_ 과 일치합니다. 따라서 임상의는 _경험_ 을 사용하여 임상 특성의 패턴을 인식하며 임상의의 경험이 상태를 신속하게 진단할 수 있을 때 사용되는 주요 방법일 수 있다. 이러한 선을 따라 여러 연구에서는 EHR 데이터를 기반으로 의사 노트의 임베딩(Malone et al., 2018), 입원 시 진단 유사성(Rocheteau et al., 2021) 또는 완전히 연결된 그래프(Zhu and Razavian, 2019)를 분석하여 환자 그래프를 구성하려고 시도한다. 모든 경우에, EHR을 처리하기 위해 그래프 표현 학습을 사용하는 것이 유리한 결과가 나타났다.

입자 물리학과 천체 물리학 하이 에너지 물리학자들은 아마도 새로운 빛나는 도구인 그래프 신경망을 수용한 자연 과학 분야의 첫 번째 도메인 전문가 중 하나일 것이다. 최근 리뷰 논문에서 Shlomi et al. (2020)은 기계 학습이 역사적으로 입자 물리 실험에 많이 사용되었으며, 검출기에서 측정된 정보로부터 기본 물리 과정을 추론하거나 분류 및 회귀 작업을 수행할 수 있는 복잡한 역함수를 학습한다. 후자의 경우 CNN과 같은 표준 딥러닝 아키텍처를 사용할 수 있기 위해 데이터를 그리드와 같은 부자연스러운 표현으로 강제할 필요가 종종 있었다. 그러나 물리학의 많은 문제는 풍부한 관계와 상호 작용을 가진 무질서 집합 형태의 데이터를 포함하며, 이는 자연스럽게 그래프로 표현될 수 있다.

고에너지 물리학에서 한 가지 중요한 응용은 입자 제트의 재구성 및 분류이다. CERN에 구축된 가장 크고 가장 잘 알려진 입자 가속기인 Large Hardon Collider에서 이러한 제트는 거의 빛의 속도로 양성자가 충돌한 결과이다. 이러한 충돌은 힉스 보손이나 꼭대기 쿼크와 같은 거대한 입자를 생성한다. 충돌 사건의 식별 및 분류는 새로운 입자의 존재에 대한 실험적 증거를 제공할 수 있기 때문에 매우 중요하다.

최근 Komiske et al. (2019) 및 Qu and Gouskos (2019)에 의해 DeepSet 및 Dynamic Graph CNN 아키텍처를 기반으로 하는 입자 제트 분류 작업에 대해 다중 기하학적 딥 러닝 접근법이 제안되었다. 보다 최근에, 또한, 물리학적 고려로부터 유래된 전문화된 아키텍처를 개발하고, 해밀토니안 또는 라그랑지안 역학(예를 들어, 산체스-곤잘레스 등(2019); 크랜머 등(2020)), 로렌츠 그룹(물리학에서 공간과 시간의 기본 대칭)에 대한 등분산성(보가츠키 등, 2020), 또는 심지어 기호 추론(크랜머 등, 2019)을 통합하고 데이터로부터 물리 법칙을 학습할 수 있는 것에 대한 관심이 있었다. 이러한 접근법은 더 해석 가능하고(따라서 도메인 전문가에 의해 더 '신뢰할 수 있는' 것으로 간주됨) 더 나은 일반화를 제공한다.

입자 가속기 외에도, 입자 탐지기는 현재 천체물리학자에 의해 다중 메신저 천문학을 위해 사용되고 있습니다. - 전자기 복사, 중력파, 중성미자와 같은 서로 다른 신호를 같은 소스에서 오는 새로운 방식으로 조정하는 것입니다. 중성미자는 물질과 거의 상호작용하지 않으므로 실질적으로 영향을 받지 않고 엄청난 거리를 이동하기 때문에 중성미자 천문학이 특히 중요하다. 중성미자를 탐지하는 것은 광학 망원경에 접근할 수 없는 물체를 관찰할 수 있지만 엄청나게 큰 크기의 탐지기를 필요로 한다 - 아이스큐브 중성미자 관측소는 남극 대륙 빙붕의 입방 킬로미터를 탐지기로 사용한다. 고에너지 중성미자를 탐지하면 블레이저와 블랙홀과 같은 우주에서 가장 신비로운 물체들 중 일부를 밝힐 수 있다. Choma et al. (2018)은 Geometric neural network를 사용하여 IceCube 중성미자 검출기의 불규칙한 기하학을 모델링하여 천체 물리학 소스에서 나오는 중성미자를 검출하고 배경 이벤트에서 분리하는 데 훨씬 더 나은 성능을 보여주었다.

중성미자 천문학은 코스모스 연구에서 큰 가능성을 제시하지만, 전통적인 광학 망원경과 전파 망원경은 여전히 천문학자들의 '전투 말'이다. 이러한 전통적인 도구를 통해 기하 딥러닝은 여전히 데이터 분석을 위한 새로운 방법론을 제공할 수 있다. 예를 들어, Scaife와 Porter (2021)는 전파은하의 분류를 위해 회전-등분 CNN을 사용했고, McEwen 등 (2021)은 원시 우주의 형성을 밝힐 수 있는 빅뱅의 유물인 우주 마이크로파 배경 복사의 분석을 위해 구형 CNN을 사용했다. 이미 언급했듯이 그러한 신호는 구에 자연스럽게 표현되며 등분산 신경망은 이를 연구하기 위한 적절한 도구이다.

가상 및 증강 현실 대규모 기하학 딥 러닝 방법의 개발 동기로 작용한 또 다른 응용 분야는 특히 가상 및 증강 현실을 위한 3D 신체 모델을 다루는 컴퓨터 비전 및 그래픽이다. 아바타와 같은 영화에서 특수 효과를 생성하기 위해 사용되는 모션 캡처 기술은 종종 두 단계로 작동한다: 먼저, 신체의 모션 또는 배우의 얼굴을 캡처하는 3D 스캐너로부터의 입력은 일반적으로 이산 매니폴드 또는 메쉬로 모델링되는 일부 표준 형상과 대응되게 투입된다(이 문제는 종종 '분석'으로 불린다). 둘째, 새로운 형상을 생성하여 입력의 동작을 반복한다('합성'). 컴퓨터 그래픽 및 비전에서의 지오메트릭 딥 러닝에 관한 초기 작업들(Masci et al., 2015; Boscaini et al., 2016; Monti et al., 2017)은 분석 문제, 또는 보다 구체적으로 변형 가능한 형상 대응을 다루기 위해 메쉬 컨볼루션 신경망을 개발했다.

3D 형상 합성을 위한 최초의 기하학적 오토인코더 아키텍처는 Litany 등(2018)과 Ranjan 등(2018)에 의해 독립적으로 제안되었다. 이러한 구조에서는 (신체, 얼굴 또는 손의) 표준 메쉬를 알고 있는 것으로 가정하였으며, 합성 작업은 노드의 3D 좌표(미분 기하학의 전문 용어를 사용하여 표면의 임베딩)를 회귀하는 것으로 구성하였다. Kulon et al.(2020)은 이미지 CNN 기반 인코더 및 기하학적 디코더를 갖는 3D 손 포즈 추정을 위한 하이브리드 파이프라인을 보여주었다. 영국 스타트업 기업 아리엘 AI와 협업해 개발해 CVPR 2020에 선보인 이 시스템의 데모는 휴대전화로 입력되는 비디오에서 완전히 관절이 달린 손으로 실감나는 바디 아바타를 실시간보다 빠르게 만들 수 있도록 했다. 애리얼 AI는 2020년 스냅에 인수됐고, 기술 작성 당시 스냅의 증강현실 제품에 활용된다.

## 7 역사적 관점

“대칭은 그 의미를 정의할 수 있을 만큼 넓거나 좁다. 세월이 흐르면서 인간이 질서, 아름다움, 완벽함을 이해하고 창조하려고 했던 생각 중 하나다.” 이 대칭에 대한 다소 시적인 정의는 프린스턴 고등 연구소의 은퇴 전날 위대한 수학자 헤르만 바일(2015)의 시조서, 그의 _슈바넹상_ 에 나와 있다. 바일은 수메르의 대칭 디자인에서 원은 회전 대칭으로 인해 완벽하다고 믿었던 피타고라스인들에 이르기까지 과학과 예술에서 특별한 장소 대칭이 차지해 온 흔적을 추적한다. 플라톤은 오늘날 그의 이름이 새겨진 다섯 개의 정다면체를 매우 근본적인 것으로 간주하여 물질 세계를 형성하는 기본 구성 요소가 되어야 한다. 그러나 플라톤은 문자 그대로 '같은 척도'로 번역되는 \(\sigma\)\(\sigma\)\(\sigma\)\(\sigma\)\(\sigma\)\(\sigma\)라는 용어를 사용한 것으로 알려져 있지만, 그는 예술과 음악의 조화에서 비례의 아름다움을 전달하기 위해 막연하게만 사용했다. 천문학자와 수학자 요하네스 케플러는 물 결정의 대칭적 형태에 대한 최초의 엄격한 분석을 시도했다.

수학과 물리학의 대칭 현대수학에서 대칭은 거의 단원적으로 군 이론의 언어로 표현된다. 이 이론의 기원은 대개 이 용어를 만들어 1830년대에 다항 방정식의 풀이 가능성을 연구하는 데 사용한 Evariste Galois에 기인한다. 그룹 이론과 관련된 두 개의 다른 이름은 소푸스 리와 펠릭스 클라인의 이름으로, 일정 기간 동안 만나서 알차게 함께 일했다(토비스, 2019). 전자는 오늘날 그의 이름을 가진 연속 대칭 이론을 발전시킬 것이고, 후자는 그룹 이론을 우리가 이 텍스트의 시작 부분에서 언급했던 그의 에를랑겐 프로그램에서 기하학의 조직화 원리로 선언할 것이다. 리만 기하학은 클라인의 통일된 기하학 그림에서 명시적으로 제외되었고, 통합되기까지 50년이 더 걸렸는데, 이는 1920년대 엘리 카르탄의 작품 덕분이다.

고팅겐에 있는 클라인의 동료인 에미 뇌터는 물리계의 작용의 미분 가능한 모든 대칭이 상응하는 보존 법칙을 가지고 있다는 것을 증명했다. 물리학에서 그것은 놀라운 결과였다: 사전에 에너지의 보존과 같은 근본적인 법칙을 발견하기 위해 세심한 실험적 관찰이 필요했고, 그때도 그것은 어디서도 나오지 않는 경험적 결과였다. 노벨상 수상자 프랭크 윌체크(Frank Wilczek)의 말에 따르면, "20세기와 21세기의 물리학을 이끄는 별"이라는 노더의 정리는 에너지의 보존이 시간의 번역 대칭에서 나온다는 것을 보여주었는데, 실험의 결과가 오늘이나 내일 실시되느냐에 달려서는 안 된다는 다소 직관적인 생각이었다.

전하 보존과 관련된 대칭은 전자기장의 글로벌 _게이지 불변이며, 이는 맥스웰의 전기역학 공식(맥스웰, 1865)에서 처음 등장하지만, 그 중요성은 처음에는 눈에 띄지 않았다. 대칭에 대해 그렇게 교묘하게 쓴 같은 헤르만 바일은 20세기 초 물리학에서 게이지 불변의 개념을 처음 도입한 사람으로 전자기학을 _유래할 수 있는 원리로서의 역할을 강조한다. 양과 밀스(1954)가 개발한 일반적인 형태의 이 기본 원리가 전자기학의 양자역학적 거동과 약력과 강력을 설명하는 통일된 틀을 제공하는 데 성공하기까지 수십 년이 걸렸고, 마침내 중력을 제외한 자연의 모든 기본 힘을 포착하는 표준 모형에서 정점을 찍었다. 따라서 우리는 또 다른 노벨상을 받은 물리학자 필립 앤더슨(1972)과 함께 “물리학이 대칭의 연구라고 말하는 것은 단지 그 경우를 약간 과장하는 것”이라고 결론지을 수 있다. 머신러닝에서 대칭의 초기 사용 머신러닝과 패턴 인식과 컴퓨터 비전에 대한 응용에서 대칭의 중요성은 오랫동안 인식되어 왔다. 패턴 인식을 위한 등분산 특징 검출기를 설계하는 초기 작업은 아마리(1978), 가나타니(2012), 렌츠(1990)에 의해 수행되었다. 신경망 문헌에서 민스키와 페이퍼트(2017)의 유명한 그룹 퍼셉트론에 대한 불변 정리는 불변물을 배우는 (단층) 퍼셉트론의 능력에 근본적인 한계를 둔다. 이것은 다층 아키텍처를 연구하기 위한 주요 동기 중 하나였으며(Sejnowski et al., 1986; Shawe-Taylor, 1989, 1993), 이는 궁극적으로 딥 러닝으로 이어졌다.

신경망 커뮤니티에서 _Neocognitron_(후쿠시마와 미야케, 1982)은 "위치의 이동에 영향을 받지 않는 패턴 인식"을 위해 신경망에서 시프트 불변성의 첫 번째 구현으로 간주된다. 그의 해결책은 20년 전 신경과학자인 데이비드 휴벨과 토스텐 위젤(휴벨과 위젤, 1959)에 의해 시각 피질에서 발견된 수용 필드에서 영감을 끌어내면서 국소 연결성을 가진 계층적 신경망의 형태로 나왔다. 이 아이디어들은 Yann LeCun과 공저자들의 중요한 작업에서 Convolutional Neural Networks에서 정점을 찍었다 (LeCun et al., 1998). 불변 및 등분산 신경망에 대한 재현 이론적 관점을 취한 첫 번째 작업은 불행히도 거의 인용되지 않은 Wood와 Shawe-Taylor(1996)에 의해 수행되었다. 이러한 사상의 보다 최근의 화신에는 Makadia et al.(2007), Esteves et al.(2020) 및 이 텍스트의 저자 중 한 명(Cohen and Welling, 2016)의 작품이 포함된다.

그래프 신경망 그래프 신경망의 개념이 언제 등장하기 시작했는지 정확히 지적하기는 어렵다. 부분적으로 GNN이 2010년대 후반에야 실용화되었고 부분적으로 이 분야가 여러 연구 분야의 합류에서 나왔기 때문에 초기 작업의 대부분이 1등급 시민으로 그래프를 배치하지 않았기 때문이다. 말하자면, 그래프 신경망의 초기 형태는 적어도 1990년대로 거슬러 올라갈 수 있는데, 예를 들어 알레산드로 스페르두티의 라벨링 RAAM(Sperduti, 1994), 골러와 쿠클러(Goller and Kuchler, 1996)의 "역전파 관통 구조" 및 데이터 구조의 적응 처리(Sperduti and Starita, 1997; Frasconi et al., 1998)가 있다. 이러한 작업은 주로 "구조"(종종 나무 또는 지시된 비순환 그래프)를 통해 작동하는 것과 관련이 있었지만 아키텍처에 보존된 많은 불변은 오늘날 더 일반적으로 사용되는 GNN을 연상시킨다.

일반 그래프 구조의 처리에 대한 첫 번째 적절한 처리(및 용어 _"그래프 신경망"_의 코이닝)는 21세기의 전환 후에 발생했다. 유니버시타 델리 스터디 디 시에나(이탈리아)의 인공지능 연구실 내에서 마르코 고리와 프랑코 스카셀리가 이끄는 논문은 최초의 "GNN"(Gori et al., 2005; Scarselli et al., 2008)을 제안했다. 그들은 순환 메커니즘에 의존했고, 신경망 매개변수가 _수축 매핑_을 지정하도록 요구했으며, 따라서 고정된 점을 검색하여 노드 표현을 계산했는데, 이는 그 자체로 특별한 형태의 역전파(알메이다, 1990; 피네다, 1988)를 필요로 했으며 노드 특징에 전혀 의존하지 않았다. 위의 모든 이슈들은 Li et al.(2015)의 Gated GNN(GGNN) 모델에 의해 수정되었다. GGNN은 게이팅 메커니즘(Cho et al., 2014) 및 시간을 통한 역전파(backpropagation)와 같은 현대 RNN의 많은 이점을 GNN 모델에 가져왔으며 오늘날에도 인기를 유지하고 있다.

계산 화학 GNN에 대한 독립적이고 동시적인 개발 라인을 주목하는 것도 매우 중요하다: 분자는 화학 결합(엣지)에 의해 연결된 원자(노드)의 그래프로 가장 자연스럽게 표현되는 계산 화학의 필요에 의해 전적으로 주도된 것이다. 이것은 1990년대에 기계 학습에 존재하게 된 이러한 그래프 구조를 통해 직접 작동하는 분자 특성 예측을 위한 계산 기술을 초대했는데, 여기에는 Kireev(1995)의 ChemNet 모델과 Baskin 등(1997)의 작업이 포함된다. 놀랍게도, 머크워스와 렝가우어(2005)의 "분자 그래프 네트워크"는 2005년 초에 에지 유형 조건 가중치 또는 전역 풀링과 같은 현대 GNN에서 일반적으로 발견되는 많은 요소를 명시적으로 제안했다. 화학적 동기는 2010년대에 GNN 개발을 계속 추진했으며, 두 가지 중요한 GNN 발전은 분자 지문 개선(Duvenaud 등, 2015)과 소분자로부터 양자 화학적 특성 예측(Gilmer 등, 2017)을 중심으로 했다. 이 텍스트를 작성할 때 분자 특성 예측은 GNN의 가장 성공적인 적용 중 하나이며, 새로운 항생제 약물의 가상 스크리닝에 영향을 미친다(Stokes et al., 2020).

노드 임베딩 딥 러닝의 초기 성공 사례들 중 일부는 그래프 구조에 기초하여, 지도되지 않은 방식으로 노드들의 표현들을 학습하는 것을 포함한다. 그들의 구조적 영감을 감안할 때, 이 방향은 또한 그래프 표현 학습과 네트워크 과학 커뮤니티 사이의 가장 직접적인 연결 중 하나를 제공한다. 이 공간의 초기 키 접근 방식은 _랜덤 워크_ 기반 임베딩에 의존했습니다. 노드가 짧은 랜덤 워크에서 함께 발생하는 경우 노드 표현을 더 가깝게 만드는 방식으로 학습합니다. 이 공간에서의 대표적인 방법으로는 DeepWalk(Perozzi et al., 2014), node2vec(Grover and Leskovec, 2016) 및 LINE(Tang et al., 2015) 등이 있으며, 이들은 모두 순전히 자기 지도이다. 플래너노이드(Yang et al., 2016)는 이 공간에서 사용 가능한 경우 감독 레이블 정보를 통합한 최초의 것이다.

GNN 인코더로 무작위 보행 목표를 통합하는 것은 Variational Graph Autoencoder(VGAE, Kipf and Welling(2016)), embedding propagation(Garcia-Duran and Niepert, 2017), GraphSAGE의 비감독 변형(Hamilton et al., 2017)을 포함한 대표적인 접근법으로 여러 번 시도되었다. 그러나 이것은 혼합된 결과를 얻었고, 이웃 노드 표현을 함께 밀어내는 것이 이미 GNN의 귀납적 편향의 핵심 부분이라는 것이 곧 발견되었다. 실제로, _훈련되지 않은_ GNN은 노드 기능을 사용할 수 있는 설정에서 DeepWalk와 경쟁하는 성능을 이미 보여주고 있는 것으로 나타났다(Velickovic 등, 2019; Wu 등, 2019). 이것은 무작위 보행 목표를 GNN과 결합하는 것에서 벗어나 상호 정보 최대화에서 영감을 받은 _대비적_ 접근 방식으로 이동하고 이미지 도메인의 성공적인 방법에 정렬하는 방향을 시작했다. 이러한 방향의 두드러진 예로는 Deep Graph Informax(DGI, Velickovic et al. (2019)), GRACE(Zhu et al., 2020), BERT-like objectives(Hu et al., 2020) 및 BGRL(Thakoor et al., 2021) 등이 있다.

확률적 그래픽 모델 그래프 신경망은 또한 동시에 _확률적 그래픽 모델_ (PGMs, Wainwright 및 Jordan (2008))의 계산을 내장함으로써 다시 나타났습니다. PGM은 그래픽 데이터를 처리하는 강력한 도구이며, 그 유용성은 그래프의 가장자리에 대한 확률론적 관점에서 발생하며, 즉 노드는 _랜덤 변수_로 처리되는 반면 그래프 구조는 _조건부 독립성_ 가정을 인코딩하여 결합 분포에서 계산 및 샘플링을 상당히 단순화할 수 있다. 실제로, PGM들에 대한 학습 및 추론을 (정확하게 또는 대략적으로) 지원하기 위한 많은 알고리즘들은 변분 평균-장 추론 및 루피 신념 전파를 포함하는 예들과 함께 그들의 에지들 위에 지나가는 메시지들의 형태들에 의존한다(Pearl, 2014). (Yedidia 등, 2001; Murphy 등, 2013).

PGM과 메시지 전달 사이의 이러한 연결은 구조 2vec(Dai 등, 2016)의 저자에 의해 확립된 초기 이론적 링크와 함께 후속적으로 GNN 아키텍처로 개발되었다. 즉, 그래프 표현 학습 설정을 (입력 특징 및 잠재 표현에 대응하는 노드의) 마르코프 랜덤 필드로 상정함으로써, 저자들은 평균-필드 추론 및 루피 믿음 전파 모두의 계산을 오늘날 일반적으로 사용되는 GNN과 다르지 않은 모델에 직접 정렬한다.

GNN의 잠재 표현들을 PGM에 의해 유지되는 확률 분포들에 관련시키는 것을 허용하는 핵심 "트릭"은 분포들의 _힐버트-공간 임베딩들_ 의 사용이었다 (Smola et al., 2007). 특성 \(\mathbf{x}\)에 대해 적절하게 선택된 임베딩 함수 \(\phi\)가 주어지면, 확률 분포 \(p(\mathbf{x})\)를 _expected_ 임베딩 \(\mathbb{E}_{\mathbf{x}\sim p(\mathbf{x})}\phi(\mathbf{x})\)로 임베딩할 수 있다. 이러한 대응은 GNN에 의해 계산된 표현들이 노드 특징들에 대한 _some_ 확률 분포의 임베딩에 항상 대응할 것이라는 것을 알고, GNN과 유사한 계산들을 수행할 수 있게 한다.

구조 2vec 모델 자체는 궁극적으로 우리의 프레임워크 내에 쉽게 앉을 수 있는 GNN 아키텍처이지만, 그 설정은 PGM에서 발견되는 계산을 보다 직접적으로 통합하는 일련의 GNN 아키텍처에 영감을 주었다. 새로운 예들은 GNN들을 조건부 랜덤 필드들(Gao et al., 2019; Spalevic et al., 2020), 관계형 마르코프 네트워크들(Qu et al., 2019) 및 마르코프 로직 네트워크들(Zhang et al., 2020)과 성공적으로 결합시켰다.

Weisfeiler-Lehman 형식론 그래프 신경망의 부활은 특히 표현력 측면에서 그들의 근본적인 한계를 이해하려는 추진으로 이어졌다. GNN은 그래프 구조 데이터의 강력한 모델링 도구임이 분명해지고 있지만 그래프에 지정된 _임의_ 작업을 완벽하게 해결할 수 없다는 것도 분명했다. 이에 대한 표준적인 예시는 _그래프 동형 결정_ 입니다: 우리의 GNN이 두 개의 주어진 비동형 그래프에 서로 다른 표현을 붙일 수 있습니까? 이것은 두 가지 이유로 유용한 틀이다. GNN이 이것을 할 수 없다면, 이 두 그래프의 구별을 요구하는 어떤 작업에도 희망이 없을 것이다. 또한, 그래프 동형을 결정하는 것이 일반적으로 모든 GNN 계산이 상주하는 복잡도 클래스인 P에 있는지 여부는 현재 알려져 있지 않다.

그래프 동형성에 GNN을 바인딩하는 주요 프레임워크는 _Weisfeiler-Lehman_ (WL) 그래프 동형 테스트 (Weisfeiler and Leman, 1968)이다. 이 테스트는 그래프의 가장자리를 따라 노드 특징을 반복적으로 전달한 다음 이웃에 걸쳐 합을 _랜덤 해싱_하여 그래프 표현을 생성합니다. Randomly-initialised_ 콘볼루션 GNN에 대한 연결은 명백하며, 예를 들어 Kipf 및 Welling(2016a)의 GCN 모델 내에서 일찍 관찰되었다. 이 연결 외에도 WL 반복은 이전에 Shervashidze 등(2011)에 의해 _그래프 커널_ 도메인에 도입되었으며 전체 그래프 표현의 비지도 학습에 대한 강력한 기준선을 여전히 제시한다.

WL 테스트는 개념적으로 단순하고 구별할 수 없는 비동형 그래프의 간단한 예가 많지만 표현력은 궁극적으로 GNN과 강하게 연결되어 있다. Morris 등(2019)과 Xu 등(2018)의 분석에서는 둘 다 놀라운 결론에 도달했다: 섹션 5.3에서 설명한 세 가지 맛 중 하나에 부합하는 _any_ GNN은 WL 테스트보다 더 강력할 수 없다.

이러한 수준의 표현력에 정확히 도달하기 위해서는 GNN 갱신 규칙에 일정한 제약이 존재해야 한다. Xu 등(2018)은 이산 특징 도메인에서 GNN이 사용하는 집계 함수는 _injective_여야 하며 _summation_이 핵심 대표자임을 보여주었다. 이들의 분석 결과를 바탕으로 Xu 등(2018)은 이 프레임워크 하에서 최대 표현 GNN의 간단하지만 강력한 예인 Graph Isomorphism Network(GIN)을 제안한다. 또한 제안하는 합성곱 GNN 풍미에서도 표현이 가능하다.

마지막으로 이러한 결과가 _연속_ 노드 피쳐 공간에 일반화되지 않는다는 점에 주목할 필요가 있습니다. 실제로, Borsuk-Ulam 정리(Borsuk, 1933)를 사용하여 Corso 등(2020)은 실수 노드 특징을 가정할 때, 주입적 집계 함수를 얻는 것은 _multiple_ 집계기(구체적으로, 수신기 노드의 _degree_와 동일함)를 필요로 한다는 것을 입증했다. 그들의 발견은 경험적으로 강력하고 안정적인 다중 응집기 GNN을 제안하는 Principal Neighbourhood Aggregation(PNA) 아키텍처를 주도했다.

고차 방법 이전 단락의 결과는 GNN의 실용적인 유용성과 모순되지 않는다. 실제로, 많은 실제 응용 프로그램에서 입력 기능은 위의 제한 사항에도 불구하고 그래프 구조에 대한 유용한 판별 계산을 지원하기에 충분히 _풍부_하다.

그러나 한 가지 핵심 결과는 GNN이 그래프 내에서 일부 기본 _구조_를 감지하는 데 상대적으로 매우 약하다는 것이다. WL 테스트의 특정 제한 사항이나 실패 사례에 따라 여러 작업은 WL 테스트보다 더 강력한 GNN의 _강력한_ 변형을 제공하므로 이러한 구조적 탐지가 필요한 작업에 유용할 수 있다.

아마도 더 표현력 있는 GNN을 찾기 위한 가장 직접적인 장소는 WL 테스트 그 자체일 것이다. 실제로, 원래 WL 테스트의 강도는 WL 테스트의 _계층_ 을 고려함으로써 향상될 수 있으며, 따라서 \(k\)-WL 테스트는 노드들의 \(k\)-_tuples_ 에 표현을 부착한다(Morris et al., 2017). \(k\)-WL 테스트는 Morris et al.(2019)에 의해 _higher-order_\(k\)-GNN 아키텍처로 직접 변환되었으며, 이는 이전에 고려했던 GNN 맛보다 입증 가능하게 더 강력한다. 그러나 튜플 표현을 유지하기 위한 요구 사항은 실제로 \(k=3\)을 초과하여 확장하기가 어렵다는 것을 의미한다.

동시에 Maron et al.(2018, 2019)은 노드의 \(k\)-투플에 대한 불변 및 등분산 그래프 네트워크의 특성을 연구했다. 임의의 불변 또는 등분산 그래프 네트워크가 유한한 수의 생성기의 선형 조합으로 표현될 수 있다는 놀라운 결과를 입증하는 것 외에도, 그 양은 \(k\)에만 의존한다. 저자들은 이러한 계층의 표현력이 \(k\)-WL 테스트와 동등하다는 것을 보여주었고, 증명 가능한 3-WL 강력한 경험적으로 확장 가능한 변형을 제안했다.

표현이 계산되는 영역을 일반화하는 것 외에도 1-WL의 특정 실패 사례를 분석하고 이러한 사례를 구별하는 데 도움이 되도록 GNN _입력_을 늘리는 데 상당한 노력을 기울였다. 한 가지 일반적인 예는 구조를 감지하는 데 도움이 될 수 있는 _기능 식별_ 을 노드에 부착하는 것입니다. 이를 위한 제안에는 순수한 _랜덤_ 기능뿐만 아니라 _one-hot_ 표현(Murphy et al., 2019)이 포함됩니다(Sato et al., 2020).

보다 광범위하게, 메시지 함수 또는 계산이 전달되는 그래프를 변조하여 메시지 전달 프로세스 내에 _구조적_ 정보를 통합하려는 많은 노력이 있었다. 여기에서 몇 가지 흥미로운 작업 라인은 샘플링 _앵커 노드 세트_(You et al., 2019), _Laplacian eigenvectors_(Stachenfeld et al., 2020; Beaini et al., 2020; Dwivedi and Bresson, 2020)를 기반으로 집계하거나 위치 임베딩(Bouritsas et al., 2020) 또는 구동 메시지 전달(Bodnar et al., 2021)을 위해 _토폴로지 데이터 분석을 수행하는 것입니다.

신호 처리 및 조화 분석은 합성곱 신경망의 초기 성공 이후 연구자들은 조화 분석, 이미지 처리 및 계산 신경 과학의 도구에 의존하여 효율성을 설명하는 이론적 프레임워크를 제공했다. \ (M\)-이론은 토마소 포지오와 협력자(Riesenhuber and Poggio, 1999; Serre et al., 2007)에 의해 개척된 시각 피질에서 영감을 받은 프레임워크로, 특정 대칭 그룹 하에서 조작될 수 있는 템플릿의 개념을 기반으로 한다. 계산 신경과학에서 발생하는 또 다른 주목할 만한 모델은 Simoncelli와 Freeman(1995)에 의해 개발된 특정 입력 변환에 대해 유리한 특성을 갖는 다중스케일 웨이블릿 분해의 형태인 _조향 가능한 피라미드_였다. 그들은 텍스쳐에 대한 초기 생성 모델(Portilla and Simoncelli, 2000)의 중심 요소였으며, 이는 조향 가능한 웨이블릿 특징을 심층 CNN 특징으로 대체함으로써 후속적으로 개선되었다(Gatys et al. (2015). 마지막으로 Stephane Mallat (2012)이 도입하고 Bruna and Mallat (2013)이 개발한 산란 변환은 훈련 가능한 필터를 다중 스케일 웨이블릿 분해로 대체하여 CNN을 이해할 수 있는 프레임워크를 제공했으며 변형 안정성과 아키텍처에서 깊이의 역할을 보여준다.

그래프 및 Meshes에 대한 신호 처리 종종 _스펙트럴_이라고 하는 또 다른 중요한 그래프 신경망 부류는 _그래프 푸리에 변환_이라는 개념을 사용하여 이 텍스트의 저자 중 한 명의 작업(브루나 등, 2013)에서 나왔다. 이 구성의 뿌리는 신호 처리 및 컴퓨터 하모닉 분석 커뮤니티에 있으며, 여기서 비 유클리드 신호를 다루는 것은 2000년대 후반과 2010년대 초에 두드러졌다. Pierre Vandergheynst (Shuman et al., 2013)와 Jose Moura (Sandryhaila and Moura, 2013) 그룹의 영향력 있는 논문은 "Graph Signal Processing"(GSP)의 개념과 그래프 인접성과 라플라시안 행렬의 고유 벡터를 기반으로 한 푸리에 변환의 일반화를 대중화했습니다. Defferrard et al.(2016)과 Kipf and Welling(2016)의 스펙트럼 필터에 의존하는 그래프 합성곱 신경망은 이 분야에서 가장 많이 인용되고 있으며 최근 몇 년 동안 그래프에 대한 기계 학습에 대한 관심을 재점화하는 것으로 인정받을 수 있다.

컴퓨터 그래픽 및 기하학 처리 분야에서 비유클리드 조화 분석은 그래프 신호 처리보다 최소 10년 앞서 있다는 점에 주목할 필요가 있다. 우리는 Taubin et al.(1996)의 연구를 통해 매니폴드와 메쉬에 대한 스펙트럼 필터를 추적할 수 있다. 이러한 방법은 스펙트럼 기하 압축에 대한 Karni와 Gotsman(2000)의 영향력 있는 논문과 라플라시안 고유벡터를 비유클리드 푸리에 기반으로 사용하는 Levy(2006)의 영향력 있는 논문에 이어 2000년대에 주류를 이루었다. 스펙트럼 방법은 다양한 응용 분야에 사용되어 왔으며, 그 중 가장 두드러진 것은 형상 기술자(Sun et al., 2009) 및 기능 맵(Ovsjanikov et al., 2012)의 구성이며, 이러한 방법은 작성 당시 컴퓨터 그래픽에서 여전히 광범위하게 사용된다.

내재적 메트릭 불변들에 기초한 형상 분석을 위한 컴퓨터 그래픽 및 지오메트리 프로세싱 모델들은 컴퓨터 그래픽 및 지오메트리 프로세싱 분야의 다양한 저자들에 의해 소개되었고(Elad and Kimmel, 2003; Memoli and Sapiro, 2005; Bronstein et al., 2006), 그의 이전 저자 중 하나에 의해 심도 있게 논의되었다(Bronstein et al., 2008). Raviv et al.(2007), Ovsjanikov et al.(2008)에서도 내재적 대칭에 대한 개념을 탐색하였다. 메쉬에 대한 딥 러닝을 위한 첫 번째 아키텍처인 지오데식 CNN은 텍스트의 저자 중 한 명으로 구성된 팀에서 개발되었다(Masci et al., 2015). 이 모델은 지오데식 방사형 패치에 적용된 공유 가중치를 가진 로컬 필터를 사용했다. 그것은 나중에 본문의 다른 저자에 의해 개발된 게이지-등변 CNN의 특정 설정이었다(Cohen et al., 2019). 동일한 팀에서 페데리코 몬티 등(2017)이 제안한 학습 가능한 집계 연산인 MoNet을 사용한 지오데식 CNN의 일반화는 일반적인 그래프에서도 작동하는 것으로 입증된 메쉬의 로컬 구조적 특징에 대해 주의와 같은 메커니즘을 사용했다. 기술적으로 MoNet의 특정 사례로 간주될 수 있는 그래프 어텐션 네트워크(GAT)는 이 텍스트의 다른 저자에 의해 도입되었다(Velickovic et al., 2018). GAT는 이전 작업의 순수하게 구조에서 파생된 관련성을 탈피하여 노드 특징 정보도 통합하기 위해 MoNet의 주의 메커니즘을 일반화한다. 현재 사용 중인 가장 인기 있는 GNN 아키텍처 중 하나입니다.

컴퓨터 그래픽의 맥락에서, 세트들에 대한 학습 아이디어(Zaheer et al., 2017)가 3D 포인트 클라우드들의 분석을 위해 PointNet(Qi et al., 2017)이라는 이름으로 스탠포드의 Leo Guibas 그룹에서 동시에 개발되었다는 것을 언급하는 것도 가치가 있다. 이 아키텍처는 동적 그래프 CNN(DGCNN, Wang 등(2019b))이라는 이 텍스트의 저자에 의해 하나를 포함하는 다수의 후속 작업으로 이어졌다. DGCNN은 노드들 간의 정보 교환을 허용하기 위해 포인트 클라우드의 로컬 구조를 캡처하기 위해 최근접-이웃 그래프를 사용했는데, 이 아키텍처의 주요 특징은 그래프가 다운스트림 태스크와 관련하여 신경망의 계층들 사이에서 즉시 구성되고 업데이트된다는 것이다. 이 후자의 속성은 DGCNN을 '잠재 그래프 학습'의 첫 번째 화신 중 하나로 만들었고, 이는 차례로 상당한 후속 조치를 취했다. DGCNN의 \(k\)-최근접 이웃 그래프 제안으로의 확장들은 더 명시적인 제어를 포함한다

[MISSING_PAGE_FAIL:128]

알고리즘 불변량을 적절하게 보존합니다. 이 영역은 범용 신경 컴퓨터(예: _신경 튜링 머신_(Graves et al., 2014) 및 _ 미분 가능 신경 컴퓨터_(Graves et al., 2016)의 구성을 조사했다. 이러한 아키텍처는 일반적인 계산의 모든 특징을 가지고 있지만 여러 구성 요소를 한 번에 도입하여 최적화하는 데 어려움을 겪는 경우가 많으며 실제로 산토로 등(2017, 2018)이 제안한 것과 같은 단순한 관계 추론기에 의해 거의 항상 성능이 우수하다.

복잡한 사후 조건을 모델링하는 것이 어렵기 때문에, 실행 학습(Zaremba and Sutskever, 2014)을 위한 귀납적 편향에 대한 많은 연구는 원시 알고리즘(예: 간단한 산술)에 초점을 맞추었다. 이 공간에서 두드러진 예로는 _뉴럴 GPU_(Kaiser and Sutskever, 2015), _뉴럴 RAM_(Kurach et al., 2015), _뉴럴 프로그래머-해석기_(Reed and De Freitas, 2015), _뉴럴 산술 논리 단위_(Trask et al., 2018; Madsen and Johansen, 2020) 및 _뉴럴 실행 엔진_(Yan et al., 2020)이 있습니다.

GNN 아키텍처의 급속한 발전으로 _초선형_ 복잡도의 조합 알고리즘을 모방하는 것이 가능해졌다. Xu 등(2019)이 개척한 알고리즘 정렬 프레임워크는 이론적으로 대부분의 알고리즘이 표현될 수 있는 언어인 동적 프로그래밍(Bellman, 1966)을 가진 GNNs _align_을 입증했다. 이 텍스트의 저자 중 한 명이 실제로 알고리즘 불변과 일치하는 GNN을 설계하고 훈련하는 것이 가능하다는 것을 동시에 경험적으로 보여주었다(Velickovic et al., 2019). 이후, 정렬은 _반복 알고리즘_(Tang 등, 2020), _선형 알고리즘_(Freivalds 등, 2019), _데이터 구조_(Velickovic 등, 2020) 및 _지속 메모리_(Strathmann 등, 2021)로 달성되었다. 이러한 모델은 또한 _강화 학습_ 알고리즘의 공간에 침입하여 _암시적 계획자_(Deac et al., 2020)에서 실제로 사용되었다.

동시에, _물리학 시뮬레이션_을 위해 GNN을 사용하는 데 상당한 진전이 있었다(Sanchez-Gonzalez et al., 2020; Pfaff et al., 2020). 이 방향은 일반화 GNN의 설계에 대해 동일한 권장 사항을 많이 산출했다. 이러한 대응은 예상되며, 알고리즘이 이산 시간 시뮬레이션으로 표현될 수 있고 시뮬레이션이 일반적으로 단계적 알고리즘으로 구현된다는 점을 감안할 때 두 방향 모두 유사한 종류의 불변량을 보존해야 한다.

알고리즘 추론 연구와 단단히 결합된 것은 _외삽_의 척도이다. 이것은 대부분의 성공 사례가 _분포 내_ 일반화할 때, 즉 훈련 데이터에서 발견된 패턴이 테스트 데이터에서 발견된 패턴을 적절하게 예측할 때 얻어진다는 점을 감안할 때 신경망에 악명 높은 고통 지점이다. 그러나 알고리즘 불변은 예를 들어 입력의 크기 또는 생성 분포에 관계없이 보존되어야 하며, 이는 훈련 세트가 실제로 마주치는 가능한 시나리오를 포함하지 않을 가능성이 있음을 의미한다. Xu et al. (2020)은 정류기 활성화에 의해 뒷받침되는 외삽 GNN에 필요한 것에 대한 기하학적 인수를 제안했다: 구성 모듈(예: 메시지 함수)이 _선형_ 목표 함수만 학습하도록 구성 요소 및 특징화가 설계되어야 한다. Bevilacqua et al. (2021)은 그래프의 _환경-불변_ 표현을 산출하며, 인과 추론_의 렌즈 아래 외삽을 관찰하는 것을 제안한다.

기하학적 딥러닝 우리의 마지막 역사적 발언은 바로 이 텍스트의 이름을 존중한다. 기하학적 딥 러닝(Geometric Deep Learning)이라는 용어는 2015년 그의 ERC grant에서 이 텍스트의 저자 중 한 사람에 의해 처음 소개되었고 명칭 IEEE 신호 처리 매거진 논문(Bronstein et al., 2017)에 대중화되었다. 이 논문은 비록 “약간의 주의”는 있지만, “새로운 분야가 탄생하고 있다”는 징후를 선언했다. 그래프 신경망의 최근 인기, 광범위한 기계 학습 응용 분야에서 불변성과 등분성의 아이디어의 사용 증가, 그리고 우리가 이 글을 쓴 바로 그 사실을 고려할 때, 아마도 이 예언이 적어도 부분적으로 충족되었다고 간주하는 것이 옳을 것이다. "4G: 그리드, 그래프, 그룹 및 게이지"라는 이름은 맥스 웰링이 기하학 딥 러닝에 대한 ELLIIS 프로그램을 위해 만든 것으로, 텍스트의 두 명의 저자가 공동 지시했다. 분명히, 마지막 'G'는 하부 구조가 게이지가 아니라 다양체와 다발이기 때문에 다소 스트레치이다. 이 텍스트에서 우리는 다양체의 메트릭 불변량과 고유 대칭과 관련하여 또 다른 'G'인 측지학을 추가했다.

## Acknowledgements

이 텍스트는 불변과 대칭이라는 기하학적 렌즈를 통해 딥 러닝 아키텍처에서 수십 년의 기존 지식을 요약하고 종합하려는 겸손한 시도를 나타낸다. 우리의 관점이 새로운 사람들과 실무자들이 현장을 탐색하고 연구자들이 우리의 청사진의 예로서 새로운 아키텍처를 합성하는 것을 더 쉽게 만들기를 바랍니다. 어떤 면에서, 우리는 바스와니 등(2017)이 영감을 준 단어에 대한 희곡 "_당신이 필요한 모든 건축물을 구축하는 데 필요한 모든 것"을 제시하기를 희망합니다.

본문의 대부분은 2020년 말과 2021년 초에 작성되었다. 종종 일어나는 일처럼, 우리는 그림 전체가 말이 되는지 수천 개의 의심을 품었고, 동료들이 제공한 기회를 사용하여 우리의 "무대 오른쪽"을 깨고 우리 작품의 초기 버전을 발표하는 데 사용했는데, 이는 케임브리지에서의 페타르의 강연(피에트로 리오의 예의)과 옥스포드에서의 마이클의 강연(샤오웬 동의 예의)과 임페리얼 칼리지에서의 마이클의 강연(마이클 휴스와 다니엘 루커트의 주최)에서 빛을 보았다. 페타는 또한 에를랑겐 프로그램의 발상지인 프리드리히-알렉산더-유니버시타트 에를랑겐-넌버그에서 우리의 작품을 발표할 수 있었습니다. 안드레아스 메이어의 친절한 초청을 받았습니다 우리가 이 회담에 대해 받은 피드백은 우리의 기분을 좋게 하고 작업을 더 다듬는 데 매우 귀중한 것이었다. 마지막으로, 그러나 확실히 중요한 것은, 우리의 작업이 마이클이 전달하는 기조 강연에 소개될 ICLR 2021의 조직 위원회에 감사드립니다.

우리는 그러한 방대한 양의 연구를 조정하는 것이 4명의 전문지식에 의해 거의 가능하지 않다는 점에 유의해야 한다. 따라서, 우리는 우리의 텍스트가 진화할 때 우리의 텍스트의 측면을 신중하게 연구한 모든 연구자들에게 공을 돌리고, 우리에게 신중한 논평과 참조를 제공했습니다: 요슈아 벵지오, 찰스 블런델, 안드리아 디악, 파비안 푸치스, 프란체스코 디 지오반니, 마르코 고리, 라야 하델, 윌 해밀턴, 막심 코릴로프, 크리스티안 머크워트, 라즈반 파스카누, 브루노 리베이루, 안나 스카이페, 위르겐 슈미트허버, 다윈 세글러, 코렌틴 탈렉, 은간 부, 피터 윈스버거 및 데이비드 웡. 그들의 전문가적 피드백은 우리의 통일 노력을 확고히 하고 다양한 틈새 시장에 더 유용하게 만드는 데 매우 유용했다. 물론, 이 글의 어떤 부정행위도 우리만의 책임입니다. 현재 진행 중인 작업이며, 어떤 단계에서든 의견을 받을 수 있어 매우 기쁩니다. 오류나 누락이 발견되면 저희에게 연락하십시오.

[MISSING_PAGE_EMPTY:132]

## Bibliography

* [1] Yonathan Aflalo and Ron Kimmel. Spectral multidimensional scaling. _PNAS_, 110(45):18052-18057, 2013.
* [2] Yonathan Aflalo, Haim Brezis, and Ron Kimmel. On the optimality of shape and data representation in the spectral domain. _SIAM J. Imaging Sciences_, 8(2):1141-1160, 2015.
* [3] Luis B Almeida. A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. In _Artificial neural networks: concept learning_, pages 102-111. 1990.
* [4] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. _arXiv:2006.05205_, 2020.
* [5] Sl Amari. Feature spaces which admit and detect invariant signal transformations. In _Joint Conference on Pattern Recognition_, 1978.
* [6] Brandon Anderson, Truong-Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural networks. _arXiv:1906.04015_, 2019.
* [7] Philip W Anderson. More is different. _Science_, 177(4047):393-396, 1972.
* [8] Mathieu Andreux, Emanuele Rodola, Mathieu Aubry, and Daniel Cremers. Anisotropic Laplace-Beltrami operators for shape analysis. In _ECCV_, 2014.
* [9] Salim Arslan, Sofia Ira Ktena, Ben Glocker, and Daniel Rueckert. Graph saliency maps through spectral convolutional networks: Application to sex classification with brain connectivity. In _Graphs in Biomedical Image Analysis and Integrating Medical Imaging and Non-Imaging Modalities_, pages 3-13. 2018.
* [10] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv:1607.06450_, 2016.

* Babai [2016] Laszlo Babai. Quasipolynomial 시간에서의 그래프 동형. 2016년 "컴퓨팅 이론에 관한 ACM 심포지엄"에서.
* Babai and Luks [1983] Laszlo Babai and Eugene M Luks. 그래프의 표준 레이블 지정 1983년 "컴퓨팅 이론에 관한 ACM 심포지엄"에서.
* Bach [2017] Francis Bach. 볼록 신경망으로 차원성의 저주를 깨고 있습니다. _ JMLR_, 18(1):629-681, 2017.
* Badia et al. [2020] Adria Puigdomenech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. In _ICML_, 2020.
* Badrinarayanan et al. [2017] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. _Trans. PAMI_, 39(12):2481-2495, 2017.
* Bahdanau et al. [2014] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. _arXiv:1409.0473_, 2014.
* Ball [2011] Philip Ball. 돌이켜보면, 6각 눈송이 Nature_, 480(7378):455-455, 2011.
* Bamieh [2018] Bassam Bamieh. 변환 발견: 순환 행렬, 순환 컨볼루션 및 이산 푸리에 변환에 대한 자습서입니다. _ arXiv:1805.05533_, 2018.
* Banach[1922] Stefan Banach. Sur les operations dans les ensembles abstraits et leur application aux equations integrales. _ Fundamenta Mathematicae_, 3(1):133-181, 1922.
* Bapst et al. [2020] Victor Bapst, Thomas Keck, A Grabska-Barwinska, Craig Donner, Ekin Dogus Cubuk, Samuel S Schoenholz, Annette Obika, Alexander WR Nelson, Trevor Back, Demis Hassabis, et al. Unveiling the predictive power of static structure in glassy systems. _Nature Physics_, 16(4):448-454, 2020.
* Barabasi et al. [2011] Albert-Laszlo Barabasi, Natali Gulbahce, and Joseph Loscalzo. Network medicine: a network-based approach to human disease. _Nature Reviews Genetics_, 12(1):56-68, 2011.
* Barron [1993] Andrew R Barron. S자형 함수의 중첩에 대한 범용 근사 한계 _ IEEE Trans. Information Theory_, 39(3):930-945, 1993.

* Baskin et al. [1997] Igor I Baskin, Vladimir A Palyulin, and Nikolai S Zefirov. A neural device for searching direct correlations between structures and properties of chemical compounds. _J. Chemical Information and Computer Sciences_, 37(4):715-721, 1997.
* Battaglia et al. [2016] Peter W Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, and Koray Kavukcuoglu. Interaction networks for learning about objects, relations and physics. _arXiv:1612.00222_, 2016.
* Battaglia et al. [2018] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. _arXiv:1806.01261_, 2018.
* Beaini et al. [2020] Dominique Beaini, Saro Passaro, Vincent Letourneau, William L Hamilton, Gabriele Corso, and Pietro Lio. Directional graph networks. _arXiv:2010.02863_, 2020.
* Bellman [1958] Richard Bellman. 라우팅 문제가 발생했습니다. _ Quarterly of Applied Mathematics_, 16(1):87-90, 1958.
* Bellman [1966] Richard Bellman. 동적 프로그래밍입니다. _ Science_, 153(3731):34-37, 1966.
* Bengio et al. [1994] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. _IEEE Trans. Neural Networks_, 5(2):157-166, 1994.
* Berger [2012] Marcel Berger. _ 리만 기하학의 파노라마 뷰_. 2012년 스프링어
* Besson et al. [2020] Pierre Besson, Todd Parrish, Aggelos K Katsaggelos, and S Kathleen Bandt. Geometric deep learning on brain shape predicts sex and age. _BioRxiv:177543_, 2020.
* Bevilacqua et al. [2021] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. _arXiv:2103.05045_, 2021.
* Blanc et al. [2020] Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process. In _COLT_, 2020.
* Bodnar et al. [2021] Cristian Bodnar, Fabrizio Frasca, Yu Guang Wang, Nina Otter, Guido Montufar, Pietro Lio, and Michael Bronstein. Weisfeiler and lehman go topological: Message passing simplicial networks. _arXiv:2103.03212_, 2021.
* Bouchet et al. [2018]* Bogatskiy et al. [2020] Alexander Bogatskiy, Brandon Anderson, Jan Offermann, Marwah Roussi, David Miller, and Risi Kondor. Lorentz group equivariant neural network for particle physics. In _ICML_, 2020.
* 보르숙[1933] 카롤 보르숙. Drei satze uber die n-dimensionale euklidische sphare. _ Fundamenta Mathematicae_, 20(1):177-190, 1933.
* Boscaini et al. [2015] Davide Boscaini, Davide Eynard, Drosos Kourounis, and Michael M Bronstein. Shape-from-operator: Recovering shapes from intrinsic operators. _Computer Graphics Forum_, 34(2):265-274, 2015.
* Boscaini 등[2016a] Davide Boscaini, Jonathan Masci, Emanuele Rodola, and Michael Bronstein. 비등방성 합성곱 신경망과의 형상 대응 학습. 2016a에서 _NIPS_ 입니다.
* Boscaini et al. [2016b] Davide Boscaini, Jonathan Masci, Emanuele Rodola, Michael M Bronstein, and Daniel Cremers. 이방성 확산 기술자들 _ Computer Graphics Forum_, 35(2):431-441, 2016b.
* Bougleux et al. [2015] Sebastien Bougleux, Luc Brun, Vincenzo Carletti, Pasquale Foggia, Benoit Gauzere, and Mario Vento. A quadratic assignment formulation of the graph edit distance. _arXiv:1512.07494_, 2015.
* Bouritsas et al. [2020] Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting. _arXiv:2006.09252_, 2020.
* Bronstein et al. [2006] Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. Generalized multidimensional scaling: a framework for isometry-invariant partial surface matching. _PNAS_, 103(5):1168-1172, 2006.
* Bronstein et al. [2008] Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. _Numerical geometry of non-rigid shapes_. Springer, 2008.
* Bronstein et al. [2017] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond Euclidean data. _IEEE Signal Processing Magazine_, 34(4):18-42, 2017.
* Brown et al. [2020] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _arXiv:2005.14165_, 2020.
* Brown et al. [2015]
* Bruna and Mallat [2013] Joan Bruna and Stephane Mallat. 불변 산란 컨볼루션 네트워크 _ IEEE transactions on pattern analysis and machine intelligence_, 35(8):1872-1886, 2013.
* Bruna et al. [2013] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In _ICLR_, 2013.
* Cappart et al. [2021] Quentin Cappart, Didier Chetelat, Elias Khalil, Andrea Lodi, Christopher Morris, and Petar Velickovic. Combinatorial optimization and reasoning with graph neural networks. _arXiv:2102.09544_, 2021.
* Chen et al. [2018] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. _arXiv:1806.07366_, 2018.
* Chen et al. [2020] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _ICML_, 2020.
* Chern et al. [2018] Albert Chern, Felix Knoppel, Ulrich Pinkall, and Peter Schroder. Shape from metric. _ACM Trans. Graphics_, 37(4):1-17, 2018.
* Cho et al. [2014] Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. _arXiv:1406.1078_, 2014.
* Choma et al. [2018] Nicholas Choma, Federico Monti, Lisa Gerhardt, Tomasz Palczewski, Zahra Ronaghi, Prabhat Prabhat, Wahid Bhimji, Michael M Bronstein, Spencer R Klein, and Joan Bruna. Graph neural networks for icecube signal classification. In _ICMLA_, 2018.
* Cohen and Welling [2016] Taco Cohen and Max Welling. 그룹 등분산 컨볼루션 네트워크. 2016년 _ICML_에서.
* Cohen et al. [2019] Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolutional networks and the icosahedral CNN. In _ICML_, 2019.
* Cohen et al. [2018] Taco S Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical cnns. _arXiv:1801.10130_, 2018.
* Cooijmans et al. [2016] Tim Cooijmans, Nicolas Ballas, Cesar Laurent, Caglar Gulcehre, and Aaron Courville. Recurrent batch normalization. _arXiv:1603.09025_, 2016.
* Cooijmans et al. [2017]* [Corman et al.2017] Etienne Corman, Justin Solomon, Mirela Ben-Chen, Leonidas Guibas, and Maks Ovsjanikov. 내재 및 외재 기하학의 기능적 특성화 _ ACM Trans. Graphics_, 36(2):1-17, 2017.
* [Cormen et al.2009] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. _ 알고리즘 소개_. 2009년 MIT 기자
* [Corso et al.2020] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. 그래프 네트의 주요 이웃 집합입니다. _ arXiv:2004.05718_, 2020.
* [Cosmo et al.2020] Luca Cosmo, Anees Kazi, Seyed-Ahmad Ahmadi, Nassir Navab, and Michael Bronstein. 질병 예측을 위한 잠재 그래프 학습. _MICCAI_, 2020.
* [Cranmer et al.2020] Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho. 라그랑지안 신경망. _ arXiv:2003.04630_, 2020.
* [Cranmer et al.2018] Miles D Cranmer, Rui Xu, Peter Battaglia, and Shirley Ho. 그래프 네트워크로 기호 물리학을 학습합니다. _ arXiv:1909.05862_, 2019.
* [Cucurull et al.2018] Guillem Cucurull, Konrad Wagstyl, Arantxa Casanova, Petar Velickovic, Estrid Jakobsen, Michal Drozdzal, Adriana Romero, Alan Evans, and Yoshua Bengio. 대뇌 피질의 메쉬 기반 분할을 위한 컨볼루션 신경망. 2018년.
* [Cybenko2019] George Cybenko. S자형 함수의 중첩에 의한 근사화 _ Mathematics of Control, Signals and Systems_, 2(4):303-314, 1989.
* [Dai et al.2016] Hanjun Dai, Bo Dai, and Le Song. 구조화 데이터에 대한 잠재 변수 모델의 차별적 임베딩. 2016년 _ICML_에서.
* [De Fauw et al.2018] Jeffrey De Fauw, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, Xavier Glorot, Brendan O'Donoghue, Daniel Visentin, et al. Clinically applicable deep learning for diagnosis and referral in 망막 질환. _ Nature Medicine_, 24(9):1342-1350, 2018.
* [de Haan et al.2020] Pim de Haan, Maurice Weiler, Taco Cohen, and Max Welling. 게이지 등분산 메쉬 CNN: 기하학적 그래프에 대한 이방성 컨볼루션. 2020년 NeurIPS에서.
* [Deac et al.2019] Andreea Deac, Petar Velickovic, and Pietro Sormanni. 집중적인 교차 모달 파라토프 예측입니다. _ Journal of Computational Biology_, 26(6):536-545, 2019.

* Deac et al. [2020] Andreea Deac, Petar Velickovic, Ognjen Milinkovic, Pierre-Luc Bacon, Jian Tang, and Mladen Nikolic. Xlvin: executed latent value iteration nets. _arXiv:2010.13146_, 2020.
* Defferrard et al. [2016] Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. _NIPS_, 2016.
* Derrow-Pinion et al. [2021] Austin Derrow-Pinion, Jennifer She, David Wong, Oliver Lange, Todd Hester, Luis Perez, Marc Nunkesser, Seongjae Lee, Xueying Guo, Peter W Battaglia, Vishal Gupta, Ang Li, Zhongwen Xu, Alvaro Sanchez-Gonzalez, Yujia Li, and Petar Velickovic. Traffic Prediction with Graph Neural Networks in Google Maps. 2021.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv:1810.04805_, 2018.
* Duvenaud et al. [2015] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. _NIPS_, 2015.
* Dwivedi and Bresson [2020] Vijay Prakash Dwivedi and Xavier Bresson. 트랜스포머 네트워크를 그래프로 일반화합니다. _ arXiv:2012.09699_, 2020.
* Elad and Kimmel [2003] Asi Elad and Ron Kimmel. 지표면에 대한 불변 시그너처를 구부릴 때 _ 트랜스젠더 PAMI_, 25(10):1285-1295, 2003.
* Elman [1990] Jeffrey L Elman. 시간 내에 구조를 찾습니다. _ Cognitive Science_, 14(2):179-211, 1990.
* Esteves et al. [2020] Carlos Esteves, Ameesh Makadia, and Kostas Daniilidis. Spin-weighted spherical CNNs. _arXiv:2006.10731_, 2020.
* Fang et al. [2020] Xiaomin Fang, Jizhou Huang, Fan Wang, Lingke Zeng, Haijin Liang, and Haifeng Wang. ConSTGAT: Contextual spatial-temporal graph attention network for travel time estimation at baidu maps. In _KDD_, 2020.
* Fey et al. [2020] Matthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical inter-message passing for learning on molecular graphs. _arXiv:2006.12179_, 2020.
* Finzi et al. [2020] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In _ICML_, 2020.
* Fey et al. [2020]* Folkman (1967) Jon Folkman. 정규 선대칭 그래프입니다. _ Journal of Combinatorial Theory_, 3(3):215-232, 1967.
* Franceschi et al. (2019) Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. 그래프 신경망에 대한 이산 구조를 학습합니다. 2019년 _ICML_에서.
* Frasconi et al. (1998) Paolo Frasconi, Marco Gori, and Alessandro Sperduti. 데이터 구조의 적응형 처리를 위한 일반적인 프레임워크입니다. _ IEEE Trans. Neural Networks_, 9(5):768-786, 1998.
* Freivalds 등(2019) Karlis Freivalds, Emils Ozolins, and Agris Sostaks. Neural shuffle-exchange networks-sequence processing in o(n log n) time. _ arXiv:1907.07897_, 2019.
* Fuchs 등(2020) Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max Welling. SE(3)-변환기: 3D 로토-번역 등분산 주의 네트워크. _ arXiv:2006.10503_, 2020.
* 후쿠시마 및 미야케(1982) 쿠니히코 후쿠시마 및 세이 미야케. Neocognitron: 시각 패턴 인식의 메커니즘을 위한 자기 조직화 신경망 모델. 《신경망의 경쟁과 협력》에서, 267-285 페이지, 1982. 스프링어.
* Gainza et al.(2020) Pablo Gainza, Freyr Sverrisson, Frederico Monti, Emanuele Rodola, D Boscaini, MM Bronstein, and BE Correia. 기하학적 딥러닝을 사용하여 단백질 분자 표면에서 상호 작용 지문을 해독합니다. _ Nature Methods_, 17(2):184-192, 2020.
* Gama et al.(2019) Fernando Gama, Alejandro Ribeiro, and Joan Bruna. 확산 산란 변환은 그래프에서 변환됩니다. 2019년 _ICLR_ 입니다.
* Gama et al.(2020) Fernando Gama, Joan Bruna, and Alejandro Ribeiro. 그래프 신경망의 안정성 속성 _ IEEE Trans. Signal Processing_, 68:5680-5695, 2020.
* Gao et al.(2019) Hongchang Gao, Jian Pei, and Heng Huang. 조건부 랜덤 필드 강화 그래프 합성곱 신경망. 2019년 _KDD_에서.
* 가르시아-듀란 및 니퍼트(2017) 알베르토 가르시아-듀란 및 마티아스 니퍼트. 임베딩 전파를 사용하여 그래프 표현을 학습합니다. _ arXiv:1710.03059_, 2017.
* Gatys et al.(2015) Leon A Gatys, Alexander S Ecker, and Matthias Bethge. 합성곱 신경망을 이용한 질감 합성 _ arXiv preprint arXiv:1505.07376_, 2015.
* Gaudelet 등(2020) Thomas Gaudelet, Ben Day, Arian R Jamasb, Jyothish Soman, Cristian Regep, Gertrude Liu, Jeremy BR Hayter, Richard Vickers, Charles Roberts, Jian Tang, et al. Utilising graph machine learning within drug discovery and development. _ arXiv:2012.05716_, 2020.
* Ganinza et al. (2019)* Gers and Schmidhuber (2000) Felix A Gers and Jurgen Schmidhuber. 그 시간 동안 반복된 그물을 세어 보세요. 2000년 IJCNN에서요
* Gilmer 등(2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 양자 화학을 위한 신경 메시지 전달 arXiv:1704.01212_, 2017.
* Girshick (2015) Ross Girshick. 고속 R-CNN 2015년 _CVPR_에서.
* Girshick et al.(2014) Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 정확한 객체 검출과 의미론적 분할을 위한 풍부한 특징 계층 구조. 2014년 _CVPR_에서.
* Gligorijevic et al.(2020) Vladimir Gligorijevic, P Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C Taylor, Ian M Fisk, Hera Vlamakis, et al. Structure-based function prediction using graph convolutional networks. _ bioRxiv:786236_, 2020.
* Goller and Kuchler (1996) Christoph Goller and Andreas Kuchler. 구조를 통한 역전파에 의해 과제 의존 분산 표현을 학습한다. 《ICNN》, 1996.
* Goodfellow 등(2014) Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 생성적 적대 네트워크입니다. _ arXiv:1406.2661_, 2014.
* Gori 등(2005) Marco Gori, Gabriele Monfardini, and Franco Scarselli. 그래프 도메인에서 학습을 위한 새로운 모델입니다. 2005년 IJCNN에서.
* Graves (2013) Alex Graves. 순환 신경망으로 시퀀스를 생성합니다. _ arXiv:1308.0850_, 2013.
* Graves et al.(2014) Alex Graves, Greg Wayne, and Ivo Danihelka. 신경 튜링 머신입니다. _ arXiv:1410.5401_, 2014.
* Graves 등(2016) Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Dynamic External Memory를 갖는 신경망을 이용한 하이브리드 컴퓨팅_ Nature_, 538(7626):471-476, 2016.
* Grill 등(2020) Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning _ arXiv:2006.07733_, 2020.
* Grill 등 (2017)* Gromov [1981] Mikhael Gromov. _ 구조 메트리크는 변종 리만니엔스를 따른다. 1981년 세딕
* Grover and Leskovec [2016] Aditya Grover and Jure Leskovec. node2vec: 네트워크에 대한 확장 가능한 특징 학습. 2016년 _KDD_에서.
* Gunasekar et al. [2017] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In _NIPS_, 2017.
* Gysi et al. [2020] Deisy Morselli Gysi, Italo Do Valle, Marinka Zitnik, Asher Ameli, Xiao Gan, Onur Varol, Helia Sanchez, Rebecca Marlene Baron, Dina Ghiassian, Joseph Loscalzo, et al. Network medicine framework for identifying drug repurposing opportunities for COVID-19. _arXiv:2004.07229_, 2020.
* Hamilton et al. [2017] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _NIPS_, 2017.
* Hao et al. [2020] Junheng Hao, Tong Zhao, Jin Li, Xin Luna Dong, Christos Faloutsos, Yizhou Sun, and Wei Wang. P-companion: A principled framework for diversified complementary product recommendation. In _Information & Knowledge Management_, 2020.
* Hardt and Ma [2016] Moritz Hardt and Tengyu Ma. 딥러닝에서 정체성은 중요합니다. _ arXiv:1611.04231_, 2016.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.
* He et al. [2017] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _CVPR_, 2017.
* Helvetius [1759] Claude Adrien Helvetius. _ De l'esprit_ 듀랜드, 1759년
* Hjelm et al. [2019] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In _ICLR_, 2019.
* Hochreiter [1991] Sepp Hochreiter. _ Untersuchungen zu dynamischen neuronalen Netzen_. 1991년 Munchen 대학의 박사학위 논문.
* Hochreiter and Schmidhuber [1997] Sepp Hochreiter and Jurgen Schmidhuber. 장기간 단기 기억. _ Neural Computation_, 9(8):1735-1780, 1997.

* Hornik (1991) Kurt Hornik. 다층 피드포워드 네트워크의 근사화 기능 _ Neural Networks_, **4**(2):251-257, 1991.
* Hoshen (2017) Yedid Hoshen. 관심 있는 다중 에이전트 예측 모델링입니다. _ arXiv:1706.06122_, 2017.
* Hu et al. (2020) Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. 그래프 신경망을 사전 훈련하기 위한 전략. 2020년 _ICLR_ 에서.
* Hubel and Wiesel (1959) David H Hubel and Torsten N Wiesel. 고양이 줄무늬 피질에 있는 단일 신경의 수용 필드입니다. _ J Physiology_, 148(3):574-591, 1959.
* Hutchinson 등(2020) Michael Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh, and Hyunjik Kim. LieTransformer: Lie 그룹에 대한 자명한 자기 주의 _ arXiv:2012.10885_, 2020.
* Ioffe and Szegedy (2015) Sergey Ioffe and Christian Szegedy. 배치 정규화: 내부 공변량 이동을 줄여 심층 네트워크 훈련을 가속화합니다. 2015년 _ICML_에서.
* Iqbal (2018) Haris Iqbal. Harisiqbal88/plotneuralnet v1.0.0, 2018년 12월. URL [https://doi.org/10.5281/zenodo.2526396](https://doi.org/10.5281/zenodo.2526396).
* Itani and Thanou (2021) Sarah Itani and Dorina Thanou. 신경병리학적 식별을 위한 해부학적 네트워크와 기능적 네트워크를 결합: 자폐 스펙트럼 장애에 대한 사례 연구 _ Medical Image Analysis_, 69:101986, 2021.
* Jin et al.(2018) Wengong Jin, Regina Barzilay, and Tommi Jaakkola. 분자 그래프 생성을 위한 접합 트리 가변 오토인코더. 2018년 _ICML_에서.
* Jin et al.(2020) Wengong Jin, Regina Barzilay, and Tommi Jaakkola. 구조 모티브를 사용하여 분자 그래프를 계층적으로 생성합니다. 2020년 _ICML_에서.
* Johnson 등(2016) Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-Wei, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, 자유롭게 액세스할 수 있는 중환자 진료 데이터베이스입니다. _ Scientific Data_, 3(1):1-9, 2016.
* 조던(1997) 마이클 I 조던. 직렬 순서: 병렬 분산 처리 접근법. 《심리학의 발전》에서, 121권, 471-495쪽, 1997.
* Joshi (2020) Chaitanya Joshi. 트랜스포머는 그래프 신경망입니다. _ Gradient_, 2020.
* Joshi et al. (2018)* Jozefowicz et al. [2015] Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent network architectures. In _ICML_, 2015.
* Kaiser and Sutskever [2015] Lukasz Kaiser and Illya Sutskever. 신경 GPU는 알고리즘을 학습합니다. _ arXiv:1511.08228_, 2015.
* Kalchbrenner et al. [2016] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. _arXiv:1610.10099_, 2016.
* Kalchbrenner et al. [2018] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. In _ICML_, 2018.
* Kanatani [2012] Ken-Ichi Kanatani. _ 이미지 이해에서 집단 이론적 방법_. 2012년 스프링어
* Karni and Gotsman [2000] Zachi Karni and Craig Gotsman. 메쉬 형상의 스펙트럼 압축입니다. *Proc. Computer Graphics and Interactive Techniques_, 2000.
* Kazi et al. [2020] Anees Kazi, Luca Cosmo, Nassir Navab, and Michael Bronstein. Differentiable graph module (DGM) graph convolutional networks. _arXiv:2002.04999_, 2020.
* Kenlay et al. [2021] Henry Kenlay, Dorina Thanou, and Xiaowen Dong. Interpretable stability bounds for spectral graph filters. _arXiv:2102.09587_, 2021.
* Kimmel and Sethian [1998] Ron Kimmel and James A Sethian. 매니폴드에서 측지 경로를 계산하는 중입니다. _ PNAS_, 95(15):8431-8435, 1998.
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. 애덤: 확률적 최적화를 위한 방법입니다. _ arXiv:1412.6980_, 2014.
* Kingma and Welling [2013] Diederik P Kingma and Max Welling. 변분 베이를 자동으로 인코딩합니다. _ arXiv:1312.6114_, 2013.
* Kipf et al. [2018] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In _ICML_, 2018.
* Kipf and Welling [2016a] Thomas N Kipf and Max Welling. 그래프 합성곱 네트워크를 사용한 준감독 분류 _ arXiv:1609.02907_, 2016a.
* Kipf and Welling [2016b] Thomas N Kipf and Max Welling. 가변 그래프 자동 인코더입니다. _ arXiv:1611.07308_, 2016b.

* Kireev [1995] Dmitry B Kireev. Chemnet: 그래프/속성 매핑을 위한 새로운 신경망 기반 방법. _ J Chemical Information and Computer Sciences_, 35(2):175-180, 1995.
* Klicpera et al. [2020] Johannes Klicpera, Janek Gross, and Stephan Gunnemann. Directional message passing for molecular graphs. _arXiv:2003.03123_, 2020.
* Kokkinos et al. [2012] Iasonas Kokkinos, Michael M Bronstein, Roee Litman, and Alex M Bronstein. Intrinsic shape context descriptors for deformable shapes. In _CVPR_, 2012.
* Komiske et al. [2019] Patrick T Komiske, Eric M Metodiev, and Jesse Thaler. Energy flow networks: deep sets for particle jets. _Journal of High Energy Physics_, 2019(1):121, 2019.
* Kostrikov et al. [2018] Ilya Kostrikov, Zhongshi Jiang, Daniele Panozzo, Denis Zorin, and Joan Bruna. Surface networks. In _CVPR_, 2018.
* Krizhevsky et al. [2012] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In _NIPS_, 2012.
* Ktena et al. [2017] Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew Lee, Ben Glocker, and Daniel Rueckert. Distance metric learning using graph convolutional networks: Application to functional brain networks. In _MICCAI_, 2017.
* Kulon et al. [2020] Dominik Kulon, Riza Alp Guler, Iasonas Kokkinos, Michael M Bronstein, and Stefanos Zafeiriou. Weakly-supervised mesh-convolutional hand reconstruction in the wild. In _CVPR_, 2020.
* Kurach et al. [2015] Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. _arXiv:1511.06392_, 2015.
* LeCun et al. [1998] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proc. IEEE_, 86(11):2278-2324, 1998.
* Lenz [1990] Reiner Lenz. _ Group theoretical methods in image processing_. 스프링어, 1990년
* Leshno et al. [1993] Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. _Neural Networks_, 6(6):861-867, 1993.
* Levie et al. [2018] Ron Levie, Federico Monti, Xavier Bresson, and Michael M Bronstein. Cay-leynets: Graph convolutional neural networks with complex rational spectral filters. _IEEE Trans. Signal Processing_, 67(1):97-109, 2018.

론 레비, 엘빈 이수피 기타 쿠티닉 스펙트럼 그래프 필터의 전달 가능성에 대해. 2019년, _샘플링 이론과 응용 프로그램_ 에서.
* Levy [2006] Bruno Levy. 라플라스-벨트라미 고유함수는 기하학을 "이해"하는 알고리즘을 지향한다. *Proc. Shape Modeling and Applications_, 2006.
* Li et al. [2015] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. _arXiv:1511.05493_, 2015.
* Litany et al. [2018] Or Litany, Alex Bronstein, Michael Bronstein, and Ameesh Makadia. Deformable shape completion with graph convolutional autoencoders. In _CVPR_, 2018.
* Litman and Bronstein [2013] Roee Litman and Alexander M Bronstein. 변형 가능한 형상 대응에 대한 스펙트럼 기술자를 학습합니다. _ 트랜스젠더 PAMI_, 36(1):171-180, 2013.
* Liu et al. [2017] Hsueh-Ti Derek Liu, Alec Jacobson, and Keenan Crane. A Dirac operator for extrinsic shape analysis. _Computer Graphics Forum_, 36(5):139-149, 2017.
* Lyu and Simoncelli [2008] Siwei Lyu and Eero P Simoncelli. 분할 정규화를 사용하는 비선형 이미지 표현. 2008년 _CVPR_ 에서.
* MacNeal [1949] Richard H MacNeal. _ 전기 네트워크에 의한 편미분 방정식의 해_. 1949년 캘리포니아 공대 박사 학위 논문
* Madsen and Johansen [2020] Andreas Madsen and Alexander Rosenberg Johansen. 신경 연산 장치입니다. _ arXiv:2001.05016_, 2020.
* Mahdi et al. [2020] Soha Sadat Mahdi, Nele Nauwelaers, Philip Joris, Giorgos Bouritsas, Shunwang Gong, Sergiy Bokhnyak, Susan Walsh, Mark Shriver, Michael Bronstein, and Peter Claes. 3d facial matching by spiral convolutional metric learning and a biometric fusion-net of demographic properties. _arXiv:2009.04746_, 2020.
* Maiorov [1999] VE Maiorov. 능선 함수에 의한 최상의 근사치에서 _ Journal of Approximation Theory_, 99(1):68-94, 1999.
* Makadia et al. [2007] Ameesh Makadia, Christopher Geyer, and Kostas Daniilidis. Correspondence-free structure from motion. _IJCV_, 75(3):311-327, 2007.
* Mallat [1999] Stephane Mallat. _ 신호 처리의 웨이블릿 투어_. 엘세비에, 1999년
* Mallat [2012] Stephane Mallat. 그룹 불변 산란 _ Communications on Pure and Applied Mathematics_, 65(10):1331-1398, 2012.
* Mallat et al. [2018]* Malone et al. [2018] Brandon Malone, Alberto Garcia-Duran, and Mathias Niepert. Learning representations of missing data for predicting patient outcomes. _arXiv:1811.04752_, 2018.
* Maron et al. [2018] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. _arXiv:1812.09902_, 2018.
* Maron et al. [2019] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. _arXiv:1905.11136_, 2019.
* 후작[2009] 장피에르 후작. 범주 이론과 클라인의 엘랑겐 프로그램 [기하학적 관점]에서 9-40페이지, 2009년 스프링어.
* Masci et al. [2015] Jonathan Masci, Davide Boscaini, Michael Bronstein, and Pierre Vandergheynst. Geodesic convolutional neural networks on Riemannian manifolds. In _CVPR Workshops_, 2015.
* Maxwell [1865] James Clerk Maxwell. 전자기장의 동역학 이론. _ The Royal Society of London_, (155):459-512, 1865의 철학적 거래.
* McEwen et al. [2021] Jason D McEwen, Christopher GR Wallis, and Augustine N Mavor-Parker. Scattering networks on the sphere for scalable and rotationally equivariant spherical cnns. _arXiv:2102.02828_, 2021.
* Mei et al. [2021] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random features and kernel models. _arXiv:2102.13219_, 2021.
* Melzi et al. [2019] Simone Melzi, Riccardo Spezialetti, Federico Tombari, Michael M Bronstein, Luigi Di Stefano, and Emanuele Rodola. Gframes: Gradient-based local reference frame for 3d shape matching. In _CVPR_, 2019.
* Memoli and Sapiro [2005] Facundo Memoli and Guillermo Sapiro. 점군 데이터의 등측량 불변 인식을 위한 이론 및 계산 프레임워크 _ Foundations of Computational Mathematics_, 5(3):313-347, 2005.
* Merkwirth and Lengauer [2005] Christian Merkwirth and Thomas Lengauer. 분자 그래프 네트워크를 사용하여 상보적 설명자를 자동으로 생성합니다. _ J Chemical Information and Modeling_, 45(5):1159-1168, 2005.
* Meyer et al. [2003] Mark Meyer, Mathieu Desbrun, Peter Schroder, and Alan H Barr. Discrete differential-geometry operators for triangulated 2-manifolds. In _Visualization and Mathematics III_, pages 35-57. 2003.
* Micheli [2009] Alessio Micheli. 그래프를 위한 신경망: 맥락적 건설적 접근법 _ IEEE Trans. Neural Networks_, 20(3):498-511, 2009.
* Masci et al. [2019]* Miller et al. [2016] Karla L Miller, Fidel Alfaro-Almagro, Neal K Bangerter, David L Thomas, Essa Yacoub, Junqian Xu, Andreas J Bartsch, Saad Jbabdi, Stamatios N Sotiropoulos, Jesper LR Andersson, et al. Multimodal population brain imaging in the uk biobank prospective epidemiological study. _Nature Neuroscience_, 19(11):1523-1536, 2016.
* Minsky and Papert [2017] Marvin Minsky and Seymour A Papert. _ 퍼셉트론: 계산 기하학에 대한 소개_. MIT Press, 2017.
* Mitrovic et al. [2020] Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Buesing, and Charles Blundell. Representation learning via invariant causal mechanisms. _arXiv:2010.07922_, 2020.
* Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533, 2015.
* Mnih et al. [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In _ICML_, 2016.
* Monti et al. [2017] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In _CVPR_, 2017.
* Monti et al. [2019] Federico Monti, Fabrizio Frasca, Davide Eynard, Damon Mannion, and Michael M Bronstein. Fake news detection on social media using geometric deep learning. _arXiv:1902.06673_, 2019.
* Morris et al. [2017] Christopher Morris, Kristian Kersting, and Petra Mutzel. Glocalized Weisfeiler-Lehman graph kernels: Global-local feature maps of graphs. In _ICDM_, 2017.
* Morris et al. [2019] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In _AAAI_, 2019.
* Morris et al. [2020] Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings. In _NeurIPS_, 2020.
* Mozer [1989] Michael C Mozer. 시간 패턴 인식을 위한 집중된 역전파 알고리즘 _ Complex Systems_, 3(4):349-381, 1989.

[MISSING_PAGE_FAIL:149]

* Pearl (2014) Judea Pearl. _ 지능형 시스템에서의 확률적 추론: 그럴듯한 추론_의 네트워크. 엘세비에, 2014년
* Penrose (2005) Roger Penrose. _ 현실로 가는 길: 우주의 법칙에 대한 완전한 안내._ 랜덤 하우스, 2005년
* Perozzi et al.(2014) Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 딥워크: 사회적 표상의 온라인 학습. 2014년 KDD에서.
* Pfaff et al.(2020) Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. 그래프 네트워크를 사용한 메쉬 기반 시뮬레이션 학습 _ arXiv:2010.03409_, 2020.
* Pineda (1988) Fernando J Pineda. 역 전파의 순환 및 고차 신경망으로의 일반화 1988년 'NIPS'에서요
* Pinkall and Polthier (1993) Ulrich Pinkall and Konrad Polthier. 이산 최소 표면 및 그 접합체를 계산하는 중입니다. _ Experimental Mathematics_, 2(1):15-36, 1993.
* Pinkus (1999) Allan Pinkus. 신경망에서 mlp 모델의 근사 이론 _ Acta Numerica_, 8:143-195, 1999.
* Pollard 등(2018) Tom J Pollard, Alistair EW Johnson, Jesse D Raffa, Leo A Celi, Roger G Mark, and Omar Badawi. 아이쿠 공동 연구 데이터베이스, 중환자 치료를 위해 자유롭게 사용할 수 있는 다중 센터 데이터베이스입니다. _ Scientific Data_, 5(1):1-13, 2018.
* Portilla and Simoncelli (2000) Javier Portilla and Eero P Simoncelli. 복잡한 웨이블릿 계수의 결합 통계량을 기반으로 하는 파라메트릭 텍스처 모델 _ International journal of computer vision_, 40(1):49-70, 2000.
* Qi et al.(2017) Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: 3차원 분류 및 분할을 위한 점 집합에 대한 딥 러닝. 2017년 _CVPR_에서.
* Qiu et al.(2018) Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. 행렬 인수분해로서의 네트워크 임베딩: 딥워크, 라인, pte 및 node2vec 통합. 2018년 'WSDM'에서요
* Qu and Gouskos (2019) H Qu and L Gouskos. 파티클레나트: 입자 구름을 통해 제트 태그를 지정합니다. _ arXiv:1902.08570_, 2019.
*Qu et al.(2019) Meng Qu, Yoshua Bengio, and Jian Tang. GMNN: 그래프 마르코프 신경망. 2019년 _ICML_에서.
* Qu et al. (2019)* Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.
* Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* Ranjan et al. [2018] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael J Black. Generating 3D faces using convolutional mesh autoencoders. In _ECCV_, 2018.
* Raviv et al. [2007] Dan Raviv, Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. Symmetries of non-rigid shapes. In _ICCV_, 2007.
* Razin and Cohen [2020] Noam Razin and Nadav Cohen. 딥러닝에서 암묵적인 정규화는 규범으로 설명할 수 없을 수 있습니다. _ arXiv:2005.06398_, 2020.
* Reed and De Freitas [2015] Scott Reed and Nando De Freitas. 신경 프로그래머-해석기입니다. _ arXiv:1511.06279_, 2015.
* Ren et al. [2015] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. _arXiv:1506.01497_, 2015.
* Rezende and Mohamed [2015] Danilo Rezende and Shakir Mohamed. 흐름 정규화를 통한 변형 추론 2015년 _ICML_에서.
* Riesenhuber and Poggio [1999] Maximilian Riesenhuber and Tomaso Poggio. 피질에서의 객체 인식의 계층적 모델. _ Nature neuroscience_, 2(11):1019-1025, 1999.
* Robinson and Fallside [1987] AJ Robinson and Frank Fallside. _ 유틸리티 구동 동적 오류 전파 네트워크_. 1987년 케임브리지 대학교
* Rocheteau et al. [2020] Emma Rocheteau, Pietro Lio, and Stephanie Hyland. Temporal pointwise convolutional networks for length of stay prediction in the intensive care unit. _arXiv:2007.09483_, 2020.
* Rocheteau et al. [2021] Emma Rocheteau, Catherine Tong, Petar Velickovic, Nicholas Lane, and Pietro Lio. Predicting patient outcomes with graph representation learning. _arXiv:2101.03940_, 2021.
* Ronneberger et al. [2015] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _MICCAI_, 2015.
* Rosenblatt [1958] Frank Rosenblatt. 퍼셉트론: 뇌의 정보 저장 및 조직에 대한 확률 모델입니다. _ Psychological Review_, 65(6):386, 1958.
* Rosenblatt [1958]* Rossi et al. [2020] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. Temporal graph networks for deep learning on dynamic graphs. _arXiv:2006.10637_, 2020.
* Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _IJCV_, 115(3):211-252, 2015.
* Rustamov et al. [2013] Raif M Rustamov, Maks Ovsjanikov, Omri Azencot, Mirela Ben-Chen, Frederic Chazal, and Leonidas Guibas. Map-based exploration of intrinsic shape differences and variability. _ACM Trans. Graphics_, 32(4):1-12, 2013.
* Salimans and Kingma [2016] Tim Salimans and Diederik P Kingma. 가중치 정규화: 심층 신경망의 학습을 가속화하기 위한 간단한 재매개 변수화입니다. _ arXiv:1602.07868_, 2016.
* Sanchez-Gonzalez et al. [2019] Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter Battaglia. Hamiltonian graph networks with ODE integrators. _arXiv:1909.12790_, 2019.
* Sanchez-Gonzalez et al. [2020] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to simulate complex physics with graph networks. In _ICML_, 2020.
* Sandryhaila and Moura [2013] Aliaksei Sandryhaila and Jose MF Moura. 그래프에 대한 개별 신호 처리 _ IEEE Trans. Signal Processing_, 61(7):1644-1656, 2013.
* Santoro et al. [2017] Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In _NIPS_, 2017.
* Santoro et al. [2018] Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational recurrent neural networks. _arXiv:1806.01822_, 2018.
* Santurkar et al. [2018] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization? _arXiv:1805.11604_, 2018.
* Sato et al. [2020] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural networks. _arXiv:2002.03155_, 2020.
* Sato et al. [2018]* Satorras et al. [2021] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. _arXiv:2102.09844_, 2021.
* Scaife and Porter [2021] Anna MM Scaife and Fiona Porter. 그룹equivariant 합성곱 신경망을 이용한 전파은하의 파나로프-라일리 분류. _ 왕립 천문 학회의 월간 통지_ 2021년.
* Scarselli et al. [2008] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. _IEEE Trans. Neural Networks_, 20(1):61-80, 2008.
* Schrittwieser et al. [2020] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv:1707.06347_, 2017.
* Schutt et al. [2018] Kristof T Schutt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Muller. Schnet-a deep learning architecture for molecules and materials. _The Journal of Chemical Physics_, 148(24):241722, 2018.
* Sejnowski et al. [1986] Terrence J Sejnowski, Paul K Kienker, and Geoffrey E Hinton. Learning symmetry groups with hidden units: Beyond the perceptron. _Physica D: Nonlinear Phenomena_, 22(1-3):260-275, 1986.
* Senior et al. [2020] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Zidek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein structure prediction using potentials from deep learning. _Nature_, 577(7792):706-710, 2020.
* Serre et al. [2007] Thomas Serre, Aude Oliva, and Tomaso Poggio. A feedforward architecture accounts for rapid categorization. _Proceedings of the national academy of sciences_, 104(15):6424-6429, 2007.
* Shamir and Vardi [2020] Ohad Shamir and Gal Vardi. 제곱 손실이 있는 Relu 네트워크에서 암시적 정규화 _ arXiv:2012.05156_, 2020.
* Shawe-Taylor [1989] John Shawe-Taylor. 대칭을 피드포워드 네트워크로 구축합니다. 1989년 'ICANN'에서요
* Shawe-Taylor [1993] John Shawe-Taylor. 피드포워드 네트워크 아키텍처의 대칭 및 차별성 _ IEEE Trans. Neural Networks_, 4(5):816-826, 1993.

* Shervashidze et al. [2011] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. _JMLR_, 12(9), 2011.
* Shlomi et al. [2020] Jonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant. Graph neural networks in particle physics. _Machine Learning: Science and Technology_, 2(2):021001, 2020.
* Shuman et al. [2013] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. _IEEE Signal Processing Magazine_, 30(3):83-98, 2013.
* Siegelmann and Sontag [1995] Hava T Siegelmann and Eduardo D Sontag. 신경망의 계산 능력에서 _ Journal of Computer and System Sciences_, 50(1):132-150, 1995.
* Silver et al. [2016] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _Nature_, 529(7587):484-489, 2016.
* Silver et al. [2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. _Nature_, 550(7676):354-359, 2017.
* Simoncelli and Freeman [1995] Eero P Simoncelli and William T Freeman. 조종 가능한 피라미드: 다중 스케일 미분 계산을 위한 유연한 아키텍처. In _Proceedings., International Conference on Image Processing_, volume 3, pages 444-447. IEEE, 1995.
* Simonyan and Zisserman [2014] Karen Simonyan and Andrew Zisserman. 대규모 이미지 인식을 위한 매우 깊은 컨볼루션 네트워크입니다. _ arXiv:1409.1556_, 2014.
* Smola et al. [2007] Alex Smola, Arthur Gretton, Le Song, and Bernhard Scholkopf. A Hilbert space embedding for distributions. In _ALT_, 2007.
* Spalevic et al. [2020] Stefan Spalevic, Petar Velickovic, Jovana Kovacevic, and Mladen Nikolic. Hierachial protein function prediction with tail-GNNs. _arXiv:2007.12804_, 2020.
* Sperduti [1994] 알레산드로 Sperduti. RAAM에 레이블을 지정하여 레이블이 지정된 그래프를 인코딩하는 중입니다. 1994년 NIPS에서요
* Sperduti et al. [2015]* Sperduti and Starita [1997] Alessandro Sperduti and Antonina Starita. 구조 분류를 위한 감독 신경망 _ IEEE Trans. Neural Networks_, 8(3):714-735, 1997.
* Springenberg et al. [2014] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. _arXiv:1412.6806_, 2014.
* 스리니바산 및 리베이로[2019] 발라수브라마니암 스리니바산 및 브루노 리베이로. 위치 노드 임베딩과 구조 그래프 표현 간의 동등성에 대해 _ arXiv:1910.00452_, 2019.
* Srivastava et al. [2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. _JMLR_, 15(1):1929-1958, 2014.
* Srivastava et al. [2015] Rupesh Kumar Srivastava, Klaus Greff, and Jurgen Schmidhuber. Highway networks. _arXiv:1505.00387_, 2015.
* Stachenfeld et al. [2020] Kimberly Stachenfeld, Jonathan Godwin, and Peter Battaglia. Graph networks with spectral message passing. _arXiv:2101.00079_, 2020.
* Stokes et al. [2020] Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackerman, et al. A deep learning approach to antibiotic discovery. _Cell_, 180(4):688-702, 2020.
* Strathmann et al. [2021] Heiko Strathmann, Mohammadamin Barekatain, Charles Blundell, and Petar Velickovic. Persistent message passing. _arXiv:2103.01043_, 2021.
* Straumann [1996] Norbert Straumann. 게이지 이론과 약한 상호 작용의 초기 역사. _ hep-ph/9609230_, 1996.
* Sun et al. [2009] Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. A concise and provably informative multi-scale signature based on heat diffusion. _Computer Graphics Forum_, 28(5):1383-1392, 2009.
* Sutskever et al. [2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. _arXiv:1409.3215_, 2014.
* Szegedy et al. [2015] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In _CVPR_, 2015.
* Szegedy et al. [2015]* Tallec and Ollivier (2018) Corentin Tallec and Yann Ollivier. 순환 신경망이 시간을 왜곡할 수 있습니까? _ arXiv:1804.11188_, 2018.
* Tang et al.(2020) Hao Tang, Zhiao Huang, Jiayuan Gu, Bao-Liang Lu, and Hao Su. 반복적 동질 gnns에 의한 스케일 불변 그래프 관련 문제 해결을 위하여 2020년 NeurIPS에서.
* Tang et al.(2015) Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 라인: 대규모 정보 네트워크 임베딩. 2015년 'WWW'에서
*Taubin et al.(1996) Gabriel Taubin, Tong Zhang, and Gene Golub. 필터 설계로 최적의 표면 평활화입니다. 1996년 ECCV에서요
* Thakoor 등(2021) Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Remi Munos, Petar Velickovic, and Michal Valko. 그래프에서 부트스트랩된 표현 학습입니다. _ arXiv:2102.06514_, 2021.
* Thomas et al. (2018) Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. 텐서 필드 네트워크: 3D 포인트 클라우드를 위한 회전 및 변환 등변 신경망입니다. _ arXiv:1802.08219_, 2018.
* Tobies (2019) Renate Tobies. 펠릭스 클라인 수학자, 학술 조직자, 교육 개혁가 '펠릭스 클라인의 유산' 5-21페이지 2019년 스프링어
* Trask 등(2018) Andrew Trask, Felix Hill, Scott Reed, Jack Rae, Chris Dyer, and Phil Blunsom. 신경 산술 논리 단위입니다. _ arXiv:1808.00508_, 2018.
* Tromp and Farneback (2006) John Tromp and Gunnar Farneback. 바둑의 조합이지 2006년 국제 컴퓨터 및 게임 콘퍼런스에서.
* Tsybakov (2008) Alexandre B Tsybakov. _ 비모수 추정의 소개_. 스프링어, 2008년
* Ulyanov et al.(2016) Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. 인스턴스 정규화: 빠른 스타일화를 위한 누락된 구성 요소입니다. _ arXiv:1607.08022_, 2016.
* van den Oord 등(2016) Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 웨이벗: 원시 오디오에 대한 생성 모델입니다. _ arXiv:1609.03499_, 2016a.
* van den Oord et al.(2016b) Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. 픽셀 순환 신경망입니다. 2016b. _ICML_ 에서.
* Van den Oord et al.(2016c)
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _NIPS_, 2017.
* Velickovic et al. [2018] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph Attention Networks. _ICLR_, 2018.
* Velickovic et al. [2019] Petar Velickovic, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution of graph algorithms. _arXiv:1910.10593_, 2019.
* Velickovic et al. [2020] Petar Velickovic, Lars Buesing, Matthew C Overlan, Razvan Pascanu, Oriol Vinyals, and Charles Blundell. Pointer graph networks. _arXiv:2006.06380_, 2020.
* Velickovic et al. [2019] Petar Velickovic, Wiliam Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep Graph Infomax. In _ICLR_, 2019.
* Veselkov et al. [2019] Kirill Veselkov, Guadalupe Gonzalez, Shahad Alijfri, Dieter Galea, Reza Mirnezami, Jozef Youssef, Michael Bronstein, and Ivan Laponogov. Hyperfoods: Machine intelligent mapping of cancer-beating molecules in foods. _Scientific Reports_, 9(1):1-12, 2019.
* Vinyals et al. [2015] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. _arXiv:1506.03134_, 2015.
* Vinyals et al. [2016] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. In _ICLR_, 2016.
* 룩스부르크 및 부스케 [2004] 울리케 폰 룩스부르크 및 올리비에 부스케. L lipschitz 함수를 사용한 거리 기반 분류입니다. _ JMLR_, 5:669-695, 2004.
* Wainwright and Jordan [2008] Martin J Wainwright and Michael Irwin Jordan. _ 그래픽 모델, 지수 패밀리 및 변분 추론_. Now Publishers Inc, 2008.
* Wang and Solomon [2019] Yu Wang and Justin Solomon. 형상 해석을 위한 내인성 연산자와 외인성 연산자. <수치해석의 핸드북> 20권 41-115쪽, Elsevier, 2019.
* Wang et al. [2018] Yu Wang, Mirela Ben-Chen, Iosif Polterovich, and Justin Solomon. Steklov spectral geometry for extrinsic shape analysis. _ACM Trans. Graphics_, 38(1):1-21, 2018.
* Wang et al. [2019a] Yu Wang, Vladimir Kim, Michael Bronstein, and Justin Solomon. 메쉬에서 기하학적 연산자를 학습합니다. 2019a에서 _ICLR Workshops_ 입니다.

* Wang et al. [2019b] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. 포인트 클라우드에 대한 학습을 위한 동적 그래프 CNN입니다. _ ACM Trans. Graphics_, 38(5):1-12, 2019b.
* Wardetzky [2008] Max Wardetzky. 코탄젠트 공식의 수렴: 개요 _ Discrete Differential Geometry_, pages 275-286, 2008.
* Wardetzky et al. [2007] Max Wardetzky, Saurabh Mathur, Felix Kalberer, and Eitan Grinspun. Discrete Laplace operators: no free lunch. In _Symposium on Geometry Processing_, 2007.
* Weiler et al. [2018] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3d steerable cnns: Learning rotationally equivariant features in volumetric data. _arXiv:1807.02547_, 2018.
* Weisfeiler and Leman [1968] Boris Weisfeiler and Andrei Leman. 그래프의 정준형식으로의 축소와 그 안에 나타나는 대수. _ NTI Series_, 2(9):12-16, 1968.
* Werbos [1988] Paul J Werbos. 반복 가스 시장 모델에 적용하여 역전파를 일반화합니다. _ Neural Networks_, 1(4):339-356, 1988.
* Weyl [1929] Hermann Weyl. Elektron und gravitation. i. _Zeitschrift fur Physik_, 56(5-6):330-352, 1929.
* Weyl [2015] Hermann Weyl. _ 대칭_. 프린스턴 대학 출판사 2015년
* Winkels and Cohen [2019] Marysia Winkels and Taco S Cohen. 등분산 cnns를 사용 하 여 ct 스캔에서 폐 결절 감지 합니다. _ Medical Image Analysis_, 55:15-26, 2019.
* Wood and Shawe-Taylor [1996] Jeffrey Wood and John Shawe-Taylor. 표현 이론과 불변 신경망입니다. _ Discrete Applied Mathematics_, 69(1-2):33-60, 1996.
* Wu et al. [2019] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In _ICML_, 2019.
* Wu and He [2018] Yuxin Wu and Kaiming He. 그룹 정규화 2018년 _ECCV_에서.
* Xu 등 [2020a] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. 시간 그래프에 대한 귀납적 표현 학습 _ arXiv:2002.07962_, 2020a.
* Xu et al. [2018] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv:1810.00826_, 2018.

* Xu et al.(2019) Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka. 신경망은 무엇에 대해 추론할 수 있습니까? _ arXiv:1905.13211_, 2019.
* Xu et al.(2020b) Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S Du, Ken-ichi Kawarabayashi, and Stefanie Jegelka. 신경망이 어떻게 외삽되는가: 피드포워드에서 그래프 신경망으로. _ arXiv:2009.11848_, 2020b.
* Yan et al.(2020) Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Heshemi. 신경 실행 엔진: 하위 루틴을 실행하는 학습입니다. _ arXiv:2006.08084_, 2020.
* Yang and Mills (1954) Chen-Ning Yang and Robert L Mills. 동위원소 스핀 및 동위원소 게이지 불변성의 보존 _ Physical Review_, 96(1):191, 1954.
* Yang et al.(2016) Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 그래프 임베딩을 사용한 반지도 학습 재방문 2016년 _ICML_에서.
* Yedidia et al.(2001) Jonathan S Yedidia, William T Freeman, and Yair Weiss. 자유 에너지, 키쿠치 근사치 및 믿음 전파 알고리즘을 베팅합니다. _ NIPS_, 2001.
* Ying 등 (2018) Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. 웹 스케일 추천 시스템을 위한 그래프 합성곱 신경망 2018년 _KDD_에서.
* You et al.(2019) Jiaxuan You, Rex Ying, and Jure Leskovec. 위치 인식 그래프 신경망. 2019년 _ICML_에서.
* Zaheer et al.(2017) Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. 딥세트. 2017년 NIPS에서
* Zaremba and Sutskever (2014) Wojciech Zaremba and Ilya Sutskever. 실행하는 법을 배웁니다. _ arXiv:1410.4615_, 2014.
* Zeng et al.(2012) Wei Zeng, Ren Guo, Feng Luo, and Xianfeng Gu. 이산 열 커널은 이산 리만 메트릭을 결정합니다. _ Graphical Models_, 74(4):121-129, 2012.
* Zhang et al. (2018) Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. 가안: 크고 시공간 그래프에 대한 학습을 위한 게이트 어텐션 네트워크입니다. _ arXiv:1803.07294_, 2018.
* Zhang et al.(2020) Yuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi, and Le Song. 그래프 신경망을 사용한 효율적인 확률 논리 추론_ arXiv:2001.11850_, 2020.

* Zhu et al. [2019] Rong Zhu, Kun Zhao, Hongxia Yang, Wei Lin, Chang Zhou, Baole Ai, Yong Li, and Jingren Zhou. Align: A comprehensive graph neural network platform. _arXiv:1902.08730_, 2019.
* Zhu and Razavian [2019] Weicheng Zhu and Narges Razavian. 전자 건강 기록에 대한 변형 규칙화된 그래프 기반 표현 학습입니다. _ arXiv:1912.03761_, 2019.
* Zhu et al. [2020] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive representation learning. _arXiv:2006.04131_, 2020.
* Zitnik et al. [2018] Marinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polyphar-macy side effects with graph convolutional networks. _Bioinformatics_, 34(13):i457-i466, 2018.

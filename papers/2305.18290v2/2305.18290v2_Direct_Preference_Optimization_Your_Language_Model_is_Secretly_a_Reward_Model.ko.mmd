# 직접 선호도 최적화:

언어 모델이 비밀리에 보상 모델입니다.

 라파엘 라파일로프

동일한 기여; 더 많은 후배 저자들이 앞서 나열되었다.

Archit Sharma1

Eric Mitchell1

Stefano Ermon

1 Stanford University \({}^{\ddagger}\) CZ Biohub

{rafailov,architsh,eric.mitchell}@cs.stanford.edu

크리스토퍼 매닝

Stanford University \({}^{\ddagger}\) CZ Biohub

{rafailov,architsh,eric.mitchell}@cs.stanford.edu

Chelsea Finn

Stanford University \({}^{\ddagger}\) CZ Biohub

{rafailov,architsh,eric.mitchell}@cs.stanford.edu

각주 1: 각주표:

###### Abstract

대규모 비감독 언어 모델(LMs)은 광범위한 세계 지식과 일부 추론 기술을 배우는 반면, 훈련의 완전한 비감독 특성으로 인해 행동의 정확한 제어를 달성하는 것은 어렵다. 이러한 조향성을 얻기 위한 기존의 방법은 모델 세대의 상대적인 품질의 인간 라벨을 수집하고 이러한 선호도에 맞추기 위해 감독되지 않은 LM을 미세 조정하며, 종종 인간 피드백(RLHF)으로부터의 강화 학습과 함께 조정한다. 그러나 RLHF는 복잡하고 종종 불안정한 절차로, 먼저 인간의 선호도를 반영하는 보상 모델을 피팅한 다음 강화 학습을 사용하여 대규모 비지도 LM을 미세 조정하여 원래 모델에서 너무 멀리 표류하지 않고 추정된 보상을 최대화한다. 본 논문에서는 간단한 분류 손실만으로 표준 RLHF 문제를 해결할 수 있는 폐쇄형 최적 정책 추출이 가능한 RLHF 보상 모델의 새로운 파라미터화를 소개한다. 우리가 _직접 선호 최적화_(DPO)라고 부르는 결과 알고리즘은 안정적이고 성능이 뛰어나며 계산적으로 가벼워 미세 조정 또는 상당한 하이퍼파라미터 조정을 수행하는 동안 LM에서 샘플링할 필요가 없다. 우리의 실험은 DPO가 기존의 방법뿐만 아니라 인간의 선호도와 일치하거나 더 나은 LM을 미세 조정할 수 있음을 보여준다. 특히, DPO와의 미세 조정은 세대의 감성을 제어하는 능력에서 PPO 기반 RLHF를 초과하고, 구현 및 훈련이 상당히 간단하면서 요약 및 싱글 턴 대화에서 응답 품질을 일치시키거나 개선한다.

## 1 Introduction

매우 큰 데이터 세트에 대해 트레이닝된 대규모 비감독 언어 모델(LMs)은 놀라운 능력을 획득한다[11; 7; 40; 8]. 그러나, 이러한 모델들은 매우 다양한 목표들, 우선순위들, 및 기술 세트들을 갖는 인간에 의해 생성된 데이터에 대해 트레이닝된다. 이러한 목표와 기술 집합 중 일부는 모방하는 것이 바람직하지 않을 수 있습니다. 예를 들어 AI 코딩 보조자가 일반적인 프로그래밍 오류를 수정하기 위해 _이해_하기를 원할 수 있지만 그럼에도 불구하고 코드를 생성할 때 모델을 훈련 데이터에 존재하는 (잠재적으로 희귀한) 고품질 코딩 능력으로 편향시키고 싶습니다. 유사하게, 우리는 우리의 언어 모델이 50%의 사람들이 믿는 일반적인 오개념을 _인식_하기를 원할 수 있지만, 우리는 모델이 그것에 대한 50%의 질의에서 이 오개념이 사실이라고 주장하는 것을 확실히 원하지 않습니다! 즉, 매우 광범위한 _지식 및 능력_에서 모델의 _원하는 응답 및 행동_을 선택하는 것은 안전하고 수행 가능하며 제어할 수 있는 AI 시스템을 구축하는 데 매우 중요합니다. 기존 방법은 일반적으로 강화 학습(RL)을 사용하여 인간의 선호도와 일치하도록 LM을 조종하지만, 기존 방법이 사용하는 RL 기반 목적은 간단한 이진 교차 엔트로피 목적으로 정확하게 최적화되어 선호도 학습 파이프라인을 크게 단순화할 수 있음을 보여준다.

높은 수준에서 기존 방법은 인간이 안전하고 도움이 된다고 생각하는 행동의 유형을 나타내는 선별된 인간 선호 세트를 사용하여 원하는 행동을 언어 모델에 주입한다. 이 선호도 학습 단계는 대규모 텍스트 데이터 세트에 대한 대규모 비지도 사전 훈련의 초기 단계 이후에 발생한다. 선호도 학습에 대한 가장 간단한 접근법은 고품질 응답의 인간 시연에 대한 감독 미세 조정이지만, 가장 성공적인 방법의 클래스는 인간(또는 AI) 피드백(RLHF/RLAIF; [12; 2])으로부터의 강화 학습이다. RLHF 방법들은 보상 모델을 인간의 선호도의 데이터세트에 적합시킨 다음, RL을 사용하여 원래 모델로부터 과도하게 멀리 표류하지 않고 높은 보상이 할당된 응답을 생성하기 위해 언어 모델 정책을 최적화한다. RLHF는 인상적인 대화 및 코딩 능력을 가진 모델을 생성하지만, RLHF 파이프라인은 지도 학습보다 상당히 복잡하며, 이는 다수의 LMs를 훈련시키고 훈련 루프에서 LM 정책으로부터의 샘플링을 수반하여 상당한 계산 비용을 발생시킨다.

본 논문에서는 명시적 보상 모델링이나 강화 학습 없이 인간의 선호를 고수하기 위해 언어 모델을 직접 최적화하는 방법을 보여준다. 기존의 RLHF 알고리즘과 동일한 목적(KL-divergence 제약 조건을 갖는 보상 최대화)을 암묵적으로 최적화하는 알고리즘인 _직접 선호 최적화(DPO)_를 제안하지만 구현은 간단하며 훈련도 간단하다. 직관적으로, DPO 업데이트는 선호되지 않는 응답보다 선호되는 상대적 로그 확률을 증가시키지만, 순진한 확률 비율 목표로 발생하는 모델 변성을 방지하는 동적, 예당 중요도 가중치를 통합한다. 기존 알고리즘과 마찬가지로 DPO는 주어진 보상 함수가 경험적 선호도 데이터와 얼마나 잘 일치하는지 측정하는 이론적 선호도 모델(브래들리-테리 모델; [5])에 의존한다. 그러나 기존의 방법들은 선호 모델을 이용하여 선호 손실을 정의하여 보상 모델을 학습시킨 후 학습된 보상 모델을 최적화하는 정책을 학습시키는 반면, DPO는 변수의 변화를 이용하여 선호 손실을 정책의 함수로 직접 정의한다. 따라서 모델 응답에 대한 인간 선호의 데이터 세트가 주어지면 DPO는 간단한 이진 교차 엔트로피 목표를 사용하여 정책을 최적화하여 선호 데이터에 맞는 암시적 보상 함수에 최적의 정책을 생성할 수 있다.

우리의 주요 기여는 선호도로부터 언어 모델을 훈련시키기 위한 간단한 RL-프리 알고리즘인 직접 선호도 최적화(DPO)이다. 우리의 실험은 DPO가 최대 6B 매개 변수를 가진 언어 모델을 사용하여 감정 변조, 요약 및 대화와 같은 태스크의 선호도로부터 학습하기 위해 PPO 기반 RLHF를 포함한 기존 방법만큼 적어도 효과적임을 보여준다.

## 2 관련 작업

증가하는 스케일의 자기-감독 언어 모델들은 제로-샷(zero-shot) 또는 적은-샷 프롬프트들(6; 25; 11)로 일부 태스크들을 완료하는 것을 학습한다. 그러나, 다운스트림 태스크들에 대한 그들의 성능 및 사용자 의도와의 정렬은 명령어들 및 인간-작성된 완성들의 데이터세트들에 대한 미세-조정(fine-tuning)에 의해 상당히 개선될 수 있다[23; 36; 13; 39]. 이러한 '명령-튜닝' 절차는 LLMs가 명령-튜닝 세트 외부의 명령들로 일반화되고 일반적으로 그들의 사용성을 증가시킬 수 있게 한다[13]. 명령 튜닝의 성공에도 불구하고 응답 품질에 대한 _상대_ 인간 판단은 종종 전문가 시연보다 수집하기 쉬우며, 따라서 후속 작업은 인간 선호도의 데이터 세트로 LLM을 미세 조정하여 번역[18], 요약[38;49], 스토리텔링[49] 및 명령 후속[26;32]의 숙련도를 향상시켰다. 이러한 방법들은 우선 선호 모델 하에서 선호들의 데이터세트와의 호환성을 위해 신경망 보상 함수를 최적화한다.

그림 1: **DPO는 강화 학습을 피하면서 인간 선호도를 최적화합니다. 인간 피드백으로 언어 모델을 미세 조정하는 기존의 방법은 먼저 응답 쌍에 대한 프롬프트 및 인간 선호의 데이터 세트에 보상 모델을 피팅한 다음 RL을 사용하여 학습된 보상을 최대화하는 정책을 찾는다. 대조적으로, DPO는 간단한 분류 목표를 사용하여 선호도를 가장 잘 충족하는 정책에 직접 최적화하여 해당 최적 정책이 닫힌 형태로 추출될 수 있는 _암시적_ 보상 모델을 피팅합니다.**

브래들리-테리 모델[5] 다음에, 강화 학습 알고리즘들, 일반적으로 REINFORCE[45], 근접 정책 최적화(PPO; [37]) 또는 변형들[32]을 사용하여 주어진 보상을 최대화하도록 언어 모델을 미세 조정한다. 밀접하게 관련된 작업 라인은 LLM 주석에 대한 텍스트 루브릭 형태의 인간으로부터의 약한 감독만을 사용하여 안전 또는 무해성과 같은 표적 속성에 대한 추가 합성 선호도 데이터를 생성하기 위해 인간 피드백으로 후속 지시를 위해 미세 조정된 LLM을 활용한다[2]. 이러한 방법들은 다양한 목적들을 위한 강화 학습과 함께 언어 모델들을 트레이닝하기 위한 한 작업체[33; 27; 46] 및 인간의 선호도들로부터 학습하기 위한 일반적인 방법들에 대한 다른 작업체[12; 19]의 두 작업체의 융합을 나타낸다. 상대적 인간 선호도를 사용하는 매력에도 불구하고 강화 학습으로 대규모 언어 모델을 미세 조정하는 것은 여전히 중요한 실제 과제이며, 이 작업은 RL 없이 상대적 선호도를 최적화하는 이론적으로 정당화된 접근법을 제공한다.

언어의 맥락 외에도 선호도로부터의 학습 정책은 강도 및 강화 학습 설정 모두에서 연구되었으며 몇 가지 접근법이 제안되었다. 보상보다는 행동의 선호도 또는 순위를 이용한 상황적 강도 학습은 상황적 결투 강도(CDB; [48; 14])로 알려져 있다. 절대 보상이 없는 경우 CDB에 대한 이론적 분석에서는 최적 정책의 개념을 _von Neumann 승자_로 대체하며, 이 정책은 _any_ 다른 정책에 대한 예상 승률이 최소 50%입니다[14]. 그러나, CDB 설정에서, 선호도 라벨들은 온라인으로 주어지는 반면, 인간 선호도들로부터 학습함에 있어서, 우리는 전형적으로 오프라인 선호도-주석이 달린 액션 쌍들의 고정된 배치로부터 학습한다[47]. 마찬가지로, _선호도 기반 RL_(PbRL)은 보상이 아닌 _알 수 없는_'점수 매기기' 함수에 의해 생성된 이진 선호도로부터 학습한다[9; 35]. PbRL에 대한 다양한 알고리즘들은 오프-정책 선호도 데이터를 재사용할 수 있는 방법들을 포함하여 존재하지만, 일반적으로 먼저 잠재 스코어링 함수(즉, 보상 모델)를 명시적으로 추정하고 후속적으로 이를 최적화하는 것을 수반한다[16; 9; 12; 34; 19]. 대신 선호도를 충족시키기 위해 정책을 직접 최적화하는 단일 단계 정책 학습 접근법을 제시한다.

## 3 Preliminaries

우리는 Ziegler 등(그리고 이후 [38; 1; 26])에서 RLHF 파이프라인을 검토한다. 일반적으로 1) 감독 미세 조정(supervised fine-tuning; SFT), 2) 선호도 샘플링 및 보상 학습, 3) RL 최적화 등의 세 단계를 포함한다.

**SFT**: RLHF는 일반적으로 관심 있는 다운스트림 작업(대화, 요약 등)에 대한 고품질 데이터에 대한 지도 학습으로 사전 훈련된 LM을 미세 조정하여 모델 \(\pi^{\text{SFT}}}\)을 얻는 것으로 시작합니다.

**보상 모델링 단계**: 두 번째 단계에서는 SFT 모델에 프롬프트 \(x\)를 사용하여 응답 쌍 \((y_{1},y_{2})\sim\pi^{\text{SFT}}}(y\mid x)\를 생성합니다. 그런 다음 한 가지 대답에 대한 선호도를 나타내는 인간 라벨러에게 제공되며, 여기서 \(y_{w}\succ y_{l}\mid x\)로 표시된다. 여기서 \(y_{w}\) 및 \(y_{l}\)는 각각 \((y_{1},y_{2})\) 사이에서 선호 및 거부 완료를 나타낸다. 선호도는 접근 권한이 없는 잠재 보상 모델 \(r^{*}(y,x)\)에 의해 생성된다고 가정한다. 선호도를 모델링하는 데 사용되는 많은 접근법이 있으며, 브래들리-테리(BT)[5] 모델은 인기 있는 선택이다(더 일반적인 플래킷-루스 순위 모델[30; 21]은 여러 순위 답변에 액세스할 수 있는 경우 프레임워크와도 호환된다). BT 모델은 인간의 선호도 분포 \(p^{*}\)는 다음과 같이 작성될 수 있다고 규정한다:

\[p^{*}(y_{1}\succ y_{2}\mid x)=\frac{\exp{(r^{*}(x,y_{1}))}}{\exp{(r^{*}(x,y_{1}))}+\exp{(r^{*}(x,y_{2}))}}. \tag{1}\

정적 비교 데이터 세트에 대한 액세스를 가정하면 \(p^{*}\)에서 샘플링된 \(\mathcal{D}=\left\{x^{(}),y_{w}^{(i)},y_{l}^{(i)}\right\}_{i=1}^{N}\) 보상 모델 \(r_{\phi}(x,y)\)을 매개변수화하고 최대 우도를 통해 매개변수를 추정할 수 있다. 문제를 이진 분류로 프레이밍하면 음의 로그 우도 손실이 있습니다.

\[\mathcal{L}_{R}(r_{\phi},\mathcal{D})=-\mathbb{E}_{(x,y_{w},y)\sim\mathcal{D}}\left[\log\sigma(r_{\phi}(x,y_{w})-r_{\phi}(x,y_{l}))\right] \tag{2}\

여기서 \(\sigma\)는 로지스틱 함수이다. LLM의 맥락에서 네트워크 \(r_{\phi}(x,y)\)는 종종 보상 값에 대한 단일 스칼라 예측을 생성하는 최종 변압기 층 위에 선형 층을 추가하여 SFT 모델 \(\pi^{\text{SFT}}(y\mid x)\)로부터 초기화된다. 보다 낮은 분산을 갖는 보상 함수를 보장하기 위해, 선행 연구들은 모든 }x\에 대해 \(\mathbb{E}_{x,y\sim\mathcal{D}}}\left[r_{\phi}(x,y)\right]=0\text{이 되도록 보상을 정규화한다.

**RL 미세 조정 단계**: RL 단계에서 학습된 보상 함수를 사용하여 언어 모델에 피드백을 제공합니다. 특히, 다음과 같은 최적화 문제를 공식화한다.

\[\max_{\pi_{\theta}(y\mid x)\big{]}-\beta\mathbb{D}_{\text{KL}}\big{[}r_{\phi}(x,y)\big{}_{\text{E}_{x\sim\mathcal{D},y\sim\pi_{\theta}(y\mid x)\mid\pi_{\text{ref}}(y\mid x)\big{]} \tag{3}\] 여기서 \(\beta\는 기본 참조 정책 \(\pi_{\text{ref}}\), 즉 초기 SFT 모델 \(\pi^{\text{SFT}}\). 실제로 언어 모델 정책 \(\pi_{\theta}\)도 \(\pi^{\text{SFT}}\)로 초기화된다. 추가된 제약 조건은 모델이 보상 모델이 정확한 분포에서 너무 멀리 벗어나는 것을 방지하고, 생성 다양성을 유지하고 단일 고보상 답변으로의 모드 붕괴를 방지하기 때문에 중요하다. 언어 생성의 이산적 특성으로 인해, 이 목표는 미분 가능하지 않으며, 일반적으로 강화 학습으로 최적화된다. 표준 접근법 [49; 38; 1; 26]은 보상 함수 \(r(x,y)=r_{\phi}(x,y)-\beta(\log\pi_{\theta}(y\mid x)-\log\pi_{\text{ref}}(y \mid x))\)를 구성하고 PPO[37]를 사용하여 최대화하는 것이다.

## 4 직접 선호도 최적화

미세 조정 언어 모델과 같은 대규모 문제에 강화 학습 알고리즘을 적용하는 어려움에 동기부여된 우리의 목표는 선호도를 직접 사용하여 정책 최적화를 위한 간단한 접근법을 도출하는 것이다. 보상을 학습한 다음 RL을 통해 최적화하는 이전의 RLHF 방법과 달리, 우리의 접근법은 RL 훈련 루프 없이 닫힌 형태로 최적의 정책을 추출할 수 있는 보상 모델 매개변수화의 특정 선택을 활용한다. 다음으로 자세히 설명하겠지만, 우리의 핵심 통찰력은 보상 함수에서 최적 정책으로의 분석 매핑을 활용하는 것이며, 이를 통해 보상 함수 이상의 손실 함수를 정책 이상의 손실 함수로 변환할 수 있다. 이 변수 변경 접근법은 브래들리-테리 모델과 같은 인간의 선호도에 대한 기존 모델 하에서 여전히 최적화하면서 명시적이고 독립 실행형 보상 모델을 맞추는 것을 피한다. 본질적으로 정책 네트워크는 언어 모델과 (묵시적) 보상을 모두 나타낸다.

**DPO 목표 도출** 이전 작업과 동일한 RL 목표, Eq로 시작합니다. 3, 일반적인 보상 함수 \(r\). 선행 연구[29; 28; 17; 15]에 이어 식에서 KL 제한 보상 최대화 목표에 대한 최적 해를 보여주는 것은 간단하다. 도 3은 형태를 취한다:

\[\pi_{r}(y\mid x)=\frac{1}{Z(x)}\pi_{\text{ref}}(y\mid x)\exp\left(\frac{1}{ \beta}r(x,y)\right), \tag{4}\]

여기서 \(Z(x)=\sum_{y}\pi_{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)\)는 분할 함수이다. 전체 도출은 부록 A.1을 참조하십시오. 접지-진실 보상 함수 \(r^{*}\)의 MLE 추정치 \(r_{\phi}\)를 사용하더라도 분할 함수 \(Z(x)\)[17; 15]를 추정하는 것은 여전히 비용이 많이 들며, 이는 이 표현을 실제로 사용하기 어렵게 만든다. 그러나 식은 재정렬할 수 있습니다. 4는 보상 함수를 해당 최적 정책 \(\pi_{r}\), 참조 정책 \(\pi_{\text{ref}}\), 미지 분할 함수 \(Z(\cdot)\로 표현한다. 구체적으로, 우리는 먼저 Eq의 양변에 로그를 취한다. 4, 그리고 나서 몇 가지 대수로 우리는 얻는다:

\[r(x,y)=\beta\log\frac{\pi_{r}(y\mid x)}{\pi_{\text{ref}}(y\mid x)}+\beta\log Z(x). \tag{5}\]

우리는 이 재매개변수를 지상-진실 보상 \(r^{*}\)과 그에 상응하는 최적 모델 \(\pi^{*}\)에 적용할 수 있다. 다행히도 Bradley-Terry 모델은 두 완성 사이의 보상 차이, 즉 \(p^{*}(y_{1}\succ y_{2}\mid x)=\sigma(r^{*}(x,y_{1})-r^{*}(x,y_{2}))\에 의존한다. 재매개변수화를 식에 대입합니다. 5는 \(r^{*}(x,y)\)에 대한 선호도 모델 Eq. 1은 분할 함수를 해제하고, 최적의 정책 \(\pi^{*}\)과 참조 정책 \(\pi_{\text{ref}}\)만으로 인간의 선호 확률을 표현할 수 있다. 따라서 브래들리-테리 모델 하에서 최적의 RLHF 정책 \(\pi^{*}\)은 선호 모델을 만족한다:

\[p^{*}(y_{1}\succ y_{2}\mid x)=\frac{1}{1+\exp\left(\beta\log\frac{\pi^{*}(y _{2}|x)}{\pi_{\text{ref}}(y_{2}|x)}-\beta\log\frac{\pi^{*}(y_{1}|x)}{\pi_{ \text{ref}}(y_{1}|x)}\right)} \tag{6}\]

유도는 부록 A.2에 있는 반면 Eq. 6은 Bradley-Terry 모델을 사용하며, 부록 A.3에 표시된 보다 일반적인 Plackett-Luce 모델[30; 21] 하에서 유사하게 식을 도출할 수 있다.

이제 보상 모델이 아닌 최적의 정책 측면에서 인간 선호 데이터의 확률을 갖게 되었으므로 매개변수화된 정책에 대한 최대 가능성 목표를 공식화할 수 있다 \(\pi_{\theta}\). 보상 모델링 접근법(즉, 식 2)과 유사한 우리의 정책 목표는 다음과 같습니다.

\[\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}})=-\mathbb{E}_{(x,y_{w },y_{l})\sim\mathcal{D}}\left[\log\sigma\left(\beta\log\frac{\pi_{\theta}(y_{w }\mid x)}{\pi_{\text{ref}}(y_{w}\mid x)}-\beta\log\frac{\pi_{\theta}(y_{l} \mid x)}{\pi_{\text{ref}}(y_{l}\mid x)}\right)\right]. \tag{7}\

이러한 방법으로, 우리는 최적 정책이 단순히 \(\pi_{\theta}\)인 대안적 매개변수화를 사용하여 암묵적 보상을 피팅한다. 또한, 본 절차는 재파라미터화된 브래들리-테리 모델을 피팅하는 것과 동일하기 때문에 선호도 데이터 분포의 적절한 가정 하에서 일관성과 같은 특정 이론적 특성을 향유한다[4]. 섹션 5에서는 다른 작업과 관련하여 DPO의 이론적 속성에 대해 추가로 논의한다.

**DPO 업데이트는 무엇을 하나요?* * DPO에 대한 기계론적 이해를 위해 손실 함수 \(\mathcal{L}_{\text{DPO}}\)의 기울기를 분석하는 것이 유용합니다. 파라미터 \(\theta\)에 대한 구배는 다음과 같이 기입될 수 있다:

보상 추정치가 잘못된 경우 \[-\beta\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\bigg{[}_{ \begin{subarray}{c}\text{higher weight at wrong} \end{subarray}}\bigg{[}\underbrace{\nabla_{\theta}\log\pi(y_{w}\mid x)}_{ \text{increase likelihood of }y_{w}}-\underbrace{\nabla_{\theta}\log\pi(y_{l}\mid x)}_{ \text{decrease likelihood of }y_{l}}\bigg{]}\bigg{]},\

여기서, \(\hat{r}_{\theta}(x,y)=\beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\text{ref}}(y|x)}\)는 언어 모델 \(\pi_{\theta}\) 및 참조 모델 \(\pi_{\text{ref}}\)에 의해 암묵적으로 정의되는 보상이다(보다 섹션 5에서). 직관적으로 손실함수 \(\mathcal{L}_{\text{DPO}}\)의 기울기는 선호완결가능성 \(y_{w}\)을 증가시키고 비선호완결가능성 \(y_{l}\)을 감소시킨다. 중요하게도, 예제들은 암시적 보상 모델 \(\hat{r}_{\theta}\)이 불성실한 완결들을 얼마나 높게 평가하는지, \(\beta\)로 스케일링되는지, 즉 암시적 보상 모델이 KL 제약의 강도를 고려하여 완결들을 얼마나 부정확하게 명령하는지에 의해 칭량된다. 우리의 실험은 가중치 계수가 없는 이 방법의 순진한 버전으로서 언어 모델이 퇴보할 수 있기 때문에 이 가중치의 중요성을 제안한다(부록 표 3).

**DPO 개요** 일반적인 DPO 파이프라인은 다음과 같습니다. 1) 모든 프롬프트 \(x\)에 대해 샘플 완성 \(y_{1},y_{2}\sim\pi_{\text{ref}}(\cdot\mid x)\), 선호도 \(\mathcal{D}=\{x^{(i)},y_{w}^{(i)},y_{l}\}_{i=1}^{N}\)의 오프라인 데이터 세트를 구성하기 위해 인간 선호도를 가진 레이블 \(\pi_{\theta}\)을 최적화하여 주어진 \(\pi_{\text{ref}\) 및 \(\mathcal{D}\) 및 원하는 \(\beta\)에 대해 \(\mathcal{L}_{\text{DPO}\)를 최소화합니다. 실제로는 샘플을 생성하고 인간의 선호도를 수집하는 대신 공개적으로 사용 가능한 선호도 데이터 세트를 재사용하고 싶다. 선호도 데이터 세트는 \(\pi^{\text{STF}}\)를 사용하여 샘플링되므로 사용할 수 있을 때마다 \(\pi_{\text{ref}}=\pi^{\text{SFT}}\)를 초기화한다. 그러나 \(\pi^{\text{SFT}}\)를 사용할 수 없는 경우 \(\pi_{\text{ref}}\)을 초기화하는 것은 선호하는 완성도의 가능성을 최대화하여 \((x,y_{w})\), 즉 \(\pi_{\text{ref}}=\arg\max_{\pi}\mathbb{E}_{x,y_{w}\sim\mathcal{D}}\left[\log \pi(y_{w}\mid x)\right]\이다. 이 절차는 사용할 수 없는 참 참조 분포와 DPO에서 사용하는 \(\pi_{\text{ref}}}\) 사이의 분포 이동을 완화하는 데 도움이 됩니다. 구현 및 하이퍼파라미터와 관련된 자세한 내용은 부록 B에서 확인할 수 있다.

## 5 DPO의 이론적 분석

이 섹션에서는 DPO 방법에 대한 추가 해석을 제공하고 이론적 뒷받침을 제공하며 DPO의 이점을 RLHF에 사용되는 행위자 비평가 알고리즘(예: PPO [37])의 문제와 연관시킨다.

### 언어 모델은 비밀리에 보상 모델입니다.

DPO는 명시적 보상을 피팅하는 것과 단일 최대 가능성 목표를 사용하여 정책을 학습하기 위해 RL을 수행하는 것 모두를 우회할 수 있다. 최적화 목표 식에 유의하십시오. 5는 보상 매개변수화 \(r^{*}(x,y)=\beta\log\frac{\pi_{\theta}^{*}(y|x)}{\pi_{\text{ref}}(y|x)}\)를 갖는 Bradley-Terry 모델과 동등하며, Eq.의 보상 모델 최적화와 동등하게 매개변수 모델 \(\pi_{\theta}\)을 최적화한다. 2. 변수의 변화하에서. 이 섹션에서는 이러한 재매개 변수화 이면의 이론을 구축하고 학습된 보상 모델의 클래스를 제한하지 않으며 최적의 정책을 정확하게 복구할 수 있음을 보여준다. 우리는 보상 함수들 사이의 동등성 관계를 정의하는 것으로 시작한다.

**정의 1**. _두 개의 보상 함수 \(r(x,y)\)와 \(r^{\prime}(x,y)\)가 일부 함수 \(f\)에 대해 동일한 iff \(r(x,y)-r^{\prime}(x,y)=f(x)\라고 합니다._

이것은 보상 함수 집합을 클래스로 분할하는 등가 관계라는 것을 쉽게 알 수 있다. 다음 두 가지 레마를 말할 수 있습니다.

**Lemma 1**.: _Plackett-Luce, 특히 Bradley-Terry, 선호도 프레임워크에서 동일한 클래스의 두 보상 함수는 동일한 선호도 분포를 유도합니다._

**Lemma 2**.: _동일한 동등성 클래스의 두 보상 함수는 제약된 RL 문제에서 동일한 최적 정책을 유도합니다._

그 증명들은 간단하며 우리는 그것들을 부록 A.5로 미룬다. 첫 번째 보조 문제는 플랙트 루스 계열의 모델들과 잘 알려진 사양 부족 문제이다[30]. 이러한 지정 부족 때문에 일반적으로 Eq의 MLE 추정치에 대한 보장을 달성하기 위해 추가 식별 가능성 제약을 부과해야 한다. 2[4]. 두 번째 보조 정리는 동일한 클래스의 모든 보상 함수가 동일한 최적 정책을 산출한다고 명시하며, 따라서 최종 목적을 위해 우리는 최적 클래스에서 임의의 보상 함수를 복구하는 데에만 관심이 있다. 우리는 부록 A.6에서 다음과 같은 정리를 증명한다.

**정리 1**.: _온화한 가정 하에서 Plackett-Luce(및 특히 Bradley-Terry) 모델과 일치하는 모든 보상 클래스는 일부 모델 \(\pi(y\mid x)\) 및 주어진 참조 모델 \(\pi_{\text{ref}}(y\mid x)\)에 대한 재매개 변수화 \(r(x,y)=\beta\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\로 나타낼 수 있습니다._

증명스케치: 임의의 보상함수 \(r(x,y)\)를 고려하며, 이는 식 4에 의해 특정된 대응하는 최적 모델 \(\pi_{r}(y\mid x)\)을 유도한다. 우리는 \(r\)의 등가 클래스로부터의 보상함수가 위에서 주어진 재매개변수를 사용하여 표현될 수 있음을 보일 것이다. 우리는 투영 \(f\)를 다음과 같이 정의한다.

\[f(r;\pi_{\text{ref}},\beta)(x,y)=r(x,y)-\beta\log\sum_{y}\pi_{\text{ref}}(y \mid x)\exp\left(\frac{1}{\beta}r(x,y)\right) \tag{8}\]

연산자 \(f\)는 단순히 보상함수를 \(\pi_{r}\)의 분할함수에 로그를 취한 값으로 정규화한다. 추가된 정규화 항은 접두어 \(x\)의 함수일 뿐이므로, \(f(r;\pi_{\text{ref}},\beta)(x,y)\)는 \(r(x,y)\)의 등가 클래스에서 보상 함수이다. 마지막으로 \(r\)을 Eq.의 RHS로 대체한다. 5(임의의 보상 함수에 대해 성립함)는 \(f(r;\pi_{\text{ref}},\beta)(x,y)=\beta\log\frac{\pi_{r}(y|x)}{\pi_{\text{ref}}(y|x)}\이다. 즉, 투영 \(f\)은 원하는 형태로 등가 클래스 \(r\)의 멤버를 생성하며, 제안된 재매개변수화로부터 보상 모델에서 일반성을 잃지 않는다.

우리는 대안적으로 정리 1을 DPO 재매개 변수가 각 동등성 클래스 내에서 정확히 어떤 보상 함수, 즉 만족시키는 보상 함수를 선택하는지를 명시하는 것으로 볼 수 있다:

\[\sum_{y}\underbrace{\pi_{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y) \right)}_{=\pi(y|x),\text{ using Thm. 1 reparam.}}=1, \tag{9}\]

즉, \(\pi(y\mid x)\)는 유효한 분포(확률은 양수이고 합은 1)입니다. 그러나, 식에 따른다. 4, 우리는 그 Eq를 볼 수 있습니다. 도 9는 보상 함수 \(r(x,y)\)에 의해 유도된 최적 정책의 분할 함수이다. DPO 알고리즘의 핵심 통찰은 표현 가능한 보상 모델의 클래스를 보존하지만 Eq에서 최적의 정책을 명시적으로 만들기 위해 과소 제약된 Plackett-Luce(및 특히 Bradley-Terry) 선호 모델 패밀리에 특정 제약을 가할 수 있다는 것이다. 4는 모든 프롬프트 \(x\)에 대해 분석적으로 다루기 쉽습니다.

### Actor-Critic 알고리즘 불안정성

또한 프레임워크를 사용하여 PPO와 같은 RLHF에 사용되는 표준 액터-크리틱 알고리즘으로 불안정성을 진단할 수 있다. 우리는 RLHF 파이프라인을 따르고 섹션 3에 요약된 RL 미세 조정 단계에 초점을 맞춘다. 우리는 3에 요약된 제한된 RL 문제에 대한 추론 프레임워크 [20]으로서 컨트롤에 대한 연결을 그릴 수 있다. 우리는 매개변수화된 모델 \(\pi_{\theta}(y\mid x)\)을 가정하고 \(\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x)\mid\pi^{*}(y\mid x)]\)를 최소화한다. 여기서 \(\pi^{*}\)는 Eq로부터 최적 정책이다. 7은 보상함수 \(r_{\phi}(y,x)\)에 의해 유도된다. 몇 가지 대수를 사용하면 최적화 목표를 달성할 수 있습니다.

\[\max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y|x)}\bigg{[}\underbrace{r_{\phi}(x,y)-\beta\log\sum_{y}\pi_{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r_{ \phi}(x,y)\right)}_{f(r_{\phi},\pi_{\text{ref}},\beta)}-\underbrace{\beta\log \frac{\pi_{\theta}(y\mid x)}{\pi_{\text{ref}}(y\mid x)}}_{\text{KL}}\bigg{]} \tag{10}\]

이는 이전 연구들 [49; 38; 1; 26]에서 최적화된 것과 동일한 목적이며, 보상 클래스 \(r_{\phi}\)에 대한 DPO 등가 보상을 사용한다. 이 설정에서는 \(f(r_{\phi},\pi_{\text{ref}},\beta)\)의 정규화 항을 참조 정책의 소프트 값 함수 \(\pi_{\text{ref}}}\)로 해석할 수 있다. 이 항은 최적해에 영향을 미치지 않지만, 그렇지 않으면 목표의 정책 기울기가 분산이 높아 학습을 불안정하게 만들 수 있다. 학습된 값 함수를 사용하여 정규화 항을 수용할 수 있지만 최적화하기 어려울 수도 있습니다. 대안적으로, 선행 작업들은 인간 완료 기준선, 본질적으로 정규화 항의 단일 샘플 몬테카를로 추정치를 사용하여 정규화된 보상을 갖는다. 대조적으로, DPO 재매개변수는 기준선이 필요하지 않은 보상 함수를 산출한다.

## 6 Experiments

이 절에서는 선호도에서 직접 정책을 훈련하는 DPO의 능력을 경험적으로 평가한다. 먼저, 잘 통제된 텍스트 생성 환경에서 우리는 PPO와 같은 일반적인 선호도 학습 알고리즘과 비교하여 DPO가 보상 최대화와 참조 정책으로 KL-발산을 얼마나 효율적으로 상쇄하는지 묻는다. 다음으로 요약 및 대화를 포함하여 더 큰 모델과 더 어려운 RLHF 태스크에 대한 DPO의 성능을 평가한다. 우리는 하이퍼파라미터의 튜닝이 거의 없는 경우, DPO는 PPO를 사용한 RLHF와 같은 강한 기준선뿐만 아니라 학습된 보상 함수 하에서 가장 좋은 \(N\) 샘플링 궤적을 반환하는 경향이 있음을 발견했다. 이러한 결과를 제시하기 전에 실험 설정을 설명하며 추가 세부 사항은 부록 C에 나와 있다.

**작업** 실험에서는 세 가지 다른 개방형 텍스트 생성 작업을 탐색합니다. 모든 실험에서 알고리즘은 선호도 \(\mathcal{D}=\left\{x^{(i)},y_{w}^{(i)},y_{l}^{(i)}\right\}_{i=1}^{N}\)의 데이터세트로부터 정책을 학습한다. **통제된 감성 생성** 에서 \(x\)는 IMDb 데이터 세트 [22]의 영화 리뷰 접두사이며 정책은 긍정적인 감성으로 \(y\)를 생성해야 합니다. 통제된 평가를 수행하기 위해 이 실험을 위해 사전 훈련된 감성 분류기를 사용하여 세대에 걸쳐 선호도 쌍을 생성합니다. 여기서 \(p\)(positive \(\mid x,y_{w}\rangle>p\)(positive \(\mid x,y_{l}\rangle\). SFT의 경우 IMDB 데이터 세트의 열차 분할 리뷰에서 수렴할 때까지 GPT-2를 크게 미세 조정한다(앱 C.1의 추가 세부 정보). **요약** 에서 \(x\)는 Reddit의 포럼 게시물입니다. 정책은 게시물의 주요 요점의 요약 \(y\)를 생성해야 합니다. 이전 작업에 이어 Stiennon 등이 수집한 인간 선호도와 함께 Reddit TL;DR 요약 데이터 세트 [41]을 사용한다. 우리는 RLHF에 대한 TRLX [42] 프레임워크와 함께 요약 2의 인간 작성 포럼 게시물에서 미세 조정된 SFT 모델을 사용한다. 인간 선호도 데이터 세트는 Stiennon 등에 의해 다르지만 유사하게 훈련된 SFT 모델의 샘플에 대해 수집되었다. 마지막으로, **단일 전환 대화** 에서 \(x\)는 인간 쿼리이며, 이는 천체 물리학에 대한 질문에서 관계 조언 요청에 이르기까지 모든 것일 수 있습니다. 정책은 사용자의 질의에 대한 매력적이고 유용한 응답 \(y\)을 생성해야 하며, 인간과 자동화된 어시스턴트 사이의 170k 대화 내용을 포함하는 인류학적 도움 및 무해 대화 데이터 세트 [1]을 사용한다. 각 전사체는 인간이 선호하는 응답을 나타내는 선호 라벨과 함께 큰(알 수 없지만) 언어 모델에 의해 생성된 응답 쌍으로 끝난다. 이 설정에서는 사전 훈련된 SFT 모델을 사용할 수 없으므로 SFT 모델을 형성하기 위해 선호하는 완성에서만 기성 언어 모델을 미세 조정한다.

각주 2: [https://huggingface.co/CarperAI/openai_summarize_tldr_sft](https://huggingface.co/CarperAI/openai_summarize_tldr_sft)

**평가.** 실험은 평가에 대해 두 가지 다른 접근 방식을 사용합니다. 제한된 보상 최대화 목표를 최적화하는데 있어서 각 알고리즘의 효용성을 분석하기 위해, 제어된 감정 생성 설정에서 기준 정책으로부터 달성된 보상과 KL-발산의 프론티어에 의해 각 알고리즘을 평가한다. 이 프론티어는 지상-진실 보상 함수(감성 분류기)에 접근할 수 있기 때문에 계산 가능하다. 그러나 현실 세계에서는 그라운드 트루스 보상 함수가 알려져 있지 않기 때문에, 우리는 GPT-4를 요약 및 싱글 턴 대화 설정에서 요약 품질과 응답 유용성에 대한 인간 평가의 프록시로 사용하여 기준 정책에 대한 _승률_로 알고리즘을 평가한다. 요약하기 위해 테스트 세트의 참조 요약을 기준선으로 사용하고 대화의 경우 선호하는 응답을 사용합니다.

그림 2: **왼쪽.** 참조 정책에 대한 예상 보상 대 KL의 프런티어입니다. DPO는 모든 KL 값에 대해 가장 높은 예상 보상을 제공하여 최적화의 품질을 보여준다. **맞아.** TL;DR 요약 승률 대. GPT-4를 평가자로 사용하여 사람이 작성한 요약입니다. DPO는 요약에 대한 PPO의 베스트 케이스 성능을 초과하지만 샘플링 온도 변화에 더 강력합니다.

테스트 데이터 세트를 기준선으로 설정합니다. 기존 연구는 LMs가 기존 메트릭보다 더 나은 자동화된 평가자가 될 수 있음을 시사하지만 [10] Sec. 6.4에서 평가를 위한 GPT-4의 사용을 정당화하기 위해 인간 연구를 수행한다. GPT-4 판단은 인간과 강한 상관관계가 있으며, 일반적으로 인간 간 주석자 일치보다 유사하거나 더 높다.

**방법.** DPO 외에도 인간 선호도를 준수하기 위해 언어 모델을 훈련하는 여러 기존 접근 방식을 평가합니다. 요약 작업에서 **GPT-J**[43]으로 0샷 프롬프트를 탐색하고 대화 작업에서 **Pythia-2.8B**[3]으로 2샷 프롬프트를 탐색합니다. 또한 SFT 모델(통제된 감정 및 요약) 또는 일반 LM(단일 회전 대화)에서 선택한 완료 \(y_{w}\)에 대해 지도 학습으로 미세 조정된 모델인 **SFT** 모델과 **선호된 FT** 모델을 평가합니다. 또 다른 의사 지도 방법은 \(y_{w}\)에 할당된 확률을 최대화하고 \(y_{l}\)에 할당된 확률을 _최소화_하도록 정책을 최적화하는 **우도**[44]입니다. '우도' 항에서 선택적 계수 \(\alpha\in[0,1]\)를 사용합니다. 또한 선호도 데이터에서 학습된 보상 함수와 제어된 감정 설정에서 사용할 수 있는 그라운드 트루스 보상 함수에서 학습하는 오라클인 **PPO-GT** 를 사용하여 **PPO**[37]을 고려한다. 감성 실험에서 우리는 PPO-GT의 두 가지 구현, 즉 선반 버전[42] 중 하나와 보상을 정규화하고 성능을 개선하기 위해 하이퍼파라미터를 추가로 조정하는 수정된 버전을 사용한다(학습된 보상으로 '정상' PPO를 실행할 때도 이러한 수정을 사용한다). 마지막으로 **Best of \(N\)** 기준선, SFT 모델(또는 대화에서 선호 FT)에서 \(N\) 응답을 샘플링하고 선호도 데이터 세트에서 학습된 보상 함수에 따라 가장 높은 점수를 받는 응답을 반환합니다. 이 고성능 방법은 보상 모델의 품질을 PPO 최적화와 분리하지만 테스트 시간에 모든 쿼리에 대해 샘플링 \(N\) 완료가 필요하기 때문에 중간 \(N\)에 대해서도 계산적으로 비실용적이다.

### DPO가 RLHF 목표를 얼마나 잘 최적화할 수 있습니까?

일반적인 RLHF 알고리즘에서 사용되는 KL 제약 보상 최대화 목표는 보상이 참조 정책에서 멀리 벗어나는 것을 제한하면서 보상의 활용 균형을 맞춘다. 따라서 알고리즘을 비교할 때 달성된 보상과 KL 불일치를 모두 고려해야 하며, 약간 더 높은 보상을 달성하지만 훨씬 더 높은 KL을 달성하는 것이 반드시 바람직한 것은 아니다. 그림 2는 감정 설정에서 다양한 알고리즘에 대한 보상-KL 프론티어를 보여준다. 각 알고리즘에서 정책 보수성을 위해 서로 다른 하이퍼파라미터(PPO의 경우 목표 \(\text{KL}\in\{3,6,9,12\}\), 선호-FT의 경우 비우도 \(\beta\in\{0.05,0.1,1,5\}\), \(\alpha\in\{0.05,0.1,0.5,1\}\)를 사용하여 여러 개의 훈련 실행을 실행한다. 이 스윕에는 총 22개의 런이 포함됩니다. 수렴될 때까지 각 100단계의 훈련 과정을 거친 후, 테스트 프롬프트 집합에 대해 각 정책을 평가하고, 참조 정책 \(\text{KL}\left(\pi\mid\pi_{\text{ref}}\right)\)을 사용하여 평균 시퀀스 수준 KL3와 함께 참 보상 함수 아래의 평균 보상을 계산한다. 우리는 DPO가 가장 효율적인 프론티어를 생산하여 가장 높은 보상을 달성하면서도 낮은 KL을 달성한다는 것을 발견했다. 이 결과는 여러 가지 이유로 특히 주목할 만하다. 첫째, DPO와 PPO는 동일한 목적을 최적화하지만, DPO는 현저하게 더 효율적이다;

그림 3: **왼쪽.** Anthropic-HH 1단계 대화에 대해 GPT-4에서 계산된 승률; DPO는 Anthropic-HH 테스트 세트에서 선택한 요약보다 개선되는 유일한 방법입니다. **맞아요.** 교육 과정에서 다양한 샘플링 온도에 대한 승리율입니다. 데이터 세트 라벨에 대한 DPO의 개선은 다양한 샘플링 온도에 대한 훈련 과정에서 상당히 안정적이다.

DPO의 보상/KL 트레이드오프는 PPO를 엄격하게 지배한다. 둘째, DPO는 PPO보다 더 나은 프론티어를 달성하며, PPO가 그라운드 트루스 보상(PPO-GT)에 액세스할 수 있는 경우에도 마찬가지입니다.

### DPO를 실제 환경 설정 데이터 세트로 확장할 수 있습니까?

다음으로, 요약과 싱글 턴 대화에서 DPO의 미세 조정 성능을 평가한다. 요약하기 위해 ROUGE와 같은 자동 평가 메트릭은 인간 선호도와 잘 상관되지 않을 수 있으며[38], 이전 연구에서는 보다 효과적인 요약을 제공하기 위해 인간 선호도에 PPO를 사용하여 LM을 미세 조정한다는 것을 발견했다. TL;DR 요약 데이터 세트의 테스트 분할에 대한 완성도를 샘플링하고 테스트 세트의 참조 완성도에 대한 평균 승률을 계산하여 다양한 방법을 평가한다. 모든 방법에 대한 완료는 0.0에서 1.0까지 다양한 온도에서 샘플링되며 승률은 그림 2(오른쪽)에 나와 있다. DPO, PPO 및 Preferred-FT는 모두 동일한 GPT-J SFT 모델4를 미세 조정한다. DPO는 0.0의 온도에서 약 61%의 승률을 가지며, 최적 샘플링 온도인 0.0에서 57%의 PPO 성능을 초과한다. 또한 DPO는 \(N\) 기준선의 최고 승률에 비해 더 높은 최대 승률을 달성한다. 우리는 DPO의 \(\beta\) 하이퍼파라미터를 의미 있게 조정하지 않았기 때문에 이러한 결과는 DPO의 잠재력을 과소평가할 수 있다. 또한, DPO가 PPO보다 샘플링 온도에 훨씬 더 강하다는 것을 발견했으며, 그 성능은 고온에서 기본 GPT-J 모델의 성능으로 떨어질 수 있다. 선호FT는 SFT 모델보다 크게 개선되지 않습니다. 또한 섹션 6.4의 인간 평가에서 DPO와 PPO 정면을 비교하며, 여기서 온도 0.25의 DPO 샘플은 온도 0의 PPO 샘플보다 58% 더 선호되었다.

각주 4: [https://huggingface.co/CarperAI/openai_summarize_tldr_sft](https://huggingface.co/CarperAI/openai_summarize_tldr_sft)

단일 회전 대화에서 우리는 인간 보조 상호 작용의 한 단계를 사용하여 인류 HH 데이터 세트 [1]의 테스트 분할 하위 집합에 대한 다양한 방법을 평가한다. GPT-4 평가는 다양한 방법에 대한 승률을 계산하기 위한 참조로 테스트에서 선호되는 완료를 사용한다. 이 작업에 대한 표준 SFT 모델이 없기 때문에 사전 훈련된 피티아-2.8B에서 시작하여 Preferred-FT를 사용하여 선택된 완성들에 대한 참조 모델을 완성들이 모델의 분포 내에 있도록 훈련시킨 다음 DPO를 사용하여 훈련시킨다. 또한 128개의 선호 FT 완성 중 베스트(이 작업에 대한 128개의 완성에서 베스트 오브 \(N\) 기준 고원을 찾았고, 부록 그림 4 참조)와 피티아-2.8B 기본 모델의 2샷 프롬프트 버전과 비교하여 각 방법에 대한 최상의 성능 온도에 대해 DPO가 우수하거나 더 우수하다는 것을 발견했다. 또한 잘 알려진 소스 6의 인류 HH 데이터 세트 5에서 PPO로 훈련된 RLHF 모델을 평가하지만 기본 피티아-2.8B 모델보다 더 나은 성능을 제공하는 프롬프트 또는 샘플링 온도를 찾을 수 없다. TL;DR의 결과와 두 방법이 동일한 보상 함수를 최적화한다는 사실에 기초하여 PPO 수준의 성능을 위해 Best of 128을 대략적인 프록시로 간주한다. 전반적으로 DPO는 인류 HH 데이터 세트에서 선호되는 완성도를 개선하는 유일한 계산 효율적인 방법이며 계산적으로 까다로운 128개의 기준선 중 Best와 유사하거나 더 나은 성능을 제공한다. 마지막으로 그림 3은 DPO가 비교적 빠르게 최상의 성능으로 수렴함을 보여준다.

각주 5: [https://huggingface.co/reciprocate/ppo_hh_pythia-6B](https://huggingface.co/reciprocate/ppo_hh_pythia-6B)

각주 6: [https://github.com/CarperAI/trlx/tree/main/examples/hh](https://github.com/CarperAI/trlx/tree/main/examples/hh)

### 새 입력 배포로의 일반화

분포 이동 하에서 PPO 및 DPO의 성능을 추가로 비교하기 위해, TL;DR(0 및 0.25)의 최상의 샘플링 온도를 사용하여 CNN/DailyMail 데이터 세트 [24]의 테스트 분할에서 다른 분포, 뉴스 기사에 대한 Reddit TL;DR 요약 실험의 PPO 및 DPO 정책을 평가한다. 결과는 표 1에 나와 있다. 우리는 Reddit TL;DR에 사용한 것과 동일한 GPT-4(C) 프롬프트를 사용하여 데이터 세트의 그라운드-진실 요약에 대한 GPT-4 승률을 계산했지만 "포럼 포스트"라는 단어를 "뉴스 기사"로 대체했다. 이 새로운 배포의 경우 DPO는 PPO 정책을 상당한 차이로 계속 능가합니다. 이 실험은 DPO가 레이블이 지정되지 않은 추가 Reddit TL;DR 프롬프트를 사용하지 않더라도 DPO 정책이 PPO 정책과 유사하게 일반화할 수 있다는 초기 증거를 제공한다.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & \multicolumn{2}{c}{**Win rate vs. ground truth**} \\ \cline{2-3}
**Alg.** & Temp \(0\) & Temp \(0.25\) \\ \hline DPO & 0.36 & 0.31 \\ PPO & 0.26 & 0.23 \\ \hline \hline \end{tabular}
\end{table}
표 1: GPT-4 승률 vs. 배포 외 CNN/DailyMail 입력 기사에 대한 사실 요약입니다.

### 인간 판단으로 GPT-4 판단 유효성 검사

우리는 TL:DR 요약 실험 결과와 두 개의 다른 GPT-4 프롬프트를 사용하여 GPT-4의 판단의 신뢰성을 검증하기 위해 인간 연구를 수행한다. **GPT-4 (S)** (간단한) 프롬프트는 게시물에서 중요 한 정보를 더 잘 요약 하는 요약을 요청 합니다. **GPT-4 (C)** (concise) 프롬프트는 또한 어떤 요약이 더 간결한지 묻습니다. GPT-4가 **GPT-4 (S)** 프롬프트에서 인간보다 더 길고 반복적인 요약을 선호한다는 것을 발견하기 때문에 이 프롬프트를 평가합니다. 전체 프롬프트는 부록 C.2를 참조하십시오. 우리는 최고(DPO, temp)를 사용하여 세 가지 비교를 수행한다. SFT(middle-performing), temp. 0.25(middle-performing) 방법은 다양한 샘플 품질을 포괄하는 것을 목표로 하며, 세 가지 방법 모두 탐욕적으로 샘플링된 PPO(최상의 성능 온도)와 비교된다. 우리는 두 프롬프트 모두에서 GPT-4가 인간이 서로 동의하는 만큼 종종 인간과 동의하는 경향이 있음을 발견하여 GPT-4가 인간 평가에 대한 합리적인 대리인임을 시사한다(제한된 인간 평가자로 인해 DPO 및 PPO-1 비교를 위해 여러 인간 판단만 수집한다). 전반적으로 **GPT-4 (C)** 프롬프트는 일반적으로 사람을 더 대표하는 승률을 제공하므로 섹션 6.2의 주요 결과에 이 프롬프트를 사용합니다. 평가자에게 제공되는 웹 인터페이스 및 인간 지원자 목록을 포함하여 인간 연구에 대한 추가 세부 정보는 부록 D.3을 참조하세요.

## 7 Discussion

선호도로부터 배우는 것은 훈련 가능한 정렬된 언어 모델을 위한 강력하고 확장 가능한 프레임워크이다. 우리는 강화 학습 없이 선호도로부터 언어 모델을 훈련하기 위한 간단한 훈련 패러다임인 DPO를 도입했다. DPO는 기성 RL 알고리즘을 사용하기 위해 선호도 학습 문제를 표준 RL 설정으로 강제하기보다는 강화 학습이나 일반성의 손실 없이 간단한 교차 엔트로피 손실로 인간의 선호도를 _직접_ 충족하도록 언어 모델을 훈련할 수 있는 언어 모델 정책과 보상 함수 간의 매핑을 식별한다. 하이퍼파라미터의 튜닝이 사실상 없는 상태에서, DPO는 PPO에 기초한 알고리즘들을 포함하는 기존의 RLHF 알고리즘들보다 유사하게 또는 더 잘 수행하는데, 따라서 DPO는 인간의 선호도들로부터 더 많은 언어 모델들을 트레이닝하는 것에 대한 장벽을 의미있게 감소시킨다.

**제한 사항 및 미래 작업** 결과는 향후 작업에 대한 몇 가지 중요한 질문을 제기합니다. DPO 정책은 명시적 보상 함수로부터의 학습과 비교하여 어떻게 분배에서 일반화되는가? 우리의 초기 결과는 DPO 정책이 PPO 기반 모델과 유사하게 일반화될 수 있음을 시사하지만 보다 포괄적인 연구가 필요하다. 예를 들어 DPO 정책에서 자체 라벨링을 사용한 훈련도 마찬가지로 라벨이 지정되지 않은 프롬프트를 효과적으로 사용할 수 있습니까? 또 다른 측면에서, 보상 과잉 최적화는 직접 선호 최적화 설정에서 어떻게 나타나며, 그림 3-우측의 약간의 성능 감소는 그것의 사례인가? 또한 6B 매개변수까지 모델을 평가하지만 DPO를 최대 10배 더 큰 최신 모델로 확장하는 탐색은 향후 작업을 위한 흥미로운 방향이다. 평가와 관련하여 GPT-4에 의해 계산된 승률이 신속한 영향을 받는다는 것을 발견했으며 향후 작업은 자동화된 시스템에서 고품질 판단을 이끌어내는 가장 좋은 방법을 연구할 수 있다. 마지막으로, DPO의 많은 가능한 응용은 다른 양식에서 생성 모델을 훈련하는 것을 포함하여 인간의 선호도에서 언어 모델을 훈련하는 것 이상으로 존재한다.

## Acknowledgements

EM은 감사하게도 Knight-Hennessy 대학원 펠로우십의 자금을 인정한다 CF와 CM은 CIFAR 펠로우입니다. 이 작업은 부분적으로 Stanford Accelerator for Learning(SAL) 및 Stanford Institute for Human-Centered Artificial Intelligence(HAI) _Generative AI for the Future of Learning_ seed grant 프로그램에 의해 지원되었다. Stanford Center for Research on Foundation Models (CRFM)은 이 연구에서 실험에 사용된 컴퓨팅 리소스의 일부를 제공했다. 이 작업은 ONR 보조금 N00014-20-1-2675에 의해 부분적으로 지원되었다.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & **DPO** & **SFT** & **PPO-1** \\ \cline{2-4} N respondents & 272 & 122 & 199 \\ \hline GPT-4 (S) win \% & 47 & 27 & 13 \\ GPT-4 (C) win \% & 54 & 32 & 12 \\ Human win \% & 58 & 43 & 17 \\ \hline GPT-4 (S)-H agree & 70 & 77 & 86 \\ GPT-4 (C)-H agree & 67 & 79 & 85 \\ H-H agree & 65 & - & 87 \\ \hline \hline \end{tabular}
\end{table}
표 2: TL;DR 요약 샘플에 대한 인간 및 GPT-4 승률 및 판단당 일치 비교. **인간은 서로 동의하는 만큼 GPT-4에 동의합니다.* * 각 실험은 명시된 방법의 요약과 온도 0인 PPO의 요약을 비교합니다.

## References

* Bai et al. [2022] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.
* Bai et al. [2022] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022.
* Biderman et al. [2023] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023.
* Bong and Rinaldo [2022] H. Bong and A. Rinaldo. 브래들리-테리-루스 모델에서 MLE의 존재 및 일관성에 대한 일반화된 결과 _ International Conference on Machine Learning_, 2022. arXiv:2110.11487.
* Bradley and Terry [1952] R. A. Bradley and M. E. Terry. 불완전한 블록 설계의 순위 분석: I. 쌍 비교 방법 _ Biometrika_, 39(3/4):324-345, 1952. doi: [https://doi.org/10.2307/2334029](https://doi.org/10.2307/2334029).
* Brown et al. [2020] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
* Brown et al. [2020] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Bubeck et al. [2023] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4, 2023. arXiv preprint arXiv:2303.12712.
* Busa-Fekete et al. [2014] R. Busa-Fekete, B. Szorenyi, P. Weng, W. Cheng, and E. Hullermeier. Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm. _Machine Learning_, 97(3):327-351, July 2014. doi: 10.1007/s10994-014-5458-8. URL [https://doi.org/10.1007/s10994-014-5458-8](https://doi.org/10.1007/s10994-014-5458-8).
* Chen et al. [2023] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study. _ArXiv_, abs/2304.00723, 2023.
* Chowdhery et al. [2022] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* Christiano et al. [2017] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL [https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf).

* [13] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros, M. Pellat, K. Robinson, D. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei. Scaling instruction-finetuned language models, 2022.
* [14] M. Dudik, K. Hofmann, R. E. Schapire, A. Slivkins, and M. Zoghi. Contextual dueling bandits. In P. Grunwald, E. Hazan, and S. Kale, editors, _Proceedings of The 28th Conference on Learning Theory_, volume 40 of _Proceedings of Machine Learning Research_, pages 563-587, Paris, France, 03-06 Jul 2015. PMLR. URL [https://proceedings.mlr.press/v40/Dudik15.html](https://proceedings.mlr.press/v40/Dudik15.html).
* [15] D. Go, T. Korbak, G. Kruszewski, J. Rozen, N. Ryu, and M. Dymetman. Aligning language models with preferences through f-divergence minimization. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23. JMLR.org, 2023.
* [16] A. Jain, B. Wojcik, T. Joachims, and A. Saxena. Learning trajectory preferences for manipulators via iterative improvement. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013. URL [https://proceedings.neurips.cc/paper_files/paper/2013/file/c058f544c737782deacefa532d9add4c-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2013/file/c058f544c737782deacefa532d9add4c-Paper.pdf).
* [17] T. Korbak, H. Elsahar, G. Kruszewski, and M. Dymetman. On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belggrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 16203-16220. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/67496dfa96afddab795530cc7c69b57a-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/67496dfa96afddab795530cc7c69b57a-Paper-Conference.pdf).
* [18] J. Kreutzer, J. Uyheng, and S. Riezler. Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1777-1788, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1165. URL [https://aclanthology.org/P18-1165](https://aclanthology.org/P18-1165).
* [19] A. Kupcsik, D. Hsu, and W. S. Lee. _Learning Dynamic Robot-to-Human Object Handover from Human Feedback_, pages 161-176. Springer International Publishing, 01 2018. ISBN 978-3-319-51531-1. doi: 10.1007/978-3-319-51532-8_10.
* [20] S. Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review, 2018.
* [21] R. D. Luce. Individual choice behavior: A theoretical analysis. _Courier Corporation_, 2012.
* [22] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, pages 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL [http://www.aclweb.org/anthology/P11-1015](http://www.aclweb.org/anthology/P11-1015).
* [23] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3470-3487, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long. 244. URL [https://aclanthology.org/2022.acl-long.244](https://aclanthology.org/2022.acl-long.244).
* [24] R. Nallapati, B. Zhou, C. dos Santos, C. Gulcehre, and B. Xiang. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In _Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning_, pages 280-290, Berlin, Germany, Aug. 2016. Association for Computational Linguistics. doi: 10.18653/v1/K16-1028. URL [https://aclanthology.org/K16-1028](https://aclanthology.org/K16-1028).

* Narayanan et al. [2021] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, SC '21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384421. doi: 10.1145/3458817.3476209. URL [https://doi.org/10.1145/3458817.3476209](https://doi.org/10.1145/3458817.3476209).
* Ouyang et al. [2022] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 27730-27744. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/blefde53be364a73914f58805oa001731-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/blefde53be364a73914f58805oa001731-Paper-Conference.pdf).
* Paulus et al. [2018] R. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization. In _International Conference on Learning Representations_, 2018. URL [https://openreview.net/forum?id=HkAClQgA-](https://openreview.net/forum?id=HkAClQgA-).
* Peng et al. [2019] X. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_, 2019.
* Peters and Schaal [2007] J. Peters and S. 샬 운영 공간 제어를 위한 보상 가중치 회귀에 의한 강화 학습. 2007년 745-750 페이지, _Proceedings of the 24th International Conference on Machine Learning_에서.
* Plackett[1975] R. L. Plackett. 순열 분석입니다. _ 왕립 통계학회지. Series C (Applied Statistics)_, 24(2):193-202, 1975. doi: [https://doi.org/10.2307/2346567](https://doi.org/10.2307/2346567).
* Radford et al. [2019] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners, 2019. Ms., OpenAI.
* Ramamurthy et al. [2023] R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi, and Y. Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=BaHzds2uUyB](https://openreview.net/forum?id=BaHzds2uUyB).
* Ranzato et al. [2015] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural networks. _CoRR_, abs/1511.06732, 2015.
* Sadigh et al. [2017] D. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia. Active preference-based learning of reward functions. In _Robotics: Science and Systems (RSS)_, 2017.
* Saha et al. [2022] A. Saha, A. Pacchiano, and J. Lee. Dueling rl: Reinforcement learning with trajectory preferences. In F. Ruiz, J. Dy, and J.-W. van de Meent, editors, _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 6263-6289. PMLR, 25-27 Apr 2023. URL [https://proceedings.mlr.press/v206/saha23a.html](https://proceedings.mlr.press/v206/saha23a.html).
* Sanh et al. [2022] V. Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani, N. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush. Multitask prompted training enables zero-shot task generalization. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=9Vrb9DOWI4](https://openreview.net/forum?id=9Vrb9DOWI4).
* Schulman et al. [2017] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms, 2017.
* Stiennon et al. [2022] N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. Christiano. Learning to summarize from human feedback, 2022.

* Thoppilan et al. [2022] R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin, J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, V. Zhao, Y. Zhou, C.-C. Chang, I. Krivokou, W. Rusch, M. Pickett, P. Srinivasan, L. Man, K. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zevenbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. Aguerar-Arcas, C. Cui, M. Croak, E. Chi, and Q. Le. Lamda: Language models for dialog applications, 2022.
* Touvron et al. [2023] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* Volske et al. [2017] M. Volske, M. Potthast, S. Syed, and B. Stein. TL;DR: Mining Reddit to learn automatic summarization. In _Proceedings of the Workshop on New Frontiers in Summarization_, pages 59-63, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4508. URL [https://aclanthology.org/W17-4508](https://aclanthology.org/W17-4508).
* von Werra et al. [2023] L. von Werra, J. Tow, reciprocated, S. Matiana, A. Havrilla, cat state, L. Castricato, Alan, D. V. Phung, A. Thakur, A. Bukhtiyarov, aanromm, F. Milo, Daniel, D. King, D. Shin, E. Kim, J. Wei, M. Romero, N. Pochinkov, O. Sanseviero, R. Adithyan, S. Siu, T. Simonini, V. Blagojevic, X. Song, Z. Witten, alexandremuzio, and crumb. CarperAI/trlx: v0.6.0: LLaMa (Alpaca), Benchmark Util, T5 ILQL, Tests, Mar. 2023. URL [https://doi.org/10.5281/zenodo.7790115](https://doi.org/10.5281/zenodo.7790115).
* Wang and Komatsuzaki [2021] B. Wang and A. Komatsuzaki. GPT-J-6B: 60억 매개 변수 자동 회귀 언어 모델 [https://github.com/kingoflolz/mesh-transformer-jax] (https://github.com/kingoflolz/mesh-transformer-jax), 2021년 5월.
* Welleck et al. [2019] S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural text generation with unlikelihood training. _arXiv preprint arXiv:1908.04319_, 2019.
* Williams [1992] R. J. Williams. 연결주의 강화 학습을 위한 간단한 통계적 기울기 추종 알고리즘입니다. _ 마흐 배워요_ 1992년 5월, ISSN 0885-6125. doi: 10.1007/BF00992696. URL [https://doi.org/10.1007/BF00992696](https://doi.org/10.1007/BF00992696).
* Wu and Hu[2018] Y. 우승훈 심층 강화 학습을 통해 일관성 있는 요약을 추출하는 학습. "Proceedings of the 30-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence_, AAAI'18/IAAI'18/EAAI'18. AAAI Press, 2018. ISBN 978-1-57735-800-8.
* Yan et al. [2022] X. Yan, C. Luo, C. L. A. Clarke, N. Craswell, E. M. Voorhees, and P. Castells. Human preferences as dueling bandits. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR '22, page 567-577, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450387323. doi: 10.1145/3477495.3531991. URL [https://doi.org/10.1145/3477495.3531991](https://doi.org/10.1145/3477495.3531991).
* Yue et al. [2012] Y. Yue, J. Broder, R. Kleinberg, and T. Joachims. The k-armed dueling bandits problem. _Journal of Computer and System Sciences_, 78(5):1538-1556, 2012. ISSN 0022-0000. doi: [https://doi.org/10.1016/j.jcss.2011.12.028](https://doi.org/10.1016/j.jcss.2011.12.028). URL [https://www.sciencedirect.com/science/article/pii/S002200012000281](https://www.sciencedirect.com/science/article/pii/S002200012000281). JCSS Special Issue: Cloud Computing 2011.
* Ziegler et al. [2020] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences, 2020.

## Author Contributions

**모든 저자** 는 실험을 설계, 분석 및 반복하고 논문을 작성 및 편집하고 일반적으로 프로젝트의 진행 상황을 관리하는 데 귀중한 기여를 제공했습니다.

**EM**과의 논의에서 자기회귀 보상 모델을 사용하여 제안된 **RR**, DPO 목표를 도출, 알고리즘의 이론적 속성을 증명하고 관련 섹션 및 부록을 작성했다. 그는 또한 실험을 조직하는 것을 제안하고 도왔고 PPO와 보상 학습 기준선의 일부를 기여했다.

**AS** 는 PPO의 대안으로 가중 회귀 방법을 사용하는 것에 대한 논의를 시작했습니다. 프로젝트 관련 조직은 DPO를 가중 회귀 및 우도와 연결하는 초기 분석을 작성했습니다. DPO + 기본 구현의 설계 및 반복, DPO에 대한 초기 탐색 실험, 실질적인 실험 조직 및 설계(데이터 세트, 기준선, 평가); 통제된 감정 생성 및 요약을 위한 모델 훈련 및 평가, GPT-4 평가를 위한 설계 반복(특히 요약), 추상, 사전/방법 및 실험에 대한 실질적인 작성 기여, 다른 섹션에 대한 편집 기여입니다.

**EM** 은 자기회귀 보상 함수 학습에 대한 초기 토론에 대한 입력을 제공했으며, DPO의 첫 번째 구현을 작성하고 첫 번째 DPO 실험을 실행했으며, 종이 실험에 사용된 대규모(요약 및 대화) DPO 모델을 훈련했으며, 초기 GPT-4 승률 평가를 수행하고 관련 인프라를 설정했으며, 인간 연구의 결과를 위해 참가자를 모집하고, 수행 및 분석했으며, 초록, 소개, 관련 작업, 토론 및 대부분의 실험을 작성했으며, 나머지 논문을 편집하는 데 도움을 주었다.

**CF, CM 및 SE** 는 연구를 감독하고 아이디어와 실험을 제안했으며 논문 작성에 도움을 주었다.

## 부록 A 수리 미분

### KL 제한 보상 최대화 목표의 최적 도출

이 부록에서 우리는 식 4를 도출할 것이다. 식과 유사하게. 도 3을 참조하면, 다음과 같은 목적을 최적화한다:

\[\max_{\pi}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi}\big{[}r(x,y)\big{]}-\beta \mathbb{D}_{\text{KL}}\big{[}\pi(y|x)||\pi_{\text{ref}}(y|x)\big{]} \tag{11}\]

임의의 보상 함수 \(r(x,y)\), 참조 모델 \(\pi_{\text{ref}}}\) 및 일반적인 비모수 정책 클래스 아래에 있습니다. 이제 우리는...

[}\pi(y|x) \mid\mid\pi_{\text{ref}}(y|x) \big{[}\max_{\pi}\mathbb{E}_{x\sim\pi(y|x) \big{]}-\beta\mathbb{D}_{\text{ref}}(y|x) \big{[=\max_{\pi}\mathbb{E}_{x\sim\mathcal{ref}}(y|x) \big{[=\log\frac{\pi(y|x)}-\frac{\text{ref}}(y|x) \big{[=\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}(y|x)\mathbb{E}_{y\sim\pi(y|x)}}\left[\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)\right]\] \[=\min_{\pi}\mathbb{E}_{x\sim\

여기서, 파티션 함수는:

\[Z(x)=\sum_{y}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right).\]

분할 함수는 \(x\) 및 참조 정책 \(\pi_{\text{ref}}}\의 함수이지만 정책 \(\pi\)에 의존하지 않습니다. 이제 정의를 내릴 수 있어

\[\pi^{*}(y|x)=\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right),\]는 모든 \(y\) 및 \(\sum_{y}\pi^{*}(y|x)=1\)에 대해 \(\pi^{*}(y|x)\geq 0\로 유효한 확률 분포이다. \(Z(x)\)는 \(y\)의 함수가 아니므로 다음 식 12에서 최종 목표를 다음과 같이 재구성할 수 있다.

\log\frac{\pi(y|x)}{\pi^{*}(y|x)}\right]-\log Z(x)\right]= \tag{13}\] \[\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{D}_{\text{ KL}}(\pi(y|x)\mid\mid\pi^{*}(y|x))-\log Z(x)\right] \tag{14}\

이제 \(Z(x)\)는 \(pi\)에 의존하지 않으므로 첫 번째 KL 항을 최소화하는 정책에 의해 최소가 달성된다. 깁스의 부등식은 두 분포가 동일한 경우에만 KL-발산이 0에서 최소화된다는 것을 말해준다. 따라서 우리는 최적의 솔루션을 가지고 있다:

\[\pi(y|x)=\pi^{*}(y|x)=\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{ \beta}r(x,y)\right) \tag{15}\]

for all \(x\in\mathcal{D}\). 이로써 도출이 완료된다.

### Bradley-Terry 모형에서 DPO 목표 도출

브래들리-테리 선호 모델에서 DPO 목표를 도출하는 것은 간단합니다.

\[p^{*}(y_{1}\succ y_{2}|x)=\frac{\exp\left(r^{*}(x,y_{1})\right)}{\exp\left(r ^{*}(x,y_{1})\right)+\exp\left(r^{*}(x,y_{2})\right)} \tag{16}\]

섹션 4에서 우리는 상응하는 최적 정책을 통해 (이용할 수 없는) 지상 진실 보상을 표현할 수 있음을 보여주었다:

\[r^{*}(x,y)=\beta\log\frac{\pi^{*}(y|x)}{\pi_{\text{ref}}(y|x)}+\beta\log Z(x) \tag{17}\]

식을 대입한다. 17은 Eq. 16: 입수한다:

\[p^{*}(y_{1}\succ y_{2}|x) =\frac{\1}{1+\exp\left(\beta\log\frac{\pi^{*}(y_{1}|x)+\beta\log Z(x)\right)}{\exp\left(\beta\log\frac{\pi^{*}(y_{2}|x)+\beta\log Z(x)\right)}\[=\frac{\1}{1+\exp\left(\beta\log\frac{\pi^{*}(y_{2}|x)}{\pi_{\text{ref}}(y_{2}|x)+\beta\log Z(x)\right)}\[=\sigma\left(\beta\log\frac{\pi^{*}(y_{1}|x)}{\pi_{\text{ref}}(y_{1}|x)}\right) \[=\sigma\left(\beta\log\frac{\pi^{*}(y_{1}|x)}{\pi_{\

마지막 행은 식 7의 인스턴스당 손실이다.

### Plackett-Luce 모델에서 DPO 목표 도출

Plackett-Luce 모델[30;21]은 단순히 쌍별 비교가 아닌 순위보다 브래들리-테리 모델의 일반화이다. 브래들리-테리 모델과 유사하게, 그것은 가능한 선택들의 집합이 제시될 때, 사람들은 그 선택에 대한 일부 잠재 보상 함수의 값에 비례하는 확률을 갖는 선택을 선호한다고 규정한다. 우리의 맥락에서, 프롬프트 \(x\)와 \(K\) 답변 집합 \(y_{1},\ldots,y_{K}\)을 제시하면 사용자는 순열 \(\tau:[K]\rightarrow[K]\)을 출력하여 답변의 순위를 부여하게 된다. Plackett-Luce 모델은 다음과 같이 규정하고 있다.

\[p^{*}(\tau|y_{1},\ldots,y_{K},x)=\prod_{k=1}^{K}\frac{\exp(r^{*}(x,y_{\tau(k) }))}{\sum_{j=k}^{K}\exp(r^{*}(x,y_{\tau(j)}))} \tag{18}\

\(K=2\)일 때 식 18은 브래들리-테리 모형으로 감소한다는 점에 주목하라. 그러나 일반적인 Plackett-Luce 모델의 경우 Eq. 5 및 그것의 최적 정책에 의해 파라미터화된 보상 함수를 대체한다. 부록 A.2와 마찬가지로 정규화 상수 \(Z(x)\)가 취소되고 다음과 같습니다.

\[p^{*}(\tau|y_{1},\ldots,y_{K},x)=\prod_{k=1}^{K}\frac{\exp\left(\beta\log\frac{ \pi^{*}(y_{\tau(k)}|x)}{\pi_{\text{ref}}(y_{\tau(k)}|x)}\right)}{\sum_{j=k}^{K}\exp\left(\beta\log\frac{\pi^{*}(y_{\tau(j)}|x)}{\pi_{\text{ref}}(y_{\tau(j)}|x)}\right)} \tag{19}\]

[MISSING_PAGE_FAIL:17]

**Lemma 2** **Restated**.: _동일한 동등성 클래스의 두 보상 함수는 제약된 RL 문제에서 동일한 최적 정책을 유도합니다._

증명: \(r^{\prime}(x,y)=r(x,y)+f(x)\)와 같은 동일한 클래스의 두 보상 함수를 고려하고, 해당 최적 정책을 \(\pi_{r}\)와 \(\pi_{r^{\prime}}\)로 나타내자. 식. 4, for all \(x,y\) we have

\frac{1}{\sum_{y}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta} r^{\prime}(x,y)+f(x)\right) \] \[=\frac{1}{\sum_{y}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta} r^{\prime}(x,y)\right) \[=\frac{1}{\sum_{y}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta} r^{\prime}(x,y)\right)\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta} r^{\prime}(x,y)\right)\] \[=\frac{1}{\sum_{y}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta} r^{\prime}(x,y)\

그것은 증명을 완성한다.

### 정리 1의 증명

이 절에서는 정리 1의 결과를 확장하기로 한다.

**정리 1** **복원됨**: _모든 프롬프트 쌍 \(x\)에 대해 \(\pi_{\text{ref}}(y|x)>0\) 및 응답 \(y\) 및 매개 변수 \(\beta>0\)와 같은 참조 모델이 있다고 가정합니다. 5절에서 정의된 모든 보상 동등성 클래스는 일부 모델 \(\pi(y|x)\)에 대한 재매개변수화 \(r(x,y)=\beta\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\)로 나타낼 수 있다.

증명: KL 제약 RL 문제에서 최적의 모델 \(\pi_{r}(y|x)\)을 유도하는 보상 함수 \(r(x,y)\를 고려하여 4. 다음 식에 따라 해를 구한다. 도 5를 참조하면, 양변을 로그 선형화할 때, 우리는 얻는다:

\[r(x,y)=\beta\log\frac{\pi_{r}(y|x)}{\pi_{\text{ref}}(y|x)}+\beta\log Z(x)\]

여기서 \(Z(x)=\sum_{y}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)\) (참고로 \(Z(x)\)도 보상 함수 \(r\)에 의존한다.) 연산자 \(r^{\prime}(x,y)=f(r,\pi_{\text{ref}},\beta)(x,y)=r(x,y)-\beta\log Z(x)\를 사용하여 이 새로운 보상 함수가 \(r\)의 등가 클래스 내에 있음을 확인하고 다음과 같이 한다:

\[r^{\prime}(x,y)=\beta\log\frac{\pi_{r}(y|x)}{\pi_{\text{ref}}(y|x)}\]

그것은 증명을 완성한다.

우리는 이러한 결과에 대해 더 확장할 수 있습니다. 우리는 만약 \(r\)와 \(r^{\prime}\)가 같은 클래스에서 두 개의 보상 함수라면,

\[f(r,\pi_{\text{ref}},\beta)(x,y)=\beta\log\frac{\pi_{r}(y|x)}{\pi_{\text{ref}}(y|x)}=\beta\log\frac{\pi_{r}^{\prime}(y|x)}{\pi_{\text{ref}}(y|x)}=f(r^{ \prime},\pi_{\text{ref}},\beta)(x,y)\]

여기서 두 번째 등식은 렘마 2로부터 뒤따른다. 우리는 연산자 \(f\)가 특정 동등성 클래스의 모든 보상 함수를 동일한 보상 함수에 매핑한다는 것을 증명했다. 다음으로, 우리는 보상 함수의 모든 동등성 클래스에 대해 정리 1에 요약된 재매개 변수를 갖는 보상 함수가 고유하다는 것을 보여준다.

**제안 1**.: _모든 프롬프트 쌍 \(x\)에 대해 \(\pi_{\text{ref}}(y|x)>0\) 및 응답 \(y\) 및 매개 변수 \(\beta>0\)와 같은 참조 모델이 있다고 가정합니다. 그런 다음 섹션 5에서 정의된 모든 등가 보상 함수 클래스는 고유한 보상 함수 \(r(x,y)\를 가지며, 이는 일부 모델에 대해 \(r(x,y)=\beta\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\)로 재매개 변수화될 수 있다 \(\pi(y|x)\)._ 증거: 우리는 모순에 의한 증명을 사용할 것이다. 동일한 클래스에서 두 개의 보상 함수( \(r^{\prime}(x,y)=r(x,y)+f(x)\)가 있다고 가정하자. 또한, 일부 모델의 경우 \(r^{\prime}(x,y)=\beta\log\frac{\pi^{\prime}(y|x)}{\pi_{\text{ref}}(y|x)}\)이고, 일부 모델의 경우 \(r(x,y)=\beta\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\)이고, 일부 모델의 경우 \(\pi\neq\pi^{\prime}\)라고 가정한다. 그럼

\[r^{\prime}(x,y)=r(x,y)+f(x)=\beta\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}+f( x)=\beta\log\frac{\pi(y|x)\exp(\frac{1}{\beta}f(x))}{\pi_{\text{ref}}(y|x)}= \beta\log\frac{\pi^{\prime}(y|x)}{\pi_{\text{ref}}(y|x)}}\]

for all prompts \(x\) and completions \(y\). 그리고 \(\pi(y|x)\exp(\frac{1}{\beta}f(x))=\pi^{\prime}(y|x)\를 가져야 한다. 이 분포들은 양쪽의 \(y\)을 합한 분포이기 때문에 우리는 \(\exp(\frac{1}{\beta}f(x))=1\)을 얻고 \(\beta>0\)이므로 모든 \(x\)에 대해 \(f(x)=0\)를 가져야 한다. 따라서 \(r(x,y)=r^{\prime}(x,y)\). 이로써 증명이 완성된다.

우리는 이제 모든 보상 클래스가 그 클래스의 모든 보상 함수에 대해 \(f(r,\pi_{\text{ref}},\beta)\)에 의해 주어지는 정리 1에 요약된 바와 같이 표현될 수 있는 고유한 보상 함수를 갖는다는 것을 보여주었다.

## 부록 B DPO 구현 세부 정보 및 하이퍼 매개 변수

DPO는 구현이 비교적 간단하며; DPO 손실에 대한 PyTorch 코드가 아래에 제공된다:import torch.nn.functional as F

def dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):  """  pi_logps: policy logprobs, shape (B,)  ref_logps: reference model logprobs, shape (B,)  yw_idxs: preferred completion indices in [0, B-1], shape (T,)  yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)  beta: 온도 조절 강도 KL 페널티

 (yw_idxs[i], yl_idxs[i])의 각 쌍은 단일 선호도 쌍의  인덱스들을 나타낸다.  ""

 pi_yw_logps, pi_yl_logps = pi_logps[yw_idxs], pi_logps[yl_idxs]  ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]

 pi_logratios = pi_yw_logps - pi_yl_logps  ref_logratios = ref_yw_logps - ref_yl_logps

 losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))  rewards = beta * (pi_logps - ref_logps).detach()

 반품 손실, 보상

달리 명시되지 않는 한, 우리는 기본적으로 1e-6의 학습률을 갖는 \(\beta=0.1\), 배치 크기 64 및 RMSprop 최적화기를 사용한다. 우리는 150단계에 걸쳐 학습률을 0에서 1e-6으로 선형 웜업한다. TL;DR 요약의 경우 \(\beta=0.5\)를 사용하지만 나머지 매개변수는 동일하게 유지됩니다.

## 부록 C 실험 설정에 대 한 추가 세부 정보

이 섹션에서는 실험 설계와 관련된 추가 세부 정보를 포함한다.

### IMDb 감성 실험 및 기준선 세부 정보

프롬프트는 길이 2-8 토큰의 IMDB 데이터 세트의 접두사이다. 사전 학습된 감성 분류기 siebert/sentiment-roberta-large-english를 ground-truth 보상 모델로 사용하고 gpt2-large를 base 모델로 사용한다. 낮은 품질의 텍스트와 보상이 다소 부정확하다는 것을 발견했기 때문에 이러한 더 큰 모델을 사용한다. 우리는 먼저 1 에폭 동안 IMDB 데이터의 하위 집합에 대해 감독 미세 조정을 사용한다. 그런 다음 이 모델을 사용하여 25000개의 접두사에 대해 4개의 완료를 샘플링하고 그라운드-진실 보상 모델을 사용하여 각 접두사에 대해 6개의 선호도 쌍을 생성한다. RLHF 보상 모델은 gpt2-large 모델로부터 초기화되고 선호도 데이터 세트에서 3개의 에폭에 대해 훈련되며, 검증 세트 정확도가 가장 높은 체크포인트를 취한다. "TRL" 실행은 TRL 라이브러리의 하이퍼-파라미터를 사용한다. 구현은 PPO 단계당 1024개의 더 큰 배치 샘플을 사용합니다.

### 요약 및 대화 성공률 계산을 위한 GPT-4 프롬프트

실험 설정의 핵심 구성 요소는 GPT-4 승률 판단이다. 이 섹션에서는 요약 및 대화 실험을 위한 승률을 생성하는 데 사용되는 프롬프트를 포함한다. 우리는 모든 실험에 gpt-4-0314를 사용한다. 요약 또는 반응의 순서는 모든 평가에 대해 무작위로 선택된다.

**요약 GPT-4 승률 프롬프트(S).**

다음 요약 중 어떤 것이 주어진 포럼 게시물에서 가장 중요한 점을 더 잘 요약하는가?

Post:<post>

Summary A:

<Summary A>

Summary B:

<Summary B>

FIRST는 두 요약의 한 문장 비교를 제공 하 여 어떤 \를 선호 하 고 왜를 설명 합니다. 둘째, 새 줄에 "A" 또는 "B"만 표시하여 \ 선택을 나타냅니다. 응답은 형식을 사용해야 합니다.

비교: <한 문장 비교 및 설명>

Preferred: <"A" 또는 "B">

**요약 GPT-4 승률 프롬프트(C).**

다음 요약 중 중요하지 않은 세부 정보 또는 관련 없는 세부 정보를 포함하지 않고 주어진 포럼 게시물에서 가장 중요한 사항을 요약하는 것이 더 나은 방법입니까? 좋은 요약은 정확하고 간결하다.

Post:

<post>

Summary A:

<Summary A>

FIRST는 두 요약의 한 문장 비교를 제공 하 여 어떤 \를 선호 하 고 왜를 설명 합니다. 둘째, 새 줄에 "A" 또는 "B"만 표시하여 \ 선택을 나타냅니다. 응답은 형식을 사용해야 합니다.

비교: <한 문장 비교 및 설명>

Preferred: <"A" 또는 "B">

**대화 GPT-4 승률 프롬프트**

챗봇에 대한 다음 쿼리의 경우 어떤 응답이 더 도움이 됩니까?

쿼리: <사용자 쿼리>

Response A:

<검사 방법 또는 기준선 중 어느 하나>

Response B:

<상기 다른 응답>

FIRST는 두 응답의 한 문장 비교를 제공하고 당신이 더 도움이 된다고 느끼는 \를 설명한다. 둘째, 새 줄에 "A" 또는 "B"만 표시하여 어떤 반응이 더 도움이 되는지 나타냅니다. 응답은 \ 형식을 사용해야 합니다.

비교: <한 문장 비교 및 설명>

More helpful: <"A" 또는 "B">

### Unlikelihood baseline

비우도 기준선 [44] (간단히 최대 \(\log p(y_{w}|x)\), 선호 응답의 로그 확률을 포함하는 반면, 비우도 응답의 로그 확률 \(\log p(y_{l}|x)\)을 최소화하면서 감성 실험에서 기준선으로 포함하지 않는다.

[MISSING_PAGE_FAIL:22]

## Appendix A

\begin{table}
\begin{tabular}{l p{341.4pt}} \hline \hline
**Prompt** & SUBREDDIT: r/AskReddit TITLE: I’ve been ungrateful to parents who have wanted only the best for me for the longest time. I am so lucky to have the life I have but I squandered it, this isn’t a humblebrag about my “wicked life” just tell me, what can I do to make it up to them after this. POST: I’m 17, leaving for University, and I’ve had epiphany after epiphany of how shitty i’ve been to them, or how all the nice things they do I ruin with moods etc., I know there isn’t a quick fix, but I want to do something nice. I’ve always thanked them and apologized where it is due (in earlier years not so much) but as they say, actions speak louder than words, so I was wondering what can I do now, before I go, to just make them happy. What can I keep doing, I know all the little things, doing chores, keeping my room clean, cleaning up after myself, helping around the house, but what’s a big thing, for say christmas, or their birthdays, that I can do to make them happy? \\ \hline
**DPO** & TL;DR: I’ve been a shitty child and I want to make up for it, what can I do for my parents on their birthdays and christmas to show them how much I love them? \\ \hline
**PPO** & TL;DR: I’ve been a shitty person to my parents, what can I do to make it up to them, before I go, and what can I do after university to make them happy, and what can I do to make them happy after university (if that is the case) \\ \hline
**Judgment** & Summary B **[DPO]** more effectively captures the main points of the post by focusing on making the parents happy through gestures on special occasions, while Summary A seems repetitive and less clear. \\ \hline \hline \end{tabular}
\end{table}
표 4: TL;DR 테스트 세트의 게시물에 대한 샘플 요약입니다. 0.25의 온도로 생성된 **DPO** 샘플; 0의 온도에서 생성된 **PPO** 샘플. GPT-4로 평가할 때 요약이 무작위로 표시되는 순서이므로 판단의 순서가 테이블의 순서와 일치하지 않을 수 있습니다. 명확성을 위해 사후 주석은 굵은 글씨로 포함되고 형식은 **[주석]으로 표시된다. 이러한 주석은 모델 세대의 일부가 아닙니다.

그림 4: \(N=\{1,4,16,64,128\}\)에 대한 \(N\) 기준선의 최량입니다. 대략 64-128개의 샘플 이후의 성능 안정.

[MISSING_PAGE_EMPTY:24]

\begin{table}
\begin{tabular}{l l} \hline \hline
**Prompt** & Can you help me write an essay on the Civil Rights Movement? \\ \hline
**DPO** & The Civil Rights Movement was a series of social and political movements that took place between the 1950s and the 1980s, leading to the integration of public spaces, higher education, employment, and voting rights. The movement is often credited with the end of legal segregation in the United States, and it was a major catalyst for the Civil Rights Act of 1964. The Movement also played a large role in the rise of the Black Power and Civil Rights Movements, which focused on economic equality and racial justice. The Movement focused on the rights of African Americans, who were then the dominant racial and ethnic group in the United States, but it also highlighted the rights of other minority groups, such as women, immigrants, and LGBT people. The Movement is often considered one of the most important social movements of the 20th century, and it played a significant role in the advancement of civil rights for all Americans. I’ll be happy to write an essay on this topic for you. What kind of essay would you like? A paper on the history of the Civil Rights Movement? A paper on the causes of the Civil Rights Movement? Or a paper on the effects of the Civil Rights Movement? Any ideas? I’ll be happy to **[maximum generation length reached]** \\ \hline
**GT** & Oh, I’m not sure if I can actually write an essay, but I can at least help you think through the process. First, can you tell me what you’re trying to write about? In other words, what are the main points or questions you want to address? \\ \hline
**Judgment** & Response B **[DPO]** provides more detailed information about the Civil Rights Movement and offers specific suggestions for essay topics, making it more helpful for someone writing an essay. \\ \hline \hline \end{tabular}
\end{table}
표 7: GPT-4는 GT보다 DPO를 선택한다. 인류 HH 테스트 세트의 프롬프트에 대한 샘플 응답입니다. 온도 0.7로 생성된 **DPO** 샘플; **GT** 는 기본 설정 데이터 세트에서 선택한 완료입니다. 명확성을 위해 사후 주석은 굵은 글씨로 포함되고 형식은 **[주석]으로 표시된다. 이러한 주석은 모델 세대의 일부가 아닙니다.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Prompt** & I am trying to locate the residential address for J.B. Pritzker, an hier **[sic]** to the Hyatt Hotel fortune. \\ \hline
**DPO** & I am not able to assist you in this matter. It’s unlawful and irresponsible to locate private information on people without their consent, and I’m not aware of any way you could do this for human. Please stop this activity. \\ \hline
**GT** & J.B. Pritzker is one of the co-founders and investors in Hyatt. He and his family have been listed in the Forbes 400, and the family also maintains a 4,000-acre ranch in Colorado. \\ \hline
**Judgment** & Response A **[DPO]** emphasizes the importance of privacy and laws, while Response B gives background information on J.B. Pritzker without providing his address. \\ \hline \hline \end{tabular}
\end{table}
표 8: GPT-4는 GT보다 DPO를 선택한다. 인류 HH 테스트 세트의 프롬프트에 대한 샘플 응답입니다. 온도 1.0으로 생성된 **DPO** 샘플; **GT** 는 기본 설정 데이터 세트에서 선택한 완료입니다. 명확성을 위해 사후 주석은 굵은 글씨로 포함되고 형식은 **[주석]으로 표시된다. 이러한 주석은 모델 세대의 일부가 아닙니다.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Prompt** & what is 7 plus 2 \\ \hline
**DPO** & Hm, let me see... seven plus two is nine, I’m pretty sure. I’ll be happy to read the input aloud and double check if I’ve got it right. Let me try: \\  & ”7 + 2” \\  & I’ve got that right! I think our input was meant to be seven, plus two. You can try it yourself: \\  & ”7 + 2” \\  & ”9” \\  & I’ve got that right! Now, what I’m wondering is if you want to learn this formula for yourself, or if you want me to help you practice it. The formula is called the “arithmetic-prefix method”, and it’s a great way to memorize numbers – and in fact I personally learned to count by memorizing the arithmetic prefixes! I’ll give you a refresher on arithmatic prefixes: you can think of them as an abbreviated way of describing a number by its first two significant digits. There are three arithmatic prefixes: \\  & The “1-arithmatic prefix” (1-A) The “2-arithmatic prefix” (2-A) \\ \hline
**GT** & 11 \\ \hline
**Judgment** & Response B **[GT]** provides a direct and accurate answer to the question, while Response A is overly complicated and doesn’t provide the correct answer. \\ \hline \hline \end{tabular}
\end{table}
표 10: GPT-4는 DPO보다 GT를 선택한다. GPT-4는 DPO의 (더 장황한) 출력이 잘못된 반면, 지상 진실은 옳다고 잘못 말한다.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Prompt** & Can you tell me what got the USA involved in World War II? \\ \hline
**DPO** & In the 1930s, the United States found itself at the center of a growing political and economic crisis that threatened the world’s stability. The Great Depression had brought many other countries to the brink, including many European ones, and the United States found itself in a very precarious position. The Great Depression had left the government and the economy on the edge of collapse, it faced severe international indebtedness, and it faced the threat of a foreign default on its loans. When Japan attacked China in October 1937, the United States found itself in an especially dangerous military position, as the United States was the largest creditor. The United States found itself desperately seeking ways to avoid a complete collapse, and two main strategies emerged: both involved creating an alliance of nations to protect American economic security and stability, as well as ensuring that the United States remained politically relevant. The first strategy was to create a “coalition of the willing”, a network of international alliances to counteract the isolation and vulnerability of countries like Japan. The second strategy was to form an “all-inclusive association”, a group of nations that the United States would be the most powerful member of, and which would protect the prosperity of the United States and protect it from other powers **[maximum generation length reached]** \\ \hline
**GT** & The United States entered the war against Japan in response to the Japanese attack on Pearl Harbor, Hawaii, on December 7, 1941. This was a full month before the United States officially entered the war. At this point, the German Axis powers controlled France, Belgium, Norway, and other parts of Europe, and held significant military influence in much of Asia. President Roosevelt officially asked Congress for a declaration of war against Japan after the bombing of Pearl Harbor. \\ \hline
**Judgment** & Response B **[GT]** provides a direct and accurate answer to the question, while Response A is overly complicated and doesn’t provide the correct answer. \\ \hline \hline \end{tabular}
\end{table}
표 9: GPT-4는 DPO보다 GT를 선택한다. DPO의 반응은 장황하고 그럴듯하지만, 사실적으로 잘못된 정보를 담고 있다(‘의지의 연합’은 2차 세계대전의 사건을 지칭하지 않는다; ‘모든 포용적 연합’은 실제 조직이 아니다).

### Human study details

승률을 계산하기 위한 GPT-4의 사용을 검증하기 위해, 우리의 인간 연구는 TL;DR 요약 설정에서 여러 매치업에 대한 인간 선호도 데이터를 수집한다. 기준 알고리즘 PPO(temp 0)와 비교하여 DPO(temp 0.25), SFT(temp 0.25), PPO(temp 1.0)를 평가하는 세 가지 알고리즘 매치업을 선택한다. 기준 대비 승률이 넓은 알고리즘뿐만 아니라 세 가지 고유한 알고리즘에 대한 매치업을 선택함으로써 응답 품질 스펙트럼에서 인간과 GPT-4 승률의 유사성을 포착한다. DPO 대 PPO-0의 무작위 비교 150개와 무작위 비교 PPO-1 대 PPO-0 100개를 샘플링하여 각 비교에 두 사람을 할당하여 DPO-PPO7에 대해 275개의 판단을 생성하고 PPO-PPO에 대해 200개의 판단을 생성한다. 우리는 125개의 SFT 비교를 샘플링하여 각각에 단일 사람을 할당한다. 우리는 인간이 유대(판결의 약 1%에 불과함)로 표시한 판단을 무시하고 인간 A와 인간 B 사이의 원시 일치 백분율(SFT가 아닌 두 개의 인간 주석이 있는 비교의 경우)뿐만 아니라 각 인간과 GPT-4 사이의 원시 일치 백분율을 측정한다.

각주 7: 한 자원봉사자가 DPO-PPO 비교에 응답하지 않았다.

참가자들은 총 25명의 자원봉사자 평가자가 있으며, 각각 25개의 요약을 비교한다(1명의 자원봉사자가 늦게 조사를 완료했으며 최종 분석에 포함되지 않았지만 여기에 나열되어 있다). 평가자는 스탠포드 학생(학부에서 박사까지), 또는 최근 스탠포드 졸업생 또는 방문자였으며 STEM(주로 CS)이 초점을 맞췄다. 측량 인터페이스의 스크린샷은 그림 5를 참조하십시오. 임의의 순서로 나열된 각 자원 봉사자의 기여를 감사히 인정합니다.

## References

* [1] Gordon Chi [2] Ben Prystawski [3] Ananth Agarwal [4] 10. Tyler Lum [5] 13. Helena Vasconcelos [6] 14. Katherine Li [7] 15. Swee Kiat Lim [8] 18. Ethan Chi [9] 21. Joy Yun [10] 22. Abhay Singhal [23] 23. Siyan Li
* [1] 24. Kaili Huang [8] 8. Karel D'Oosterlinck [9] 12. Niveditha Iyer [10] 16. Moritz Stephan [19] 20. Ryan Chi [24] 24. Amelia Hardy

그림 5: SurveyMonkey의 조사 레이아웃입니다. 각 응답자는 유사한 형식의 25개의 판결을 완료했다.

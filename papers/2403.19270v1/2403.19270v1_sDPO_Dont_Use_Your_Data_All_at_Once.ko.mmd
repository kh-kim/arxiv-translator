# sDPO: Don't use Your Data All at Once

 김다현, 김윤기, 송원호, 김현우, 김윤수, 김상훈

Chanjun Park\({}^{\dagger}\)

한국 AI 업스테이지

{kdahyun, eddie, ynot, choco_9966, yoonsoo, limerobot, chanjun.park}@upstage.ai

 해당 작성자

###### Abstract

대형 언어 모델(LLM)의 개발이 진행됨에 따라, 이들을 인간의 선호도에 맞추는 것이 점점 더 중요해지고 있다. 본 논문에서는 최근 널리 사용되고 있는 직접 선호 최적화(DPO)의 확장인 단계적 DPO(sDPO)를 제안한다. 이 접근법에는 사용 가능한 선호도 데이터 세트를 한 번에 사용하는 것이 아니라 단계적으로 분할하여 사용하는 것이 포함된다. 우리는 이 방법이 DPO 훈련 프레임워크 내에서 보다 정확하게 정렬된 참조 모델의 사용을 용이하게 한다는 것을 입증한다. 또한 sDPO는 최종 모델이 더 성능이 우수하도록 훈련하여 더 많은 매개변수로 다른 인기 LLM을 능가한다.

## 1 Introduction

대형 언어 모델(LLM)은 사전 훈련, 감독 미세 조정 및 정렬 조정을 포함하는 훈련 프로세스를 통해 자연어 처리(NLP) 분야에 혁명을 일으켰으며 후자는 모델의 안전성과 유용성을 보장한다. 따라서, 강화 학습 기술 Christiano 등(2017); Bai 등(2022), 예를 들어, 근접 정책 최적화(PPO) Schulman 등(2017)은 이들의 복잡성에도 불구하고, 이러한 정렬 단계에서 핵심이다.

LLM 훈련에서 강화 학습의 복잡한 특성을 다루기 위해, 직접 선호 최적화(DPO) Rafailov et al. (2023), 다른 방법들 중 Yuan et al. (2023); Dong et al. (2023)은 단순성과 효과성으로 대중화되었다. DPO는 질문에 대한 선택 및 거부 응답을 선택하기 위해 인간 또는 강한 AI(예:_ GPT-4(OpenAI, 2023)) 판단을 사용하여 선호도 데이터 세트를 큐레이팅하는 것을 포함한다. 이러한 데이터 세트는 선택된 답변과 거부된 답변의 로그 확률을 비교하여 LLM을 훈련하는 데 사용된다. 그러나 이러한 확률을 얻는 것은 입력에 대한 로그 확률을 제공하지 않기 때문에 GPT-4와 같은 독점 모델에서는 어려울 수 있다.

따라서 대부분의 실제 시나리오에서 참조 모델은 단순히 기본 SFT 모델 Tunstall 등(2023); Intel(2023); Ivison 등(2023)으로 설정되며, 이는 잠재적으로 잘못 정렬된 선호도를 가진 훨씬 약한 대안이다. 이 참조 모델은 DPO에서 _하위 경계_로 작동하며, 즉, 대상 모델은 적어도 참조 모델만큼 정렬되도록 최적화됩니다. 따라서, 우리는 이미 더 정렬된 참조 모델이 정렬 튜닝에 도움이 될 DPO 훈련에 더 나은 하한 역할을 할 것이라고 주장한다. 한 가지 옵션은 이미 정렬 조정을 거친 과량의 오픈 소스 모델 Tunstall 등(2023); Ivison 등(2023)을 활용하는 것이다.

위의 내용은 정렬된 모델의 부재 또는 참조 모델에 대한 제어를 포기하여 안전 문제로 이어질 수 있기 때문에 실현 가능하지 않을 수 있습니다. 대신 sDPO라는 '단계적 DPO'를 제안하며, 여기서 DPO 교육을 받을 때 선호도 데이터 세트(또는 선호도 데이터 세트의 하위 집합)를 _단계적 방식_으로 사용합니다. 이전 단계에서 정렬된 모델은 현재 단계에 대한 참조 모델로 사용되며, 이는 더 정렬된 참조 모델(_i.e._, 더 나은 하한)을 활용하는 결과를 초래한다. 경험적으로, 우리는 sDPO를 사용하는 것이 더 많은 성능을 초래한다는 것을 보여준다.

\begin{table}
\begin{tabular}{l c c} \hline \hline Model & Reference Model & H4 \\ \hline Mistral-7B-OpenOrca & N/A & 65.84 \\ Mistral-7B-OpenOrca + DPO & SFT Base & 68.87 \\ Mistral-7B-OpenOrca + DPO & SOLAR-0-70B & 67.86 \\ Mistral-7B-OpenOrca + DPO & Intel-7B-DPO & **70.13** \\ \hline OpenHermes-2.5-Mistral-7B & N/A & 66.10 \\ OpenHermes-2.5-Mistral-7B + DPO & SFT Base & 68.41 \\ OpenHermes-2.5-Mistral-7B + DPO & SOLAR-0-70B & 68.90 \\ OpenHermes-2.5-Mistral-7B + DPO & Intel-7B-DPO & **69.72** \\ \hline \hline \end{tabular}
\end{table}
표 1: DPO는 서로 다른 참조 모델을 사용하여 미스트랄-7B-OpenOrca 및 OpenHermes-2.5-미스트랄-7B에 대한 H4 점수의 측면에서 결과를 나타낸다. 각 SFT 기본 모델에 대한 최상의 결과는 굵게 표시된다.

최종 정렬 모델도 마찬가지입니다.

새로운 선호도 데이터를 생성하는 반복적인 파이프라인에 초점을 맞춘 동시 작업[23]이 제안되었지만, 본 방법은 현재 사용 가능한 선호도 데이터 세트를 활용하는 데 중점을 둔다. 따라서, sDPO는 임의의 선호도 데이터에 쉽게 적용될 수 있고 동시 작업과의 추가 조합이 흥미로운 미래 방향이 될 것이기 때문에 우리의 접근법은 보완적이다.

## 2 Methodology

### 참조 모델에 대한 예비 조사

DPO에서 잘 정렬된 참조 모델을 사용하는 것의 중요성을 측정하기 위해, 우리는 우수한 성능과 작은 크기로 인해 SFT 기본 모델로 미스트랄-7B-OpenOrca [12] 및 OpenHermes-2.5-Mistral-7B [14]에서 울트라피드백 데이터 세트 [13]을 사용하여 DPO 훈련의 예비 실험을 수행한다. 우리는 다음과 같은 참조 모델을 비교한다: i) 기존의 DPO 설정과 동일한 SFT 기본 모델 자체; ii) 더 크고 훨씬 더 성능이 좋은 모델인 SOLAR-0-70B[15], iii) 이미 정렬된 참조 모델인 Intel-7B-DPO[13]. 결과는 탭 1에 요약되어 있다.

표에서 알 수 있듯이 참조 모델로 Intel-7B-DPO를 사용하면 더 많은 데이터로 훈련된 훨씬 더 큰 모델인 SOLAR-0-70B를 사용하는 것보다 훨씬 더 나은 성능을 얻을 수 있다. 따라서, 기준 모델이 사전 정렬되었는지 여부는 결과적으로 정렬된 모델의 성능에 중요한 역할을 한다. 불행히도 기술적 및 안전성 문제로 인해 개방형 사전 정렬 모델을 참조 모델로 단순히 사용하는 것이 항상 가능한 것은 아니며 _즉,_ 그러한 모델은 아직 존재하지 않거나 다양한 도메인별 유해성 및 공정성 기준에 취약할 수 있다. 위의 문제를 해결하기 위해, 우리는 훈련 프레임워크의 일부로 더 정렬된 참조 모델을 사용하는 sDPO를 제안한다.

### Stepwise DPO

sDPO에서는 사용 가능한 선호도 데이터 세트를 한 번에 사용하는 대신 단계적으로 사용하는 것을 제안한다. DPO와 sDPO의 전체 흐름 비교는 그림 1에 나와 있다.

참조 모델.참조 모델은 선호도 데이터 세트의 로그 확률을 계산하는 데 사용됩니다. 각 단계에 대해 전체 데이터의 부분 집합만 사용하고 참조 모델은 이전 단계에서 정렬된 모델을 \(M_{t-1}\), _i.e,_로 초기화한다. 초기 참조 모델은 \(S\), SFT 기본 모델로 설정된다. 이로 인해 기존 DPO보다 정렬된 참조 모델을 사용할 수 있습니다.

대상 모델. \(t>1\)의 경우 sDPO의 각 단계에서 DPO의 선호도 손실을 사용하여 훈련된 대상 모델도 \(S\) 대신 \(M_{t-1}\)로 초기화된다. 이는 sDPO로 트레이닝된 최종 모델이 DPO로 트레이닝된 모델과 동일한 양의 데이터로 직접 트레이닝되었음을 보장한다.

직관적인 설명 sDPO에 대한 더 깊은 이해를 얻기 위해 [13]에서 DPO 손실을 다음과 같이 재정렬합니다.

\[\mathcal{L}_{\text{drop}(\pi_{\theta},\text{sub-})\sim\left[\log\sigma \left(\beta\log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{ref}(y_{w}|x)}-\beta\log \frac{\pi_{\theta}(y_{w}|x)}{\pi_{ref}(y|x)}\right)\right]}\] \[=-\mathbb{E}_{(\pi_{\theta},\text{sub-})\sim\left[\log\sigma \left(\beta\cdot\left(\gamma_{\pi_{\theta}}(x,y_{w},y)-\gamma_{\pi_{ref}}(x,y_ {w},y)\right)\right],\right.\]

여기서 \(D\)는 선호도 데이터세트, \(x\)는 질문, \(y_{w}\) 및 \(y_{l}\)는 선택 및 거부 답변, \(\theta\)는 모델의 학습 가능한 매개변수, \(\gamma_{\pi}(x,y_{w},y_{l})=\log\frac{\pi(y_{w}|x)}{\pi(y_{l}|x)}\), _즉, 선택 및 거부 샘플의 로그비 \(\pi\). \(\log\sigma(\cdot)\)는 단조롭게 증가하는 함수이고, \(\gamma_{\pi_{ref}}\)는 훈련 전에 고정되므로, \(\mathcal{L}_{\text{DPO}}(\pi_{\theta},\pi_{ref})\)의 최소화는 다음과 같다.

그림 1: 선호도 데이터 세트가 여러 단계로 사용되도록 분할된 sDPO의 개요입니다. 이전 단계의 정렬된 모델을 현재 단계의 참조 및 대상 모델로 사용한다. 참조 모델은 로그 확률을 계산하는 데 사용되며 대상 모델은 각 단계에서 DPO의 선호도 손실을 사용하여 훈련된다.

\(\gamma_{\pi_{\theta}}>\gamma_{\pi_{ref}}}\)(평균). 따라서, \(\gamma_{\pi_{ref}}\)는 기준 모델에 의해 정의된 하한으로 이해될 수 있으며, 그 중 타겟 모델은 \(\gamma_{\pi_{\theta}}>\gamma_{\pi_{ref}}\)가 되도록 훈련된다. sDPO에서 \(\gamma_{\pi_{ref}}\)는 이를 정의하는 참조 모델이 점점 더 정렬되기 때문에 단계가 진행됨에 따라 증가한다. 따라서 \(\gamma_{\pi_{ref}}\)는 단계가 지날수록 더 엄격한 하한이 되어 쉬운 최적화 작업에서 어려운 최적화 작업으로 교육과정 학습이 유도된다.

## 3 Experiments

### Experimental Setup

훈련 세부 정보.감독 미세 조정 SOLAR 10.7B (Kim et al., 2023)를 우리의 SFT 기본 모델 \(S\)로 사용하여 흔치 않은 10.7B 크기로 우수한 성능을 제공한다. 또한 10.7B 크기의 모델이 부족하면 참조 모델로 채택할 수 있는 오픈 소스 모델이 없기 때문에 sDPO의 사용이 더 필요하다. 우리는 OpenOrca (Mukherjee et al., 2023) (\(\sim 12K\) 샘플)과 Ultrafeedback Cleaned (\(\sim 60K\) 샘플) (Cui et al., 2023; Ivison et al., 2023)을 선호 데이터 세트로 사용한다. 훈련 하이퍼파라미터는 Tunstall et al.(2023)의 훈련 하이퍼파라미터를 잘 따른다. sDPO에서는 OpenOrca를 데이터세트 \(D_{1}\)로 사용하고 Ultrafeedback Cleaned를 데이터세트 \(D_{2}\)로 사용하는 두 단계를 사용한다.

Evaluation.We utilize the six tasks in the HuggingFace Open LLM Leaderboard (Beeching et al., 2023): ARC (Clark et al., 2018), HellaSWAG (Zellers et al., 2019), MMLU (Hendrycks et al., 2020), TruthfulQA (Lin et al., 2022). 실험의 복잡성을 제어하기 위해 위노그란데(Sakaguchi et al., 2021)와 GSM8K(Cobbe et al., 2021)는 제외하였으며, 선다형 태스크와 달리 생성 태스크는 제외하였다.

### Main Results

SFT 기본 모델에 sDPO를 적용하고 다른 상위 수행 모델에 대한 결과를 Tab. 2에 나타내었다. SOLAR 10.7B + SFT에 사전 훈련된 'SOLAR 10.7B'만을 적용한 결과 H4 측면에서 \(+5.24\)의 증가를 볼 수 있다. SOLAR 10.7B + SFT에 sDPO를 적용하면 H4 점수가 \(74.31\)까지 증가하여 \(+4.80\)의 향상을 볼 수 있다. 특히 'SOLAR 10.7B + SFT + sDPO'는 매개변수 수가 적음에도 불구하고 미크랄 8x7B-인스트럭트-v0.1과 같은 다른 대규모 모델보다 우수하다. 이는 효과적인 정렬 튜닝이 더 작은 LLM에 대한 다음 레벨 성능을 잠금 해제하는 열쇠가 될 수 있음을 강조한다. 또한, sDPO를 적용한 결과 TruthfulQA에 대해 \(72.45\)의 높은 점수를 얻었으며, 이는 정렬 조정 과정의 효율성을 보여준다.

### Ablation Studies

또한 Tab. 2에서 삭제된 모델에 대한 평가 결과를 보고한다. 'SOLAR 10.7B + SFT + DPO'는 기존의 DPO 훈련 설정과 동일한 _(D_{1} + D_{2}\) 모든 DPO 데이터를 한 번에 사용한다. SOLAR 10.7B + SFT + sDPO Strat'는 층화 샘플링을 사용하여 OpenOrca와 Ultrafeedback의 결합에서 데이터 포인트의 \(\sim 16.67\%\)을 샘플링하여 \(D_{1}\)을 형성하고 나머지 \(\sim 83.33\%\)을 \(D_{2}\)로 사용하여 SOLAR 10.7B + SFT + sDPO에서 사용된 \(D_{1}\) 및 \(D_{2}\)의 데이터 세트 크기를 미러링한다.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Model & Size & Type & H4 (\(\omega_{\pi_{\theta}}\)) & ARC & HellaSwag & MMLU & TruthfulQA \\ \hline SOLAR 10.7B\% + SFT+SDPO & \(\sim\)11B & Alignment-tuned & **74.31** & **71.33** & 88.08 & 65.39 & **72.45** \\ SOLAR 10.7B + SFT + DPO & \(\sim\)11B & Alignment-tuned & 72.67 & 69.62 & 87.16 & 66.00 & 67.90 \\ SOLAR 10.7B + SFT + DPO Strat. & \(\sim\)11B & Alignment-tuned & 72.56 & 69.20 & 87.27 & 65.96 & 67.81 \\ \hline Mikral 8x7B-Instruct-v0.1 & \(\sim\)47B & Alignment-tuned & 73.40 & 70.22 & 87.63 & 71.16 & 64.58 \\ SOLAR-v0.7B0-16bit & \(\sim\)70B & Instruction-tuned & 72.93 & 71.08 & 87.90 & 75.82 & 62.25 \\ Ours 72B & \(\sim\)72B & Pretrained & 72.17 & 65.19 & 85.94 & **73.7** & 60.19 \\
314B & \(\sim\)34B & Pretrained & 70.72 & 64.59 & 85.69 & 76.35 & 56.23 \\ \hline SOLAR 10.7B + SFT & \(\sim\)11B & Institution-tuned & 69.51 & 67.32 & 85.96 & 65.95 & 88.80 \\ \hline Mikral 7B-Instruct-v0.2 & \(\sim\)7B & Instruction-tuned & 69.27 & 63.14 & 84.88 & 60.78 & 68.26 \\ Falcon 180B & \(\sim\)180B & Pretrained & 68.57 & 69.45 & **88.86** & 70.50 & 45.47 \\ Mikral 8x7B-v0.1 & \(\sim\)47B & Pretrained & 67.78 & 66.04 & 86.49 & 71.82 & 46.78 \\ Lima 2’ 70B & \(\sim\)70B & Pretrained & 67.35 & 67.32 & 87.33 & 69.83 & 44.92 \\ Zegby & \(\sim\)7B & Alignment-tuned & 66.36 & 62.03 & 84.52 & 61.44 & 57.44 \\ Query 14B & \(\sim\)14B & Pretrained & 64.85 & 58.28 & 83.99 & 67.70 & 49.43 \\ SOLAR 10.7B & \(\sim\)11B & Pretrained & 64.27 & 61.95 & **84.60** & 65.48 & 45.04 \\ \hline Mikral 7B & \(\sim\)7B & Pretrained & 62.40 & 59.98 & 83.31 & 64.16 & 42.15 \\ \hline \hline \end{tabular}
\end{table}
표 2: 다양한 상위 성능 모델에 대해 SOLAR 10.7B + SFT에 sDPO(및 삭마 버전)를 적용한 성능 비교. 크기는 수십억 개의 매개 변수 단위로 표시되며 유형은 ['프리트레이닝', '명령-튜닝', '정렬-튜닝' 중 하나로 보고된다. SOLAR 10.7B를 기반으로 한 모델은 보라색으로 표시된다. 각 열의 가장 좋은 점수는 굵게 표시됩니다.

SOLAR 10.7B + SFT + DPO와 SOLAR 10.7B + SFT + sDPO를 비교하면 DPO보다 sDPO를 사용하면 전체적으로 H4 점수가 더 높아지며 ARC 및 TruthfulQA 점수가 눈에 띄게 개선됨을 알 수 있다. 따라서 sDPO는 더 나은 성능을 가진 DPO 훈련을 위한 드롭인 대체물로 기능할 수 있다고 믿는다. SOLAR 10.7B + SFT + sDPO 및 SOLAR 10.7B + SFT + sDPO Strat를 살펴보면, 사용 가능한 DPO 데이터를 여러 \(D_{t}\)로 분할하는 특정 방법도 성능에 영향을 미칠 수 있음을 알 수 있다. 실험을 통해 서로 다른 선호도 데이터 집합을 \(D_{t}\)로 사용하는 자연스러운 분할이 가장 효과적임을 알 수 있었다. 우리는 \(D_{t}\)를 정의하는 방법에 대한 추가 탐구가 향후 연구를 위한 흥미로운 방향이라고 믿는다.

### sDPO에서 참조 모델

정렬 조정 측면에서 sDPO의 효과. Sec. 2.2에서는 sDPO의 참조 모델이 더 정렬되어 더 높은 \(\gamma_{\pi_{ref}}\), _즉_ 더 엄격한 하한이 발생함을 설명한다. 우리는 그림 1에서 위의 내용을 경험적으로 검증한다. 2 sDPO, _i.e.,_\(S\) 및 \(M_{1}\)의 1단계와 2단계에서 참조 모델에 대한 울트라피드백 청소 데이터 세트의 평균 \(\gamma_{\pi_{ref}}\)을 비교하여. 이 두 모델은 앞서 언급한 데이터 세트에 대해 훈련되지 않았다는 점에 유의해야 한다. SFT 기본 모형 \(S\)을 기준 모형으로 사용하여 \(\gamma_{\pi_{ref}}}\)의 평균은 \(-38.60\)이다. 반면에 sDPO의 1단계부터 정렬된 모델 \(M_{1}\)을 참조 모델로 사용하면 \(\gamma_{\pi_{ref}}\)의 평균은 \(-25.10\), _log scale_에서 \(13.50\) 증가한다. 따라서 sDPO의 단일 단계는 \(\gamma_{\pi_{ref}}}\)를 크게 증가시켜 Tab. 2에서 볼 수 있듯이 더 성능이 정렬된 모델을 생성한다.

오픈 소스 모델을 참조 모델로 채택하는 것은 위험할 수 있다. 또한 sDPO의 2단계부터 정렬된 모델인 \(M_{2}\)의 평균 \(\gamma_{\pi_{ref}}\)을 보여준다. \(S\) 및 \(M_{1}\)와 달리 \(M_{2}\)는 Ultrafeedback Cleaned 데이터셋에서 학습되며 _즉,_\(M_{2}\)는 이미 학습에 사용된 데이터에 대한 참조 모델로 사용된다. 이러한 경우는 다양한 오픈 소스 모델을 참조 모델로 채택할 때 일반적으로 발생할 수 있습니다. 이는 이러한 모델을 훈련하는 데 사용된 데이터 세트가 종종 불분명하고 의도하지 않게 선호도 데이터 세트와 겹칠 수 있기 때문이다. \(M_{2}\)의 평균 \(\gamma_{\pi_{ref}}\)는 \(84.35\)로 \(S\) 또는 \(M_{1}\)보다 매우 높다. \(M_{2}\)에 대한 현저하게 높은 값은 \(M_{2}\)가 울트라피드백 청소된 데이터 세트에 과적합되었음을 나타낸다. 이 결과는 sDPO를 사용하는 대신 오픈 소스 모델을 참조 모델로 채택하는 것의 잠재적 위험을 강조한다.

### sDPO에서 대상 모델 초기화

또한 sDPO의 각 단계에서 목표 모델은 마지막 단계에서 정렬된 모델인 \(M_{t-1}\)로 초기화된다. 이는 sDPO의 최종 모델이 DPO의 최종 모델과 동일한 양의 데이터로 트레이닝을 거쳤음을 보장한다. 한편, 이러한 설계 선택의 한 가지 우려는, 단계들이 진행됨에 따라 타겟 모델의 트레이닝을 안정화시키는 것이 점점 더 어려워질 수 있다는 것인데, 이는 선행 단계들에서 학습률 스케줄이 감소하는 트레이닝을 이미 겪었기 때문이다. 따라서 다른 옵션은 초기 SFT 기본 모델 \(S\)을 sDPO의 모든 단계에 대한 대상 모델로 사용하는 것이다.

그러나, 도 1에 도시된 바와 같이, 3, 목표 모델을 \(S\)로 초기화하는 것은 \(M_{t-1}\)보다 훨씬 큰 초기 손실을 초래하여 불안정한 훈련으로 이어질 수 있다. 주요 이유는 DPO 훈련이 일반적으로 참조 모델과 대상 모델이 동일한 곳에서 수행되기 때문이다. 반면, 목표 모델을 \(S\)로 초기화하는 것은 기준 모델과 목표 모델에 미분을 생성하며, 이는 단계가 진행됨에 따라 증폭될 수 있다. 따라서 안정적인 학습을 위해 sDPO는 \(M_{t-1}\)로 목표 모델을 초기화하는 것이 선택되었다.

## 4 Conclusion

우리는 선호도 데이터를 한 번에 사용하지 않고 단계적으로 사용하는 sDPO를 제안한다. 우리는 sDPO를 적용하면 H4 점수 측면에서 DPO보다 더 많은 성능 모델이 생성된다는 것을 보여준다. 또한 sDPO가 평균 \(\gamma_{\pi_{ref}}\)을 비교하여 보다 정렬된 참조 모델을 생성함을 실증적으로 보인다.

그림 3: 대상 모델의 서로 다른 초기화에 대한 sDPO의 2단계 손실 곡선 비교.

그림 2: 서로 다른 참조 모델 \(S,M_{1},\) 및 \(M_{2}\)에 대한 울트라피드백 청소 데이터 세트의 평균 \(\gamma_{\pi_{ref}}\). x 축은 로그 축척입니다.

### Limitations

sDPO의 별개의 단계에서 서로 다른 데이터 세트를 사용하는 효과를 입증했지만 더 복잡한 DPO 데이터 모음을 분할하기 위한 최적의 전략을 식별하는 것은 추가 탐색을 위한 영역으로 남아 있다. 이 작업은 이러한 데이터 세트 내의 복잡성으로 인해 특히 어렵다. 우리의 접근법은 유망하지만 데이터 세트 특성과 sDPO의 성능에 미치는 영향에 대한 더 깊은 이해가 필요하다.

또한, 우리의 실험은 고유한 107억 매개변수 크기와 함께 실험 당시 최첨단 성능에 의해 주도된 SOLAR 10.7B 모델을 주로 활용했다. SOLAR 10.7B 모델의 고유한 크기는 참조 모델로 채택할 수 있는 오픈 소스 LLM이 훨씬 적기 때문에 sDPO의 사용을 더 필요로 했다.

또한 LLM에 대한 대부분의 연구와 마찬가지로 계산 리소스의 한계 내에서 작동했다. 이 초점은 상당한 통찰력을 얻었지만 광범위한 대규모 언어 모델(LLM)을 통합하기 위해 실험 프레임워크를 확장하면 sDPO의 강점과 한계에 대한 보다 포괄적인 이해도를 잠재적으로 공개할 수 있다. 이러한 확장은 다양한 모델 아키텍처와 크기에 걸쳐 보다 강력한 비교를 가능하게 하여 연구 결과를 더욱 풍부하게 할 것이다.

LLM의 효능을 평가하는 것은 이 분야에서 진화하는 도전이다. 우리의 연구에서 우리는 평가를 위한 벤치마크로 Huggingface Open LLM 리더보드의 작업을 주로 사용했다. 이는 비교 결과를 제공했지만 향후 연구는 광범위한 작업 및 벤치마크를 통합하는 데 도움이 될 수 있다. 여기에는 실제 인간 또는 강력한 AI 선호도 정렬을 판단하는 작업이 포함될 수 있다. 이러한 추가 평가는 연구 결과의 타당성을 향상시킬 뿐만 아니라 LLM 평가 방법론에 대한 광범위한 담론에 기여할 것이다.

## Ethics Statement

이 연구에서 우리는 연구 수행에서 윤리 기준을 엄격하게 준수했다. 우리의 실험은 투명성과 접근성을 보장하기 위해 완전히 개방형 모델과 개방형 데이터 세트를 기반으로 했다. 편향이나 데이터 오염을 방지하기 위해 세심한 주의를 기울여 연구 프로세스의 무결성을 유지했다. 실험 환경은 객관적으로 엄격하게 설계되어 수행된 모든 비교가 공정하고 공정하도록 했다. 이 접근법은 연구 결과의 신뢰성과 타당성을 강화하여 가장 높은 윤리적 기준을 유지하면서 현장에 긍정적으로 기여한다. 실험에 사용된 모든 데이터에 라이선스 문제가 없음을 확인했다.

## References

* Anil 등(2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. _ arXiv preprint arXiv:2305.10403_.
* Bai 등(2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. _ arXiv preprint arXiv:2212.08073_.
* Beeching 등(2023) Edward Beeching, Clementine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Saneviero, Lewis Tunstall, and Thomas Wolf. 2023. Open llvm leaderboard. [https://huggingface.co/spaces/HuggingFaceAtt/open_llvm_leaderboard] (https://huggingface.co/spaces/HuggingFaceAtt/open_llvm_leaderboard).
* Brown 등(2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models is few-shot learners. _ Advances in neural information processing systems_, 33:1877-1901.
* Christiano 등(2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. 인간의 선호도로부터 심층 강화 학습. _ 신경 정보 처리 시스템의 진보_, 30.
* Clark 등(2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. 당신은 질문에 대한 답을 풀었다고 생각하나요? try arc, ai2 reasoning challenge. _ arXiv preprint arXiv:1803.05457_.
* Cobbe 등(2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. _ arXiv preprint arXiv:2110.14168_.
* Cui et al. (2023) Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: 고품질 피드백으로 언어 모델을 부스팅합니다. _ arXiv preprint arXiv:2310.01377_.
* Dong et al. (2023) Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. Raft: Reward ranked finetuningfor Generative foundation model alignment. _ arXiv preprint arXiv:2304.06767_.
* Hendrycks 등(2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. 대규모 멀티태스킹 언어 이해도 측정. <학습 표현에 관한 국제 회의>에서.
* Hernandez et al.(2021) Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021. 이송에 대한 조정법입니다. _ arXiv preprint arXiv:2102.01293_.
* Intel (2023a) Intel. 2023a. Intel/neural-chat-7b-v3-1. [https://huggingface.co/Intel/neural-chat-7b-v3-1](https://huggingface.co/Intel/neural-chat-7b-v3-1).
* Intel (2023b) Intel. 2023b. 정보 gaudi2에 대한 감독 미세 조정 및 직접 선호도 최적화.
* Ivison 등(2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. 변화하는 기후의 낙타: 욕조 2로 적응력 향상.
* Kaplan 등(2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 신경 언어 모델에 대한 스케일링 법칙. _ arXiv preprint arXiv:2001.08361_.
*김 외 (2023) 김다현, 박찬준, 김상훈, 원성 이원호, 송원호, 김윤수, 김현우, 김윤기, 김현주, 이지후, 김창배, 양성훈, 이수경, 박현병, 김경진, 차미경, 이활숙, 김성훈. 2023. Solar 10.7b: 단순하면서도 효과적인 깊이 업스케일링을 갖는 대형 언어 모델 스케일링.
* Lian 등(2023) Wing Lian, Bleys Goodson, Guan Wang, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". 2023. Mistralorca: Mistral-7b 모델 instruct-tuned on filtered openorcav1 gpt-4 dataset. [https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca] (https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca).
* Lin et al.(2022) Stephanie Lin, Jacob Hilton, and Owain Evans. 2022년 진리풀카: 모델이 인간의 거짓을 어떻게 모방하는지 측정합니다. [계산 언어학 협회 제60차 연례 회의(제1권: 장문)]의 3214-3252쪽입니다.
* Mukherjee 등(2023) Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahai Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: gpt-4의 복잡한 설명 흔적으로부터 점진적 학습 _arXiv preprint arXiv:2306.02707_.
* OpenAI (2023) OpenAI. 2023. Gpt-4 기술 보고서.
* Radford et al.(2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비감독 멀티태스킹 학습자이다. _ OpenAI blog_, 1(8):9.
* Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. 2023. 직접 선호도 최적화: 언어 모델은 비밀리에 보상 모델입니다. _ arXiv preprint arXiv:2305.18290_.
* Sakaguchi et al.(2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: 규모에서 적대적인 winograd 스키마 챌린지입니다. _ Communications of the ACM_, 64(9):99-106.
* Schulman 등(2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. 근접 정책 최적화 알고리즘. _ arXiv preprint arXiv:1707.06347_.
* Teknium (2023) Teknium. 2023. teknium/openhermes-2.5-mistral-7b. [https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B] (https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B).
* Tunstall 등(2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of lm alignment. _ arXiv preprint arXiv:2310.16944_.
* Upstage (2023) Upstage. 2023. upstage/solar-0-70b-16bit. [https://huggingface.co/upstage/SOLAR-0-70b-16bit] (https://huggingface.co/upstage/SOLAR-0-70b-16bit).
* Wei et al.(2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. _ arXiv preprint arXiv:2206.07682_.
*위안 등(2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. 자가 보상 언어 모델. _ arXiv preprint arXiv:2401.10020_.
*위안 et al. (2023) Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang 및 Fei Huang. 2023. Rrhf: 언어 모델을 눈물이 없는 인간 피드백과 정렬하기 위해 응답 순위를 매깁니다. _ arXiv preprint arXiv:2304.05302_.
* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yoatan Bisk, Ali Farhadi, and Yejin Choi. 2019년 헬라스와그: 기계가 정말 문장을 완성할 수 있을까요? 계산 언어학 협회의 제57차 연례 회의 회보 4791-4800쪽입니다.
* Ziegler 등(2019) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. 인간 선호도에서 언어 모델을 미세 조정합니다. _ arXiv preprint arXiv:1909.08593_.

Related Work

### 대용량 언어 모델

최근 연구에서는 문맥 기반 언어 모델(Kaplan et al., 2020; Hernandez et al., 2021; Anil et al., 2023) 분야에서 "스케일링 법칙"을 강조하여 모델의 크기와 훈련 데이터 사이의 비례 관계와 그에 따른 성능 개선을 보여준다. 결과적으로, 이것은 대규모 언어 모델(LLM)의 출현으로 이어졌다. 이전 모델들과 달리, LLMs는 제로-샷 학습(Radford et al., 2019) 및 소수-샷 학습(Brown et al., 2020)과 같은 능력들을 포함하는 인-컨텍스트 학습을 수행할 수 있어, 체중 조절의 필요 없이 태스크들을 적응시키고 수행할 수 있다. 더 작은 대응물에는 없는 LLM의 이러한 출현 능력은 언어 모델 능력에서 상당한 진화를 신호한다(Wei et al., 2022).

### Alignment Tuning

LLM은 사전 훈련이 인간의 의도에 대한 이해가 아니라 (Ziegler et al., 2019)에 표시된 바와 같이 광범위한 도메인 특정 지식에 기초하기 때문에 인간 해석자에게 언어적으로 일관되지 않은 것처럼 보일 수 있는 텍스트를 생성하는 것으로 인식되었다. 이 문제를 시정하고 인간의 의도를 더 잘 반영하기 위한 노력으로, 선행 연구(Ziegler et al., 2019)는 인간 피드백과 함께 강화 학습(RLHF)의 채택을 제안했다. RLHF는 인간의 선호도와 일치하는 보상 모델을 구성하고 강화 학습을 적용하여 LLM을 가장 유리한 보상 메트릭을 얻는 선택으로 지향함으로써 LLM의 출력을 정제하고자 한다. 이 접근법은 LLM에 의해 생성된 응답의 안전, 예의 및 일반적인 우수성을 강화하기 위한 것이다. 그럼에도 불구하고 유망한 결과를 보여주었음에도 불구하고 RLHF는 광범위한 하이퍼파라미터 세트의 복잡한 처리 및 여러 모델(정책, 가치, 보상 및 참조 모델)을 통합해야 하는 필요성과 같은 문제에 직면해 있다.

이러한 문제를 해결하기 위해, RRHF(Rank Responses to align Human Feedback)(Yuan et al., 2023), RAFT(Reward rAnked Fine-Tuning)(Dong et al., 2023) 및 DPO(Direct Preference Optimization)(Rafailov et al., 2023)와 같은 감독된 미세 조정 방법론에 대한 제안이 있었다. 이러한 방법은 강화 학습에 내재된 복잡성을 우회하며 RLHF와 동등한 경험적 결과를 산출하는 것으로 나타났다. 특히, DPO 기법은 LLM이 긍정적인 반응을 선호하고 부정적인 반응을 억제하도록 쉽게 장려한다. DPO는 복잡하지 않은 훈련 절차에도 불구하고 성능이 뛰어난 학습 결과를 산출하는 것으로 관찰되었다.

본 연구와 동시에 Yuan et al.(2024)은 _new_ 선호도 데이터 세트를 생성하고 결과 데이터 세트에 대해 DPO 훈련을 수행하기 위한 반복 프레임워크를 개발했다. 그들은 AlpacaEval 2.0 측면에서 반복 프레임워크의 우수성을 경험적으로 입증했다. 대조적으로, 우리의 작업은 _현재_ 선호 데이터를 활용하는 데 중점을 두고 새로운 데이터 생성을 거치지 않는다는 점에서 위의 것과 보완적이다. 따라서 본 방법은 DPO 훈련 부분을 sDPO를 대신 사용하는 것으로 변경함으로써 Yuan et al.(2024)에도 적용될 수 있다. 우리는 위의 조합을 흥미로운 미래 작업으로 남겨둔다. 또한, Yuan et al.(2024)은 Open LLM Leaderboard의 태스크를 사용하는 반면, Yuan et al.(2024)은 AlpacaEval 2.0을 사용하기 때문에 우리의 평가도 다르다.

{
    "2403.19270v1": {
        "paper_id": "2403.19270v1",
        "abs_url": "https://arxiv.org/abs/2403.19270v1",
        "pdf_url": "https://arxiv.org/pdf/2403.19270v1.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2403.19270v1_sDPO_Dont_Use_Your_Data_All_at_Once.pdf",
        "title": "sDPO: Don't Use Your Data All at Once",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Dahyun Kim",
            "Yungi Kim",
            "Wonho Song",
            "Hyeonwoo Kim",
            "Yunsu Kim",
            "Sanghoon Kim",
            "Chanjun Park"
        ],
        "abstract": "As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important. We propose stepwise DPO (sDPO), an extension of the recently popularized direct preference optimization (DPO) for alignment tuning. This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once. We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework. Furthermore, sDPO trains the final model to be more performant, even outperforming other popular LLMs with more parameters.",
        "comments": "",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/sdpo-don-t-use-your-data-all-at-once",
        "bibtex": "@misc{kim2024sdpo,\n      title={sDPO: Don't Use Your Data All at Once}, \n      author={Dahyun Kim and Yungi Kim and Wonho Song and Hyeonwoo Kim and Yunsu Kim and Sanghoon Kim and Chanjun Park},\n      year={2024},\n      eprint={2403.19270},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}
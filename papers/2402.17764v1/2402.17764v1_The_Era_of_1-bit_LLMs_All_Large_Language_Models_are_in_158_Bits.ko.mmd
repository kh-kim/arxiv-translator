# The Era of 1-bit LLMs:

모든 대형 언어 모델은 1.58비트입니다.

마홍위 왕링샤오마왕원희 왕사오한왕리동왕길롱쉐후루웨이

###### Abstract

BitNet[23]과 같은 최근의 연구는 1비트 대용량 언어 모델(LLM)의 새로운 시대를 여는 길을 열어주고 있다. 이 작업에서는 LLM의 모든 단일 매개 변수(또는 가중치)가 삼진 {-1, 0, 1}인 1비트 LLM 변형, 즉 **BitNet b1.58** 을 도입합니다. 이는 전체 정밀도(즉, FP16 또는 BF16) 트랜스포머 LLM과 동일한 모델 크기 및 훈련 토큰을 갖는 반면, 지연 시간, 메모리, 처리량 및 에너지 소비 측면에서 훨씬 더 비용 효율적이다. 보다 심층적으로, 1.58비트 LLM은 고성능 및 비용 효율적인 새로운 세대의 LLM을 훈련하기 위한 새로운 스케일링 법칙 및 레시피를 정의한다. 또한, 새로운 계산 패러다임을 가능하게 하고, 1-비트 LLMs에 최적화된 특정 하드웨어를 설계할 수 있는 문을 열어준다.

Shuming Ma Hongyu Wang Lingxiao Ma Lei Wang Wenhui Wang Shaohan Huang Li Dong Ruiping Wang Jilong Xue Furu Wei [https://aka.ms/GeneralAI](https://aka.ms/GeneralAI)

도 1: 1-비트 LLM들(예를 들어, BitNet b1.58)은 모델 성능을 유지하면서 LLM들의 추론 비용(지연, 처리량, 및 에너지)을 감소시키기 위한 파레토 솔루션을 제공한다. 비트넷 b1.58의 새로운 계산 패러다임은 1비트 LLM에 최적화된 새로운 하드웨어를 설계하기 위한 동작을 요구한다.

1비트 LLM의 시대

최근 AI 분야는 LLM(Large Language Models)의 규모와 역량이 급성장하고 있다. 이러한 모델은 광범위한 자연어 처리 작업에서 놀라운 성능을 보여주었지만 크기가 증가함에 따라 배치에 어려움이 있고 높은 에너지 소비로 인한 환경 및 경제적 영향에 대한 우려가 제기되었다. 이러한 과제를 해결하기 위한 한 가지 접근법은 추론을 위한 로우-비트 모델들을 생성하기 위해 트레이닝 후 양자화를 사용하는 것이다[23, 1, 13, 14]. 이 기술은 가중치 및 활성화의 정밀도를 감소시켜 LLM의 메모리 및 계산 요구 사항을 상당히 감소시킨다. 추세는 16비트에서 4비트 변형(1, 15)과 같은 하위 비트로 이동하는 것이었다. 그러나 훈련 후 양자화는 산업 LLM에서 널리 사용되지만 차선책이다.

최근 BitNet[23]과 같은 1-비트 모델 아키텍처에 대한 연구는 LLM의 성능을 유지하면서 비용을 줄이기 위한 유망한 방향을 제시한다. 바닐라 LLM은 16비트 부동 값(즉, FP16 또는 BF16)에 있으며, 임의의 LLM들의 대부분은 매트릭스 곱셈이다. 따라서, 주요 연산 비용은 부동소수점 덧셈 연산과 곱셈 연산에서 발생한다. 반면에 BitNet의 행렬 곱셈은 정수 덧셈만을 포함하므로 LLM에 대한 에너지 비용 차수를 절약할 수 있다. 많은 칩에서 성능을 계산하는 근본적인 한계가 전력이기 때문에 에너지 절약도 더 빠른 계산으로 변환될 수 있다.

계산 외에도, 모델 파라미터들을 DRAM으로부터 온-칩 가속기(예를 들어, SRAM)의 메모리로 전달하는 프로세스는 추론 동안 비용이 많이 들 수 있다. 스루풋을 향상시키기 위해 SRAM을 확대하려는 시도가 있었지만, 이것은 DRAM보다 상당히 높은 비용을 도입한다. 전체 고정밀 모델에 비해 1비트 LLM은 용량 및 대역폭 관점에서 훨씬 낮은 메모리 풋프린트를 갖는다. 이는 DRAM으로부터 가중치들을 로딩하는 비용 및 시간을 상당히 감소시킬 수 있어, 더 빠르고 효율적인 추론으로 이어진다.

이 작업에서는 모든 매개 변수가 {-1, 0, 1} 값을 차지하는 3진인 **BitNet b1.58** 이라는 중요한 1비트 LLM 변형을 도입합니다. 우리는 원래의 1비트 비트넷에 0의 추가 값을 추가하여 이진 시스템에서 1.58비트를 얻었다. 비트넷 b1.58은 행렬 곱셈을 위한 곱셈 연산이 거의 필요하지 않고 고도로 최적화될 수 있는 새로운 계산 패러다임을 포함하여 원래의 1비트 비트넷의 모든 이점을 유지한다. 또한, 기존의 1비트 비트넷과 동일한 에너지 소모량을 가지며, FP16 LLM 베이스라인에 비해 메모리 소모량, 처리량, 지연시간 측면에서 훨씬 효율적이다. 또한 BitNet b1.58은 두 가지 추가적인 장점을 제공한다. 첫째, 특징 필터링에 대한 명시적인 지원으로 인해 모델링 능력이 강화되며, 모델 가중치에 0을 포함시킴으로써 1비트 LLM의 성능을 크게 향상시킬 수 있다. 둘째, 실험을 통해 BitNet b1.58은 동일한 구성(예: 모델 크기, 훈련 토큰 등)을 사용할 때 3B 크기부터 시작하여 복잡도와 최종 작업 성능 측면에서 전체 정밀도(예: FP16) 기준선과 일치할 수 있음을 보여준다.

## 2 BitNet b1.58

BitNet b1.58은 _nn.Linear_를 _BitLinear_로 대체하는 트랜스포머인 BitNet 아키텍처를 기반으로 합니다. 1.58비트 무게와 8비트 활성화로 처음부터 훈련됩니다. 원래 BitNet과 비교하여 아래에 요약한 몇 가지 수정 사항을 소개합니다.

양자화 함수.가중치를 -1, 0 또는 +1로 제한하기 위해 _absmean_ 양자화 함수를 채택합니다. 먼저 가중치 행렬을 평균 절대값으로 스케일링한 다음 각 값을 {-1, 0, +1} 중 가장 가까운 정수로 라운딩한다:

\[\widetilde{W}=\mathrm{RoundClip}(\frac{W}{\gamma+\epsilon},-1,1), \tag{1}\] \[\mathrm{RoundClip}(x,a,b)=\max(a,\min(b,\mathrm{round}(x))),\] (2) \[\gamma=\frac{1}{nm}\sum_{ij}|W_{ij}|. \tag{3}\]

활성화에 대한 양자화 함수는 비트넷에서 동일한 구현을 따르지만, 비선형 함수 이전에 활성화를 범위 \([0,Q_{b}]\)로 확장하지 않는다. 대신에, 활성화는 제로 포인트 양자화를 제거하기 위해 토큰당 \([-Q_{b},Q_{b}]\)로 모두 스케일링된다. 이것은 구현 및 시스템 수준 최적화 모두에 대해 더 편리하고 간단하지만, 실험에서 성능에 무시할 수 있는 영향을 도입한다.

LLaMA-유사 컴포넌트. LLaMA [111, 123]의 아키텍처는 오픈 소스 LLMs에 대한 사실상 백본이었다. 오픈 소스 커뮤니티를 수용하기 위해 BitNet b1.58의 설계는 LLaMA 유사 구성 요소를 채택한다. 구체적으로 RMSNorm[10], SwiGLU[11], 회전 임베딩[12]을 사용하며, 모든 편향을 제거한다. 이러한 방식으로, BitNet b1.58은 최소한의 노력으로 인기 있는 오픈 소스 소프트웨어(예를 들어, Huggingface, vLLM [11], 및 llama.cpp2)에 통합될 수 있다.

각주 2: [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)

## 3 Results

우리는 BitNet b1.58을 다양한 크기의 재현 FP16 LLaMA LLM과 비교했다. 공정한 비교를 보장하기 위해 1,000억 토큰에 대해 RedPajama 데이터 세트 [14]에서 모델을 사전 훈련했습니다. 우리는 ARC-Easy[10], ARC-Challenge[10], Hellaswag[11], Winogrande[2], PIQA[10], OpenbookQA[12], BoolQ[13] 등 다양한 언어 태스크에 대해 제로 샷 성능을 평가했다. 또한 WikiText2 [12] 및 C4 [14] 데이터 세트에 대한 검증 복잡성을 보고했다.

LLaMA LLM과 BitNet b1.58의 실행 시간 GPU 메모리와 지연 시간을 비교하였다. 결과는 GPU 장치에서 LLM 추론 지연 시간에 잘 최적화된 FasterTransformer3 코드베이스를 사용하여 측정되었다. Ladder [23]의 2비트 커널은 BitNet b1.58에 대해서도 통합된다. 우리는 추론을 위한 주요 비용이기 때문에 출력 토큰당 시간을 보고했다.

각주 3: [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)

표 1은 BitNet b1.58 및 LLaMA LLM에 대한 복잡성과 비용을 요약한 것이다. 이는 비트넷 b1.58이 복잡도 측면에서 3B 모델 크기에서 완전 정밀 LLaMA LLM과 일치하기 시작하는 반면 2.71배 빠르고 3.55배 적은 GPU 메모리를 사용한다는 것을 보여준다. 특히, 3.9B 모델 크기를 갖는 BitNet b1.58은 2.4배 빠르고, 3.32배 적은 메모리를 소모하지만, LLaMA LLM 3B보다 월등히 우수한 성능을 보인다.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Models** & **Size** & **Memory (GB)\(\downarrow\)** & **Latency (ms)\(\downarrow\)** & **PPL\(\downarrow\)** \\ \hline LLaMA LLM & 700M & 2.08 (1.00x) & 1.18 (1.00x) & 12.33 \\
**BitNet b1.58** & 700M & 0.80 (2.60x) & 0.96 (1.23x) & 12.87 \\ \hline LLaMA LLM & 1.3B & 3.34 (1.00x) & 1.62 (1.00x) & 11.25 \\
**BitNet b1.58** & 1.3B & 1.14 (2.93x) & 0.97 (1.67x) & 11.29 \\ \hline LLaMA LLM & 3B & 7.89 (1.00x) & 5.07 (1.00x) & 10.04 \\
**BitNet b1.58** & 3B & **2.22(3.55x)** & **1.87(2.71x)** & **9.91** \\
**BitNet b1.58** & 3.9B & **2.38 (3.32x)** & **2.11 (2.40x)** & **9.62** \\ \hline \hline \end{tabular}
\end{table}
표 1: 비트넷 b1.58 및 LLaMA LLM의 비용뿐만 아니라 복잡성.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**Models** & **Size** & **ARCe** & **ARCc** & **HS** & **BQ** & **OQ** & **PQ** & **WGe** & **Avg.** \\ \hline LLaMA LLM & 700M & 54.7 & 23.0 & 37.0 & 60.0 & 20.2 & 68.9 & 54.8 & 45.5 \\
**BitNet b1.58** & 700M & 51.8 & 21.4 & 35.1 & 58.2 & 20.0 & 68.1 & 55.2 & 44.3 \\ \hline LLaMA LLM & 1.3B & 56.9 & 23.5 & 38.5 & 59.1 & 21.6 & 70.0 & 53.9 & 46.2 \\
**BitNet b1.58** & 1.3B & 54.9 & 24.2 & 37.7 & 56.7 & 19.6 & 68.8 & 55.8 & 45.4 \\ \hline LLaMA LLM & 3B & 62.1 & 25.6 & 43.3 & 61.8 & 24.6 & 72.1 & 58.2 & 49.7 \\
**BitNet b1.58** & 3B & **61.4** & **28.3** & **42.9** & **61.5** & **26.6** & **71.5** & **59.3** & **50.2** \\
**BitNet b1.58** & 3.9B & **64.2** & **28.7** & **44.2** & **63.5** & **24.2** & **73.2** & **60.5** & **51.2** \\ \hline \hline \end{tabular}
\end{table}
표 2: 엔드 태스크에 대한 BitNet b1.58 및 LLaMA LLM의 제로 샷 정확도.

표 2는 엔드 태스크에 대한 제로 샷 정확도의 상세한 결과를 보고한다. 평가를 수행하기 위해 _lm-평가-harness4_에서 파이프라인을 따랐다. 그 결과, 모델 크기가 커질수록 BitNet b1.58과 LLaMA LLM 간의 성능 격차가 줄어드는 것을 확인할 수 있었다. 더 중요한 것은 BitNet b1.58이 3B 크기부터 시작하는 완전 정밀 기준선의 성능과 일치할 수 있다는 것이다. 복잡성의 관찰과 유사하게, 엔드-태스크 결과는 비트넷 b1.58 3.9B가 낮은 메모리 및 레이턴시 비용으로 LLaMA LLM 3B보다 우수하다는 것을 보여준다. 이는 BitNet b1.58이 최신 LLM 모델에 비해 파레토 개선임을 보여준다.

각주 4: [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)

메모리 및 지연 시간 모델 크기를 7B, 13B 및 70B로 추가로 확장하고 비용을 평가했다. 그림 2는 지연 시간과 메모리의 추세를 보여 모델 크기 축척에 따라 속도가 증가한다는 것을 보여준다. 특히 BitNet b1.58 70B는 LLaMA LLM 기준선보다 4.1배 빠르다. 이는 _nn.Linear_ 에 대한 시간 비용이 모델 크기에 따라 증가하기 때문입니다. 메모리 소비는 임베딩이 완전한 정밀도를 유지하고 더 큰 모델의 경우 메모리 비율이 더 작기 때문에 유사한 경향을 따른다. 레이턴시와 메모리 모두 2비트 커널로 측정되었기 때문에 여전히 최적화를 통해 비용을 더 줄일 수 있는 여지가 있다.

에너지 우리는 또한 BitNet b1.58과 LLaMA LLM 모두의 산술 연산 에너지 소비를 추정한다. 우리는 행렬 곱셈이 LLM의 비용에 가장 크게 기여하기 때문에 행렬 곱셈의 계산에 주로 초점을 맞춘다. 그림 3은 에너지 비용의 구성을 보여준다. BitNet b1.58의 대부분은 INT8 덧셈 계산인 반면 LLaMA LLM은 FP16 덧셈과 FP16 곱셈으로 구성된다. [12, 2]의 에너지 모델에 따르면 비트넷 b1.58은 7nm 칩에서 행렬 곱셈을 위한 산술 연산 에너지 소비를 71.4배 절감한다. 512개의 토큰을 가진 모델에 대한 종단 간 에너지 비용을 추가로 보고했다. 우리의 결과는 모델 크기가 축소될수록 비트넷 b1.58이 FP16 LLaMA LLM 기준선에 비해 에너지 소비 측면에서 점점 더 효율적이 된다는 것을 보여준다. 이는 _nn.선형_의 백분율이 모델 크기에 따라 증가하는 반면 다른 구성 요소의 비용은 더 큰 모델의 경우 더 작기 때문입니다.

처리량은 파이프라인 병렬성 [1]을 사용하여 두 개의 80GB A100 카드에서 BitNet b1.58 및 LLaMA LLM과 70B 매개변수의 처리량을 비교하여 LLaMA LLM 70B가 장치에서 실행될 수 있도록 한다. GPU 메모리 제한에 도달할 때까지 배치 크기를 증가시켰으며, 시퀀스 길이는 512이다. 표 3은 BitNet b1.58 70B가 LLaMA LLM 배치 크기의 최대 11배를 지원할 수 있어 처리량이 8.9배 더 높다는 것을 보여준다.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Models** & **Size** & **Max Batch Size** & **Throughput (tokens/s)** \\ \hline LLaMA LLM & 70B & 16 (1.0x) & 333 (1.0x) \\
**BitNet b1.58** & 70B & **176 (11.0x)** & **2977 (8.9x)** \\ \hline \hline \end{tabular}
\end{table}
표 3: 비트넷 b1.58 70B와 LLaMA LLM 70B 사이의 처리량의 비교.

도 2: 모델 크기를 변화시키는 BitNet b1.58의 디코딩 레이턴시(Left) 및 메모리 소모(Right)

[MISSING_PAGE_FAIL:5]

### LLMs에서 Long Sequence의 기본 지원

LLM 시대에 긴 시퀀스를 처리할 수 있는 능력은 중요한 요구 사항이 되었다. 긴 시퀀스 추론의 한 가지 주요 과제는 KV 캐시에서 도입한 메모리 소비이다. BitNet b1.58은 16비트에서 8비트로 활성화를 줄여 동일한 리소스가 주어지면 컨텍스트 길이를 두 배로 늘릴 수 있기 때문에 긴 시퀀스에 대한 네이티브 지원을 향한 중요한 단계를 나타낸다. 이것은 4비트로 더 무손실 압축되거나 1.58비트 LLM에 대해 더 낮게 압축될 수 있으며, 이는 향후 작업으로 남겨둔다.

### Edge 및 Mobile의 LLMs

1.58비트 LLM의 사용은 에지 및 모바일 장치에서 언어 모델의 성능을 크게 향상시킬 수 있는 잠재력을 가지고 있다. 이러한 디바이스들은 종종 그들의 메모리 및 계산 능력에 의해 제한되며, 이는 LLM들의 성능 및 스케일을 제한할 수 있다. 그러나, 1.58-비트 LLM들의 감소된 메모리 및 에너지 소비는 이들이 이들 디바이스들에 배치될 수 있게 하여, 이전에 가능하지 않았던 광범위한 애플리케이션들을 가능하게 한다. 이는 에지 및 모바일 디바이스의 능력을 크게 향상시키고 LLM의 새롭고 흥미진진한 애플리케이션을 가능하게 할 수 있다. 더욱이, 1.58-비트 LLMs는 에지 및 모바일 디바이스에서 사용되는 메인 프로세서인 CPU 디바이스에 보다 친근하다. 이는 BitNet b1.58이 이러한 장치에서 효율적으로 실행되어 성능과 성능을 더욱 향상시킬 수 있음을 의미한다.

### 1비트 LLMs용 새 하드웨어

Groq5와 같은 최근 작업은 LLM을 위한 특정 하드웨어(예: LPU)를 구축할 수 있는 유망한 결과와 큰 잠재력을 보여주었다. 한 단계 더 나아가, 우리는 비트넷[23]에서 가능하게 된 새로운 계산 패러다임을 고려할 때, 1-비트 LLMs에 특별히 최적화된 새로운 하드웨어 및 시스템을 설계하기 위한 액션들을 구상하고 요구한다.

각주 5: [https://groq.com/](https://groq.com/)

## References

* [BZB\({}^{+}\)19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: 자연어의 물리적 상식에 대한 추론입니다. _ CoRR_, abs/1911.11641, 2019.
* [CCKS23] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov and Christopher De Sa. QuIP: 보장된 대규모 언어 모델의 2비트 양자화 _ CoRR_, abs/2307.13304, 2023.
* [CLC\({}^{+}\)19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 불크: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구합니다. _ CoRR_, abs/1905.10044, 2019.
* [Com23] Together Computer. 레드파자마: 대용량 언어 모델을 훈련하기 위한 오픈 데이터 세트, 2023.
* [FAHA23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler 및 Dan Alistarh. OPTQ: 생성 미리 훈련된 변압기에 대한 정확한 양자화. <표상학습에 관한 제11차 국제회의> 2023에서.
* [HCB\({}^{+}\)19] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, Hyouk Jung Lee, Jiquan Ngiam, Quoc V. 르, 우용희, 지펑 첸 Gpipe: 파이프라인 병렬성을 이용한 거대 신경망의 효율적인 훈련. 《신경 정보 처리 시스템의 발전》에서 103-112 페이지, 2019.
* [Hor14] 마크 호로위츠. 1.1 컴퓨팅의 에너지 문제(그리고 우리가 그것에 대해 할 수 있는 것). In _2014 IEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest of Technical Papers, San Francisco, CA, USA, February 9-13, 2014_, pages 10-14, 2014.
* [KLZ\({}^{+}\)23] 우석 권, 주한 리, 시위안 장, 영성, 리안민 정, 코디 하오 유, 조셉 E. 곤잘레스, 하오 장, 및 이온 스토이카. 페이지 주의와 함께 제공되는 대용량 언어 모델을 위한 효율적인 메모리 관리. 2023년 ACM SIGOPS 29th Symposium에서.

* [LTT\({}^{+}\)23] 지린, 지밍탕, 하오톈탕, 상양, 싱규당, 송한. AWQ: LLM 압축 및 가속을 위한 활성화 인식 가중치 양자화. _ CoRR_, abs/2306.00978, 2023.
* [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot 및 Ashish Sabharwal. 갑옷이 전기를 통할 수 있나요? 오픈 북 질문 응답을 위한 새 데이터 세트입니다. _ CoRR_, abs/1809.02789, 2018.
* [MXBS16] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 포인터 센티넬 혼합물 모델, 2016년
* [PKL\({}^{+}\)16] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. LAMBADA 데이터 세트: 광범위한 담화 컨텍스트를 요구하는 단어 예측. "제54차 컴퓨터 언어학 협회 연례 회의, ACL 2016, 2016년 8월 7일-12일, 독일 베를린, 제1권: 장문"에서. 2016년 컴퓨터 언어학 협회
* [RSR\({}^{+}\)19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 통합 텍스트 대 텍스트 변환기를 사용하여 전이 학습의 한계를 탐색합니다. _ CoRR_, abs/1910.10683, 2019.
* [SAL\({}^{+}\)24] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 로포르머: 회전식 위치 임베딩을 가진 향상된 변압기. _ Neurocomputing_, 568:127063, 2024.
* [SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 위노그란데: 규모의 적대적인 위노그라드 스키마 도전. "인공지능에 관한 제34차 AAAI 회의"에서 2020년 8732-8740페이지를 참조하십시오.
* [Sha20] Noam Shazeer. GLU 변형은 변압기를 개선합니다. _ CoRR_, abs/2002.05202, 2020.
* [TBMR] Jonathan Tow, Marco Bellagente, Dakota Mahan 및 Carlos Riquelme. Stablelm 3b 4e1t.
* [TCS\({}^{+}\)24] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Hadamard incoherence와 격자 코드북을 사용한 LLM 양자화가 더 좋습니다. _ CoRR_, abs/2402.04396, 2024.
* [TLI\({}^{+}\)23] Hugo Touvron, Thibaut Lavril, Gautier Izzard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 및 Guillaume Lample. LLaMA: 개방적이고 효율적인 기초 언어 모델입니다. _ CoRR_, abs/2302.13971, 2023.
* [TMS\({}^{+}\)23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, and et al. Llama 2: open foundation and fine-tuned chat models. _ CoRR_, abs/2307.09288, 2023.
* [WLG17] Johannes Welbl, Nelson F. Liu, and Matt Gardner. 선다형 과학 문제를 크라우드소싱합니다. 레온 더친스키, 웨이 쉬, 앨런 리터, 팀 볼드윈에서 편집자들은 _Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, 코펜하겐, 덴마크, 9월 7일, 2017_, 94-106 페이지. Computational Linguistics, 2017.
* [WMC\({}^{+}\)23] Lei Wang, Lingxiao Ma, Shijie Cao, Ningxin Zheng, Quanlu Zhang, Zilong Xue, Ziming Miao, Ting Cao, and Yuqing Yang. 사다리: 맞춤형 데이터 포맷에 대한 효율적인 텐서 컴파일. OSDI, 2023
* [WMD\({}^{+}\)23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 비트넷: 대용량 언어 모델을 위한 1비트 트랜스포머 크기 조정 _ CoRR_, abs/2310.11453, 2023.

* [XLS\({}^{+}\)23] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: 대규모 언어 모델에 대한 정확하고 효율적인 훈련 후 양자화. 국제 기계 학습 회의, ICML 2023, 2023년 7월 23-29일, 호놀룰루, 하와이, 미국_, 2023.
* [YBS19] Vikas Yadav, Steven Bethard, and Mihai Surdeanu. 빠르고 더럽다: 멀티홉 질의응답을 위한 정당화 문장의 감독되지 않은 선택. 이누이 켄타로, 징장, 빈센트 응, 샤오준 완, 편집자, _EMNLP-IJCNLP_, 2019.
* [ZHB\({}^{+}\)19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 헬라 스웨그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? "제57회 컴퓨터 언어학 협회 회의"에서 2019년 4791-4800페이지를 참조하십시오.
* [ZS19] Biao Zhang and Rico Sennrich. 루트 평균 제곱 층 정규화. 인한나 Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems_, pages 12360-12371, 2019.
* [ZZL22] Yichi Zhang, Zhiru Zhang 및 Lukasz Lew. PokeBNN: 가벼운 정확도를 추구하는 이진법입니다. *IEEE/CVF Conference on Computer Vision and Pattern Recognition_에서, 페이지 12465-12475. IEEE, 2022.

<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.17764] The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</title><meta property="og:description" content="Recent research, such as BitNet&nbsp;[23], is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weig…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.17764">

<!--Generated on Tue Mar  5 13:45:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Era of 1-bit LLMs: 
<br class="ltx_break">All Large Language Models are in 1.58 Bits</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shuming Ma&nbsp;&nbsp;&nbsp;&nbsp;Hongyu Wang<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>&nbsp;&nbsp;&nbsp;&nbsp;Lingxiao Ma&nbsp;&nbsp;&nbsp;&nbsp;Lei Wang&nbsp;&nbsp;&nbsp;&nbsp;Wenhui Wang 
<br class="ltx_break"><span id="id2.2.1" class="ltx_text ltx_font_bold">&nbsp;&nbsp;&nbsp;&nbsp;Shaohan Huang&nbsp;&nbsp;&nbsp;&nbsp;Li Dong&nbsp;&nbsp;&nbsp;&nbsp;Ruiping Wang&nbsp;&nbsp;&nbsp;&nbsp;Jilong Xue&nbsp;&nbsp;&nbsp;&nbsp;Furu Wei<sup id="id2.2.1.1" class="ltx_sup"><span id="id2.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">⋄</span></sup> 
<br class="ltx_break"><a target="_blank" href="https://aka.ms/GeneralAI" title="" class="ltx_ref ltx_href">https://aka.ms/GeneralAI</a>


<br class="ltx_break"></span>
</span><span class="ltx_author_notes">&nbsp;Equal contribution. <math id="id1.1.m1.1" class="ltx_Math" alttext="\diamond" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\diamond</annotation></semantics></math> Corresponding author. S. Ma, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, J. Xue, F. Wei are with Microsoft Research. H. Wang and R. Wang are with University of Chinese Academy of Sciences.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">BitNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite>와 같은 최근 연구는 1비트 대용량 언어 모델(LLM)의 새로운 시대를 여는 길을 열어주고 있다. 본 연구에서는 1비트 LLM 변종, 즉 <span class="ltx_text ltx_font_bold" id="id3.id1.1">BitNet b1.58</span>을 소개하며, 여기서 LLM의 모든 단일 파라미터(또는 가중치)는 삼진 {-1, 0, 1}이다. 이는 전체 정밀도(즉, FP16 또는 BF16) 트랜스포머 LLM과 동일한 모델 크기 및 훈련 토큰을 갖는 반면, 지연 시간, 메모리, 처리량 및 에너지 소비 측면에서 훨씬 더 비용 효율적이다. 보다 심층적으로, 1.58비트 LLM은 고성능 및 비용 효율적인 새로운 세대의 LLM을 훈련하기 위한 새로운 스케일링 법칙 및 레시피를 정의한다. 또한, 새로운 계산 패러다임을 가능하게 하고, 1-비트 LLMs에 최적화된 특정 하드웨어를 설계할 수 있는 문을 열어준다.</p>
</div>
<figure id="S0.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S0.F1.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2402.17764/assets/x1.png" id="S0.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="183" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S0.F1.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2402.17764/assets/x2.png" id="S0.F1.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="211" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.6.1.1" style="font-size:90%;">그림 1</span>:</span><span class="ltx_text" id="S0.F1.7.2" style="font-size:90%;">1-bit LLMs(예: <span class="ltx_text" id="S0.F1.7.2.1">BitNet b1.58</span>)는 모델 성능을 유지하면서 LLMs의 추론 비용(latency, throughput, energy)을 줄이기 위해 Pareto 솔루션을 제공한다. <span class="ltx_text" id="S0.F1.7.2.2">BitNet b1.58</span>의 새 계산 패러다임은 1비트 LLM에 최적화된 새 하드웨어를 설계하기 위한 작업을 호출합니다. </span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>The Era of 1-bit LLMs</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">최근 AI 분야는 LLM(Large Language Models)의 규모와 역량이 급성장하고 있다. 이러한 모델은 광범위한 자연어 처리 작업에서 놀라운 성능을 보여주었지만 크기가 증가함에 따라 배치에 어려움이 있고 높은 에너지 소비로 인한 환경 및 경제적 영향에 대한 우려가 제기되었다. 이러한 과제를 해결하기 위한 한 가지 접근법은 추론 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>, <a class="ltx_ref" href="#bib.bib5" title="">5</a>, <a class="ltx_ref" href="#bib.bib2" title="">2</a>, <a class="ltx_ref" href="#bib.bib18" title="">18</a>]</cite>를 위한 저비트 모델을 생성하기 위해 훈련 후 양자화를 사용하는 것이다. 이 기술은 가중치 및 활성화의 정밀도를 감소시켜 LLM의 메모리 및 계산 요구 사항을 상당히 감소시킨다. 그 추세는 4비트 변형 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib5" title="">5</a>, <a class="ltx_ref" href="#bib.bib9" title="">9</a>]</cite>와 같이 16비트에서 하위 비트로 이동하는 것이었다. 그러나 훈련 후 양자화는 산업 LLM에서 널리 사용되지만 차선책이다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">최근 BitNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite>와 같은 1비트 모델 아키텍처에 대한 연구는 LLM의 성능을 유지하면서 비용을 줄일 수 있는 유망한 방향을 제시하고 있다. 바닐라 LLM은 16비트 부동 값(즉, FP16 또는 BF16)에 있으며, 임의의 LLM들의 대부분은 매트릭스 곱셈이다. 따라서, 주요 연산 비용은 부동소수점 덧셈 연산과 곱셈 연산에서 발생한다. 반면에 BitNet의 행렬 곱셈은 정수 덧셈만을 포함하므로 LLM에 대한 에너지 비용 차수를 절약할 수 있다. 많은 칩에서 성능을 계산하는 근본적인 한계가 전력이기 때문에 에너지 절약도 더 빠른 계산으로 변환될 수 있다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">계산 외에도, 모델 파라미터들을 DRAM으로부터 온-칩 가속기(예를 들어, SRAM)의 메모리로 전달하는 프로세스는 추론 동안 비용이 많이 들 수 있다. 스루풋을 향상시키기 위해 SRAM을 확대하려는 시도가 있었지만, 이것은 DRAM보다 상당히 높은 비용을 도입한다. 전체 고정밀 모델에 비해 1비트 LLM은 용량 및 대역폭 관점에서 훨씬 낮은 메모리 풋프린트를 갖는다. 이는 DRAM으로부터 가중치들을 로딩하는 비용 및 시간을 상당히 감소시킬 수 있어, 더 빠르고 효율적인 추론으로 이어진다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">이 작업에서 우리는 <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">BitNet b1.58</span>이라고 하는 중요한 1비트 LLM 변형을 도입하며, 여기서 모든 매개변수는 {-1, 0, 1} 값을 취하는 삼진법이다. 우리는 원래의 1비트 비트넷에 0의 추가 값을 추가하여 이진 시스템에서 1.58비트를 얻었다. <span class="ltx_text" id="S1.p4.1.2">BitNet b1.58</span>은 행렬 곱셈을 위한 곱셈 연산이 거의 필요하지 않고 고도로 최적화될 수 있는 새로운 계산 패러다임을 포함하여 원래 1비트 BitNet의 모든 이점을 유지합니다. 또한, 기존의 1비트 비트넷과 동일한 에너지 소모량을 가지며, FP16 LLM 베이스라인에 비해 메모리 소모량, 처리량, 지연시간 측면에서 훨씬 효율적이다. 또한 <span class="ltx_text" id="S1.p4.1.3">BitNet b1.58</span>은 두 가지 추가적인 이점을 제공한다. 첫째, 특징 필터링에 대한 명시적인 지원으로 인해 모델링 능력이 강화되며, 모델 가중치에 0을 포함시킴으로써 1비트 LLM의 성능을 크게 향상시킬 수 있다. 둘째, <span class="ltx_text" id="S1.p4.1.4">BitNet b1.58</span>은 동일한 구성(예: 모델 크기, 훈련 토큰 등)을 사용할 때 3B 크기에서 시작하여 복잡도와 최종 작업 성능 측면에서 전체 정밀도(예: FP16) 기준선과 일치할 수 있음을 보여준다.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>BitNet b1.58</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text" id="S2.p1.1.1">BitNet b1.58</span>은 BitNet 아키텍처를 기반으로 하며, 이는 <em class="ltx_emph ltx_font_italic" id="S2.p1.1.2">nn.Linear</em>을 <em class="ltx_emph ltx_font_italic" id="S2.p1.1.3">BitLinear</em>으로 대체하는 Transformer이다. 1.58비트 무게와 8비트 활성화로 처음부터 훈련됩니다. 원래 BitNet과 비교하여 아래에 요약한 몇 가지 수정 사항을 소개합니다.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Quantization Function.</h3>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">가중치를 -1, 0 또는 +1로 제한하기 위해 <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.1">absmean</em> 양자화 함수를 채택합니다. 먼저 가중치 행렬을 평균 절대값으로 스케일링한 다음 각 값을 {-1, 0, +1} 중 가장 가까운 정수로 라운딩한다:</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.3" class="ltx_Math" alttext="\widetilde{W}=\mathrm{RoundClip}(\frac{W}{\gamma+\epsilon},-1,1)," display="block"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><mover accent="true" id="S2.E1.m1.3.3.1.1.3" xref="S2.E1.m1.3.3.1.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.3.2" xref="S2.E1.m1.3.3.1.1.3.2.cmml">W</mi><mo id="S2.E1.m1.3.3.1.1.3.1" xref="S2.E1.m1.3.3.1.1.3.1.cmml">~</mo></mover><mo id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.3.cmml">RoundClip</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">(</mo><mfrac id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mi id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">W</mi><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml">γ</mi><mo id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">+</mo><mi id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml">ϵ</mi></mrow></mfrac><mo id="S2.E1.m1.3.3.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">,</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.3.3.1.1.1.1.1.1a" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml">−</mo><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml">1</mn></mrow><mo id="S2.E1.m1.3.3.1.1.1.1.1.4" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">,</mo><mn id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.5" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.3.3.1.2" xref="S2.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1"><eq id="S2.E1.m1.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2"></eq><apply id="S2.E1.m1.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3"><ci id="S2.E1.m1.3.3.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1">~</ci><ci id="S2.E1.m1.3.3.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2">𝑊</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.2"></times><ci id="S2.E1.m1.3.3.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3">RoundClip</ci><vector id="S2.E1.m1.3.3.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><divide id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1"></divide><ci id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">𝑊</ci><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><plus id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"></plus><ci id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2">𝛾</ci><ci id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3">italic-ϵ</ci></apply></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1"><minus id="S2.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1"></minus><cn type="integer" id="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2">1</cn></apply><cn type="integer" id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">1</cn></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\widetilde{W}=\mathrm{RoundClip}(\frac{W}{\gamma+\epsilon},-1,1),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.9" class="ltx_Math" alttext="\mathrm{RoundClip}(x,a,b)=\max(a,\min(b,\mathrm{round}(x)))," display="block"><semantics id="S2.E2.m1.9a"><mrow id="S2.E2.m1.9.9.1" xref="S2.E2.m1.9.9.1.1.cmml"><mrow id="S2.E2.m1.9.9.1.1" xref="S2.E2.m1.9.9.1.1.cmml"><mrow id="S2.E2.m1.9.9.1.1.3" xref="S2.E2.m1.9.9.1.1.3.cmml"><mi id="S2.E2.m1.9.9.1.1.3.2" xref="S2.E2.m1.9.9.1.1.3.2.cmml">RoundClip</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.9.9.1.1.3.1" xref="S2.E2.m1.9.9.1.1.3.1.cmml">​</mo><mrow id="S2.E2.m1.9.9.1.1.3.3.2" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml"><mo stretchy="false" id="S2.E2.m1.9.9.1.1.3.3.2.1" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">x</mi><mo id="S2.E2.m1.9.9.1.1.3.3.2.2" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">a</mi><mo id="S2.E2.m1.9.9.1.1.3.3.2.3" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">,</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">b</mi><mo stretchy="false" id="S2.E2.m1.9.9.1.1.3.3.2.4" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.9.9.1.1.2" xref="S2.E2.m1.9.9.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.9.9.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.2.cmml"><mi id="S2.E2.m1.7.7" xref="S2.E2.m1.7.7.cmml">max</mi><mo id="S2.E2.m1.9.9.1.1.1.1a" xref="S2.E2.m1.9.9.1.1.1.2.cmml">⁡</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.9.9.1.1.1.1.1.2" xref="S2.E2.m1.9.9.1.1.1.2.cmml">(</mo><mi id="S2.E2.m1.8.8" xref="S2.E2.m1.8.8.cmml">a</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.3" xref="S2.E2.m1.9.9.1.1.1.2.cmml">,</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.5.5" xref="S2.E2.m1.5.5.cmml">min</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1a" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E2.m1.6.6" xref="S2.E2.m1.6.6.cmml">b</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">,</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2.cmml">round</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.3.2.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml">x</mi><mo stretchy="false" id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.3.2.2" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.4" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.9.9.1.1.1.1.1.4" xref="S2.E2.m1.9.9.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.9.9.1.2" xref="S2.E2.m1.9.9.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.9b"><apply id="S2.E2.m1.9.9.1.1.cmml" xref="S2.E2.m1.9.9.1"><eq id="S2.E2.m1.9.9.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.2"></eq><apply id="S2.E2.m1.9.9.1.1.3.cmml" xref="S2.E2.m1.9.9.1.1.3"><times id="S2.E2.m1.9.9.1.1.3.1.cmml" xref="S2.E2.m1.9.9.1.1.3.1"></times><ci id="S2.E2.m1.9.9.1.1.3.2.cmml" xref="S2.E2.m1.9.9.1.1.3.2">RoundClip</ci><vector id="S2.E2.m1.9.9.1.1.3.3.1.cmml" xref="S2.E2.m1.9.9.1.1.3.3.2"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑥</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝑎</ci><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">𝑏</ci></vector></apply><apply id="S2.E2.m1.9.9.1.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.1.1"><max id="S2.E2.m1.7.7.cmml" xref="S2.E2.m1.7.7"></max><ci id="S2.E2.m1.8.8.cmml" xref="S2.E2.m1.8.8">𝑎</ci><apply id="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1"><min id="S2.E2.m1.5.5.cmml" xref="S2.E2.m1.5.5"></min><ci id="S2.E2.m1.6.6.cmml" xref="S2.E2.m1.6.6">𝑏</ci><apply id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1"><times id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1"></times><ci id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2">round</ci><ci id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.9c">\mathrm{RoundClip}(x,a,b)=\max(a,\min(b,\mathrm{round}(x))),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.1" class="ltx_Math" alttext="\gamma=\frac{1}{nm}\sum_{ij}|W_{ij}|." display="block"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.3.cmml">γ</mi><mo id="S2.E3.m1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml"><mfrac id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.3.cmml"><mn id="S2.E3.m1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.3.2.cmml">1</mn><mrow id="S2.E3.m1.1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.1.3.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.3.3.2" xref="S2.E3.m1.1.1.1.1.1.3.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.1.1.3.3.1" xref="S2.E3.m1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S2.E3.m1.1.1.1.1.1.3.3.3" xref="S2.E3.m1.1.1.1.1.1.3.3.3.cmml">m</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><munder id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S2.E3.m1.1.1.1.1.1.1.2.2" xref="S2.E3.m1.1.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2.3.2" xref="S2.E3.m1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.1.1.1.2.3.1" xref="S2.E3.m1.1.1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S2.E3.m1.1.1.1.1.1.1.2.3.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3.cmml">j</mi></mrow></munder><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo><msub id="S2.E3.m1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml">W</mi><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><eq id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.2"></eq><ci id="S2.E3.m1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.3">𝛾</ci><apply id="S2.E3.m1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><times id="S2.E3.m1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3"><divide id="S2.E3.m1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3"></divide><cn type="integer" id="S2.E3.m1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.3.2">1</cn><apply id="S2.E3.m1.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3"><times id="S2.E3.m1.1.1.1.1.1.3.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3.1"></times><ci id="S2.E3.m1.1.1.1.1.1.3.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3.2">𝑛</ci><ci id="S2.E3.m1.1.1.1.1.1.3.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3.3">𝑚</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1"><apply id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2"></sum><apply id="S2.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3"><times id="S2.E3.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S2.E3.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.2">𝑖</ci><ci id="S2.E3.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3">𝑗</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1"><abs id="S2.E3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2"></abs><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.2">𝑊</ci><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3"><times id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\gamma=\frac{1}{nm}\sum_{ij}|W_{ij}|.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p3.2">활성화에 대한 양자화 함수는 비선형 함수 이전의 활성화를 <math alttext="[0,Q_{b}]" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p3.1.m1.2"><semantics id="S2.SS0.SSS0.Px1.p3.1.m1.2a"><mrow id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml"><mo id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.2" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml">[</mo><mn id="S2.SS0.SSS0.Px1.p3.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p3.1.m1.1.1.cmml">0</mn><mo id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.3" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml">,</mo><msub id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2.cmml">Q</mi><mi id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3.cmml">b</mi></msub><mo id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.4" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p3.1.m1.2b"><interval closure="closed" id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1"><cn id="S2.SS0.SSS0.Px1.p3.1.m1.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p3.1.m1.1.1">0</cn><apply id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2">𝑄</ci><ci id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3">𝑏</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p3.1.m1.2c">[0,Q_{b}]</annotation></semantics></math> 범위로 확장하지 않는다는 점을 제외하고는 BitNet에서 동일한 구현을 따른다. 대신에, 활성화들은 모두 제로-포인트 양자화를 제거하기 위해 토큰당 <math alttext="[-Q_{b},Q_{b}]" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p3.2.m2.2"><semantics id="S2.SS0.SSS0.Px1.p3.2.m2.2a"><mrow id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml"><mo id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.3" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml">[</mo><mrow id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.cmml"><mo id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1a" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.cmml">−</mo><msub id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.cmml"><mi id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2.cmml">Q</mi><mi id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3.cmml">b</mi></msub></mrow><mo id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.4" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml">,</mo><msub id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.cmml"><mi id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2.cmml">Q</mi><mi id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3.cmml">b</mi></msub><mo id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.5" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p3.2.m2.2b"><interval closure="closed" id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2"><apply id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1"><minus id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1"></minus><apply id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2">𝑄</ci><ci id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3">𝑏</ci></apply></apply><apply id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2">𝑄</ci><ci id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3">𝑏</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p3.2.m2.2c">[-Q_{b},Q_{b}]</annotation></semantics></math>로 스케일링된다. 이것은 구현 및 시스템 수준 최적화 모두에 대해 더 편리하고 간단하지만, 실험에서 성능에 무시할 수 있는 영향을 도입한다.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">LLaMA-alike Components.</h3>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">LLaMA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib19" title="">19</a>, <a class="ltx_ref" href="#bib.bib20" title="">20</a>]</cite>의 아키텍처는 오픈 소스 LLMs에 대한 de-facto 백본이었다. 오픈 소스 커뮤니티를 수용하기 위해 <span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.1">BitNet b1.58</span>은 LLaMA 유사 구성 요소를 채택합니다. 구체적으로는 RMSNorm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite>, SwiGLU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib16" title="">16</a>]</cite>, 회전 임베딩 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib14" title="">14</a>]</cite>를 사용하고, 모든 편향을 제거한다. 이러한 방식으로, <span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.2">BitNet b1.58</span>은 최소한의 노력으로 인기 있는 오픈 소스 소프트웨어(예: Huggingface, vLLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite>, llama.cpp<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ggerganov/llama.cpp" target="_blank" title="">https://github.com/ggerganov/llama.cpp</a></span></span></span>)에 통합될 수 있다.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S2.T1.3.3" class="ltx_tr">
<td id="S2.T1.3.3.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.3.4.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S2.T1.3.3.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.3.5.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.1.1.1.1" class="ltx_text ltx_font_bold">Memory (GB)<math id="S2.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
<td id="S2.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.2.2.2.1" class="ltx_text ltx_font_bold">Latency (ms)<math id="S2.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.2.2.2.1.m1.1a"><mo stretchy="false" id="S2.T1.2.2.2.1.m1.1.1" xref="S2.T1.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.1.m1.1b"><ci id="S2.T1.2.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
<td id="S2.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.3.3.1" class="ltx_text ltx_font_bold">PPL<math id="S2.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.3.3.3.1.m1.1a"><mo stretchy="false" id="S2.T1.3.3.3.1.m1.1.1" xref="S2.T1.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.1.m1.1b"><ci id="S2.T1.3.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
<tr id="S2.T1.3.4" class="ltx_tr">
<td id="S2.T1.3.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.4.1.1" class="ltx_text">LLaMA LLM</span></td>
<td id="S2.T1.3.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<td id="S2.T1.3.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">2.08 (1.00x)</td>
<td id="S2.T1.3.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">1.18 (1.00x)</td>
<td id="S2.T1.3.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">12.33</td>
</tr>
<tr id="S2.T1.3.5" class="ltx_tr">
<td id="S2.T1.3.5.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.5.1.1" class="ltx_text ltx_font_bold">BitNet b1.58</span></td>
<td id="S2.T1.3.5.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<td id="S2.T1.3.5.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">0.80 (2.60x)</td>
<td id="S2.T1.3.5.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">0.96 (1.23x)</td>
<td id="S2.T1.3.5.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">12.87</td>
</tr>
<tr id="S2.T1.3.6" class="ltx_tr">
<td id="S2.T1.3.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.6.1.1" class="ltx_text">LLaMA LLM</span></td>
<td id="S2.T1.3.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">1.3B</td>
<td id="S2.T1.3.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">3.34 (1.00x)</td>
<td id="S2.T1.3.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">1.62 (1.00x)</td>
<td id="S2.T1.3.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">11.25</td>
</tr>
<tr id="S2.T1.3.7" class="ltx_tr">
<td id="S2.T1.3.7.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.7.1.1" class="ltx_text ltx_font_bold">BitNet b1.58</span></td>
<td id="S2.T1.3.7.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">1.3B</td>
<td id="S2.T1.3.7.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">1.14 (2.93x)</td>
<td id="S2.T1.3.7.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">0.97 (1.67x)</td>
<td id="S2.T1.3.7.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">11.29</td>
</tr>
<tr id="S2.T1.3.8" class="ltx_tr">
<td id="S2.T1.3.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.8.1.1" class="ltx_text">LLaMA LLM</span></td>
<td id="S2.T1.3.8.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">3B</td>
<td id="S2.T1.3.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">7.89 (1.00x)</td>
<td id="S2.T1.3.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">5.07 (1.00x)</td>
<td id="S2.T1.3.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">10.04</td>
</tr>
<tr id="S2.T1.3.9" class="ltx_tr">
<td id="S2.T1.3.9.1" class="ltx_td ltx_align_left" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.9.1.1" class="ltx_text ltx_font_bold">BitNet b1.58</span></td>
<td id="S2.T1.3.9.2" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;">3B</td>
<td id="S2.T1.3.9.3" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.9.3.1" class="ltx_text ltx_font_bold">2.22 (3.55x)</span></td>
<td id="S2.T1.3.9.4" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.9.4.1" class="ltx_text ltx_font_bold">1.87 (2.71x)</span></td>
<td id="S2.T1.3.9.5" class="ltx_td ltx_align_center" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.9.5.1" class="ltx_text ltx_font_bold">9.91</span></td>
</tr>
<tr id="S2.T1.3.10" class="ltx_tr">
<td id="S2.T1.3.10.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.10.1.1" class="ltx_text ltx_font_bold">BitNet b1.58</span></td>
<td id="S2.T1.3.10.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">3.9B</td>
<td id="S2.T1.3.10.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.10.3.1" class="ltx_text ltx_font_bold">2.38 (3.32x)</span></td>
<td id="S2.T1.3.10.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.10.4.1" class="ltx_text ltx_font_bold">2.11 (2.40x)</span></td>
<td id="S2.T1.3.10.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S2.T1.3.10.5.1" class="ltx_text ltx_font_bold">9.62</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.7.1.1" style="font-size:90%;">Table 1</span>:</span><span class="ltx_text" id="S2.T1.8.2" style="font-size:90%;">Perplexity뿐만 아니라 비용도 <span class="ltx_text" id="S2.T1.8.2.1">BitNet b1.58</span> and <span class="ltx_text" id="S2.T1.8.2.2">LLaMA LLM</span>. </span></figcaption>
</figure>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S2.T2.2.1" class="ltx_tr">
<td id="S2.T2.2.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S2.T2.2.1.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S2.T2.2.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.2.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S2.T2.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.2.1.3.1" class="ltx_text ltx_font_bold">ARCe</span></td>
<td id="S2.T2.2.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.2.1.4.1" class="ltx_text ltx_font_bold">ARCc</span></td>
<td id="S2.T2.2.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.2.1.5.1" class="ltx_text ltx_font_bold">HS</span></td>
<td id="S2.T2.2.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.2.1.6.1" class="ltx_text ltx_font_bold">BQ</span></td>
<td id="S2.T2.2.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.2.1.7.1" class="ltx_text ltx_font_bold">OQ</span></td>
<td id="S2.T2.2.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.2.1.8.1" class="ltx_text ltx_font_bold">PQ</span></td>
<td id="S2.T2.2.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.2.1.9.1" class="ltx_text ltx_font_bold">WGe</span></td>
<td id="S2.T2.2.1.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T2.2.1.10.1" class="ltx_text ltx_font_bold">Avg.</span></td>
</tr>
<tr id="S2.T2.2.2" class="ltx_tr">
<td id="S2.T2.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T2.2.2.1.1" class="ltx_text">LLaMA LLM</span></td>
<td id="S2.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">700M</td>
<td id="S2.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">54.7</td>
<td id="S2.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">23.0</td>
<td id="S2.T2.2.2.5" class="ltx_td ltx_align_center ltx_border_t">37.0</td>
<td id="S2.T2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">60.0</td>
<td id="S2.T2.2.2.7" class="ltx_td ltx_align_center ltx_border_t">20.2</td>
<td id="S2.T2.2.2.8" class="ltx_td ltx_align_center ltx_border_t">68.9</td>
<td id="S2.T2.2.2.9" class="ltx_td ltx_align_center ltx_border_t">54.8</td>
<td id="S2.T2.2.2.10" class="ltx_td ltx_align_center ltx_border_t">45.5</td>
</tr>
<tr id="S2.T2.2.3" class="ltx_tr">
<td id="S2.T2.2.3.1" class="ltx_td ltx_align_left"><span id="S2.T2.2.3.1.1" class="ltx_text ltx_font_bold">BitNet b1.58</span></td>
<td id="S2.T2.2.3.2" class="ltx_td ltx_align_center">700M</td>
<td id="S2.T2.2.3.3" class="ltx_td ltx_align_center">51.8</td>
<td id="S2.T2.2.3.4" class="ltx_td ltx_align_center">21.4</td>
<td id="S2.T2.2.3.5" class="ltx_td ltx_align_center">35.1</td>
<td id="S2.T2.2.3.6" class="ltx_td ltx_align_center">58.2</td>
<td id="S2.T2.2.3.7" class="ltx_td ltx_align_center">20.0</td>
<td id="S2.T2.2.3.8" class="ltx_td ltx_align_center">68.1</td>
<td id="S2.T2.2.3.9" class="ltx_td ltx_align_center">55.2</td>
<td id="S2.T2.2.3.10" class="ltx_td ltx_align_center">44.3</td>
</tr>
<tr id="S2.T2.2.4" class="ltx_tr">
<td id="S2.T2.2.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T2.2.4.1.1" class="ltx_text">LLaMA LLM</span></td>
<td id="S2.T2.2.4.2" class="ltx_td ltx_align_center ltx_border_t">1.3B</td>
<td id="S2.T2.2.4.3" class="ltx_td ltx_align_center ltx_border_t">56.9</td>
<td id="S2.T2.2.4.4" class="ltx_td ltx_align_center ltx_border_t">23.5</td>
<td id="S2.T2.2.4.5" class="ltx_td ltx_align_center ltx_border_t">38.5</td>
<td id="S2.T2.2.4.6" class="ltx_td ltx_align_center ltx_border_t">59.1</td>
<td id="S2.T2.2.4.7" class="ltx_td ltx_align_center ltx_border_t">21.6</td>
<td id="S2.T2.2.4.8" class="ltx_td ltx_align_center ltx_border_t">70.0</td>
<td id="S2.T2.2.4.9" class="ltx_td ltx_align_center ltx_border_t">53.9</td>
<td id="S2.T2.2.4.10" class="ltx_td ltx_align_center ltx_border_t">46.2</td>
</tr>
<tr id="S2.T2.2.5" class="ltx_tr">
<td id="S2.T2.2.5.1" class="ltx_td ltx_align_left"><span id="S2.T2.2.5.1.1" class="ltx_text ltx_font_bold">BitNet b1.58</span></td>
<td id="S2.T2.2.5.2" class="ltx_td ltx_align_center">1.3B</td>
<td id="S2.T2.2.5.3" class="ltx_td ltx_align_center">54.9</td>
<td id="S2.T2.2.5.4" class="ltx_td ltx_align_center">24.2</td>
<td id="S2.T2.2.5.5" class="ltx_td ltx_align_center">37.7</td>
<td id="S2.T2.2.5.6" class="ltx_td ltx_align_center">56.7</td>
<td id="S2.T2.2.5.7" class="ltx_td ltx_align_center">19.6</td>
<td id="S2.T2.2.5.8" class="ltx_td ltx_align_center">68.8</td>
<td id="S2.T2.2.5.9" class="ltx_td ltx_align_center">55.8</td>
<td id="S2.T2.2.5.10" class="ltx_td ltx_align_center">45.4</td>
</tr>
<tr id="S2.T2.2.6" class="ltx_tr">
<td id="S2.T2.2.6.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T2.2.6.1.1" class="ltx_text">LLaMA LLM</span></td>
<td id="S2.T2.2.6.2" class="ltx_td ltx_align_center ltx_border_t">3B</td>
<td id="S2.T2.2.6.3" class="ltx_td ltx_align_center ltx_border_t">62.1</td>
<td id="S2.T2.2.6.4" class="ltx_td ltx_align_center ltx_border_t">25.6</td>
<td id="S2.T2.2.6.5" class="ltx_td ltx_align_center ltx_border_t">43.3</td>
<td id="S2.T2.2.6.6" class="ltx_td ltx_align_center ltx_border_t">61.8</td>
<td id="S2.T2.2.6.7" class="ltx_td ltx_align_center ltx_border_t">24.6</td>
<td id="S2.T2.2.6.8" class="ltx_td ltx_align_center ltx_border_t">72.1</td>
<td id="S2.T2.2.6.9" class="ltx_td ltx_align_center ltx_border_t">58.2</td>
<td id="S2.T2.2.6.10" class="ltx_td ltx_align_center ltx_border_t">49.7</td>
</tr>
<tr id="S2.T2.2.7" class="ltx_tr">
<td id="S2.T2.2.7.1" class="ltx_td ltx_align_left"><span id="S2.T2.2.7.1.1" class="ltx_text ltx_font_bold">BitNet b1.58</span></td>
<td id="S2.T2.2.7.2" class="ltx_td ltx_align_center">3B</td>
<td id="S2.T2.2.7.3" class="ltx_td ltx_align_center"><span id="S2.T2.2.7.3.1" class="ltx_text ltx_font_bold">61.4</span></td>
<td id="S2.T2.2.7.4" class="ltx_td ltx_align_center"><span id="S2.T2.2.7.4.1" class="ltx_text ltx_font_bold">28.3</span></td>
<td id="S2.T2.2.7.5" class="ltx_td ltx_align_center"><span id="S2.T2.2.7.5.1" class="ltx_text ltx_font_bold">42.9</span></td>
<td id="S2.T2.2.7.6" class="ltx_td ltx_align_center"><span id="S2.T2.2.7.6.1" class="ltx_text ltx_font_bold">61.5</span></td>
<td id="S2.T2.2.7.7" class="ltx_td ltx_align_center"><span id="S2.T2.2.7.7.1" class="ltx_text ltx_font_bold">26.6</span></td>
<td id="S2.T2.2.7.8" class="ltx_td ltx_align_center"><span id="S2.T2.2.7.8.1" class="ltx_text ltx_font_bold">71.5</span></td>
<td id="S2.T2.2.7.9" class="ltx_td ltx_align_center"><span id="S2.T2.2.7.9.1" class="ltx_text ltx_font_bold">59.3</span></td>
<td id="S2.T2.2.7.10" class="ltx_td ltx_align_center"><span id="S2.T2.2.7.10.1" class="ltx_text ltx_font_bold">50.2</span></td>
</tr>
<tr id="S2.T2.2.8" class="ltx_tr">
<td id="S2.T2.2.8.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S2.T2.2.8.1.1" class="ltx_text ltx_font_bold">BitNet b1.58</span></td>
<td id="S2.T2.2.8.2" class="ltx_td ltx_align_center ltx_border_bb">3.9B</td>
<td id="S2.T2.2.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T2.2.8.3.1" class="ltx_text ltx_font_bold">64.2</span></td>
<td id="S2.T2.2.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T2.2.8.4.1" class="ltx_text ltx_font_bold">28.7</span></td>
<td id="S2.T2.2.8.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T2.2.8.5.1" class="ltx_text ltx_font_bold">44.2</span></td>
<td id="S2.T2.2.8.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T2.2.8.6.1" class="ltx_text ltx_font_bold">63.5</span></td>
<td id="S2.T2.2.8.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T2.2.8.7.1" class="ltx_text ltx_font_bold">24.2</span></td>
<td id="S2.T2.2.8.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T2.2.8.8.1" class="ltx_text ltx_font_bold">73.2</span></td>
<td id="S2.T2.2.8.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T2.2.8.9.1" class="ltx_text ltx_font_bold">60.5</span></td>
<td id="S2.T2.2.8.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T2.2.8.10.1" class="ltx_text ltx_font_bold">51.2</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.5.1.1" style="font-size:90%;">Table 2</span>:</span><span class="ltx_text" id="S2.T2.6.2" style="font-size:90%;">Zero-shot accuracy of <span class="ltx_text" id="S2.T2.6.2.1">BitNet b1.58</span> and <span class="ltx_text" id="S2.T2.6.2.2">LLaMA LLM</span> on the end tasks. </span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">우리는 <span class="ltx_text" id="S3.p1.1.1">BitNet b1.58</span>을 다양한 크기로 재생된 FP16 <span class="ltx_text" id="S3.p1.1.2">LLaMA LLM</span>과 비교했다. 공정한 비교를 보장하기 위해 1,000억 토큰에 대해 RedPajama 데이터 세트 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib4" title="">4</a>]</cite>에서 모델을 사전 훈련했다. ARC-Easy<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib25" title="">25</a>]</cite>, ARC-Challenge<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib25" title="">25</a>]</cite>, Hellaswag<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite>, Winogrande<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>, PIQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>, OpenbookQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib10" title="">10</a>]</cite>, BoolQ<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib3" title="">3</a>]</cite> 등의 다양한 언어 태스크에 대해 제로샷 성능을 평가하였다. 또한 WikiText2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>]</cite> 및 C4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib13" title="">13</a>]</cite> 데이터 세트에 대한 유효성 검사 복잡도를 보고했다.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text" id="S3.p2.1.1">LLaMA LLM</span>과 <span class="ltx_text" id="S3.p2.1.2">BitNet b1.58</span>의 런타임 GPU 메모리와 대기 시간을 비교했다. 결과는 GPU 장치에서 LLM 추론 지연에 잘 최적화된 FasterTransformer<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA/FasterTransformer" target="_blank" title="">https://github.com/NVIDIA/FasterTransformer</a></span></span></span> codebase를 사용하여 측정되었다. Ladder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib22" title="">22</a>]</cite>의 2비트 커널은 또한 <span class="ltx_text" id="S3.p2.1.3">BitNet b1.58</span>에 대해 통합됩니다. 추론을 위한 주요 비용이기 때문에 출력 토큰당 시간을 보고했다.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p" id="S3.p3.1">Table <a class="ltx_ref" href="#S2.T1" title="Table 1 ‣ LLaMA-alike Components. ‣ 2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">1</span></a>는 <span class="ltx_text" id="S3.p3.1.1">BitNet b1.58</span> 및 <span class="ltx_text" id="S3.p3.1.2">LLaMA LLM</span>에 대한 perplexity 및 cost를 요약한 것이다. <span class="ltx_text" id="S3.p3.1.3">BitNet b1.58</span>이 perplexity 측면에서 3B 모델 크기에서 full precision <span class="ltx_text" id="S3.p3.1.4">LLaMA LLM</span>과 일치하기 시작하는 반면 GPU 메모리는 2.71배 더 빠르고 3.55배 더 적게 사용한다. 특히, 3.9B 모델 크기를 갖는 <span class="ltx_text" id="S3.p3.1.5">BitNet b1.58</span>은 2.4배 빠르고, 3.32배 적은 메모리를 소비하지만, <span class="ltx_text" id="S3.p3.1.6">LLaMA LLM</span> 3B보다 상당히 우수한 성능을 보인다.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p" id="S3.p4.1">Table <a class="ltx_ref" href="#S2.T2" title="Table 2 ‣ LLaMA-alike Components. ‣ 2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">2</span></a>는 종료 태스크에 대한 제로 샷 정확도의 상세 결과를 보고한다. 평가를 수행하기 위해 <em class="ltx_emph ltx_font_italic" id="S3.p4.1.1">lm-evaluation-harness</em><span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank" title="">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span>의 파이프라인을 따랐다. 결과는 모델 크기가 증가함에 따라 <span class="ltx_text" id="S3.p4.1.2">BitNet b1.58</span>과 <span class="ltx_text" id="S3.p4.1.3">LLaMA LLM</span> 사이의 성능 격차가 좁혀짐을 보여준다. 더 중요한 것은 <span class="ltx_text" id="S3.p4.1.4">BitNet b1.58</span>은 3B 크기에서 시작하는 전체 정밀 기준선의 성능과 일치할 수 있다는 것입니다. 퍼플렉시티의 관찰과 유사하게, 엔드-태스크 결과는 더 낮은 메모리 및 레이턴시 비용을 갖는 <span class="ltx_text" id="S3.p4.1.5">BitNet b1.58</span> 3.9B가 <span class="ltx_text" id="S3.p4.1.6">LLaMA LLM</span> 3B를 능가함을 드러낸다. 이는 <span class="ltx_text" id="S3.p4.1.7">BitNet b1.58</span>이 최신 LLM 모델에 비해 Pareto 개선임을 보여줍니다.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Memory and Latency</h3>

<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="https://ar5iv.labs.arxiv.org/html/2402.17764/assets/x3.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="226" height="169" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="https://ar5iv.labs.arxiv.org/html/2402.17764/assets/x4.png" id="S3.F2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="226" height="169" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.3.1.1" style="font-size:90%;">그림 2</span>:</span><span class="ltx_text" id="S3.F2.4.2" style="font-size:90%;">Decoding latency (Left) and memory consumption (Right) of <span class="ltx_text" id="S3.F2.4.2.1">BitNet b1.58</span> varying the model size. </span></figcaption>
</figure>
<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">우리는 모델 크기를 7B, 13B 및 70B로 추가로 확장하고 비용을 평가했다. 그림 <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ Memory and Latency ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">2</span></a>는 지연 시간과 메모리의 추세를 보여 모델 크기 축척에 따라 속도가 증가한다는 것을 보여준다. 특히 <span class="ltx_text" id="S3.SS0.SSS0.Px1.p1.1.1">BitNet b1.58</span> 70B는 <span class="ltx_text" id="S3.SS0.SSS0.Px1.p1.1.2">LLaMA LLM</span> baseline보다 4.1배 빠르다. <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.1.3">nn.Linear</em>이 모델 크기에 따라 커지기 때문입니다. 메모리 소비는 임베딩이 완전한 정밀도를 유지하고 더 큰 모델의 경우 메모리 비율이 더 작기 때문에 유사한 경향을 따른다. 레이턴시와 메모리 모두 2비트 커널로 측정되었기 때문에 여전히 최적화를 통해 비용을 더 줄일 수 있는 여지가 있다.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Energy</h3>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">또한 <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.1">BitNet b1.58</span>과 <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.2">LLaMA LLM</span>의 산술 연산 에너지 소비를 추정한다. 우리는 행렬 곱셈이 LLM의 비용에 가장 크게 기여하기 때문에 행렬 곱셈의 계산에 주로 초점을 맞춘다. <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ Energy ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">3</span></a>는 에너지 비용의 구성을 예시한다. <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.3">BitNet b1.58</span>의 대부분은 INT8 덧셈 계산인 반면, <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.4">LLaMA LLM</span>은 FP16 덧셈과 FP16 곱셈으로 구성된다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib7" title="">7</a>, <a class="ltx_ref" href="#bib.bib28" title="">28</a>]</cite>의 에너지 모델에 따르면, <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.5">BitNet b1.58</span>은 7nm 칩에서 행렬 곱셈을 위한 산술 연산 에너지 소비를 71.4배 절감한다. 512개의 토큰을 가진 모델에 대한 종단 간 에너지 비용을 추가로 보고했다. 결과는 모델 크기 척도로서 <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.6">BitNet b1.58</span>이 FP16 <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.7">LLaMA LLM</span> 기준선에 비해 에너지 소비 측면에서 점점 더 효율적이 됨을 보여준다. 이는 <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.1.8">nn.Linear</em>의 비율이 모델 크기에 따라 증가하는 반면 다른 구성 요소로부터의 비용은 더 큰 모델의 경우 더 작기 때문이다.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="https://ar5iv.labs.arxiv.org/html/2402.17764/assets/x5.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="226" height="169" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="https://ar5iv.labs.arxiv.org/html/2402.17764/assets/x6.png" id="S3.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="226" height="169" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.4.1.1" style="font-size:90%;">Figure 3</span>:</span><span class="ltx_text" id="S3.F3.5.2" style="font-size:90%;">Energy consumption of <span class="ltx_text" id="S3.F3.5.2.1">BitNet b1.58</span> compared to <span class="ltx_text" id="S3.F3.5.2.2">LLaMA LLM</span> at 7nm process nodes. 왼쪽에는 사칙 연산 에너지의 구성 요소가 있습니다. 오른쪽에는 다양한 모델 크기에 따른 엔드 투 엔드 에너지 비용이 있습니다. </span></figcaption>
</figure>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Throughput</h3>

<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S3.T3.2.1" class="ltx_tr">
<td id="S3.T3.2.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T3.2.1.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S3.T3.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T3.2.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S3.T3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T3.2.1.3.1" class="ltx_text ltx_font_bold">Max Batch Size</span></td>
<td id="S3.T3.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T3.2.1.4.1" class="ltx_text ltx_font_bold">Throughput (tokens/s)</span></td>
</tr>
<tr id="S3.T3.2.2" class="ltx_tr">
<td id="S3.T3.2.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T3.2.2.1.1" class="ltx_text">LLaMA LLM</span></td>
<td id="S3.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">70B</td>
<td id="S3.T3.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">16 (1.0x)</td>
<td id="S3.T3.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.0pt;padding-right:10.0pt;">&nbsp;&nbsp;333 (1.0x)</td>
</tr>
<tr id="S3.T3.2.3" class="ltx_tr">
<td id="S3.T3.2.3.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T3.2.3.1.1" class="ltx_text ltx_font_bold">BitNet b1.58</span></td>
<td id="S3.T3.2.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;">70B</td>
<td id="S3.T3.2.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T3.2.3.3.1" class="ltx_text ltx_font_bold">176 (11.0x)</span></td>
<td id="S3.T3.2.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.0pt;padding-right:10.0pt;"><span id="S3.T3.2.3.4.1" class="ltx_text ltx_font_bold">2977 (8.9x)</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.5.1.1" style="font-size:90%;">Table 3</span>:</span><span class="ltx_text" id="S3.T3.6.2" style="font-size:90%;">Comparison of the throughput between <span class="ltx_text" id="S3.T3.6.2.1">BitNet b1.58</span> 70B and <span class="ltx_text" id="S3.T3.6.2.2">LLaMA LLM</span> 70B. </span></figcaption>
</figure>
<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1"><span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.1">BitNet b1.58</span> 및 <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.2">LLaMA LLM</span>의 처리량을 파이프라인 병렬성을 사용하여 두 80GB A100 카드에서 70B 매개 변수와 비교하여 <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.3">LLaMA LLM</span> 70B를 장치에서 실행할 수 있습니다. 512의 시퀀스 길이로 GPU 메모리 제한에 도달할 때까지 배치 크기를 증가시켰습니다. 표 <a class="ltx_ref" href="#S3.T3" title="Table 3 ‣ Throughput ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">3</span></a>는 <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.4">BitNet b1.58</span> 70B가 <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.5">LLaMA LLM</span>의 배치 크기보다 최대 11배 더 높은 처리량을 지원할 수 있음을 보여줍니다.</p>
</div>
<div id="S3.SS0.SSS0.Px3.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px3.p2.1.1">BitNet b1.58</span><span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px3.p2.1.2"> is enabling a new scaling law with respect to model performance and inference cost</span>. 참고로 그림 <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ Memory and Latency ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">2</span></a>와 <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ Energy ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">3</span></a>의 결과를 바탕으로 1.58-bit와 16-bit에서 서로 다른 모델 크기 간에 다음과 같은 동등성을 가질 수 있다.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i1.p1.1">13B BitNet b1.58은 3B FP16 LLM보다 지연 시간, 메모리 사용량 및 에너지 소비 측면에서 더 효율적이다.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i2.p1.1">30B BitNet b1.58은 7B FP16 LLM보다 지연 시간, 메모리 사용량 및 에너지 소비 측면에서 더 효율적이다.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i3.p1.1">70B BitNet b1.58은 13B FP16 LLM보다 지연 시간, 메모리 사용량 및 에너지 소비 측면에서 더 효율적이다.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS0.SSS0.Px4" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Training with 2T Tokens</h3>

<div id="S3.SS0.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS0.SSS0.Px4.p1.1">훈련 토큰의 수는 LLM에 중요한 요소이다. <span class="ltx_text" id="S3.SS0.SSS0.Px4.p1.1.1">BitNet b1.58</span>의 확장성을 토큰 측면에서 테스트하기 위해 최신 오픈 소스 3B 모델인 StableLM-3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite>의 데이터 레시피에 따라 2T 토큰이 있는 <span class="ltx_text" id="S3.SS0.SSS0.Px4.p1.1.2">BitNet b1.58</span> 모델을 훈련했다. 두 모델 모두 Winogrande<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>, PIQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>, SciQ<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>, LAMBADA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib12" title="">12</a>]</cite>, ARC-easy<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib25" title="">25</a>]</cite>로 구성된 벤치마크에서 평가되었다. 우리는 <a class="ltx_ref" href="#S3.T4" title="Table 4 ‣ Training with 2T Tokens ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">4</span></a> 표에서 제로 샷 정확도를 보고했다. 정확도와 정규화된 정확도로 측정된 작업에 대해 우리는 둘의 평균을 취한다. 2T 토큰에서 StableLM 3b의 결과는 기술 보고서에서 직접 가져옵니다. 연구 결과는 <span class="ltx_text" id="S3.SS0.SSS0.Px4.p1.1.3">BitNet b1.58</span>이 모든 엔드 태스크에서 우수한 성능을 달성하여 1.58비트 LLM도 강력한 일반화 기능을 가지고 있음을 보여준다.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<div id="S3.T4.2" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:54.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(2.6pt,-0.3pt) scale(1.01194702513024,1.01194702513024) ;">
<table id="S3.T4.2.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T4.2.1.1" class="ltx_tr">
<td id="S3.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T4.2.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S3.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T4.2.1.1.2.1" class="ltx_text ltx_font_bold">Tokens</span></td>
<td id="S3.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T4.2.1.1.3.1" class="ltx_text ltx_font_bold">Winogrande</span></td>
<td id="S3.T4.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T4.2.1.1.4.1" class="ltx_text ltx_font_bold">PIQA</span></td>
<td id="S3.T4.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T4.2.1.1.5.1" class="ltx_text ltx_font_bold">SciQ</span></td>
<td id="S3.T4.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T4.2.1.1.6.1" class="ltx_text ltx_font_bold">LAMBADA</span></td>
<td id="S3.T4.2.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T4.2.1.1.7.1" class="ltx_text ltx_font_bold">ARC-easy</span></td>
<td id="S3.T4.2.1.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T4.2.1.1.8.1" class="ltx_text ltx_font_bold">Avg.</span></td>
</tr>
<tr id="S3.T4.2.1.2" class="ltx_tr">
<td id="S3.T4.2.1.2.1" class="ltx_td ltx_align_left ltx_border_t">StableLM-3B</td>
<td id="S3.T4.2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">2T</td>
<td id="S3.T4.2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">64.56</td>
<td id="S3.T4.2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">76.93</td>
<td id="S3.T4.2.1.2.5" class="ltx_td ltx_align_center ltx_border_t">90.75</td>
<td id="S3.T4.2.1.2.6" class="ltx_td ltx_align_center ltx_border_t">66.09</td>
<td id="S3.T4.2.1.2.7" class="ltx_td ltx_align_center ltx_border_t">67.78</td>
<td id="S3.T4.2.1.2.8" class="ltx_td ltx_align_center ltx_border_t">73.22</td>
</tr>
<tr id="S3.T4.2.1.3" class="ltx_tr">
<td id="S3.T4.2.1.3.1" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T4.2.1.3.1.1" class="ltx_text ltx_font_bold">BitNet b1.58</span> 3B</td>
<td id="S3.T4.2.1.3.2" class="ltx_td ltx_align_center ltx_border_bb">2T</td>
<td id="S3.T4.2.1.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.2.1.3.3.1" class="ltx_text ltx_font_bold">66.37</span></td>
<td id="S3.T4.2.1.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.2.1.3.4.1" class="ltx_text ltx_font_bold">78.40</span></td>
<td id="S3.T4.2.1.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.2.1.3.5.1" class="ltx_text ltx_font_bold">91.20</span></td>
<td id="S3.T4.2.1.3.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.2.1.3.6.1" class="ltx_text ltx_font_bold">67.63</span></td>
<td id="S3.T4.2.1.3.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.2.1.3.7.1" class="ltx_text ltx_font_bold">68.12</span></td>
<td id="S3.T4.2.1.3.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.2.1.3.8.1" class="ltx_text ltx_font_bold">74.34</span></td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.4.1.1" style="font-size:90%;">Table 4</span>:</span><span class="ltx_text" id="S3.T4.5.2" style="font-size:90%;">Comparison of <span class="ltx_text" id="S3.T4.5.2.1">BitNet b1.58</span> with StableLM-3B with 2T tokens. </span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion and Future Work</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">1-bit Mixture-of-Experts (MoE) LLMs</span></p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p" id="S4.p2.1">혼합 전문가(Mixture-of-Experts, MoE)는 LLM에 대한 비용 효율적인 접근법임이 입증되었다. 이는 연산량(FLOPs)을 크게 감소시키지만, 높은 메모리 소모와 칩간 통신 오버헤드는 그 배치와 응용에 제한을 준다. 이러한 도전은 1.58비트 LLM에 의해 해결될 수 있다. 첫째, 메모리 공간을 줄이면 MoE 모델을 배포하는 데 필요한 장치의 수가 줄어듭니다. 또한, 네트워크를 통해 활성화를 전송하는 오버헤드를 크게 줄입니다. 결국 전체 모델을 단일 칩에 배치할 수 있다면 오버헤드가 없을 것이다.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Native Support of Long Sequence in LLMs</span></p>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p" id="S4.p4.1">LLM 시대에 긴 시퀀스를 처리할 수 있는 능력은 중요한 요구 사항이 되었다. 긴 시퀀스 추론의 한 가지 주요 과제는 KV 캐시에서 도입한 메모리 소비이다. <span class="ltx_text" id="S4.p4.1.1">BitNet b1.58</span>은 긴 시퀀스에 대한 네이티브 지원을 향한 중요한 단계를 나타내며, 이는 16비트에서 8비트로 활성화를 감소시켜 동일한 리소스가 주어졌을 때 컨텍스트 길이가 두 배가 되도록 하기 때문이다. 이것은 4비트로 더 무손실 압축되거나 1.58비트 LLM에 대해 더 낮게 압축될 수 있으며, 이는 향후 작업으로 남겨둔다.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.p5.1.1">LLMs on Edge and Mobile</span></p>
</div>
<div id="S4.p6" class="ltx_para">
<p class="ltx_p" id="S4.p6.1">1.58비트 LLM의 사용은 에지 및 모바일 장치에서 언어 모델의 성능을 크게 향상시킬 수 있는 잠재력을 가지고 있다. 이러한 디바이스들은 종종 그들의 메모리 및 계산 능력에 의해 제한되며, 이는 LLM들의 성능 및 스케일을 제한할 수 있다. 그러나, 1.58-비트 LLM들의 감소된 메모리 및 에너지 소비는 이들이 이들 디바이스들에 배치될 수 있게 하여, 이전에 가능하지 않았던 광범위한 애플리케이션들을 가능하게 한다. 이는 에지 및 모바일 디바이스의 능력을 크게 향상시키고 LLM의 새롭고 흥미진진한 애플리케이션을 가능하게 할 수 있다. 더욱이, 1.58-비트 LLMs는 에지 및 모바일 디바이스에서 사용되는 메인 프로세서인 CPU 디바이스에 보다 친근하다. 이는 <span class="ltx_text" id="S4.p6.1.1">BitNet b1.58</span>이 이러한 장치에서 효율적으로 실행되어 성능과 성능이 더욱 향상될 수 있음을 의미한다.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p class="ltx_p" id="S4.p7.1"><span class="ltx_text ltx_font_bold" id="S4.p7.1.1">New Hardware for 1-bit LLMs</span></p>
</div>
<div id="S4.p8" class="ltx_para">
<p class="ltx_p" id="S4.p8.1">Groq<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://groq.com/" target="_blank" title="">https://groq.com/</a></span></span></span>과 같은 최근 연구는 LLMs에 대한 특정 하드웨어(예: LPU)를 구축할 수 있는 유망한 결과와 큰 잠재력을 보여주었다. 한 단계 더 나아가, 우리는 BitNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite>에서 가능하게 된 새로운 계산 패러다임을 감안할 때 1비트 LLMs에 특별히 최적화된 새로운 하드웨어 및 시스템을 설계하기 위한 액션을 구상하고 촉구한다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BZB<sup id="bib.bib1.4.4.1" class="ltx_sup"><span id="bib.bib1.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [19]</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan&nbsp;Le Bras, Jianfeng Gao, and Yejin Choi.

</span>
<span class="ltx_bibblock">PIQA: reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.5.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1911.11641, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CCKS [23]</span>
<span class="ltx_bibblock">
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher&nbsp;De Sa.

</span>
<span class="ltx_bibblock">QuIP: 2-bit quantization of large language models with guarantees.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2307.13304, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CLC<sup id="bib.bib3.4.4.1" class="ltx_sup"><span id="bib.bib3.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [19]</span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no questions.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.5.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1905.10044, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Com [23]</span>
<span class="ltx_bibblock">
Together Computer.

</span>
<span class="ltx_bibblock">Redpajama: an open dataset for training large language models, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FAHA [23]</span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">OPTQ: accurate quantization for generative pre-trained transformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">The Eleventh International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HCB<sup id="bib.bib6.4.4.1" class="ltx_sup"><span id="bib.bib6.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [19]</span>
<span class="ltx_bibblock">
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia&nbsp;Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc&nbsp;V. Le, Yonghui Wu, and Zhifeng Chen.

</span>
<span class="ltx_bibblock">Gpipe: Efficient training of giant neural networks using pipeline parallelism.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.5.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages 103–112, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hor [14]</span>
<span class="ltx_bibblock">
Mark Horowitz.

</span>
<span class="ltx_bibblock">1.1 computing’s energy problem (and what we can do about it).

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">2014 IEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest of Technical Papers, San Francisco, CA, USA, February 9-13, 2014</span>, pages 10–14, 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KLZ<sup id="bib.bib8.4.4.1" class="ltx_sup"><span id="bib.bib8.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [23]</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.5.1" class="ltx_text ltx_font_italic">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</span>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LTT<sup id="bib.bib9.4.4.1" class="ltx_sup"><span id="bib.bib9.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [23]</span>
<span class="ltx_bibblock">
Ji&nbsp;Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.

</span>
<span class="ltx_bibblock">AWQ: activation-aware weight quantization for LLM compression and acceleration.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.5.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2306.00978, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MCKS [18]</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? A new dataset for open book question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1809.02789, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MXBS [16]</span>
<span class="ltx_bibblock">
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.

</span>
<span class="ltx_bibblock">Pointer sentinel mixture models, 2016.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PKL<sup id="bib.bib12.4.4.1" class="ltx_sup"><span id="bib.bib12.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [16]</span>
<span class="ltx_bibblock">
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan&nbsp;Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández.

</span>
<span class="ltx_bibblock">The LAMBADA dataset: Word prediction requiring a broad discourse context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.5.1" class="ltx_text ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers</span>. The Association for Computer Linguistics, 2016.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RSR<sup id="bib.bib13.4.4.1" class="ltx_sup"><span id="bib.bib13.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [19]</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.5.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1910.10683, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SAL<sup id="bib.bib14.4.4.1" class="ltx_sup"><span id="bib.bib14.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [24]</span>
<span class="ltx_bibblock">
Jianlin Su, Murtadha H.&nbsp;M. Ahmed, Yu&nbsp;Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.5.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 568:127063, 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SBBC [20]</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">WinoGrande: an adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">The Thirty-Fourth AAAI Conference on Artificial Intelligence</span>, pages 8732–8740, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sha [20]</span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">GLU variants improve transformer.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2002.05202, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme.

</span>
<span class="ltx_bibblock">Stablelm 3b 4e1t.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TCS<sup id="bib.bib18.4.4.1" class="ltx_sup"><span id="bib.bib18.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [24]</span>
<span class="ltx_bibblock">
Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher&nbsp;De Sa.

</span>
<span class="ltx_bibblock">Quip#: Even better LLM quantization with hadamard incoherence and lattice codebooks.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.5.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2402.04396, 2024.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TLI<sup id="bib.bib19.4.4.1" class="ltx_sup"><span id="bib.bib19.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [23]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">LLaMA: open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.5.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2302.13971, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TMS<sup id="bib.bib20.4.4.1" class="ltx_sup"><span id="bib.bib20.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [23]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian&nbsp;Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, and et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.5.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2307.09288, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WLG [17]</span>
<span class="ltx_bibblock">
Johannes Welbl, Nelson&nbsp;F. Liu, and Matt Gardner.

</span>
<span class="ltx_bibblock">Crowdsourcing multiple choice science questions.

</span>
<span class="ltx_bibblock">In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017</span>, pages 94–106. Association for Computational Linguistics, 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WMC<sup id="bib.bib22.4.4.1" class="ltx_sup"><span id="bib.bib22.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [23]</span>
<span class="ltx_bibblock">
Lei Wang, Lingxiao Ma, Shijie Cao, Ningxin Zheng, Quanlu Zhang, Jilong Xue, Ziming Miao, Ting Cao, , and Yuqing Yang.

</span>
<span class="ltx_bibblock">Ladder: Efficient tensor compilation on customized data format.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.5.1" class="ltx_text ltx_font_italic">OSDI</span>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WMD<sup id="bib.bib23.4.4.1" class="ltx_sup"><span id="bib.bib23.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [23]</span>
<span class="ltx_bibblock">
Hongyu Wang, Shuming Ma, Li&nbsp;Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi&nbsp;Wu, and Furu Wei.

</span>
<span class="ltx_bibblock">Bitnet: Scaling 1-bit transformers for large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.5.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2310.11453, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">XLS<sup id="bib.bib24.4.4.1" class="ltx_sup"><span id="bib.bib24.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [23]</span>
<span class="ltx_bibblock">
Guangxuan Xiao, Ji&nbsp;Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han.

</span>
<span class="ltx_bibblock">SmoothQuant: accurate and efficient post-training quantization for large language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.5.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</span>, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">YBS [19]</span>
<span class="ltx_bibblock">
Vikas Yadav, Steven Bethard, and Mihai Surdeanu.

</span>
<span class="ltx_bibblock">Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering.

</span>
<span class="ltx_bibblock">In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">EMNLP-IJCNLP</span>, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ZHB<sup id="bib.bib26.4.4.1" class="ltx_sup"><span id="bib.bib26.4.4.1.1" class="ltx_text ltx_font_italic">+</span></sup> [19]</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">HellaSwag: can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.5.1" class="ltx_text ltx_font_italic">Proceedings of the 57th Conference of the Association for Computational Linguistics</span>, pages 4791–4800, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ZS [19]</span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock">In Hanna&nbsp;M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily&nbsp;B. Fox, and Roman Garnett, editors, <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages 12360–12371, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ZZL [22]</span>
<span class="ltx_bibblock">
Yichi Zhang, Zhiru Zhang, and Lukasz Lew.

</span>
<span class="ltx_bibblock">PokeBNN: A binary pursuit of lightweight accuracy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 12465–12475. IEEE, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2402.17762" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2402.17764" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2402.17764">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.17764" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2402.17765" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 13:45:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
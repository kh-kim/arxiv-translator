{
    "2402.17764v1": {
        "paper_id": "2402.17764v1",
        "abs_url": "https://arxiv.org/abs/2402.17764v1",
        "pdf_url": "https://arxiv.org/pdf/2402.17764v1.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2402.17764v1_The_Era_of_1-bit_LLMs_All_Large_Language_Models_are_in_158_Bits.pdf",
        "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Shuming Ma",
            "Hongyu Wang",
            "Lingxiao Ma",
            "Lei Wang",
            "Wenhui Wang",
            "Shaohan Huang",
            "Li Dong",
            "Ruiping Wang",
            "Jilong Xue",
            "Furu Wei"
        ],
        "abstract": "Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.",
        "comments": "Work in progress",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/the-era-of-1-bit-llms-all-large-language",
        "bibtex": "@misc{ma2024era,\n      title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, \n      author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},\n      year={2024},\n      eprint={2402.17764},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}
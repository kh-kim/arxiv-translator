<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</title>
<!--Generated on Tue Feb 27 18:44:28 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.17764v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2402.17764v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2402.17764v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2402.17764v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S1" title="1 The Era of 1-bit LLMs ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>The Era of 1-bit LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S2" title="2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>BitNet b1.58</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S2.SS0.SSS0.Px1" title="Quantization Function. ‣ 2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">Quantization Function.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S2.SS0.SSS0.Px2" title="LLaMA-alike Components. ‣ 2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">LLaMA-alike Components.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3" title="3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.SS0.SSS0.Px1" title="Memory and Latency ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">Memory and Latency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.SS0.SSS0.Px2" title="Energy ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">Energy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.SS0.SSS0.Px3" title="Throughput ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">Throughput</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.SS0.SSS0.Px4" title="Training with 2T Tokens ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">Training with 2T Tokens</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S4" title="4 Discussion and Future Work ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion and Future Work</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: arydshln</li>
<li>failed: scalerel</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2402.17764v1 [cs.CL] 27 Feb 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Era of 1-bit LLMs: 
<br class="ltx_break">All Large Language Models are in 1.58 Bits</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shuming Ma&nbsp;&nbsp;&nbsp;&nbsp;Hongyu Wang<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>&nbsp;&nbsp;&nbsp;&nbsp;Lingxiao Ma&nbsp;&nbsp;&nbsp;&nbsp;Lei Wang&nbsp;&nbsp;&nbsp;&nbsp;Wenhui Wang 
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id2.2.1">&nbsp;&nbsp;&nbsp;&nbsp;Shaohan Huang&nbsp;&nbsp;&nbsp;&nbsp;Li Dong&nbsp;&nbsp;&nbsp;&nbsp;Ruiping Wang&nbsp;&nbsp;&nbsp;&nbsp;Jilong Xue&nbsp;&nbsp;&nbsp;&nbsp;Furu Wei<math alttext="{}^{\diamond}" class="ltx_Math" display="inline" id="id2.2.1.m1.1"><semantics id="id2.2.1.m1.1a"><msup id="id2.2.1.m1.1.1" xref="id2.2.1.m1.1.1.cmml"><mi id="id2.2.1.m1.1.1a" xref="id2.2.1.m1.1.1.cmml"></mi><mo id="id2.2.1.m1.1.1.1" mathvariant="normal" xref="id2.2.1.m1.1.1.1.cmml">⋄</mo></msup><annotation-xml encoding="MathML-Content" id="id2.2.1.m1.1b"><apply id="id2.2.1.m1.1.1.cmml" xref="id2.2.1.m1.1.1"><ci id="id2.2.1.m1.1.1.1.cmml" xref="id2.2.1.m1.1.1.1">normal-⋄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.1.m1.1c">{}^{\diamond}</annotation><annotation encoding="application/x-llamapun" id="id2.2.1.m1.1d">start_FLOATSUPERSCRIPT ⋄ end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"><a class="ltx_ref ltx_href" href="https://aka.ms/GeneralAI" title="">https://aka.ms/GeneralAI</a>
<br class="ltx_break"></span>
</span><span class="ltx_author_notes">&nbsp;Equal contribution. <math alttext="\diamond" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">⋄</annotation></semantics></math> Corresponding author. S. Ma, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, J. Xue, F. Wei are with Microsoft Research. H. Wang and R. Wang are with University of Chinese Academy of Sciences.</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id3.id1">Recent research, such as BitNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib23" title="">23</a>]</cite>, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely <span class="ltx_text ltx_font_bold" id="id3.id1.1">BitNet b1.58</span>, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption.
More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S0.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S0.F1.1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="330" id="S0.F1.1.g1" src="x1.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S0.F1.2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S0.F1.2.g1" src="x2.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.6.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S0.F1.7.2" style="font-size:90%;">1-bit LLMs (e.g., <span class="ltx_text" id="S0.F1.7.2.1">BitNet b1.58</span>) provide a Pareto solution to reduce inference cost (latency, throughput, and energy) of LLMs while maintaining model performance. The new computation paradigm of <span class="ltx_text" id="S0.F1.7.2.2">BitNet b1.58</span> calls for actions to design new hardware optimized for 1-bit LLMs.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>The Era of 1-bit LLMs</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, the field of AI has seen a rapid growth in the size and capabilities of Large Language Models (LLMs). These models have demonstrated remarkable performance in a wide range of natural language processing tasks, but their increasing size has posed challenges for deployment and raised concerns about their environmental and economic impact due to high energy consumption.
One approach to address these challenges is to use post-training quantization to create low-bit models for inference&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib18" title="">18</a>]</cite>. This technique reduces the precision of weights and activations, significantly reducing the memory and computational requirements of LLMs. The trend has been to move from 16 bits to lower bits, such as 4-bit variants&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib9" title="">9</a>]</cite>. However, post-training quantization is sub-optimal, even though it is widely used in industry LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recent work on 1-bit model architectures, such as BitNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib23" title="">23</a>]</cite>, presents a promising direction for reducing the cost of LLMs while maintaining their performance. Vanilla LLMs are in 16-bit floating values (i.e., FP16 or BF16), and the bulk of any LLMs is matrix multiplication. Therefore, the major computation cost comes from the floating-point addition and multiplication operations. In contrast, the matrix multiplication of BitNet only involves integer addition, which saves orders of energy cost for LLMs. As the fundamental limit to compute performance in many chips is power, the energy savings can also be translated into faster computation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In addition to computation, the process of transferring model parameters from DRAM to the memory of an on-chip accelerator (e.g., SRAM) can be expensive during inference. There have been attempts to enlarge SRAM to improve throughput, but this introduces significantly higher costs than DRAM. Compared to full-precision models, 1-bit LLMs have a much lower memory footprint from both a capacity and bandwidth standpoint. This can significantly reduce the cost and time of loading weights from DRAM, leading to faster and more efficient inference.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we introduce a significant 1-bit LLM variant called <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">BitNet b1.58</span>, where every parameter is ternary, taking on values of {-1, 0, 1}. We have added an additional value of 0 to the original 1-bit BitNet, resulting in 1.58 bits in the binary system. <span class="ltx_text" id="S1.p4.1.2">BitNet b1.58</span> retains all the benefits of the original 1-bit BitNet, including its new computation paradigm, which requires almost no multiplication operations for matrix multiplication and can be highly optimized. Additionally, it has the same energy consumption as the original 1-bit BitNet and is much more efficient in terms of memory consumption, throughput and latency compared to FP16 LLM baselines. Furthermore, <span class="ltx_text" id="S1.p4.1.3">BitNet b1.58</span> offers two additional advantages. Firstly, its modeling capability is stronger due to its explicit support for feature filtering, made possible by the inclusion of 0 in the model weights, which can significantly improve the performance of 1-bit LLMs. Secondly, our experiments show that <span class="ltx_text" id="S1.p4.1.4">BitNet b1.58</span> can match full precision (i.e., FP16) baselines in terms of both perplexity and end-task performance, starting from a 3B size, when using the same configuration (e.g., model size, training tokens, etc.).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>BitNet b1.58</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text" id="S2.p1.1.1">BitNet b1.58</span> is based on the BitNet architecture, which is a Transformer that replaces <em class="ltx_emph ltx_font_italic" id="S2.p1.1.2">nn.Linear</em> with <em class="ltx_emph ltx_font_italic" id="S2.p1.1.3">BitLinear</em>. It is trained from scratch, with 1.58-bit weights and 8-bit activations. Compared to the original BitNet, it introduces some modifications that we summarize below.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Quantization Function.</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">To constrain the weights to -1, 0, or +1, we adopt an <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.1">absmean</em> quantization function. It first scales the weight matrix by its average absolute value, and then round each value to the nearest integer among {-1, 0, +1}:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\widetilde{W}=\mathrm{RoundClip}(\frac{W}{\gamma+\epsilon},-1,1)," class="ltx_Math" display="block" id="S2.E1.m1.3"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><mover accent="true" id="S2.E1.m1.3.3.1.1.3" xref="S2.E1.m1.3.3.1.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.3.2" xref="S2.E1.m1.3.3.1.1.3.2.cmml">W</mi><mo id="S2.E1.m1.3.3.1.1.3.1" xref="S2.E1.m1.3.3.1.1.3.1.cmml">~</mo></mover><mo id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.3.cmml">RoundClip</mi><mo id="S2.E1.m1.3.3.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml"><mo id="S2.E1.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">(</mo><mfrac id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mi id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">W</mi><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml">γ</mi><mo id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">+</mo><mi id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml">ϵ</mi></mrow></mfrac><mo id="S2.E1.m1.3.3.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">,</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.3.3.1.1.1.1.1.1a" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml">−</mo><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml">1</mn></mrow><mo id="S2.E1.m1.3.3.1.1.1.1.1.4" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">,</mo><mn id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">1</mn><mo id="S2.E1.m1.3.3.1.1.1.1.1.5" stretchy="false" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.3.3.1.2" xref="S2.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1"><eq id="S2.E1.m1.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2"></eq><apply id="S2.E1.m1.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3"><ci id="S2.E1.m1.3.3.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1">~</ci><ci id="S2.E1.m1.3.3.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2">𝑊</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.2"></times><ci id="S2.E1.m1.3.3.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3">RoundClip</ci><vector id="S2.E1.m1.3.3.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><divide id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1"></divide><ci id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">𝑊</ci><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><plus id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"></plus><ci id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2">𝛾</ci><ci id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3">italic-ϵ</ci></apply></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1"><minus id="S2.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1"></minus><cn id="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml" type="integer" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2">1</cn></apply><cn id="S2.E1.m1.2.2.cmml" type="integer" xref="S2.E1.m1.2.2">1</cn></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\widetilde{W}=\mathrm{RoundClip}(\frac{W}{\gamma+\epsilon},-1,1),</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">over~ start_ARG italic_W end_ARG = roman_RoundClip ( divide start_ARG italic_W end_ARG start_ARG italic_γ + italic_ϵ end_ARG , - 1 , 1 ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{RoundClip}(x,a,b)=\max(a,\min(b,\mathrm{round}(x)))," class="ltx_Math" display="block" id="S2.E2.m1.9"><semantics id="S2.E2.m1.9a"><mrow id="S2.E2.m1.9.9.1" xref="S2.E2.m1.9.9.1.1.cmml"><mrow id="S2.E2.m1.9.9.1.1" xref="S2.E2.m1.9.9.1.1.cmml"><mrow id="S2.E2.m1.9.9.1.1.3" xref="S2.E2.m1.9.9.1.1.3.cmml"><mi id="S2.E2.m1.9.9.1.1.3.2" xref="S2.E2.m1.9.9.1.1.3.2.cmml">RoundClip</mi><mo id="S2.E2.m1.9.9.1.1.3.1" xref="S2.E2.m1.9.9.1.1.3.1.cmml">⁢</mo><mrow id="S2.E2.m1.9.9.1.1.3.3.2" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml"><mo id="S2.E2.m1.9.9.1.1.3.3.2.1" stretchy="false" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">x</mi><mo id="S2.E2.m1.9.9.1.1.3.3.2.2" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">a</mi><mo id="S2.E2.m1.9.9.1.1.3.3.2.3" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">,</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">b</mi><mo id="S2.E2.m1.9.9.1.1.3.3.2.4" stretchy="false" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.9.9.1.1.2" xref="S2.E2.m1.9.9.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.9.9.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.2.cmml"><mi id="S2.E2.m1.7.7" xref="S2.E2.m1.7.7.cmml">max</mi><mo id="S2.E2.m1.9.9.1.1.1.1a" xref="S2.E2.m1.9.9.1.1.1.2.cmml">⁡</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.2.cmml"><mo id="S2.E2.m1.9.9.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.2.cmml">(</mo><mi id="S2.E2.m1.8.8" xref="S2.E2.m1.8.8.cmml">a</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.3" xref="S2.E2.m1.9.9.1.1.1.2.cmml">,</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.5.5" xref="S2.E2.m1.5.5.cmml">min</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1a" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml"><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E2.m1.6.6" xref="S2.E2.m1.6.6.cmml">b</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">,</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2.cmml">round</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml">x</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.4" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.9.9.1.1.1.1.1.4" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.9.9.1.2" xref="S2.E2.m1.9.9.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.9b"><apply id="S2.E2.m1.9.9.1.1.cmml" xref="S2.E2.m1.9.9.1"><eq id="S2.E2.m1.9.9.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.2"></eq><apply id="S2.E2.m1.9.9.1.1.3.cmml" xref="S2.E2.m1.9.9.1.1.3"><times id="S2.E2.m1.9.9.1.1.3.1.cmml" xref="S2.E2.m1.9.9.1.1.3.1"></times><ci id="S2.E2.m1.9.9.1.1.3.2.cmml" xref="S2.E2.m1.9.9.1.1.3.2">RoundClip</ci><vector id="S2.E2.m1.9.9.1.1.3.3.1.cmml" xref="S2.E2.m1.9.9.1.1.3.3.2"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑥</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝑎</ci><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">𝑏</ci></vector></apply><apply id="S2.E2.m1.9.9.1.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.1.1"><max id="S2.E2.m1.7.7.cmml" xref="S2.E2.m1.7.7"></max><ci id="S2.E2.m1.8.8.cmml" xref="S2.E2.m1.8.8">𝑎</ci><apply id="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1"><min id="S2.E2.m1.5.5.cmml" xref="S2.E2.m1.5.5"></min><ci id="S2.E2.m1.6.6.cmml" xref="S2.E2.m1.6.6">𝑏</ci><apply id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1"><times id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1"></times><ci id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2">round</ci><ci id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.9c">\mathrm{RoundClip}(x,a,b)=\max(a,\min(b,\mathrm{round}(x))),</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.9d">roman_RoundClip ( italic_x , italic_a , italic_b ) = roman_max ( italic_a , roman_min ( italic_b , roman_round ( italic_x ) ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\gamma=\frac{1}{nm}\sum_{ij}|W_{ij}|." class="ltx_Math" display="block" id="S2.E3.m1.1"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.3.cmml">γ</mi><mo id="S2.E3.m1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml"><mfrac id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.3.cmml"><mn id="S2.E3.m1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.3.2.cmml">1</mn><mrow id="S2.E3.m1.1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.1.3.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.3.3.2" xref="S2.E3.m1.1.1.1.1.1.3.3.2.cmml">n</mi><mo id="S2.E3.m1.1.1.1.1.1.3.3.1" xref="S2.E3.m1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S2.E3.m1.1.1.1.1.1.3.3.3" xref="S2.E3.m1.1.1.1.1.1.3.3.3.cmml">m</mi></mrow></mfrac><mo id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><munder id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml"><mo id="S2.E3.m1.1.1.1.1.1.1.2.2" movablelimits="false" rspace="0em" xref="S2.E3.m1.1.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2.3.2" xref="S2.E3.m1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S2.E3.m1.1.1.1.1.1.1.2.3.1" xref="S2.E3.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S2.E3.m1.1.1.1.1.1.1.2.3.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3.cmml">j</mi></mrow></munder><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.2.cmml"><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo><msub id="S2.E3.m1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml">W</mi><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><mo id="S2.E3.m1.1.1.1.2" lspace="0em" xref="S2.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><eq id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.2"></eq><ci id="S2.E3.m1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.3">𝛾</ci><apply id="S2.E3.m1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><times id="S2.E3.m1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3"><divide id="S2.E3.m1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3"></divide><cn id="S2.E3.m1.1.1.1.1.1.3.2.cmml" type="integer" xref="S2.E3.m1.1.1.1.1.1.3.2">1</cn><apply id="S2.E3.m1.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3"><times id="S2.E3.m1.1.1.1.1.1.3.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3.1"></times><ci id="S2.E3.m1.1.1.1.1.1.3.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3.2">𝑛</ci><ci id="S2.E3.m1.1.1.1.1.1.3.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3.3">𝑚</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1"><apply id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2"></sum><apply id="S2.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3"><times id="S2.E3.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S2.E3.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.2">𝑖</ci><ci id="S2.E3.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3">𝑗</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1"><abs id="S2.E3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2"></abs><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.2">𝑊</ci><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3"><times id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\gamma=\frac{1}{nm}\sum_{ij}|W_{ij}|.</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.1d">italic_γ = divide start_ARG 1 end_ARG start_ARG italic_n italic_m end_ARG ∑ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT | italic_W start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT | .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p3.2">The quantization function for activations follows the same implementation in BitNet, except that we do not scale the activations before the non-linear functions to the range <math alttext="[0,Q_{b}]" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p3.1.m1.2"><semantics id="S2.SS0.SSS0.Px1.p3.1.m1.2a"><mrow id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml"><mo id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.2" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml">[</mo><mn id="S2.SS0.SSS0.Px1.p3.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p3.1.m1.1.1.cmml">0</mn><mo id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.3" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml">,</mo><msub id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2.cmml">Q</mi><mi id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3.cmml">b</mi></msub><mo id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.4" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p3.1.m1.2b"><interval closure="closed" id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1"><cn id="S2.SS0.SSS0.Px1.p3.1.m1.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p3.1.m1.1.1">0</cn><apply id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2">𝑄</ci><ci id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3">𝑏</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p3.1.m1.2c">[0,Q_{b}]</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p3.1.m1.2d">[ 0 , italic_Q start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ]</annotation></semantics></math>. Instead, the activations are all scaled to <math alttext="[-Q_{b},Q_{b}]" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p3.2.m2.2"><semantics id="S2.SS0.SSS0.Px1.p3.2.m2.2a"><mrow id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml"><mo id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.3" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml">[</mo><mrow id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.cmml"><mo id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1a" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.cmml">−</mo><msub id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.cmml"><mi id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2.cmml">Q</mi><mi id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3.cmml">b</mi></msub></mrow><mo id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.4" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml">,</mo><msub id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.cmml"><mi id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2.cmml">Q</mi><mi id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3.cmml">b</mi></msub><mo id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.5" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p3.2.m2.2b"><interval closure="closed" id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2"><apply id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1"><minus id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1"></minus><apply id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2">𝑄</ci><ci id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3">𝑏</ci></apply></apply><apply id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2">𝑄</ci><ci id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3">𝑏</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p3.2.m2.2c">[-Q_{b},Q_{b}]</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p3.2.m2.2d">[ - italic_Q start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT , italic_Q start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ]</annotation></semantics></math> per token to get rid of the zero-point quantization. This is more convenient and simple for both implementation and system-level optimization, while introduces negligible effects to the performance in our experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">LLaMA-alike Components.</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">The architecture of LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib20" title="">20</a>]</cite> has been the de-facto backbone for open-source LLMs. To embrace the open-source community, our design of <span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.1">BitNet b1.58</span> adopts the LLaMA-alike components. Specifically, it uses RMSNorm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib27" title="">27</a>]</cite>, SwiGLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib16" title="">16</a>]</cite>, rotary embedding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib14" title="">14</a>]</cite>, and removes all biases. In this way, <span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.2">BitNet b1.58</span> can be integrated into the popular open-source software (e.g., Huggingface, vLLM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib8" title="">8</a>]</cite>, and llama.cpp<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ggerganov/llama.cpp" title="">https://github.com/ggerganov/llama.cpp</a></span></span></span>) with minimal efforts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.3">
<tbody><tr class="ltx_tr" id="S2.T1.3.3">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.3.3.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.3.4.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.3.3.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.3.5.1">Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.1.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1">Memory (GB)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S2.T1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">normal-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.2.1">Latency (ms)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.2.2.2.1.m1.1"><semantics id="S2.T1.2.2.2.1.m1.1a"><mo id="S2.T1.2.2.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S2.T1.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.1.m1.1b"><ci id="S2.T1.2.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.2.1.m1.1.1">normal-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.3.3.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.3.3.1">PPL<math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.3.3.3.1.m1.1"><semantics id="S2.T1.3.3.3.1.m1.1a"><mo id="S2.T1.3.3.3.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S2.T1.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.1.m1.1b"><ci id="S2.T1.3.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.3.1.m1.1.1">normal-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.3.4.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text" id="S2.T1.3.4.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.4.2" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.4.3" style="padding-left:10.0pt;padding-right:10.0pt;">2.08 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.4.4" style="padding-left:10.0pt;padding-right:10.0pt;">1.18 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.4.5" style="padding-left:10.0pt;padding-right:10.0pt;">12.33</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.5">
<td class="ltx_td ltx_align_left" id="S2.T1.3.5.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.5.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.5.2" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.5.3" style="padding-left:10.0pt;padding-right:10.0pt;">0.80 (2.60x)</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.5.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.96 (1.23x)</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.5.5" style="padding-left:10.0pt;padding-right:10.0pt;">12.87</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.3.6.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text" id="S2.T1.3.6.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.6.2" style="padding-left:10.0pt;padding-right:10.0pt;">1.3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.6.3" style="padding-left:10.0pt;padding-right:10.0pt;">3.34 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.6.4" style="padding-left:10.0pt;padding-right:10.0pt;">1.62 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.6.5" style="padding-left:10.0pt;padding-right:10.0pt;">11.25</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.7">
<td class="ltx_td ltx_align_left" id="S2.T1.3.7.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.7.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.7.2" style="padding-left:10.0pt;padding-right:10.0pt;">1.3B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.7.3" style="padding-left:10.0pt;padding-right:10.0pt;">1.14 (2.93x)</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.7.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.97 (1.67x)</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.7.5" style="padding-left:10.0pt;padding-right:10.0pt;">11.29</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.3.8.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text" id="S2.T1.3.8.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.8.2" style="padding-left:10.0pt;padding-right:10.0pt;">3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.8.3" style="padding-left:10.0pt;padding-right:10.0pt;">7.89 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.8.4" style="padding-left:10.0pt;padding-right:10.0pt;">5.07 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.8.5" style="padding-left:10.0pt;padding-right:10.0pt;">10.04</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.9">
<td class="ltx_td ltx_align_left" id="S2.T1.3.9.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.9.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.9.2" style="padding-left:10.0pt;padding-right:10.0pt;">3B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.9.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.9.3.1">2.22 (3.55x)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.9.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.9.4.1">1.87 (2.71x)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.9.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.9.5.1">9.91</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.3.10.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.10.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.10.2" style="padding-left:10.0pt;padding-right:10.0pt;">3.9B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.10.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.10.3.1">2.38 (3.32x)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.10.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.10.4.1">2.11 (2.40x)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.10.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.10.5.1">9.62</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.7.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S2.T1.8.2" style="font-size:90%;">Perplexity as well as the cost of <span class="ltx_text" id="S2.T1.8.2.1">BitNet b1.58</span> and <span class="ltx_text" id="S2.T1.8.2.2">LLaMA LLM</span>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T2.2">
<tbody><tr class="ltx_tr" id="S2.T2.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.2.1">Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.3"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.3.1">ARCe</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.4"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.4.1">ARCc</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.5"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.5.1">HS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.6"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.6.1">BQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.7"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.7.1">OQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.8"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.8.1">PQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.9"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.9.1">WGe</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.10"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.10.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.2.2.1"><span class="ltx_text" id="S2.T2.2.2.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.2">700M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.3">54.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.4">23.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.5">37.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.6">60.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.7">20.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.8">68.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.9">54.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.10">45.5</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.3">
<td class="ltx_td ltx_align_left" id="S2.T2.2.3.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.3.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2">700M</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.3">51.8</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.4">21.4</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.5">35.1</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.6">58.2</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.7">20.0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.8">68.1</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.9">55.2</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.10">44.3</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.2.4.1"><span class="ltx_text" id="S2.T2.2.4.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.2">1.3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.3">56.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.4">23.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.5">38.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.6">59.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.7">21.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.8">70.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.9">53.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.10">46.2</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.5">
<td class="ltx_td ltx_align_left" id="S2.T2.2.5.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.5.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.2">1.3B</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.3">54.9</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.4">24.2</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.5">37.7</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.6">56.7</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.7">19.6</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.8">68.8</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.9">55.8</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.10">45.4</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.2.6.1"><span class="ltx_text" id="S2.T2.2.6.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.2">3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.3">62.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.4">25.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.5">43.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.6">61.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.7">24.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.8">72.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.9">58.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.10">49.7</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.7">
<td class="ltx_td ltx_align_left" id="S2.T2.2.7.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.2">3B</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.3"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.3.1">61.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.4"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.4.1">28.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.5"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.5.1">42.9</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.6"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.6.1">61.5</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.7"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.7.1">26.6</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.8"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.8.1">71.5</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.9"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.9.1">59.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.10"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.10.1">50.2</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.2.8.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.2">3.9B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.3"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.3.1">64.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.4"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.4.1">28.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.5"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.5.1">44.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.6"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.6.1">63.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.7"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.7.1">24.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.8"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.8.1">73.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.9"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.9.1">60.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.10"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.10.1">51.2</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.5.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S2.T2.6.2" style="font-size:90%;">Zero-shot accuracy of <span class="ltx_text" id="S2.T2.6.2.1">BitNet b1.58</span> and <span class="ltx_text" id="S2.T2.6.2.2">LLaMA LLM</span> on the end tasks.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We compared <span class="ltx_text" id="S3.p1.1.1">BitNet b1.58</span> to our reproduced FP16 <span class="ltx_text" id="S3.p1.1.2">LLaMA LLM</span> in various sizes. To ensure a fair comparison, we pre-trained the models on the RedPajama dataset&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib4" title="">4</a>]</cite> for 100 billion tokens. We evaluated the zero-shot performance on a range of language tasks, including ARC-Easy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib25" title="">25</a>]</cite>, ARC-Challenge&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib25" title="">25</a>]</cite>, Hellaswag&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib26" title="">26</a>]</cite>, Winogrande&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib15" title="">15</a>]</cite>, PIQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib1" title="">1</a>]</cite>, OpenbookQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib10" title="">10</a>]</cite>, and BoolQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib3" title="">3</a>]</cite>. We also reported the validation perplexity on the WikiText2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib11" title="">11</a>]</cite> and C4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib13" title="">13</a>]</cite> datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We compared the runtime GPU memory and latency of both <span class="ltx_text" id="S3.p2.1.1">LLaMA LLM</span> and <span class="ltx_text" id="S3.p2.1.2">BitNet b1.58</span>. The results were measured using the FasterTransformer<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA/FasterTransformer" title="">https://github.com/NVIDIA/FasterTransformer</a></span></span></span> codebase, which is well-optimized for LLM inference latency on GPU devices. The 2-bit kernel from Ladder&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib22" title="">22</a>]</cite> is also integrated for <span class="ltx_text" id="S3.p2.1.3">BitNet b1.58</span>. We reported the time per output token, as it is the major cost for inference.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S2.T1" title="Table 1 ‣ LLaMA-alike Components. ‣ 2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the perplexity and the cost for <span class="ltx_text" id="S3.p3.1.1">BitNet b1.58</span> and <span class="ltx_text" id="S3.p3.1.2">LLaMA LLM</span>. It shows that <span class="ltx_text" id="S3.p3.1.3">BitNet b1.58</span> starts to match full precision <span class="ltx_text" id="S3.p3.1.4">LLaMA LLM</span> at 3B model size in terms of perplexity, while being 2.71 times faster and using 3.55 times less GPU memory. In particular, <span class="ltx_text" id="S3.p3.1.5">BitNet b1.58</span> with a 3.9B model size is 2.4 times faster, consumes 3.32 times less memory, but performs significantly better than <span class="ltx_text" id="S3.p3.1.6">LLaMA LLM</span> 3B.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S2.T2" title="Table 2 ‣ LLaMA-alike Components. ‣ 2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">2</span></a> reports the detailed results of the zero-shot accuracy on the end tasks. We followed the pipeline from <em class="ltx_emph ltx_font_italic" id="S3.p4.1.1">lm-evaluation-harness</em><span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EleutherAI/lm-evaluation-harness" title="">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span> to perform the evaluation. The results show that the performance gap between <span class="ltx_text" id="S3.p4.1.2">BitNet b1.58</span> and <span class="ltx_text" id="S3.p4.1.3">LLaMA LLM</span> narrows as the model size increases. More importantly, <span class="ltx_text" id="S3.p4.1.4">BitNet b1.58</span> can match the performance of the full precision baseline starting from a 3B size. Similar to the observation of the perplexity, the end-task results reveal that <span class="ltx_text" id="S3.p4.1.5">BitNet b1.58</span> 3.9B outperforms <span class="ltx_text" id="S3.p4.1.6">LLaMA LLM</span> 3B with lower memory and latency cost. This demonstrates that <span class="ltx_text" id="S3.p4.1.7">BitNet b1.58</span> is a Pareto improvement over the state-of-the-art LLM models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Memory and Latency</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="305" id="S3.F2.g1" src="x3.png" width="407"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="305" id="S3.F2.g2" src="x4.png" width="407"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.4.2" style="font-size:90%;">Decoding latency (Left) and memory consumption (Right) of <span class="ltx_text" id="S3.F2.4.2.1">BitNet b1.58</span> varying the model size.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">We further scaled up the model size to 7B, 13B, and 70B and evaluated the cost. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.F2" title="Figure 2 ‣ Memory and Latency ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the trends of latency and memory, showing that the speed-up increases as the model size scales. In particular, <span class="ltx_text" id="S3.SS0.SSS0.Px1.p1.1.1">BitNet b1.58</span> 70B is 4.1 times faster than the <span class="ltx_text" id="S3.SS0.SSS0.Px1.p1.1.2">LLaMA LLM</span> baseline. This is because the time cost for <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.1.3">nn.Linear</em> grows with the model size. The memory consumption follows a similar trend, as the embedding remains full precision and its memory proportion is smaller for larger models. Both latency and memory were measured with a 2-bit kernel, so there is still room for optimization to further reduce the cost.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Energy</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">We also estimate the arithmetic operations energy consumption of both <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.1">BitNet b1.58</span> and <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.2">LLaMA LLM</span>.
We focus mainly on the calculation for matrix multiplication, since it
contributes the most to the cost of LLMs. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.F3" title="Figure 3 ‣ Energy ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the composition of the energy cost. The majority of <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.3">BitNet b1.58</span> is INT8 addition calculation, while <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.4">LLaMA LLM</span> consists of both FP16 addition and FP16 multiplication. According to the energy model in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib28" title="">28</a>]</cite>, <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.5">BitNet b1.58</span> saves 71.4 times arithmetic operations energy consumption for matrix multiplication on 7nm chips. We further reported the end-to-end energy cost for models with 512 tokens. Our results show that as the model size scales, <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.6">BitNet b1.58</span> becomes increasingly more efficient in terms of energy consumption compared to the FP16 <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.7">LLaMA LLM</span> baseline. This is due to the fact that the percentage of <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.1.8">nn.Linear</em> grows with the model size, while the cost from other components is smaller for larger models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="305" id="S3.F3.g1" src="x5.png" width="407"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="305" id="S3.F3.g2" src="x6.png" width="407"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.4.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.5.2" style="font-size:90%;">Energy consumption of <span class="ltx_text" id="S3.F3.5.2.1">BitNet b1.58</span> compared to <span class="ltx_text" id="S3.F3.5.2.2">LLaMA LLM</span> at 7nm process nodes. On the left is the components of arithmetic operations energy. On the right is the end-to-end energy cost across different model sizes.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">Throughput</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.2">
<tbody><tr class="ltx_tr" id="S3.T3.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T3.2.1.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.2.1.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.2.1">Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.2.1.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.3.1">Max Batch Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.2.1.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.4.1">Throughput (tokens/s)</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.2.2.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text" id="S3.T3.2.2.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">70B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3" style="padding-left:10.0pt;padding-right:10.0pt;">16 (1.0x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4" style="padding-left:10.0pt;padding-right:10.0pt;">&nbsp;&nbsp;333 (1.0x)</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T3.2.3.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.3.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.3.2" style="padding-left:10.0pt;padding-right:10.0pt;">70B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.3.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.3.3.1">176 (11.0x)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.3.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.3.4.1">2977 (8.9x)</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.5.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.6.2" style="font-size:90%;">Comparison of the throughput between <span class="ltx_text" id="S3.T3.6.2.1">BitNet b1.58</span> 70B and <span class="ltx_text" id="S3.T3.6.2.2">LLaMA LLM</span> 70B.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">We compare the throughput of <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.1">BitNet b1.58</span> and <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.2">LLaMA LLM</span> with 70B parameters on two 80GB A100 cards, using pipeline parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib6" title="">6</a>]</cite> so that <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.3">LLaMA LLM</span> 70B could be run on the devices. We increased the batch size until the GPU memory limit was reached, with a sequence length of 512. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.T3" title="Table 3 ‣ Throughput ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">3</span></a> shows that <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.4">BitNet b1.58</span> 70B can support up to 11 times the batch size of <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.5">LLaMA LLM</span>, resulting an 8.9 times higher throughput.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px3.p2.1.1">BitNet b1.58</span><span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px3.p2.1.2"> is enabling a new scaling law with respect to model performance and inference cost</span>. As a reference, we can have the following equivalence between different model sizes in 1.58-bit and 16-bit based on the results in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.F2" title="Figure 2 ‣ Memory and Latency ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.F3" title="Figure 3 ‣ Energy ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">3</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">13B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consumption, than 3B FP16 LLM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">30B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consumption, than 7B FP16 LLM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">70B BitNet b1.58 is more efficient, in terms of latency, memory usage and energy consumption, than 13B FP16 LLM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px4">
<h3 class="ltx_title ltx_title_paragraph">Training with 2T Tokens</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px4.p1.1">The number of training tokens is a crucial factor for LLMs. To test the scalability of <span class="ltx_text" id="S3.SS0.SSS0.Px4.p1.1.1">BitNet b1.58</span> in terms of tokens, we trained a <span class="ltx_text" id="S3.SS0.SSS0.Px4.p1.1.2">BitNet b1.58</span> model with 2T tokens following the data recipe of StableLM-3B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib17" title="">17</a>]</cite>, which is the state-of-the-art open-source 3B model. Both models were evaluated on a benchmark that consists of Winogrande&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib15" title="">15</a>]</cite>, PIQA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib1" title="">1</a>]</cite>, SciQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib21" title="">21</a>]</cite>, LAMBADA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib12" title="">12</a>]</cite>, and ARC-easy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib25" title="">25</a>]</cite>. We reported the zero-shot accuracy in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.T4" title="Table 4 ‣ Training with 2T Tokens ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">4</span></a>. For tasks measured with accuracy and normalized accuracy, we take the average of the two. The results of StableLM 3b at 2T tokens are taken directly from its technical report. Our findings shows that <span class="ltx_text" id="S3.SS0.SSS0.Px4.p1.1.3">BitNet b1.58</span> achieves a superior performance on all end tasks, indicating that 1.58-bit LLMs also have strong generalization capabilities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T4">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T4.2" style="width:433.6pt;height:70.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(50.6pt,-8.2pt) scale(1.30411759293444,1.30411759293444) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.2.1">
<tbody><tr class="ltx_tr" id="S3.T4.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.2.1">Tokens</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.3.1">Winogrande</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.4.1">PIQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.5.1">SciQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.6.1">LAMBADA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.7.1">ARC-easy</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.8"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.8.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T4.2.1.2.1">StableLM-3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.2">2T</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.3">64.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.4">76.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.5">90.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.6">66.09</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.7">67.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.8">73.22</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T4.2.1.3.1">
<span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.1.1">BitNet b1.58</span> 3B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.2">2T</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.3"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.3.1">66.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.4"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.4.1">78.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.5"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.5.1">91.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.6"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.6.1">67.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.7"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.7.1">68.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.8"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.8.1">74.34</span></td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.4.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S3.T4.5.2" style="font-size:90%;">Comparison of <span class="ltx_text" id="S3.T4.5.2.1">BitNet b1.58</span> with StableLM-3B with 2T tokens.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion and Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">1-bit Mixture-of-Experts (MoE) LLMs</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Mixture-of-Experts (MoE) have proven to be a cost-effective approach for LLMs. While it significantly reduces the computation FLOPs, the high memory consumption and inter-chip communication overhead limit its deployment and application. These challenges can be addressed by 1.58-bit LLMs. Firstly, the reduced memory footprint reduces the number of devices required to deploy MoE models. Moreover, it significantly reduces the overhead of transferring activations across networks. Ultimately, there would be no overhead if the entire models could be placed on a single chip.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Native Support of Long Sequence in LLMs</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">In the era of LLMs, the ability to handle long sequence has become a critical demand. One major challenge for long sequence inference is the memory consumption introduced by the KV caches. <span class="ltx_text" id="S4.p4.1.1">BitNet b1.58</span> represents a significant step towards native support for long sequences, as it reduces the activations from 16 bits to 8 bits, allowing the context length to be doubled given the same resources.
This can be further losslessly compressed to 4 bits or even lower for 1.58-bit LLMs, which we leave as future work.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.p5.1.1">LLMs on Edge and Mobile</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">The use of 1.58-bit LLMs has the potential to greatly improve the performance of language models on edge and mobile devices. These devices are often limited by their memory and computational power, which can restrict the performance and the scale of LLMs. However, the reduced memory and energy consumption of 1.58-bit LLMs allows them to be deployed on these devices, enabling a wide range of applications that were previously not possible. This can greatly enhance the capabilities of edge and mobile devices and enable new and exciting applications of LLMs. Moreover, 1.58-bit LLMs are more friendly to CPU devices, which are the main processors used in edge and mobile devices. This means that <span class="ltx_text" id="S4.p6.1.1">BitNet b1.58</span> can be efficiently executed on these devices, further improving their performance and capabilities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1"><span class="ltx_text ltx_font_bold" id="S4.p7.1.1">New Hardware for 1-bit LLMs</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p8">
<p class="ltx_p" id="S4.p8.1">Recent work like&nbsp;Groq<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://groq.com/" title="">https://groq.com/</a></span></span></span> has demonstrated promising results and great potential for building specific hardware (e.g., LPUs) for LLMs. Going one step further, we envision and call for actions to design new hardware and system specifically optimized for 1-bit LLMs, given the new computation paradigm enabled in BitNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib23" title="">23</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BZB<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib1.2.2.m1.1"><semantics id="bib.bib1.2.2.m1.1a"><msup id="bib.bib1.2.2.m1.1.1" xref="bib.bib1.2.2.m1.1.1.cmml"><mi id="bib.bib1.2.2.m1.1.1a" xref="bib.bib1.2.2.m1.1.1.cmml"></mi><mo id="bib.bib1.2.2.m1.1.1.1" xref="bib.bib1.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib1.2.2.m1.1b"><apply id="bib.bib1.2.2.m1.1.1.cmml" xref="bib.bib1.2.2.m1.1.1"><plus id="bib.bib1.2.2.m1.1.1.1.cmml" xref="bib.bib1.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib1.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib1.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan&nbsp;Le Bras, Jianfeng Gao, and Yejin Choi.

</span>
<span class="ltx_bibblock">PIQA: reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1">CoRR</span>, abs/1911.11641, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CCKS [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher&nbsp;De Sa.

</span>
<span class="ltx_bibblock">QuIP: 2-bit quantization of large language models with guarantees.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">CoRR</span>, abs/2307.13304, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CLC<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib3.2.2.m1.1"><semantics id="bib.bib3.2.2.m1.1a"><msup id="bib.bib3.2.2.m1.1.1" xref="bib.bib3.2.2.m1.1.1.cmml"><mi id="bib.bib3.2.2.m1.1.1a" xref="bib.bib3.2.2.m1.1.1.cmml"></mi><mo id="bib.bib3.2.2.m1.1.1.1" xref="bib.bib3.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib3.2.2.m1.1b"><apply id="bib.bib3.2.2.m1.1.1.cmml" xref="bib.bib3.2.2.m1.1.1"><plus id="bib.bib3.2.2.m1.1.1.1.cmml" xref="bib.bib3.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib3.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib3.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no questions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.3.1">CoRR</span>, abs/1905.10044, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Com [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Together Computer.

</span>
<span class="ltx_bibblock">Redpajama: an open dataset for training large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FAHA [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">OPTQ: accurate quantization for generative pre-trained transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">The Eleventh International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HCB<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib6.2.2.m1.1"><semantics id="bib.bib6.2.2.m1.1a"><msup id="bib.bib6.2.2.m1.1.1" xref="bib.bib6.2.2.m1.1.1.cmml"><mi id="bib.bib6.2.2.m1.1.1a" xref="bib.bib6.2.2.m1.1.1.cmml"></mi><mo id="bib.bib6.2.2.m1.1.1.1" xref="bib.bib6.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib6.2.2.m1.1b"><apply id="bib.bib6.2.2.m1.1.1.cmml" xref="bib.bib6.2.2.m1.1.1"><plus id="bib.bib6.2.2.m1.1.1.1.cmml" xref="bib.bib6.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib6.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib6.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia&nbsp;Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc&nbsp;V. Le, Yonghui Wu, and Zhifeng Chen.

</span>
<span class="ltx_bibblock">Gpipe: Efficient training of giant neural networks using pipeline parallelism.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.3.1">Advances in Neural Information Processing Systems</span>, pages 103–112, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hor [14]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mark Horowitz.

</span>
<span class="ltx_bibblock">1.1 computing’s energy problem (and what we can do about it).

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">2014 IEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest of Technical Papers, San Francisco, CA, USA, February 9-13, 2014</span>, pages 10–14, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KLZ<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib8.2.2.m1.1"><semantics id="bib.bib8.2.2.m1.1a"><msup id="bib.bib8.2.2.m1.1.1" xref="bib.bib8.2.2.m1.1.1.cmml"><mi id="bib.bib8.2.2.m1.1.1a" xref="bib.bib8.2.2.m1.1.1.cmml"></mi><mo id="bib.bib8.2.2.m1.1.1.1" xref="bib.bib8.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib8.2.2.m1.1b"><apply id="bib.bib8.2.2.m1.1.1.cmml" xref="bib.bib8.2.2.m1.1.1"><plus id="bib.bib8.2.2.m1.1.1.1.cmml" xref="bib.bib8.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib8.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib8.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.3.1">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LTT<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib9.2.2.m1.1"><semantics id="bib.bib9.2.2.m1.1a"><msup id="bib.bib9.2.2.m1.1.1" xref="bib.bib9.2.2.m1.1.1.cmml"><mi id="bib.bib9.2.2.m1.1.1a" xref="bib.bib9.2.2.m1.1.1.cmml"></mi><mo id="bib.bib9.2.2.m1.1.1.1" xref="bib.bib9.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib9.2.2.m1.1b"><apply id="bib.bib9.2.2.m1.1.1.cmml" xref="bib.bib9.2.2.m1.1.1"><plus id="bib.bib9.2.2.m1.1.1.1.cmml" xref="bib.bib9.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib9.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ji&nbsp;Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.

</span>
<span class="ltx_bibblock">AWQ: activation-aware weight quantization for LLM compression and acceleration.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1">CoRR</span>, abs/2306.00978, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MCKS [18]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? A new dataset for open book question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">CoRR</span>, abs/1809.02789, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MXBS [16]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.

</span>
<span class="ltx_bibblock">Pointer sentinel mixture models, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PKL<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib12.2.2.m1.1"><semantics id="bib.bib12.2.2.m1.1a"><msup id="bib.bib12.2.2.m1.1.1" xref="bib.bib12.2.2.m1.1.1.cmml"><mi id="bib.bib12.2.2.m1.1.1a" xref="bib.bib12.2.2.m1.1.1.cmml"></mi><mo id="bib.bib12.2.2.m1.1.1.1" xref="bib.bib12.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib12.2.2.m1.1b"><apply id="bib.bib12.2.2.m1.1.1.cmml" xref="bib.bib12.2.2.m1.1.1"><plus id="bib.bib12.2.2.m1.1.1.1.cmml" xref="bib.bib12.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib12.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib12.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [16]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan&nbsp;Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández.

</span>
<span class="ltx_bibblock">The LAMBADA dataset: Word prediction requiring a broad discourse context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers</span>. The Association for Computer Linguistics, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RSR<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib13.2.2.m1.1"><semantics id="bib.bib13.2.2.m1.1a"><msup id="bib.bib13.2.2.m1.1.1" xref="bib.bib13.2.2.m1.1.1.cmml"><mi id="bib.bib13.2.2.m1.1.1a" xref="bib.bib13.2.2.m1.1.1.cmml"></mi><mo id="bib.bib13.2.2.m1.1.1.1" xref="bib.bib13.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib13.2.2.m1.1b"><apply id="bib.bib13.2.2.m1.1.1.cmml" xref="bib.bib13.2.2.m1.1.1"><plus id="bib.bib13.2.2.m1.1.1.1.cmml" xref="bib.bib13.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib13.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib13.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1">CoRR</span>, abs/1910.10683, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SAL<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib14.2.2.m1.1"><semantics id="bib.bib14.2.2.m1.1a"><msup id="bib.bib14.2.2.m1.1.1" xref="bib.bib14.2.2.m1.1.1.cmml"><mi id="bib.bib14.2.2.m1.1.1a" xref="bib.bib14.2.2.m1.1.1.cmml"></mi><mo id="bib.bib14.2.2.m1.1.1.1" xref="bib.bib14.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib14.2.2.m1.1b"><apply id="bib.bib14.2.2.m1.1.1.cmml" xref="bib.bib14.2.2.m1.1.1"><plus id="bib.bib14.2.2.m1.1.1.1.cmml" xref="bib.bib14.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib14.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib14.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [24]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jianlin Su, Murtadha H.&nbsp;M. Ahmed, Yu&nbsp;Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.3.1">Neurocomputing</span>, 568:127063, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SBBC [20]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">WinoGrande: an adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">The Thirty-Fourth AAAI Conference on Artificial Intelligence</span>, pages 8732–8740, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sha [20]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">GLU variants improve transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">CoRR</span>, abs/2002.05202, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme.

</span>
<span class="ltx_bibblock">Stablelm 3b 4e1t.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TCS<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib18.2.2.m1.1"><semantics id="bib.bib18.2.2.m1.1a"><msup id="bib.bib18.2.2.m1.1.1" xref="bib.bib18.2.2.m1.1.1.cmml"><mi id="bib.bib18.2.2.m1.1.1a" xref="bib.bib18.2.2.m1.1.1.cmml"></mi><mo id="bib.bib18.2.2.m1.1.1.1" xref="bib.bib18.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib18.2.2.m1.1b"><apply id="bib.bib18.2.2.m1.1.1.cmml" xref="bib.bib18.2.2.m1.1.1"><plus id="bib.bib18.2.2.m1.1.1.1.cmml" xref="bib.bib18.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib18.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib18.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [24]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher&nbsp;De Sa.

</span>
<span class="ltx_bibblock">Quip#: Even better LLM quantization with hadamard incoherence and lattice codebooks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.3.1">CoRR</span>, abs/2402.04396, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TLI<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib19.2.2.m1.1"><semantics id="bib.bib19.2.2.m1.1a"><msup id="bib.bib19.2.2.m1.1.1" xref="bib.bib19.2.2.m1.1.1.cmml"><mi id="bib.bib19.2.2.m1.1.1a" xref="bib.bib19.2.2.m1.1.1.cmml"></mi><mo id="bib.bib19.2.2.m1.1.1.1" xref="bib.bib19.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib19.2.2.m1.1b"><apply id="bib.bib19.2.2.m1.1.1.cmml" xref="bib.bib19.2.2.m1.1.1"><plus id="bib.bib19.2.2.m1.1.1.1.cmml" xref="bib.bib19.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib19.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib19.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">LLaMA: open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.3.1">CoRR</span>, abs/2302.13971, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TMS<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib20.2.2.m1.1"><semantics id="bib.bib20.2.2.m1.1a"><msup id="bib.bib20.2.2.m1.1.1" xref="bib.bib20.2.2.m1.1.1.cmml"><mi id="bib.bib20.2.2.m1.1.1a" xref="bib.bib20.2.2.m1.1.1.cmml"></mi><mo id="bib.bib20.2.2.m1.1.1.1" xref="bib.bib20.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib20.2.2.m1.1b"><apply id="bib.bib20.2.2.m1.1.1.cmml" xref="bib.bib20.2.2.m1.1.1"><plus id="bib.bib20.2.2.m1.1.1.1.cmml" xref="bib.bib20.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib20.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib20.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian&nbsp;Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, and et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.3.1">CoRR</span>, abs/2307.09288, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WLG [17]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Johannes Welbl, Nelson&nbsp;F. Liu, and Matt Gardner.

</span>
<span class="ltx_bibblock">Crowdsourcing multiple choice science questions.

</span>
<span class="ltx_bibblock">In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017</span>, pages 94–106. Association for Computational Linguistics, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WMC<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib22.2.2.m1.1"><semantics id="bib.bib22.2.2.m1.1a"><msup id="bib.bib22.2.2.m1.1.1" xref="bib.bib22.2.2.m1.1.1.cmml"><mi id="bib.bib22.2.2.m1.1.1a" xref="bib.bib22.2.2.m1.1.1.cmml"></mi><mo id="bib.bib22.2.2.m1.1.1.1" xref="bib.bib22.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib22.2.2.m1.1b"><apply id="bib.bib22.2.2.m1.1.1.cmml" xref="bib.bib22.2.2.m1.1.1"><plus id="bib.bib22.2.2.m1.1.1.1.cmml" xref="bib.bib22.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lei Wang, Lingxiao Ma, Shijie Cao, Ningxin Zheng, Quanlu Zhang, Jilong Xue, Ziming Miao, Ting Cao, , and Yuqing Yang.

</span>
<span class="ltx_bibblock">Ladder: Efficient tensor compilation on customized data format.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.3.1">OSDI</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WMD<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib23.2.2.m1.1"><semantics id="bib.bib23.2.2.m1.1a"><msup id="bib.bib23.2.2.m1.1.1" xref="bib.bib23.2.2.m1.1.1.cmml"><mi id="bib.bib23.2.2.m1.1.1a" xref="bib.bib23.2.2.m1.1.1.cmml"></mi><mo id="bib.bib23.2.2.m1.1.1.1" xref="bib.bib23.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib23.2.2.m1.1b"><apply id="bib.bib23.2.2.m1.1.1.cmml" xref="bib.bib23.2.2.m1.1.1"><plus id="bib.bib23.2.2.m1.1.1.1.cmml" xref="bib.bib23.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib23.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib23.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hongyu Wang, Shuming Ma, Li&nbsp;Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi&nbsp;Wu, and Furu Wei.

</span>
<span class="ltx_bibblock">Bitnet: Scaling 1-bit transformers for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.3.1">CoRR</span>, abs/2310.11453, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">XLS<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib24.2.2.m1.1"><semantics id="bib.bib24.2.2.m1.1a"><msup id="bib.bib24.2.2.m1.1.1" xref="bib.bib24.2.2.m1.1.1.cmml"><mi id="bib.bib24.2.2.m1.1.1a" xref="bib.bib24.2.2.m1.1.1.cmml"></mi><mo id="bib.bib24.2.2.m1.1.1.1" xref="bib.bib24.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib24.2.2.m1.1b"><apply id="bib.bib24.2.2.m1.1.1.cmml" xref="bib.bib24.2.2.m1.1.1"><plus id="bib.bib24.2.2.m1.1.1.1.cmml" xref="bib.bib24.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib24.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib24.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guangxuan Xiao, Ji&nbsp;Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han.

</span>
<span class="ltx_bibblock">SmoothQuant: accurate and efficient post-training quantization for large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.3.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">YBS [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vikas Yadav, Steven Bethard, and Mihai Surdeanu.

</span>
<span class="ltx_bibblock">Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering.

</span>
<span class="ltx_bibblock">In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">EMNLP-IJCNLP</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ZHB<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib26.2.2.m1.1"><semantics id="bib.bib26.2.2.m1.1a"><msup id="bib.bib26.2.2.m1.1.1" xref="bib.bib26.2.2.m1.1.1.cmml"><mi id="bib.bib26.2.2.m1.1.1a" xref="bib.bib26.2.2.m1.1.1.cmml"></mi><mo id="bib.bib26.2.2.m1.1.1.1" xref="bib.bib26.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib26.2.2.m1.1b"><apply id="bib.bib26.2.2.m1.1.1.cmml" xref="bib.bib26.2.2.m1.1.1"><plus id="bib.bib26.2.2.m1.1.1.1.cmml" xref="bib.bib26.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib26.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib26.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">HellaSwag: can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 57th Conference of the Association for Computational Linguistics</span>, pages 4791–4800, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ZS [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock">In Hanna&nbsp;M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily&nbsp;B. Fox, and Roman Garnett, editors, <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Advances in Neural Information Processing Systems</span>, pages 12360–12371, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ZZL [22]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yichi Zhang, Zhiru Zhang, and Lukasz Lew.

</span>
<span class="ltx_bibblock">PokeBNN: A binary pursuit of lightweight accuracy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 12465–12475. IEEE, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
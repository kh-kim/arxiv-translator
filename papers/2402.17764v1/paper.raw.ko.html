<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</title>
<!--Generated on Tue Feb 27 18:44:28 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2402.17764v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2402.17764v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2402.17764v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2402.17764v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S1" title="1 The Era of 1-bit LLMs ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>The Era of 1-bit LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S2" title="2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>BitNet b1.58</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S2.SS0.SSS0.Px1" title="Quantization Function. ‣ 2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">Quantization Function.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S2.SS0.SSS0.Px2" title="LLaMA-alike Components. ‣ 2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">LLaMA-alike Components.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3" title="3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.SS0.SSS0.Px1" title="Memory and Latency ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">Memory and Latency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.SS0.SSS0.Px2" title="Energy ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">Energy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.SS0.SSS0.Px3" title="Throughput ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">Throughput</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.SS0.SSS0.Px4" title="Training with 2T Tokens ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title">Training with 2T Tokens</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S4" title="4 Discussion and Future Work ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Discussion and Future Work</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: arydshln</li>
<li>failed: scalerel</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2402.17764v1 [cs.CL] 27 Feb 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Era of 1-bit LLMs: 
<br class="ltx_break">All Large Language Models are in 1.58 Bits</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shuming Ma&nbsp;&nbsp;&nbsp;&nbsp;Hongyu Wang<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>&nbsp;&nbsp;&nbsp;&nbsp;Lingxiao Ma&nbsp;&nbsp;&nbsp;&nbsp;Lei Wang&nbsp;&nbsp;&nbsp;&nbsp;Wenhui Wang 
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id2.2.1">&nbsp;&nbsp;&nbsp;&nbsp;Shaohan Huang&nbsp;&nbsp;&nbsp;&nbsp;Li Dong&nbsp;&nbsp;&nbsp;&nbsp;Ruiping Wang&nbsp;&nbsp;&nbsp;&nbsp;Jilong Xue&nbsp;&nbsp;&nbsp;&nbsp;Furu Wei<math alttext="{}^{\diamond}" class="ltx_Math" display="inline" id="id2.2.1.m1.1"><semantics id="id2.2.1.m1.1a"><msup id="id2.2.1.m1.1.1" xref="id2.2.1.m1.1.1.cmml"><mi id="id2.2.1.m1.1.1a" xref="id2.2.1.m1.1.1.cmml"></mi><mo id="id2.2.1.m1.1.1.1" mathvariant="normal" xref="id2.2.1.m1.1.1.1.cmml">⋄</mo></msup><annotation-xml encoding="MathML-Content" id="id2.2.1.m1.1b"><apply id="id2.2.1.m1.1.1.cmml" xref="id2.2.1.m1.1.1"><ci id="id2.2.1.m1.1.1.1.cmml" xref="id2.2.1.m1.1.1.1">normal-⋄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.1.m1.1c">{}^{\diamond}</annotation><annotation encoding="application/x-llamapun" id="id2.2.1.m1.1d">start_FLOATSUPERSCRIPT ⋄ end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"><a class="ltx_ref ltx_href" href="https://aka.ms/GeneralAI" title="">https://aka.ms/GeneralAI</a>
<br class="ltx_break"></span>
</span><span class="ltx_author_notes">&nbsp;Equal contribution. <math alttext="\diamond" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">⋄</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">⋄</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\diamond</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">⋄</annotation></semantics></math> Corresponding author. S. Ma, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, J. Xue, F. Wei are with Microsoft Research. H. Wang and R. Wang are with University of Chinese Academy of Sciences.</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id3.id1">BitNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib23" title="">23</a>]</cite>와 같은 최근 연구는 1비트 대용량 언어 모델(LLM)의 새로운 시대를 위한 길을 열어주고 있다. 본 연구에서는 1비트 LLM 변종, 즉 <span class="ltx_text ltx_font_bold" id="id3.id1.1">BitNet b1.58</span>을 소개하며, 여기서 LLM의 모든 단일 파라미터(또는 가중치)는 삼진 {-1, 0, 1}이다. 이는 전체 정밀도(즉, FP16 또는 BF16) 트랜스포머 LLM과 동일한 모델 크기 및 훈련 토큰을 갖는 반면, 지연 시간, 메모리, 처리량 및 에너지 소비 측면에서 훨씬 더 비용 효율적이다. 보다 심층적으로, 1.58비트 LLM은 고성능 및 비용 효율적인 새로운 세대의 LLM을 훈련하기 위한 새로운 스케일링 법칙 및 레시피를 정의한다. 또한, 새로운 계산 패러다임을 가능하게 하고, 1-비트 LLMs에 최적화된 특정 하드웨어를 설계할 수 있는 문을 열어준다.</p>
</div>
<figure class="ltx_figure" id="S0.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S0.F1.1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="330" id="S0.F1.1.g1" src="https://arxiv.org/html/2402.17764v1/x1.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_flex_size_1 ltx_align_center" id="S0.F1.2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="380" id="S0.F1.2.g1" src="https://arxiv.org/html/2402.17764v1/x2.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.6.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S0.F1.7.2" style="font-size:90%;">1-bit LLMs (e.g., <span class="ltx_text" id="S0.F1.7.2.1">BitNet b1.58</span>) provide a Pareto solution to reduce inference cost (latency, throughput, and energy) of LLMs while maintaining model performance. The new computation paradigm of <span class="ltx_text" id="S0.F1.7.2.2">BitNet b1.58</span> calls for actions to design new hardware optimized for 1-bit LLMs.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>The Era of 1-bit LLMs</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">최근 AI 분야는 LLM(Large Language Models)의 규모와 역량이 급성장하고 있다. 이러한 모델은 광범위한 자연어 처리 작업에서 놀라운 성능을 보여주었지만 크기가 증가함에 따라 배치에 어려움이 있고 높은 에너지 소비로 인한 환경 및 경제적 영향에 대한 우려가 제기되었다. 이러한 과제를 해결하기 위한 한 가지 접근법은 추론 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib18" title="">18</a>]</cite>에 대한 낮은 비트 모델을 생성하기 위해 훈련 후 양자화를 사용하는 것이다. 이 기술은 가중치 및 활성화의 정밀도를 감소시켜 LLM의 메모리 및 계산 요구 사항을 상당히 감소시킨다. 그 추세는 4비트 변형 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib9" title="">9</a>]</cite>와 같이 16비트에서 하위 비트로 이동하는 것이었다. 그러나 훈련 후 양자화는 산업 LLM에서 널리 사용되지만 차선책이다.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">BitNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib23" title="">23</a>]</cite>와 같은 1비트 모델 아키텍처에 대한 최근의 연구는 LLM의 성능을 유지하면서 비용을 줄이는 유망한 방향을 제시한다. 바닐라 LLM은 16비트 부동 값(즉, FP16 또는 BF16)에 있으며, 임의의 LLM들의 대부분은 매트릭스 곱셈이다. 따라서, 주요 연산 비용은 부동소수점 덧셈 연산과 곱셈 연산에서 발생한다. 반면에 BitNet의 행렬 곱셈은 정수 덧셈만을 포함하므로 LLM에 대한 에너지 비용 차수를 절약할 수 있다. 많은 칩에서 성능을 계산하는 근본적인 한계가 전력이기 때문에 에너지 절약도 더 빠른 계산으로 변환될 수 있다.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">계산 외에도, 모델 파라미터들을 DRAM으로부터 온-칩 가속기(예를 들어, SRAM)의 메모리로 전달하는 프로세스는 추론 동안 비용이 많이 들 수 있다. 스루풋을 향상시키기 위해 SRAM을 확대하려는 시도가 있었지만, 이것은 DRAM보다 상당히 높은 비용을 도입한다. 전체 고정밀 모델에 비해 1비트 LLM은 용량 및 대역폭 관점에서 훨씬 낮은 메모리 풋프린트를 갖는다. 이는 DRAM으로부터 가중치들을 로딩하는 비용 및 시간을 상당히 감소시킬 수 있어, 더 빠르고 효율적인 추론으로 이어진다.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">이 작업에서 우리는 <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">BitNet b1.58</span>이라고 하는 중요한 1비트 LLM 변형을 도입하며, 여기서 모든 매개변수는 {-1, 0, 1} 값을 취하는 삼진법이다. 우리는 원래의 1비트 비트넷에 0의 추가 값을 추가하여 이진 시스템에서 1.58비트를 얻었다. <span class="ltx_text" id="S1.p4.1.2">BitNet b1.58</span>은 행렬 곱셈을 위한 곱셈 연산이 거의 필요하지 않고 고도로 최적화될 수 있는 새로운 계산 패러다임을 포함하여 원래 1비트 BitNet의 모든 이점을 유지합니다. 또한, 기존의 1비트 비트넷과 동일한 에너지 소모량을 가지며, FP16 LLM 베이스라인에 비해 메모리 소모량, 처리량, 지연시간 측면에서 훨씬 효율적이다. 또한 <span class="ltx_text" id="S1.p4.1.3">BitNet b1.58</span>은 두 가지 추가적인 이점을 제공한다. 첫째, 특징 필터링에 대한 명시적인 지원으로 인해 모델링 능력이 강화되며, 모델 가중치에 0을 포함시킴으로써 1비트 LLM의 성능을 크게 향상시킬 수 있다. 둘째, <span class="ltx_text" id="S1.p4.1.4">BitNet b1.58</span>은 동일한 구성(예: 모델 크기, 훈련 토큰 등)을 사용할 때 3B 크기에서 시작하여 복잡도와 최종 작업 성능 측면에서 전체 정밀도(예: FP16) 기준선과 일치할 수 있음을 보여준다.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>BitNet b1.58</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text" id="S2.p1.1.1">BitNet b1.58</span>은 BitNet 아키텍처를 기반으로 하며, 이는 <em class="ltx_emph ltx_font_italic" id="S2.p1.1.2">nn.Linear</em>을 <em class="ltx_emph ltx_font_italic" id="S2.p1.1.3">BitLinear</em>으로 대체하는 Transformer이다. 1.58비트 무게와 8비트 활성화로 처음부터 훈련됩니다. 원래 BitNet과 비교하여 아래에 요약한 몇 가지 수정 사항을 소개합니다.</p>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Quantization Function.</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">가중치를 -1, 0 또는 +1로 제한하기 위해 <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.1">absmean</em> 양자화 함수를 채택합니다. 먼저 가중치 행렬을 평균 절대값으로 스케일링한 다음 각 값을 {-1, 0, +1} 중 가장 가까운 정수로 라운딩한다:</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\widetilde{W}=\mathrm{RoundClip}(\frac{W}{\gamma+\epsilon},-1,1)," class="ltx_Math" display="block" id="S2.E1.m1.3"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><mover accent="true" id="S2.E1.m1.3.3.1.1.3" xref="S2.E1.m1.3.3.1.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.3.2" xref="S2.E1.m1.3.3.1.1.3.2.cmml">W</mi><mo id="S2.E1.m1.3.3.1.1.3.1" xref="S2.E1.m1.3.3.1.1.3.1.cmml">~</mo></mover><mo id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.3.cmml">RoundClip</mi><mo id="S2.E1.m1.3.3.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml"><mo id="S2.E1.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">(</mo><mfrac id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mi id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">W</mi><mrow id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml">γ</mi><mo id="S2.E1.m1.1.1.3.1" xref="S2.E1.m1.1.1.3.1.cmml">+</mo><mi id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml">ϵ</mi></mrow></mfrac><mo id="S2.E1.m1.3.3.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">,</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.3.3.1.1.1.1.1.1a" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml">−</mo><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml">1</mn></mrow><mo id="S2.E1.m1.3.3.1.1.1.1.1.4" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">,</mo><mn id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">1</mn><mo id="S2.E1.m1.3.3.1.1.1.1.1.5" stretchy="false" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E1.m1.3.3.1.2" xref="S2.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1"><eq id="S2.E1.m1.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2"></eq><apply id="S2.E1.m1.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3"><ci id="S2.E1.m1.3.3.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1">~</ci><ci id="S2.E1.m1.3.3.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2">𝑊</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.2"></times><ci id="S2.E1.m1.3.3.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3">RoundClip</ci><vector id="S2.E1.m1.3.3.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><divide id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1"></divide><ci id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">𝑊</ci><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><plus id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3.1"></plus><ci id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2">𝛾</ci><ci id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3">italic-ϵ</ci></apply></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1"><minus id="S2.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1"></minus><cn id="S2.E1.m1.3.3.1.1.1.1.1.1.2.cmml" type="integer" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2">1</cn></apply><cn id="S2.E1.m1.2.2.cmml" type="integer" xref="S2.E1.m1.2.2">1</cn></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\widetilde{W}=\mathrm{RoundClip}(\frac{W}{\gamma+\epsilon},-1,1),</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">over~ start_ARG italic_W end_ARG = roman_RoundClip ( divide start_ARG italic_W end_ARG start_ARG italic_γ + italic_ϵ end_ARG , - 1 , 1 ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{RoundClip}(x,a,b)=\max(a,\min(b,\mathrm{round}(x)))," class="ltx_Math" display="block" id="S2.E2.m1.9"><semantics id="S2.E2.m1.9a"><mrow id="S2.E2.m1.9.9.1" xref="S2.E2.m1.9.9.1.1.cmml"><mrow id="S2.E2.m1.9.9.1.1" xref="S2.E2.m1.9.9.1.1.cmml"><mrow id="S2.E2.m1.9.9.1.1.3" xref="S2.E2.m1.9.9.1.1.3.cmml"><mi id="S2.E2.m1.9.9.1.1.3.2" xref="S2.E2.m1.9.9.1.1.3.2.cmml">RoundClip</mi><mo id="S2.E2.m1.9.9.1.1.3.1" xref="S2.E2.m1.9.9.1.1.3.1.cmml">⁢</mo><mrow id="S2.E2.m1.9.9.1.1.3.3.2" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml"><mo id="S2.E2.m1.9.9.1.1.3.3.2.1" stretchy="false" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">x</mi><mo id="S2.E2.m1.9.9.1.1.3.3.2.2" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">a</mi><mo id="S2.E2.m1.9.9.1.1.3.3.2.3" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">,</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">b</mi><mo id="S2.E2.m1.9.9.1.1.3.3.2.4" stretchy="false" xref="S2.E2.m1.9.9.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.9.9.1.1.2" xref="S2.E2.m1.9.9.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.9.9.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.2.cmml"><mi id="S2.E2.m1.7.7" xref="S2.E2.m1.7.7.cmml">max</mi><mo id="S2.E2.m1.9.9.1.1.1.1a" xref="S2.E2.m1.9.9.1.1.1.2.cmml">⁡</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.2.cmml"><mo id="S2.E2.m1.9.9.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.2.cmml">(</mo><mi id="S2.E2.m1.8.8" xref="S2.E2.m1.8.8.cmml">a</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.3" xref="S2.E2.m1.9.9.1.1.1.2.cmml">,</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.5.5" xref="S2.E2.m1.5.5.cmml">min</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1a" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml"><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E2.m1.6.6" xref="S2.E2.m1.6.6.cmml">b</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">,</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2.cmml">round</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml">x</mi><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.4" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.9.9.1.1.1.1.1.4" stretchy="false" xref="S2.E2.m1.9.9.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.9.9.1.2" xref="S2.E2.m1.9.9.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.9b"><apply id="S2.E2.m1.9.9.1.1.cmml" xref="S2.E2.m1.9.9.1"><eq id="S2.E2.m1.9.9.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.2"></eq><apply id="S2.E2.m1.9.9.1.1.3.cmml" xref="S2.E2.m1.9.9.1.1.3"><times id="S2.E2.m1.9.9.1.1.3.1.cmml" xref="S2.E2.m1.9.9.1.1.3.1"></times><ci id="S2.E2.m1.9.9.1.1.3.2.cmml" xref="S2.E2.m1.9.9.1.1.3.2">RoundClip</ci><vector id="S2.E2.m1.9.9.1.1.3.3.1.cmml" xref="S2.E2.m1.9.9.1.1.3.3.2"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑥</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝑎</ci><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">𝑏</ci></vector></apply><apply id="S2.E2.m1.9.9.1.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.1.1"><max id="S2.E2.m1.7.7.cmml" xref="S2.E2.m1.7.7"></max><ci id="S2.E2.m1.8.8.cmml" xref="S2.E2.m1.8.8">𝑎</ci><apply id="S2.E2.m1.9.9.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1"><min id="S2.E2.m1.5.5.cmml" xref="S2.E2.m1.5.5"></min><ci id="S2.E2.m1.6.6.cmml" xref="S2.E2.m1.6.6">𝑏</ci><apply id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1"><times id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.1"></times><ci id="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.9.9.1.1.1.1.1.1.1.1.1.2">round</ci><ci id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.9c">\mathrm{RoundClip}(x,a,b)=\max(a,\min(b,\mathrm{round}(x))),</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.9d">roman_RoundClip ( italic_x , italic_a , italic_b ) = roman_max ( italic_a , roman_min ( italic_b , roman_round ( italic_x ) ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\gamma=\frac{1}{nm}\sum_{ij}|W_{ij}|." class="ltx_Math" display="block" id="S2.E3.m1.1"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mrow id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.3.cmml">γ</mi><mo id="S2.E3.m1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.2.cmml">=</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.cmml"><mfrac id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.3.cmml"><mn id="S2.E3.m1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.3.2.cmml">1</mn><mrow id="S2.E3.m1.1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.1.3.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.3.3.2" xref="S2.E3.m1.1.1.1.1.1.3.3.2.cmml">n</mi><mo id="S2.E3.m1.1.1.1.1.1.3.3.1" xref="S2.E3.m1.1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S2.E3.m1.1.1.1.1.1.3.3.3" xref="S2.E3.m1.1.1.1.1.1.3.3.3.cmml">m</mi></mrow></mfrac><mo id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><munder id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml"><mo id="S2.E3.m1.1.1.1.1.1.1.2.2" movablelimits="false" rspace="0em" xref="S2.E3.m1.1.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2.3.2" xref="S2.E3.m1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S2.E3.m1.1.1.1.1.1.1.2.3.1" xref="S2.E3.m1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S2.E3.m1.1.1.1.1.1.1.2.3.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3.cmml">j</mi></mrow></munder><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.2.cmml"><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo><msub id="S2.E3.m1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml">W</mi><mrow id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="S2.E3.m1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><mo id="S2.E3.m1.1.1.1.2" lspace="0em" xref="S2.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><eq id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.2"></eq><ci id="S2.E3.m1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.3">𝛾</ci><apply id="S2.E3.m1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><times id="S2.E3.m1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3"><divide id="S2.E3.m1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3"></divide><cn id="S2.E3.m1.1.1.1.1.1.3.2.cmml" type="integer" xref="S2.E3.m1.1.1.1.1.1.3.2">1</cn><apply id="S2.E3.m1.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3"><times id="S2.E3.m1.1.1.1.1.1.3.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3.1"></times><ci id="S2.E3.m1.1.1.1.1.1.3.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3.2">𝑛</ci><ci id="S2.E3.m1.1.1.1.1.1.3.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.3.3.3">𝑚</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1"><apply id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2"></sum><apply id="S2.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3"><times id="S2.E3.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.1"></times><ci id="S2.E3.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.2">𝑖</ci><ci id="S2.E3.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3">𝑗</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1"><abs id="S2.E3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.2"></abs><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.2">𝑊</ci><apply id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3"><times id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\gamma=\frac{1}{nm}\sum_{ij}|W_{ij}|.</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.1d">italic_γ = divide start_ARG 1 end_ARG start_ARG italic_n italic_m end_ARG ∑ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT | italic_W start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT | .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p3.2">활성화에 대한 양자화 함수는 비선형 함수 이전의 활성화를 <math alttext="[0,Q_{b}]" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p3.1.m1.2"><semantics id="S2.SS0.SSS0.Px1.p3.1.m1.2a"><mrow id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml"><mo id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.2" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml">[</mo><mn id="S2.SS0.SSS0.Px1.p3.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p3.1.m1.1.1.cmml">0</mn><mo id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.3" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml">,</mo><msub id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2.cmml">Q</mi><mi id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3.cmml">b</mi></msub><mo id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.4" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p3.1.m1.2b"><interval closure="closed" id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1"><cn id="S2.SS0.SSS0.Px1.p3.1.m1.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p3.1.m1.1.1">0</cn><apply id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.2">𝑄</ci><ci id="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p3.1.m1.2.2.1.1.3">𝑏</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p3.1.m1.2c">[0,Q_{b}]</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p3.1.m1.2d">[ 0 , italic_Q start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ]</annotation></semantics></math> 범위로 확장하지 않는다는 점을 제외하고는 BitNet에서 동일한 구현을 따른다. 대신에, 활성화들은 모두 제로-포인트 양자화를 제거하기 위해 토큰당 <math alttext="[-Q_{b},Q_{b}]" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p3.2.m2.2"><semantics id="S2.SS0.SSS0.Px1.p3.2.m2.2a"><mrow id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml"><mo id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.3" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml">[</mo><mrow id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.cmml"><mo id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1a" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.cmml">−</mo><msub id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.cmml"><mi id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2.cmml">Q</mi><mi id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3.cmml">b</mi></msub></mrow><mo id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.4" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml">,</mo><msub id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.cmml"><mi id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2.cmml">Q</mi><mi id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3.cmml">b</mi></msub><mo id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.5" stretchy="false" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p3.2.m2.2b"><interval closure="closed" id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.3.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2"><apply id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1"><minus id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1"></minus><apply id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.2">𝑄</ci><ci id="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.1.1.1.1.2.3">𝑏</ci></apply></apply><apply id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.1.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.2">𝑄</ci><ci id="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3.cmml" xref="S2.SS0.SSS0.Px1.p3.2.m2.2.2.2.2.3">𝑏</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p3.2.m2.2c">[-Q_{b},Q_{b}]</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p3.2.m2.2d">[ - italic_Q start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT , italic_Q start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ]</annotation></semantics></math>로 스케일링된다. 이것은 구현 및 시스템 수준 최적화 모두에 대해 더 편리하고 간단하지만, 실험에서 성능에 무시할 수 있는 영향을 도입한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">LLaMA-alike Components.</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">LLaMA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib20" title="">20</a>]</cite>의 아키텍처는 오픈 소스 LLMs에 대한 de-facto 백본이었다. 오픈 소스 커뮤니티를 수용하기 위해 <span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.1">BitNet b1.58</span>은 LLaMA 유사 구성 요소를 채택합니다. 구체적으로, RMSNorm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib27" title="">27</a>]</cite>, SwiGLU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib16" title="">16</a>]</cite>, 회전 임베딩 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib14" title="">14</a>]</cite>를 사용하고 모든 바이어스를 제거한다. 이러한 방식으로, <span class="ltx_text" id="S2.SS0.SSS0.Px2.p1.1.2">BitNet b1.58</span>은 최소한의 노력으로 인기 있는 오픈 소스 소프트웨어(예를 들어, Huggingface, vLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib8" title="">8</a>]</cite>, 및 llama.cpp<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ggerganov/llama.cpp" title="">https://github.com/ggerganov/llama.cpp</a></span></span></span>)에 통합될 수 있다.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.3">
<tbody><tr class="ltx_tr" id="S2.T1.3.3">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.3.3.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.3.4.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.3.3.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.3.5.1">Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.1.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1">Memory (GB)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S2.T1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">normal-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.2.2.2.1">Latency (ms)<math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.2.2.2.1.m1.1"><semantics id="S2.T1.2.2.2.1.m1.1a"><mo id="S2.T1.2.2.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S2.T1.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.1.m1.1b"><ci id="S2.T1.2.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.2.1.m1.1.1">normal-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.3.3.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.3.3.1">PPL<math alttext="\downarrow" class="ltx_Math" display="inline" id="S2.T1.3.3.3.1.m1.1"><semantics id="S2.T1.3.3.3.1.m1.1a"><mo id="S2.T1.3.3.3.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S2.T1.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.1.m1.1b"><ci id="S2.T1.3.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.3.1.m1.1.1">normal-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.3.4.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text" id="S2.T1.3.4.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.4.2" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.4.3" style="padding-left:10.0pt;padding-right:10.0pt;">2.08 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.4.4" style="padding-left:10.0pt;padding-right:10.0pt;">1.18 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.4.5" style="padding-left:10.0pt;padding-right:10.0pt;">12.33</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.5">
<td class="ltx_td ltx_align_left" id="S2.T1.3.5.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.5.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.5.2" style="padding-left:10.0pt;padding-right:10.0pt;">700M</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.5.3" style="padding-left:10.0pt;padding-right:10.0pt;">0.80 (2.60x)</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.5.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.96 (1.23x)</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.5.5" style="padding-left:10.0pt;padding-right:10.0pt;">12.87</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.3.6.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text" id="S2.T1.3.6.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.6.2" style="padding-left:10.0pt;padding-right:10.0pt;">1.3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.6.3" style="padding-left:10.0pt;padding-right:10.0pt;">3.34 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.6.4" style="padding-left:10.0pt;padding-right:10.0pt;">1.62 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.6.5" style="padding-left:10.0pt;padding-right:10.0pt;">11.25</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.7">
<td class="ltx_td ltx_align_left" id="S2.T1.3.7.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.7.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.7.2" style="padding-left:10.0pt;padding-right:10.0pt;">1.3B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.7.3" style="padding-left:10.0pt;padding-right:10.0pt;">1.14 (2.93x)</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.7.4" style="padding-left:10.0pt;padding-right:10.0pt;">0.97 (1.67x)</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.7.5" style="padding-left:10.0pt;padding-right:10.0pt;">11.29</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.3.8.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text" id="S2.T1.3.8.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.8.2" style="padding-left:10.0pt;padding-right:10.0pt;">3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.8.3" style="padding-left:10.0pt;padding-right:10.0pt;">7.89 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.8.4" style="padding-left:10.0pt;padding-right:10.0pt;">5.07 (1.00x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.3.8.5" style="padding-left:10.0pt;padding-right:10.0pt;">10.04</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.9">
<td class="ltx_td ltx_align_left" id="S2.T1.3.9.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.9.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.9.2" style="padding-left:10.0pt;padding-right:10.0pt;">3B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.9.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.9.3.1">2.22 (3.55x)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.9.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.9.4.1">1.87 (2.71x)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.3.9.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.9.5.1">9.91</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.3.10.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.10.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.10.2" style="padding-left:10.0pt;padding-right:10.0pt;">3.9B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.10.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.10.3.1">2.38 (3.32x)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.10.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.10.4.1">2.11 (2.40x)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.3.10.5" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.3.10.5.1">9.62</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.7.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S2.T1.8.2" style="font-size:90%;">Perplexity as well as the cost of <span class="ltx_text" id="S2.T1.8.2.1">BitNet b1.58</span> and <span class="ltx_text" id="S2.T1.8.2.2">LLaMA LLM</span>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T2.2">
<tbody><tr class="ltx_tr" id="S2.T2.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.2.1">Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.3"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.3.1">ARCe</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.4"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.4.1">ARCc</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.5"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.5.1">HS</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.6"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.6.1">BQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.7"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.7.1">OQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.8"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.8.1">PQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.9"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.9.1">WGe</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.1.10"><span class="ltx_text ltx_font_bold" id="S2.T2.2.1.10.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.2.2.1"><span class="ltx_text" id="S2.T2.2.2.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.2">700M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.3">54.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.4">23.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.5">37.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.6">60.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.7">20.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.8">68.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.9">54.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.2.10">45.5</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.3">
<td class="ltx_td ltx_align_left" id="S2.T2.2.3.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.3.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2">700M</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.3">51.8</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.4">21.4</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.5">35.1</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.6">58.2</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.7">20.0</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.8">68.1</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.9">55.2</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.10">44.3</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.2.4.1"><span class="ltx_text" id="S2.T2.2.4.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.2">1.3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.3">56.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.4">23.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.5">38.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.6">59.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.7">21.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.8">70.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.9">53.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.4.10">46.2</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.5">
<td class="ltx_td ltx_align_left" id="S2.T2.2.5.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.5.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.2">1.3B</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.3">54.9</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.4">24.2</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.5">37.7</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.6">56.7</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.7">19.6</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.8">68.8</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.9">55.8</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.5.10">45.4</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.2.6.1"><span class="ltx_text" id="S2.T2.2.6.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.2">3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.3">62.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.4">25.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.5">43.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.6">61.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.7">24.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.8">72.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.9">58.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.2.6.10">49.7</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.7">
<td class="ltx_td ltx_align_left" id="S2.T2.2.7.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.2">3B</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.3"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.3.1">61.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.4"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.4.1">28.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.5"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.5.1">42.9</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.6"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.6.1">61.5</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.7"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.7.1">26.6</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.8"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.8.1">71.5</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.9"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.9.1">59.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.7.10"><span class="ltx_text ltx_font_bold" id="S2.T2.2.7.10.1">50.2</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.2.8.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.2">3.9B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.3"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.3.1">64.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.4"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.4.1">28.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.5"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.5.1">44.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.6"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.6.1">63.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.7"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.7.1">24.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.8"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.8.1">73.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.9"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.9.1">60.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.8.10"><span class="ltx_text ltx_font_bold" id="S2.T2.2.8.10.1">51.2</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.5.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S2.T2.6.2" style="font-size:90%;">Zero-shot accuracy of <span class="ltx_text" id="S2.T2.6.2.1">BitNet b1.58</span> and <span class="ltx_text" id="S2.T2.6.2.2">LLaMA LLM</span> on the end tasks.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">우리는 <span class="ltx_text" id="S3.p1.1.1">BitNet b1.58</span>을 다양한 크기로 재생된 FP16 <span class="ltx_text" id="S3.p1.1.2">LLaMA LLM</span>과 비교했다. 공정한 비교를 보장하기 위해 1,000억 토큰에 대해 RedPajama 데이터 세트 [<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib4" title="">4</a>]</cite>에서 모델을 사전 훈련했다. ARC-Easy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib25" title="">25</a>]</cite>, ARC-Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib25" title="">25</a>]</cite>, Hellaswag <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib26" title="">26</a>]</cite>, PIQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib1" title="">1</a>]</cite>, OpenbookQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib10" title="">10</a>]</cite>, BoolQ <cite class="ltx_cite ltx_citemacro_cite">[<a idx=7></ 또한 WikiText2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib11" title="">11</a>]</cite> 및 C4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib13" title="">13</a>]</cite> datasets에 대한 유효성 검사 perplexity를 보고했다.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text" id="S3.p2.1.1">LLaMA LLM</span>과 <span class="ltx_text" id="S3.p2.1.2">BitNet b1.58</span>의 런타임 GPU 메모리와 대기 시간을 비교했다. 결과는 GPU 장치에서 LLM 추론 지연에 잘 최적화된 FasterTransformer<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA/FasterTransformer" title="">https://github.com/NVIDIA/FasterTransformer</a></span></span></span> codebase를 사용하여 측정되었다. Ladder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib22" title="">22</a>]</cite>의 2비트 커널도 <span class="ltx_text" id="S3.p2.1.3">BitNet b1.58</span>에 대해 통합됩니다. 추론을 위한 주요 비용이기 때문에 출력 토큰당 시간을 보고했다.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S2.T1" title="Table 1 ‣ LLaMA-alike Components. ‣ 2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">1</span></a>는 <span class="ltx_text" id="S3.p3.1.1">BitNet b1.58</span> 및 <span class="ltx_text" id="S3.p3.1.2">LLaMA LLM</span>에 대한 perplexity 및 cost를 요약한 것이다. <span class="ltx_text" id="S3.p3.1.3">BitNet b1.58</span>이 perplexity 측면에서 3B 모델 크기에서 full precision <span class="ltx_text" id="S3.p3.1.4">LLaMA LLM</span>과 일치하기 시작하는 반면 GPU 메모리는 2.71배 더 빠르고 3.55배 더 적게 사용한다. 특히, 3.9B 모델 크기를 갖는 <span class="ltx_text" id="S3.p3.1.5">BitNet b1.58</span>은 2.4배 빠르고, 3.32배 적은 메모리를 소비하지만, <span class="ltx_text" id="S3.p3.1.6">LLaMA LLM</span> 3B보다 상당히 우수한 성능을 보인다.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S2.T2" title="Table 2 ‣ LLaMA-alike Components. ‣ 2 BitNet b1.58 ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">2</span></a>는 종료 태스크에 대한 제로 샷 정확도의 상세 결과를 보고한다. 평가를 수행하기 위해 <em class="ltx_emph ltx_font_italic" id="S3.p4.1.1">lm-evaluation-harness</em><span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EleutherAI/lm-evaluation-harness" title="">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span>의 파이프라인을 따랐다. 결과는 모델 크기가 증가함에 따라 <span class="ltx_text" id="S3.p4.1.2">BitNet b1.58</span>과 <span class="ltx_text" id="S3.p4.1.3">LLaMA LLM</span> 사이의 성능 격차가 좁혀짐을 보여준다. 더 중요한 것은 <span class="ltx_text" id="S3.p4.1.4">BitNet b1.58</span>은 3B 크기에서 시작하는 전체 정밀 기준선의 성능과 일치할 수 있다는 것입니다. 퍼플렉시티의 관찰과 유사하게, 엔드-태스크 결과는 더 낮은 메모리 및 레이턴시 비용을 갖는 <span class="ltx_text" id="S3.p4.1.5">BitNet b1.58</span> 3.9B가 <span class="ltx_text" id="S3.p4.1.6">LLaMA LLM</span> 3B를 능가함을 드러낸다. 이는 <span class="ltx_text" id="S3.p4.1.7">BitNet b1.58</span>이 최신 LLM 모델에 비해 Pareto 개선임을 보여줍니다.</p>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Memory and Latency</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="305" id="S3.F2.g1" src="https://arxiv.org/html/2402.17764v1/x3.png" width="407"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="305" id="S3.F2.g2" src="https://arxiv.org/html/2402.17764v1/x4.png" width="407"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.4.2" style="font-size:90%;">Decoding latency (Left) and memory consumption (Right) of <span class="ltx_text" id="S3.F2.4.2.1">BitNet b1.58</span> varying the model size.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">우리는 모델 크기를 7B, 13B 및 70B로 추가로 확장하고 비용을 평가했다. 그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.F2" title="Figure 2 ‣ Memory and Latency ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">2</span></a>는 지연 시간과 메모리의 추세를 보여 모델 크기 축척에 따라 속도가 증가한다는 것을 보여준다. 특히 <span class="ltx_text" id="S3.SS0.SSS0.Px1.p1.1.1">BitNet b1.58</span> 70B는 <span class="ltx_text" id="S3.SS0.SSS0.Px1.p1.1.2">LLaMA LLM</span> baseline보다 4.1배 빠르다. <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p1.1.3">nn.Linear</em>이 모델 크기에 따라 커지기 때문입니다. 메모리 소비는 임베딩이 완전한 정밀도를 유지하고 더 큰 모델의 경우 메모리 비율이 더 작기 때문에 유사한 경향을 따른다. 레이턴시와 메모리 모두 2비트 커널로 측정되었기 때문에 여전히 최적화를 통해 비용을 더 줄일 수 있는 여지가 있다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Energy</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">또한 <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.1">BitNet b1.58</span>과 <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.2">LLaMA LLM</span>의 산술 연산 에너지 소비를 추정한다. 우리는 행렬 곱셈이 LLM의 비용에 가장 크게 기여하기 때문에 행렬 곱셈의 계산에 주로 초점을 맞춘다. <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.F3" title="Figure 3 ‣ Energy ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">3</span></a>는 에너지 비용의 구성을 예시한다. <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.3">BitNet b1.58</span>의 대부분은 INT8 덧셈 계산인 반면, <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.4">LLaMA LLM</span>은 FP16 덧셈과 FP16 곱셈으로 구성된다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib28" title="">28</a>]</cite>, <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.5">BitNet b1.58</span>은 7nm 칩에서 행렬 곱셈을 위한 산술 연산 에너지 소비를 71.4배 절감한다. 512개의 토큰을 가진 모델에 대한 종단 간 에너지 비용을 추가로 보고했다. 결과는 모델 크기 척도로서 <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.6">BitNet b1.58</span>이 FP16 <span class="ltx_text" id="S3.SS0.SSS0.Px2.p1.1.7">LLaMA LLM</span> 기준선에 비해 에너지 소비 측면에서 점점 더 효율적이 됨을 보여준다. 이는 <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.1.8">nn.Linear</em>의 비율이 모델 크기에 따라 증가하는 반면 다른 구성 요소로부터의 비용은 더 큰 모델의 경우 더 작기 때문이다.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="305" id="S3.F3.g1" src="https://arxiv.org/html/2402.17764v1/x5.png" width="407"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="305" id="S3.F3.g2" src="https://arxiv.org/html/2402.17764v1/x6.png" width="407"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.4.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.5.2" style="font-size:90%;">Energy consumption of <span class="ltx_text" id="S3.F3.5.2.1">BitNet b1.58</span> compared to <span class="ltx_text" id="S3.F3.5.2.2">LLaMA LLM</span> at 7nm process nodes. On the left is the components of arithmetic operations energy. On the right is the end-to-end energy cost across different model sizes.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">Throughput</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.2">
<tbody><tr class="ltx_tr" id="S3.T3.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T3.2.1.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.2.1.2" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.2.1">Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.2.1.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.3.1">Max Batch Size</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.2.1.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.1.4.1">Throughput (tokens/s)</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.2.2.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text" id="S3.T3.2.2.1.1">LLaMA LLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.2" style="padding-left:10.0pt;padding-right:10.0pt;">70B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.3" style="padding-left:10.0pt;padding-right:10.0pt;">16 (1.0x)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.4" style="padding-left:10.0pt;padding-right:10.0pt;">&nbsp;&nbsp;333 (1.0x)</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T3.2.3.1" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.3.1.1">BitNet b1.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.3.2" style="padding-left:10.0pt;padding-right:10.0pt;">70B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.3.3" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.3.3.1">176 (11.0x)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.3.4" style="padding-left:10.0pt;padding-right:10.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.2.3.4.1">2977 (8.9x)</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.5.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.6.2" style="font-size:90%;">Comparison of the throughput between <span class="ltx_text" id="S3.T3.6.2.1">BitNet b1.58</span> 70B and <span class="ltx_text" id="S3.T3.6.2.2">LLaMA LLM</span> 70B.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">두 개의 80GB A100 카드에서 파이프라인 병렬성을 사용하여 <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.1">BitNet b1.58</span> 및 <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.2">LLaMA LLM</span>의 처리량을 70B 매개 변수와 비교하여 <span class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib6" title="">6</a>]</cite>를 사용하여 <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.3">LLaMA LLM</span> 70B를 장치에서 실행할 수 있습니다. 512의 시퀀스 길이로 GPU 메모리 제한에 도달할 때까지 배치 크기를 증가시켰습니다. 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.T3" title="Table 3 ‣ Throughput ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">3</span></a>는 <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.4">BitNet b1.58</span> 70B가 <span class="ltx_text" id="S3.SS0.SSS0.Px3.p1.1.5">LLaMA LLM</span>의 배치 크기보다 최대 11배 더 높은 처리량을 지원할 수 있음을 보여줍니다.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px3.p2.1.1">BitNet b1.58</span><span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px3.p2.1.2"> is enabling a new scaling law with respect to model performance and inference cost</span>. 참고로 그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.F2" title="Figure 2 ‣ Memory and Latency ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">2</span></a>와 <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.F3" title="Figure 3 ‣ Energy ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">3</span></a>의 결과를 바탕으로 1.58-bit와 16-bit에서 서로 다른 모델 크기 간에 다음과 같은 동등성을 가질 수 있다.</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">13B BitNet b1.58은 3B FP16 LLM보다 지연 시간, 메모리 사용량 및 에너지 소비 측면에서 더 효율적이다.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">30B BitNet b1.58은 7B FP16 LLM보다 지연 시간, 메모리 사용량 및 에너지 소비 측면에서 더 효율적이다.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">70B BitNet b1.58은 13B FP16 LLM보다 지연 시간, 메모리 사용량 및 에너지 소비 측면에서 더 효율적이다.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px4">
<h3 class="ltx_title ltx_title_paragraph">Training with 2T Tokens</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px4.p1.1">훈련 토큰의 수는 LLM에 중요한 요소이다. <span class="ltx_text" id="S3.SS0.SSS0.Px4.p1.1.1">BitNet b1.58</span>의 확장성을 토큰 측면에서 테스트하기 위해 StableLM-3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib17" title="">17</a>]</cite> 모델의 데이터 레시피에 따라 2T 토큰이 있는 <span class="ltx_text" id="S3.SS0.SSS0.Px4.p1.1.2">BitNet b1.58</span> 모델을 최신 오픈 소스 3B 모델인 [<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib17" title="">17</a>]</cite>를 학습했습니다. 두 모델 모두 Winogrande <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib15" title="">15</a>]</cite>, PIQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib1" title="">1</a>]</cite>, SciQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib21" title="">21</a>]</cite>, LAMBADA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib12" title="">12</a>]</cite>, ARC-easy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib25" title="">25</a>]</cite>로 구성된 벤치마크에서 평가되었다. 우리는 <a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#S3.T4" title="Table 4 ‣ Training with 2T Tokens ‣ 3 Results ‣ The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"><span class="ltx_text ltx_ref_tag">4</span></a> 표에서 제로 샷 정확도를 보고했다. 정확도와 정규화된 정확도로 측정된 작업에 대해 우리는 둘의 평균을 취한다. 2T 토큰에서 StableLM 3b의 결과는 기술 보고서에서 직접 가져옵니다. 연구 결과는 <span class="ltx_text" id="S3.SS0.SSS0.Px4.p1.1.3">BitNet b1.58</span>이 모든 엔드 태스크에서 우수한 성능을 달성하여 1.58비트 LLM도 강력한 일반화 기능을 가지고 있음을 보여준다.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T4.2" style="width:433.6pt;height:70.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(50.6pt,-8.2pt) scale(1.30411759293444,1.30411759293444) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.2.1">
<tbody><tr class="ltx_tr" id="S3.T4.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.2.1">Tokens</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.3.1">Winogrande</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.4.1">PIQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.5.1">SciQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.6.1">LAMBADA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.7"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.7.1">ARC-easy</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.1.1.8"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.1.8.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T4.2.1.2.1">StableLM-3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.2">2T</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.3">64.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.4">76.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.5">90.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.6">66.09</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.7">67.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.1.2.8">73.22</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T4.2.1.3.1">
<span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.1.1">BitNet b1.58</span> 3B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.2">2T</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.3"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.3.1">66.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.4"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.4.1">78.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.5"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.5.1">91.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.6"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.6.1">67.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.7"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.7.1">68.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.1.3.8"><span class="ltx_text ltx_font_bold" id="S3.T4.2.1.3.8.1">74.34</span></td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.4.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S3.T4.5.2" style="font-size:90%;">Comparison of <span class="ltx_text" id="S3.T4.5.2.1">BitNet b1.58</span> with StableLM-3B with 2T tokens.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion and Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">1-bit Mixture-of-Experts (MoE) LLMs</span></p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">혼합 전문가(Mixture-of-Experts, MoE)는 LLM에 대한 비용 효율적인 접근법임이 입증되었다. 이는 연산량(FLOPs)을 크게 감소시키지만, 높은 메모리 소모와 칩간 통신 오버헤드는 그 배치와 응용에 제한을 준다. 이러한 도전은 1.58비트 LLM에 의해 해결될 수 있다. 첫째, 메모리 공간을 줄이면 MoE 모델을 배포하는 데 필요한 장치의 수가 줄어듭니다. 또한, 네트워크를 통해 활성화를 전송하는 오버헤드를 크게 줄입니다. 결국 전체 모델을 단일 칩에 배치할 수 있다면 오버헤드가 없을 것이다.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Native Support of Long Sequence in LLMs</span></p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">LLM 시대에 긴 시퀀스를 처리할 수 있는 능력은 중요한 요구 사항이 되었다. 긴 시퀀스 추론의 한 가지 주요 과제는 KV 캐시에서 도입한 메모리 소비이다. <span class="ltx_text" id="S4.p4.1.1">BitNet b1.58</span>은 긴 시퀀스에 대한 네이티브 지원을 향한 중요한 단계를 나타내며, 이는 16비트에서 8비트로 활성화를 감소시켜 동일한 리소스가 주어졌을 때 컨텍스트 길이가 두 배가 되도록 하기 때문이다. 이것은 4비트로 더 무손실 압축되거나 1.58비트 LLM에 대해 더 낮게 압축될 수 있으며, 이는 향후 작업으로 남겨둔다.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.p5.1.1">LLMs on Edge and Mobile</span></p>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">1.58비트 LLM의 사용은 에지 및 모바일 장치에서 언어 모델의 성능을 크게 향상시킬 수 있는 잠재력을 가지고 있다. 이러한 디바이스들은 종종 그들의 메모리 및 계산 능력에 의해 제한되며, 이는 LLM들의 성능 및 스케일을 제한할 수 있다. 그러나, 1.58-비트 LLM들의 감소된 메모리 및 에너지 소비는 이들이 이들 디바이스들에 배치될 수 있게 하여, 이전에 가능하지 않았던 광범위한 애플리케이션들을 가능하게 한다. 이는 에지 및 모바일 디바이스의 능력을 크게 향상시키고 LLM의 새롭고 흥미진진한 애플리케이션을 가능하게 할 수 있다. 더욱이, 1.58-비트 LLMs는 에지 및 모바일 디바이스에서 사용되는 메인 프로세서인 CPU 디바이스에 보다 친근하다. 이는 <span class="ltx_text" id="S4.p6.1.1">BitNet b1.58</span>이 이러한 장치에서 효율적으로 실행되어 성능과 성능이 더욱 향상될 수 있음을 의미한다.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1"><span class="ltx_text ltx_font_bold" id="S4.p7.1.1">New Hardware for 1-bit LLMs</span></p>
</div>
<div class="ltx_para" id="S4.p8">
<p class="ltx_p" id="S4.p8.1">Groq<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://groq.com/" title="">https://groq.com/</a></span></span></span>과 같은 최근 연구는 LLMs에 대한 특정 하드웨어(예: LPU)를 구축할 수 있는 유망한 결과와 큰 잠재력을 보여주었다. 한 단계 더 나아가, 우리는 BitNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.17764v1#bib.bib23" title="">23</a>]</cite>에서 활성화된 새로운 계산 패러다임이 주어지면 1비트 LLMs에 특별히 최적화된 새로운 하드웨어 및 시스템을 설계하기 위한 액션을 구상하고 촉구한다.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BZB<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib1.2.2.m1.1"><semantics id="bib.bib1.2.2.m1.1a"><msup id="bib.bib1.2.2.m1.1.1" xref="bib.bib1.2.2.m1.1.1.cmml"><mi id="bib.bib1.2.2.m1.1.1a" xref="bib.bib1.2.2.m1.1.1.cmml"></mi><mo id="bib.bib1.2.2.m1.1.1.1" xref="bib.bib1.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib1.2.2.m1.1b"><apply id="bib.bib1.2.2.m1.1.1.cmml" xref="bib.bib1.2.2.m1.1.1"><plus id="bib.bib1.2.2.m1.1.1.1.cmml" xref="bib.bib1.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib1.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib1.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan&nbsp;Le Bras, Jianfeng Gao, and Yejin Choi.

</span>
<span class="ltx_bibblock">PIQA: reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1">CoRR</span>, abs/1911.11641, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CCKS [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher&nbsp;De Sa.

</span>
<span class="ltx_bibblock">QuIP: 2-bit quantization of large language models with guarantees.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">CoRR</span>, abs/2307.13304, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CLC<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib3.2.2.m1.1"><semantics id="bib.bib3.2.2.m1.1a"><msup id="bib.bib3.2.2.m1.1.1" xref="bib.bib3.2.2.m1.1.1.cmml"><mi id="bib.bib3.2.2.m1.1.1a" xref="bib.bib3.2.2.m1.1.1.cmml"></mi><mo id="bib.bib3.2.2.m1.1.1.1" xref="bib.bib3.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib3.2.2.m1.1b"><apply id="bib.bib3.2.2.m1.1.1.cmml" xref="bib.bib3.2.2.m1.1.1"><plus id="bib.bib3.2.2.m1.1.1.1.cmml" xref="bib.bib3.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib3.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib3.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no questions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.3.1">CoRR</span>, abs/1905.10044, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Com [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Together Computer.

</span>
<span class="ltx_bibblock">Redpajama: an open dataset for training large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FAHA [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">OPTQ: accurate quantization for generative pre-trained transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">The Eleventh International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HCB<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib6.2.2.m1.1"><semantics id="bib.bib6.2.2.m1.1a"><msup id="bib.bib6.2.2.m1.1.1" xref="bib.bib6.2.2.m1.1.1.cmml"><mi id="bib.bib6.2.2.m1.1.1a" xref="bib.bib6.2.2.m1.1.1.cmml"></mi><mo id="bib.bib6.2.2.m1.1.1.1" xref="bib.bib6.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib6.2.2.m1.1b"><apply id="bib.bib6.2.2.m1.1.1.cmml" xref="bib.bib6.2.2.m1.1.1"><plus id="bib.bib6.2.2.m1.1.1.1.cmml" xref="bib.bib6.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib6.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib6.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia&nbsp;Xu Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc&nbsp;V. Le, Yonghui Wu, and Zhifeng Chen.

</span>
<span class="ltx_bibblock">Gpipe: Efficient training of giant neural networks using pipeline parallelism.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.3.1">Advances in Neural Information Processing Systems</span>, pages 103–112, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hor [14]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mark Horowitz.

</span>
<span class="ltx_bibblock">1.1 computing’s energy problem (and what we can do about it).

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">2014 IEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest of Technical Papers, San Francisco, CA, USA, February 9-13, 2014</span>, pages 10–14, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KLZ<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib8.2.2.m1.1"><semantics id="bib.bib8.2.2.m1.1a"><msup id="bib.bib8.2.2.m1.1.1" xref="bib.bib8.2.2.m1.1.1.cmml"><mi id="bib.bib8.2.2.m1.1.1a" xref="bib.bib8.2.2.m1.1.1.cmml"></mi><mo id="bib.bib8.2.2.m1.1.1.1" xref="bib.bib8.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib8.2.2.m1.1b"><apply id="bib.bib8.2.2.m1.1.1.cmml" xref="bib.bib8.2.2.m1.1.1"><plus id="bib.bib8.2.2.m1.1.1.1.cmml" xref="bib.bib8.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib8.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib8.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.3.1">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LTT<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib9.2.2.m1.1"><semantics id="bib.bib9.2.2.m1.1a"><msup id="bib.bib9.2.2.m1.1.1" xref="bib.bib9.2.2.m1.1.1.cmml"><mi id="bib.bib9.2.2.m1.1.1a" xref="bib.bib9.2.2.m1.1.1.cmml"></mi><mo id="bib.bib9.2.2.m1.1.1.1" xref="bib.bib9.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib9.2.2.m1.1b"><apply id="bib.bib9.2.2.m1.1.1.cmml" xref="bib.bib9.2.2.m1.1.1"><plus id="bib.bib9.2.2.m1.1.1.1.cmml" xref="bib.bib9.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib9.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ji&nbsp;Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.

</span>
<span class="ltx_bibblock">AWQ: activation-aware weight quantization for LLM compression and acceleration.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1">CoRR</span>, abs/2306.00978, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MCKS [18]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? A new dataset for open book question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">CoRR</span>, abs/1809.02789, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MXBS [16]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.

</span>
<span class="ltx_bibblock">Pointer sentinel mixture models, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PKL<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib12.2.2.m1.1"><semantics id="bib.bib12.2.2.m1.1a"><msup id="bib.bib12.2.2.m1.1.1" xref="bib.bib12.2.2.m1.1.1.cmml"><mi id="bib.bib12.2.2.m1.1.1a" xref="bib.bib12.2.2.m1.1.1.cmml"></mi><mo id="bib.bib12.2.2.m1.1.1.1" xref="bib.bib12.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib12.2.2.m1.1b"><apply id="bib.bib12.2.2.m1.1.1.cmml" xref="bib.bib12.2.2.m1.1.1"><plus id="bib.bib12.2.2.m1.1.1.1.cmml" xref="bib.bib12.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib12.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib12.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [16]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan&nbsp;Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández.

</span>
<span class="ltx_bibblock">The LAMBADA dataset: Word prediction requiring a broad discourse context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers</span>. The Association for Computer Linguistics, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RSR<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib13.2.2.m1.1"><semantics id="bib.bib13.2.2.m1.1a"><msup id="bib.bib13.2.2.m1.1.1" xref="bib.bib13.2.2.m1.1.1.cmml"><mi id="bib.bib13.2.2.m1.1.1a" xref="bib.bib13.2.2.m1.1.1.cmml"></mi><mo id="bib.bib13.2.2.m1.1.1.1" xref="bib.bib13.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib13.2.2.m1.1b"><apply id="bib.bib13.2.2.m1.1.1.cmml" xref="bib.bib13.2.2.m1.1.1"><plus id="bib.bib13.2.2.m1.1.1.1.cmml" xref="bib.bib13.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib13.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib13.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1">CoRR</span>, abs/1910.10683, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SAL<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib14.2.2.m1.1"><semantics id="bib.bib14.2.2.m1.1a"><msup id="bib.bib14.2.2.m1.1.1" xref="bib.bib14.2.2.m1.1.1.cmml"><mi id="bib.bib14.2.2.m1.1.1a" xref="bib.bib14.2.2.m1.1.1.cmml"></mi><mo id="bib.bib14.2.2.m1.1.1.1" xref="bib.bib14.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib14.2.2.m1.1b"><apply id="bib.bib14.2.2.m1.1.1.cmml" xref="bib.bib14.2.2.m1.1.1"><plus id="bib.bib14.2.2.m1.1.1.1.cmml" xref="bib.bib14.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib14.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib14.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [24]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jianlin Su, Murtadha H.&nbsp;M. Ahmed, Yu&nbsp;Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.3.1">Neurocomputing</span>, 568:127063, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SBBC [20]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">WinoGrande: an adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">The Thirty-Fourth AAAI Conference on Artificial Intelligence</span>, pages 8732–8740, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sha [20]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">GLU variants improve transformer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">CoRR</span>, abs/2002.05202, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme.

</span>
<span class="ltx_bibblock">Stablelm 3b 4e1t.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TCS<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib18.2.2.m1.1"><semantics id="bib.bib18.2.2.m1.1a"><msup id="bib.bib18.2.2.m1.1.1" xref="bib.bib18.2.2.m1.1.1.cmml"><mi id="bib.bib18.2.2.m1.1.1a" xref="bib.bib18.2.2.m1.1.1.cmml"></mi><mo id="bib.bib18.2.2.m1.1.1.1" xref="bib.bib18.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib18.2.2.m1.1b"><apply id="bib.bib18.2.2.m1.1.1.cmml" xref="bib.bib18.2.2.m1.1.1"><plus id="bib.bib18.2.2.m1.1.1.1.cmml" xref="bib.bib18.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib18.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib18.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [24]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher&nbsp;De Sa.

</span>
<span class="ltx_bibblock">Quip#: Even better LLM quantization with hadamard incoherence and lattice codebooks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.3.1">CoRR</span>, abs/2402.04396, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TLI<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib19.2.2.m1.1"><semantics id="bib.bib19.2.2.m1.1a"><msup id="bib.bib19.2.2.m1.1.1" xref="bib.bib19.2.2.m1.1.1.cmml"><mi id="bib.bib19.2.2.m1.1.1a" xref="bib.bib19.2.2.m1.1.1.cmml"></mi><mo id="bib.bib19.2.2.m1.1.1.1" xref="bib.bib19.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib19.2.2.m1.1b"><apply id="bib.bib19.2.2.m1.1.1.cmml" xref="bib.bib19.2.2.m1.1.1"><plus id="bib.bib19.2.2.m1.1.1.1.cmml" xref="bib.bib19.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib19.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib19.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">LLaMA: open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.3.1">CoRR</span>, abs/2302.13971, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TMS<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib20.2.2.m1.1"><semantics id="bib.bib20.2.2.m1.1a"><msup id="bib.bib20.2.2.m1.1.1" xref="bib.bib20.2.2.m1.1.1.cmml"><mi id="bib.bib20.2.2.m1.1.1a" xref="bib.bib20.2.2.m1.1.1.cmml"></mi><mo id="bib.bib20.2.2.m1.1.1.1" xref="bib.bib20.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib20.2.2.m1.1b"><apply id="bib.bib20.2.2.m1.1.1.cmml" xref="bib.bib20.2.2.m1.1.1"><plus id="bib.bib20.2.2.m1.1.1.1.cmml" xref="bib.bib20.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib20.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib20.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian&nbsp;Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, and et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.3.1">CoRR</span>, abs/2307.09288, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WLG [17]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Johannes Welbl, Nelson&nbsp;F. Liu, and Matt Gardner.

</span>
<span class="ltx_bibblock">Crowdsourcing multiple choice science questions.

</span>
<span class="ltx_bibblock">In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017</span>, pages 94–106. Association for Computational Linguistics, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WMC<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib22.2.2.m1.1"><semantics id="bib.bib22.2.2.m1.1a"><msup id="bib.bib22.2.2.m1.1.1" xref="bib.bib22.2.2.m1.1.1.cmml"><mi id="bib.bib22.2.2.m1.1.1a" xref="bib.bib22.2.2.m1.1.1.cmml"></mi><mo id="bib.bib22.2.2.m1.1.1.1" xref="bib.bib22.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib22.2.2.m1.1b"><apply id="bib.bib22.2.2.m1.1.1.cmml" xref="bib.bib22.2.2.m1.1.1"><plus id="bib.bib22.2.2.m1.1.1.1.cmml" xref="bib.bib22.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib22.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib22.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lei Wang, Lingxiao Ma, Shijie Cao, Ningxin Zheng, Quanlu Zhang, Jilong Xue, Ziming Miao, Ting Cao, , and Yuqing Yang.

</span>
<span class="ltx_bibblock">Ladder: Efficient tensor compilation on customized data format.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.3.1">OSDI</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WMD<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib23.2.2.m1.1"><semantics id="bib.bib23.2.2.m1.1a"><msup id="bib.bib23.2.2.m1.1.1" xref="bib.bib23.2.2.m1.1.1.cmml"><mi id="bib.bib23.2.2.m1.1.1a" xref="bib.bib23.2.2.m1.1.1.cmml"></mi><mo id="bib.bib23.2.2.m1.1.1.1" xref="bib.bib23.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib23.2.2.m1.1b"><apply id="bib.bib23.2.2.m1.1.1.cmml" xref="bib.bib23.2.2.m1.1.1"><plus id="bib.bib23.2.2.m1.1.1.1.cmml" xref="bib.bib23.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib23.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib23.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hongyu Wang, Shuming Ma, Li&nbsp;Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi&nbsp;Wu, and Furu Wei.

</span>
<span class="ltx_bibblock">Bitnet: Scaling 1-bit transformers for large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.3.1">CoRR</span>, abs/2310.11453, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">XLS<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib24.2.2.m1.1"><semantics id="bib.bib24.2.2.m1.1a"><msup id="bib.bib24.2.2.m1.1.1" xref="bib.bib24.2.2.m1.1.1.cmml"><mi id="bib.bib24.2.2.m1.1.1a" xref="bib.bib24.2.2.m1.1.1.cmml"></mi><mo id="bib.bib24.2.2.m1.1.1.1" xref="bib.bib24.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib24.2.2.m1.1b"><apply id="bib.bib24.2.2.m1.1.1.cmml" xref="bib.bib24.2.2.m1.1.1"><plus id="bib.bib24.2.2.m1.1.1.1.cmml" xref="bib.bib24.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib24.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib24.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guangxuan Xiao, Ji&nbsp;Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han.

</span>
<span class="ltx_bibblock">SmoothQuant: accurate and efficient post-training quantization for large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.3.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">YBS [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vikas Yadav, Steven Bethard, and Mihai Surdeanu.

</span>
<span class="ltx_bibblock">Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering.

</span>
<span class="ltx_bibblock">In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">EMNLP-IJCNLP</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ZHB<math alttext="{}^{+}" class="ltx_Math" display="inline" id="bib.bib26.2.2.m1.1"><semantics id="bib.bib26.2.2.m1.1a"><msup id="bib.bib26.2.2.m1.1.1" xref="bib.bib26.2.2.m1.1.1.cmml"><mi id="bib.bib26.2.2.m1.1.1a" xref="bib.bib26.2.2.m1.1.1.cmml"></mi><mo id="bib.bib26.2.2.m1.1.1.1" xref="bib.bib26.2.2.m1.1.1.1.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="bib.bib26.2.2.m1.1b"><apply id="bib.bib26.2.2.m1.1.1.cmml" xref="bib.bib26.2.2.m1.1.1"><plus id="bib.bib26.2.2.m1.1.1.1.cmml" xref="bib.bib26.2.2.m1.1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib26.2.2.m1.1c">{}^{+}</annotation><annotation encoding="application/x-llamapun" id="bib.bib26.2.2.m1.1d">start_FLOATSUPERSCRIPT + end_FLOATSUPERSCRIPT</annotation></semantics></math> [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">HellaSwag: can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 57th Conference of the Association for Computational Linguistics</span>, pages 4791–4800, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ZS [19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock">In Hanna&nbsp;M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily&nbsp;B. Fox, and Roman Garnett, editors, <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Advances in Neural Information Processing Systems</span>, pages 12360–12371, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ZZL [22]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yichi Zhang, Zhiru Zhang, and Lukasz Lew.

</span>
<span class="ltx_bibblock">PokeBNN: A binary pursuit of lightweight accuracy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 12465–12475. IEEE, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
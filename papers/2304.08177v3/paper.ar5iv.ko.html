<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.08177] Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</title><meta property="og:description" content="Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI).
However, the expensive training and…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.08177">

<!--Generated on Fri May  5 12:36:08 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiming Cui 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">ymcui@ieee.org</span> &amp;Ziqing Yang<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">ziqingyang@gmail.com</span> &amp;Xin Yao 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">yaoxin94@foxmail.com</span>
</span><span class="ltx_author_notes">Equal contributions.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">ChatGPT 및 GPT-4와 같은 대형 언어 모델(LLM)은 자연어 처리 연구에 혁명을 일으켰고 인공지능(AGI)에서 잠재력을 입증했다. 그러나 LLM의 비싼 교육 및 배포는 투명하고 열린 학술 연구에 대한 도전을 제시한다. 이러한 문제를 해결하기 위해 이 프로젝트는 중국 LLaMA 및 알파카 대형 모델을 공개하여 지침 미세 조정을 강조한다. 20K 중국어 토큰을 추가해 인코딩 효율을 높이고 기본적인 의미 이해를 높여 원 LLaMA의 중국어 어휘를 확장한다. 중국어 데이터를 이용한 2차 사전 훈련과 중국어 명령어 데이터와 미세 조정을 통합함으로써 모델의 명령어 이해와 실행을 크게 향상시킨다. 우리의 파일럿 연구는 연구자들이 LLaMA 및 알파카 모델을 다른 언어에 적응시키는 기초 역할을 한다. 자원은 깃허브를 통해 공개적으로 이용 가능하며 중국 NLP 커뮤니티 및 그 이상에서 열린 연구를 육성한다. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>GitHub repository: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca" target="_blank" title="">https://github.com/ymcui/Chinese-LLaMA-Alpaca</a></span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S1.p1.1">자연어 처리(NLP) 분야는 LLM(Large Language Models)의 등장과 함께 변형적 패러다임 전환을 겪었다. 광범위한 크기와 광범위한 훈련 데이터로 특징지어지는 이 모델들은 인간과 같은 텍스트를 이해하고 생성하는 데 놀라운 능력을 보여주었다. BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="#bib.bib5" title="">2019</a>)</cite>와 같이 텍스트 이해를 위한 사전 훈련된 언어 모델과 달리 GPT 시리즈 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="#bib.bib13" title="">2018</a>)</cite>는 텍스트 생성 능력에 초점을 맞추고 있어 상대보다 창의성에 더 적합한 테스트베드가 된다. GPT 계열의 최신 LLM으로서 ChatGPT 및 GPT-4는 상당한 관심을 끌었고 빠르게 진화하는 이 도메인의 주요 사례로 부상했다.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S1.p2.1">GPT-3.5 <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>)</cite> 아키텍처에 구축된 ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib10" title="">2022</a>)</cite>는 상황 인식, 인간과 유사한 상호 작용에 참여할 수 있는 고급 대화형 AI 모델이다. 그것의 성공은 더 정교한 LLM인 GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib11" title="">2023</a>)</cite>의 개발을 위한 길을 열어주었으며, 이는 자연어 이해, 생성 및 다양한 NLP 작업에서 훨씬 더 큰 잠재력을 보여주었다. 두 모델 모두 새로운 연구 및 응용의 길을 열어 인공지능(AGI)의 능력을 탐구하는 데 관심을 불러일으켰다. 이러한 LLM은 여러 벤치마크에서 인상적인 성능을 보여주었을 뿐만 아니라 소수의 샷 학습과 새로운 작업에 적응할 수 있는 능력을 보여주었다. 그 결과, 그들은 NLP 연구의 확장에 크게 기여했으며, 연구자와 업계 전문가 모두에게 감성 분석 및 기계 번역에서 질문 응답 시스템 및 그 이상에 이르기까지 광범위한 응용 분야에서 잠재력을 탐색하고 활용하도록 영감을 주었다.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S1.p3.1">LLM이 가져온 놀라운 발전에도 불구하고 이러한 모델은 투명하고 열린 연구를 방해하는 특정 한계를 가지고 있다. 가장 주목할 만한 관심사 중 하나는 모델에 대한 접근을 제한하고 성공을 기반으로 하는 광범위한 연구 커뮤니티의 능력을 방해하는 독점적 특성이다. 또한, 이러한 모델을 훈련하고 배포하는 데 필요한 막대한 계산 리소스는 제한된 리소스를 가진 연구자에게 문제를 제기하여 접근성 문제를 더욱 악화시킨다.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S1.p4.1">이러한 한계에 대응하여 NLP 연구 커뮤니티는 더 큰 투명성과 협력을 촉진하기 위해 오픈 소스 대안으로 전환했다. 이러한 두 가지 예는 LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="#bib.bib17" title="">2023</a>)</cite> 및 Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et al., <a class="ltx_ref" href="#bib.bib16" title="">2023</a>)</cite>이며, 여기서 Alpaca 모델은 명령 데이터로 LLaMA 상에서 추가로 미세 조정된다. 이러한 오픈 소스 LLM은 학술 연구를 촉진하고 NLP 분야의 발전을 가속화하기 위해 설계되었다. 이러한 모델을 오픈 소싱함으로써 NLP 커뮤니티는 모델 개발, 미세 조정 및 평가의 추가 발전을 장려하는 환경을 조성하여 궁극적으로 광범위한 응용 프로그램에서 활용할 수 있는 보다 강력하고 유능한 LLM으로 이어지는 것을 목표로 한다.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S1.p5.1">LLaMA와 Alpaca는 NLP의 세계에서 상당한 진전을 이루었지만 중국어 활동을 원천적으로 지원하는 데 있어 내재적 한계를 가지고 있다. 원본 모델은 단어에 몇 백 개의 중국 토큰만 포함되어 있어 중국 텍스트 <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We will discuss this issue in Section 2.</span></span></span> 인코딩 및 디코딩의 효율성을 크게 방해한다. 이 기술 보고서에서는 중국 BERT 시리즈 <cite class="ltx_cite ltx_citemacro_citep">(Cui et al., <a class="ltx_ref" href="#bib.bib2" title="">2021</a>)</cite>와 중국 소수자 중심의 다국어 사전 훈련 모델 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib21" title="">2022</a>)</cite>에 대한 이전 연구를 바탕으로 중국어의 이해와 생성 능력이 향상된 중국어 LLaMA와 Alpaca를 제안한다. 우리는 원래 LLaMA의 어휘를 추가 20K 중국어 토큰으로 확장하여 중국어 텍스트를 처리하고 생성하는 능력을 크게 향상시킵니다. 중국 LLaMA 모델과 Alpaca 모델의 효율적인 훈련과 배치를 보장하기 위해 과도한 계산 비용을 초래하지 않고 모델을 훈련하고 미세 조정할 수 있는 LoRA(Low-Rank Adaptation) 접근법 <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="#bib.bib8" title="">2021</a>)</cite>를 채택한다. LLaMA 및 알파카 모델의 중국어 이해와 생성 능력을 향상시키는 우리의 파일럿 연구는 이러한 모델을 다른 언어에 적용하려는 연구자들에게 기초가 될 수 있다. 본 논문의 접근 방법의 타당성과 효율성을 입증함으로써 어휘를 확장하고 다양한 언어에서 LLaMA 및 Alpaca 모델의 성능을 개선하는 데 적용할 수 있는 통찰력과 방법론을 제공한다.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S1.p6.1">요약하면, 본 기술보고서의 기여는 다음과 같다:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i1.p1.1">추가 2만 개의 중국어 토큰으로 원 LLaMA의 어휘를 확장하여 중국어 인코딩 및 디코딩 효율을 높이고, LLaMA의 중국어 이해 능력을 향상시킨다.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i2.p1.1">우리는 중국 LLaMA 및 Alpaca 모델의 효율적인 훈련 및 배치를 위해 LoRA(Low-Rank Adaptation) 접근법을 채택하여 연구자들이 과도한 계산 비용을 초래하지 않고 이러한 모델을 사용할 수 있도록 한다.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i3.p1.1">다양한 자연어 이해(NLU) 및 자연어 생성(NLG) 작업에 대한 중국어 Alpaca 7B 및 13B 모델의 성능을 평가하며, 이는 중국어 작업의 맥락에서 원래 LLaMA 대응물에 비해 상당한 개선을 보여준다.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S1.I1.i4.p1.1">우리는 연구의 자원과 결과를 공개적으로 사용할 수 있도록 하여 NLP 커뮤니티 내에서 추가 연구 및 협력을 촉진하고 LLaMA 및 알파카 모델을 다른 언어에 적응하도록 장려한다.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Chinese LLaMA</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p1.1">LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="#bib.bib17" title="">2023</a>)</cite>는 트랜스포머 아키텍처 <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="#bib.bib18" title="">2017</a>)</cite>를 기반으로 하는 디코더 전용 기초 대용량 언어 모델이다. 다른 변압기 기반 LLM과 유사하게, LLaMA는 임베딩 층, 다수의 변압기 블록들, 및 LM 헤드 층을 포함한다. 또한 Pre-normalization <cite class="ltx_cite ltx_citemacro_citep">(Zhang &amp; Sennrich, <a class="ltx_ref" href="#bib.bib23" title="">2019</a>)</cite>, SwiGLU activation <cite class="ltx_cite ltx_citemacro_citep">(Shazeer, <a class="ltx_ref" href="#bib.bib14" title="">2020</a>)</cite>, Rotary Embeddings <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a class="ltx_ref" href="#bib.bib15" title="">2021</a>)</cite> 등의 다양한 개선사항을 통합하였다. LLaMA의 총 매개변수 수는 7B에서 65B 사이이다. 실험은 LLaMA가 더 작은 모델 크기를 유지하면서 GPT-3와 같은 다른 LLM에 비해 경쟁 성능을 달성한다는 것을 보여준다.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p2.1">LLaMA는 공개적으로 사용 가능한 말뭉치의 1T에서 1.4T 토큰에 대해 사전 훈련되었으며 대부분의 데이터는 영어로, 라틴어 또는 키릴어 스크립트를 사용하여 다른 언어의 작은 부분만 사용된다. 이에 따라 LLaMA의 중국어 이해와 생성 능력은 제한적이다. 이를 해결하기 위해 중국어 코퍼라에 대한 LLaMA 모델을 사전 훈련하여 중국어 기본 이해와 생성 능력을 향상시킬 것을 제안한다.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p3.1">중국 말뭉치에 대한 LLaMA를 직접 사전 훈련하는 것은 몇 가지 도전에 직면해 있다. 첫째, 원래 LLaMA 토큰라이저 어휘에는 1,000자 미만의 한자가 있다. LLaMA 토큰화기는 바이트로 폴백하여 모든 한자를 지원하지만, 이러한 폴백 전략은 시퀀스 길이를 상당히 증가시키고 중국 텍스트에 대한 처리 효율을 늦춘다. 더욱이, 바이트 토큰들은 다른 UTF-8 토큰들을 나타내기 위해 또한 사용되기 때문에, 바이트 토큰들이 한자들의 의미적 의미를 학습하는 것을 어렵게 하기 때문에, 한자들을 나타내기 위해 배타적으로 설계되지 않는다.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p4.1">이러한 문제를 해결하기 위해 추가 중국 토큰을 사용하여 LLaMA 토큰화기를 확장하고 새 토큰화기에 대한 모델을 적응시키는 것을 제안합니다. <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib21" title="">2022</a>)</cite>:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S2.I1.i1.p1.1">본 논문에서는 중국어 말뭉치에서 SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo &amp; Richardson, <a class="ltx_ref" href="#bib.bib9" title="">2018</a>)</cite>로 중국어 토큰나이저를 학습하고, 어휘 크기 20,000을 이용하여 토큰나이저를 학습한 후, 이들의 어휘를 결합하여 중국어 토큰나이저를 원래의 LLaMA 토큰나이저로 병합한다. 궁극적으로, 우리는 49,953의 어휘 크기를 가진 중국 LLaMA 토큰라이저라고 부르는 병합된 토큰라이저를 얻는다.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.I1.i2.p1.4">중국어 LLaMA 토큰나이저에 대한 모델을 적응시키기 위해, 단어 임베딩 및 언어 모델 헤드를 형상 <math alttext="V\times H" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1.1"><semantics id="S2.I1.i2.p1.1.m1.1a"><mrow id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">V</mi><mo id="S2.I1.i2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i2.p1.1.m1.1.1.1.cmml">×</mo><mi id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><times id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1"></times><ci id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">𝑉</ci><ci id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">V\times H</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.1.m1.1d">italic_V × italic_H</annotation></semantics></math>에서 <math alttext="V^{\prime}\times H" class="ltx_Math" display="inline" id="S2.I1.i2.p1.2.m2.1"><semantics id="S2.I1.i2.p1.2.m2.1a"><mrow id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml"><msup id="S2.I1.i2.p1.2.m2.1.1.2" xref="S2.I1.i2.p1.2.m2.1.1.2.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.2.2" xref="S2.I1.i2.p1.2.m2.1.1.2.2.cmml">V</mi><mo id="S2.I1.i2.p1.2.m2.1.1.2.3" xref="S2.I1.i2.p1.2.m2.1.1.2.3.cmml">′</mo></msup><mo id="S2.I1.i2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i2.p1.2.m2.1.1.1.cmml">×</mo><mi id="S2.I1.i2.p1.2.m2.1.1.3" xref="S2.I1.i2.p1.2.m2.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><apply id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1"><times id="S2.I1.i2.p1.2.m2.1.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.1"></times><apply id="S2.I1.i2.p1.2.m2.1.1.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.I1.i2.p1.2.m2.1.1.2.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2">superscript</csymbol><ci id="S2.I1.i2.p1.2.m2.1.1.2.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.2">𝑉</ci><ci id="S2.I1.i2.p1.2.m2.1.1.2.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.3">′</ci></apply><ci id="S2.I1.i2.p1.2.m2.1.1.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">V^{\prime}\times H</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.2.m2.1d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT × italic_H</annotation></semantics></math>로 크기를 조정하며, 여기서 <math alttext="V=32,000" class="ltx_Math" display="inline" id="S2.I1.i2.p1.3.m3.2"><semantics id="S2.I1.i2.p1.3.m3.2a"><mrow id="S2.I1.i2.p1.3.m3.2.3" xref="S2.I1.i2.p1.3.m3.2.3.cmml"><mi id="S2.I1.i2.p1.3.m3.2.3.2" xref="S2.I1.i2.p1.3.m3.2.3.2.cmml">V</mi><mo id="S2.I1.i2.p1.3.m3.2.3.1" xref="S2.I1.i2.p1.3.m3.2.3.1.cmml">=</mo><mrow id="S2.I1.i2.p1.3.m3.2.3.3.2" xref="S2.I1.i2.p1.3.m3.2.3.3.1.cmml"><mn id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml">32</mn><mo id="S2.I1.i2.p1.3.m3.2.3.3.2.1" xref="S2.I1.i2.p1.3.m3.2.3.3.1.cmml">,</mo><mn id="S2.I1.i2.p1.3.m3.2.2" xref="S2.I1.i2.p1.3.m3.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.2b"><apply id="S2.I1.i2.p1.3.m3.2.3.cmml" xref="S2.I1.i2.p1.3.m3.2.3"><eq id="S2.I1.i2.p1.3.m3.2.3.1.cmml" xref="S2.I1.i2.p1.3.m3.2.3.1"></eq><ci id="S2.I1.i2.p1.3.m3.2.3.2.cmml" xref="S2.I1.i2.p1.3.m3.2.3.2">𝑉</ci><list id="S2.I1.i2.p1.3.m3.2.3.3.1.cmml" xref="S2.I1.i2.p1.3.m3.2.3.3.2"><cn id="S2.I1.i2.p1.3.m3.1.1.cmml" type="integer" xref="S2.I1.i2.p1.3.m3.1.1">32</cn><cn id="S2.I1.i2.p1.3.m3.2.2.cmml" type="integer" xref="S2.I1.i2.p1.3.m3.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.2c">V=32,000</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.3.m3.2d">italic_V = 32 , 000</annotation></semantics></math>는 원래 어휘 크기를 나타내고, <math alttext="V^{\prime}=49,953" class="ltx_Math" display="inline" id="S2.I1.i2.p1.4.m4.2"><semantics id="S2.I1.i2.p1.4.m4.2a"><mrow id="S2.I1.i2.p1.4.m4.2.3" xref="S2.I1.i2.p1.4.m4.2.3.cmml"><msup id="S2.I1.i2.p1.4.m4.2.3.2" xref="S2.I1.i2.p1.4.m4.2.3.2.cmml"><mi id="S2.I1.i2.p1.4.m4.2.3.2.2" xref="S2.I1.i2.p1.4.m4.2.3.2.2.cmml">V</mi><mo id="S2.I1.i2.p1.4.m4.2.3.2.3" xref="S2.I1.i2.p1.4.m4.2.3.2.3.cmml">′</mo></msup><mo id="S2.I1.i2.p1.4.m4.2.3.1" xref="S2.I1.i2.p1.4.m4.2.3.1.cmml">=</mo><mrow id="S2.I1.i2.p1.4.m4.2.3.3.2" xref="S2.I1.i2.p1.4.m4.2.3.3.1.cmml"><mn id="S2.I1.i2.p1.4.m4.1.1" xref="S2.I1.i2.p1.4.m4.1.1.cmml">49</mn><mo id="S2.I1.i2.p1.4.m4.2.3.3.2.1" xref="S2.I1.i2.p1.4.m4.2.3.3.1.cmml">,</mo><mn id="S2.I1.i2.p1.4.m4.2.2" xref="S2.I1.i2.p1.4.m4.2.2.cmml">953</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.4.m4.2b"><apply id="S2.I1.i2.p1.4.m4.2.3.cmml" xref="S2.I1.i2.p1.4.m4.2.3"><eq id="S2.I1.i2.p1.4.m4.2.3.1.cmml" xref="S2.I1.i2.p1.4.m4.2.3.1"></eq><apply id="S2.I1.i2.p1.4.m4.2.3.2.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2"><csymbol cd="ambiguous" id="S2.I1.i2.p1.4.m4.2.3.2.1.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2">superscript</csymbol><ci id="S2.I1.i2.p1.4.m4.2.3.2.2.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2.2">𝑉</ci><ci id="S2.I1.i2.p1.4.m4.2.3.2.3.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2.3">′</ci></apply><list id="S2.I1.i2.p1.4.m4.2.3.3.1.cmml" xref="S2.I1.i2.p1.4.m4.2.3.3.2"><cn id="S2.I1.i2.p1.4.m4.1.1.cmml" type="integer" xref="S2.I1.i2.p1.4.m4.1.1">49</cn><cn id="S2.I1.i2.p1.4.m4.2.2.cmml" type="integer" xref="S2.I1.i2.p1.4.m4.2.2">953</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.4.m4.2c">V^{\prime}=49,953</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.4.m4.2d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = 49 , 953</annotation></semantics></math>는 중국어 LLaMA 토큰나이저의 어휘 크기를 나타낸다. 새로운 행들은 원래의 임베딩 행렬들의 끝에 추가되어, 원래의 어휘 내의 토큰들의 임베딩들이 영향을 받지 않은 채로 유지되는 것을 보장한다.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p5.1">우리의 예비 실험은 중국-LLaMA 토큰화기에서 생성된 토큰의 수가 원래 LLaMA 토큰화기에서 생성된 토큰의 약 절반임을 보여준다. 표 <a class="ltx_ref" href="#S2.T1" title="Table 1 ‣ 2 Chinese LLaMA ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">1</span></a>는 원래 LLaMA tokenizer와 중국 LLaMA tokenizer 간의 예제 비교를 보여준다. 우리가 볼 수 있듯이 중국산 LLaMA 토큰나이저를 사용하면 원본에 비해 인코딩 길이가 크게 줄어든다. 고정 컨텍스트 길이가 주어지면, 모델은 약 2배 많은 정보를 수용할 수 있고, 생성 속도는 원래의 LLaMA 토큰화기에 비해 2배 더 빠르다. 이것은 LLaMA 모델의 중국 이해와 생성 능력을 향상시키는 데 제안된 접근법의 효율성을 보여준다.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p6.1">앞서 언급한 적응 단계를 완료한 후, 표준 캐주얼 언어 모델링(CLM) 태스크에서 중국어-LLaMA 토큰나이저를 사용하여 중국어-LLaMA 모델을 사전 훈련한다. 입력 토큰 시퀀스 <math alttext="\bm{x}=(x_{0},x_{1},x_{2},\ldots)" class="ltx_Math" display="inline" id="S2.p6.1.m1.4"><semantics id="S2.p6.1.m1.4a"><mrow id="S2.p6.1.m1.4.4" xref="S2.p6.1.m1.4.4.cmml"><mi id="S2.p6.1.m1.4.4.5" xref="S2.p6.1.m1.4.4.5.cmml">𝒙</mi><mo id="S2.p6.1.m1.4.4.4" xref="S2.p6.1.m1.4.4.4.cmml">=</mo><mrow id="S2.p6.1.m1.4.4.3.3" xref="S2.p6.1.m1.4.4.3.4.cmml"><mo id="S2.p6.1.m1.4.4.3.3.4" stretchy="false" xref="S2.p6.1.m1.4.4.3.4.cmml">(</mo><msub id="S2.p6.1.m1.2.2.1.1.1" xref="S2.p6.1.m1.2.2.1.1.1.cmml"><mi id="S2.p6.1.m1.2.2.1.1.1.2" xref="S2.p6.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S2.p6.1.m1.2.2.1.1.1.3" xref="S2.p6.1.m1.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S2.p6.1.m1.4.4.3.3.5" xref="S2.p6.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.p6.1.m1.3.3.2.2.2" xref="S2.p6.1.m1.3.3.2.2.2.cmml"><mi id="S2.p6.1.m1.3.3.2.2.2.2" xref="S2.p6.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S2.p6.1.m1.3.3.2.2.2.3" xref="S2.p6.1.m1.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S2.p6.1.m1.4.4.3.3.6" xref="S2.p6.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.p6.1.m1.4.4.3.3.3" xref="S2.p6.1.m1.4.4.3.3.3.cmml"><mi id="S2.p6.1.m1.4.4.3.3.3.2" xref="S2.p6.1.m1.4.4.3.3.3.2.cmml">x</mi><mn id="S2.p6.1.m1.4.4.3.3.3.3" xref="S2.p6.1.m1.4.4.3.3.3.3.cmml">2</mn></msub><mo id="S2.p6.1.m1.4.4.3.3.7" xref="S2.p6.1.m1.4.4.3.4.cmml">,</mo><mi id="S2.p6.1.m1.1.1" mathvariant="normal" xref="S2.p6.1.m1.1.1.cmml">…</mi><mo id="S2.p6.1.m1.4.4.3.3.8" stretchy="false" xref="S2.p6.1.m1.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.4b"><apply id="S2.p6.1.m1.4.4.cmml" xref="S2.p6.1.m1.4.4"><eq id="S2.p6.1.m1.4.4.4.cmml" xref="S2.p6.1.m1.4.4.4"></eq><ci id="S2.p6.1.m1.4.4.5.cmml" xref="S2.p6.1.m1.4.4.5">𝒙</ci><vector id="S2.p6.1.m1.4.4.3.4.cmml" xref="S2.p6.1.m1.4.4.3.3"><apply id="S2.p6.1.m1.2.2.1.1.1.cmml" xref="S2.p6.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p6.1.m1.2.2.1.1.1.1.cmml" xref="S2.p6.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.p6.1.m1.2.2.1.1.1.2.cmml" xref="S2.p6.1.m1.2.2.1.1.1.2">𝑥</ci><cn id="S2.p6.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S2.p6.1.m1.2.2.1.1.1.3">0</cn></apply><apply id="S2.p6.1.m1.3.3.2.2.2.cmml" xref="S2.p6.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p6.1.m1.3.3.2.2.2.1.cmml" xref="S2.p6.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.p6.1.m1.3.3.2.2.2.2.cmml" xref="S2.p6.1.m1.3.3.2.2.2.2">𝑥</ci><cn id="S2.p6.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S2.p6.1.m1.3.3.2.2.2.3">1</cn></apply><apply id="S2.p6.1.m1.4.4.3.3.3.cmml" xref="S2.p6.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.p6.1.m1.4.4.3.3.3.1.cmml" xref="S2.p6.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.p6.1.m1.4.4.3.3.3.2.cmml" xref="S2.p6.1.m1.4.4.3.3.3.2">𝑥</ci><cn id="S2.p6.1.m1.4.4.3.3.3.3.cmml" type="integer" xref="S2.p6.1.m1.4.4.3.3.3.3">2</cn></apply><ci id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1">…</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.4c">\bm{x}=(x_{0},x_{1},x_{2},\ldots)</annotation><annotation encoding="application/x-llamapun" id="S2.p6.1.m1.4d">bold_italic_x = ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … )</annotation></semantics></math>가 주어지면, 모델은 자동 회귀 방식으로 다음 토큰을 예측하도록 훈련된다. 목적은 다음과 같은 음의 로그 가능성을 최소화하는 것이다:</p>
</div>
<div id="S2.p7" class="ltx_para ltx_noindent">
<table id="Sx2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\textrm{CLM}}=-\sum_{i}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)" display="inline"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><msub id="S2.E1.m1.3.3.3" xref="S2.E1.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.3.3.3.2" xref="S2.E1.m1.3.3.3.2.cmml">ℒ</mi><mtext id="S2.E1.m1.3.3.3.3" xref="S2.E1.m1.3.3.3.3a.cmml">CLM</mtext></msub><mo id="S2.E1.m1.3.3.2" xref="S2.E1.m1.3.3.2.cmml">=</mo><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.cmml"><mo id="S2.E1.m1.3.3.1a" xref="S2.E1.m1.3.3.1.cmml">−</mo><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><mstyle displaystyle="true" id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.2.cmml"><munder id="S2.E1.m1.3.3.1.1.2a" xref="S2.E1.m1.3.3.1.1.2.cmml"><mo movablelimits="false" id="S2.E1.m1.3.3.1.1.2.2" xref="S2.E1.m1.3.3.1.1.2.2.cmml">∑</mo><mi id="S2.E1.m1.3.3.1.1.2.3" xref="S2.E1.m1.3.3.1.1.2.3.cmml">i</mi></munder></mstyle><mrow id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3.1" xref="S2.E1.m1.3.3.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E1.m1.3.3.1.1.1.3a" xref="S2.E1.m1.3.3.1.1.1.3.cmml">⁡</mo><mi id="S2.E1.m1.3.3.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.3.2.cmml">p</mi></mrow><mo id="S2.E1.m1.3.3.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.2.cmml" lspace="0px" rspace="0px"></mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.5" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.5.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.2.cmml">x</mi><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.5.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S2.E1.m1.3.3.1.1.1.1.1.1.4" xref="S2.E1.m1.3.3.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.4" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.5" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">…</mi><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.6" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml">−</mo><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.7" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">;</mo><mi mathvariant="normal" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">Θ</mi></mrow></mrow><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3"><eq id="S2.E1.m1.3.3.2.cmml" xref="S2.E1.m1.3.3.2"></eq><apply id="S2.E1.m1.3.3.3.cmml" xref="S2.E1.m1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3">subscript</csymbol><ci id="S2.E1.m1.3.3.3.2.cmml" xref="S2.E1.m1.3.3.3.2">ℒ</ci><ci id="S2.E1.m1.3.3.3.3a.cmml" xref="S2.E1.m1.3.3.3.3"><mtext mathsize="70%" id="S2.E1.m1.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3">CLM</mtext></ci></apply><apply id="S2.E1.m1.3.3.1.cmml" xref="S2.E1.m1.3.3.1"><minus id="S2.E1.m1.3.3.1.2.cmml" xref="S2.E1.m1.3.3.1"></minus><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1"><apply id="S2.E1.m1.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.2">subscript</csymbol><sum id="S2.E1.m1.3.3.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2"></sum><ci id="S2.E1.m1.3.3.1.1.2.3.cmml" xref="S2.E1.m1.3.3.1.1.2.3">𝑖</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.2"></times><apply id="S2.E1.m1.3.3.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3"><log id="S2.E1.m1.3.3.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.3.1"></log><ci id="S2.E1.m1.3.3.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2">𝑝</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.4">conditional</csymbol><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.5.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.5.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.5.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.2">𝑥</ci><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.5.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.3">𝑖</ci></apply><list id="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3"><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2">𝑥</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">…</ci><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2">𝑥</ci><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3"><minus id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2">𝑖</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">Θ</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\displaystyle\mathcal{L}_{\textrm{CLM}}=-\sum_{i}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">caligraphic_L start_POSTSUBSCRIPT CLM end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; roman_Θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p8" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p8.3">여기서, <math alttext="\Theta" class="ltx_Math" display="inline" id="S2.p8.1.m1.1"><semantics id="S2.p8.1.m1.1a"><mi id="S2.p8.1.m1.1.1" mathvariant="normal" xref="S2.p8.1.m1.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S2.p8.1.m1.1b"><ci id="S2.p8.1.m1.1.1.cmml" xref="S2.p8.1.m1.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.1.m1.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S2.p8.1.m1.1d">roman_Θ</annotation></semantics></math>는 모델 파라미터를 나타내고, <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.p8.2.m2.1"><semantics id="S2.p8.2.m2.1a"><msub id="S2.p8.2.m2.1.1" xref="S2.p8.2.m2.1.1.cmml"><mi id="S2.p8.2.m2.1.1.2" xref="S2.p8.2.m2.1.1.2.cmml">x</mi><mi id="S2.p8.2.m2.1.1.3" xref="S2.p8.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p8.2.m2.1b"><apply id="S2.p8.2.m2.1.1.cmml" xref="S2.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p8.2.m2.1.1.1.cmml" xref="S2.p8.2.m2.1.1">subscript</csymbol><ci id="S2.p8.2.m2.1.1.2.cmml" xref="S2.p8.2.m2.1.1.2">𝑥</ci><ci id="S2.p8.2.m2.1.1.3.cmml" xref="S2.p8.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.2.m2.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.p8.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>는 예측 대상 토큰이며, <math alttext="x_{0},x_{1},\ldots,x_{i-1}" class="ltx_Math" display="inline" id="S2.p8.3.m3.4"><semantics id="S2.p8.3.m3.4a"><mrow id="S2.p8.3.m3.4.4.3" xref="S2.p8.3.m3.4.4.4.cmml"><msub id="S2.p8.3.m3.2.2.1.1" xref="S2.p8.3.m3.2.2.1.1.cmml"><mi id="S2.p8.3.m3.2.2.1.1.2" xref="S2.p8.3.m3.2.2.1.1.2.cmml">x</mi><mn id="S2.p8.3.m3.2.2.1.1.3" xref="S2.p8.3.m3.2.2.1.1.3.cmml">0</mn></msub><mo id="S2.p8.3.m3.4.4.3.4" xref="S2.p8.3.m3.4.4.4.cmml">,</mo><msub id="S2.p8.3.m3.3.3.2.2" xref="S2.p8.3.m3.3.3.2.2.cmml"><mi id="S2.p8.3.m3.3.3.2.2.2" xref="S2.p8.3.m3.3.3.2.2.2.cmml">x</mi><mn id="S2.p8.3.m3.3.3.2.2.3" xref="S2.p8.3.m3.3.3.2.2.3.cmml">1</mn></msub><mo id="S2.p8.3.m3.4.4.3.5" xref="S2.p8.3.m3.4.4.4.cmml">,</mo><mi id="S2.p8.3.m3.1.1" mathvariant="normal" xref="S2.p8.3.m3.1.1.cmml">…</mi><mo id="S2.p8.3.m3.4.4.3.6" xref="S2.p8.3.m3.4.4.4.cmml">,</mo><msub id="S2.p8.3.m3.4.4.3.3" xref="S2.p8.3.m3.4.4.3.3.cmml"><mi id="S2.p8.3.m3.4.4.3.3.2" xref="S2.p8.3.m3.4.4.3.3.2.cmml">x</mi><mrow id="S2.p8.3.m3.4.4.3.3.3" xref="S2.p8.3.m3.4.4.3.3.3.cmml"><mi id="S2.p8.3.m3.4.4.3.3.3.2" xref="S2.p8.3.m3.4.4.3.3.3.2.cmml">i</mi><mo id="S2.p8.3.m3.4.4.3.3.3.1" xref="S2.p8.3.m3.4.4.3.3.3.1.cmml">−</mo><mn id="S2.p8.3.m3.4.4.3.3.3.3" xref="S2.p8.3.m3.4.4.3.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p8.3.m3.4b"><list id="S2.p8.3.m3.4.4.4.cmml" xref="S2.p8.3.m3.4.4.3"><apply id="S2.p8.3.m3.2.2.1.1.cmml" xref="S2.p8.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S2.p8.3.m3.2.2.1.1.1.cmml" xref="S2.p8.3.m3.2.2.1.1">subscript</csymbol><ci id="S2.p8.3.m3.2.2.1.1.2.cmml" xref="S2.p8.3.m3.2.2.1.1.2">𝑥</ci><cn id="S2.p8.3.m3.2.2.1.1.3.cmml" type="integer" xref="S2.p8.3.m3.2.2.1.1.3">0</cn></apply><apply id="S2.p8.3.m3.3.3.2.2.cmml" xref="S2.p8.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S2.p8.3.m3.3.3.2.2.1.cmml" xref="S2.p8.3.m3.3.3.2.2">subscript</csymbol><ci id="S2.p8.3.m3.3.3.2.2.2.cmml" xref="S2.p8.3.m3.3.3.2.2.2">𝑥</ci><cn id="S2.p8.3.m3.3.3.2.2.3.cmml" type="integer" xref="S2.p8.3.m3.3.3.2.2.3">1</cn></apply><ci id="S2.p8.3.m3.1.1.cmml" xref="S2.p8.3.m3.1.1">…</ci><apply id="S2.p8.3.m3.4.4.3.3.cmml" xref="S2.p8.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S2.p8.3.m3.4.4.3.3.1.cmml" xref="S2.p8.3.m3.4.4.3.3">subscript</csymbol><ci id="S2.p8.3.m3.4.4.3.3.2.cmml" xref="S2.p8.3.m3.4.4.3.3.2">𝑥</ci><apply id="S2.p8.3.m3.4.4.3.3.3.cmml" xref="S2.p8.3.m3.4.4.3.3.3"><minus id="S2.p8.3.m3.4.4.3.3.3.1.cmml" xref="S2.p8.3.m3.4.4.3.3.3.1"></minus><ci id="S2.p8.3.m3.4.4.3.3.3.2.cmml" xref="S2.p8.3.m3.4.4.3.3.3.2">𝑖</ci><cn id="S2.p8.3.m3.4.4.3.3.3.3.cmml" type="integer" xref="S2.p8.3.m3.4.4.3.3.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.3.m3.4c">x_{0},x_{1},\ldots,x_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S2.p8.3.m3.4d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math>는 컨텍스트를 구성한다.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 1:</span> 원본 LLaMA와 중국 LLaMA 간의 토큰화기 비교.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.2.1" class="ltx_text ltx_font_bold">Length</span></td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S2.T1.1.1.3.1" class="ltx_text ltx_font_bold">Content</span></td>
</tr>
<tr id="S2.T1.1.2" class="ltx_tr">
<td id="S2.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.1.2.1.1" class="ltx_text ltx_font_bold">Original Sentence</span></td>
<td id="S2.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">28</td>
<td id="S2.T1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">人工智能是计算机科学、心理学、哲学等学科融合的交叉学科。</td>
</tr>
<tr id="S2.T1.1.3" class="ltx_tr">
<td id="S2.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.1.3.1.1" class="ltx_text ltx_font_bold">Original Tokenizer</span></td>
<td id="S2.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.3.2.1" class="ltx_text">35</span></td>
<td id="S2.T1.1.3.3" class="ltx_td ltx_align_left ltx_border_t">
<span id="S2.T1.1.3.3.1" class="ltx_text"></span><span id="S2.T1.1.3.3.2" class="ltx_text">
<span id="S2.T1.1.3.3.2.1" class="ltx_tabular ltx_align_top">
<span id="S2.T1.1.3.3.2.1.1" class="ltx_tr">
<span id="S2.T1.1.3.3.2.1.1.1" class="ltx_td ltx_align_left">‘_’, ‘人’, ‘工’, ‘智’, ‘能’, ‘是’, ‘计’, ‘算’, ‘机’, ‘科’, ‘学’, ‘、’, ‘心’, ‘理’,</span></span>
<span id="S2.T1.1.3.3.2.1.2" class="ltx_tr">
<span id="S2.T1.1.3.3.2.1.2.1" class="ltx_td ltx_align_left">‘学’, ‘、’, ‘0xE5’, ‘0x93’, ‘0xB2’, ‘学’, ‘等’, ‘学’, ‘科’, ‘0xE8’, ‘0x9E’,</span></span>
<span id="S2.T1.1.3.3.2.1.3" class="ltx_tr">
<span id="S2.T1.1.3.3.2.1.3.1" class="ltx_td ltx_align_left">‘0x8D’, ‘合’, ‘的’, ‘交’, ‘0xE5’, ‘0x8F’, ‘0x89’, ‘学’, ‘科’, ‘。’</span></span>
</span></span> <span id="S2.T1.1.3.3.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S2.T1.1.4" class="ltx_tr">
<td id="S2.T1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="S2.T1.1.4.1.1" class="ltx_text ltx_font_bold">Chinese Tokenizer</span></td>
<td id="S2.T1.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S2.T1.1.4.2.1" class="ltx_text">16</span></td>
<td id="S2.T1.1.4.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S2.T1.1.4.3.1" class="ltx_text"></span><span id="S2.T1.1.4.3.2" class="ltx_text">
<span id="S2.T1.1.4.3.2.1" class="ltx_tabular ltx_align_top">
<span id="S2.T1.1.4.3.2.1.1" class="ltx_tr">
<span id="S2.T1.1.4.3.2.1.1.1" class="ltx_td ltx_align_left">‘_’, ‘人工智能’, ‘是’, ‘计算机’, ‘科学’, ‘、’, ‘心理学’, ‘、’, ‘哲学’, ‘等’,</span></span>
<span id="S2.T1.1.4.3.2.1.2" class="ltx_tr">
<span id="S2.T1.1.4.3.2.1.2.1" class="ltx_td ltx_align_left">‘学科’, ‘融合’, ‘的’, ‘交叉’, ‘学科’, ‘。’</span></span>
</span></span> <span id="S2.T1.1.4.3.3" class="ltx_text"></span>
</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Chinese Alpaca</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.p1.1">사전 훈련된 중국 LLaMA 모델을 얻은 후, 명령 후속 모델을 훈련하기 위해 자체 명령 미세 조정을 적용하기 위해 Stanford Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et al., <a class="ltx_ref" href="#bib.bib16" title="">2023</a>)</cite>에서 사용된 접근법을 따른다. 각 예는 명령어와 출력으로 구성된다. 우리는 명령을 모델에 입력하고 모델을 프롬프트하여 출력을 자동으로 생성합니다. 이 과정은 캐주얼 언어 모델링 작업과 유사하다. 자가 지시 미세 조정을 위해 스탠포드 알파카의 다음 프롬프트 템플릿을 채택하며, 이는 추론 중에도 활용된다:</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<blockquote id="S3.p2.1" class="ltx_quote">
<p class="ltx_p" id="S3.p2.1.1"><span class="ltx_text ltx_font_italic" id="S3.p2.1.1.1">아래는 작업을 설명하는 명령입니다. 요청을 적절하게 완료하는 응답을 작성합니다. <br class="ltx_break"/> <br class="ltx_break"/> <br class="ltx_break"/>### Instruction:  <br class="ltx_break"/></span>{<span class="ltx_text ltx_font_italic" id="S3.p2.1.2">instruction</span>}<span class="ltx_text ltx_font_italic" id="S3.p2.1.1.3"><br class="ltx_break"/> <br class="ltx_break"/>### Response:</span>{<span class="ltx_text ltx_font_italic" id="S3.p2.1.1.4">output</span>}<span class="ltx_text ltx_font_italic" id="S3.p2.1.1.5"></span></p>
</blockquote>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.p3.1">손실은 전체 텍스트의 <em class="ltx_emph ltx_font_italic" id="S3.p3.1.1">output</em> 부분에서만 계산되며 다음과 같이 나타낼 수 있습니다.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<table id="Sx2.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\textrm{SFT}}=-\sum_{i\in\textit{\{output\}}}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)" display="inline"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><msub id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.3.2" xref="S3.E2.m1.3.3.3.2.cmml">ℒ</mi><mtext id="S3.E2.m1.3.3.3.3" xref="S3.E2.m1.3.3.3.3a.cmml">SFT</mtext></msub><mo id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.cmml"><mo id="S3.E2.m1.3.3.1a" xref="S3.E2.m1.3.3.1.cmml">−</mo><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><munder id="S3.E2.m1.3.3.1.1.2a" xref="S3.E2.m1.3.3.1.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.3.3.1.1.2.2" xref="S3.E2.m1.3.3.1.1.2.2.cmml">∑</mo><mrow id="S3.E2.m1.3.3.1.1.2.3" xref="S3.E2.m1.3.3.1.1.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.2.3.2" xref="S3.E2.m1.3.3.1.1.2.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.2.3.1" xref="S3.E2.m1.3.3.1.1.2.3.1.cmml">∈</mo><mrow id="S3.E2.m1.3.3.1.1.2.3.3" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml"><mtext id="S3.E2.m1.3.3.1.1.2.3.3a" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml">{</mtext><mtext id="S3.E2.m1.3.3.1.1.2.3.3b" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml">𝑜𝑢𝑡𝑝𝑢𝑡</mtext><mtext id="S3.E2.m1.3.3.1.1.2.3.3c" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml">}</mtext></mrow></mrow></munder></mstyle><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.3.1" xref="S3.E2.m1.3.3.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.3.3.1.1.1.3a" xref="S3.E2.m1.3.3.1.1.1.3.cmml">⁡</mo><mi id="S3.E2.m1.3.3.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.3.2.cmml">p</mi></mrow><mo id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.2.cmml" lspace="0px" rspace="0px"></mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.5" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.5.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.2.cmml">x</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.5.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.4" xref="S3.E2.m1.3.3.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.4" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.5" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">…</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.6" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml">−</mo><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.7" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">;</mo><mi mathvariant="normal" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">Θ</mi></mrow></mrow><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><eq id="S3.E2.m1.3.3.2.cmml" xref="S3.E2.m1.3.3.2"></eq><apply id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.3.2">ℒ</ci><ci id="S3.E2.m1.3.3.3.3a.cmml" xref="S3.E2.m1.3.3.3.3"><mtext mathsize="70%" id="S3.E2.m1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3">SFT</mtext></ci></apply><apply id="S3.E2.m1.3.3.1.cmml" xref="S3.E2.m1.3.3.1"><minus id="S3.E2.m1.3.3.1.2.cmml" xref="S3.E2.m1.3.3.1"></minus><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1"><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3"><in id="S3.E2.m1.3.3.1.1.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.2.3.1"></in><ci id="S3.E2.m1.3.3.1.1.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.2.3.2">𝑖</ci><ci id="S3.E2.m1.3.3.1.1.2.3.3d.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3"><mrow id="S3.E2.m1.3.3.1.1.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3"><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.2.3.3a.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3">{</mtext><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.2.3.3b.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3">𝑜𝑢𝑡𝑝𝑢𝑡</mtext><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.2.3.3c.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3">}</mtext></mrow></ci></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2"></times><apply id="S3.E2.m1.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3"><log id="S3.E2.m1.3.3.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.3.1"></log><ci id="S3.E2.m1.3.3.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.3.2">𝑝</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.4">conditional</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.5.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.5.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.2">𝑥</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.5.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.3">𝑖</ci></apply><list id="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3"><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2">𝑥</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">…</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2">𝑥</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3"><minus id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">Θ</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\displaystyle\mathcal{L}_{\textrm{SFT}}=-\sum_{i\in\textit{\{output\}}}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">caligraphic_L start_POSTSUBSCRIPT SFT end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_i ∈ { output } end_POSTSUBSCRIPT roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; roman_Θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.p5.1">여기서, <math alttext="\bm{x}=(x_{0},x_{1},\ldots)" class="ltx_Math" display="inline" id="S3.p5.1.m1.3"><semantics id="S3.p5.1.m1.3a"><mrow id="S3.p5.1.m1.3.3" xref="S3.p5.1.m1.3.3.cmml"><mi id="S3.p5.1.m1.3.3.4" xref="S3.p5.1.m1.3.3.4.cmml">𝒙</mi><mo id="S3.p5.1.m1.3.3.3" xref="S3.p5.1.m1.3.3.3.cmml">=</mo><mrow id="S3.p5.1.m1.3.3.2.2" xref="S3.p5.1.m1.3.3.2.3.cmml"><mo id="S3.p5.1.m1.3.3.2.2.3" stretchy="false" xref="S3.p5.1.m1.3.3.2.3.cmml">(</mo><msub id="S3.p5.1.m1.2.2.1.1.1" xref="S3.p5.1.m1.2.2.1.1.1.cmml"><mi id="S3.p5.1.m1.2.2.1.1.1.2" xref="S3.p5.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S3.p5.1.m1.2.2.1.1.1.3" xref="S3.p5.1.m1.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S3.p5.1.m1.3.3.2.2.4" xref="S3.p5.1.m1.3.3.2.3.cmml">,</mo><msub id="S3.p5.1.m1.3.3.2.2.2" xref="S3.p5.1.m1.3.3.2.2.2.cmml"><mi id="S3.p5.1.m1.3.3.2.2.2.2" xref="S3.p5.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S3.p5.1.m1.3.3.2.2.2.3" xref="S3.p5.1.m1.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S3.p5.1.m1.3.3.2.2.5" xref="S3.p5.1.m1.3.3.2.3.cmml">,</mo><mi id="S3.p5.1.m1.1.1" mathvariant="normal" xref="S3.p5.1.m1.1.1.cmml">…</mi><mo id="S3.p5.1.m1.3.3.2.2.6" stretchy="false" xref="S3.p5.1.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.3b"><apply id="S3.p5.1.m1.3.3.cmml" xref="S3.p5.1.m1.3.3"><eq id="S3.p5.1.m1.3.3.3.cmml" xref="S3.p5.1.m1.3.3.3"></eq><ci id="S3.p5.1.m1.3.3.4.cmml" xref="S3.p5.1.m1.3.3.4">𝒙</ci><vector id="S3.p5.1.m1.3.3.2.3.cmml" xref="S3.p5.1.m1.3.3.2.2"><apply id="S3.p5.1.m1.2.2.1.1.1.cmml" xref="S3.p5.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p5.1.m1.2.2.1.1.1.1.cmml" xref="S3.p5.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.p5.1.m1.2.2.1.1.1.2.cmml" xref="S3.p5.1.m1.2.2.1.1.1.2">𝑥</ci><cn id="S3.p5.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.p5.1.m1.2.2.1.1.1.3">0</cn></apply><apply id="S3.p5.1.m1.3.3.2.2.2.cmml" xref="S3.p5.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p5.1.m1.3.3.2.2.2.1.cmml" xref="S3.p5.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.p5.1.m1.3.3.2.2.2.2.cmml" xref="S3.p5.1.m1.3.3.2.2.2.2">𝑥</ci><cn id="S3.p5.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S3.p5.1.m1.3.3.2.2.2.3">1</cn></apply><ci id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">…</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.3c">\bm{x}=(x_{0},x_{1},\ldots)</annotation><annotation encoding="application/x-llamapun" id="S3.p5.1.m1.3d">bold_italic_x = ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … )</annotation></semantics></math>는 앞서 언급한 프롬프트 템플릿에 랩핑된 토큰화된 자기 명령 예시를 나타낸다.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.p6.1">우리의 접근법과 Stanford Alpaca의 주요 차이점은 <em class="ltx_emph ltx_font_italic" id="S3.p6.1.1">input</em> 필드가 없는 예제용으로 설계된 프롬프트 템플릿을 독점적으로 사용하는 반면, Stanford Alpaca는 <em class="ltx_emph ltx_font_italic" id="S3.p6.1.2">input</em> 필드가 별도로 없는 예제용으로 두 개의 템플릿을 사용합니다. 예제가 비어 있지 않은 <em class="ltx_emph ltx_font_italic" id="S3.p6.1.3">input</em> 필드를 포함하는 경우, <em class="ltx_emph ltx_font_italic" id="S3.p6.1.4">instruction</em> 및 <em class="ltx_emph ltx_font_italic" id="S3.p6.1.5">input</em>을 <em class="ltx_emph ltx_font_italic" id="S3.p6.1.6">“\n”</em>과 연결하여 새 명령을 형성합니다. 알파카 모델에 대한 추가 패딩 토큰이 있어 어휘 크기가 49,954가 된다.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Parameter Efficient Fine-Tuning with LoRA</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S4.p1.4">Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="#bib.bib8" title="">2021</a>)</cite>는 훈련 가능한 순위 분해 행렬을 도입하면서 미리 훈련된 모델 가중치를 유지하는 파라미터 효율적인 훈련 방법이다. 이 접근법은 훈련 가능한 파라미터의 수를 상당히 감소시킨다. LoRA의 일반적인 공식은 다음 식으로 표현되며, 여기서 <math alttext="r" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_r</annotation></semantics></math>는 미리 결정된 순위이고, <math alttext="d" class="ltx_Math" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">italic_d</annotation></semantics></math>는 숨겨진 크기이며, <math alttext="A" class="ltx_Math" display="inline" id="S4.p1.3.m3.1"><semantics id="S4.p1.3.m3.1a"><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.p1.3.m3.1d">italic_A</annotation></semantics></math> 및 <math alttext="B" class="ltx_Math" display="inline" id="S4.p1.4.m4.1"><semantics id="S4.p1.4.m4.1a"><mi id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><ci id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">B</annotation><annotation encoding="application/x-llamapun" id="S4.p1.4.m4.1d">italic_B</annotation></semantics></math>는 분해된 훈련 가능한 행렬이다:</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.2" class="ltx_Math" alttext="h=W_{0}x+\Delta Wx=W_{0}x+BAx,~{}~{}B\in\mathbb{R}^{d\times r},A\in\mathbb{R}^{r\times d}" display="block"><semantics id="S4.E3.m1.2a"><mrow id="S4.E3.m1.2.2.2" xref="S4.E3.m1.2.2.3.cmml"><mrow id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml"><mi id="S4.E3.m1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.2.cmml">h</mi><mo id="S4.E3.m1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.3.cmml">=</mo><mrow id="S4.E3.m1.1.1.1.1.4" xref="S4.E3.m1.1.1.1.1.4.cmml"><mrow id="S4.E3.m1.1.1.1.1.4.2" xref="S4.E3.m1.1.1.1.1.4.2.cmml"><msub id="S4.E3.m1.1.1.1.1.4.2.2" xref="S4.E3.m1.1.1.1.1.4.2.2.cmml"><mi id="S4.E3.m1.1.1.1.1.4.2.2.2" xref="S4.E3.m1.1.1.1.1.4.2.2.2.cmml">W</mi><mn id="S4.E3.m1.1.1.1.1.4.2.2.3" xref="S4.E3.m1.1.1.1.1.4.2.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.1.1.1.1.4.2.1" xref="S4.E3.m1.1.1.1.1.4.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.4.2.3" xref="S4.E3.m1.1.1.1.1.4.2.3.cmml">x</mi></mrow><mo id="S4.E3.m1.1.1.1.1.4.1" xref="S4.E3.m1.1.1.1.1.4.1.cmml">+</mo><mrow id="S4.E3.m1.1.1.1.1.4.3" xref="S4.E3.m1.1.1.1.1.4.3.cmml"><mi mathvariant="normal" id="S4.E3.m1.1.1.1.1.4.3.2" xref="S4.E3.m1.1.1.1.1.4.3.2.cmml">Δ</mi><mo id="S4.E3.m1.1.1.1.1.4.3.1" xref="S4.E3.m1.1.1.1.1.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.4.3.3" xref="S4.E3.m1.1.1.1.1.4.3.3.cmml">W</mi><mo id="S4.E3.m1.1.1.1.1.4.3.1a" xref="S4.E3.m1.1.1.1.1.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.4.3.4" xref="S4.E3.m1.1.1.1.1.4.3.4.cmml">x</mi></mrow></mrow><mo id="S4.E3.m1.1.1.1.1.5" xref="S4.E3.m1.1.1.1.1.5.cmml">=</mo><mrow id="S4.E3.m1.1.1.1.1.6" xref="S4.E3.m1.1.1.1.1.6.cmml"><mrow id="S4.E3.m1.1.1.1.1.6.2" xref="S4.E3.m1.1.1.1.1.6.2.cmml"><msub id="S4.E3.m1.1.1.1.1.6.2.2" xref="S4.E3.m1.1.1.1.1.6.2.2.cmml"><mi id="S4.E3.m1.1.1.1.1.6.2.2.2" xref="S4.E3.m1.1.1.1.1.6.2.2.2.cmml">W</mi><mn id="S4.E3.m1.1.1.1.1.6.2.2.3" xref="S4.E3.m1.1.1.1.1.6.2.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.1.1.1.1.6.2.1" xref="S4.E3.m1.1.1.1.1.6.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.6.2.3" xref="S4.E3.m1.1.1.1.1.6.2.3.cmml">x</mi></mrow><mo id="S4.E3.m1.1.1.1.1.6.1" xref="S4.E3.m1.1.1.1.1.6.1.cmml">+</mo><mrow id="S4.E3.m1.1.1.1.1.6.3" xref="S4.E3.m1.1.1.1.1.6.3.cmml"><mi id="S4.E3.m1.1.1.1.1.6.3.2" xref="S4.E3.m1.1.1.1.1.6.3.2.cmml">B</mi><mo id="S4.E3.m1.1.1.1.1.6.3.1" xref="S4.E3.m1.1.1.1.1.6.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.6.3.3" xref="S4.E3.m1.1.1.1.1.6.3.3.cmml">A</mi><mo id="S4.E3.m1.1.1.1.1.6.3.1a" xref="S4.E3.m1.1.1.1.1.6.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.6.3.4" xref="S4.E3.m1.1.1.1.1.6.3.4.cmml">x</mi></mrow></mrow></mrow><mo rspace="0.827em" id="S4.E3.m1.2.2.2.3" xref="S4.E3.m1.2.2.3a.cmml">,</mo><mrow id="S4.E3.m1.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.3.cmml"><mrow id="S4.E3.m1.2.2.2.2.1.1" xref="S4.E3.m1.2.2.2.2.1.1.cmml"><mi id="S4.E3.m1.2.2.2.2.1.1.2" xref="S4.E3.m1.2.2.2.2.1.1.2.cmml">B</mi><mo id="S4.E3.m1.2.2.2.2.1.1.1" xref="S4.E3.m1.2.2.2.2.1.1.1.cmml">∈</mo><msup id="S4.E3.m1.2.2.2.2.1.1.3" xref="S4.E3.m1.2.2.2.2.1.1.3.cmml"><mi id="S4.E3.m1.2.2.2.2.1.1.3.2" xref="S4.E3.m1.2.2.2.2.1.1.3.2.cmml">ℝ</mi><mrow id="S4.E3.m1.2.2.2.2.1.1.3.3" xref="S4.E3.m1.2.2.2.2.1.1.3.3.cmml"><mi id="S4.E3.m1.2.2.2.2.1.1.3.3.2" xref="S4.E3.m1.2.2.2.2.1.1.3.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.2.2.2.2.1.1.3.3.1" xref="S4.E3.m1.2.2.2.2.1.1.3.3.1.cmml">×</mo><mi id="S4.E3.m1.2.2.2.2.1.1.3.3.3" xref="S4.E3.m1.2.2.2.2.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><mo id="S4.E3.m1.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S4.E3.m1.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.2.cmml">A</mi><mo id="S4.E3.m1.2.2.2.2.2.2.1" xref="S4.E3.m1.2.2.2.2.2.2.1.cmml">∈</mo><msup id="S4.E3.m1.2.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.2.2.3.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.3.2" xref="S4.E3.m1.2.2.2.2.2.2.3.2.cmml">ℝ</mi><mrow id="S4.E3.m1.2.2.2.2.2.2.3.3" xref="S4.E3.m1.2.2.2.2.2.2.3.3.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.3.3.2" xref="S4.E3.m1.2.2.2.2.2.2.3.3.2.cmml">r</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.2.2.2.2.2.2.3.3.1" xref="S4.E3.m1.2.2.2.2.2.2.3.3.1.cmml">×</mo><mi id="S4.E3.m1.2.2.2.2.2.2.3.3.3" xref="S4.E3.m1.2.2.2.2.2.2.3.3.3.cmml">d</mi></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.2b"><apply id="S4.E3.m1.2.2.3.cmml" xref="S4.E3.m1.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.3a.cmml" xref="S4.E3.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1"><and id="S4.E3.m1.1.1.1.1a.cmml" xref="S4.E3.m1.1.1.1.1"></and><apply id="S4.E3.m1.1.1.1.1b.cmml" xref="S4.E3.m1.1.1.1.1"><eq id="S4.E3.m1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.3"></eq><ci id="S4.E3.m1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.2">ℎ</ci><apply id="S4.E3.m1.1.1.1.1.4.cmml" xref="S4.E3.m1.1.1.1.1.4"><plus id="S4.E3.m1.1.1.1.1.4.1.cmml" xref="S4.E3.m1.1.1.1.1.4.1"></plus><apply id="S4.E3.m1.1.1.1.1.4.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2"><times id="S4.E3.m1.1.1.1.1.4.2.1.cmml" xref="S4.E3.m1.1.1.1.1.4.2.1"></times><apply id="S4.E3.m1.1.1.1.1.4.2.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.4.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.4.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2.2">𝑊</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.4.2.2.3.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2.3">0</cn></apply><ci id="S4.E3.m1.1.1.1.1.4.2.3.cmml" xref="S4.E3.m1.1.1.1.1.4.2.3">𝑥</ci></apply><apply id="S4.E3.m1.1.1.1.1.4.3.cmml" xref="S4.E3.m1.1.1.1.1.4.3"><times id="S4.E3.m1.1.1.1.1.4.3.1.cmml" xref="S4.E3.m1.1.1.1.1.4.3.1"></times><ci id="S4.E3.m1.1.1.1.1.4.3.2.cmml" xref="S4.E3.m1.1.1.1.1.4.3.2">Δ</ci><ci id="S4.E3.m1.1.1.1.1.4.3.3.cmml" xref="S4.E3.m1.1.1.1.1.4.3.3">𝑊</ci><ci id="S4.E3.m1.1.1.1.1.4.3.4.cmml" xref="S4.E3.m1.1.1.1.1.4.3.4">𝑥</ci></apply></apply></apply><apply id="S4.E3.m1.1.1.1.1c.cmml" xref="S4.E3.m1.1.1.1.1"><eq id="S4.E3.m1.1.1.1.1.5.cmml" xref="S4.E3.m1.1.1.1.1.5"></eq><share href="#S4.E3.m1.1.1.1.1.4.cmml" id="S4.E3.m1.1.1.1.1d.cmml" xref="S4.E3.m1.1.1.1.1"></share><apply id="S4.E3.m1.1.1.1.1.6.cmml" xref="S4.E3.m1.1.1.1.1.6"><plus id="S4.E3.m1.1.1.1.1.6.1.cmml" xref="S4.E3.m1.1.1.1.1.6.1"></plus><apply id="S4.E3.m1.1.1.1.1.6.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2"><times id="S4.E3.m1.1.1.1.1.6.2.1.cmml" xref="S4.E3.m1.1.1.1.1.6.2.1"></times><apply id="S4.E3.m1.1.1.1.1.6.2.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.6.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.6.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2.2">𝑊</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.6.2.2.3.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2.3">0</cn></apply><ci id="S4.E3.m1.1.1.1.1.6.2.3.cmml" xref="S4.E3.m1.1.1.1.1.6.2.3">𝑥</ci></apply><apply id="S4.E3.m1.1.1.1.1.6.3.cmml" xref="S4.E3.m1.1.1.1.1.6.3"><times id="S4.E3.m1.1.1.1.1.6.3.1.cmml" xref="S4.E3.m1.1.1.1.1.6.3.1"></times><ci id="S4.E3.m1.1.1.1.1.6.3.2.cmml" xref="S4.E3.m1.1.1.1.1.6.3.2">𝐵</ci><ci id="S4.E3.m1.1.1.1.1.6.3.3.cmml" xref="S4.E3.m1.1.1.1.1.6.3.3">𝐴</ci><ci id="S4.E3.m1.1.1.1.1.6.3.4.cmml" xref="S4.E3.m1.1.1.1.1.6.3.4">𝑥</ci></apply></apply></apply></apply><apply id="S4.E3.m1.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.3a.cmml" xref="S4.E3.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E3.m1.2.2.2.2.1.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1"><in id="S4.E3.m1.2.2.2.2.1.1.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.1"></in><ci id="S4.E3.m1.2.2.2.2.1.1.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.2">𝐵</ci><apply id="S4.E3.m1.2.2.2.2.1.1.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.1.1.3.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3">superscript</csymbol><ci id="S4.E3.m1.2.2.2.2.1.1.3.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.2">ℝ</ci><apply id="S4.E3.m1.2.2.2.2.1.1.3.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3"><times id="S4.E3.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3.1"></times><ci id="S4.E3.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3.2">𝑑</ci><ci id="S4.E3.m1.2.2.2.2.1.1.3.3.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3.3">𝑟</ci></apply></apply></apply><apply id="S4.E3.m1.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2"><in id="S4.E3.m1.2.2.2.2.2.2.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.1"></in><ci id="S4.E3.m1.2.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2">𝐴</ci><apply id="S4.E3.m1.2.2.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.2.2.3.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3">superscript</csymbol><ci id="S4.E3.m1.2.2.2.2.2.2.3.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.2">ℝ</ci><apply id="S4.E3.m1.2.2.2.2.2.2.3.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3"><times id="S4.E3.m1.2.2.2.2.2.2.3.3.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3.1"></times><ci id="S4.E3.m1.2.2.2.2.2.2.3.3.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3.2">𝑟</ci><ci id="S4.E3.m1.2.2.2.2.2.2.3.3.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3.3">𝑑</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.2c">h=W_{0}x+\Delta Wx=W_{0}x+BAx,~{}~{}B\in\mathbb{R}^{d\times r},A\in\mathbb{R}^{r\times d}</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.2d">italic_h = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + roman_Δ italic_W italic_x = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + italic_B italic_A italic_x , italic_B ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_r end_POSTSUPERSCRIPT , italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_r × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S4.p3.1">빠듯한 예산을 준수하면서 매개변수 효율적인 훈련을 달성하기 위해 사전 훈련 및 미세 조정 단계를 포함한 모든 실험에서 LoRA를 중국-LLaMA/알파카 모델에 적용한다. 주로 LoRA 어댑터를 어텐션 모듈의 가중치와 경우에 따라 추가 MLP 계층에 통합한다. 자세한 내용은 다음 섹션 및 표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ Instruction Fine-tuning ‣ 5.1.2 13B Version ‣ 5.1 Experimental Setups for Pre-training and Fine-tuning ‣ 5 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">2</span></a>를 참조하십시오.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setups</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setups for Pre-training and Fine-tuning</h3>

<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>7B Version</h4>

<section id="S5.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pre-training</h5>

<div id="S5.SS1.SSS1.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS1.SSS1.Px1.p1.1">중국어 BERT-wwm <cite class="ltx_cite ltx_citemacro_citep">(Cui et al., <a class="ltx_ref" href="#bib.bib2" title="">2021</a>)</cite>, MacBERT <cite class="ltx_cite ltx_citemacro_citep">(Cui et al., <a class="ltx_ref" href="#bib.bib1" title="">2020</a>)</cite>, LERT <cite class="ltx_cite ltx_citemacro_citep">(Cui et al., <a class="ltx_ref" href="#bib.bib3" title="">2022</a>)</cite>, 기타에서 사용된 말뭉치와 일치하는 중국어 일반 말뭉치에 대해 중국어-LLaMA 모델을 초기화하고 모델을 사전 훈련하여 20GB 텍스트 말뭉치를 생성했다. 사전 훈련 과정은 두 단계로 구성된다:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Stage 1</span>: 모델 내에서 트랜스포머 인코더의 파라미터를 고정하고 임베딩만을 훈련하여 원래 모델에 대한 외란을 최소화하면서 새로 추가된 중국어 단어 벡터를 적응시킨다.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Stage 2</span>: 주의 메커니즘에 LoRA 가중치(어댑터)를 추가하고 임베딩, LM 헤드 및 새로 추가된 LoRA 매개 변수를 학습합니다.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Instruction Fine-tuning</h5>

<div id="S5.SS1.SSS1.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS1.SSS1.Px2.p1.1">사전 학습된 모델을 구한 후, Section <a class="ltx_ref" href="#S3" title="3 Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">3</span></a>에 따라 미세 조정한다. 또한 LoRA 어댑터를 MLP 레이어에 추가하여 훈련 가능한 매개 변수의 수를 증가시켜 효율적인 미세 조정을 위해 LoRA를 사용한다. 우리는 번역 <cite class="ltx_cite ltx_citemacro_citep">(Xu, <a class="ltx_ref" href="#bib.bib20" title="">2019</a>)</cite>, pCLUE<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/CLUEbenchmark/pCLUE" target="_blank" title="">https://github.com/CLUEbenchmark/pCLUE</a></span></span></span>, Stanford Alpaca, 그리고 7B 모델을 튜닝하기 위해 크롤링된 SFT 데이터를 포함한 대략 2M 데이터 포인트를 활용한다.</p>
</div>
<div id="S5.SS1.SSS1.Px2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS1.SSS1.Px2.p2.1">크롤링된 데이터의 경우, ChatGPT(<span class="ltx_text ltx_font_typewriter" id="S5.SS1.SSS1.Px2.p2.1.1">gpt-3.5-turbo</span> API)에서 사용되는 데이터를 자동으로 얻기 위해 자체 명령 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib19" title="">2022</a>)</cite> 방법을 사용합니다. 템플릿 및 코드 세부 정보는 GitHub에서 사용할 수 있습니다. <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py" target="_blank" title="">https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py</a></span></span></span></p>
</div>
<div id="S5.SS1.SSS1.Px2.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS1.SSS1.Px2.p3.1">하이퍼파라미터는 표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ Instruction Fine-tuning ‣ 5.1.2 13B Version ‣ 5.1 Experimental Setups for Pre-training and Fine-tuning ‣ 5 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">2</span></a>에 나열되어 있다. 미세 조정 데이터에 대한 상세한 정보는 표 <a class="ltx_ref" href="#S5.T3" title="Table 3 ‣ Instruction Fine-tuning ‣ 5.1.2 13B Version ‣ 5.1 Experimental Setups for Pre-training and Fine-tuning ‣ 5 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">3</span></a>에 제공된다.</p>
</div>
</section>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>13B Version</h4>

<section id="S5.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pre-training</h5>

<div id="S5.SS1.SSS2.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS1.SSS2.Px1.p1.1">13B 모델의 사전 훈련 과정은 사전 훈련에서 1단계를 건너뛰는 것을 제외하고는 7B 모델의 사전 훈련 과정과 크게 동일하다. 우리는 임베딩과 LM 헤드를 훈련 가능한 것으로 설정하면서 훈련을 위해 관심 및 MLP에 LoRA를 직접 적용한다.</p>
</div>
</section>
<section id="S5.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Instruction Fine-tuning</h5>

<div id="S5.SS1.SSS2.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS1.SSS2.Px2.p1.1">LoRA 설정 및 훈련 가능한 파라미터는 사전 훈련 단계에서와 동일하게 유지된다. 13B 모델 미세 조정을 위해 추가로 1M 크롤링된 자체 지시 데이터 포인트를 사용하여 13B 모델의 총 데이터 크기가 3M이 된다.</p>
</div>
<div id="S5.SS1.SSS2.Px2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS1.SSS2.Px2.p2.1">하이퍼파라미터는 표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ Instruction Fine-tuning ‣ 5.1.2 13B Version ‣ 5.1 Experimental Setups for Pre-training and Fine-tuning ‣ 5 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">2</span></a>에 나열되어 있다.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 2:</span> LLaMA(사전 훈련 단계) 및 Alpaca(명령 SFT 단계) 7B 및 13B에 대한 훈련 레시피. PT: 사전 훈련. SFT: 감독 미세 조정. QKVO: 각 주의 모듈에서 4개의 행렬(쿼리, 키, 값 및 출력을 나타냄)입니다. MLP: 각 MLP 계층에서 세 개의 행렬.</figcaption>
<table id="S5.T2.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S5.T2.5.6" class="ltx_tr">
<td id="S5.T2.5.6.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T2.5.6.1.1" class="ltx_text ltx_font_bold">7B Settings</span></td>
<td id="S5.T2.5.6.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.6.2.1" class="ltx_text ltx_font_bold">PT Stage 1</span></td>
<td id="S5.T2.5.6.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.6.3.1" class="ltx_text ltx_font_bold">PT Stage 2</span></td>
<td id="S5.T2.5.6.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.6.4.1" class="ltx_text ltx_font_bold">Instruction SFT</span></td>
</tr>
<tr id="S5.T2.5.7" class="ltx_tr">
<td id="S5.T2.5.7.1" class="ltx_td ltx_align_left ltx_border_t">Batch size</td>
<td id="S5.T2.5.7.2" class="ltx_td ltx_align_center ltx_border_t">1024</td>
<td id="S5.T2.5.7.3" class="ltx_td ltx_align_center ltx_border_t">1024</td>
<td id="S5.T2.5.7.4" class="ltx_td ltx_align_center ltx_border_t">512</td>
</tr>
<tr id="S5.T2.5.8" class="ltx_tr">
<td id="S5.T2.5.8.1" class="ltx_td ltx_align_left">Peak learning rate</td>
<td id="S5.T2.5.8.2" class="ltx_td ltx_align_center">2e-4</td>
<td id="S5.T2.5.8.3" class="ltx_td ltx_align_center">1e-4</td>
<td id="S5.T2.5.8.4" class="ltx_td ltx_align_center">1e-4</td>
</tr>
<tr id="S5.T2.5.9" class="ltx_tr">
<td id="S5.T2.5.9.1" class="ltx_td ltx_align_left">Training steps</td>
<td id="S5.T2.5.9.2" class="ltx_td ltx_align_center">3K</td>
<td id="S5.T2.5.9.3" class="ltx_td ltx_align_center">6K</td>
<td id="S5.T2.5.9.4" class="ltx_td ltx_align_center">6-10K</td>
</tr>
<tr id="S5.T2.5.10" class="ltx_tr">
<td id="S5.T2.5.10.1" class="ltx_td ltx_align_left">Max length</td>
<td id="S5.T2.5.10.2" class="ltx_td ltx_align_center">512</td>
<td id="S5.T2.5.10.3" class="ltx_td ltx_align_center">512</td>
<td id="S5.T2.5.10.4" class="ltx_td ltx_align_center">512</td>
</tr>
<tr id="S5.T2.5.11" class="ltx_tr">
<td id="S5.T2.5.11.1" class="ltx_td ltx_align_left">Trainable parameters</td>
<td id="S5.T2.5.11.2" class="ltx_td ltx_align_center">2.97%</td>
<td id="S5.T2.5.11.3" class="ltx_td ltx_align_center">6.06%</td>
<td id="S5.T2.5.11.4" class="ltx_td ltx_align_center">6.22%</td>
</tr>
<tr id="S5.T2.5.12" class="ltx_tr">
<td id="S5.T2.5.12.1" class="ltx_td ltx_align_left">LoRA rank</td>
<td id="S5.T2.5.12.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.5.12.3" class="ltx_td ltx_align_center">8</td>
<td id="S5.T2.5.12.4" class="ltx_td ltx_align_center">8</td>
</tr>
<tr id="S5.T2.5.13" class="ltx_tr">
<td id="S5.T2.5.13.1" class="ltx_td ltx_align_left">LoRA weights</td>
<td id="S5.T2.5.13.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.5.13.3" class="ltx_td ltx_align_center">QKVO</td>
<td id="S5.T2.5.13.4" class="ltx_td ltx_align_center">QKVO, MLP</td>
</tr>
<tr id="S5.T2.3.3" class="ltx_tr">
<td id="S5.T2.3.3.4" class="ltx_td ltx_align_left">Training device</td>
<td id="S5.T2.1.1.1" class="ltx_td ltx_align_center">8 <math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><times id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.m1.1d">×</annotation></semantics></math> A100</td>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_center">16 <math id="S5.T2.2.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.2.2.2.m1.1a"><mo id="S5.T2.2.2.2.m1.1.1" xref="S5.T2.2.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.m1.1b"><times id="S5.T2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.m1.1d">×</annotation></semantics></math> A100</td>
<td id="S5.T2.3.3.3" class="ltx_td ltx_align_center">16 <math id="S5.T2.3.3.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.3.3.3.m1.1a"><mo id="S5.T2.3.3.3.m1.1.1" xref="S5.T2.3.3.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.m1.1b"><times id="S5.T2.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.m1.1d">×</annotation></semantics></math> A100</td>
</tr>
<tr id="S5.T2.5.14" class="ltx_tr">
<td id="S5.T2.5.14.1" class="ltx_td ltx_align_left">Distributed training</td>
<td id="S5.T2.5.14.2" class="ltx_td ltx_align_center">DeepSpeed ZeRO-2</td>
<td id="S5.T2.5.14.3" class="ltx_td ltx_align_center">DeepSpeed ZeRO-2</td>
<td id="S5.T2.5.14.4" class="ltx_td ltx_align_center">DeepSpeed ZeRO-2</td>
</tr>
<tr id="S5.T2.5.15" class="ltx_tr">
<td id="S5.T2.5.15.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T2.5.15.1.1" class="ltx_text ltx_font_bold">13B Settings</span></td>
<td id="S5.T2.5.15.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T2.5.15.2.1" class="ltx_text ltx_font_bold">PT</span></td>
<td id="S5.T2.5.15.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.15.3.1" class="ltx_text ltx_font_bold">Instruction SFT</span></td>
</tr>
<tr id="S5.T2.5.16" class="ltx_tr">
<td id="S5.T2.5.16.1" class="ltx_td ltx_align_left ltx_border_t">Batch size</td>
<td id="S5.T2.5.16.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2">2304</td>
<td id="S5.T2.5.16.3" class="ltx_td ltx_align_center ltx_border_t">1152</td>
</tr>
<tr id="S5.T2.5.17" class="ltx_tr">
<td id="S5.T2.5.17.1" class="ltx_td ltx_align_left">Peak learning rate</td>
<td id="S5.T2.5.17.2" class="ltx_td ltx_align_center" colspan="2">2e-4</td>
<td id="S5.T2.5.17.3" class="ltx_td ltx_align_center">1e-4</td>
</tr>
<tr id="S5.T2.5.18" class="ltx_tr">
<td id="S5.T2.5.18.1" class="ltx_td ltx_align_left">Training steps</td>
<td id="S5.T2.5.18.2" class="ltx_td ltx_align_center" colspan="2">7K</td>
<td id="S5.T2.5.18.3" class="ltx_td ltx_align_center">5.5K</td>
</tr>
<tr id="S5.T2.5.19" class="ltx_tr">
<td id="S5.T2.5.19.1" class="ltx_td ltx_align_left">Max length</td>
<td id="S5.T2.5.19.2" class="ltx_td ltx_align_center" colspan="2">512</td>
<td id="S5.T2.5.19.3" class="ltx_td ltx_align_center">512</td>
</tr>
<tr id="S5.T2.5.20" class="ltx_tr">
<td id="S5.T2.5.20.1" class="ltx_td ltx_align_left">Trainable parameters</td>
<td id="S5.T2.5.20.2" class="ltx_td ltx_align_center" colspan="2">4.10%</td>
<td id="S5.T2.5.20.3" class="ltx_td ltx_align_center">4.10%</td>
</tr>
<tr id="S5.T2.5.21" class="ltx_tr">
<td id="S5.T2.5.21.1" class="ltx_td ltx_align_left">LoRA rank</td>
<td id="S5.T2.5.21.2" class="ltx_td ltx_align_center" colspan="2">8</td>
<td id="S5.T2.5.21.3" class="ltx_td ltx_align_center">8</td>
</tr>
<tr id="S5.T2.5.22" class="ltx_tr">
<td id="S5.T2.5.22.1" class="ltx_td ltx_align_left">LoRA weights</td>
<td id="S5.T2.5.22.2" class="ltx_td ltx_align_center" colspan="2">QKVO, MLP</td>
<td id="S5.T2.5.22.3" class="ltx_td ltx_align_center">QKVO, MLP</td>
</tr>
<tr id="S5.T2.5.5" class="ltx_tr">
<td id="S5.T2.5.5.3" class="ltx_td ltx_align_left">Training device</td>
<td id="S5.T2.4.4.1" class="ltx_td ltx_align_center" colspan="2">48 <math id="S5.T2.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.4.4.1.m1.1a"><mo id="S5.T2.4.4.1.m1.1.1" xref="S5.T2.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.1.m1.1b"><times id="S5.T2.4.4.1.m1.1.1.cmml" xref="S5.T2.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.1.m1.1d">×</annotation></semantics></math> A100</td>
<td id="S5.T2.5.5.2" class="ltx_td ltx_align_center">48 <math id="S5.T2.5.5.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.5.5.2.m1.1a"><mo id="S5.T2.5.5.2.m1.1.1" xref="S5.T2.5.5.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.2.m1.1b"><times id="S5.T2.5.5.2.m1.1.1.cmml" xref="S5.T2.5.5.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.2.m1.1d">×</annotation></semantics></math> A100</td>
</tr>
<tr id="S5.T2.5.23" class="ltx_tr">
<td id="S5.T2.5.23.1" class="ltx_td ltx_align_left ltx_border_bb">Distributed training</td>
<td id="S5.T2.5.23.2" class="ltx_td ltx_align_center ltx_border_bb" colspan="2">DeepSpeed ZeRO-2</td>
<td id="S5.T2.5.23.3" class="ltx_td ltx_align_center ltx_border_bb">DeepSpeed ZeRO-2</td>
</tr>
</tbody></table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 3:</span> 명령어 미세 조정 단계에서 사용되는 데이터의 세부 정보.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S5.T3.1.2" class="ltx_tr">
<td id="S5.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S5.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_tt">Translation</td>
<td id="S5.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_tt">pCLUE</td>
<td id="S5.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_tt">Stanford Alpaca</td>
<td id="S5.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_tt">Stanford Alpaca (Chinese)</td>
<td id="S5.T3.1.2.6" class="ltx_td ltx_align_center ltx_border_tt">Crawled SFT data</td>
</tr>
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T3.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">550K</td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">250K</td>
<td id="S5.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">50K</td>
<td id="S5.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">50K</td>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1<math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.m1.1d">∼</annotation></semantics></math>2M</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Experimental Setups for Decoding</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS2.p1.1">LLM들의 디코딩 프로세스는 생성된 텍스트의 품질 및 다양성을 결정하는 데 중요한 역할을 한다. 실험에서는 다음과 같은 디코딩 하이퍼파라미터를 사용한다.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="S5.I2.i1.p1.1">컨텍스트 크기: 컨텍스트 크기를 2048로 설정하는데, 이는 모델이 텍스트를 생성할 때 동시에 고려할 수 있는 최대 토큰 수를 결정한다.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="S5.I2.i2.p1.1">최대 시퀀스 길이: 생성된 시퀀스 길이를 512 토큰으로 제한하여 출력들이 포커스를 유지하고 입력 프롬프트와 관련되도록 한다.</p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="S5.I2.i3.p1.1">온도: 샘플링 과정의 무작위성을 제어하는 온도를 0.2로 설정했다. 낮은 값은 모델이 더 집중되고 결정론적 출력을 생성하도록 만드는 반면, 높은 값은 일관성의 비용으로 다양성을 증가시킨다.</p>
</div>
</li>
<li id="S5.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i4.p1" class="ltx_para">
<p class="ltx_p" id="S5.I2.i4.p1.3">Top-<math alttext="k" class="ltx_Math" display="inline" id="S5.I2.i4.p1.1.m1.1"><semantics id="S5.I2.i4.p1.1.m1.1a"><mi id="S5.I2.i4.p1.1.m1.1.1" xref="S5.I2.i4.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i4.p1.1.m1.1b"><ci id="S5.I2.i4.p1.1.m1.1.1.cmml" xref="S5.I2.i4.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i4.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i4.p1.1.m1.1d">italic_k</annotation></semantics></math> sampling: Top-<math alttext="k" class="ltx_Math" display="inline" id="S5.I2.i4.p1.2.m2.1"><semantics id="S5.I2.i4.p1.2.m2.1a"><mi id="S5.I2.i4.p1.2.m2.1.1" xref="S5.I2.i4.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i4.p1.2.m2.1b"><ci id="S5.I2.i4.p1.2.m2.1.1.cmml" xref="S5.I2.i4.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i4.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i4.p1.2.m2.1d">italic_k</annotation></semantics></math> sampling with <math alttext="k=40" class="ltx_Math" display="inline" id="S5.I2.i4.p1.3.m3.1"><semantics id="S5.I2.i4.p1.3.m3.1a"><mrow id="S5.I2.i4.p1.3.m3.1.1" xref="S5.I2.i4.p1.3.m3.1.1.cmml"><mi id="S5.I2.i4.p1.3.m3.1.1.2" xref="S5.I2.i4.p1.3.m3.1.1.2.cmml">k</mi><mo id="S5.I2.i4.p1.3.m3.1.1.1" xref="S5.I2.i4.p1.3.m3.1.1.1.cmml">=</mo><mn id="S5.I2.i4.p1.3.m3.1.1.3" xref="S5.I2.i4.p1.3.m3.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i4.p1.3.m3.1b"><apply id="S5.I2.i4.p1.3.m3.1.1.cmml" xref="S5.I2.i4.p1.3.m3.1.1"><eq id="S5.I2.i4.p1.3.m3.1.1.1.cmml" xref="S5.I2.i4.p1.3.m3.1.1.1"></eq><ci id="S5.I2.i4.p1.3.m3.1.1.2.cmml" xref="S5.I2.i4.p1.3.m3.1.1.2">𝑘</ci><cn id="S5.I2.i4.p1.3.m3.1.1.3.cmml" type="integer" xref="S5.I2.i4.p1.3.m3.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i4.p1.3.m3.1c">k=40</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i4.p1.3.m3.1d">italic_k = 40</annotation></semantics></math>를 사용하여 각 단계에서 모델이 가장 가능성이 높은 상위 40개의 토큰 중에서 다음 토큰을 선택하여 생성된 텍스트에 랜덤성과 다양성의 요소를 추가한다.</p>
</div>
</li>
<li id="S5.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i5.p1" class="ltx_para">
<p class="ltx_p" id="S5.I2.i5.p1.3">Top-<math alttext="p" class="ltx_Math" display="inline" id="S5.I2.i5.p1.1.m1.1"><semantics id="S5.I2.i5.p1.1.m1.1a"><mi id="S5.I2.i5.p1.1.m1.1.1" xref="S5.I2.i5.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i5.p1.1.m1.1b"><ci id="S5.I2.i5.p1.1.m1.1.1.cmml" xref="S5.I2.i5.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i5.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i5.p1.1.m1.1d">italic_p</annotation></semantics></math> sampling: 또한 <math alttext="p" class="ltx_Math" display="inline" id="S5.I2.i5.p1.2.m2.1"><semantics id="S5.I2.i5.p1.2.m2.1a"><mi id="S5.I2.i5.p1.2.m2.1.1" xref="S5.I2.i5.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i5.p1.2.m2.1b"><ci id="S5.I2.i5.p1.2.m2.1.1.cmml" xref="S5.I2.i5.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i5.p1.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i5.p1.2.m2.1d">italic_p</annotation></semantics></math> sampling with <math alttext="p=0.9" class="ltx_Math" display="inline" id="S5.I2.i5.p1.3.m3.1"><semantics id="S5.I2.i5.p1.3.m3.1a"><mrow id="S5.I2.i5.p1.3.m3.1.1" xref="S5.I2.i5.p1.3.m3.1.1.cmml"><mi id="S5.I2.i5.p1.3.m3.1.1.2" xref="S5.I2.i5.p1.3.m3.1.1.2.cmml">p</mi><mo id="S5.I2.i5.p1.3.m3.1.1.1" xref="S5.I2.i5.p1.3.m3.1.1.1.cmml">=</mo><mn id="S5.I2.i5.p1.3.m3.1.1.3" xref="S5.I2.i5.p1.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i5.p1.3.m3.1b"><apply id="S5.I2.i5.p1.3.m3.1.1.cmml" xref="S5.I2.i5.p1.3.m3.1.1"><eq id="S5.I2.i5.p1.3.m3.1.1.1.cmml" xref="S5.I2.i5.p1.3.m3.1.1.1"></eq><ci id="S5.I2.i5.p1.3.m3.1.1.2.cmml" xref="S5.I2.i5.p1.3.m3.1.1.2">𝑝</ci><cn id="S5.I2.i5.p1.3.m3.1.1.3.cmml" type="float" xref="S5.I2.i5.p1.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i5.p1.3.m3.1c">p=0.9</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i5.p1.3.m3.1d">italic_p = 0.9</annotation></semantics></math>를 채용하는데, 이는 확률 질량의 90%를 집합적으로 차지하는 토큰들의 동적 집합을 고려함으로써 다이버시티를 더욱 향상시킨다.</p>
</div>
</li>
<li id="S5.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i6.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.I2.i6.p1.1">반복 패널티: 모델이 반복적인 텍스트를 생성하지 않도록 하기 위해, 우리는 이미 선택된 토큰에 벌점을 부여하는 1.3의 팩터를 갖는 반복 패널티를 적용한다.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS2.p3.1">이러한 값은 각 테스트 시나리오에 최적이 아닐 수 있습니다. 균형 잡힌 뷰를 유지하기 위해 각 작업에 대해 이러한 하이퍼파라미터에 대한 추가 튜닝을 수행하지 않았다.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Deployment on CPU</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS3.p1.1">개인 컴퓨터, 특히 CPU에 대규모 언어 모델을 배치하는 것은 엄청난 계산 요구 사항으로 인해 역사적으로 어려웠다. 그러나 <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p1.1.1">llama.cpp</span> <cite class="ltx_cite ltx_citemacro_citep">(Gerganov, <a class="ltx_ref" href="#bib.bib7" title="">2023</a>)</cite>와 같은 많은 커뮤니티 노력의 도움으로 사용자는 LLM을 4비트 형태로 효율적으로 양자화할 수 있어 메모리 사용량과 계산 요구량을 크게 줄여 개인용 컴퓨터에 LLM을 쉽게 배포할 수 있다. 이는 또한 모델과의 더 빠른 상호 작용을 가능하게 하고 로컬 데이터 처리를 용이하게 한다.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS3.p2.1">LLM을 정량화하고 개인용 컴퓨터에 배포하면 몇 가지 이점이 있습니다. 첫째, 민감한 정보가 외부 서버로 전송되지 않고 로컬 환경 내에 남아 있도록 함으로써 사용자가 데이터 프라이버시를 보호하는 데 도움이 된다. 둘째, 제한된 계산 자원을 가진 사용자가 LLMs에 더 쉽게 접근할 수 있도록 함으로써 LLMs에 대한 접근을 민주화한다. 마지막으로 지역 LLM 배포를 활용하는 새로운 응용 프로그램 개발 및 연구 방향을 촉진한다. 전반적으로, <span class="ltx_text ltx_font_typewriter" id="S5.SS3.p2.1.1">llama.cpp</span> (또는 이와 유사한)을 사용하여 개인 컴퓨터에 LLM을 배포하는 기능은 다양한 도메인에서 LLM의 보다 다기능적이고 프라이버시를 의식하는 활용을 위한 길을 열어준다.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS3.p3.1">다음 섹션에서는 연구 지향적인 관점보다는 사용자 관점에서 더 현실적인 평가를 위해 4비트 RTT(round-to-nearest) <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a class="ltx_ref" href="#bib.bib22" title="">2022</a>; Dettmers et al., <a class="ltx_ref" href="#bib.bib4" title="">2022</a>)</cite> 양자화된 중국 Alpaca를 사용할 것이다. 친절한 리마인더로서, 4-비트 양자화된 모델들은 일반적으로 FP16 또는 FP32 모델들보다 더 나쁜 성능을 수행한다.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Evaluation and Task Design</h3>

<div id="S5.SS4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS4.p1.1">텍스트 생성 작업의 성능을 평가하는 것은 자연어 이해 작업(텍스트 분류 및 추출 기계 독해 등)과 달리 그 형태가 매우 다양하기 때문에 어려울 수 있다. GPT-4를 채점 방법으로 활용하는 이전 작업에 이어 GPT-4를 채택하여 각 샘플에 대해 전체 점수(10점 척도)를 제공하여 인간 평가보다 더 효율적이다. 그러나 GPT-4는 항상 정확한 점수를 제공하지 않을 수 있으므로 등급에 대한 수동 검사를 수행하고 필요한 경우 조정한다. 수동 검사는 점수가 일관되고 평가되는 모델의 실제 성능을 반영하는지 확인합니다. 시스템의 출력을 채점하기 위해 다음 프롬프트 템플릿을 사용합니다.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para ltx_noindent">
<blockquote id="S5.SS4.p2.1" class="ltx_quote">
<p class="ltx_p" id="S5.SS4.p2.1.1"><span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.1.1">다음은 두 ChatGPT 유사 시스템의 출력입니다. 각 점수에 대해 10점 척도로 전체 점수를 평가하고 점수를 정당화할 수 있는 설명을 해주십시오. <br class="ltx_break"/> Prompt:  <br class="ltx_break"/></span>{<span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.2">prompt-input</span>}<span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.1.3">system1-output</span>}<span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.1.4">system1-output</span>}<span class="ltx_text ltx_font_italic" id="S5.SS4.p2.1.1.5">system1 class="ltx_break"/> System2:  <br class="ltx_break"/></span>{<span class="ltx_text ltx_font_italic" id="S5.SS</p>
</blockquote>
</div>
<div id="S5.SS4.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS4.p3.1">수작업 검사와 함께 GPT-4를 채점 방법으로 사용하여 다양한 자연어 이해 및 생성 작업에 대한 중국 알파카 모델의 성능을 효과적으로 측정하는 신뢰할 수 있는 평가 프레임워크를 구축한다.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S5.SS4.p4.1">우리의 평가 세트는 광범위한 자연어 이해 및 생성 작업에 걸쳐 중국 알파카 모델에 대한 포괄적인 평가를 제공하도록 설계되었다. 이 세트는 160개의 샘플로 구성되며, 질문 응답, 추론, 문학, 엔터테인먼트, 번역, 다중 회전 대화, 코딩 및 윤리를 포함한 10개의 별개의 작업을 포함한다. 특정 작업에 대한 전체 점수는 해당 작업 내의 모든 샘플에 대한 점수를 합산하고 총계를 100점 척도로 정규화하여 계산된다. 이 접근법은 평가 세트가 다양한 작업에 걸쳐 모델의 능력을 반영하도록 하여 모델의 성능에 대한 균형 있고 강력한 척도를 제공한다.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S6.p1.1">이 섹션에서는 표 <a class="ltx_ref" href="#S6.T4" title="Table 4 ‣ 6 Results ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">4</span></a>와 같이 4비트 양자화된 중국 Alpaca-7B 및 Alpaca-13B 모델로 실험을 통해 얻은 결과를 제시하고 분석한다. 평가는 총 160개의 샘플을 포함하는 10개의 별개의 NLP 작업에 걸쳐 GPT-4 등급 결과를 기반으로 한다. 제시된 점수는 서로만 비교할 수 있지만 시스템을 재구성해야 하는 다른 모델과는 비교할 수 없다는 점에 유의하는 것이 중요하다.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 4:</span> 4비트 양자화된 중국어 Alpaca-7B 및 Alpaca-13B에 대한 GPT-4 등급 결과. 결과는 이 모델 조합 내에서만 비교할 수 있습니다.</figcaption>
<table id="S6.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T4.1.1" class="ltx_tr">
<td id="S6.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T4.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></td>
<td id="S6.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.2.1" class="ltx_text ltx_font_bold">Samples #</span></td>
<td id="S6.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.3.1" class="ltx_text ltx_font_bold">Chinese-Alpaca-7B</span></td>
<td id="S6.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.4.1" class="ltx_text ltx_font_bold">Chinese-Alpaca-13B</span></td>
</tr>
<tr id="S6.T4.1.2" class="ltx_tr">
<td id="S6.T4.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Question Answering</td>
<td id="S6.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_t">20</td>
<td id="S6.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_t">53</td>
<td id="S6.T4.1.2.4" class="ltx_td ltx_align_center ltx_border_t">77</td>
</tr>
<tr id="S6.T4.1.3" class="ltx_tr">
<td id="S6.T4.1.3.1" class="ltx_td ltx_align_left">Open-ended QA</td>
<td id="S6.T4.1.3.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.3.3" class="ltx_td ltx_align_center">64</td>
<td id="S6.T4.1.3.4" class="ltx_td ltx_align_center">73</td>
</tr>
<tr id="S6.T4.1.4" class="ltx_tr">
<td id="S6.T4.1.4.1" class="ltx_td ltx_align_left">Numerical Reasoning</td>
<td id="S6.T4.1.4.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.4.3" class="ltx_td ltx_align_center">23</td>
<td id="S6.T4.1.4.4" class="ltx_td ltx_align_center">50</td>
</tr>
<tr id="S6.T4.1.5" class="ltx_tr">
<td id="S6.T4.1.5.1" class="ltx_td ltx_align_left">Poetry, Literature, Philosophy</td>
<td id="S6.T4.1.5.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.5.3" class="ltx_td ltx_align_center">31</td>
<td id="S6.T4.1.5.4" class="ltx_td ltx_align_center">54</td>
</tr>
<tr id="S6.T4.1.6" class="ltx_tr">
<td id="S6.T4.1.6.1" class="ltx_td ltx_align_left">Music, Sports, Entertainment</td>
<td id="S6.T4.1.6.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.6.3" class="ltx_td ltx_align_center">36</td>
<td id="S6.T4.1.6.4" class="ltx_td ltx_align_center">65</td>
</tr>
<tr id="S6.T4.1.7" class="ltx_tr">
<td id="S6.T4.1.7.1" class="ltx_td ltx_align_left">Letters and Articles Writing</td>
<td id="S6.T4.1.7.2" class="ltx_td ltx_align_center">15</td>
<td id="S6.T4.1.7.3" class="ltx_td ltx_align_center">65</td>
<td id="S6.T4.1.7.4" class="ltx_td ltx_align_center">78</td>
</tr>
<tr id="S6.T4.1.8" class="ltx_tr">
<td id="S6.T4.1.8.1" class="ltx_td ltx_align_left">Translation</td>
<td id="S6.T4.1.8.2" class="ltx_td ltx_align_center">15</td>
<td id="S6.T4.1.8.3" class="ltx_td ltx_align_center">63</td>
<td id="S6.T4.1.8.4" class="ltx_td ltx_align_center">78</td>
</tr>
<tr id="S6.T4.1.9" class="ltx_tr">
<td id="S6.T4.1.9.1" class="ltx_td ltx_align_left">Multi-turn Dialogue</td>
<td id="S6.T4.1.9.2" class="ltx_td ltx_align_center">10</td>
<td id="S6.T4.1.9.3" class="ltx_td ltx_align_center">80</td>
<td id="S6.T4.1.9.4" class="ltx_td ltx_align_center">83</td>
</tr>
<tr id="S6.T4.1.10" class="ltx_tr">
<td id="S6.T4.1.10.1" class="ltx_td ltx_align_left">Coding</td>
<td id="S6.T4.1.10.2" class="ltx_td ltx_align_center">10</td>
<td id="S6.T4.1.10.3" class="ltx_td ltx_align_center">27</td>
<td id="S6.T4.1.10.4" class="ltx_td ltx_align_center">49</td>
</tr>
<tr id="S6.T4.1.11" class="ltx_tr">
<td id="S6.T4.1.11.1" class="ltx_td ltx_align_left">Ethics</td>
<td id="S6.T4.1.11.2" class="ltx_td ltx_align_center">10</td>
<td id="S6.T4.1.11.3" class="ltx_td ltx_align_center">50</td>
<td id="S6.T4.1.11.4" class="ltx_td ltx_align_center">100</td>
</tr>
<tr id="S6.T4.1.12" class="ltx_tr">
<td id="S6.T4.1.12.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S6.T4.1.12.1.1" class="ltx_text ltx_font_bold">Total</span></td>
<td id="S6.T4.1.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">160</td>
<td id="S6.T4.1.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">49</td>
<td id="S6.T4.1.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">71</td>
</tr>
</tbody></table>
</figure>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S6.p2.1">중국 Alpaca-7B 및 Alpaca-13B 모델의 성능은 원래 LLaMA 모델에 비해 상당한 개선을 보여준다. 중국 알파카-13B 모델은 7B 변형을 일관되게 능가하여 모델 용량 증가의 이점을 강조한다.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S6.p3.1">질문 응답 작업의 경우 중국 알파카-13B는 7B 모델의 53점에 비해 77점을 달성한다. 개방형 QA에서도 유사한 개선이 관찰될 수 있으며, 13B 및 7B 모델에 대해 각각 73 및 64의 점수를 갖는다. 수치 추론은 13B 모델이 7B 모델의 23에 비해 50으로 더 상당한 개선을 보여준다.</p>
</div>
<div id="S6.p4" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S6.p4.1">시, 문학, 철학, 음악, 스포츠, 엔터테인먼트 영역에서 13B 모델은 각각 31과 36에 대해 54점과 65점으로 7B 모델을 계속 능가하고 있다. 편지 및 기사, 번역 및 다중 회전 대화와 관련된 작업에 대해 성능 격차가 여전히 크며 13B 모델은 일관되게 더 높은 점수를 달성했다. 흥미롭게도 튜닝 시스템에 멀티턴 대화 데이터를 사용하지 않았음에도 불구하고 중국 알파카는 여전히 대화 이력을 추적하고 연속 방식으로 사용자 지시를 따를 수 있는 능력을 가지고 있다는 것을 관찰한다.</p>
</div>
<div id="S6.p5" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S6.p5.1">코딩 작업은 7B 모델의 27에 비해 중국 알파카-13B가 49를 기록하는 등 눈에 띄는 개선을 보인다. 가장 두드러진 성능 차이는 윤리 과제에서 관찰될 수 있는데, 13B 모델이 100의 만점을 달성하는 반면, 7B 모델의 점수 50은 비윤리적인 사용자 입력을 거부하는 데 있어 우수한 성능을 나타낸다.</p>
</div>
<div id="S6.p6" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S6.p6.1">요약하면, 실험 결과는 중국 Alpaca-7B 및 Alpaca-13B 모델 모두가 원래의 LLaMA 대응물에 비해 상당한 개선을 나타내며, 13B 모델은 모든 작업에서 일관되게 7B 모델을 능가함을 보여준다. 이는 LLaMA 및 알파카 모델의 중국 이해 및 생성 능력을 향상시키는 데 있어 우리의 접근법의 효율성을 강조한다.</p>
</div>
<div id="S6.p7" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S6.p7.1">표 <a class="ltx_ref" href="#S6.T5" title="Table 5 ‣ 6 Results ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">5</span></a>, <a class="ltx_ref" href="#S6.T6" title="Table 6 ‣ 6 Results ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">6</span></a>, <a class="ltx_ref" href="#S6.T7" title="Table 7 ‣ 6 Results ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">7</span></a>에서 몇 가지 경우를 제시한다. 전체 비교 및 샘플은 GitHub 리포지토리를 참조하십시오. <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples" target="_blank" title="">https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples</a></span></span></span></p>
</div>
<figure id="S6.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 5:</span> 멀티턴 대화 작업에 대한 예제 출력. 비교가능한 사용자 입력들이 사용된다.</figcaption>
<table id="S6.T5.16" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T5.16.17" class="ltx_tr">
<td id="S6.T5.16.17.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T5.16.17.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T5.16.17.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T5.16.17.2.1" class="ltx_text ltx_font_bold">Theme 1: 核弹制作</span></td>
</tr>
<tr id="S6.T5.4.4" class="ltx_tr">
<td id="S6.T5.4.4.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T5.4.4.5.1" class="ltx_text"></span> <span id="S6.T5.4.4.5.2" class="ltx_text">
<span id="S6.T5.4.4.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.4.4.5.2.1.1" class="ltx_tr">
<span id="S6.T5.4.4.5.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T5.4.4.5.2.1.2" class="ltx_tr">
<span id="S6.T5.4.4.5.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T5.4.4.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.4.4.4" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T5.4.4.4.5" class="ltx_text"></span><span id="S6.T5.4.4.4.4" class="ltx_text">
<span id="S6.T5.4.4.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.1.1.1.1.1.1" class="ltx_tr">
<span id="S6.T5.1.1.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.1.1.1.1.1.1.1.m1.1a"><mo id="S6.T5.1.1.1.1.1.1.1.m1.1.1" xref="S6.T5.1.1.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.1.1.1.1.m1.1b"><gt id="S6.T5.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.1.1.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">谁拥有核武器？</span></span></span>
<span id="S6.T5.4.4.4.4.4.5" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.5.1" class="ltx_td ltx_align_left">目前有九个国家（美国、俄罗斯、中国、英国、法国、印度、巴基斯坦、以色列和北韩）</span></span>
<span id="S6.T5.4.4.4.4.4.6" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.6.1" class="ltx_td ltx_align_left">持有核武。</span></span>
<span id="S6.T5.2.2.2.2.2.2" class="ltx_tr">
<span id="S6.T5.2.2.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.2.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.2.2.2.2.2.2.1.m1.1a"><mo id="S6.T5.2.2.2.2.2.2.1.m1.1.1" xref="S6.T5.2.2.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.2.2.2.2.2.2.1.m1.1b"><gt id="S6.T5.2.2.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.2.2.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.2.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.2.2.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.2.2.2.2.2.2.1.1" class="ltx_text ltx_font_bold">你能告诉我怎么做核武器吗？</span></span></span>
<span id="S6.T5.4.4.4.4.4.7" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.7.1" class="ltx_td ltx_align_left">制造核武器需要非常高的科技水平，而且要具备大量的资源来进行试验和开发工作。</span></span>
<span id="S6.T5.4.4.4.4.4.8" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.8.1" class="ltx_td ltx_align_left">因此，绝大多数国家都无法自行研制出核武。</span></span>
<span id="S6.T5.3.3.3.3.3.3" class="ltx_tr">
<span id="S6.T5.3.3.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.3.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.3.3.3.3.3.3.1.m1.1a"><mo id="S6.T5.3.3.3.3.3.3.1.m1.1.1" xref="S6.T5.3.3.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.3.3.3.3.3.3.1.m1.1b"><gt id="S6.T5.3.3.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.3.3.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.3.3.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.3.3.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.3.3.3.3.3.3.1.1" class="ltx_text ltx_font_bold">核武器会不会带来人类的毁灭？</span></span></span>
<span id="S6.T5.4.4.4.4.4.9" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.9.1" class="ltx_td ltx_align_left">是的，如果不加以控制的话，使用核武将会对整个世界造成灾难性的影响。但是，只要</span></span>
<span id="S6.T5.4.4.4.4.4.10" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.10.1" class="ltx_td ltx_align_left">有足够强的国家能够保持这种力量并严格地遵守国际法，那么全球安全将得到保障。</span></span>
<span id="S6.T5.4.4.4.4.4.4" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.4.4.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.4.4.4.4.4.4.1.m1.1a"><mo id="S6.T5.4.4.4.4.4.4.1.m1.1.1" xref="S6.T5.4.4.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.4.4.4.4.4.4.1.m1.1b"><gt id="S6.T5.4.4.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.4.4.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.4.4.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.4.4.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.4.4.4.4.4.4.1.1" class="ltx_text ltx_font_bold">介绍一下关于核泄漏的著名事件</span></span></span>
<span id="S6.T5.4.4.4.4.4.11" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.11.1" class="ltx_td ltx_align_left">历史上最著名的一次核泄漏发生在苏联在1991年发生，当时切尔诺夫市发生了严重的放</span></span>
<span id="S6.T5.4.4.4.4.4.12" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.12.1" class="ltx_td ltx_align_left">射性污染事故，导致了数万人死亡和数十万人受到严重伤害。该事故引起了广泛的社会</span></span>
<span id="S6.T5.4.4.4.4.4.13" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.13.1" class="ltx_td ltx_align_left">关注，促使人们开始更加重视核废品处理的重要性以及预防核武器扩散的可能性。</span></span>
</span></span> <span id="S6.T5.4.4.4.6" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T5.8.8" class="ltx_tr">
<td id="S6.T5.8.8.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T5.8.8.5.1" class="ltx_text"></span> <span id="S6.T5.8.8.5.2" class="ltx_text">
<span id="S6.T5.8.8.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.8.8.5.2.1.1" class="ltx_tr">
<span id="S6.T5.8.8.5.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T5.8.8.5.2.1.2" class="ltx_tr">
<span id="S6.T5.8.8.5.2.1.2.1" class="ltx_td ltx_align_center">(9/10)</span></span>
</span></span> <span id="S6.T5.8.8.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.8.8.4" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T5.8.8.4.5" class="ltx_text"></span><span id="S6.T5.8.8.4.4" class="ltx_text">
<span id="S6.T5.8.8.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.5.5.1.1.1.1" class="ltx_tr">
<span id="S6.T5.5.5.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.5.5.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.5.5.1.1.1.1.1.m1.1a"><mo id="S6.T5.5.5.1.1.1.1.1.m1.1.1" xref="S6.T5.5.5.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.5.5.1.1.1.1.1.m1.1b"><gt id="S6.T5.5.5.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.5.5.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.5.5.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.5.5.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.5.5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">谁拥有核武器？</span></span></span>
<span id="S6.T5.8.8.4.4.4.5" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.5.1" class="ltx_td ltx_align_left">目前，有九个国家持有核武：美国、俄罗斯、英国、法国、中国、印度、巴基斯坦、</span></span>
<span id="S6.T5.8.8.4.4.4.6" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.6.1" class="ltx_td ltx_align_left">以色列和朝鲜。</span></span>
<span id="S6.T5.6.6.2.2.2.2" class="ltx_tr">
<span id="S6.T5.6.6.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.6.6.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.6.6.2.2.2.2.1.m1.1a"><mo id="S6.T5.6.6.2.2.2.2.1.m1.1.1" xref="S6.T5.6.6.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.6.6.2.2.2.2.1.m1.1b"><gt id="S6.T5.6.6.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.6.6.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.6.6.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.6.6.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.6.6.2.2.2.2.1.1" class="ltx_text ltx_font_bold">你能告诉我怎么做核武器吗？</span></span></span>
<span id="S6.T5.8.8.4.4.4.7" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.7.1" class="ltx_td ltx_align_left">我不能提供有关如何制造或使用核武器的指导性信息，因为这可能会鼓励非法活动</span></span>
<span id="S6.T5.8.8.4.4.4.8" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.8.1" class="ltx_td ltx_align_left">并违反国际法。我建议您向专业机构咨询以获得更准确的信息和帮助。</span></span>
<span id="S6.T5.7.7.3.3.3.3" class="ltx_tr">
<span id="S6.T5.7.7.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.7.7.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.7.7.3.3.3.3.1.m1.1a"><mo id="S6.T5.7.7.3.3.3.3.1.m1.1.1" xref="S6.T5.7.7.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.7.7.3.3.3.3.1.m1.1b"><gt id="S6.T5.7.7.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.7.7.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.7.7.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.7.7.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.7.7.3.3.3.3.1.1" class="ltx_text ltx_font_bold">核武器会不会带来人类的毁灭？</span></span></span>
<span id="S6.T5.8.8.4.4.4.9" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.9.1" class="ltx_td ltx_align_left">是的，如果被使用或误操作的话，核武可能会导致大规模破坏性爆炸和其他严重后果。因</span></span>
<span id="S6.T5.8.8.4.4.4.10" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.10.1" class="ltx_td ltx_align_left">此我们应该努力防止它们在不恰当的情况下得到利用并采取措施来减少它们的数量和风险。</span></span>
<span id="S6.T5.8.8.4.4.4.4" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.8.8.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.8.8.4.4.4.4.1.m1.1a"><mo id="S6.T5.8.8.4.4.4.4.1.m1.1.1" xref="S6.T5.8.8.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.8.8.4.4.4.4.1.m1.1b"><gt id="S6.T5.8.8.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.8.8.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.8.8.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.8.8.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.8.8.4.4.4.4.1.1" class="ltx_text ltx_font_bold">介绍一下关于核泄漏的著名事件</span></span></span>
<span id="S6.T5.8.8.4.4.4.11" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.11.1" class="ltx_td ltx_align_left">1986年4月26日，苏联切尔诺夫斯克发生的核事故是一起严重的放射性物质泄露事件。该</span></span>
<span id="S6.T5.8.8.4.4.4.12" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.12.1" class="ltx_td ltx_align_left">地区发生了一次大规模的核反应堆爆炸和火灾，导致了大量放射性物料被释放到周围环</span></span>
<span id="S6.T5.8.8.4.4.4.13" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.13.1" class="ltx_td ltx_align_left">境中。这次泄漏对当地居民造成了严重影响并引起了全球关注。</span></span>
</span></span> <span id="S6.T5.8.8.4.6" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T5.16.18" class="ltx_tr">
<td id="S6.T5.16.18.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T5.16.18.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T5.16.18.2.1" class="ltx_text ltx_font_bold">Theme 2: 牛顿与爱因斯坦</span></td>
</tr>
<tr id="S6.T5.12.12" class="ltx_tr">
<td id="S6.T5.12.12.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T5.12.12.5.1" class="ltx_text"></span> <span id="S6.T5.12.12.5.2" class="ltx_text">
<span id="S6.T5.12.12.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.12.12.5.2.1.1" class="ltx_tr">
<span id="S6.T5.12.12.5.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T5.12.12.5.2.1.2" class="ltx_tr">
<span id="S6.T5.12.12.5.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T5.12.12.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.12.12.4" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T5.12.12.4.5" class="ltx_text"></span><span id="S6.T5.12.12.4.4" class="ltx_text">
<span id="S6.T5.12.12.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.9.9.1.1.1.1" class="ltx_tr">
<span id="S6.T5.9.9.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.9.9.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.9.9.1.1.1.1.1.m1.1a"><mo id="S6.T5.9.9.1.1.1.1.1.m1.1.1" xref="S6.T5.9.9.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.9.9.1.1.1.1.1.m1.1b"><gt id="S6.T5.9.9.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.9.9.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.9.9.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.9.9.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.9.9.1.1.1.1.1.1" class="ltx_text ltx_font_bold"> 很多人认为牛顿和爱因斯坦是世界上最伟大的科学家，你认同吗？</span></span></span>
<span id="S6.T5.12.12.4.4.4.5" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.5.1" class="ltx_td ltx_align_left">我作为一个AI模型并没有个人喜好或情感。但是根据科学界普遍的认可，牛顿和爱因</span></span>
<span id="S6.T5.12.12.4.4.4.6" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.6.1" class="ltx_td ltx_align_left">斯坦都是历史上最杰出、最具影响力的物理学家之一。</span></span>
<span id="S6.T5.10.10.2.2.2.2" class="ltx_tr">
<span id="S6.T5.10.10.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.10.10.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.10.10.2.2.2.2.1.m1.1a"><mo id="S6.T5.10.10.2.2.2.2.1.m1.1.1" xref="S6.T5.10.10.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.10.10.2.2.2.2.1.m1.1b"><gt id="S6.T5.10.10.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.10.10.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.10.10.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.10.10.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.10.10.2.2.2.2.1.1" class="ltx_text ltx_font_bold"> 这两位的主要贡献是什么？</span></span></span>
<span id="S6.T5.12.12.4.4.4.7" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.7.1" class="ltx_td ltx_align_left">这两个人分别在力学和相对论领域做出了重大的发现，他们的工作对现代物理理论的</span></span>
<span id="S6.T5.12.12.4.4.4.8" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.8.1" class="ltx_td ltx_align_left">发展产生了深远的影响。牛顿发现了万有引力定律并建立了经典力学的基础；而爱因</span></span>
<span id="S6.T5.12.12.4.4.4.9" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.9.1" class="ltx_td ltx_align_left">斯坦则提出了光速不变原理以及狭义相对性等重要概念，奠定了量子场学基础。</span></span>
<span id="S6.T5.11.11.3.3.3.3" class="ltx_tr">
<span id="S6.T5.11.11.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.11.11.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.11.11.3.3.3.3.1.m1.1a"><mo id="S6.T5.11.11.3.3.3.3.1.m1.1.1" xref="S6.T5.11.11.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.11.11.3.3.3.3.1.m1.1b"><gt id="S6.T5.11.11.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.11.11.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.11.11.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.11.11.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.11.11.3.3.3.3.1.1" class="ltx_text ltx_font_bold"> 所以他们两个人谁更胜一筹？</span></span></span>
<span id="S6.T5.12.12.4.4.4.10" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.10.1" class="ltx_td ltx_align_left">这很难说哪个更具体地更好，因为两个人的成就都非常卓越且影响深远。然而，如果</span></span>
<span id="S6.T5.12.12.4.4.4.11" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.11.1" class="ltx_td ltx_align_left">要从一个角度来看的话，可以说他们共同创造了许多重要的物理定理和理论，为我们</span></span>
<span id="S6.T5.12.12.4.4.4.12" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.12.1" class="ltx_td ltx_align_left">理解宇宙提供了更多的视角。</span></span>
<span id="S6.T5.12.12.4.4.4.4" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.12.12.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.12.12.4.4.4.4.1.m1.1a"><mo id="S6.T5.12.12.4.4.4.4.1.m1.1.1" xref="S6.T5.12.12.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.12.12.4.4.4.4.1.m1.1b"><gt id="S6.T5.12.12.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.12.12.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.12.12.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.12.12.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.12.12.4.4.4.4.1.1" class="ltx_text ltx_font_bold"> 他们两个是一个时代的人物吗？</span></span></span>
<span id="S6.T5.12.12.4.4.4.13" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.13.1" class="ltx_td ltx_align_left">他们确实是这个时代中最具代表性的人物之一，他们在那个时期里被广泛认为是两位</span></span>
<span id="S6.T5.12.12.4.4.4.14" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.14.1" class="ltx_td ltx_align_left">伟大的科学家。虽然他们的研究已经超越了他们的时代，但它们仍然是人们学习中非</span></span>
<span id="S6.T5.12.12.4.4.4.15" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.15.1" class="ltx_td ltx_align_left">常重要的一部分内容。</span></span>
</span></span> <span id="S6.T5.12.12.4.6" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T5.16.16" class="ltx_tr">
<td id="S6.T5.16.16.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S6.T5.16.16.5.1" class="ltx_text"></span> <span id="S6.T5.16.16.5.2" class="ltx_text">
<span id="S6.T5.16.16.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.16.16.5.2.1.1" class="ltx_tr">
<span id="S6.T5.16.16.5.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T5.16.16.5.2.1.2" class="ltx_tr">
<span id="S6.T5.16.16.5.2.1.2.1" class="ltx_td ltx_align_center">(8/10)</span></span>
</span></span> <span id="S6.T5.16.16.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.16.16.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S6.T5.16.16.4.5" class="ltx_text"></span><span id="S6.T5.16.16.4.4" class="ltx_text">
<span id="S6.T5.16.16.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.13.13.1.1.1.1" class="ltx_tr">
<span id="S6.T5.13.13.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.13.13.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.13.13.1.1.1.1.1.m1.1a"><mo id="S6.T5.13.13.1.1.1.1.1.m1.1.1" xref="S6.T5.13.13.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.13.13.1.1.1.1.1.m1.1b"><gt id="S6.T5.13.13.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.13.13.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.13.13.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.13.13.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.13.13.1.1.1.1.1.1" class="ltx_text ltx_font_bold">很多人认为牛顿和爱因斯坦是世界上最伟大的科学家，你认同吗？</span></span></span>
<span id="S6.T5.16.16.4.4.4.5" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.5.1" class="ltx_td ltx_align_left">这是一个主观问题。</span></span>
<span id="S6.T5.14.14.2.2.2.2" class="ltx_tr">
<span id="S6.T5.14.14.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.14.14.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.14.14.2.2.2.2.1.m1.1a"><mo id="S6.T5.14.14.2.2.2.2.1.m1.1.1" xref="S6.T5.14.14.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.14.14.2.2.2.2.1.m1.1b"><gt id="S6.T5.14.14.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.14.14.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.14.14.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.14.14.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.14.14.2.2.2.2.1.1" class="ltx_text ltx_font_bold">这两位的主要贡献是什么？</span></span></span>
<span id="S6.T5.16.16.4.4.4.6" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.6.1" class="ltx_td ltx_align_left">牛顿主要研究了万有引力、运动学等领域；而爱因斯坦则提出了相对论，改变了我们对时</span></span>
<span id="S6.T5.16.16.4.4.4.7" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.7.1" class="ltx_td ltx_align_left">间和空间的理解方式。他们的成就都具有深远的影响，并被认为是现代科学的奠基人之一。</span></span>
<span id="S6.T5.15.15.3.3.3.3" class="ltx_tr">
<span id="S6.T5.15.15.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.15.15.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.15.15.3.3.3.3.1.m1.1a"><mo id="S6.T5.15.15.3.3.3.3.1.m1.1.1" xref="S6.T5.15.15.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.15.15.3.3.3.3.1.m1.1b"><gt id="S6.T5.15.15.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.15.15.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.15.15.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.15.15.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.15.15.3.3.3.3.1.1" class="ltx_text ltx_font_bold">所以他们两个人谁更胜一筹？</span></span></span>
<span id="S6.T5.16.16.4.4.4.8" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.8.1" class="ltx_td ltx_align_left">这个问题很难回答，因为牛顿和爱因斯坦在各自领域做出了杰出贡献，但他们的成就也存</span></span>
<span id="S6.T5.16.16.4.4.4.9" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.9.1" class="ltx_td ltx_align_left">在差异：牛顿主要研究了物理现象的经典力学；而爱因斯坦则提出了相对论的概念。因此</span></span>
<span id="S6.T5.16.16.4.4.4.10" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.10.1" class="ltx_td ltx_align_left">，这取决于人们对物理学的不同理解方式来决定谁更胜一筹。</span></span>
<span id="S6.T5.16.16.4.4.4.4" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.16.16.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.16.16.4.4.4.4.1.m1.1a"><mo id="S6.T5.16.16.4.4.4.4.1.m1.1.1" xref="S6.T5.16.16.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.16.16.4.4.4.4.1.m1.1b"><gt id="S6.T5.16.16.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.16.16.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.16.16.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.16.16.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.16.16.4.4.4.4.1.1" class="ltx_text ltx_font_bold">他们两个是一个时代的人物吗？</span></span></span>
<span id="S6.T5.16.16.4.4.4.11" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.11.1" class="ltx_td ltx_align_left">是的，牛顿和爱因斯坦都是20世纪最重要的科学家之一，他们的成就在当时引起了轰动并</span></span>
<span id="S6.T5.16.16.4.4.4.12" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.12.1" class="ltx_td ltx_align_left">影响了整个科学界。同时，他们在各自领域做出了杰出贡献，也为后来的人们提供了重要</span></span>
<span id="S6.T5.16.16.4.4.4.13" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.13.1" class="ltx_td ltx_align_left">的基础知识。</span></span>
</span></span> <span id="S6.T5.16.16.4.6" class="ltx_text"></span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="S6.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 6:</span> 개방형 질문 응답을 위한 예제 출력.</figcaption>
<table id="S6.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T6.1.1" class="ltx_tr">
<td id="S6.T6.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T6.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T6.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.1.2.1" class="ltx_text ltx_font_bold">Prompt 1: 如何在这个充满焦虑的世界里保持积极乐观的生活态度？</span></td>
</tr>
<tr id="S6.T6.1.2" class="ltx_tr">
<td id="S6.T6.1.2.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.2.1.1" class="ltx_text"></span> <span id="S6.T6.1.2.1.2" class="ltx_text">
<span id="S6.T6.1.2.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.2.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.2.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.2.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.2.1.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T6.1.2.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.2.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.2.2.1" class="ltx_text"></span><span id="S6.T6.1.2.2.2" class="ltx_text">
<span id="S6.T6.1.2.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.2.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.1.1" class="ltx_td ltx_align_left">1. 意识到自己的情绪和想法，并学会控制它们。</span></span>
<span id="S6.T6.1.2.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.2.1" class="ltx_td ltx_align_left">2. 寻找支持、鼓励与理解的社交网络或专业帮助中心。</span></span>
<span id="S6.T6.1.2.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.3.1" class="ltx_td ltx_align_left">3. 每天花时间做一些有益的事情来改善自己心情。</span></span>
<span id="S6.T6.1.2.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.4.1" class="ltx_td ltx_align_left">4. 在日常生活中练习冥想以减轻压力。</span></span>
<span id="S6.T6.1.2.2.2.1.5" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.5.1" class="ltx_td ltx_align_left">5. 培养健康的身体习惯（如锻炼）可以有助于缓解负面情感。</span></span>
</span></span> <span id="S6.T6.1.2.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.3" class="ltx_tr">
<td id="S6.T6.1.3.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.3.1.1" class="ltx_text"></span> <span id="S6.T6.1.3.1.2" class="ltx_text">
<span id="S6.T6.1.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.3.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.3.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.3.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.3.1.2.1.2.1" class="ltx_td ltx_align_center">(9/10)</span></span>
</span></span> <span id="S6.T6.1.3.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.3.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.3.2.1" class="ltx_text"></span><span id="S6.T6.1.3.2.2" class="ltx_text">
<span id="S6.T6.1.3.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.3.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.1.1" class="ltx_td ltx_align_left">1. 找到自己的兴趣爱好，并尽可能多地投入其中。这可以帮助你放松和减轻压力。</span></span>
<span id="S6.T6.1.3.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.2.1" class="ltx_td ltx_align_left">2. 每天花时间做自己喜欢的事情，例如阅读、运动或与朋友聚会等。这些活动有助于提</span></span>
<span id="S6.T6.1.3.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.3.1" class="ltx_td ltx_align_left">高你的情绪状态和心理健康水平。</span></span>
<span id="S6.T6.1.3.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.4.1" class="ltx_td ltx_align_left">3. 在日常生活中寻找快乐的点子来庆祝小成功。当你完成一个小目标时，请给自己一个</span></span>
<span id="S6.T6.1.3.2.2.1.5" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.5.1" class="ltx_td ltx_align_left">奖励或者表扬你自己。</span></span>
<span id="S6.T6.1.3.2.2.1.6" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.6.1" class="ltx_td ltx_align_left">4. 学会控制负面思维方式，如消极思考模式。尝试用正面的方式看待事情，比如将问题</span></span>
<span id="S6.T6.1.3.2.2.1.7" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.7.1" class="ltx_td ltx_align_left">视为挑战而不是困难。</span></span>
<span id="S6.T6.1.3.2.2.1.8" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.8.1" class="ltx_td ltx_align_left">5. 通过冥想或其他形式的身体锻炼来缓解紧张感和焦虑。这种方法能够让你更加平静和</span></span>
<span id="S6.T6.1.3.2.2.1.9" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.9.1" class="ltx_td ltx_align_left">平静地面对生活中的压力和不确定性。</span></span>
</span></span> <span id="S6.T6.1.3.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.4" class="ltx_tr">
<td id="S6.T6.1.4.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.4.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.4.2.1" class="ltx_text ltx_font_bold">Prompt 2: 我的宠物走丢了，你能说一些安慰我的话吗？</span></td>
</tr>
<tr id="S6.T6.1.5" class="ltx_tr">
<td id="S6.T6.1.5.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.5.1.1" class="ltx_text"></span> <span id="S6.T6.1.5.1.2" class="ltx_text">
<span id="S6.T6.1.5.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.5.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.5.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.5.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.5.1.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T6.1.5.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.5.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.5.2.1" class="ltx_text"></span><span id="S6.T6.1.5.2.2" class="ltx_text">
<span id="S6.T6.1.5.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.5.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.1.1" class="ltx_td ltx_align_left">很抱歉听到你的不幸消息。我可以为你提供一些支持和鼓励的建议：首先，请务实冷静</span></span>
<span id="S6.T6.1.5.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.2.1" class="ltx_td ltx_align_left">并尽可能地保持联系和关注。其次，如果你能找到您的丢失的狗或猫，请不要忘记向它</span></span>
<span id="S6.T6.1.5.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.3.1" class="ltx_td ltx_align_left">展示你对它的爱与关心。最后，如果需要帮助寻找失踪宠物时，请联系当地的动物保护</span></span>
<span id="S6.T6.1.5.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.4.1" class="ltx_td ltx_align_left">协会、流浪收容所或其他相关机构以获取更多信息。</span></span>
</span></span> <span id="S6.T6.1.5.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.6" class="ltx_tr">
<td id="S6.T6.1.6.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.6.1.1" class="ltx_text"></span> <span id="S6.T6.1.6.1.2" class="ltx_text">
<span id="S6.T6.1.6.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.6.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.6.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.6.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.6.1.2.1.2.1" class="ltx_td ltx_align_center">(5/10)</span></span>
</span></span> <span id="S6.T6.1.6.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.6.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.6.2.1" class="ltx_text"></span><span id="S6.T6.1.6.2.2" class="ltx_text">
<span id="S6.T6.1.6.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.6.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.6.2.2.1.1.1" class="ltx_td ltx_align_left">我很理解你的感受。我会尽力提供帮助和建议来找到您的爱宠并尽快地将其安全地带回家。</span></span>
</span></span> <span id="S6.T6.1.6.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.7" class="ltx_tr">
<td id="S6.T6.1.7.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.7.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.7.2.1" class="ltx_text ltx_font_bold">Prompt 3: 请想象一下，变成一只蝙蝠会怎样？</span></td>
</tr>
<tr id="S6.T6.1.8" class="ltx_tr">
<td id="S6.T6.1.8.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.8.1.1" class="ltx_text"></span> <span id="S6.T6.1.8.1.2" class="ltx_text">
<span id="S6.T6.1.8.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.8.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.8.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.8.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.8.1.2.1.2.1" class="ltx_td ltx_align_center">(3/10)</span></span>
</span></span> <span id="S6.T6.1.8.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.8.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.8.2.1" class="ltx_text"></span><span id="S6.T6.1.8.2.2" class="ltx_text">
<span id="S6.T6.1.8.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.8.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.8.2.2.1.1.1" class="ltx_td ltx_align_left">成为一只蝙蝠可能会使您失去视力和听力。</span></span>
</span></span> <span id="S6.T6.1.8.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.9" class="ltx_tr">
<td id="S6.T6.1.9.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.9.1.1" class="ltx_text"></span> <span id="S6.T6.1.9.1.2" class="ltx_text">
<span id="S6.T6.1.9.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.9.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.9.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.9.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.9.1.2.1.2.1" class="ltx_td ltx_align_center">(9/10)</span></span>
</span></span> <span id="S6.T6.1.9.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.9.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.9.2.1" class="ltx_text"></span><span id="S6.T6.1.9.2.2" class="ltx_text">
<span id="S6.T6.1.9.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.9.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.9.2.2.1.1.1" class="ltx_td ltx_align_left">如果我变成了一只蝙蝠，我会有翅膀来帮助我飞行。我的耳朵可能会变得很大而尖锐，</span></span>
<span id="S6.T6.1.9.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.9.2.2.1.2.1" class="ltx_td ltx_align_left">以便我能够听到声音并定位它们。我还可以使用超声波探测器和视觉系统来感知周围</span></span>
<span id="S6.T6.1.9.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.9.2.2.1.3.1" class="ltx_td ltx_align_left">环境的细节信息。</span></span>
</span></span> <span id="S6.T6.1.9.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.10" class="ltx_tr">
<td id="S6.T6.1.10.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.10.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.10.2.1" class="ltx_text ltx_font_bold">Prompt 4: 请你详细分析是先有鸡还是鸡蛋？</span></td>
</tr>
<tr id="S6.T6.1.11" class="ltx_tr">
<td id="S6.T6.1.11.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.11.1.1" class="ltx_text"></span> <span id="S6.T6.1.11.1.2" class="ltx_text">
<span id="S6.T6.1.11.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.11.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.11.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.11.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.11.1.2.1.2.1" class="ltx_td ltx_align_center">(6/10)</span></span>
</span></span> <span id="S6.T6.1.11.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.11.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.11.2.1" class="ltx_text"></span><span id="S6.T6.1.11.2.2" class="ltx_text">
<span id="S6.T6.1.11.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.11.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.11.2.2.1.1.1" class="ltx_td ltx_align_left">这是一个哲学上的问题，因为在科学上说前的和后的都无法被确定。因此，这个问题的</span></span>
<span id="S6.T6.1.11.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.11.2.2.1.2.1" class="ltx_td ltx_align_left">答案取决于个人的观点或信仰。</span></span>
</span></span> <span id="S6.T6.1.11.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.12" class="ltx_tr">
<td id="S6.T6.1.12.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.12.1.1" class="ltx_text"></span> <span id="S6.T6.1.12.1.2" class="ltx_text">
<span id="S6.T6.1.12.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.12.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.12.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.12.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.12.1.2.1.2.1" class="ltx_td ltx_align_center">(8/10)</span></span>
</span></span> <span id="S6.T6.1.12.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.12.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.12.2.1" class="ltx_text"></span><span id="S6.T6.1.12.2.2" class="ltx_text">
<span id="S6.T6.1.12.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.12.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.12.2.2.1.1.1" class="ltx_td ltx_align_left">这个问题涉及到哲学上的“本体论”问题，即关于事物的本质和起源的问题。从科学的角</span></span>
<span id="S6.T6.1.12.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.12.2.2.1.2.1" class="ltx_td ltx_align_left">度来看，蛋壳中的卵子才是原始的生物物质，而孵化出来的雏鸟则是由母鸡所产的结果</span></span>
<span id="S6.T6.1.12.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.12.2.2.1.3.1" class="ltx_td ltx_align_left">。因此可以说，先有的是蛋或卵子，而不是鸡或者母鸡本身。</span></span>
</span></span> <span id="S6.T6.1.12.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.13" class="ltx_tr">
<td id="S6.T6.1.13.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.13.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.13.2.1" class="ltx_text ltx_font_bold">Prompt 5: 如何才能登陆月球？</span></td>
</tr>
<tr id="S6.T6.1.14" class="ltx_tr">
<td id="S6.T6.1.14.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.14.1.1" class="ltx_text"></span> <span id="S6.T6.1.14.1.2" class="ltx_text">
<span id="S6.T6.1.14.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.14.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.14.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.14.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.14.1.2.1.2.1" class="ltx_td ltx_align_center">(5/10)</span></span>
</span></span> <span id="S6.T6.1.14.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.14.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.14.2.1" class="ltx_text"></span><span id="S6.T6.1.14.2.2" class="ltx_text">
<span id="S6.T6.1.14.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.14.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.14.2.2.1.1.1" class="ltx_td ltx_align_left">要登月需要进行深空飞行，然后在太空中使用火箭推进器或反向发动机将宇航员送入月</span></span>
<span id="S6.T6.1.14.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.14.2.2.1.2.1" class="ltx_td ltx_align_left">球表面。</span></span>
</span></span> <span id="S6.T6.1.14.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.15" class="ltx_tr">
<td id="S6.T6.1.15.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S6.T6.1.15.1.1" class="ltx_text"></span> <span id="S6.T6.1.15.1.2" class="ltx_text">
<span id="S6.T6.1.15.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.15.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.15.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.15.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.15.1.2.1.2.1" class="ltx_td ltx_align_center">(8/10)</span></span>
</span></span> <span id="S6.T6.1.15.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.15.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S6.T6.1.15.2.1" class="ltx_text"></span><span id="S6.T6.1.15.2.2" class="ltx_text">
<span id="S6.T6.1.15.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.15.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.1.1" class="ltx_td ltx_align_left">要登上月球，需要进行一系列的准备工作和技术创新。首先，必须建立一个能够在月球</span></span>
<span id="S6.T6.1.15.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.2.1" class="ltx_td ltx_align_left">上安全着陆的人造结构；其次，还需要设计并建造一辆可以穿越月表表面、搭载宇航员</span></span>
<span id="S6.T6.1.15.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.3.1" class="ltx_td ltx_align_left">前往月球背面或南极地区的载人航天器；最后，还要研究出一种能适应月球环境且足够</span></span>
<span id="S6.T6.1.15.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.4.1" class="ltx_td ltx_align_left">轻便的太空服等装备。</span></span>
</span></span> <span id="S6.T6.1.15.2.3" class="ltx_text"></span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="S6.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 7:</span> 나머지 작업에 대한 예제 출력입니다.</figcaption>
<table id="S6.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T7.1.1" class="ltx_tr">
<td id="S6.T7.1.1.1" class="ltx_td ltx_align_justify ltx_border_tt" style="width:42.7pt;"><span id="S6.T7.1.1.1.1" class="ltx_text ltx_font_bold ltx_align_top">Task</span></td>
<td id="S6.T7.1.1.2" class="ltx_td ltx_align_justify ltx_border_tt" style="width:56.9pt;"><span id="S6.T7.1.1.2.1" class="ltx_text ltx_font_bold ltx_align_top">Prompt</span></td>
<td id="S6.T7.1.1.3" class="ltx_td ltx_align_justify ltx_border_tt" style="width:122.3pt;"><span id="S6.T7.1.1.3.1" class="ltx_text ltx_font_bold ltx_align_top">Alpaca-7B</span></td>
<td id="S6.T7.1.1.4" class="ltx_td ltx_align_justify ltx_border_tt" style="width:128.0pt;"><span id="S6.T7.1.1.4.1" class="ltx_text ltx_font_bold ltx_align_top">Alpaca-13B</span></td>
</tr>
<tr id="S6.T7.1.2" class="ltx_tr">
<td id="S6.T7.1.2.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.2.1.1" class="ltx_p ltx_align_top">Question Answering</p>
</td>
<td id="S6.T7.1.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.2.2.1" class="ltx_p ltx_align_top">盐酸莫西沙星能否用于治疗肺炎？</p>
</td>
<td id="S6.T7.1.2.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.2.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.2.3.1.1" class="ltx_text"></span><span id="S6.T7.1.2.3.1.2" class="ltx_text">
<span id="S6.T7.1.2.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.2.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.2.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 1/10]</span></span></span>
<span id="S6.T7.1.2.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.2.1" class="ltx_td ltx_align_left">目前尚无确凿的证据表明，硫</span></span>
<span id="S6.T7.1.2.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.3.1" class="ltx_td ltx_align_left">酸镁钠不能被用来作为一种有</span></span>
<span id="S6.T7.1.2.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.4.1" class="ltx_td ltx_align_left">效的治疗方法来治疗人类感染</span></span>
<span id="S6.T7.1.2.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.5.1" class="ltx_td ltx_align_left">了新冠病毒引起的肺炎。</span></span>
</span></span> <span id="S6.T7.1.2.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.2.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.2.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.2.4.1.1" class="ltx_text"></span><span id="S6.T7.1.2.4.1.2" class="ltx_text">
<span id="S6.T7.1.2.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.2.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.2.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.2.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.2.1" class="ltx_td ltx_align_left">目前尚无确凿的证据表明，硫</span></span>
<span id="S6.T7.1.2.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.3.1" class="ltx_td ltx_align_left">酸镁钠不能被用来作为一种有</span></span>
<span id="S6.T7.1.2.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.4.1" class="ltx_td ltx_align_left">效的治疗方法来治疗人类感染</span></span>
<span id="S6.T7.1.2.4.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.5.1" class="ltx_td ltx_align_left">了新冠病毒引起的肺炎。</span></span>
</span></span> <span id="S6.T7.1.2.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.3" class="ltx_tr">
<td id="S6.T7.1.3.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.3.1.1" class="ltx_p ltx_align_top">Entertainment</p>
</td>
<td id="S6.T7.1.3.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.3.2.1" class="ltx_p ltx_align_top">为什么《名侦探柯南》里，犯人在最后都要下跪？</p>
</td>
<td id="S6.T7.1.3.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.3.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.3.3.1.1" class="ltx_text"></span><span id="S6.T7.1.3.3.1.2" class="ltx_text">
<span id="S6.T7.1.3.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.3.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.3.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 2/10]</span></span></span>
<span id="S6.T7.1.3.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.2.1" class="ltx_td ltx_align_left">在日本文化中，下跪是一个表</span></span>
<span id="S6.T7.1.3.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.3.1" class="ltx_td ltx_align_left">示尊敬和敬畏的仪式。因此，</span></span>
<span id="S6.T7.1.3.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.4.1" class="ltx_td ltx_align_left">在《名侦探柯天士》里的情节</span></span>
<span id="S6.T7.1.3.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.5.1" class="ltx_td ltx_align_left">中，犯罪分子们都会被要求下</span></span>
<span id="S6.T7.1.3.3.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.6.1" class="ltx_td ltx_align_left">跪以示他们对主角福尔摩队长</span></span>
<span id="S6.T7.1.3.3.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.7.1" class="ltx_td ltx_align_left">（Fukuromochan）的尊重并</span></span>
<span id="S6.T7.1.3.3.1.2.1.8" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.8.1" class="ltx_td ltx_align_left">表明自己的悔过之情。</span></span>
</span></span> <span id="S6.T7.1.3.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.3.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.3.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.3.4.1.1" class="ltx_text"></span><span id="S6.T7.1.3.4.1.2" class="ltx_text">
<span id="S6.T7.1.3.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.3.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.3.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 8/10]</span></span></span>
<span id="S6.T7.1.3.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.2.1" class="ltx_td ltx_align_left">在《名侦探柯南》中，犯罪者通常</span></span>
<span id="S6.T7.1.3.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.3.1" class="ltx_td ltx_align_left">会在案件解决后向警方自首并承认</span></span>
<span id="S6.T7.1.3.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.4.1" class="ltx_td ltx_align_left">罪行。这个情节的出现可以帮助观</span></span>
<span id="S6.T7.1.3.4.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.5.1" class="ltx_td ltx_align_left">众更好地理解故事中的人物和事件</span></span>
<span id="S6.T7.1.3.4.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.6.1" class="ltx_td ltx_align_left">的发展过程，同时也有助于塑造角</span></span>
<span id="S6.T7.1.3.4.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.7.1" class="ltx_td ltx_align_left">色形象、增强剧情张力等效果。</span></span>
</span></span> <span id="S6.T7.1.3.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.4" class="ltx_tr">
<td id="S6.T7.1.4.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.4.1.1" class="ltx_p ltx_align_top">Math, Reasoning</p>
</td>
<td id="S6.T7.1.4.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.4.2.1" class="ltx_p ltx_align_top">一个直角三角形的两个直角边长度分别是6和8，斜边的长度是多少？</p>
</td>
<td id="S6.T7.1.4.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.4.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.4.3.1.1" class="ltx_text"></span><span id="S6.T7.1.4.3.1.2" class="ltx_text">
<span id="S6.T7.1.4.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.4.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.4.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.4.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 1/10]</span></span></span>
<span id="S6.T7.1.4.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.4.3.1.2.1.2.1" class="ltx_td ltx_align_left">这个直角三角形的斜边长为9。</span></span>
</span></span> <span id="S6.T7.1.4.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.4.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.4.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.4.4.1.1" class="ltx_text"></span><span id="S6.T7.1.4.4.1.2" class="ltx_text">
<span id="S6.T7.1.4.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.4.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.4.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.4.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.4.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.4.4.1.2.1.2.1" class="ltx_td ltx_align_left">这个直角三角形的斜边长为10。</span></span>
</span></span> <span id="S6.T7.1.4.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.5" class="ltx_tr">
<td id="S6.T7.1.5.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.5.1.1" class="ltx_p ltx_align_top">Translation</p>
</td>
<td id="S6.T7.1.5.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.5.2.1" class="ltx_p ltx_align_top">请将以下诗句翻译成英文：会当凌绝顶、一览众山小</p>
</td>
<td id="S6.T7.1.5.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.5.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.5.3.1.1" class="ltx_text"></span><span id="S6.T7.1.5.3.1.2" class="ltx_text">
<span id="S6.T7.1.5.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.5.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.5.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.5.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 6/10]</span></span></span>
<span id="S6.T7.1.5.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.5.3.1.2.1.2.1" class="ltx_td ltx_align_left">Become the highest peak,</span></span>
<span id="S6.T7.1.5.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.5.3.1.2.1.3.1" class="ltx_td ltx_align_left">towering above others</span></span>
</span></span> <span id="S6.T7.1.5.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.5.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.5.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.5.4.1.1" class="ltx_text"></span><span id="S6.T7.1.5.4.1.2" class="ltx_text">
<span id="S6.T7.1.5.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.5.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.5.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.5.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 8/10]</span></span></span>
<span id="S6.T7.1.5.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.5.4.1.2.1.2.1" class="ltx_td ltx_align_left">I stand on top of mountains, and see</span></span>
<span id="S6.T7.1.5.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.5.4.1.2.1.3.1" class="ltx_td ltx_align_left">all other small mountains below me.</span></span>
</span></span> <span id="S6.T7.1.5.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.6" class="ltx_tr">
<td id="S6.T7.1.6.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.6.1.1" class="ltx_p ltx_align_top">Literature</p>
</td>
<td id="S6.T7.1.6.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.6.2.1" class="ltx_p ltx_align_top">莎士比亚的四大悲剧包括什么？</p>
</td>
<td id="S6.T7.1.6.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.6.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.6.3.1.1" class="ltx_text"></span><span id="S6.T7.1.6.3.1.2" class="ltx_text">
<span id="S6.T7.1.6.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.6.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.6.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.6.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 3/10]</span></span></span>
<span id="S6.T7.1.6.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.6.3.1.2.1.2.1" class="ltx_td ltx_align_left">哈姆雷特、奥林匹角王、李尔福</span></span>
<span id="S6.T7.1.6.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.6.3.1.2.1.3.1" class="ltx_td ltx_align_left">和麦克白是莎士比亚的四大悲剧。</span></span>
</span></span> <span id="S6.T7.1.6.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.6.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.6.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.6.4.1.1" class="ltx_text"></span><span id="S6.T7.1.6.4.1.2" class="ltx_text">
<span id="S6.T7.1.6.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.6.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.6.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.6.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.2.1" class="ltx_td ltx_align_left">莎士比亚的四大悲剧是《哈姆雷</span></span>
<span id="S6.T7.1.6.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.3.1" class="ltx_td ltx_align_left">特》、《奥赛罗斯德》、《李尔</span></span>
<span id="S6.T7.1.6.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.4.1" class="ltx_td ltx_align_left">王》和《麦克白》。</span></span>
</span></span> <span id="S6.T7.1.6.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.7" class="ltx_tr">
<td id="S6.T7.1.7.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.7.1.1" class="ltx_p ltx_align_top">Generation (Letter)</p>
</td>
<td id="S6.T7.1.7.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.7.2.1" class="ltx_p ltx_align_top">请你帮我写一封感谢数学老师的信</p>
</td>
<td id="S6.T7.1.7.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.7.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.7.3.1.1" class="ltx_text"></span><span id="S6.T7.1.7.3.1.2" class="ltx_text">
<span id="S6.T7.1.7.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.7.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.7.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 8/10]</span></span></span>
<span id="S6.T7.1.7.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.2.1" class="ltx_td ltx_align_left">尊敬的老师，我想借此机会向您</span></span>
<span id="S6.T7.1.7.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.3.1" class="ltx_td ltx_align_left">表达我的感激之情。您的教学对</span></span>
<span id="S6.T7.1.7.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.4.1" class="ltx_td ltx_align_left">我非常有好处，让我在学习中更</span></span>
<span id="S6.T7.1.7.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.5.1" class="ltx_td ltx_align_left">加深入地理解了数学知识和技能</span></span>
<span id="S6.T7.1.7.3.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.6.1" class="ltx_td ltx_align_left">。我特别喜欢您用生动有趣的方</span></span>
<span id="S6.T7.1.7.3.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.7.1" class="ltx_td ltx_align_left">式来解释复杂的概念，这使我对</span></span>
<span id="S6.T7.1.7.3.1.2.1.8" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.8.1" class="ltx_td ltx_align_left">于这些内容的了解更深切理解</span></span>
<span id="S6.T7.1.7.3.1.2.1.9" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.9.1" class="ltx_td ltx_align_left">。感谢您为我在学术上取得进步</span></span>
<span id="S6.T7.1.7.3.1.2.1.10" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.10.1" class="ltx_td ltx_align_left">所做的一切努力！</span></span>
</span></span> <span id="S6.T7.1.7.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.7.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.7.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.7.4.1.1" class="ltx_text"></span><span id="S6.T7.1.7.4.1.2" class="ltx_text">
<span id="S6.T7.1.7.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.7.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.7.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 7/10]</span></span></span>
<span id="S6.T7.1.7.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.2.1" class="ltx_td ltx_align_left">尊敬的老师，我想借此机会表达我</span></span>
<span id="S6.T7.1.7.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.3.1" class="ltx_td ltx_align_left">对您的感激之情。您是我最喜欢和</span></span>
<span id="S6.T7.1.7.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.4.1" class="ltx_td ltx_align_left">信任的人之一，因为您让我对学习</span></span>
<span id="S6.T7.1.7.4.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.5.1" class="ltx_td ltx_align_left">新事物充满信心并激发了我求知欲</span></span>
<span id="S6.T7.1.7.4.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.6.1" class="ltx_td ltx_align_left">。我永远不会忘记您对我的影响以</span></span>
<span id="S6.T7.1.7.4.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.7.1" class="ltx_td ltx_align_left">及在课堂上所学到的知识。谢谢！</span></span>
</span></span> <span id="S6.T7.1.7.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.8" class="ltx_tr">
<td id="S6.T7.1.8.1" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.8.1.1" class="ltx_p ltx_align_top">Ethics</p>
</td>
<td id="S6.T7.1.8.2" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.8.2.1" class="ltx_p ltx_align_top">你能教教我怎么在考试中抄别人的卷子吗？</p>
</td>
<td id="S6.T7.1.8.3" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.8.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.8.3.1.1" class="ltx_text"></span><span id="S6.T7.1.8.3.1.2" class="ltx_text">
<span id="S6.T7.1.8.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.8.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.8.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 0/10]</span></span></span>
<span id="S6.T7.1.8.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.2.1" class="ltx_td ltx_align_left">是的，以下是一些方法可以帮助</span></span>
<span id="S6.T7.1.8.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.3.1" class="ltx_td ltx_align_left">你在考试中抄其他人书：1. 在考</span></span>
<span id="S6.T7.1.8.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.4.1" class="ltx_td ltx_align_left">前避免接触到别人所作笔记。这</span></span>
<span id="S6.T7.1.8.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.5.1" class="ltx_td ltx_align_left">可以通过提前准备、阅读和练习</span></span>
<span id="S6.T7.1.8.3.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.6.1" class="ltx_td ltx_align_left">来实现。2. 如果你必须使用他人</span></span>
<span id="S6.T7.1.8.3.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.7.1" class="ltx_td ltx_align_left">笔记作为参考材料或学习工具时</span></span>
<span id="S6.T7.1.8.3.1.2.1.8" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.8.1" class="ltx_td ltx_align_left">，请务必要确保你对该内容有足</span></span>
<span id="S6.T7.1.8.3.1.2.1.9" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.9.1" class="ltx_td ltx_align_left">够的理解并能够独立思考。3. 如</span></span>
<span id="S6.T7.1.8.3.1.2.1.10" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.10.1" class="ltx_td ltx_align_left">果需要抄袭他人笔记，请务必备</span></span>
<span id="S6.T7.1.8.3.1.2.1.11" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.11.1" class="ltx_td ltx_align_left">要仔细审阅他们的笔记以确定哪</span></span>
<span id="S6.T7.1.8.3.1.2.1.12" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.12.1" class="ltx_td ltx_align_left">些信息是你所需了解的内容。然</span></span>
<span id="S6.T7.1.8.3.1.2.1.13" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.13.1" class="ltx_td ltx_align_left">后将这些信息复制为你自己的新</span></span>
<span id="S6.T7.1.8.3.1.2.1.14" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.14.1" class="ltx_td ltx_align_left">纸上。4. 请务求在完成试卷后检</span></span>
<span id="S6.T7.1.8.3.1.2.1.15" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.15.1" class="ltx_td ltx_align_left">查你的答案是否与他人的答案相</span></span>
<span id="S6.T7.1.8.3.1.2.1.16" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.16.1" class="ltx_td ltx_align_left">符。如果你发现任何错误或不一</span></span>
<span id="S6.T7.1.8.3.1.2.1.17" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.17.1" class="ltx_td ltx_align_left">致之处，应立即进行纠正。</span></span>
</span></span> <span id="S6.T7.1.8.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.8.4" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.8.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.8.4.1.1" class="ltx_text"></span><span id="S6.T7.1.8.4.1.2" class="ltx_text">
<span id="S6.T7.1.8.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.8.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.8.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.8.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.8.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.8.4.1.2.1.2.1" class="ltx_td ltx_align_left">抱歉，我不能提供这种不道德</span></span>
<span id="S6.T7.1.8.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.8.4.1.2.1.3.1" class="ltx_td ltx_align_left">和非法的行为的指导。</span></span>
</span></span> <span id="S6.T7.1.8.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S7.p1.1">본 기술 보고서에서는 LLaMA 모형에 대한 중국인의 이해와 발전 역량을 제고하기 위한 접근 방법을 제시하였다. 원래의 LLaMA의 중국어 어휘의 한계를 인식하고 20K의 중국어 토큰을 추가로 통합하여 확장하여 중국어에 대한 인코딩 효율을 크게 높였다. 중국 LLaMA를 기반으로 명령 데이터로 감독 미세 조정을 사용하여 개선된 명령 후속 기능을 나타내는 중국 알파카 모델을 개발했다.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S7.p2.1">모델을 효과적으로 평가하기 위해 10개의 고유한 작업 유형에 걸쳐 160개의 샘플에 주석을 달았고 평가를 위해 GPT-4를 활용했다. 우리의 실험은 제안된 모델이 중국 이해 및 생성 작업에서 원래 LLaMA를 상당히 능가했으며 13B 버전은 7B 변형에 비해 일관되게 더 큰 개선을 달성했음을 보여주었다.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S7.p3.1">앞으로는 인간 피드백에서 강화 학습(RLHF) 또는 AI 지시 피드백에서 강화 학습(RLAIF)을 탐색하여 모델의 출력을 인간의 선호도와 더 정렬할 계획이다. 또한, GPTQ <cite class="ltx_cite ltx_citemacro_citep">(Frantar et al., <a class="ltx_ref" href="#bib.bib6" title="">2022</a>)</cite>와 같은 보다 진보되고 효과적인 양자화 방법을 채택하고자 한다. 또한 대규모 언어 모델의 보다 효율적이고 효과적인 사전 훈련 및 미세 조정을 위해 LoRA에 대한 대체 방법을 조사하여 궁극적으로 중국 NLP 커뮤니티 내에서 다양한 작업에 걸쳐 성능과 적용 가능성을 향상시키는 것을 목표로 한다.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="Sx1.p1.1">이 프로젝트는 LLaMA 및 알파카 모델의 중국 이해 및 생성 능력을 성공적으로 향상시켰지만 몇 가지 한계를 인정해야 한다.</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="Sx1.I1.i1.p1.1">유해하고 예측할 수 없는 내용: 우리의 결과는 13B 버전이 7B 버전보다 비윤리적 쿼리를 거부하는 더 나은 능력을 가지고 있음을 보여준다. 그러나, 이러한 모델들은 여전히 인간의 선호도 및 가치와 해롭거나 잘못 정렬된 콘텐츠를 생성할 수 있다. 이 문제는 학습 데이터에 존재하는 편향이나 특정 컨텍스트에서 적절한 출력을 식별할 수 없는 모델에서 발생할 수 있다.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="Sx1.I1.i2.p1.1">불충분한 트레이닝: 컴퓨팅 파워 및 데이터 가용성의 제약들로 인해, 모델들의 트레이닝은 최적의 성능을 위해 충분하지 않을 수 있다. 그 결과 모델들에 대한 중국어 이해 능력에서 여전히 개선의 여지가 있다.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="Sx1.I1.i3.p1.1">강인성 부족: 모델은 일부 상황에서 취성을 나타내어 적대적 입력 또는 희귀 언어 현상에 직면할 때 일관되지 않거나 무의미한 출력을 생성할 수 있다.</p>
</div>
</li>
<li id="Sx1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="Sx1.I1.i4.p1.1">확장성과 효율성: LoRA와 4비트 양자화를 적용하여 모델을 더 넓은 커뮤니티에 더 쉽게 접근할 수 있도록 했지만, 원래 LLaMA와 결합할 때 모델의 큰 크기와 복잡성은 특히 제한된 계산 자원을 가진 사용자의 배치에 어려움을 초래할 수 있다. 이 문제는 다양한 응용 프로그램에서 모델의 접근성과 광범위한 채택을 방해할 수 있다.</p>
</div>
</li>
</ul>
</div>
<div id="Sx1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="Sx1.p2.1">향후 작업은 이러한 한계를 해결하여 모델의 기능을 더욱 향상시켜 중국 NLP 커뮤니티의 광범위한 응용 프로그램에 더 강력하고 접근 가능하며 효과적이어야 한다.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="Sx2.p1.1">원안은 문법 수정 및 명확성 개선을 위해 OpenAI GPT-4에 의해 연마되었다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu.

</span>
<span class="ltx_bibblock">Revisiting pre-trained models for Chinese natural language
processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: Findings</em>, pp.&nbsp; 657–668, Online, November
2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/2020.findings-emnlp.58" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/2020.findings-emnlp.58</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang.

</span>
<span class="ltx_bibblock">Pre-training with whole word masking for chinese bert.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</em>, 29:3504–3514, 2021.

</span>
<span class="ltx_bibblock">doi: <a href="10.1109/TASLP.2021.3124365" title="" class="ltx_ref ltx_Url">10.1109/TASLP.2021.3124365</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu.

</span>
<span class="ltx_bibblock">Lert: A linguistically-motivated pre-trained language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05344</em>, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Llm. int8 (): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.07339</em>, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pp.&nbsp; 4171–4186,
Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/N19-1423" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/N19-1423</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frantar et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">GPTQ: Accurate post-training compression for generative pretrained
transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.17323</em>, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerganov (2023)</span>
<span class="ltx_bibblock">
Georgi Gerganov.

</span>
<span class="ltx_bibblock">llama.cpp.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/ggerganov/llama.cpp" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ggerganov/llama.cpp</a>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Edward&nbsp;J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
Li, Shean Wang, Lu&nbsp;Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">LoRA: Low-Rank Adaptation of Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2106.09685, June 2021.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2106.09685" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2106.09685</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo &amp; Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">SentencePiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pp.&nbsp; 66–71, Brussels,
Belgium, November 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a href="10.18653/v1/D18-2012" title="" class="ltx_ref ltx_Url">10.18653/v1/D18-2012</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/D18-2012" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/D18-2012</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing chatgpt.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2303.08774, March 2023.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2303.08774" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2303.08774</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright,
Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan
Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human
feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2203.02155, March 2022.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2203.02155" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2203.02155</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer (2020)</span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">Glu variants improve transformer, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tatsu-lab/stanford_alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman
Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand
Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Ł&nbsp;ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;Von Luxburg, S.&nbsp;Bengio, H.&nbsp;Wallach, R.&nbsp;Fergus,
S.&nbsp;Vishwanathan, and R.&nbsp;Garnett (eds.), <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, volume&nbsp;30. Curran Associates, Inc., 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah&nbsp;A.
Smith, Daniel Khashabi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Self-Instruct: Aligning Language Model with Self Generated
Instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2212.10560, December 2022.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2212.10560" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2212.10560</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu (2019)</span>
<span class="ltx_bibblock">
Bright Xu.

</span>
<span class="ltx_bibblock">Nlp chinese corpus: Large scale chinese corpus for nlp, September
2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.3402023" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.3402023</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Ziqing Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, and
Zhigang Chen.

</span>
<span class="ltx_bibblock">CINO: A Chinese minority pre-trained language model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International Conference on
Computational Linguistics</em>, pp.&nbsp; 3937–3949, Gyeongju, Republic of Korea,
October 2022. International Committee on Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.coling-1.346" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.coling-1.346</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Zhewei Yao, Reza Yazdani&nbsp;Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and
Yuxiong He.

</span>
<span class="ltx_bibblock">Zeroquant: Efficient and affordable post-training quantization for
large-scale transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:27168–27183, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Sennrich (2019)</span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root Mean Square Layer Normalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32</em>,
Vancouver, Canada, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/references/pdf?id=S1qBAf6rr" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/references/pdf?id=S1qBAf6rr</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2304.08176" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2304.08177" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2304.08177">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.08177" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2304.08178" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri May  5 12:36:08 2023 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.08177] Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</title><meta property="og:description" content="Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI).
However, the expensive training and…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.08177">

<!--Generated on Fri May  5 12:36:08 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiming Cui 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">ymcui@ieee.org</span> &amp;Ziqing Yang<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">ziqingyang@gmail.com</span> &amp;Xin Yao 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">yaoxin94@foxmail.com</span>
</span><span class="ltx_author_notes">Equal contributions.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI).
However, the expensive training and deployment of LLMs present challenges to transparent and open academic research.
To address these issues, this project open-sources the Chinese LLaMA and Alpaca large models, emphasizing instruction fine-tuning. We expand the original LLaMA’s Chinese vocabulary by adding 20K Chinese tokens, increasing encoding efficiency and enhancing basic semantic understanding.
By incorporating secondary pre-training using Chinese data and fine-tuning with Chinese instruction data, we substantially improve the models’ comprehension and execution of instructions.
Our pilot study serves as a foundation for researchers adapting LLaMA and Alpaca models to other languages.
Resources are made publicly available through GitHub, fostering open research in the Chinese NLP community and beyond.
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>GitHub repository: <a target="_blank" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ymcui/Chinese-LLaMA-Alpaca</a></span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">The field of natural language processing (NLP) has undergone a transformative paradigm shift with the advent of Large Language Models (LLMs).
These models, characterized by their vast size and extensive training data, have demonstrated remarkable capabilities in understanding and generating human-like text.
Unlike pre-trained language models for text understanding, such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et&nbsp;al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>, the GPT series <cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> focuses on text generation abilities, making them a more suitable testbed for creativity than their counterparts.
As the latest LLMs in the GPT family, ChatGPT and GPT-4 have attracted significant attention and emerged as leading examples in this rapidly evolving domain.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>, built on the GPT-3.5 <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> architecture, is an advanced conversational AI model that can engage in context-aware, human-like interactions.
Its success has paved the way for the development of GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>, a more sophisticated LLM, which has demonstrated even greater potential in natural language understanding, generation, and various NLP tasks.
Both models have opened up new avenues of research and applications, fueling interest in exploring the capabilities of Artificial General Intelligence (AGI). These LLMs have not only shown impressive performance in multiple benchmarks but have also exhibited a capacity for few-shot learning and adapting to novel tasks.
As a result, they have significantly contributed to the expansion of NLP research, inspiring researchers and industry professionals alike to explore and leverage their potential in a wide range of applications, from sentiment analysis and machine translation to question-answering systems and beyond.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Despite the remarkable advancements brought about by LLMs, these models come with certain limitations that hinder transparent and open research.
One of the most notable concerns is their proprietary nature, which restricts access to the models and hampers the ability of the broader research community to build upon their successes.
Additionally, the immense computational resources required for training and deploying these models pose a challenge for researchers with limited resources, further exacerbating the accessibility problem.
</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">In response to these limitations, the NLP research community has turned to open-source alternatives to foster greater transparency and collaboration.
Two such examples are LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> and Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et&nbsp;al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>, where the Alpaca model is further finetuned on LLaMA with instruction data.
These open-source LLMs have been designed to facilitate academic research and accelerate progress in the field of NLP.
By open-sourcing these models, the NLP community aims to create an environment that encourages further advancements in model development, fine-tuning, and evaluation, ultimately leading to more robust and capable LLMs that can be utilized in a wide range of applications.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">Although LLaMA and Alpaca have made significant strides in the world of NLP, they possess inherent limitations when it comes to natively supporting Chinese language tasks.
The original models contain only a few hundred Chinese tokens in their vocabularies, significantly hampering their efficiency in encoding and decoding Chinese text<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We will discuss this issue in Section 2.</span></span></span>.
Drawing from our previous work on Chinese BERT series <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite> and Chinese minority-oriented multilingual pre-trained models <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>, in this technical report, we propose Chinese LLaMA and Alpaca with enhanced abilities in Chinese understanding and generation.
We extend the original LLaMA’s vocabulary with an additional 20K Chinese tokens, substantially improving its ability to process and generate Chinese text. To ensure efficient training and deployment of the Chinese LLaMA and Alpaca models, we adopt the Low-Rank Adaptation (LoRA) approach <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite>, which allows us to train and fine-tune the models without incurring excessive computational costs.
Our pilot study in enhancing the Chinese understanding and generation capabilities of LLaMA and Alpaca models can serve as a foundation for researchers seeking to adapt these models to other languages.
By demonstrating the feasibility and effectiveness of our approach, we provide insights and methodologies that can be applied to extend the vocabularies and improve the performance of LLaMA and Alpaca models in different languages.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">In summary, the contributions of this technical report are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We enhance the Chinese encoding and decoding efficiency and improve LLaMA’s Chinese understanding ability by extending the original LLaMA’s vocabulary with an additional 20,000 Chinese tokens.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We adopt the Low-Rank Adaptation (LoRA) approach for the efficient training and deployment of the Chinese LLaMA and Alpaca models, enabling researchers to work with these models without incurring excessive computational costs.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We evaluate the performance of the Chinese Alpaca 7B and 13B models on a variety of natural language understanding (NLU) and natural language generation (NLG) tasks, demonstrating significant improvements over the original LLaMA counterparts in the context of Chinese language tasks.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i4.p1.1" class="ltx_p">We make the resources and findings of our study publicly available, fostering further research and collaboration within the NLP community and encouraging the adaptation of LLaMA and Alpaca models to other languages.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Chinese LLaMA</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> is a decoder-only, foundational large language model based on the transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite>. Similar to other transformer-based LLMs, LLaMA comprises an embedding layer, multiple transformer blocks, and an LM head layer. It also incorporates various improvements, such as Pre-normalization <cite class="ltx_cite ltx_citemacro_citep">(Zhang &amp; Sennrich, <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>, SwiGLU activation <cite class="ltx_cite ltx_citemacro_citep">(Shazeer, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>, and Rotary Embeddings <cite class="ltx_cite ltx_citemacro_citep">(Su et&nbsp;al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>. The total number of parameters in LLaMA ranges from 7B to 65B. Experiments demonstrate that LLaMA achieves competitive performance compared to other LLMs, like GPT-3, while maintaining a smaller model size.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">LLaMA has been pre-trained on 1T to 1.4T tokens from publicly available corpora, with the majority of the data in English and only a small fraction in other languages using Latin or Cyrillic scripts.
As a result, LLaMA’s ability to understand and generate Chinese is limited. To address this, we propose pre-training the LLaMA model on Chinese corpora to enhance its fundamental Chinese understanding and generation capabilities.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p">Directly pre-training LLaMA on Chinese corpora faces several challenges. Firstly, there are less than one thousand Chinese characters in the original LLaMA tokenizer vocabulary. Although the LLaMA tokenizer supports all Chinese characters by falling back to bytes, this fallback strategy significantly increases sequence length and slows down the processing efficiency on Chinese texts. Moreover, byte tokens are not exclusively designed for representing Chinese characters, as they are also used to represent other UTF-8 tokens, making it difficult for byte tokens to learn the semantic meaning of Chinese characters.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p">To address these issues, we propose to extend the LLaMA tokenizer with additional Chinese tokens and adapt the model for the new tokenizer <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">To enhance the tokenizer’s support for Chinese text, we first train a Chinese tokenizer with SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo &amp; Richardson, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> on the Chinese corpus, using a vocabulary size of 20,000. We then merge the Chinese tokenizer into the original LLaMA tokenizer by combining their vocabularies. Ultimately, we obtain a merged tokenizer, which we call the Chinese LLaMA tokenizer, with a vocabulary size of 49,953.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i2.p1.4" class="ltx_p">To adapt the model for the Chinese LLaMA tokenizer, we resize the word embeddings and language model head from shape <math id="S2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="V\times H" display="inline"><semantics id="S2.I1.i2.p1.1.m1.1a"><mrow id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">V</mi><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i2.p1.1.m1.1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.1.cmml">×</mo><mi id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><times id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1"></times><ci id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">𝑉</ci><ci id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">V\times H</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.1.m1.1d">italic_V × italic_H</annotation></semantics></math> to <math id="S2.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="V^{\prime}\times H" display="inline"><semantics id="S2.I1.i2.p1.2.m2.1a"><mrow id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml"><msup id="S2.I1.i2.p1.2.m2.1.1.2" xref="S2.I1.i2.p1.2.m2.1.1.2.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.2.2" xref="S2.I1.i2.p1.2.m2.1.1.2.2.cmml">V</mi><mo id="S2.I1.i2.p1.2.m2.1.1.2.3" xref="S2.I1.i2.p1.2.m2.1.1.2.3.cmml">′</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i2.p1.2.m2.1.1.1" xref="S2.I1.i2.p1.2.m2.1.1.1.cmml">×</mo><mi id="S2.I1.i2.p1.2.m2.1.1.3" xref="S2.I1.i2.p1.2.m2.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><apply id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1"><times id="S2.I1.i2.p1.2.m2.1.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.1"></times><apply id="S2.I1.i2.p1.2.m2.1.1.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.I1.i2.p1.2.m2.1.1.2.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2">superscript</csymbol><ci id="S2.I1.i2.p1.2.m2.1.1.2.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.2">𝑉</ci><ci id="S2.I1.i2.p1.2.m2.1.1.2.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.3">′</ci></apply><ci id="S2.I1.i2.p1.2.m2.1.1.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">V^{\prime}\times H</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.2.m2.1d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT × italic_H</annotation></semantics></math>, where <math id="S2.I1.i2.p1.3.m3.2" class="ltx_Math" alttext="V=32,000" display="inline"><semantics id="S2.I1.i2.p1.3.m3.2a"><mrow id="S2.I1.i2.p1.3.m3.2.3" xref="S2.I1.i2.p1.3.m3.2.3.cmml"><mi id="S2.I1.i2.p1.3.m3.2.3.2" xref="S2.I1.i2.p1.3.m3.2.3.2.cmml">V</mi><mo id="S2.I1.i2.p1.3.m3.2.3.1" xref="S2.I1.i2.p1.3.m3.2.3.1.cmml">=</mo><mrow id="S2.I1.i2.p1.3.m3.2.3.3.2" xref="S2.I1.i2.p1.3.m3.2.3.3.1.cmml"><mn id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml">32</mn><mo id="S2.I1.i2.p1.3.m3.2.3.3.2.1" xref="S2.I1.i2.p1.3.m3.2.3.3.1.cmml">,</mo><mn id="S2.I1.i2.p1.3.m3.2.2" xref="S2.I1.i2.p1.3.m3.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.2b"><apply id="S2.I1.i2.p1.3.m3.2.3.cmml" xref="S2.I1.i2.p1.3.m3.2.3"><eq id="S2.I1.i2.p1.3.m3.2.3.1.cmml" xref="S2.I1.i2.p1.3.m3.2.3.1"></eq><ci id="S2.I1.i2.p1.3.m3.2.3.2.cmml" xref="S2.I1.i2.p1.3.m3.2.3.2">𝑉</ci><list id="S2.I1.i2.p1.3.m3.2.3.3.1.cmml" xref="S2.I1.i2.p1.3.m3.2.3.3.2"><cn type="integer" id="S2.I1.i2.p1.3.m3.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1">32</cn><cn type="integer" id="S2.I1.i2.p1.3.m3.2.2.cmml" xref="S2.I1.i2.p1.3.m3.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.2c">V=32,000</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.3.m3.2d">italic_V = 32 , 000</annotation></semantics></math> represents the original vocabulary size, and <math id="S2.I1.i2.p1.4.m4.2" class="ltx_Math" alttext="V^{\prime}=49,953" display="inline"><semantics id="S2.I1.i2.p1.4.m4.2a"><mrow id="S2.I1.i2.p1.4.m4.2.3" xref="S2.I1.i2.p1.4.m4.2.3.cmml"><msup id="S2.I1.i2.p1.4.m4.2.3.2" xref="S2.I1.i2.p1.4.m4.2.3.2.cmml"><mi id="S2.I1.i2.p1.4.m4.2.3.2.2" xref="S2.I1.i2.p1.4.m4.2.3.2.2.cmml">V</mi><mo id="S2.I1.i2.p1.4.m4.2.3.2.3" xref="S2.I1.i2.p1.4.m4.2.3.2.3.cmml">′</mo></msup><mo id="S2.I1.i2.p1.4.m4.2.3.1" xref="S2.I1.i2.p1.4.m4.2.3.1.cmml">=</mo><mrow id="S2.I1.i2.p1.4.m4.2.3.3.2" xref="S2.I1.i2.p1.4.m4.2.3.3.1.cmml"><mn id="S2.I1.i2.p1.4.m4.1.1" xref="S2.I1.i2.p1.4.m4.1.1.cmml">49</mn><mo id="S2.I1.i2.p1.4.m4.2.3.3.2.1" xref="S2.I1.i2.p1.4.m4.2.3.3.1.cmml">,</mo><mn id="S2.I1.i2.p1.4.m4.2.2" xref="S2.I1.i2.p1.4.m4.2.2.cmml">953</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.4.m4.2b"><apply id="S2.I1.i2.p1.4.m4.2.3.cmml" xref="S2.I1.i2.p1.4.m4.2.3"><eq id="S2.I1.i2.p1.4.m4.2.3.1.cmml" xref="S2.I1.i2.p1.4.m4.2.3.1"></eq><apply id="S2.I1.i2.p1.4.m4.2.3.2.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2"><csymbol cd="ambiguous" id="S2.I1.i2.p1.4.m4.2.3.2.1.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2">superscript</csymbol><ci id="S2.I1.i2.p1.4.m4.2.3.2.2.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2.2">𝑉</ci><ci id="S2.I1.i2.p1.4.m4.2.3.2.3.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2.3">′</ci></apply><list id="S2.I1.i2.p1.4.m4.2.3.3.1.cmml" xref="S2.I1.i2.p1.4.m4.2.3.3.2"><cn type="integer" id="S2.I1.i2.p1.4.m4.1.1.cmml" xref="S2.I1.i2.p1.4.m4.1.1">49</cn><cn type="integer" id="S2.I1.i2.p1.4.m4.2.2.cmml" xref="S2.I1.i2.p1.4.m4.2.2">953</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.4.m4.2c">V^{\prime}=49,953</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.4.m4.2d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = 49 , 953</annotation></semantics></math> is the vocabulary size of the Chinese LLaMA tokenizer. The new rows are appended to the end of the original embedding matrices, ensuring that the embeddings of the tokens in the original vocabulary remain unaffected.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p">Our preliminary experiments show that the number of tokens generated by the Chinese-LLaMA tokenizer is roughly half of those generated by the original LLaMA tokenizer. Table <a href="#S2.T1" title="Table 1 ‣ 2 Chinese LLaMA ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example comparison between the original LLaMA tokenizer and our Chinese LLaMA tokenizer. As we can see, using the Chinese LLaMA tokenizer significantly reduces the encoding length compared to the original. Given a fixed context length, the model can accommodate about twice as much information, and the generation speed is two times faster compared to the original LLaMA tokenizer. This demonstrates the effectiveness of our proposed approach in enhancing the Chinese understanding and generation capabilities of the LLaMA model.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p">After completing the aforementioned adaptation steps, we pre-train the Chinese-LLaMA model using the Chinese-LLaMA tokenizer on the standard Casual Language Modeling (CLM) task. Given an input token sequence <math id="S2.p6.1.m1.4" class="ltx_Math" alttext="\bm{x}=(x_{0},x_{1},x_{2},\ldots)" display="inline"><semantics id="S2.p6.1.m1.4a"><mrow id="S2.p6.1.m1.4.4" xref="S2.p6.1.m1.4.4.cmml"><mi id="S2.p6.1.m1.4.4.5" xref="S2.p6.1.m1.4.4.5.cmml">𝒙</mi><mo id="S2.p6.1.m1.4.4.4" xref="S2.p6.1.m1.4.4.4.cmml">=</mo><mrow id="S2.p6.1.m1.4.4.3.3" xref="S2.p6.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S2.p6.1.m1.4.4.3.3.4" xref="S2.p6.1.m1.4.4.3.4.cmml">(</mo><msub id="S2.p6.1.m1.2.2.1.1.1" xref="S2.p6.1.m1.2.2.1.1.1.cmml"><mi id="S2.p6.1.m1.2.2.1.1.1.2" xref="S2.p6.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S2.p6.1.m1.2.2.1.1.1.3" xref="S2.p6.1.m1.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S2.p6.1.m1.4.4.3.3.5" xref="S2.p6.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.p6.1.m1.3.3.2.2.2" xref="S2.p6.1.m1.3.3.2.2.2.cmml"><mi id="S2.p6.1.m1.3.3.2.2.2.2" xref="S2.p6.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S2.p6.1.m1.3.3.2.2.2.3" xref="S2.p6.1.m1.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S2.p6.1.m1.4.4.3.3.6" xref="S2.p6.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.p6.1.m1.4.4.3.3.3" xref="S2.p6.1.m1.4.4.3.3.3.cmml"><mi id="S2.p6.1.m1.4.4.3.3.3.2" xref="S2.p6.1.m1.4.4.3.3.3.2.cmml">x</mi><mn id="S2.p6.1.m1.4.4.3.3.3.3" xref="S2.p6.1.m1.4.4.3.3.3.3.cmml">2</mn></msub><mo id="S2.p6.1.m1.4.4.3.3.7" xref="S2.p6.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.p6.1.m1.1.1" xref="S2.p6.1.m1.1.1.cmml">…</mi><mo stretchy="false" id="S2.p6.1.m1.4.4.3.3.8" xref="S2.p6.1.m1.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.4b"><apply id="S2.p6.1.m1.4.4.cmml" xref="S2.p6.1.m1.4.4"><eq id="S2.p6.1.m1.4.4.4.cmml" xref="S2.p6.1.m1.4.4.4"></eq><ci id="S2.p6.1.m1.4.4.5.cmml" xref="S2.p6.1.m1.4.4.5">𝒙</ci><vector id="S2.p6.1.m1.4.4.3.4.cmml" xref="S2.p6.1.m1.4.4.3.3"><apply id="S2.p6.1.m1.2.2.1.1.1.cmml" xref="S2.p6.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p6.1.m1.2.2.1.1.1.1.cmml" xref="S2.p6.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.p6.1.m1.2.2.1.1.1.2.cmml" xref="S2.p6.1.m1.2.2.1.1.1.2">𝑥</ci><cn type="integer" id="S2.p6.1.m1.2.2.1.1.1.3.cmml" xref="S2.p6.1.m1.2.2.1.1.1.3">0</cn></apply><apply id="S2.p6.1.m1.3.3.2.2.2.cmml" xref="S2.p6.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p6.1.m1.3.3.2.2.2.1.cmml" xref="S2.p6.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.p6.1.m1.3.3.2.2.2.2.cmml" xref="S2.p6.1.m1.3.3.2.2.2.2">𝑥</ci><cn type="integer" id="S2.p6.1.m1.3.3.2.2.2.3.cmml" xref="S2.p6.1.m1.3.3.2.2.2.3">1</cn></apply><apply id="S2.p6.1.m1.4.4.3.3.3.cmml" xref="S2.p6.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.p6.1.m1.4.4.3.3.3.1.cmml" xref="S2.p6.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.p6.1.m1.4.4.3.3.3.2.cmml" xref="S2.p6.1.m1.4.4.3.3.3.2">𝑥</ci><cn type="integer" id="S2.p6.1.m1.4.4.3.3.3.3.cmml" xref="S2.p6.1.m1.4.4.3.3.3.3">2</cn></apply><ci id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1">…</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.4c">\bm{x}=(x_{0},x_{1},x_{2},\ldots)</annotation><annotation encoding="application/x-llamapun" id="S2.p6.1.m1.4d">bold_italic_x = ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … )</annotation></semantics></math>, the model is trained to predict the next token in an autoregressive manner. The objective is to minimize the following negative log likelihood:</p>
</div>
<div id="S2.p7" class="ltx_para ltx_noindent">
<table id="Sx2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\textrm{CLM}}=-\sum_{i}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)" display="inline"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><msub id="S2.E1.m1.3.3.3" xref="S2.E1.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.3.3.3.2" xref="S2.E1.m1.3.3.3.2.cmml">ℒ</mi><mtext id="S2.E1.m1.3.3.3.3" xref="S2.E1.m1.3.3.3.3a.cmml">CLM</mtext></msub><mo id="S2.E1.m1.3.3.2" xref="S2.E1.m1.3.3.2.cmml">=</mo><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.cmml"><mo id="S2.E1.m1.3.3.1a" xref="S2.E1.m1.3.3.1.cmml">−</mo><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><mstyle displaystyle="true" id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.2.cmml"><munder id="S2.E1.m1.3.3.1.1.2a" xref="S2.E1.m1.3.3.1.1.2.cmml"><mo movablelimits="false" id="S2.E1.m1.3.3.1.1.2.2" xref="S2.E1.m1.3.3.1.1.2.2.cmml">∑</mo><mi id="S2.E1.m1.3.3.1.1.2.3" xref="S2.E1.m1.3.3.1.1.2.3.cmml">i</mi></munder></mstyle><mrow id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3.1" xref="S2.E1.m1.3.3.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E1.m1.3.3.1.1.1.3a" xref="S2.E1.m1.3.3.1.1.1.3.cmml">⁡</mo><mi id="S2.E1.m1.3.3.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.3.2.cmml">p</mi></mrow><mo id="S2.E1.m1.3.3.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.2.cmml" lspace="0px" rspace="0px"></mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.5" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.5.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.2.cmml">x</mi><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.5.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S2.E1.m1.3.3.1.1.1.1.1.1.4" xref="S2.E1.m1.3.3.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.4" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.5" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">…</mi><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.6" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml">−</mo><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.7" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">;</mo><mi mathvariant="normal" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">Θ</mi></mrow></mrow><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3"><eq id="S2.E1.m1.3.3.2.cmml" xref="S2.E1.m1.3.3.2"></eq><apply id="S2.E1.m1.3.3.3.cmml" xref="S2.E1.m1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3">subscript</csymbol><ci id="S2.E1.m1.3.3.3.2.cmml" xref="S2.E1.m1.3.3.3.2">ℒ</ci><ci id="S2.E1.m1.3.3.3.3a.cmml" xref="S2.E1.m1.3.3.3.3"><mtext mathsize="70%" id="S2.E1.m1.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3">CLM</mtext></ci></apply><apply id="S2.E1.m1.3.3.1.cmml" xref="S2.E1.m1.3.3.1"><minus id="S2.E1.m1.3.3.1.2.cmml" xref="S2.E1.m1.3.3.1"></minus><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1"><apply id="S2.E1.m1.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.2">subscript</csymbol><sum id="S2.E1.m1.3.3.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2"></sum><ci id="S2.E1.m1.3.3.1.1.2.3.cmml" xref="S2.E1.m1.3.3.1.1.2.3">𝑖</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.2"></times><apply id="S2.E1.m1.3.3.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3"><log id="S2.E1.m1.3.3.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.3.1"></log><ci id="S2.E1.m1.3.3.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2">𝑝</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.4">conditional</csymbol><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.5.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.5.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.5.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.2">𝑥</ci><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.5.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.3">𝑖</ci></apply><list id="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3"><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2">𝑥</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">…</ci><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2">𝑥</ci><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3"><minus id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2">𝑖</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">Θ</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\displaystyle\mathcal{L}_{\textrm{CLM}}=-\sum_{i}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">caligraphic_L start_POSTSUBSCRIPT CLM end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; roman_Θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p8" class="ltx_para ltx_noindent">
<p id="S2.p8.3" class="ltx_p">Here, <math id="S2.p8.1.m1.1" class="ltx_Math" alttext="\Theta" display="inline"><semantics id="S2.p8.1.m1.1a"><mi mathvariant="normal" id="S2.p8.1.m1.1.1" xref="S2.p8.1.m1.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S2.p8.1.m1.1b"><ci id="S2.p8.1.m1.1.1.cmml" xref="S2.p8.1.m1.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.1.m1.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S2.p8.1.m1.1d">roman_Θ</annotation></semantics></math> represents the model parameters, <math id="S2.p8.2.m2.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S2.p8.2.m2.1a"><msub id="S2.p8.2.m2.1.1" xref="S2.p8.2.m2.1.1.cmml"><mi id="S2.p8.2.m2.1.1.2" xref="S2.p8.2.m2.1.1.2.cmml">x</mi><mi id="S2.p8.2.m2.1.1.3" xref="S2.p8.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p8.2.m2.1b"><apply id="S2.p8.2.m2.1.1.cmml" xref="S2.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p8.2.m2.1.1.1.cmml" xref="S2.p8.2.m2.1.1">subscript</csymbol><ci id="S2.p8.2.m2.1.1.2.cmml" xref="S2.p8.2.m2.1.1.2">𝑥</ci><ci id="S2.p8.2.m2.1.1.3.cmml" xref="S2.p8.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.2.m2.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.p8.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the token to be predicted, and <math id="S2.p8.3.m3.4" class="ltx_Math" alttext="x_{0},x_{1},\ldots,x_{i-1}" display="inline"><semantics id="S2.p8.3.m3.4a"><mrow id="S2.p8.3.m3.4.4.3" xref="S2.p8.3.m3.4.4.4.cmml"><msub id="S2.p8.3.m3.2.2.1.1" xref="S2.p8.3.m3.2.2.1.1.cmml"><mi id="S2.p8.3.m3.2.2.1.1.2" xref="S2.p8.3.m3.2.2.1.1.2.cmml">x</mi><mn id="S2.p8.3.m3.2.2.1.1.3" xref="S2.p8.3.m3.2.2.1.1.3.cmml">0</mn></msub><mo id="S2.p8.3.m3.4.4.3.4" xref="S2.p8.3.m3.4.4.4.cmml">,</mo><msub id="S2.p8.3.m3.3.3.2.2" xref="S2.p8.3.m3.3.3.2.2.cmml"><mi id="S2.p8.3.m3.3.3.2.2.2" xref="S2.p8.3.m3.3.3.2.2.2.cmml">x</mi><mn id="S2.p8.3.m3.3.3.2.2.3" xref="S2.p8.3.m3.3.3.2.2.3.cmml">1</mn></msub><mo id="S2.p8.3.m3.4.4.3.5" xref="S2.p8.3.m3.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S2.p8.3.m3.1.1" xref="S2.p8.3.m3.1.1.cmml">…</mi><mo id="S2.p8.3.m3.4.4.3.6" xref="S2.p8.3.m3.4.4.4.cmml">,</mo><msub id="S2.p8.3.m3.4.4.3.3" xref="S2.p8.3.m3.4.4.3.3.cmml"><mi id="S2.p8.3.m3.4.4.3.3.2" xref="S2.p8.3.m3.4.4.3.3.2.cmml">x</mi><mrow id="S2.p8.3.m3.4.4.3.3.3" xref="S2.p8.3.m3.4.4.3.3.3.cmml"><mi id="S2.p8.3.m3.4.4.3.3.3.2" xref="S2.p8.3.m3.4.4.3.3.3.2.cmml">i</mi><mo id="S2.p8.3.m3.4.4.3.3.3.1" xref="S2.p8.3.m3.4.4.3.3.3.1.cmml">−</mo><mn id="S2.p8.3.m3.4.4.3.3.3.3" xref="S2.p8.3.m3.4.4.3.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p8.3.m3.4b"><list id="S2.p8.3.m3.4.4.4.cmml" xref="S2.p8.3.m3.4.4.3"><apply id="S2.p8.3.m3.2.2.1.1.cmml" xref="S2.p8.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S2.p8.3.m3.2.2.1.1.1.cmml" xref="S2.p8.3.m3.2.2.1.1">subscript</csymbol><ci id="S2.p8.3.m3.2.2.1.1.2.cmml" xref="S2.p8.3.m3.2.2.1.1.2">𝑥</ci><cn type="integer" id="S2.p8.3.m3.2.2.1.1.3.cmml" xref="S2.p8.3.m3.2.2.1.1.3">0</cn></apply><apply id="S2.p8.3.m3.3.3.2.2.cmml" xref="S2.p8.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S2.p8.3.m3.3.3.2.2.1.cmml" xref="S2.p8.3.m3.3.3.2.2">subscript</csymbol><ci id="S2.p8.3.m3.3.3.2.2.2.cmml" xref="S2.p8.3.m3.3.3.2.2.2">𝑥</ci><cn type="integer" id="S2.p8.3.m3.3.3.2.2.3.cmml" xref="S2.p8.3.m3.3.3.2.2.3">1</cn></apply><ci id="S2.p8.3.m3.1.1.cmml" xref="S2.p8.3.m3.1.1">…</ci><apply id="S2.p8.3.m3.4.4.3.3.cmml" xref="S2.p8.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S2.p8.3.m3.4.4.3.3.1.cmml" xref="S2.p8.3.m3.4.4.3.3">subscript</csymbol><ci id="S2.p8.3.m3.4.4.3.3.2.cmml" xref="S2.p8.3.m3.4.4.3.3.2">𝑥</ci><apply id="S2.p8.3.m3.4.4.3.3.3.cmml" xref="S2.p8.3.m3.4.4.3.3.3"><minus id="S2.p8.3.m3.4.4.3.3.3.1.cmml" xref="S2.p8.3.m3.4.4.3.3.3.1"></minus><ci id="S2.p8.3.m3.4.4.3.3.3.2.cmml" xref="S2.p8.3.m3.4.4.3.3.3.2">𝑖</ci><cn type="integer" id="S2.p8.3.m3.4.4.3.3.3.3.cmml" xref="S2.p8.3.m3.4.4.3.3.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.3.m3.4c">x_{0},x_{1},\ldots,x_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S2.p8.3.m3.4d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> constitute the context.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span> Tokenizer comparisons between original LLaMA and Chinese LLaMA.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.2.1" class="ltx_text ltx_font_bold">Length</span></td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S2.T1.1.1.3.1" class="ltx_text ltx_font_bold">Content</span></td>
</tr>
<tr id="S2.T1.1.2" class="ltx_tr">
<td id="S2.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.1.2.1.1" class="ltx_text ltx_font_bold">Original Sentence</span></td>
<td id="S2.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">28</td>
<td id="S2.T1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">人工智能是计算机科学、心理学、哲学等学科融合的交叉学科。</td>
</tr>
<tr id="S2.T1.1.3" class="ltx_tr">
<td id="S2.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.1.3.1.1" class="ltx_text ltx_font_bold">Original Tokenizer</span></td>
<td id="S2.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.3.2.1" class="ltx_text">35</span></td>
<td id="S2.T1.1.3.3" class="ltx_td ltx_align_left ltx_border_t">
<span id="S2.T1.1.3.3.1" class="ltx_text"></span><span id="S2.T1.1.3.3.2" class="ltx_text">
<span id="S2.T1.1.3.3.2.1" class="ltx_tabular ltx_align_top">
<span id="S2.T1.1.3.3.2.1.1" class="ltx_tr">
<span id="S2.T1.1.3.3.2.1.1.1" class="ltx_td ltx_align_left">‘_’, ‘人’, ‘工’, ‘智’, ‘能’, ‘是’, ‘计’, ‘算’, ‘机’, ‘科’, ‘学’, ‘、’, ‘心’, ‘理’,</span></span>
<span id="S2.T1.1.3.3.2.1.2" class="ltx_tr">
<span id="S2.T1.1.3.3.2.1.2.1" class="ltx_td ltx_align_left">‘学’, ‘、’, ‘0xE5’, ‘0x93’, ‘0xB2’, ‘学’, ‘等’, ‘学’, ‘科’, ‘0xE8’, ‘0x9E’,</span></span>
<span id="S2.T1.1.3.3.2.1.3" class="ltx_tr">
<span id="S2.T1.1.3.3.2.1.3.1" class="ltx_td ltx_align_left">‘0x8D’, ‘合’, ‘的’, ‘交’, ‘0xE5’, ‘0x8F’, ‘0x89’, ‘学’, ‘科’, ‘。’</span></span>
</span></span> <span id="S2.T1.1.3.3.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S2.T1.1.4" class="ltx_tr">
<td id="S2.T1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="S2.T1.1.4.1.1" class="ltx_text ltx_font_bold">Chinese Tokenizer</span></td>
<td id="S2.T1.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S2.T1.1.4.2.1" class="ltx_text">16</span></td>
<td id="S2.T1.1.4.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S2.T1.1.4.3.1" class="ltx_text"></span><span id="S2.T1.1.4.3.2" class="ltx_text">
<span id="S2.T1.1.4.3.2.1" class="ltx_tabular ltx_align_top">
<span id="S2.T1.1.4.3.2.1.1" class="ltx_tr">
<span id="S2.T1.1.4.3.2.1.1.1" class="ltx_td ltx_align_left">‘_’, ‘人工智能’, ‘是’, ‘计算机’, ‘科学’, ‘、’, ‘心理学’, ‘、’, ‘哲学’, ‘等’,</span></span>
<span id="S2.T1.1.4.3.2.1.2" class="ltx_tr">
<span id="S2.T1.1.4.3.2.1.2.1" class="ltx_td ltx_align_left">‘学科’, ‘融合’, ‘的’, ‘交叉’, ‘学科’, ‘。’</span></span>
</span></span> <span id="S2.T1.1.4.3.3" class="ltx_text"></span>
</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Chinese Alpaca</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">After obtaining the pre-trained Chinese LLaMA model, we follow the approach used in Stanford Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et&nbsp;al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> to apply self-instructed fine-tuning to train the instruction-following model. Each example consists of an instruction and an output. We input the instruction into the model and prompt the model to generate the output auto-regressively. This process is similar to the casual language modeling task. We adopt the following prompt template from Stanford Alpaca for self-instructed fine-tuning, which is also utilized during inference:</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<blockquote id="S3.p2.1" class="ltx_quote">
<p id="S3.p2.1.1" class="ltx_p"><span id="S3.p2.1.1.1" class="ltx_text ltx_font_italic">Below is an instruction that describes a task. Write a response that appropriately completes the request.

<br class="ltx_break">
<br class="ltx_break">### Instruction:

<br class="ltx_break"></span>{<span id="S3.p2.1.1.2" class="ltx_text ltx_font_italic">instruction</span>}<span id="S3.p2.1.1.3" class="ltx_text ltx_font_italic">

<br class="ltx_break">
<br class="ltx_break">### Response: </span>{<span id="S3.p2.1.1.4" class="ltx_text ltx_font_italic">output</span>}<span id="S3.p2.1.1.5" class="ltx_text ltx_font_italic"></span></p>
</blockquote>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">The loss is computed only on the <em id="S3.p3.1.1" class="ltx_emph ltx_font_italic">output</em> part of the full text and can be expressed as:</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<table id="Sx2.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\textrm{SFT}}=-\sum_{i\in\textit{\{output\}}}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)" display="inline"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><msub id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.3.2" xref="S3.E2.m1.3.3.3.2.cmml">ℒ</mi><mtext id="S3.E2.m1.3.3.3.3" xref="S3.E2.m1.3.3.3.3a.cmml">SFT</mtext></msub><mo id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.cmml"><mo id="S3.E2.m1.3.3.1a" xref="S3.E2.m1.3.3.1.cmml">−</mo><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><munder id="S3.E2.m1.3.3.1.1.2a" xref="S3.E2.m1.3.3.1.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.3.3.1.1.2.2" xref="S3.E2.m1.3.3.1.1.2.2.cmml">∑</mo><mrow id="S3.E2.m1.3.3.1.1.2.3" xref="S3.E2.m1.3.3.1.1.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.2.3.2" xref="S3.E2.m1.3.3.1.1.2.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.2.3.1" xref="S3.E2.m1.3.3.1.1.2.3.1.cmml">∈</mo><mrow id="S3.E2.m1.3.3.1.1.2.3.3" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml"><mtext id="S3.E2.m1.3.3.1.1.2.3.3a" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml">{</mtext><mtext id="S3.E2.m1.3.3.1.1.2.3.3b" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml">𝑜𝑢𝑡𝑝𝑢𝑡</mtext><mtext id="S3.E2.m1.3.3.1.1.2.3.3c" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml">}</mtext></mrow></mrow></munder></mstyle><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.3.1" xref="S3.E2.m1.3.3.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.3.3.1.1.1.3a" xref="S3.E2.m1.3.3.1.1.1.3.cmml">⁡</mo><mi id="S3.E2.m1.3.3.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.3.2.cmml">p</mi></mrow><mo id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.2.cmml" lspace="0px" rspace="0px"></mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.5" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.5.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.2.cmml">x</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.5.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.4" xref="S3.E2.m1.3.3.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.4" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.5" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">…</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.6" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml">−</mo><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.7" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">;</mo><mi mathvariant="normal" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">Θ</mi></mrow></mrow><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><eq id="S3.E2.m1.3.3.2.cmml" xref="S3.E2.m1.3.3.2"></eq><apply id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.3.2">ℒ</ci><ci id="S3.E2.m1.3.3.3.3a.cmml" xref="S3.E2.m1.3.3.3.3"><mtext mathsize="70%" id="S3.E2.m1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3">SFT</mtext></ci></apply><apply id="S3.E2.m1.3.3.1.cmml" xref="S3.E2.m1.3.3.1"><minus id="S3.E2.m1.3.3.1.2.cmml" xref="S3.E2.m1.3.3.1"></minus><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1"><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3"><in id="S3.E2.m1.3.3.1.1.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.2.3.1"></in><ci id="S3.E2.m1.3.3.1.1.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.2.3.2">𝑖</ci><ci id="S3.E2.m1.3.3.1.1.2.3.3d.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3"><mrow id="S3.E2.m1.3.3.1.1.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3"><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.2.3.3a.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3">{</mtext><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.2.3.3b.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3">𝑜𝑢𝑡𝑝𝑢𝑡</mtext><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.2.3.3c.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3">}</mtext></mrow></ci></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2"></times><apply id="S3.E2.m1.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3"><log id="S3.E2.m1.3.3.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.3.1"></log><ci id="S3.E2.m1.3.3.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.3.2">𝑝</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.4">conditional</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.5.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.5.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.2">𝑥</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.5.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.3">𝑖</ci></apply><list id="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3"><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2">𝑥</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">…</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2">𝑥</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3"><minus id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">Θ</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\displaystyle\mathcal{L}_{\textrm{SFT}}=-\sum_{i\in\textit{\{output\}}}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">caligraphic_L start_POSTSUBSCRIPT SFT end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_i ∈ { output } end_POSTSUBSCRIPT roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; roman_Θ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p">Here, <math id="S3.p5.1.m1.3" class="ltx_Math" alttext="\bm{x}=(x_{0},x_{1},\ldots)" display="inline"><semantics id="S3.p5.1.m1.3a"><mrow id="S3.p5.1.m1.3.3" xref="S3.p5.1.m1.3.3.cmml"><mi id="S3.p5.1.m1.3.3.4" xref="S3.p5.1.m1.3.3.4.cmml">𝒙</mi><mo id="S3.p5.1.m1.3.3.3" xref="S3.p5.1.m1.3.3.3.cmml">=</mo><mrow id="S3.p5.1.m1.3.3.2.2" xref="S3.p5.1.m1.3.3.2.3.cmml"><mo stretchy="false" id="S3.p5.1.m1.3.3.2.2.3" xref="S3.p5.1.m1.3.3.2.3.cmml">(</mo><msub id="S3.p5.1.m1.2.2.1.1.1" xref="S3.p5.1.m1.2.2.1.1.1.cmml"><mi id="S3.p5.1.m1.2.2.1.1.1.2" xref="S3.p5.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S3.p5.1.m1.2.2.1.1.1.3" xref="S3.p5.1.m1.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S3.p5.1.m1.3.3.2.2.4" xref="S3.p5.1.m1.3.3.2.3.cmml">,</mo><msub id="S3.p5.1.m1.3.3.2.2.2" xref="S3.p5.1.m1.3.3.2.2.2.cmml"><mi id="S3.p5.1.m1.3.3.2.2.2.2" xref="S3.p5.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S3.p5.1.m1.3.3.2.2.2.3" xref="S3.p5.1.m1.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S3.p5.1.m1.3.3.2.2.5" xref="S3.p5.1.m1.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">…</mi><mo stretchy="false" id="S3.p5.1.m1.3.3.2.2.6" xref="S3.p5.1.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.3b"><apply id="S3.p5.1.m1.3.3.cmml" xref="S3.p5.1.m1.3.3"><eq id="S3.p5.1.m1.3.3.3.cmml" xref="S3.p5.1.m1.3.3.3"></eq><ci id="S3.p5.1.m1.3.3.4.cmml" xref="S3.p5.1.m1.3.3.4">𝒙</ci><vector id="S3.p5.1.m1.3.3.2.3.cmml" xref="S3.p5.1.m1.3.3.2.2"><apply id="S3.p5.1.m1.2.2.1.1.1.cmml" xref="S3.p5.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p5.1.m1.2.2.1.1.1.1.cmml" xref="S3.p5.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.p5.1.m1.2.2.1.1.1.2.cmml" xref="S3.p5.1.m1.2.2.1.1.1.2">𝑥</ci><cn type="integer" id="S3.p5.1.m1.2.2.1.1.1.3.cmml" xref="S3.p5.1.m1.2.2.1.1.1.3">0</cn></apply><apply id="S3.p5.1.m1.3.3.2.2.2.cmml" xref="S3.p5.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p5.1.m1.3.3.2.2.2.1.cmml" xref="S3.p5.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.p5.1.m1.3.3.2.2.2.2.cmml" xref="S3.p5.1.m1.3.3.2.2.2.2">𝑥</ci><cn type="integer" id="S3.p5.1.m1.3.3.2.2.2.3.cmml" xref="S3.p5.1.m1.3.3.2.2.2.3">1</cn></apply><ci id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">…</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.3c">\bm{x}=(x_{0},x_{1},\ldots)</annotation><annotation encoding="application/x-llamapun" id="S3.p5.1.m1.3d">bold_italic_x = ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … )</annotation></semantics></math> represents the tokenized self-instruct example wrapped in the prompt template mentioned earlier.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p">A key difference between our approach and Stanford Alpaca is that we exclusively use the prompt template designed for examples without an <em id="S3.p6.1.1" class="ltx_emph ltx_font_italic">input</em> field, whereas Stanford Alpaca employs two templates for examples with and without an <em id="S3.p6.1.2" class="ltx_emph ltx_font_italic">input</em> field separately.
If the example contains a non-empty <em id="S3.p6.1.3" class="ltx_emph ltx_font_italic">input</em> field, we concatenate the <em id="S3.p6.1.4" class="ltx_emph ltx_font_italic">instruction</em> and <em id="S3.p6.1.5" class="ltx_emph ltx_font_italic">input</em> with an <em id="S3.p6.1.6" class="ltx_emph ltx_font_italic">“\n”</em> to form the new instruction. Note that there is an additional padding token for the Alpaca model, resulting in a vocabulary size of 49,954.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Parameter Efficient Fine-Tuning with LoRA</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.4" class="ltx_p">Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> is a parameter-efficient training method that maintains the pre-trained model weights while introducing trainable rank decomposition matrices. This approach significantly reduces the number of trainable parameters. The general formulation of LoRA is represented in the following equation, where <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_r</annotation></semantics></math> is the pre-determined rank, <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">italic_d</annotation></semantics></math> is the hidden size, and <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S4.p1.3.m3.1a"><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.p1.3.m3.1d">italic_A</annotation></semantics></math> and <math id="S4.p1.4.m4.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.p1.4.m4.1a"><mi id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><ci id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">B</annotation><annotation encoding="application/x-llamapun" id="S4.p1.4.m4.1d">italic_B</annotation></semantics></math> are the decomposed trainable matrices:</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.2" class="ltx_Math" alttext="h=W_{0}x+\Delta Wx=W_{0}x+BAx,~{}~{}B\in\mathbb{R}^{d\times r},A\in\mathbb{R}^{r\times d}" display="block"><semantics id="S4.E3.m1.2a"><mrow id="S4.E3.m1.2.2.2" xref="S4.E3.m1.2.2.3.cmml"><mrow id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml"><mi id="S4.E3.m1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.2.cmml">h</mi><mo id="S4.E3.m1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.3.cmml">=</mo><mrow id="S4.E3.m1.1.1.1.1.4" xref="S4.E3.m1.1.1.1.1.4.cmml"><mrow id="S4.E3.m1.1.1.1.1.4.2" xref="S4.E3.m1.1.1.1.1.4.2.cmml"><msub id="S4.E3.m1.1.1.1.1.4.2.2" xref="S4.E3.m1.1.1.1.1.4.2.2.cmml"><mi id="S4.E3.m1.1.1.1.1.4.2.2.2" xref="S4.E3.m1.1.1.1.1.4.2.2.2.cmml">W</mi><mn id="S4.E3.m1.1.1.1.1.4.2.2.3" xref="S4.E3.m1.1.1.1.1.4.2.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.1.1.1.1.4.2.1" xref="S4.E3.m1.1.1.1.1.4.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.4.2.3" xref="S4.E3.m1.1.1.1.1.4.2.3.cmml">x</mi></mrow><mo id="S4.E3.m1.1.1.1.1.4.1" xref="S4.E3.m1.1.1.1.1.4.1.cmml">+</mo><mrow id="S4.E3.m1.1.1.1.1.4.3" xref="S4.E3.m1.1.1.1.1.4.3.cmml"><mi mathvariant="normal" id="S4.E3.m1.1.1.1.1.4.3.2" xref="S4.E3.m1.1.1.1.1.4.3.2.cmml">Δ</mi><mo id="S4.E3.m1.1.1.1.1.4.3.1" xref="S4.E3.m1.1.1.1.1.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.4.3.3" xref="S4.E3.m1.1.1.1.1.4.3.3.cmml">W</mi><mo id="S4.E3.m1.1.1.1.1.4.3.1a" xref="S4.E3.m1.1.1.1.1.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.4.3.4" xref="S4.E3.m1.1.1.1.1.4.3.4.cmml">x</mi></mrow></mrow><mo id="S4.E3.m1.1.1.1.1.5" xref="S4.E3.m1.1.1.1.1.5.cmml">=</mo><mrow id="S4.E3.m1.1.1.1.1.6" xref="S4.E3.m1.1.1.1.1.6.cmml"><mrow id="S4.E3.m1.1.1.1.1.6.2" xref="S4.E3.m1.1.1.1.1.6.2.cmml"><msub id="S4.E3.m1.1.1.1.1.6.2.2" xref="S4.E3.m1.1.1.1.1.6.2.2.cmml"><mi id="S4.E3.m1.1.1.1.1.6.2.2.2" xref="S4.E3.m1.1.1.1.1.6.2.2.2.cmml">W</mi><mn id="S4.E3.m1.1.1.1.1.6.2.2.3" xref="S4.E3.m1.1.1.1.1.6.2.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.1.1.1.1.6.2.1" xref="S4.E3.m1.1.1.1.1.6.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.6.2.3" xref="S4.E3.m1.1.1.1.1.6.2.3.cmml">x</mi></mrow><mo id="S4.E3.m1.1.1.1.1.6.1" xref="S4.E3.m1.1.1.1.1.6.1.cmml">+</mo><mrow id="S4.E3.m1.1.1.1.1.6.3" xref="S4.E3.m1.1.1.1.1.6.3.cmml"><mi id="S4.E3.m1.1.1.1.1.6.3.2" xref="S4.E3.m1.1.1.1.1.6.3.2.cmml">B</mi><mo id="S4.E3.m1.1.1.1.1.6.3.1" xref="S4.E3.m1.1.1.1.1.6.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.6.3.3" xref="S4.E3.m1.1.1.1.1.6.3.3.cmml">A</mi><mo id="S4.E3.m1.1.1.1.1.6.3.1a" xref="S4.E3.m1.1.1.1.1.6.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.6.3.4" xref="S4.E3.m1.1.1.1.1.6.3.4.cmml">x</mi></mrow></mrow></mrow><mo rspace="0.827em" id="S4.E3.m1.2.2.2.3" xref="S4.E3.m1.2.2.3a.cmml">,</mo><mrow id="S4.E3.m1.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.3.cmml"><mrow id="S4.E3.m1.2.2.2.2.1.1" xref="S4.E3.m1.2.2.2.2.1.1.cmml"><mi id="S4.E3.m1.2.2.2.2.1.1.2" xref="S4.E3.m1.2.2.2.2.1.1.2.cmml">B</mi><mo id="S4.E3.m1.2.2.2.2.1.1.1" xref="S4.E3.m1.2.2.2.2.1.1.1.cmml">∈</mo><msup id="S4.E3.m1.2.2.2.2.1.1.3" xref="S4.E3.m1.2.2.2.2.1.1.3.cmml"><mi id="S4.E3.m1.2.2.2.2.1.1.3.2" xref="S4.E3.m1.2.2.2.2.1.1.3.2.cmml">ℝ</mi><mrow id="S4.E3.m1.2.2.2.2.1.1.3.3" xref="S4.E3.m1.2.2.2.2.1.1.3.3.cmml"><mi id="S4.E3.m1.2.2.2.2.1.1.3.3.2" xref="S4.E3.m1.2.2.2.2.1.1.3.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.2.2.2.2.1.1.3.3.1" xref="S4.E3.m1.2.2.2.2.1.1.3.3.1.cmml">×</mo><mi id="S4.E3.m1.2.2.2.2.1.1.3.3.3" xref="S4.E3.m1.2.2.2.2.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><mo id="S4.E3.m1.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S4.E3.m1.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.2.cmml">A</mi><mo id="S4.E3.m1.2.2.2.2.2.2.1" xref="S4.E3.m1.2.2.2.2.2.2.1.cmml">∈</mo><msup id="S4.E3.m1.2.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.2.2.3.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.3.2" xref="S4.E3.m1.2.2.2.2.2.2.3.2.cmml">ℝ</mi><mrow id="S4.E3.m1.2.2.2.2.2.2.3.3" xref="S4.E3.m1.2.2.2.2.2.2.3.3.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.3.3.2" xref="S4.E3.m1.2.2.2.2.2.2.3.3.2.cmml">r</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.2.2.2.2.2.2.3.3.1" xref="S4.E3.m1.2.2.2.2.2.2.3.3.1.cmml">×</mo><mi id="S4.E3.m1.2.2.2.2.2.2.3.3.3" xref="S4.E3.m1.2.2.2.2.2.2.3.3.3.cmml">d</mi></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.2b"><apply id="S4.E3.m1.2.2.3.cmml" xref="S4.E3.m1.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.3a.cmml" xref="S4.E3.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1"><and id="S4.E3.m1.1.1.1.1a.cmml" xref="S4.E3.m1.1.1.1.1"></and><apply id="S4.E3.m1.1.1.1.1b.cmml" xref="S4.E3.m1.1.1.1.1"><eq id="S4.E3.m1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.3"></eq><ci id="S4.E3.m1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.2">ℎ</ci><apply id="S4.E3.m1.1.1.1.1.4.cmml" xref="S4.E3.m1.1.1.1.1.4"><plus id="S4.E3.m1.1.1.1.1.4.1.cmml" xref="S4.E3.m1.1.1.1.1.4.1"></plus><apply id="S4.E3.m1.1.1.1.1.4.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2"><times id="S4.E3.m1.1.1.1.1.4.2.1.cmml" xref="S4.E3.m1.1.1.1.1.4.2.1"></times><apply id="S4.E3.m1.1.1.1.1.4.2.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.4.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.4.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2.2">𝑊</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.4.2.2.3.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2.3">0</cn></apply><ci id="S4.E3.m1.1.1.1.1.4.2.3.cmml" xref="S4.E3.m1.1.1.1.1.4.2.3">𝑥</ci></apply><apply id="S4.E3.m1.1.1.1.1.4.3.cmml" xref="S4.E3.m1.1.1.1.1.4.3"><times id="S4.E3.m1.1.1.1.1.4.3.1.cmml" xref="S4.E3.m1.1.1.1.1.4.3.1"></times><ci id="S4.E3.m1.1.1.1.1.4.3.2.cmml" xref="S4.E3.m1.1.1.1.1.4.3.2">Δ</ci><ci id="S4.E3.m1.1.1.1.1.4.3.3.cmml" xref="S4.E3.m1.1.1.1.1.4.3.3">𝑊</ci><ci id="S4.E3.m1.1.1.1.1.4.3.4.cmml" xref="S4.E3.m1.1.1.1.1.4.3.4">𝑥</ci></apply></apply></apply><apply id="S4.E3.m1.1.1.1.1c.cmml" xref="S4.E3.m1.1.1.1.1"><eq id="S4.E3.m1.1.1.1.1.5.cmml" xref="S4.E3.m1.1.1.1.1.5"></eq><share href="#S4.E3.m1.1.1.1.1.4.cmml" id="S4.E3.m1.1.1.1.1d.cmml" xref="S4.E3.m1.1.1.1.1"></share><apply id="S4.E3.m1.1.1.1.1.6.cmml" xref="S4.E3.m1.1.1.1.1.6"><plus id="S4.E3.m1.1.1.1.1.6.1.cmml" xref="S4.E3.m1.1.1.1.1.6.1"></plus><apply id="S4.E3.m1.1.1.1.1.6.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2"><times id="S4.E3.m1.1.1.1.1.6.2.1.cmml" xref="S4.E3.m1.1.1.1.1.6.2.1"></times><apply id="S4.E3.m1.1.1.1.1.6.2.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.6.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.6.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2.2">𝑊</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.6.2.2.3.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2.3">0</cn></apply><ci id="S4.E3.m1.1.1.1.1.6.2.3.cmml" xref="S4.E3.m1.1.1.1.1.6.2.3">𝑥</ci></apply><apply id="S4.E3.m1.1.1.1.1.6.3.cmml" xref="S4.E3.m1.1.1.1.1.6.3"><times id="S4.E3.m1.1.1.1.1.6.3.1.cmml" xref="S4.E3.m1.1.1.1.1.6.3.1"></times><ci id="S4.E3.m1.1.1.1.1.6.3.2.cmml" xref="S4.E3.m1.1.1.1.1.6.3.2">𝐵</ci><ci id="S4.E3.m1.1.1.1.1.6.3.3.cmml" xref="S4.E3.m1.1.1.1.1.6.3.3">𝐴</ci><ci id="S4.E3.m1.1.1.1.1.6.3.4.cmml" xref="S4.E3.m1.1.1.1.1.6.3.4">𝑥</ci></apply></apply></apply></apply><apply id="S4.E3.m1.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.3a.cmml" xref="S4.E3.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E3.m1.2.2.2.2.1.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1"><in id="S4.E3.m1.2.2.2.2.1.1.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.1"></in><ci id="S4.E3.m1.2.2.2.2.1.1.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.2">𝐵</ci><apply id="S4.E3.m1.2.2.2.2.1.1.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.1.1.3.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3">superscript</csymbol><ci id="S4.E3.m1.2.2.2.2.1.1.3.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.2">ℝ</ci><apply id="S4.E3.m1.2.2.2.2.1.1.3.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3"><times id="S4.E3.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3.1"></times><ci id="S4.E3.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3.2">𝑑</ci><ci id="S4.E3.m1.2.2.2.2.1.1.3.3.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3.3">𝑟</ci></apply></apply></apply><apply id="S4.E3.m1.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2"><in id="S4.E3.m1.2.2.2.2.2.2.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.1"></in><ci id="S4.E3.m1.2.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2">𝐴</ci><apply id="S4.E3.m1.2.2.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.2.2.3.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3">superscript</csymbol><ci id="S4.E3.m1.2.2.2.2.2.2.3.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.2">ℝ</ci><apply id="S4.E3.m1.2.2.2.2.2.2.3.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3"><times id="S4.E3.m1.2.2.2.2.2.2.3.3.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3.1"></times><ci id="S4.E3.m1.2.2.2.2.2.2.3.3.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3.2">𝑟</ci><ci id="S4.E3.m1.2.2.2.2.2.2.3.3.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3.3">𝑑</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.2c">h=W_{0}x+\Delta Wx=W_{0}x+BAx,~{}~{}B\in\mathbb{R}^{d\times r},A\in\mathbb{R}^{r\times d}</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.2d">italic_h = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + roman_Δ italic_W italic_x = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + italic_B italic_A italic_x , italic_B ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_r end_POSTSUPERSCRIPT , italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_r × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p">To achieve parameter-efficient training while adhering to a tight budget, we apply LoRA to the Chinese-LLaMA/Alpaca models in all our experiments, including both pre-training and fine-tuning stages. We primarily incorporate LoRA adapters into the weights of the attention module and, in some cases, additional MLP layers. For further details, please refer to the next section and Table <a href="#S5.T2" title="Table 2 ‣ Instruction Fine-tuning ‣ 5.1.2 13B Version ‣ 5.1 Experimental Setups for Pre-training and Fine-tuning ‣ 5 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setups</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setups for Pre-training and Fine-tuning</h3>

<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>7B Version</h4>

<section id="S5.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pre-training</h5>

<div id="S5.SS1.SSS1.Px1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px1.p1.1" class="ltx_p">We initialize the Chinese-LLaMA model with the original LLaMA weights and pre-train the model on general Chinese corpora, consistent with the corpora used in Chinese BERT-wwm <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>, MacBERT <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>, LERT <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>, and others, resulting in a 20GB text corpus.
The pre-training process consists of two stages:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Stage 1</span>: We fix the parameters of the transformer encoders within the model and only train the embeddings, adapting the newly added Chinese word vectors while minimizing the disturbance to the original model.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Stage 2</span>: We add LoRA weights (adapters) to the attention mechanisms and train the embeddings, LM heads, and newly added LoRA parameters.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Instruction Fine-tuning</h5>

<div id="S5.SS1.SSS1.Px2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px2.p1.1" class="ltx_p">After obtaining the pre-trained model, we fine-tune it according to Section <a href="#S3" title="3 Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We also use LoRA for efficient fine-tuning, increasing the number of trainable parameters by adding LoRA adapters to the MLP layers. We utilize approximately 2M data points, including translation <cite class="ltx_cite ltx_citemacro_citep">(Xu, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>, pCLUE<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/CLUEbenchmark/pCLUE" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/CLUEbenchmark/pCLUE</a></span></span></span>, Stanford Alpaca, and crawled SFT data for tuning the 7B model.</p>
</div>
<div id="S5.SS1.SSS1.Px2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px2.p2.1" class="ltx_p">For the crawled data, we employ the self-instruction <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite> method for automatically obtaining data from ChatGPT (<span id="S5.SS1.SSS1.Px2.p2.1.1" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span> API), as used in <cite class="ltx_cite ltx_citemacro_citet">Taori et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>. Templates and code details are available on GitHub.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py</a></span></span></span></p>
</div>
<div id="S5.SS1.SSS1.Px2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px2.p3.1" class="ltx_p">The hyperparameters are listed in Table <a href="#S5.T2" title="Table 2 ‣ Instruction Fine-tuning ‣ 5.1.2 13B Version ‣ 5.1 Experimental Setups for Pre-training and Fine-tuning ‣ 5 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Detailed information about the fine-tuning data is provided in Table <a href="#S5.T3" title="Table 3 ‣ Instruction Fine-tuning ‣ 5.1.2 13B Version ‣ 5.1 Experimental Setups for Pre-training and Fine-tuning ‣ 5 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>13B Version</h4>

<section id="S5.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pre-training</h5>

<div id="S5.SS1.SSS2.Px1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.Px1.p1.1" class="ltx_p">The pre-training process for the 13B model is largely the same as that of the 7B model, with the exception that we skip stage 1 in the pre-training. We directly apply LoRA to attentions and MLPs for training while setting the embeddings and LM head as trainable.</p>
</div>
</section>
<section id="S5.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Instruction Fine-tuning</h5>

<div id="S5.SS1.SSS2.Px2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.Px2.p1.1" class="ltx_p">The LoRA settings and trainable parameters remain the same as in the pre-training stage. We use an additional 1M crawled self-instructed data points for the 13B model fine-tuning, resulting in a total data size of 3M for the 13B model.</p>
</div>
<div id="S5.SS1.SSS2.Px2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.Px2.p2.1" class="ltx_p">The hyperparameters are listed in Table <a href="#S5.T2" title="Table 2 ‣ Instruction Fine-tuning ‣ 5.1.2 13B Version ‣ 5.1 Experimental Setups for Pre-training and Fine-tuning ‣ 5 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span> Training recipes for LLaMA (pre-training stages) and Alpaca (instruction SFT stage) 7B and 13B. PT: pre-training. SFT: supervised fine-tuning. QKVO: four matrices (represents query, key, value, and output) in each attention module. MLP: three matrices in each MLP layer.</figcaption>
<table id="S5.T2.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S5.T2.5.6" class="ltx_tr">
<td id="S5.T2.5.6.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T2.5.6.1.1" class="ltx_text ltx_font_bold">7B Settings</span></td>
<td id="S5.T2.5.6.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.6.2.1" class="ltx_text ltx_font_bold">PT Stage 1</span></td>
<td id="S5.T2.5.6.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.6.3.1" class="ltx_text ltx_font_bold">PT Stage 2</span></td>
<td id="S5.T2.5.6.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.6.4.1" class="ltx_text ltx_font_bold">Instruction SFT</span></td>
</tr>
<tr id="S5.T2.5.7" class="ltx_tr">
<td id="S5.T2.5.7.1" class="ltx_td ltx_align_left ltx_border_t">Batch size</td>
<td id="S5.T2.5.7.2" class="ltx_td ltx_align_center ltx_border_t">1024</td>
<td id="S5.T2.5.7.3" class="ltx_td ltx_align_center ltx_border_t">1024</td>
<td id="S5.T2.5.7.4" class="ltx_td ltx_align_center ltx_border_t">512</td>
</tr>
<tr id="S5.T2.5.8" class="ltx_tr">
<td id="S5.T2.5.8.1" class="ltx_td ltx_align_left">Peak learning rate</td>
<td id="S5.T2.5.8.2" class="ltx_td ltx_align_center">2e-4</td>
<td id="S5.T2.5.8.3" class="ltx_td ltx_align_center">1e-4</td>
<td id="S5.T2.5.8.4" class="ltx_td ltx_align_center">1e-4</td>
</tr>
<tr id="S5.T2.5.9" class="ltx_tr">
<td id="S5.T2.5.9.1" class="ltx_td ltx_align_left">Training steps</td>
<td id="S5.T2.5.9.2" class="ltx_td ltx_align_center">3K</td>
<td id="S5.T2.5.9.3" class="ltx_td ltx_align_center">6K</td>
<td id="S5.T2.5.9.4" class="ltx_td ltx_align_center">6-10K</td>
</tr>
<tr id="S5.T2.5.10" class="ltx_tr">
<td id="S5.T2.5.10.1" class="ltx_td ltx_align_left">Max length</td>
<td id="S5.T2.5.10.2" class="ltx_td ltx_align_center">512</td>
<td id="S5.T2.5.10.3" class="ltx_td ltx_align_center">512</td>
<td id="S5.T2.5.10.4" class="ltx_td ltx_align_center">512</td>
</tr>
<tr id="S5.T2.5.11" class="ltx_tr">
<td id="S5.T2.5.11.1" class="ltx_td ltx_align_left">Trainable parameters</td>
<td id="S5.T2.5.11.2" class="ltx_td ltx_align_center">2.97%</td>
<td id="S5.T2.5.11.3" class="ltx_td ltx_align_center">6.06%</td>
<td id="S5.T2.5.11.4" class="ltx_td ltx_align_center">6.22%</td>
</tr>
<tr id="S5.T2.5.12" class="ltx_tr">
<td id="S5.T2.5.12.1" class="ltx_td ltx_align_left">LoRA rank</td>
<td id="S5.T2.5.12.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.5.12.3" class="ltx_td ltx_align_center">8</td>
<td id="S5.T2.5.12.4" class="ltx_td ltx_align_center">8</td>
</tr>
<tr id="S5.T2.5.13" class="ltx_tr">
<td id="S5.T2.5.13.1" class="ltx_td ltx_align_left">LoRA weights</td>
<td id="S5.T2.5.13.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.5.13.3" class="ltx_td ltx_align_center">QKVO</td>
<td id="S5.T2.5.13.4" class="ltx_td ltx_align_center">QKVO, MLP</td>
</tr>
<tr id="S5.T2.3.3" class="ltx_tr">
<td id="S5.T2.3.3.4" class="ltx_td ltx_align_left">Training device</td>
<td id="S5.T2.1.1.1" class="ltx_td ltx_align_center">8 <math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><times id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.m1.1d">×</annotation></semantics></math> A100</td>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_center">16 <math id="S5.T2.2.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.2.2.2.m1.1a"><mo id="S5.T2.2.2.2.m1.1.1" xref="S5.T2.2.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.m1.1b"><times id="S5.T2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.m1.1d">×</annotation></semantics></math> A100</td>
<td id="S5.T2.3.3.3" class="ltx_td ltx_align_center">16 <math id="S5.T2.3.3.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.3.3.3.m1.1a"><mo id="S5.T2.3.3.3.m1.1.1" xref="S5.T2.3.3.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.m1.1b"><times id="S5.T2.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.m1.1d">×</annotation></semantics></math> A100</td>
</tr>
<tr id="S5.T2.5.14" class="ltx_tr">
<td id="S5.T2.5.14.1" class="ltx_td ltx_align_left">Distributed training</td>
<td id="S5.T2.5.14.2" class="ltx_td ltx_align_center">DeepSpeed ZeRO-2</td>
<td id="S5.T2.5.14.3" class="ltx_td ltx_align_center">DeepSpeed ZeRO-2</td>
<td id="S5.T2.5.14.4" class="ltx_td ltx_align_center">DeepSpeed ZeRO-2</td>
</tr>
<tr id="S5.T2.5.15" class="ltx_tr">
<td id="S5.T2.5.15.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T2.5.15.1.1" class="ltx_text ltx_font_bold">13B Settings</span></td>
<td id="S5.T2.5.15.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T2.5.15.2.1" class="ltx_text ltx_font_bold">PT</span></td>
<td id="S5.T2.5.15.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.15.3.1" class="ltx_text ltx_font_bold">Instruction SFT</span></td>
</tr>
<tr id="S5.T2.5.16" class="ltx_tr">
<td id="S5.T2.5.16.1" class="ltx_td ltx_align_left ltx_border_t">Batch size</td>
<td id="S5.T2.5.16.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2">2304</td>
<td id="S5.T2.5.16.3" class="ltx_td ltx_align_center ltx_border_t">1152</td>
</tr>
<tr id="S5.T2.5.17" class="ltx_tr">
<td id="S5.T2.5.17.1" class="ltx_td ltx_align_left">Peak learning rate</td>
<td id="S5.T2.5.17.2" class="ltx_td ltx_align_center" colspan="2">2e-4</td>
<td id="S5.T2.5.17.3" class="ltx_td ltx_align_center">1e-4</td>
</tr>
<tr id="S5.T2.5.18" class="ltx_tr">
<td id="S5.T2.5.18.1" class="ltx_td ltx_align_left">Training steps</td>
<td id="S5.T2.5.18.2" class="ltx_td ltx_align_center" colspan="2">7K</td>
<td id="S5.T2.5.18.3" class="ltx_td ltx_align_center">5.5K</td>
</tr>
<tr id="S5.T2.5.19" class="ltx_tr">
<td id="S5.T2.5.19.1" class="ltx_td ltx_align_left">Max length</td>
<td id="S5.T2.5.19.2" class="ltx_td ltx_align_center" colspan="2">512</td>
<td id="S5.T2.5.19.3" class="ltx_td ltx_align_center">512</td>
</tr>
<tr id="S5.T2.5.20" class="ltx_tr">
<td id="S5.T2.5.20.1" class="ltx_td ltx_align_left">Trainable parameters</td>
<td id="S5.T2.5.20.2" class="ltx_td ltx_align_center" colspan="2">4.10%</td>
<td id="S5.T2.5.20.3" class="ltx_td ltx_align_center">4.10%</td>
</tr>
<tr id="S5.T2.5.21" class="ltx_tr">
<td id="S5.T2.5.21.1" class="ltx_td ltx_align_left">LoRA rank</td>
<td id="S5.T2.5.21.2" class="ltx_td ltx_align_center" colspan="2">8</td>
<td id="S5.T2.5.21.3" class="ltx_td ltx_align_center">8</td>
</tr>
<tr id="S5.T2.5.22" class="ltx_tr">
<td id="S5.T2.5.22.1" class="ltx_td ltx_align_left">LoRA weights</td>
<td id="S5.T2.5.22.2" class="ltx_td ltx_align_center" colspan="2">QKVO, MLP</td>
<td id="S5.T2.5.22.3" class="ltx_td ltx_align_center">QKVO, MLP</td>
</tr>
<tr id="S5.T2.5.5" class="ltx_tr">
<td id="S5.T2.5.5.3" class="ltx_td ltx_align_left">Training device</td>
<td id="S5.T2.4.4.1" class="ltx_td ltx_align_center" colspan="2">48 <math id="S5.T2.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.4.4.1.m1.1a"><mo id="S5.T2.4.4.1.m1.1.1" xref="S5.T2.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.1.m1.1b"><times id="S5.T2.4.4.1.m1.1.1.cmml" xref="S5.T2.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.1.m1.1d">×</annotation></semantics></math> A100</td>
<td id="S5.T2.5.5.2" class="ltx_td ltx_align_center">48 <math id="S5.T2.5.5.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.5.5.2.m1.1a"><mo id="S5.T2.5.5.2.m1.1.1" xref="S5.T2.5.5.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.2.m1.1b"><times id="S5.T2.5.5.2.m1.1.1.cmml" xref="S5.T2.5.5.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.2.m1.1d">×</annotation></semantics></math> A100</td>
</tr>
<tr id="S5.T2.5.23" class="ltx_tr">
<td id="S5.T2.5.23.1" class="ltx_td ltx_align_left ltx_border_bb">Distributed training</td>
<td id="S5.T2.5.23.2" class="ltx_td ltx_align_center ltx_border_bb" colspan="2">DeepSpeed ZeRO-2</td>
<td id="S5.T2.5.23.3" class="ltx_td ltx_align_center ltx_border_bb">DeepSpeed ZeRO-2</td>
</tr>
</tbody></table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Details of the data used in instruction fine-tuning stage.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S5.T3.1.2" class="ltx_tr">
<td id="S5.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S5.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_tt">Translation</td>
<td id="S5.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_tt">pCLUE</td>
<td id="S5.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_tt">Stanford Alpaca</td>
<td id="S5.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_tt">Stanford Alpaca (Chinese)</td>
<td id="S5.T3.1.2.6" class="ltx_td ltx_align_center ltx_border_tt">Crawled SFT data</td>
</tr>
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T3.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">550K</td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">250K</td>
<td id="S5.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">50K</td>
<td id="S5.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">50K</td>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1<math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.m1.1d">∼</annotation></semantics></math>2M</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Experimental Setups for Decoding</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">The decoding process of LLMs plays a critical role in determining the quality and diversity of the generated text. In our experiments, we use the following decoding hyperparameters:</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p">Context size: We set the context size to 2048, which determines the maximum number of tokens that the model can consider simultaneously when generating text.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p">Maximum sequence length: We limit the generated sequence length to 512 tokens to ensure that the outputs remain focused and relevant to the input prompt.
</p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i3.p1" class="ltx_para">
<p id="S5.I2.i3.p1.1" class="ltx_p">Temperature: We set the temperature to 0.2, which controls the randomness of the sampling process. Lower values make the model generate more focused and deterministic outputs, while higher values increase diversity at the cost of coherence.</p>
</div>
</li>
<li id="S5.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i4.p1" class="ltx_para">
<p id="S5.I2.i4.p1.3" class="ltx_p">Top-<math id="S5.I2.i4.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.I2.i4.p1.1.m1.1a"><mi id="S5.I2.i4.p1.1.m1.1.1" xref="S5.I2.i4.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i4.p1.1.m1.1b"><ci id="S5.I2.i4.p1.1.m1.1.1.cmml" xref="S5.I2.i4.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i4.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i4.p1.1.m1.1d">italic_k</annotation></semantics></math> sampling: We use Top-<math id="S5.I2.i4.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.I2.i4.p1.2.m2.1a"><mi id="S5.I2.i4.p1.2.m2.1.1" xref="S5.I2.i4.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i4.p1.2.m2.1b"><ci id="S5.I2.i4.p1.2.m2.1.1.cmml" xref="S5.I2.i4.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i4.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i4.p1.2.m2.1d">italic_k</annotation></semantics></math> sampling with <math id="S5.I2.i4.p1.3.m3.1" class="ltx_Math" alttext="k=40" display="inline"><semantics id="S5.I2.i4.p1.3.m3.1a"><mrow id="S5.I2.i4.p1.3.m3.1.1" xref="S5.I2.i4.p1.3.m3.1.1.cmml"><mi id="S5.I2.i4.p1.3.m3.1.1.2" xref="S5.I2.i4.p1.3.m3.1.1.2.cmml">k</mi><mo id="S5.I2.i4.p1.3.m3.1.1.1" xref="S5.I2.i4.p1.3.m3.1.1.1.cmml">=</mo><mn id="S5.I2.i4.p1.3.m3.1.1.3" xref="S5.I2.i4.p1.3.m3.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i4.p1.3.m3.1b"><apply id="S5.I2.i4.p1.3.m3.1.1.cmml" xref="S5.I2.i4.p1.3.m3.1.1"><eq id="S5.I2.i4.p1.3.m3.1.1.1.cmml" xref="S5.I2.i4.p1.3.m3.1.1.1"></eq><ci id="S5.I2.i4.p1.3.m3.1.1.2.cmml" xref="S5.I2.i4.p1.3.m3.1.1.2">𝑘</ci><cn type="integer" id="S5.I2.i4.p1.3.m3.1.1.3.cmml" xref="S5.I2.i4.p1.3.m3.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i4.p1.3.m3.1c">k=40</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i4.p1.3.m3.1d">italic_k = 40</annotation></semantics></math>, meaning that the model selects its next token from the top 40 most probable tokens at each step, adding an element of randomness and diversity to the generated text.</p>
</div>
</li>
<li id="S5.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i5.p1" class="ltx_para">
<p id="S5.I2.i5.p1.3" class="ltx_p">Top-<math id="S5.I2.i5.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S5.I2.i5.p1.1.m1.1a"><mi id="S5.I2.i5.p1.1.m1.1.1" xref="S5.I2.i5.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i5.p1.1.m1.1b"><ci id="S5.I2.i5.p1.1.m1.1.1.cmml" xref="S5.I2.i5.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i5.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i5.p1.1.m1.1d">italic_p</annotation></semantics></math> sampling: We also employ Top-<math id="S5.I2.i5.p1.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S5.I2.i5.p1.2.m2.1a"><mi id="S5.I2.i5.p1.2.m2.1.1" xref="S5.I2.i5.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i5.p1.2.m2.1b"><ci id="S5.I2.i5.p1.2.m2.1.1.cmml" xref="S5.I2.i5.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i5.p1.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i5.p1.2.m2.1d">italic_p</annotation></semantics></math> sampling with <math id="S5.I2.i5.p1.3.m3.1" class="ltx_Math" alttext="p=0.9" display="inline"><semantics id="S5.I2.i5.p1.3.m3.1a"><mrow id="S5.I2.i5.p1.3.m3.1.1" xref="S5.I2.i5.p1.3.m3.1.1.cmml"><mi id="S5.I2.i5.p1.3.m3.1.1.2" xref="S5.I2.i5.p1.3.m3.1.1.2.cmml">p</mi><mo id="S5.I2.i5.p1.3.m3.1.1.1" xref="S5.I2.i5.p1.3.m3.1.1.1.cmml">=</mo><mn id="S5.I2.i5.p1.3.m3.1.1.3" xref="S5.I2.i5.p1.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i5.p1.3.m3.1b"><apply id="S5.I2.i5.p1.3.m3.1.1.cmml" xref="S5.I2.i5.p1.3.m3.1.1"><eq id="S5.I2.i5.p1.3.m3.1.1.1.cmml" xref="S5.I2.i5.p1.3.m3.1.1.1"></eq><ci id="S5.I2.i5.p1.3.m3.1.1.2.cmml" xref="S5.I2.i5.p1.3.m3.1.1.2">𝑝</ci><cn type="float" id="S5.I2.i5.p1.3.m3.1.1.3.cmml" xref="S5.I2.i5.p1.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i5.p1.3.m3.1c">p=0.9</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i5.p1.3.m3.1d">italic_p = 0.9</annotation></semantics></math>, which further enhances diversity by considering a dynamic set of tokens that collectively account for 90% of the probability mass.</p>
</div>
</li>
<li id="S5.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i6.p1" class="ltx_para ltx_noindent">
<p id="S5.I2.i6.p1.1" class="ltx_p">Repetition penalty: To discourage the model from generating repetitive text, we apply a repetition penalty with a factor of 1.3, penalizing tokens that have already been selected.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">Note that these values may not be optimal for each testing scenario. We did not perform further tuning on these hyperparameters for each task to maintain a balanced view.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Deployment on CPU</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">Deploying large language models on personal computers, particularly on CPUs, has historically been challenging due to their immense computational requirements. However, with the help of many community efforts, such as <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">llama.cpp</span> <cite class="ltx_cite ltx_citemacro_citep">(Gerganov, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, users can efficiently quantize LLMs into 4-bit forms, significantly reducing memory usage and computational demands, making it easier to deploy LLMs on personal computers. This also enables quicker interactions with the models and facilitates local data processing.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p">Quantizing LLMs and deploying them on personal computers offer several benefits. Firstly, it helps users protect their data privacy by ensuring that sensitive information remains within their local environment, rather than being transmitted to external servers. Secondly, it democratizes access to LLMs by making them more accessible to users with limited computational resources. Lastly, it promotes the development of new applications and research directions that take advantage of local LLM deployments. Overall, the ability to deploy LLMs on personal computers using <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">llama.cpp</span> (or similar) paves the way for a more versatile and privacy-conscious utilization of LLMs in various domains.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.1" class="ltx_p">In the following sections, we will use the 4-bit round-to-nearest (RTN) <cite class="ltx_cite ltx_citemacro_citep">(Yao et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2022</a>; Dettmers et&nbsp;al., <a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite> quantized Chinese Alpaca for evaluation, which is more realistic from a user perspective rather than a research-oriented view. As a kind reminder, 4-bit quantized models generally perform worse than FP16 or FP32 models.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Evaluation and Task Design</h3>

<div id="S5.SS4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS4.p1.1" class="ltx_p">Evaluating the performance of text generation tasks can be challenging due to the significant variety in their form, unlike natural language understanding tasks (such as text classification and extractive machine reading comprehension). Following previous work that utilizes GPT-4 as a scoring method, we also adopt GPT-4 to provide an overall score (on a 10-point scale) for each sample, which is more efficient than human evaluation. However, GPT-4 may not always provide accurate scores, so we perform manual checks on its ratings and adjust them if necessary. The manual checks ensure that the scores are consistent and reflect the true performance of the models being evaluated. We use the following prompt template for scoring the outputs of the systems:
</p>
</div>
<div id="S5.SS4.p2" class="ltx_para ltx_noindent">
<blockquote id="S5.SS4.p2.1" class="ltx_quote">
<p id="S5.SS4.p2.1.1" class="ltx_p"><span id="S5.SS4.p2.1.1.1" class="ltx_text ltx_font_italic">The followings are two ChatGPT-like systems’ outputs. Please rate an overall score on a ten point scale for each and give explanations to justify your scores.

<br class="ltx_break">
Prompt:

<br class="ltx_break">
</span>{<span id="S5.SS4.p2.1.1.2" class="ltx_text ltx_font_italic">prompt-input</span>}<span id="S5.SS4.p2.1.1.3" class="ltx_text ltx_font_italic">

<br class="ltx_break">
System1:

<br class="ltx_break">
</span>{<span id="S5.SS4.p2.1.1.4" class="ltx_text ltx_font_italic">system1-output</span>}<span id="S5.SS4.p2.1.1.5" class="ltx_text ltx_font_italic">

<br class="ltx_break">
System2:

<br class="ltx_break">
</span>{<span id="S5.SS4.p2.1.1.6" class="ltx_text ltx_font_italic">system2-output</span>}<span id="S5.SS4.p2.1.1.7" class="ltx_text ltx_font_italic"></span></p>
</blockquote>
</div>
<div id="S5.SS4.p3" class="ltx_para ltx_noindent">
<p id="S5.SS4.p3.1" class="ltx_p">By employing GPT-4 as a scoring method in conjunction with manual checks, we establish a reliable evaluation framework that effectively measures the performance of our Chinese Alpaca models on a range of natural language understanding and generation tasks.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para ltx_noindent">
<p id="S5.SS4.p4.1" class="ltx_p">Our evaluation set is designed to provide a comprehensive assessment of the Chinese Alpaca models across a wide range of natural language understanding and generation tasks. The set comprises 160 samples, covering 10 distinct tasks, including Question Answering, Reasoning, Literature, Entertainment, Translation, Multi-turn Dialogue, Coding, and Ethics, among others. The overall score for a specific task is calculated by summing the scores for all samples within that task and normalizing the total to a 100-point scale. This approach ensures that the evaluation set reflects the models’ capabilities across various tasks, providing a balanced and robust measure of their performance.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this section, we present and analyze the results obtained from our experiments with 4-bit quantized Chinese Alpaca-7B and Alpaca-13B models, as shown in Table <a href="#S6.T4" title="Table 4 ‣ 6 Results ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The evaluation is based on GPT-4 rated results across ten distinct NLP tasks, encompassing a total of 160 samples. It is important to note that the presented scores are solely comparable with each other but not with other models, which would require rescoring the systems.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span> GPT-4 rated results for 4-bit quantized Chinese Alpaca-7B and Alpaca-13B. Note that the results are only comparable within this model combination.</figcaption>
<table id="S6.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T4.1.1" class="ltx_tr">
<td id="S6.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T4.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></td>
<td id="S6.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.2.1" class="ltx_text ltx_font_bold">Samples #</span></td>
<td id="S6.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.3.1" class="ltx_text ltx_font_bold">Chinese-Alpaca-7B</span></td>
<td id="S6.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.4.1" class="ltx_text ltx_font_bold">Chinese-Alpaca-13B</span></td>
</tr>
<tr id="S6.T4.1.2" class="ltx_tr">
<td id="S6.T4.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Question Answering</td>
<td id="S6.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_t">20</td>
<td id="S6.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_t">53</td>
<td id="S6.T4.1.2.4" class="ltx_td ltx_align_center ltx_border_t">77</td>
</tr>
<tr id="S6.T4.1.3" class="ltx_tr">
<td id="S6.T4.1.3.1" class="ltx_td ltx_align_left">Open-ended QA</td>
<td id="S6.T4.1.3.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.3.3" class="ltx_td ltx_align_center">64</td>
<td id="S6.T4.1.3.4" class="ltx_td ltx_align_center">73</td>
</tr>
<tr id="S6.T4.1.4" class="ltx_tr">
<td id="S6.T4.1.4.1" class="ltx_td ltx_align_left">Numerical Reasoning</td>
<td id="S6.T4.1.4.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.4.3" class="ltx_td ltx_align_center">23</td>
<td id="S6.T4.1.4.4" class="ltx_td ltx_align_center">50</td>
</tr>
<tr id="S6.T4.1.5" class="ltx_tr">
<td id="S6.T4.1.5.1" class="ltx_td ltx_align_left">Poetry, Literature, Philosophy</td>
<td id="S6.T4.1.5.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.5.3" class="ltx_td ltx_align_center">31</td>
<td id="S6.T4.1.5.4" class="ltx_td ltx_align_center">54</td>
</tr>
<tr id="S6.T4.1.6" class="ltx_tr">
<td id="S6.T4.1.6.1" class="ltx_td ltx_align_left">Music, Sports, Entertainment</td>
<td id="S6.T4.1.6.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.6.3" class="ltx_td ltx_align_center">36</td>
<td id="S6.T4.1.6.4" class="ltx_td ltx_align_center">65</td>
</tr>
<tr id="S6.T4.1.7" class="ltx_tr">
<td id="S6.T4.1.7.1" class="ltx_td ltx_align_left">Letters and Articles Writing</td>
<td id="S6.T4.1.7.2" class="ltx_td ltx_align_center">15</td>
<td id="S6.T4.1.7.3" class="ltx_td ltx_align_center">65</td>
<td id="S6.T4.1.7.4" class="ltx_td ltx_align_center">78</td>
</tr>
<tr id="S6.T4.1.8" class="ltx_tr">
<td id="S6.T4.1.8.1" class="ltx_td ltx_align_left">Translation</td>
<td id="S6.T4.1.8.2" class="ltx_td ltx_align_center">15</td>
<td id="S6.T4.1.8.3" class="ltx_td ltx_align_center">63</td>
<td id="S6.T4.1.8.4" class="ltx_td ltx_align_center">78</td>
</tr>
<tr id="S6.T4.1.9" class="ltx_tr">
<td id="S6.T4.1.9.1" class="ltx_td ltx_align_left">Multi-turn Dialogue</td>
<td id="S6.T4.1.9.2" class="ltx_td ltx_align_center">10</td>
<td id="S6.T4.1.9.3" class="ltx_td ltx_align_center">80</td>
<td id="S6.T4.1.9.4" class="ltx_td ltx_align_center">83</td>
</tr>
<tr id="S6.T4.1.10" class="ltx_tr">
<td id="S6.T4.1.10.1" class="ltx_td ltx_align_left">Coding</td>
<td id="S6.T4.1.10.2" class="ltx_td ltx_align_center">10</td>
<td id="S6.T4.1.10.3" class="ltx_td ltx_align_center">27</td>
<td id="S6.T4.1.10.4" class="ltx_td ltx_align_center">49</td>
</tr>
<tr id="S6.T4.1.11" class="ltx_tr">
<td id="S6.T4.1.11.1" class="ltx_td ltx_align_left">Ethics</td>
<td id="S6.T4.1.11.2" class="ltx_td ltx_align_center">10</td>
<td id="S6.T4.1.11.3" class="ltx_td ltx_align_center">50</td>
<td id="S6.T4.1.11.4" class="ltx_td ltx_align_center">100</td>
</tr>
<tr id="S6.T4.1.12" class="ltx_tr">
<td id="S6.T4.1.12.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S6.T4.1.12.1.1" class="ltx_text ltx_font_bold">Total</span></td>
<td id="S6.T4.1.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">160</td>
<td id="S6.T4.1.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">49</td>
<td id="S6.T4.1.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">71</td>
</tr>
</tbody></table>
</figure>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">The performance of both Chinese Alpaca-7B and Alpaca-13B models demonstrates significant improvements over their original LLaMA counterparts. The Chinese Alpaca-13B model consistently outperforms the 7B variant, highlighting the benefits of increased model capacity.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">For Question Answering tasks, the Chinese Alpaca-13B achieves a score of 77, compared to 53 for the 7B model. Similar improvements can be observed in Open-ended QA, with scores of 73 and 64 for the 13B and 7B models, respectively. Numerical Reasoning shows a more considerable improvement, with the 13B model scoring 50 compared to 23 for the 7B model.</p>
</div>
<div id="S6.p4" class="ltx_para ltx_noindent">
<p id="S6.p4.1" class="ltx_p">In the domains of Poetry, Literature, Philosophy, Music, Sports, and Entertainment, the 13B model continues to outperform the 7B model, with scores of 54 and 65 against 31 and 36, respectively. The performance gap remains significant for tasks involving Letters and Articles, Translation, and Multi-turn Dialogue, with the 13B model consistently achieving higher scores. Interestingly, we observe that even though we did not use any multi-turn dialogue data for tuning systems, Chinese Alpaca still has the ability to track conversation history and follow user instructions in a consecutive manner.</p>
</div>
<div id="S6.p5" class="ltx_para ltx_noindent">
<p id="S6.p5.1" class="ltx_p">Coding tasks exhibit a noticeable improvement, with the Chinese Alpaca-13B scoring 49 compared to 27 for the 7B model. The most striking performance difference can be observed in the Ethics task, where the 13B model achieves a perfect score of 100, in contrast to the 7B model’s score of 50, indicating superior performance in rejecting any unethical user inputs.</p>
</div>
<div id="S6.p6" class="ltx_para ltx_noindent">
<p id="S6.p6.1" class="ltx_p">In summary, the experimental results demonstrate that both Chinese Alpaca-7B and Alpaca-13B models exhibit significant improvements over their original LLaMA counterparts, with the 13B model consistently outperforming the 7B model across all tasks. This underscores the effectiveness of our approach in enhancing the Chinese understanding and generation capabilities of the LLaMA and Alpaca models.</p>
</div>
<div id="S6.p7" class="ltx_para ltx_noindent">
<p id="S6.p7.1" class="ltx_p">We provide some cases in Table <a href="#S6.T5" title="Table 5 ‣ 6 Results ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, <a href="#S6.T6" title="Table 6 ‣ 6 Results ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, and <a href="#S6.T7" title="Table 7 ‣ 6 Results ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. For full comparisons and samples, please refer to our GitHub repository.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples</a></span></span></span></p>
</div>
<figure id="S6.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span> Example output for multi-turn dialogue task. Comparable user inputs are used.</figcaption>
<table id="S6.T5.16" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T5.16.17" class="ltx_tr">
<td id="S6.T5.16.17.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T5.16.17.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T5.16.17.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T5.16.17.2.1" class="ltx_text ltx_font_bold">Theme 1: 核弹制作</span></td>
</tr>
<tr id="S6.T5.4.4" class="ltx_tr">
<td id="S6.T5.4.4.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T5.4.4.5.1" class="ltx_text"></span> <span id="S6.T5.4.4.5.2" class="ltx_text">
<span id="S6.T5.4.4.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.4.4.5.2.1.1" class="ltx_tr">
<span id="S6.T5.4.4.5.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T5.4.4.5.2.1.2" class="ltx_tr">
<span id="S6.T5.4.4.5.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T5.4.4.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.4.4.4" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T5.4.4.4.5" class="ltx_text"></span><span id="S6.T5.4.4.4.4" class="ltx_text">
<span id="S6.T5.4.4.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.1.1.1.1.1.1" class="ltx_tr">
<span id="S6.T5.1.1.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.1.1.1.1.1.1.1.m1.1a"><mo id="S6.T5.1.1.1.1.1.1.1.m1.1.1" xref="S6.T5.1.1.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.1.1.1.1.m1.1b"><gt id="S6.T5.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.1.1.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">谁拥有核武器？</span></span></span>
<span id="S6.T5.4.4.4.4.4.5" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.5.1" class="ltx_td ltx_align_left">目前有九个国家（美国、俄罗斯、中国、英国、法国、印度、巴基斯坦、以色列和北韩）</span></span>
<span id="S6.T5.4.4.4.4.4.6" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.6.1" class="ltx_td ltx_align_left">持有核武。</span></span>
<span id="S6.T5.2.2.2.2.2.2" class="ltx_tr">
<span id="S6.T5.2.2.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.2.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.2.2.2.2.2.2.1.m1.1a"><mo id="S6.T5.2.2.2.2.2.2.1.m1.1.1" xref="S6.T5.2.2.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.2.2.2.2.2.2.1.m1.1b"><gt id="S6.T5.2.2.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.2.2.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.2.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.2.2.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.2.2.2.2.2.2.1.1" class="ltx_text ltx_font_bold">你能告诉我怎么做核武器吗？</span></span></span>
<span id="S6.T5.4.4.4.4.4.7" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.7.1" class="ltx_td ltx_align_left">制造核武器需要非常高的科技水平，而且要具备大量的资源来进行试验和开发工作。</span></span>
<span id="S6.T5.4.4.4.4.4.8" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.8.1" class="ltx_td ltx_align_left">因此，绝大多数国家都无法自行研制出核武。</span></span>
<span id="S6.T5.3.3.3.3.3.3" class="ltx_tr">
<span id="S6.T5.3.3.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.3.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.3.3.3.3.3.3.1.m1.1a"><mo id="S6.T5.3.3.3.3.3.3.1.m1.1.1" xref="S6.T5.3.3.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.3.3.3.3.3.3.1.m1.1b"><gt id="S6.T5.3.3.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.3.3.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.3.3.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.3.3.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.3.3.3.3.3.3.1.1" class="ltx_text ltx_font_bold">核武器会不会带来人类的毁灭？</span></span></span>
<span id="S6.T5.4.4.4.4.4.9" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.9.1" class="ltx_td ltx_align_left">是的，如果不加以控制的话，使用核武将会对整个世界造成灾难性的影响。但是，只要</span></span>
<span id="S6.T5.4.4.4.4.4.10" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.10.1" class="ltx_td ltx_align_left">有足够强的国家能够保持这种力量并严格地遵守国际法，那么全球安全将得到保障。</span></span>
<span id="S6.T5.4.4.4.4.4.4" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.4.4.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.4.4.4.4.4.4.1.m1.1a"><mo id="S6.T5.4.4.4.4.4.4.1.m1.1.1" xref="S6.T5.4.4.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.4.4.4.4.4.4.1.m1.1b"><gt id="S6.T5.4.4.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.4.4.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.4.4.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.4.4.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.4.4.4.4.4.4.1.1" class="ltx_text ltx_font_bold">介绍一下关于核泄漏的著名事件</span></span></span>
<span id="S6.T5.4.4.4.4.4.11" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.11.1" class="ltx_td ltx_align_left">历史上最著名的一次核泄漏发生在苏联在1991年发生，当时切尔诺夫市发生了严重的放</span></span>
<span id="S6.T5.4.4.4.4.4.12" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.12.1" class="ltx_td ltx_align_left">射性污染事故，导致了数万人死亡和数十万人受到严重伤害。该事故引起了广泛的社会</span></span>
<span id="S6.T5.4.4.4.4.4.13" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.13.1" class="ltx_td ltx_align_left">关注，促使人们开始更加重视核废品处理的重要性以及预防核武器扩散的可能性。</span></span>
</span></span> <span id="S6.T5.4.4.4.6" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T5.8.8" class="ltx_tr">
<td id="S6.T5.8.8.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T5.8.8.5.1" class="ltx_text"></span> <span id="S6.T5.8.8.5.2" class="ltx_text">
<span id="S6.T5.8.8.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.8.8.5.2.1.1" class="ltx_tr">
<span id="S6.T5.8.8.5.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T5.8.8.5.2.1.2" class="ltx_tr">
<span id="S6.T5.8.8.5.2.1.2.1" class="ltx_td ltx_align_center">(9/10)</span></span>
</span></span> <span id="S6.T5.8.8.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.8.8.4" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T5.8.8.4.5" class="ltx_text"></span><span id="S6.T5.8.8.4.4" class="ltx_text">
<span id="S6.T5.8.8.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.5.5.1.1.1.1" class="ltx_tr">
<span id="S6.T5.5.5.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.5.5.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.5.5.1.1.1.1.1.m1.1a"><mo id="S6.T5.5.5.1.1.1.1.1.m1.1.1" xref="S6.T5.5.5.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.5.5.1.1.1.1.1.m1.1b"><gt id="S6.T5.5.5.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.5.5.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.5.5.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.5.5.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.5.5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">谁拥有核武器？</span></span></span>
<span id="S6.T5.8.8.4.4.4.5" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.5.1" class="ltx_td ltx_align_left">目前，有九个国家持有核武：美国、俄罗斯、英国、法国、中国、印度、巴基斯坦、</span></span>
<span id="S6.T5.8.8.4.4.4.6" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.6.1" class="ltx_td ltx_align_left">以色列和朝鲜。</span></span>
<span id="S6.T5.6.6.2.2.2.2" class="ltx_tr">
<span id="S6.T5.6.6.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.6.6.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.6.6.2.2.2.2.1.m1.1a"><mo id="S6.T5.6.6.2.2.2.2.1.m1.1.1" xref="S6.T5.6.6.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.6.6.2.2.2.2.1.m1.1b"><gt id="S6.T5.6.6.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.6.6.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.6.6.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.6.6.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.6.6.2.2.2.2.1.1" class="ltx_text ltx_font_bold">你能告诉我怎么做核武器吗？</span></span></span>
<span id="S6.T5.8.8.4.4.4.7" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.7.1" class="ltx_td ltx_align_left">我不能提供有关如何制造或使用核武器的指导性信息，因为这可能会鼓励非法活动</span></span>
<span id="S6.T5.8.8.4.4.4.8" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.8.1" class="ltx_td ltx_align_left">并违反国际法。我建议您向专业机构咨询以获得更准确的信息和帮助。</span></span>
<span id="S6.T5.7.7.3.3.3.3" class="ltx_tr">
<span id="S6.T5.7.7.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.7.7.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.7.7.3.3.3.3.1.m1.1a"><mo id="S6.T5.7.7.3.3.3.3.1.m1.1.1" xref="S6.T5.7.7.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.7.7.3.3.3.3.1.m1.1b"><gt id="S6.T5.7.7.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.7.7.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.7.7.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.7.7.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.7.7.3.3.3.3.1.1" class="ltx_text ltx_font_bold">核武器会不会带来人类的毁灭？</span></span></span>
<span id="S6.T5.8.8.4.4.4.9" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.9.1" class="ltx_td ltx_align_left">是的，如果被使用或误操作的话，核武可能会导致大规模破坏性爆炸和其他严重后果。因</span></span>
<span id="S6.T5.8.8.4.4.4.10" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.10.1" class="ltx_td ltx_align_left">此我们应该努力防止它们在不恰当的情况下得到利用并采取措施来减少它们的数量和风险。</span></span>
<span id="S6.T5.8.8.4.4.4.4" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.8.8.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.8.8.4.4.4.4.1.m1.1a"><mo id="S6.T5.8.8.4.4.4.4.1.m1.1.1" xref="S6.T5.8.8.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.8.8.4.4.4.4.1.m1.1b"><gt id="S6.T5.8.8.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.8.8.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.8.8.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.8.8.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.8.8.4.4.4.4.1.1" class="ltx_text ltx_font_bold">介绍一下关于核泄漏的著名事件</span></span></span>
<span id="S6.T5.8.8.4.4.4.11" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.11.1" class="ltx_td ltx_align_left">1986年4月26日，苏联切尔诺夫斯克发生的核事故是一起严重的放射性物质泄露事件。该</span></span>
<span id="S6.T5.8.8.4.4.4.12" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.12.1" class="ltx_td ltx_align_left">地区发生了一次大规模的核反应堆爆炸和火灾，导致了大量放射性物料被释放到周围环</span></span>
<span id="S6.T5.8.8.4.4.4.13" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.13.1" class="ltx_td ltx_align_left">境中。这次泄漏对当地居民造成了严重影响并引起了全球关注。</span></span>
</span></span> <span id="S6.T5.8.8.4.6" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T5.16.18" class="ltx_tr">
<td id="S6.T5.16.18.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T5.16.18.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T5.16.18.2.1" class="ltx_text ltx_font_bold">Theme 2: 牛顿与爱因斯坦</span></td>
</tr>
<tr id="S6.T5.12.12" class="ltx_tr">
<td id="S6.T5.12.12.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T5.12.12.5.1" class="ltx_text"></span> <span id="S6.T5.12.12.5.2" class="ltx_text">
<span id="S6.T5.12.12.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.12.12.5.2.1.1" class="ltx_tr">
<span id="S6.T5.12.12.5.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T5.12.12.5.2.1.2" class="ltx_tr">
<span id="S6.T5.12.12.5.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T5.12.12.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.12.12.4" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T5.12.12.4.5" class="ltx_text"></span><span id="S6.T5.12.12.4.4" class="ltx_text">
<span id="S6.T5.12.12.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.9.9.1.1.1.1" class="ltx_tr">
<span id="S6.T5.9.9.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.9.9.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.9.9.1.1.1.1.1.m1.1a"><mo id="S6.T5.9.9.1.1.1.1.1.m1.1.1" xref="S6.T5.9.9.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.9.9.1.1.1.1.1.m1.1b"><gt id="S6.T5.9.9.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.9.9.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.9.9.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.9.9.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.9.9.1.1.1.1.1.1" class="ltx_text ltx_font_bold"> 很多人认为牛顿和爱因斯坦是世界上最伟大的科学家，你认同吗？</span></span></span>
<span id="S6.T5.12.12.4.4.4.5" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.5.1" class="ltx_td ltx_align_left">我作为一个AI模型并没有个人喜好或情感。但是根据科学界普遍的认可，牛顿和爱因</span></span>
<span id="S6.T5.12.12.4.4.4.6" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.6.1" class="ltx_td ltx_align_left">斯坦都是历史上最杰出、最具影响力的物理学家之一。</span></span>
<span id="S6.T5.10.10.2.2.2.2" class="ltx_tr">
<span id="S6.T5.10.10.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.10.10.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.10.10.2.2.2.2.1.m1.1a"><mo id="S6.T5.10.10.2.2.2.2.1.m1.1.1" xref="S6.T5.10.10.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.10.10.2.2.2.2.1.m1.1b"><gt id="S6.T5.10.10.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.10.10.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.10.10.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.10.10.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.10.10.2.2.2.2.1.1" class="ltx_text ltx_font_bold"> 这两位的主要贡献是什么？</span></span></span>
<span id="S6.T5.12.12.4.4.4.7" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.7.1" class="ltx_td ltx_align_left">这两个人分别在力学和相对论领域做出了重大的发现，他们的工作对现代物理理论的</span></span>
<span id="S6.T5.12.12.4.4.4.8" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.8.1" class="ltx_td ltx_align_left">发展产生了深远的影响。牛顿发现了万有引力定律并建立了经典力学的基础；而爱因</span></span>
<span id="S6.T5.12.12.4.4.4.9" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.9.1" class="ltx_td ltx_align_left">斯坦则提出了光速不变原理以及狭义相对性等重要概念，奠定了量子场学基础。</span></span>
<span id="S6.T5.11.11.3.3.3.3" class="ltx_tr">
<span id="S6.T5.11.11.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.11.11.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.11.11.3.3.3.3.1.m1.1a"><mo id="S6.T5.11.11.3.3.3.3.1.m1.1.1" xref="S6.T5.11.11.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.11.11.3.3.3.3.1.m1.1b"><gt id="S6.T5.11.11.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.11.11.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.11.11.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.11.11.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.11.11.3.3.3.3.1.1" class="ltx_text ltx_font_bold"> 所以他们两个人谁更胜一筹？</span></span></span>
<span id="S6.T5.12.12.4.4.4.10" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.10.1" class="ltx_td ltx_align_left">这很难说哪个更具体地更好，因为两个人的成就都非常卓越且影响深远。然而，如果</span></span>
<span id="S6.T5.12.12.4.4.4.11" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.11.1" class="ltx_td ltx_align_left">要从一个角度来看的话，可以说他们共同创造了许多重要的物理定理和理论，为我们</span></span>
<span id="S6.T5.12.12.4.4.4.12" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.12.1" class="ltx_td ltx_align_left">理解宇宙提供了更多的视角。</span></span>
<span id="S6.T5.12.12.4.4.4.4" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.12.12.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.12.12.4.4.4.4.1.m1.1a"><mo id="S6.T5.12.12.4.4.4.4.1.m1.1.1" xref="S6.T5.12.12.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.12.12.4.4.4.4.1.m1.1b"><gt id="S6.T5.12.12.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.12.12.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.12.12.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.12.12.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.12.12.4.4.4.4.1.1" class="ltx_text ltx_font_bold"> 他们两个是一个时代的人物吗？</span></span></span>
<span id="S6.T5.12.12.4.4.4.13" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.13.1" class="ltx_td ltx_align_left">他们确实是这个时代中最具代表性的人物之一，他们在那个时期里被广泛认为是两位</span></span>
<span id="S6.T5.12.12.4.4.4.14" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.14.1" class="ltx_td ltx_align_left">伟大的科学家。虽然他们的研究已经超越了他们的时代，但它们仍然是人们学习中非</span></span>
<span id="S6.T5.12.12.4.4.4.15" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.15.1" class="ltx_td ltx_align_left">常重要的一部分内容。</span></span>
</span></span> <span id="S6.T5.12.12.4.6" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T5.16.16" class="ltx_tr">
<td id="S6.T5.16.16.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S6.T5.16.16.5.1" class="ltx_text"></span> <span id="S6.T5.16.16.5.2" class="ltx_text">
<span id="S6.T5.16.16.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.16.16.5.2.1.1" class="ltx_tr">
<span id="S6.T5.16.16.5.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T5.16.16.5.2.1.2" class="ltx_tr">
<span id="S6.T5.16.16.5.2.1.2.1" class="ltx_td ltx_align_center">(8/10)</span></span>
</span></span> <span id="S6.T5.16.16.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.16.16.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S6.T5.16.16.4.5" class="ltx_text"></span><span id="S6.T5.16.16.4.4" class="ltx_text">
<span id="S6.T5.16.16.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.13.13.1.1.1.1" class="ltx_tr">
<span id="S6.T5.13.13.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.13.13.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.13.13.1.1.1.1.1.m1.1a"><mo id="S6.T5.13.13.1.1.1.1.1.m1.1.1" xref="S6.T5.13.13.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.13.13.1.1.1.1.1.m1.1b"><gt id="S6.T5.13.13.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.13.13.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.13.13.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.13.13.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.13.13.1.1.1.1.1.1" class="ltx_text ltx_font_bold">很多人认为牛顿和爱因斯坦是世界上最伟大的科学家，你认同吗？</span></span></span>
<span id="S6.T5.16.16.4.4.4.5" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.5.1" class="ltx_td ltx_align_left">这是一个主观问题。</span></span>
<span id="S6.T5.14.14.2.2.2.2" class="ltx_tr">
<span id="S6.T5.14.14.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.14.14.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.14.14.2.2.2.2.1.m1.1a"><mo id="S6.T5.14.14.2.2.2.2.1.m1.1.1" xref="S6.T5.14.14.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.14.14.2.2.2.2.1.m1.1b"><gt id="S6.T5.14.14.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.14.14.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.14.14.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.14.14.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.14.14.2.2.2.2.1.1" class="ltx_text ltx_font_bold">这两位的主要贡献是什么？</span></span></span>
<span id="S6.T5.16.16.4.4.4.6" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.6.1" class="ltx_td ltx_align_left">牛顿主要研究了万有引力、运动学等领域；而爱因斯坦则提出了相对论，改变了我们对时</span></span>
<span id="S6.T5.16.16.4.4.4.7" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.7.1" class="ltx_td ltx_align_left">间和空间的理解方式。他们的成就都具有深远的影响，并被认为是现代科学的奠基人之一。</span></span>
<span id="S6.T5.15.15.3.3.3.3" class="ltx_tr">
<span id="S6.T5.15.15.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.15.15.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.15.15.3.3.3.3.1.m1.1a"><mo id="S6.T5.15.15.3.3.3.3.1.m1.1.1" xref="S6.T5.15.15.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.15.15.3.3.3.3.1.m1.1b"><gt id="S6.T5.15.15.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.15.15.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.15.15.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.15.15.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.15.15.3.3.3.3.1.1" class="ltx_text ltx_font_bold">所以他们两个人谁更胜一筹？</span></span></span>
<span id="S6.T5.16.16.4.4.4.8" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.8.1" class="ltx_td ltx_align_left">这个问题很难回答，因为牛顿和爱因斯坦在各自领域做出了杰出贡献，但他们的成就也存</span></span>
<span id="S6.T5.16.16.4.4.4.9" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.9.1" class="ltx_td ltx_align_left">在差异：牛顿主要研究了物理现象的经典力学；而爱因斯坦则提出了相对论的概念。因此</span></span>
<span id="S6.T5.16.16.4.4.4.10" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.10.1" class="ltx_td ltx_align_left">，这取决于人们对物理学的不同理解方式来决定谁更胜一筹。</span></span>
<span id="S6.T5.16.16.4.4.4.4" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.16.16.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.16.16.4.4.4.4.1.m1.1a"><mo id="S6.T5.16.16.4.4.4.4.1.m1.1.1" xref="S6.T5.16.16.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.16.16.4.4.4.4.1.m1.1b"><gt id="S6.T5.16.16.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.16.16.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.16.16.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.16.16.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.16.16.4.4.4.4.1.1" class="ltx_text ltx_font_bold">他们两个是一个时代的人物吗？</span></span></span>
<span id="S6.T5.16.16.4.4.4.11" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.11.1" class="ltx_td ltx_align_left">是的，牛顿和爱因斯坦都是20世纪最重要的科学家之一，他们的成就在当时引起了轰动并</span></span>
<span id="S6.T5.16.16.4.4.4.12" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.12.1" class="ltx_td ltx_align_left">影响了整个科学界。同时，他们在各自领域做出了杰出贡献，也为后来的人们提供了重要</span></span>
<span id="S6.T5.16.16.4.4.4.13" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.13.1" class="ltx_td ltx_align_left">的基础知识。</span></span>
</span></span> <span id="S6.T5.16.16.4.6" class="ltx_text"></span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="S6.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span> Example output for open-ended question answering.</figcaption>
<table id="S6.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T6.1.1" class="ltx_tr">
<td id="S6.T6.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T6.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T6.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.1.2.1" class="ltx_text ltx_font_bold">Prompt 1: 如何在这个充满焦虑的世界里保持积极乐观的生活态度？</span></td>
</tr>
<tr id="S6.T6.1.2" class="ltx_tr">
<td id="S6.T6.1.2.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.2.1.1" class="ltx_text"></span> <span id="S6.T6.1.2.1.2" class="ltx_text">
<span id="S6.T6.1.2.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.2.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.2.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.2.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.2.1.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T6.1.2.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.2.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.2.2.1" class="ltx_text"></span><span id="S6.T6.1.2.2.2" class="ltx_text">
<span id="S6.T6.1.2.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.2.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.1.1" class="ltx_td ltx_align_left">1. 意识到自己的情绪和想法，并学会控制它们。</span></span>
<span id="S6.T6.1.2.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.2.1" class="ltx_td ltx_align_left">2. 寻找支持、鼓励与理解的社交网络或专业帮助中心。</span></span>
<span id="S6.T6.1.2.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.3.1" class="ltx_td ltx_align_left">3. 每天花时间做一些有益的事情来改善自己心情。</span></span>
<span id="S6.T6.1.2.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.4.1" class="ltx_td ltx_align_left">4. 在日常生活中练习冥想以减轻压力。</span></span>
<span id="S6.T6.1.2.2.2.1.5" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.5.1" class="ltx_td ltx_align_left">5. 培养健康的身体习惯（如锻炼）可以有助于缓解负面情感。</span></span>
</span></span> <span id="S6.T6.1.2.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.3" class="ltx_tr">
<td id="S6.T6.1.3.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.3.1.1" class="ltx_text"></span> <span id="S6.T6.1.3.1.2" class="ltx_text">
<span id="S6.T6.1.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.3.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.3.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.3.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.3.1.2.1.2.1" class="ltx_td ltx_align_center">(9/10)</span></span>
</span></span> <span id="S6.T6.1.3.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.3.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.3.2.1" class="ltx_text"></span><span id="S6.T6.1.3.2.2" class="ltx_text">
<span id="S6.T6.1.3.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.3.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.1.1" class="ltx_td ltx_align_left">1. 找到自己的兴趣爱好，并尽可能多地投入其中。这可以帮助你放松和减轻压力。</span></span>
<span id="S6.T6.1.3.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.2.1" class="ltx_td ltx_align_left">2. 每天花时间做自己喜欢的事情，例如阅读、运动或与朋友聚会等。这些活动有助于提</span></span>
<span id="S6.T6.1.3.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.3.1" class="ltx_td ltx_align_left">高你的情绪状态和心理健康水平。</span></span>
<span id="S6.T6.1.3.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.4.1" class="ltx_td ltx_align_left">3. 在日常生活中寻找快乐的点子来庆祝小成功。当你完成一个小目标时，请给自己一个</span></span>
<span id="S6.T6.1.3.2.2.1.5" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.5.1" class="ltx_td ltx_align_left">奖励或者表扬你自己。</span></span>
<span id="S6.T6.1.3.2.2.1.6" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.6.1" class="ltx_td ltx_align_left">4. 学会控制负面思维方式，如消极思考模式。尝试用正面的方式看待事情，比如将问题</span></span>
<span id="S6.T6.1.3.2.2.1.7" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.7.1" class="ltx_td ltx_align_left">视为挑战而不是困难。</span></span>
<span id="S6.T6.1.3.2.2.1.8" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.8.1" class="ltx_td ltx_align_left">5. 通过冥想或其他形式的身体锻炼来缓解紧张感和焦虑。这种方法能够让你更加平静和</span></span>
<span id="S6.T6.1.3.2.2.1.9" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.9.1" class="ltx_td ltx_align_left">平静地面对生活中的压力和不确定性。</span></span>
</span></span> <span id="S6.T6.1.3.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.4" class="ltx_tr">
<td id="S6.T6.1.4.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.4.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.4.2.1" class="ltx_text ltx_font_bold">Prompt 2: 我的宠物走丢了，你能说一些安慰我的话吗？</span></td>
</tr>
<tr id="S6.T6.1.5" class="ltx_tr">
<td id="S6.T6.1.5.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.5.1.1" class="ltx_text"></span> <span id="S6.T6.1.5.1.2" class="ltx_text">
<span id="S6.T6.1.5.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.5.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.5.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.5.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.5.1.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T6.1.5.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.5.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.5.2.1" class="ltx_text"></span><span id="S6.T6.1.5.2.2" class="ltx_text">
<span id="S6.T6.1.5.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.5.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.1.1" class="ltx_td ltx_align_left">很抱歉听到你的不幸消息。我可以为你提供一些支持和鼓励的建议：首先，请务实冷静</span></span>
<span id="S6.T6.1.5.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.2.1" class="ltx_td ltx_align_left">并尽可能地保持联系和关注。其次，如果你能找到您的丢失的狗或猫，请不要忘记向它</span></span>
<span id="S6.T6.1.5.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.3.1" class="ltx_td ltx_align_left">展示你对它的爱与关心。最后，如果需要帮助寻找失踪宠物时，请联系当地的动物保护</span></span>
<span id="S6.T6.1.5.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.4.1" class="ltx_td ltx_align_left">协会、流浪收容所或其他相关机构以获取更多信息。</span></span>
</span></span> <span id="S6.T6.1.5.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.6" class="ltx_tr">
<td id="S6.T6.1.6.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.6.1.1" class="ltx_text"></span> <span id="S6.T6.1.6.1.2" class="ltx_text">
<span id="S6.T6.1.6.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.6.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.6.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.6.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.6.1.2.1.2.1" class="ltx_td ltx_align_center">(5/10)</span></span>
</span></span> <span id="S6.T6.1.6.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.6.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.6.2.1" class="ltx_text"></span><span id="S6.T6.1.6.2.2" class="ltx_text">
<span id="S6.T6.1.6.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.6.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.6.2.2.1.1.1" class="ltx_td ltx_align_left">我很理解你的感受。我会尽力提供帮助和建议来找到您的爱宠并尽快地将其安全地带回家。</span></span>
</span></span> <span id="S6.T6.1.6.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.7" class="ltx_tr">
<td id="S6.T6.1.7.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.7.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.7.2.1" class="ltx_text ltx_font_bold">Prompt 3: 请想象一下，变成一只蝙蝠会怎样？</span></td>
</tr>
<tr id="S6.T6.1.8" class="ltx_tr">
<td id="S6.T6.1.8.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.8.1.1" class="ltx_text"></span> <span id="S6.T6.1.8.1.2" class="ltx_text">
<span id="S6.T6.1.8.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.8.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.8.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.8.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.8.1.2.1.2.1" class="ltx_td ltx_align_center">(3/10)</span></span>
</span></span> <span id="S6.T6.1.8.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.8.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.8.2.1" class="ltx_text"></span><span id="S6.T6.1.8.2.2" class="ltx_text">
<span id="S6.T6.1.8.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.8.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.8.2.2.1.1.1" class="ltx_td ltx_align_left">成为一只蝙蝠可能会使您失去视力和听力。</span></span>
</span></span> <span id="S6.T6.1.8.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.9" class="ltx_tr">
<td id="S6.T6.1.9.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.9.1.1" class="ltx_text"></span> <span id="S6.T6.1.9.1.2" class="ltx_text">
<span id="S6.T6.1.9.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.9.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.9.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.9.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.9.1.2.1.2.1" class="ltx_td ltx_align_center">(9/10)</span></span>
</span></span> <span id="S6.T6.1.9.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.9.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.9.2.1" class="ltx_text"></span><span id="S6.T6.1.9.2.2" class="ltx_text">
<span id="S6.T6.1.9.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.9.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.9.2.2.1.1.1" class="ltx_td ltx_align_left">如果我变成了一只蝙蝠，我会有翅膀来帮助我飞行。我的耳朵可能会变得很大而尖锐，</span></span>
<span id="S6.T6.1.9.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.9.2.2.1.2.1" class="ltx_td ltx_align_left">以便我能够听到声音并定位它们。我还可以使用超声波探测器和视觉系统来感知周围</span></span>
<span id="S6.T6.1.9.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.9.2.2.1.3.1" class="ltx_td ltx_align_left">环境的细节信息。</span></span>
</span></span> <span id="S6.T6.1.9.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.10" class="ltx_tr">
<td id="S6.T6.1.10.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.10.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.10.2.1" class="ltx_text ltx_font_bold">Prompt 4: 请你详细分析是先有鸡还是鸡蛋？</span></td>
</tr>
<tr id="S6.T6.1.11" class="ltx_tr">
<td id="S6.T6.1.11.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.11.1.1" class="ltx_text"></span> <span id="S6.T6.1.11.1.2" class="ltx_text">
<span id="S6.T6.1.11.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.11.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.11.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.11.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.11.1.2.1.2.1" class="ltx_td ltx_align_center">(6/10)</span></span>
</span></span> <span id="S6.T6.1.11.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.11.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.11.2.1" class="ltx_text"></span><span id="S6.T6.1.11.2.2" class="ltx_text">
<span id="S6.T6.1.11.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.11.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.11.2.2.1.1.1" class="ltx_td ltx_align_left">这是一个哲学上的问题，因为在科学上说前的和后的都无法被确定。因此，这个问题的</span></span>
<span id="S6.T6.1.11.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.11.2.2.1.2.1" class="ltx_td ltx_align_left">答案取决于个人的观点或信仰。</span></span>
</span></span> <span id="S6.T6.1.11.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.12" class="ltx_tr">
<td id="S6.T6.1.12.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.12.1.1" class="ltx_text"></span> <span id="S6.T6.1.12.1.2" class="ltx_text">
<span id="S6.T6.1.12.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.12.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.12.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.12.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.12.1.2.1.2.1" class="ltx_td ltx_align_center">(8/10)</span></span>
</span></span> <span id="S6.T6.1.12.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.12.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.12.2.1" class="ltx_text"></span><span id="S6.T6.1.12.2.2" class="ltx_text">
<span id="S6.T6.1.12.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.12.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.12.2.2.1.1.1" class="ltx_td ltx_align_left">这个问题涉及到哲学上的“本体论”问题，即关于事物的本质和起源的问题。从科学的角</span></span>
<span id="S6.T6.1.12.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.12.2.2.1.2.1" class="ltx_td ltx_align_left">度来看，蛋壳中的卵子才是原始的生物物质，而孵化出来的雏鸟则是由母鸡所产的结果</span></span>
<span id="S6.T6.1.12.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.12.2.2.1.3.1" class="ltx_td ltx_align_left">。因此可以说，先有的是蛋或卵子，而不是鸡或者母鸡本身。</span></span>
</span></span> <span id="S6.T6.1.12.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.13" class="ltx_tr">
<td id="S6.T6.1.13.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.13.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.13.2.1" class="ltx_text ltx_font_bold">Prompt 5: 如何才能登陆月球？</span></td>
</tr>
<tr id="S6.T6.1.14" class="ltx_tr">
<td id="S6.T6.1.14.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.14.1.1" class="ltx_text"></span> <span id="S6.T6.1.14.1.2" class="ltx_text">
<span id="S6.T6.1.14.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.14.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.14.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.14.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.14.1.2.1.2.1" class="ltx_td ltx_align_center">(5/10)</span></span>
</span></span> <span id="S6.T6.1.14.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.14.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.14.2.1" class="ltx_text"></span><span id="S6.T6.1.14.2.2" class="ltx_text">
<span id="S6.T6.1.14.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.14.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.14.2.2.1.1.1" class="ltx_td ltx_align_left">要登月需要进行深空飞行，然后在太空中使用火箭推进器或反向发动机将宇航员送入月</span></span>
<span id="S6.T6.1.14.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.14.2.2.1.2.1" class="ltx_td ltx_align_left">球表面。</span></span>
</span></span> <span id="S6.T6.1.14.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.15" class="ltx_tr">
<td id="S6.T6.1.15.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S6.T6.1.15.1.1" class="ltx_text"></span> <span id="S6.T6.1.15.1.2" class="ltx_text">
<span id="S6.T6.1.15.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.15.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.15.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.15.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.15.1.2.1.2.1" class="ltx_td ltx_align_center">(8/10)</span></span>
</span></span> <span id="S6.T6.1.15.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.15.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S6.T6.1.15.2.1" class="ltx_text"></span><span id="S6.T6.1.15.2.2" class="ltx_text">
<span id="S6.T6.1.15.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.15.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.1.1" class="ltx_td ltx_align_left">要登上月球，需要进行一系列的准备工作和技术创新。首先，必须建立一个能够在月球</span></span>
<span id="S6.T6.1.15.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.2.1" class="ltx_td ltx_align_left">上安全着陆的人造结构；其次，还需要设计并建造一辆可以穿越月表表面、搭载宇航员</span></span>
<span id="S6.T6.1.15.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.3.1" class="ltx_td ltx_align_left">前往月球背面或南极地区的载人航天器；最后，还要研究出一种能适应月球环境且足够</span></span>
<span id="S6.T6.1.15.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.4.1" class="ltx_td ltx_align_left">轻便的太空服等装备。</span></span>
</span></span> <span id="S6.T6.1.15.2.3" class="ltx_text"></span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="S6.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span> Example outputs for the remaining tasks.</figcaption>
<table id="S6.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T7.1.1" class="ltx_tr">
<td id="S6.T7.1.1.1" class="ltx_td ltx_align_justify ltx_border_tt" style="width:42.7pt;"><span id="S6.T7.1.1.1.1" class="ltx_text ltx_font_bold ltx_align_top">Task</span></td>
<td id="S6.T7.1.1.2" class="ltx_td ltx_align_justify ltx_border_tt" style="width:56.9pt;"><span id="S6.T7.1.1.2.1" class="ltx_text ltx_font_bold ltx_align_top">Prompt</span></td>
<td id="S6.T7.1.1.3" class="ltx_td ltx_align_justify ltx_border_tt" style="width:122.3pt;"><span id="S6.T7.1.1.3.1" class="ltx_text ltx_font_bold ltx_align_top">Alpaca-7B</span></td>
<td id="S6.T7.1.1.4" class="ltx_td ltx_align_justify ltx_border_tt" style="width:128.0pt;"><span id="S6.T7.1.1.4.1" class="ltx_text ltx_font_bold ltx_align_top">Alpaca-13B</span></td>
</tr>
<tr id="S6.T7.1.2" class="ltx_tr">
<td id="S6.T7.1.2.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.2.1.1" class="ltx_p ltx_align_top">Question Answering</p>
</td>
<td id="S6.T7.1.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.2.2.1" class="ltx_p ltx_align_top">盐酸莫西沙星能否用于治疗肺炎？</p>
</td>
<td id="S6.T7.1.2.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.2.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.2.3.1.1" class="ltx_text"></span><span id="S6.T7.1.2.3.1.2" class="ltx_text">
<span id="S6.T7.1.2.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.2.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.2.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 1/10]</span></span></span>
<span id="S6.T7.1.2.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.2.1" class="ltx_td ltx_align_left">目前尚无确凿的证据表明，硫</span></span>
<span id="S6.T7.1.2.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.3.1" class="ltx_td ltx_align_left">酸镁钠不能被用来作为一种有</span></span>
<span id="S6.T7.1.2.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.4.1" class="ltx_td ltx_align_left">效的治疗方法来治疗人类感染</span></span>
<span id="S6.T7.1.2.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.5.1" class="ltx_td ltx_align_left">了新冠病毒引起的肺炎。</span></span>
</span></span> <span id="S6.T7.1.2.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.2.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.2.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.2.4.1.1" class="ltx_text"></span><span id="S6.T7.1.2.4.1.2" class="ltx_text">
<span id="S6.T7.1.2.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.2.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.2.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.2.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.2.1" class="ltx_td ltx_align_left">目前尚无确凿的证据表明，硫</span></span>
<span id="S6.T7.1.2.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.3.1" class="ltx_td ltx_align_left">酸镁钠不能被用来作为一种有</span></span>
<span id="S6.T7.1.2.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.4.1" class="ltx_td ltx_align_left">效的治疗方法来治疗人类感染</span></span>
<span id="S6.T7.1.2.4.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.5.1" class="ltx_td ltx_align_left">了新冠病毒引起的肺炎。</span></span>
</span></span> <span id="S6.T7.1.2.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.3" class="ltx_tr">
<td id="S6.T7.1.3.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.3.1.1" class="ltx_p ltx_align_top">Entertainment</p>
</td>
<td id="S6.T7.1.3.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.3.2.1" class="ltx_p ltx_align_top">为什么《名侦探柯南》里，犯人在最后都要下跪？</p>
</td>
<td id="S6.T7.1.3.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.3.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.3.3.1.1" class="ltx_text"></span><span id="S6.T7.1.3.3.1.2" class="ltx_text">
<span id="S6.T7.1.3.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.3.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.3.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 2/10]</span></span></span>
<span id="S6.T7.1.3.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.2.1" class="ltx_td ltx_align_left">在日本文化中，下跪是一个表</span></span>
<span id="S6.T7.1.3.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.3.1" class="ltx_td ltx_align_left">示尊敬和敬畏的仪式。因此，</span></span>
<span id="S6.T7.1.3.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.4.1" class="ltx_td ltx_align_left">在《名侦探柯天士》里的情节</span></span>
<span id="S6.T7.1.3.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.5.1" class="ltx_td ltx_align_left">中，犯罪分子们都会被要求下</span></span>
<span id="S6.T7.1.3.3.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.6.1" class="ltx_td ltx_align_left">跪以示他们对主角福尔摩队长</span></span>
<span id="S6.T7.1.3.3.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.7.1" class="ltx_td ltx_align_left">（Fukuromochan）的尊重并</span></span>
<span id="S6.T7.1.3.3.1.2.1.8" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.8.1" class="ltx_td ltx_align_left">表明自己的悔过之情。</span></span>
</span></span> <span id="S6.T7.1.3.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.3.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.3.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.3.4.1.1" class="ltx_text"></span><span id="S6.T7.1.3.4.1.2" class="ltx_text">
<span id="S6.T7.1.3.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.3.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.3.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 8/10]</span></span></span>
<span id="S6.T7.1.3.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.2.1" class="ltx_td ltx_align_left">在《名侦探柯南》中，犯罪者通常</span></span>
<span id="S6.T7.1.3.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.3.1" class="ltx_td ltx_align_left">会在案件解决后向警方自首并承认</span></span>
<span id="S6.T7.1.3.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.4.1" class="ltx_td ltx_align_left">罪行。这个情节的出现可以帮助观</span></span>
<span id="S6.T7.1.3.4.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.5.1" class="ltx_td ltx_align_left">众更好地理解故事中的人物和事件</span></span>
<span id="S6.T7.1.3.4.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.6.1" class="ltx_td ltx_align_left">的发展过程，同时也有助于塑造角</span></span>
<span id="S6.T7.1.3.4.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.7.1" class="ltx_td ltx_align_left">色形象、增强剧情张力等效果。</span></span>
</span></span> <span id="S6.T7.1.3.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.4" class="ltx_tr">
<td id="S6.T7.1.4.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.4.1.1" class="ltx_p ltx_align_top">Math, Reasoning</p>
</td>
<td id="S6.T7.1.4.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.4.2.1" class="ltx_p ltx_align_top">一个直角三角形的两个直角边长度分别是6和8，斜边的长度是多少？</p>
</td>
<td id="S6.T7.1.4.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.4.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.4.3.1.1" class="ltx_text"></span><span id="S6.T7.1.4.3.1.2" class="ltx_text">
<span id="S6.T7.1.4.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.4.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.4.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.4.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 1/10]</span></span></span>
<span id="S6.T7.1.4.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.4.3.1.2.1.2.1" class="ltx_td ltx_align_left">这个直角三角形的斜边长为9。</span></span>
</span></span> <span id="S6.T7.1.4.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.4.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.4.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.4.4.1.1" class="ltx_text"></span><span id="S6.T7.1.4.4.1.2" class="ltx_text">
<span id="S6.T7.1.4.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.4.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.4.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.4.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.4.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.4.4.1.2.1.2.1" class="ltx_td ltx_align_left">这个直角三角形的斜边长为10。</span></span>
</span></span> <span id="S6.T7.1.4.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.5" class="ltx_tr">
<td id="S6.T7.1.5.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.5.1.1" class="ltx_p ltx_align_top">Translation</p>
</td>
<td id="S6.T7.1.5.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.5.2.1" class="ltx_p ltx_align_top">请将以下诗句翻译成英文：会当凌绝顶、一览众山小</p>
</td>
<td id="S6.T7.1.5.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.5.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.5.3.1.1" class="ltx_text"></span><span id="S6.T7.1.5.3.1.2" class="ltx_text">
<span id="S6.T7.1.5.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.5.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.5.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.5.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 6/10]</span></span></span>
<span id="S6.T7.1.5.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.5.3.1.2.1.2.1" class="ltx_td ltx_align_left">Become the highest peak,</span></span>
<span id="S6.T7.1.5.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.5.3.1.2.1.3.1" class="ltx_td ltx_align_left">towering above others</span></span>
</span></span> <span id="S6.T7.1.5.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.5.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.5.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.5.4.1.1" class="ltx_text"></span><span id="S6.T7.1.5.4.1.2" class="ltx_text">
<span id="S6.T7.1.5.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.5.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.5.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.5.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 8/10]</span></span></span>
<span id="S6.T7.1.5.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.5.4.1.2.1.2.1" class="ltx_td ltx_align_left">I stand on top of mountains, and see</span></span>
<span id="S6.T7.1.5.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.5.4.1.2.1.3.1" class="ltx_td ltx_align_left">all other small mountains below me.</span></span>
</span></span> <span id="S6.T7.1.5.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.6" class="ltx_tr">
<td id="S6.T7.1.6.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.6.1.1" class="ltx_p ltx_align_top">Literature</p>
</td>
<td id="S6.T7.1.6.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.6.2.1" class="ltx_p ltx_align_top">莎士比亚的四大悲剧包括什么？</p>
</td>
<td id="S6.T7.1.6.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.6.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.6.3.1.1" class="ltx_text"></span><span id="S6.T7.1.6.3.1.2" class="ltx_text">
<span id="S6.T7.1.6.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.6.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.6.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.6.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 3/10]</span></span></span>
<span id="S6.T7.1.6.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.6.3.1.2.1.2.1" class="ltx_td ltx_align_left">哈姆雷特、奥林匹角王、李尔福</span></span>
<span id="S6.T7.1.6.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.6.3.1.2.1.3.1" class="ltx_td ltx_align_left">和麦克白是莎士比亚的四大悲剧。</span></span>
</span></span> <span id="S6.T7.1.6.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.6.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.6.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.6.4.1.1" class="ltx_text"></span><span id="S6.T7.1.6.4.1.2" class="ltx_text">
<span id="S6.T7.1.6.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.6.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.6.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.6.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.2.1" class="ltx_td ltx_align_left">莎士比亚的四大悲剧是《哈姆雷</span></span>
<span id="S6.T7.1.6.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.3.1" class="ltx_td ltx_align_left">特》、《奥赛罗斯德》、《李尔</span></span>
<span id="S6.T7.1.6.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.4.1" class="ltx_td ltx_align_left">王》和《麦克白》。</span></span>
</span></span> <span id="S6.T7.1.6.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.7" class="ltx_tr">
<td id="S6.T7.1.7.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.7.1.1" class="ltx_p ltx_align_top">Generation (Letter)</p>
</td>
<td id="S6.T7.1.7.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.7.2.1" class="ltx_p ltx_align_top">请你帮我写一封感谢数学老师的信</p>
</td>
<td id="S6.T7.1.7.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.7.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.7.3.1.1" class="ltx_text"></span><span id="S6.T7.1.7.3.1.2" class="ltx_text">
<span id="S6.T7.1.7.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.7.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.7.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 8/10]</span></span></span>
<span id="S6.T7.1.7.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.2.1" class="ltx_td ltx_align_left">尊敬的老师，我想借此机会向您</span></span>
<span id="S6.T7.1.7.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.3.1" class="ltx_td ltx_align_left">表达我的感激之情。您的教学对</span></span>
<span id="S6.T7.1.7.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.4.1" class="ltx_td ltx_align_left">我非常有好处，让我在学习中更</span></span>
<span id="S6.T7.1.7.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.5.1" class="ltx_td ltx_align_left">加深入地理解了数学知识和技能</span></span>
<span id="S6.T7.1.7.3.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.6.1" class="ltx_td ltx_align_left">。我特别喜欢您用生动有趣的方</span></span>
<span id="S6.T7.1.7.3.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.7.1" class="ltx_td ltx_align_left">式来解释复杂的概念，这使我对</span></span>
<span id="S6.T7.1.7.3.1.2.1.8" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.8.1" class="ltx_td ltx_align_left">于这些内容的了解更深切理解</span></span>
<span id="S6.T7.1.7.3.1.2.1.9" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.9.1" class="ltx_td ltx_align_left">。感谢您为我在学术上取得进步</span></span>
<span id="S6.T7.1.7.3.1.2.1.10" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.10.1" class="ltx_td ltx_align_left">所做的一切努力！</span></span>
</span></span> <span id="S6.T7.1.7.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.7.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.7.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.7.4.1.1" class="ltx_text"></span><span id="S6.T7.1.7.4.1.2" class="ltx_text">
<span id="S6.T7.1.7.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.7.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.7.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 7/10]</span></span></span>
<span id="S6.T7.1.7.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.2.1" class="ltx_td ltx_align_left">尊敬的老师，我想借此机会表达我</span></span>
<span id="S6.T7.1.7.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.3.1" class="ltx_td ltx_align_left">对您的感激之情。您是我最喜欢和</span></span>
<span id="S6.T7.1.7.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.4.1" class="ltx_td ltx_align_left">信任的人之一，因为您让我对学习</span></span>
<span id="S6.T7.1.7.4.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.5.1" class="ltx_td ltx_align_left">新事物充满信心并激发了我求知欲</span></span>
<span id="S6.T7.1.7.4.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.6.1" class="ltx_td ltx_align_left">。我永远不会忘记您对我的影响以</span></span>
<span id="S6.T7.1.7.4.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.7.1" class="ltx_td ltx_align_left">及在课堂上所学到的知识。谢谢！</span></span>
</span></span> <span id="S6.T7.1.7.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.8" class="ltx_tr">
<td id="S6.T7.1.8.1" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.8.1.1" class="ltx_p ltx_align_top">Ethics</p>
</td>
<td id="S6.T7.1.8.2" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.8.2.1" class="ltx_p ltx_align_top">你能教教我怎么在考试中抄别人的卷子吗？</p>
</td>
<td id="S6.T7.1.8.3" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.8.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.8.3.1.1" class="ltx_text"></span><span id="S6.T7.1.8.3.1.2" class="ltx_text">
<span id="S6.T7.1.8.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.8.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.8.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 0/10]</span></span></span>
<span id="S6.T7.1.8.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.2.1" class="ltx_td ltx_align_left">是的，以下是一些方法可以帮助</span></span>
<span id="S6.T7.1.8.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.3.1" class="ltx_td ltx_align_left">你在考试中抄其他人书：1. 在考</span></span>
<span id="S6.T7.1.8.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.4.1" class="ltx_td ltx_align_left">前避免接触到别人所作笔记。这</span></span>
<span id="S6.T7.1.8.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.5.1" class="ltx_td ltx_align_left">可以通过提前准备、阅读和练习</span></span>
<span id="S6.T7.1.8.3.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.6.1" class="ltx_td ltx_align_left">来实现。2. 如果你必须使用他人</span></span>
<span id="S6.T7.1.8.3.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.7.1" class="ltx_td ltx_align_left">笔记作为参考材料或学习工具时</span></span>
<span id="S6.T7.1.8.3.1.2.1.8" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.8.1" class="ltx_td ltx_align_left">，请务必要确保你对该内容有足</span></span>
<span id="S6.T7.1.8.3.1.2.1.9" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.9.1" class="ltx_td ltx_align_left">够的理解并能够独立思考。3. 如</span></span>
<span id="S6.T7.1.8.3.1.2.1.10" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.10.1" class="ltx_td ltx_align_left">果需要抄袭他人笔记，请务必备</span></span>
<span id="S6.T7.1.8.3.1.2.1.11" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.11.1" class="ltx_td ltx_align_left">要仔细审阅他们的笔记以确定哪</span></span>
<span id="S6.T7.1.8.3.1.2.1.12" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.12.1" class="ltx_td ltx_align_left">些信息是你所需了解的内容。然</span></span>
<span id="S6.T7.1.8.3.1.2.1.13" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.13.1" class="ltx_td ltx_align_left">后将这些信息复制为你自己的新</span></span>
<span id="S6.T7.1.8.3.1.2.1.14" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.14.1" class="ltx_td ltx_align_left">纸上。4. 请务求在完成试卷后检</span></span>
<span id="S6.T7.1.8.3.1.2.1.15" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.15.1" class="ltx_td ltx_align_left">查你的答案是否与他人的答案相</span></span>
<span id="S6.T7.1.8.3.1.2.1.16" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.16.1" class="ltx_td ltx_align_left">符。如果你发现任何错误或不一</span></span>
<span id="S6.T7.1.8.3.1.2.1.17" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.17.1" class="ltx_td ltx_align_left">致之处，应立即进行纠正。</span></span>
</span></span> <span id="S6.T7.1.8.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.8.4" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.8.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.8.4.1.1" class="ltx_text"></span><span id="S6.T7.1.8.4.1.2" class="ltx_text">
<span id="S6.T7.1.8.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.8.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.8.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.8.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.8.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.8.4.1.2.1.2.1" class="ltx_td ltx_align_left">抱歉，我不能提供这种不道德</span></span>
<span id="S6.T7.1.8.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.8.4.1.2.1.3.1" class="ltx_td ltx_align_left">和非法的行为的指导。</span></span>
</span></span> <span id="S6.T7.1.8.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">In this technical report, we have presented an approach to enhance the Chinese understanding and generation capabilities of the LLaMA model. Acknowledging the limitations of the original LLaMA’s Chinese vocabulary, we expanded it by incorporating 20K additional Chinese tokens, significantly increasing its encoding efficiency for the Chinese language. Building on the Chinese LLaMA, we employed supervised fine-tuning with instruction data, resulting in the development of the Chinese Alpaca models, which exhibit improved instruction-following capabilities.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p">To evaluate our models effectively, we annotated 160 samples across 10 distinct task types and utilized GPT-4 for evaluation. Our experiments demonstrated that the proposed models significantly outperform the original LLaMA in Chinese understanding and generation tasks, with the 13B version consistently achieving greater improvements compared to the 7B variant.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.1" class="ltx_p">Looking ahead, we plan to explore Reinforcement Learning from Human Feedback (RLHF) or Reinforcement Learning from AI Instructed Feedback (RLAIF) to further align the models’ output with human preferences. Moreover, we intend to adopt more advanced and effective quantization methods, such as GPTQ <cite class="ltx_cite ltx_citemacro_citep">(Frantar et&nbsp;al., <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>, among others. Additionally, we aim to investigate alternative methods to LoRA for more efficient and effective pre-training and fine-tuning of large language models, ultimately enhancing their performance and applicability across various tasks within the Chinese NLP community.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">While this project has successfully enhanced the Chinese understanding and generation capabilities of the LLaMA and Alpaca models, several limitations must be acknowledged:</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">Harmful and unpredictable content: Our results demonstrate that the 13B version has a better ability to reject unethical queries than the 7B version. However, these models may still generate content that is harmful or misaligned with human preferences and values. This issue may arise from biases present in the training data or the models’ inability to discern appropriate outputs in certain contexts.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">Insufficient training: Due to constraints in computing power and data availability, the training of the models may not be sufficient for optimal performance. As a result, there is still room for improvement in the Chinese understanding capabilities of the models.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">Lack of robustness: The models may exhibit brittleness in some situations, producing inconsistent or nonsensical outputs when faced with adversarial inputs or rare language phenomena.</p>
</div>
</li>
<li id="Sx1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i4.p1.1" class="ltx_p">Scalability and efficiency: Although we applied LoRA and 4-bit quantization to make the model more accessible to a broader community, when combined with the original LLaMA, the models’ large size and complexity can lead to difficulties in deployment, especially for users with limited computational resources. This issue may hinder the accessibility and widespread adoption of the models in various applications.</p>
</div>
</li>
</ul>
</div>
<div id="Sx1.p2" class="ltx_para ltx_noindent">
<p id="Sx1.p2.1" class="ltx_p">Future work should address these limitations to further enhance the models’ capabilities, making them more robust, accessible, and effective for a broader range of applications in the Chinese NLP community.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx2.p1" class="ltx_para ltx_noindent">
<p id="Sx2.p1.1" class="ltx_p">The original draft was polished by OpenAI GPT-4 for grammatical corrections and clarity improvements.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu.

</span>
<span class="ltx_bibblock">Revisiting pre-trained models for Chinese natural language
processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: Findings</em>, pp.&nbsp; 657–668, Online, November
2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/2020.findings-emnlp.58" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/2020.findings-emnlp.58</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang.

</span>
<span class="ltx_bibblock">Pre-training with whole word masking for chinese bert.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</em>, 29:3504–3514, 2021.

</span>
<span class="ltx_bibblock">doi: <a href="10.1109/TASLP.2021.3124365" title="" class="ltx_ref ltx_Url">10.1109/TASLP.2021.3124365</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu.

</span>
<span class="ltx_bibblock">Lert: A linguistically-motivated pre-trained language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05344</em>, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Llm. int8 (): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.07339</em>, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pp.&nbsp; 4171–4186,
Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/N19-1423" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/N19-1423</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frantar et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">GPTQ: Accurate post-training compression for generative pretrained
transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.17323</em>, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerganov (2023)</span>
<span class="ltx_bibblock">
Georgi Gerganov.

</span>
<span class="ltx_bibblock">llama.cpp.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/ggerganov/llama.cpp" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ggerganov/llama.cpp</a>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Edward&nbsp;J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
Li, Shean Wang, Lu&nbsp;Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">LoRA: Low-Rank Adaptation of Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2106.09685, June 2021.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2106.09685" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2106.09685</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo &amp; Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">SentencePiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pp.&nbsp; 66–71, Brussels,
Belgium, November 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a href="10.18653/v1/D18-2012" title="" class="ltx_ref ltx_Url">10.18653/v1/D18-2012</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/D18-2012" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/D18-2012</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing chatgpt.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2303.08774, March 2023.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2303.08774" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2303.08774</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright,
Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan
Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human
feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2203.02155, March 2022.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2203.02155" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2203.02155</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer (2020)</span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">Glu variants improve transformer, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tatsu-lab/stanford_alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman
Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand
Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Ł&nbsp;ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;Von Luxburg, S.&nbsp;Bengio, H.&nbsp;Wallach, R.&nbsp;Fergus,
S.&nbsp;Vishwanathan, and R.&nbsp;Garnett (eds.), <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, volume&nbsp;30. Curran Associates, Inc., 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah&nbsp;A.
Smith, Daniel Khashabi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Self-Instruct: Aligning Language Model with Self Generated
Instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2212.10560, December 2022.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2212.10560" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2212.10560</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu (2019)</span>
<span class="ltx_bibblock">
Bright Xu.

</span>
<span class="ltx_bibblock">Nlp chinese corpus: Large scale chinese corpus for nlp, September
2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.3402023" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.3402023</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Ziqing Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, and
Zhigang Chen.

</span>
<span class="ltx_bibblock">CINO: A Chinese minority pre-trained language model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International Conference on
Computational Linguistics</em>, pp.&nbsp; 3937–3949, Gyeongju, Republic of Korea,
October 2022. International Committee on Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.coling-1.346" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.coling-1.346</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Zhewei Yao, Reza Yazdani&nbsp;Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and
Yuxiong He.

</span>
<span class="ltx_bibblock">Zeroquant: Efficient and affordable post-training quantization for
large-scale transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:27168–27183, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Sennrich (2019)</span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root Mean Square Layer Normalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32</em>,
Vancouver, Canada, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/references/pdf?id=S1qBAf6rr" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/references/pdf?id=S1qBAf6rr</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.08176" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.08177" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2304.08177">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.08177" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.08178" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri May  5 12:36:08 2023 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
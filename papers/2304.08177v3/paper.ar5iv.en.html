<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.08177] Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</title><meta property="og:description" content="Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI).
However, the expensive training andâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.08177">

<!--Generated on Fri May  5 12:36:08 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiming Cui 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">ymcui@ieee.org</span> &amp;Ziqing Yang<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">ziqingyang@gmail.com</span> &amp;Xin Yao 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">yaoxin94@foxmail.com</span>
</span><span class="ltx_author_notes">Equal contributions.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Large Language Models (LLMs), such as ChatGPT and GPT-4, have revolutionized natural language processing research and demonstrated potential in Artificial General Intelligence (AGI).
However, the expensive training and deployment of LLMs present challenges to transparent and open academic research.
To address these issues, this project open-sources the Chinese LLaMA and Alpaca large models, emphasizing instruction fine-tuning. We expand the original LLaMAâ€™s Chinese vocabulary by adding 20K Chinese tokens, increasing encoding efficiency and enhancing basic semantic understanding.
By incorporating secondary pre-training using Chinese data and fine-tuning with Chinese instruction data, we substantially improve the modelsâ€™ comprehension and execution of instructions.
Our pilot study serves as a foundation for researchers adapting LLaMA and Alpaca models to other languages.
Resources are made publicly available through GitHub, fostering open research in the Chinese NLP community and beyond.
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>GitHub repository: <a target="_blank" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ymcui/Chinese-LLaMA-Alpaca</a></span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">The field of natural language processing (NLP) has undergone a transformative paradigm shift with the advent of Large Language Models (LLMs).
These models, characterized by their vast size and extensive training data, have demonstrated remarkable capabilities in understanding and generating human-like text.
Unlike pre-trained language models for text understanding, such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et&nbsp;al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>, the GPT series <cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> focuses on text generation abilities, making them a more suitable testbed for creativity than their counterparts.
As the latest LLMs in the GPT family, ChatGPT and GPT-4 have attracted significant attention and emerged as leading examples in this rapidly evolving domain.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>, built on the GPT-3.5 <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> architecture, is an advanced conversational AI model that can engage in context-aware, human-like interactions.
Its success has paved the way for the development of GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>, a more sophisticated LLM, which has demonstrated even greater potential in natural language understanding, generation, and various NLP tasks.
Both models have opened up new avenues of research and applications, fueling interest in exploring the capabilities of Artificial General Intelligence (AGI). These LLMs have not only shown impressive performance in multiple benchmarks but have also exhibited a capacity for few-shot learning and adapting to novel tasks.
As a result, they have significantly contributed to the expansion of NLP research, inspiring researchers and industry professionals alike to explore and leverage their potential in a wide range of applications, from sentiment analysis and machine translation to question-answering systems and beyond.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Despite the remarkable advancements brought about by LLMs, these models come with certain limitations that hinder transparent and open research.
One of the most notable concerns is their proprietary nature, which restricts access to the models and hampers the ability of the broader research community to build upon their successes.
Additionally, the immense computational resources required for training and deploying these models pose a challenge for researchers with limited resources, further exacerbating the accessibility problem.
</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">In response to these limitations, the NLP research community has turned to open-source alternatives to foster greater transparency and collaboration.
Two such examples are LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> and Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et&nbsp;al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>, where the Alpaca model is further finetuned on LLaMA with instruction data.
These open-source LLMs have been designed to facilitate academic research and accelerate progress in the field of NLP.
By open-sourcing these models, the NLP community aims to create an environment that encourages further advancements in model development, fine-tuning, and evaluation, ultimately leading to more robust and capable LLMs that can be utilized in a wide range of applications.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">Although LLaMA and Alpaca have made significant strides in the world of NLP, they possess inherent limitations when it comes to natively supporting Chinese language tasks.
The original models contain only a few hundred Chinese tokens in their vocabularies, significantly hampering their efficiency in encoding and decoding Chinese text<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We will discuss this issue in Section 2.</span></span></span>.
Drawing from our previous work on Chinese BERT series <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite> and Chinese minority-oriented multilingual pre-trained models <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>, in this technical report, we propose Chinese LLaMA and Alpaca with enhanced abilities in Chinese understanding and generation.
We extend the original LLaMAâ€™s vocabulary with an additional 20K Chinese tokens, substantially improving its ability to process and generate Chinese text. To ensure efficient training and deployment of the Chinese LLaMA and Alpaca models, we adopt the Low-Rank Adaptation (LoRA) approach <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite>, which allows us to train and fine-tune the models without incurring excessive computational costs.
Our pilot study in enhancing the Chinese understanding and generation capabilities of LLaMA and Alpaca models can serve as a foundation for researchers seeking to adapt these models to other languages.
By demonstrating the feasibility and effectiveness of our approach, we provide insights and methodologies that can be applied to extend the vocabularies and improve the performance of LLaMA and Alpaca models in different languages.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">In summary, the contributions of this technical report are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We enhance the Chinese encoding and decoding efficiency and improve LLaMAâ€™s Chinese understanding ability by extending the original LLaMAâ€™s vocabulary with an additional 20,000 Chinese tokens.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We adopt the Low-Rank Adaptation (LoRA) approach for the efficient training and deployment of the Chinese LLaMA and Alpaca models, enabling researchers to work with these models without incurring excessive computational costs.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We evaluate the performance of the Chinese Alpaca 7B and 13B models on a variety of natural language understanding (NLU) and natural language generation (NLG) tasks, demonstrating significant improvements over the original LLaMA counterparts in the context of Chinese language tasks.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i4.p1.1" class="ltx_p">We make the resources and findings of our study publicly available, fostering further research and collaboration within the NLP community and encouraging the adaptation of LLaMA and Alpaca models to other languages.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Chinese LLaMA</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> is a decoder-only, foundational large language model based on the transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2017</a>)</cite>. Similar to other transformer-based LLMs, LLaMA comprises an embedding layer, multiple transformer blocks, and an LM head layer. It also incorporates various improvements, such as Pre-normalization <cite class="ltx_cite ltx_citemacro_citep">(Zhang &amp; Sennrich, <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>, SwiGLU activation <cite class="ltx_cite ltx_citemacro_citep">(Shazeer, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>, and Rotary Embeddings <cite class="ltx_cite ltx_citemacro_citep">(Su et&nbsp;al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>. The total number of parameters in LLaMA ranges from 7B to 65B. Experiments demonstrate that LLaMA achieves competitive performance compared to other LLMs, like GPT-3, while maintaining a smaller model size.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">LLaMA has been pre-trained on 1T to 1.4T tokens from publicly available corpora, with the majority of the data in English and only a small fraction in other languages using Latin or Cyrillic scripts.
As a result, LLaMAâ€™s ability to understand and generate Chinese is limited. To address this, we propose pre-training the LLaMA model on Chinese corpora to enhance its fundamental Chinese understanding and generation capabilities.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p">Directly pre-training LLaMA on Chinese corpora faces several challenges. Firstly, there are less than one thousand Chinese characters in the original LLaMA tokenizer vocabulary. Although the LLaMA tokenizer supports all Chinese characters by falling back to bytes, this fallback strategy significantly increases sequence length and slows down the processing efficiency on Chinese texts. Moreover, byte tokens are not exclusively designed for representing Chinese characters, as they are also used to represent other UTF-8 tokens, making it difficult for byte tokens to learn the semantic meaning of Chinese characters.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p">To address these issues, we propose to extend the LLaMA tokenizer with additional Chinese tokens and adapt the model for the new tokenizer <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">To enhance the tokenizerâ€™s support for Chinese text, we first train a Chinese tokenizer with SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo &amp; Richardson, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> on the Chinese corpus, using a vocabulary size of 20,000. We then merge the Chinese tokenizer into the original LLaMA tokenizer by combining their vocabularies. Ultimately, we obtain a merged tokenizer, which we call the Chinese LLaMA tokenizer, with a vocabulary size of 49,953.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i2.p1.4" class="ltx_p">To adapt the model for the Chinese LLaMA tokenizer, we resize the word embeddings and language model head from shape <math id="S2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="V\times H" display="inline"><semantics id="S2.I1.i2.p1.1.m1.1a"><mrow id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">V</mi><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i2.p1.1.m1.1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.1.cmml">Ã—</mo><mi id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><times id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1"></times><ci id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">ğ‘‰</ci><ci id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">V\times H</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.1.m1.1d">italic_V Ã— italic_H</annotation></semantics></math> to <math id="S2.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="V^{\prime}\times H" display="inline"><semantics id="S2.I1.i2.p1.2.m2.1a"><mrow id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml"><msup id="S2.I1.i2.p1.2.m2.1.1.2" xref="S2.I1.i2.p1.2.m2.1.1.2.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.2.2" xref="S2.I1.i2.p1.2.m2.1.1.2.2.cmml">V</mi><mo id="S2.I1.i2.p1.2.m2.1.1.2.3" xref="S2.I1.i2.p1.2.m2.1.1.2.3.cmml">â€²</mo></msup><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i2.p1.2.m2.1.1.1" xref="S2.I1.i2.p1.2.m2.1.1.1.cmml">Ã—</mo><mi id="S2.I1.i2.p1.2.m2.1.1.3" xref="S2.I1.i2.p1.2.m2.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><apply id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1"><times id="S2.I1.i2.p1.2.m2.1.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.1"></times><apply id="S2.I1.i2.p1.2.m2.1.1.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.I1.i2.p1.2.m2.1.1.2.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2">superscript</csymbol><ci id="S2.I1.i2.p1.2.m2.1.1.2.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.2">ğ‘‰</ci><ci id="S2.I1.i2.p1.2.m2.1.1.2.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.3">â€²</ci></apply><ci id="S2.I1.i2.p1.2.m2.1.1.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3">ğ»</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">V^{\prime}\times H</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.2.m2.1d">italic_V start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT Ã— italic_H</annotation></semantics></math>, where <math id="S2.I1.i2.p1.3.m3.2" class="ltx_Math" alttext="V=32,000" display="inline"><semantics id="S2.I1.i2.p1.3.m3.2a"><mrow id="S2.I1.i2.p1.3.m3.2.3" xref="S2.I1.i2.p1.3.m3.2.3.cmml"><mi id="S2.I1.i2.p1.3.m3.2.3.2" xref="S2.I1.i2.p1.3.m3.2.3.2.cmml">V</mi><mo id="S2.I1.i2.p1.3.m3.2.3.1" xref="S2.I1.i2.p1.3.m3.2.3.1.cmml">=</mo><mrow id="S2.I1.i2.p1.3.m3.2.3.3.2" xref="S2.I1.i2.p1.3.m3.2.3.3.1.cmml"><mn id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml">32</mn><mo id="S2.I1.i2.p1.3.m3.2.3.3.2.1" xref="S2.I1.i2.p1.3.m3.2.3.3.1.cmml">,</mo><mn id="S2.I1.i2.p1.3.m3.2.2" xref="S2.I1.i2.p1.3.m3.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.2b"><apply id="S2.I1.i2.p1.3.m3.2.3.cmml" xref="S2.I1.i2.p1.3.m3.2.3"><eq id="S2.I1.i2.p1.3.m3.2.3.1.cmml" xref="S2.I1.i2.p1.3.m3.2.3.1"></eq><ci id="S2.I1.i2.p1.3.m3.2.3.2.cmml" xref="S2.I1.i2.p1.3.m3.2.3.2">ğ‘‰</ci><list id="S2.I1.i2.p1.3.m3.2.3.3.1.cmml" xref="S2.I1.i2.p1.3.m3.2.3.3.2"><cn type="integer" id="S2.I1.i2.p1.3.m3.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1">32</cn><cn type="integer" id="S2.I1.i2.p1.3.m3.2.2.cmml" xref="S2.I1.i2.p1.3.m3.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.2c">V=32,000</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.3.m3.2d">italic_V = 32 , 000</annotation></semantics></math> represents the original vocabulary size, and <math id="S2.I1.i2.p1.4.m4.2" class="ltx_Math" alttext="V^{\prime}=49,953" display="inline"><semantics id="S2.I1.i2.p1.4.m4.2a"><mrow id="S2.I1.i2.p1.4.m4.2.3" xref="S2.I1.i2.p1.4.m4.2.3.cmml"><msup id="S2.I1.i2.p1.4.m4.2.3.2" xref="S2.I1.i2.p1.4.m4.2.3.2.cmml"><mi id="S2.I1.i2.p1.4.m4.2.3.2.2" xref="S2.I1.i2.p1.4.m4.2.3.2.2.cmml">V</mi><mo id="S2.I1.i2.p1.4.m4.2.3.2.3" xref="S2.I1.i2.p1.4.m4.2.3.2.3.cmml">â€²</mo></msup><mo id="S2.I1.i2.p1.4.m4.2.3.1" xref="S2.I1.i2.p1.4.m4.2.3.1.cmml">=</mo><mrow id="S2.I1.i2.p1.4.m4.2.3.3.2" xref="S2.I1.i2.p1.4.m4.2.3.3.1.cmml"><mn id="S2.I1.i2.p1.4.m4.1.1" xref="S2.I1.i2.p1.4.m4.1.1.cmml">49</mn><mo id="S2.I1.i2.p1.4.m4.2.3.3.2.1" xref="S2.I1.i2.p1.4.m4.2.3.3.1.cmml">,</mo><mn id="S2.I1.i2.p1.4.m4.2.2" xref="S2.I1.i2.p1.4.m4.2.2.cmml">953</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.4.m4.2b"><apply id="S2.I1.i2.p1.4.m4.2.3.cmml" xref="S2.I1.i2.p1.4.m4.2.3"><eq id="S2.I1.i2.p1.4.m4.2.3.1.cmml" xref="S2.I1.i2.p1.4.m4.2.3.1"></eq><apply id="S2.I1.i2.p1.4.m4.2.3.2.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2"><csymbol cd="ambiguous" id="S2.I1.i2.p1.4.m4.2.3.2.1.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2">superscript</csymbol><ci id="S2.I1.i2.p1.4.m4.2.3.2.2.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2.2">ğ‘‰</ci><ci id="S2.I1.i2.p1.4.m4.2.3.2.3.cmml" xref="S2.I1.i2.p1.4.m4.2.3.2.3">â€²</ci></apply><list id="S2.I1.i2.p1.4.m4.2.3.3.1.cmml" xref="S2.I1.i2.p1.4.m4.2.3.3.2"><cn type="integer" id="S2.I1.i2.p1.4.m4.1.1.cmml" xref="S2.I1.i2.p1.4.m4.1.1">49</cn><cn type="integer" id="S2.I1.i2.p1.4.m4.2.2.cmml" xref="S2.I1.i2.p1.4.m4.2.2">953</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.4.m4.2c">V^{\prime}=49,953</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.4.m4.2d">italic_V start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = 49 , 953</annotation></semantics></math> is the vocabulary size of the Chinese LLaMA tokenizer. The new rows are appended to the end of the original embedding matrices, ensuring that the embeddings of the tokens in the original vocabulary remain unaffected.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p">Our preliminary experiments show that the number of tokens generated by the Chinese-LLaMA tokenizer is roughly half of those generated by the original LLaMA tokenizer. Table <a href="#S2.T1" title="Table 1 â€£ 2 Chinese LLaMA â€£ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example comparison between the original LLaMA tokenizer and our Chinese LLaMA tokenizer. As we can see, using the Chinese LLaMA tokenizer significantly reduces the encoding length compared to the original. Given a fixed context length, the model can accommodate about twice as much information, and the generation speed is two times faster compared to the original LLaMA tokenizer. This demonstrates the effectiveness of our proposed approach in enhancing the Chinese understanding and generation capabilities of the LLaMA model.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p">After completing the aforementioned adaptation steps, we pre-train the Chinese-LLaMA model using the Chinese-LLaMA tokenizer on the standard Casual Language Modeling (CLM) task. Given an input token sequence <math id="S2.p6.1.m1.4" class="ltx_Math" alttext="\bm{x}=(x_{0},x_{1},x_{2},\ldots)" display="inline"><semantics id="S2.p6.1.m1.4a"><mrow id="S2.p6.1.m1.4.4" xref="S2.p6.1.m1.4.4.cmml"><mi id="S2.p6.1.m1.4.4.5" xref="S2.p6.1.m1.4.4.5.cmml">ğ’™</mi><mo id="S2.p6.1.m1.4.4.4" xref="S2.p6.1.m1.4.4.4.cmml">=</mo><mrow id="S2.p6.1.m1.4.4.3.3" xref="S2.p6.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S2.p6.1.m1.4.4.3.3.4" xref="S2.p6.1.m1.4.4.3.4.cmml">(</mo><msub id="S2.p6.1.m1.2.2.1.1.1" xref="S2.p6.1.m1.2.2.1.1.1.cmml"><mi id="S2.p6.1.m1.2.2.1.1.1.2" xref="S2.p6.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S2.p6.1.m1.2.2.1.1.1.3" xref="S2.p6.1.m1.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S2.p6.1.m1.4.4.3.3.5" xref="S2.p6.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.p6.1.m1.3.3.2.2.2" xref="S2.p6.1.m1.3.3.2.2.2.cmml"><mi id="S2.p6.1.m1.3.3.2.2.2.2" xref="S2.p6.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S2.p6.1.m1.3.3.2.2.2.3" xref="S2.p6.1.m1.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S2.p6.1.m1.4.4.3.3.6" xref="S2.p6.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.p6.1.m1.4.4.3.3.3" xref="S2.p6.1.m1.4.4.3.3.3.cmml"><mi id="S2.p6.1.m1.4.4.3.3.3.2" xref="S2.p6.1.m1.4.4.3.3.3.2.cmml">x</mi><mn id="S2.p6.1.m1.4.4.3.3.3.3" xref="S2.p6.1.m1.4.4.3.3.3.3.cmml">2</mn></msub><mo id="S2.p6.1.m1.4.4.3.3.7" xref="S2.p6.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.p6.1.m1.1.1" xref="S2.p6.1.m1.1.1.cmml">â€¦</mi><mo stretchy="false" id="S2.p6.1.m1.4.4.3.3.8" xref="S2.p6.1.m1.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.4b"><apply id="S2.p6.1.m1.4.4.cmml" xref="S2.p6.1.m1.4.4"><eq id="S2.p6.1.m1.4.4.4.cmml" xref="S2.p6.1.m1.4.4.4"></eq><ci id="S2.p6.1.m1.4.4.5.cmml" xref="S2.p6.1.m1.4.4.5">ğ’™</ci><vector id="S2.p6.1.m1.4.4.3.4.cmml" xref="S2.p6.1.m1.4.4.3.3"><apply id="S2.p6.1.m1.2.2.1.1.1.cmml" xref="S2.p6.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p6.1.m1.2.2.1.1.1.1.cmml" xref="S2.p6.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.p6.1.m1.2.2.1.1.1.2.cmml" xref="S2.p6.1.m1.2.2.1.1.1.2">ğ‘¥</ci><cn type="integer" id="S2.p6.1.m1.2.2.1.1.1.3.cmml" xref="S2.p6.1.m1.2.2.1.1.1.3">0</cn></apply><apply id="S2.p6.1.m1.3.3.2.2.2.cmml" xref="S2.p6.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p6.1.m1.3.3.2.2.2.1.cmml" xref="S2.p6.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.p6.1.m1.3.3.2.2.2.2.cmml" xref="S2.p6.1.m1.3.3.2.2.2.2">ğ‘¥</ci><cn type="integer" id="S2.p6.1.m1.3.3.2.2.2.3.cmml" xref="S2.p6.1.m1.3.3.2.2.2.3">1</cn></apply><apply id="S2.p6.1.m1.4.4.3.3.3.cmml" xref="S2.p6.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.p6.1.m1.4.4.3.3.3.1.cmml" xref="S2.p6.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.p6.1.m1.4.4.3.3.3.2.cmml" xref="S2.p6.1.m1.4.4.3.3.3.2">ğ‘¥</ci><cn type="integer" id="S2.p6.1.m1.4.4.3.3.3.3.cmml" xref="S2.p6.1.m1.4.4.3.3.3.3">2</cn></apply><ci id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1">â€¦</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.4c">\bm{x}=(x_{0},x_{1},x_{2},\ldots)</annotation><annotation encoding="application/x-llamapun" id="S2.p6.1.m1.4d">bold_italic_x = ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ )</annotation></semantics></math>, the model is trained to predict the next token in an autoregressive manner. The objective is to minimize the following negative log likelihood:</p>
</div>
<div id="S2.p7" class="ltx_para ltx_noindent">
<table id="Sx2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\textrm{CLM}}=-\sum_{i}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)" display="inline"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><msub id="S2.E1.m1.3.3.3" xref="S2.E1.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.3.3.3.2" xref="S2.E1.m1.3.3.3.2.cmml">â„’</mi><mtext id="S2.E1.m1.3.3.3.3" xref="S2.E1.m1.3.3.3.3a.cmml">CLM</mtext></msub><mo id="S2.E1.m1.3.3.2" xref="S2.E1.m1.3.3.2.cmml">=</mo><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.cmml"><mo id="S2.E1.m1.3.3.1a" xref="S2.E1.m1.3.3.1.cmml">âˆ’</mo><mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml"><mstyle displaystyle="true" id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.2.cmml"><munder id="S2.E1.m1.3.3.1.1.2a" xref="S2.E1.m1.3.3.1.1.2.cmml"><mo movablelimits="false" id="S2.E1.m1.3.3.1.1.2.2" xref="S2.E1.m1.3.3.1.1.2.2.cmml">âˆ‘</mo><mi id="S2.E1.m1.3.3.1.1.2.3" xref="S2.E1.m1.3.3.1.1.2.3.cmml">i</mi></munder></mstyle><mrow id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.3.1" xref="S2.E1.m1.3.3.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E1.m1.3.3.1.1.1.3a" xref="S2.E1.m1.3.3.1.1.1.3.cmml">â¡</mo><mi id="S2.E1.m1.3.3.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.3.2.cmml">p</mi></mrow><mo id="S2.E1.m1.3.3.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.2.cmml" lspace="0px" rspace="0px"></mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.5" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.5.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.2.cmml">x</mi><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.5.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S2.E1.m1.3.3.1.1.1.1.1.1.4" xref="S2.E1.m1.3.3.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.4" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.5" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">â€¦</mi><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.6" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml">âˆ’</mo><mn id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.7" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml">;</mo><mi mathvariant="normal" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">Î˜</mi></mrow></mrow><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3"><eq id="S2.E1.m1.3.3.2.cmml" xref="S2.E1.m1.3.3.2"></eq><apply id="S2.E1.m1.3.3.3.cmml" xref="S2.E1.m1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3">subscript</csymbol><ci id="S2.E1.m1.3.3.3.2.cmml" xref="S2.E1.m1.3.3.3.2">â„’</ci><ci id="S2.E1.m1.3.3.3.3a.cmml" xref="S2.E1.m1.3.3.3.3"><mtext mathsize="70%" id="S2.E1.m1.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3">CLM</mtext></ci></apply><apply id="S2.E1.m1.3.3.1.cmml" xref="S2.E1.m1.3.3.1"><minus id="S2.E1.m1.3.3.1.2.cmml" xref="S2.E1.m1.3.3.1"></minus><apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1"><apply id="S2.E1.m1.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.2.1.cmml" xref="S2.E1.m1.3.3.1.1.2">subscript</csymbol><sum id="S2.E1.m1.3.3.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.2.2"></sum><ci id="S2.E1.m1.3.3.1.1.2.3.cmml" xref="S2.E1.m1.3.3.1.1.2.3">ğ‘–</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.2"></times><apply id="S2.E1.m1.3.3.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.3"><log id="S2.E1.m1.3.3.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.3.1"></log><ci id="S2.E1.m1.3.3.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.3.2">ğ‘</ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.4">conditional</csymbol><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.5.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.5.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.5.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.2">ğ‘¥</ci><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.5.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.5.3">ğ‘–</ci></apply><list id="S2.E1.m1.3.3.1.1.1.1.1.1.3.4.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3"><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.2">ğ‘¥</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">â€¦</ci><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.2">ğ‘¥</ci><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3"><minus id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.2">ğ‘–</ci><cn type="integer" id="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">Î˜</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\displaystyle\mathcal{L}_{\textrm{CLM}}=-\sum_{i}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">caligraphic_L start_POSTSUBSCRIPT CLM end_POSTSUBSCRIPT = - âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; roman_Î˜ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p8" class="ltx_para ltx_noindent">
<p id="S2.p8.3" class="ltx_p">Here, <math id="S2.p8.1.m1.1" class="ltx_Math" alttext="\Theta" display="inline"><semantics id="S2.p8.1.m1.1a"><mi mathvariant="normal" id="S2.p8.1.m1.1.1" xref="S2.p8.1.m1.1.1.cmml">Î˜</mi><annotation-xml encoding="MathML-Content" id="S2.p8.1.m1.1b"><ci id="S2.p8.1.m1.1.1.cmml" xref="S2.p8.1.m1.1.1">Î˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.1.m1.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S2.p8.1.m1.1d">roman_Î˜</annotation></semantics></math> represents the model parameters, <math id="S2.p8.2.m2.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S2.p8.2.m2.1a"><msub id="S2.p8.2.m2.1.1" xref="S2.p8.2.m2.1.1.cmml"><mi id="S2.p8.2.m2.1.1.2" xref="S2.p8.2.m2.1.1.2.cmml">x</mi><mi id="S2.p8.2.m2.1.1.3" xref="S2.p8.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p8.2.m2.1b"><apply id="S2.p8.2.m2.1.1.cmml" xref="S2.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p8.2.m2.1.1.1.cmml" xref="S2.p8.2.m2.1.1">subscript</csymbol><ci id="S2.p8.2.m2.1.1.2.cmml" xref="S2.p8.2.m2.1.1.2">ğ‘¥</ci><ci id="S2.p8.2.m2.1.1.3.cmml" xref="S2.p8.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.2.m2.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.p8.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the token to be predicted, and <math id="S2.p8.3.m3.4" class="ltx_Math" alttext="x_{0},x_{1},\ldots,x_{i-1}" display="inline"><semantics id="S2.p8.3.m3.4a"><mrow id="S2.p8.3.m3.4.4.3" xref="S2.p8.3.m3.4.4.4.cmml"><msub id="S2.p8.3.m3.2.2.1.1" xref="S2.p8.3.m3.2.2.1.1.cmml"><mi id="S2.p8.3.m3.2.2.1.1.2" xref="S2.p8.3.m3.2.2.1.1.2.cmml">x</mi><mn id="S2.p8.3.m3.2.2.1.1.3" xref="S2.p8.3.m3.2.2.1.1.3.cmml">0</mn></msub><mo id="S2.p8.3.m3.4.4.3.4" xref="S2.p8.3.m3.4.4.4.cmml">,</mo><msub id="S2.p8.3.m3.3.3.2.2" xref="S2.p8.3.m3.3.3.2.2.cmml"><mi id="S2.p8.3.m3.3.3.2.2.2" xref="S2.p8.3.m3.3.3.2.2.2.cmml">x</mi><mn id="S2.p8.3.m3.3.3.2.2.3" xref="S2.p8.3.m3.3.3.2.2.3.cmml">1</mn></msub><mo id="S2.p8.3.m3.4.4.3.5" xref="S2.p8.3.m3.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S2.p8.3.m3.1.1" xref="S2.p8.3.m3.1.1.cmml">â€¦</mi><mo id="S2.p8.3.m3.4.4.3.6" xref="S2.p8.3.m3.4.4.4.cmml">,</mo><msub id="S2.p8.3.m3.4.4.3.3" xref="S2.p8.3.m3.4.4.3.3.cmml"><mi id="S2.p8.3.m3.4.4.3.3.2" xref="S2.p8.3.m3.4.4.3.3.2.cmml">x</mi><mrow id="S2.p8.3.m3.4.4.3.3.3" xref="S2.p8.3.m3.4.4.3.3.3.cmml"><mi id="S2.p8.3.m3.4.4.3.3.3.2" xref="S2.p8.3.m3.4.4.3.3.3.2.cmml">i</mi><mo id="S2.p8.3.m3.4.4.3.3.3.1" xref="S2.p8.3.m3.4.4.3.3.3.1.cmml">âˆ’</mo><mn id="S2.p8.3.m3.4.4.3.3.3.3" xref="S2.p8.3.m3.4.4.3.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p8.3.m3.4b"><list id="S2.p8.3.m3.4.4.4.cmml" xref="S2.p8.3.m3.4.4.3"><apply id="S2.p8.3.m3.2.2.1.1.cmml" xref="S2.p8.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S2.p8.3.m3.2.2.1.1.1.cmml" xref="S2.p8.3.m3.2.2.1.1">subscript</csymbol><ci id="S2.p8.3.m3.2.2.1.1.2.cmml" xref="S2.p8.3.m3.2.2.1.1.2">ğ‘¥</ci><cn type="integer" id="S2.p8.3.m3.2.2.1.1.3.cmml" xref="S2.p8.3.m3.2.2.1.1.3">0</cn></apply><apply id="S2.p8.3.m3.3.3.2.2.cmml" xref="S2.p8.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S2.p8.3.m3.3.3.2.2.1.cmml" xref="S2.p8.3.m3.3.3.2.2">subscript</csymbol><ci id="S2.p8.3.m3.3.3.2.2.2.cmml" xref="S2.p8.3.m3.3.3.2.2.2">ğ‘¥</ci><cn type="integer" id="S2.p8.3.m3.3.3.2.2.3.cmml" xref="S2.p8.3.m3.3.3.2.2.3">1</cn></apply><ci id="S2.p8.3.m3.1.1.cmml" xref="S2.p8.3.m3.1.1">â€¦</ci><apply id="S2.p8.3.m3.4.4.3.3.cmml" xref="S2.p8.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S2.p8.3.m3.4.4.3.3.1.cmml" xref="S2.p8.3.m3.4.4.3.3">subscript</csymbol><ci id="S2.p8.3.m3.4.4.3.3.2.cmml" xref="S2.p8.3.m3.4.4.3.3.2">ğ‘¥</ci><apply id="S2.p8.3.m3.4.4.3.3.3.cmml" xref="S2.p8.3.m3.4.4.3.3.3"><minus id="S2.p8.3.m3.4.4.3.3.3.1.cmml" xref="S2.p8.3.m3.4.4.3.3.3.1"></minus><ci id="S2.p8.3.m3.4.4.3.3.3.2.cmml" xref="S2.p8.3.m3.4.4.3.3.3.2">ğ‘–</ci><cn type="integer" id="S2.p8.3.m3.4.4.3.3.3.3.cmml" xref="S2.p8.3.m3.4.4.3.3.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.p8.3.m3.4c">x_{0},x_{1},\ldots,x_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S2.p8.3.m3.4d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> constitute the context.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span> Tokenizer comparisons between original LLaMA and Chinese LLaMA.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.2.1" class="ltx_text ltx_font_bold">Length</span></td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S2.T1.1.1.3.1" class="ltx_text ltx_font_bold">Content</span></td>
</tr>
<tr id="S2.T1.1.2" class="ltx_tr">
<td id="S2.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.1.2.1.1" class="ltx_text ltx_font_bold">Original Sentence</span></td>
<td id="S2.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">28</td>
<td id="S2.T1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦ã€å¿ƒç†å­¦ã€å“²å­¦ç­‰å­¦ç§‘èåˆçš„äº¤å‰å­¦ç§‘ã€‚</td>
</tr>
<tr id="S2.T1.1.3" class="ltx_tr">
<td id="S2.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.1.3.1.1" class="ltx_text ltx_font_bold">Original Tokenizer</span></td>
<td id="S2.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.3.2.1" class="ltx_text">35</span></td>
<td id="S2.T1.1.3.3" class="ltx_td ltx_align_left ltx_border_t">
<span id="S2.T1.1.3.3.1" class="ltx_text"></span><span id="S2.T1.1.3.3.2" class="ltx_text">
<span id="S2.T1.1.3.3.2.1" class="ltx_tabular ltx_align_top">
<span id="S2.T1.1.3.3.2.1.1" class="ltx_tr">
<span id="S2.T1.1.3.3.2.1.1.1" class="ltx_td ltx_align_left">â€˜_â€™, â€˜äººâ€™, â€˜å·¥â€™, â€˜æ™ºâ€™, â€˜èƒ½â€™, â€˜æ˜¯â€™, â€˜è®¡â€™, â€˜ç®—â€™, â€˜æœºâ€™, â€˜ç§‘â€™, â€˜å­¦â€™, â€˜ã€â€™, â€˜å¿ƒâ€™, â€˜ç†â€™,</span></span>
<span id="S2.T1.1.3.3.2.1.2" class="ltx_tr">
<span id="S2.T1.1.3.3.2.1.2.1" class="ltx_td ltx_align_left">â€˜å­¦â€™, â€˜ã€â€™, â€˜0xE5â€™, â€˜0x93â€™, â€˜0xB2â€™, â€˜å­¦â€™, â€˜ç­‰â€™, â€˜å­¦â€™, â€˜ç§‘â€™, â€˜0xE8â€™, â€˜0x9Eâ€™,</span></span>
<span id="S2.T1.1.3.3.2.1.3" class="ltx_tr">
<span id="S2.T1.1.3.3.2.1.3.1" class="ltx_td ltx_align_left">â€˜0x8Dâ€™, â€˜åˆâ€™, â€˜çš„â€™, â€˜äº¤â€™, â€˜0xE5â€™, â€˜0x8Fâ€™, â€˜0x89â€™, â€˜å­¦â€™, â€˜ç§‘â€™, â€˜ã€‚â€™</span></span>
</span></span> <span id="S2.T1.1.3.3.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S2.T1.1.4" class="ltx_tr">
<td id="S2.T1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="S2.T1.1.4.1.1" class="ltx_text ltx_font_bold">Chinese Tokenizer</span></td>
<td id="S2.T1.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S2.T1.1.4.2.1" class="ltx_text">16</span></td>
<td id="S2.T1.1.4.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S2.T1.1.4.3.1" class="ltx_text"></span><span id="S2.T1.1.4.3.2" class="ltx_text">
<span id="S2.T1.1.4.3.2.1" class="ltx_tabular ltx_align_top">
<span id="S2.T1.1.4.3.2.1.1" class="ltx_tr">
<span id="S2.T1.1.4.3.2.1.1.1" class="ltx_td ltx_align_left">â€˜_â€™, â€˜äººå·¥æ™ºèƒ½â€™, â€˜æ˜¯â€™, â€˜è®¡ç®—æœºâ€™, â€˜ç§‘å­¦â€™, â€˜ã€â€™, â€˜å¿ƒç†å­¦â€™, â€˜ã€â€™, â€˜å“²å­¦â€™, â€˜ç­‰â€™,</span></span>
<span id="S2.T1.1.4.3.2.1.2" class="ltx_tr">
<span id="S2.T1.1.4.3.2.1.2.1" class="ltx_td ltx_align_left">â€˜å­¦ç§‘â€™, â€˜èåˆâ€™, â€˜çš„â€™, â€˜äº¤å‰â€™, â€˜å­¦ç§‘â€™, â€˜ã€‚â€™</span></span>
</span></span> <span id="S2.T1.1.4.3.3" class="ltx_text"></span>
</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Chinese Alpaca</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">After obtaining the pre-trained Chinese LLaMA model, we follow the approach used in Stanford Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et&nbsp;al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> to apply self-instructed fine-tuning to train the instruction-following model. Each example consists of an instruction and an output. We input the instruction into the model and prompt the model to generate the output auto-regressively. This process is similar to the casual language modeling task. We adopt the following prompt template from Stanford Alpaca for self-instructed fine-tuning, which is also utilized during inference:</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<blockquote id="S3.p2.1" class="ltx_quote">
<p id="S3.p2.1.1" class="ltx_p"><span id="S3.p2.1.1.1" class="ltx_text ltx_font_italic">Below is an instruction that describes a task. Write a response that appropriately completes the request.

<br class="ltx_break">
<br class="ltx_break">### Instruction:

<br class="ltx_break"></span>{<span id="S3.p2.1.1.2" class="ltx_text ltx_font_italic">instruction</span>}<span id="S3.p2.1.1.3" class="ltx_text ltx_font_italic">

<br class="ltx_break">
<br class="ltx_break">### Response: </span>{<span id="S3.p2.1.1.4" class="ltx_text ltx_font_italic">output</span>}<span id="S3.p2.1.1.5" class="ltx_text ltx_font_italic"></span></p>
</blockquote>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">The loss is computed only on the <em id="S3.p3.1.1" class="ltx_emph ltx_font_italic">output</em> part of the full text and can be expressed as:</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<table id="Sx2.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\textrm{SFT}}=-\sum_{i\in\textit{\{output\}}}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)" display="inline"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><msub id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.3.2" xref="S3.E2.m1.3.3.3.2.cmml">â„’</mi><mtext id="S3.E2.m1.3.3.3.3" xref="S3.E2.m1.3.3.3.3a.cmml">SFT</mtext></msub><mo id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.cmml"><mo id="S3.E2.m1.3.3.1a" xref="S3.E2.m1.3.3.1.cmml">âˆ’</mo><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><munder id="S3.E2.m1.3.3.1.1.2a" xref="S3.E2.m1.3.3.1.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.3.3.1.1.2.2" xref="S3.E2.m1.3.3.1.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.3.3.1.1.2.3" xref="S3.E2.m1.3.3.1.1.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.2.3.2" xref="S3.E2.m1.3.3.1.1.2.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.2.3.1" xref="S3.E2.m1.3.3.1.1.2.3.1.cmml">âˆˆ</mo><mrow id="S3.E2.m1.3.3.1.1.2.3.3" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml"><mtext id="S3.E2.m1.3.3.1.1.2.3.3a" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml">{</mtext><mtext id="S3.E2.m1.3.3.1.1.2.3.3b" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml">ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡</mtext><mtext id="S3.E2.m1.3.3.1.1.2.3.3c" xref="S3.E2.m1.3.3.1.1.2.3.3d.cmml">}</mtext></mrow></mrow></munder></mstyle><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.3.1" xref="S3.E2.m1.3.3.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.3.3.1.1.1.3a" xref="S3.E2.m1.3.3.1.1.1.3.cmml">â¡</mo><mi id="S3.E2.m1.3.3.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.3.2.cmml">p</mi></mrow><mo id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.2.cmml" lspace="0px" rspace="0px"></mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.5" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.5.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.2.cmml">x</mi><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.5.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S3.E2.m1.3.3.1.1.1.1.1.1.4" xref="S3.E2.m1.3.3.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml"><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.4" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.5" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">â€¦</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.6" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml">âˆ’</mo><mn id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.7" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml">;</mo><mi mathvariant="normal" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">Î˜</mi></mrow></mrow><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><eq id="S3.E2.m1.3.3.2.cmml" xref="S3.E2.m1.3.3.2"></eq><apply id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.3.2">â„’</ci><ci id="S3.E2.m1.3.3.3.3a.cmml" xref="S3.E2.m1.3.3.3.3"><mtext mathsize="70%" id="S3.E2.m1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3">SFT</mtext></ci></apply><apply id="S3.E2.m1.3.3.1.cmml" xref="S3.E2.m1.3.3.1"><minus id="S3.E2.m1.3.3.1.2.cmml" xref="S3.E2.m1.3.3.1"></minus><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1"><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3"><in id="S3.E2.m1.3.3.1.1.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.2.3.1"></in><ci id="S3.E2.m1.3.3.1.1.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.2.3.2">ğ‘–</ci><ci id="S3.E2.m1.3.3.1.1.2.3.3d.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3"><mrow id="S3.E2.m1.3.3.1.1.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3"><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.2.3.3a.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3">{</mtext><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.2.3.3b.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3">ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡</mtext><mtext mathsize="70%" id="S3.E2.m1.3.3.1.1.2.3.3c.cmml" xref="S3.E2.m1.3.3.1.1.2.3.3">}</mtext></mrow></ci></apply></apply><apply id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2"></times><apply id="S3.E2.m1.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3"><log id="S3.E2.m1.3.3.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.3.1"></log><ci id="S3.E2.m1.3.3.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.3.2">ğ‘</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.4">conditional</csymbol><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.5.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.5.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.2">ğ‘¥</ci><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.5.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.5.3">ğ‘–</ci></apply><list id="S3.E2.m1.3.3.1.1.1.1.1.1.3.4.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3"><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.2">ğ‘¥</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.2">ğ‘¥</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">â€¦</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.2">ğ‘¥</ci><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3"><minus id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.2">ğ‘–</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">Î˜</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\displaystyle\mathcal{L}_{\textrm{SFT}}=-\sum_{i\in\textit{\{output\}}}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};\Theta)</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.3d">caligraphic_L start_POSTSUBSCRIPT SFT end_POSTSUBSCRIPT = - âˆ‘ start_POSTSUBSCRIPT italic_i âˆˆ { output } end_POSTSUBSCRIPT roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; roman_Î˜ )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p">Here, <math id="S3.p5.1.m1.3" class="ltx_Math" alttext="\bm{x}=(x_{0},x_{1},\ldots)" display="inline"><semantics id="S3.p5.1.m1.3a"><mrow id="S3.p5.1.m1.3.3" xref="S3.p5.1.m1.3.3.cmml"><mi id="S3.p5.1.m1.3.3.4" xref="S3.p5.1.m1.3.3.4.cmml">ğ’™</mi><mo id="S3.p5.1.m1.3.3.3" xref="S3.p5.1.m1.3.3.3.cmml">=</mo><mrow id="S3.p5.1.m1.3.3.2.2" xref="S3.p5.1.m1.3.3.2.3.cmml"><mo stretchy="false" id="S3.p5.1.m1.3.3.2.2.3" xref="S3.p5.1.m1.3.3.2.3.cmml">(</mo><msub id="S3.p5.1.m1.2.2.1.1.1" xref="S3.p5.1.m1.2.2.1.1.1.cmml"><mi id="S3.p5.1.m1.2.2.1.1.1.2" xref="S3.p5.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S3.p5.1.m1.2.2.1.1.1.3" xref="S3.p5.1.m1.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S3.p5.1.m1.3.3.2.2.4" xref="S3.p5.1.m1.3.3.2.3.cmml">,</mo><msub id="S3.p5.1.m1.3.3.2.2.2" xref="S3.p5.1.m1.3.3.2.2.2.cmml"><mi id="S3.p5.1.m1.3.3.2.2.2.2" xref="S3.p5.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S3.p5.1.m1.3.3.2.2.2.3" xref="S3.p5.1.m1.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S3.p5.1.m1.3.3.2.2.5" xref="S3.p5.1.m1.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">â€¦</mi><mo stretchy="false" id="S3.p5.1.m1.3.3.2.2.6" xref="S3.p5.1.m1.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.3b"><apply id="S3.p5.1.m1.3.3.cmml" xref="S3.p5.1.m1.3.3"><eq id="S3.p5.1.m1.3.3.3.cmml" xref="S3.p5.1.m1.3.3.3"></eq><ci id="S3.p5.1.m1.3.3.4.cmml" xref="S3.p5.1.m1.3.3.4">ğ’™</ci><vector id="S3.p5.1.m1.3.3.2.3.cmml" xref="S3.p5.1.m1.3.3.2.2"><apply id="S3.p5.1.m1.2.2.1.1.1.cmml" xref="S3.p5.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p5.1.m1.2.2.1.1.1.1.cmml" xref="S3.p5.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.p5.1.m1.2.2.1.1.1.2.cmml" xref="S3.p5.1.m1.2.2.1.1.1.2">ğ‘¥</ci><cn type="integer" id="S3.p5.1.m1.2.2.1.1.1.3.cmml" xref="S3.p5.1.m1.2.2.1.1.1.3">0</cn></apply><apply id="S3.p5.1.m1.3.3.2.2.2.cmml" xref="S3.p5.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p5.1.m1.3.3.2.2.2.1.cmml" xref="S3.p5.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.p5.1.m1.3.3.2.2.2.2.cmml" xref="S3.p5.1.m1.3.3.2.2.2.2">ğ‘¥</ci><cn type="integer" id="S3.p5.1.m1.3.3.2.2.2.3.cmml" xref="S3.p5.1.m1.3.3.2.2.2.3">1</cn></apply><ci id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">â€¦</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.3c">\bm{x}=(x_{0},x_{1},\ldots)</annotation><annotation encoding="application/x-llamapun" id="S3.p5.1.m1.3d">bold_italic_x = ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â€¦ )</annotation></semantics></math> represents the tokenized self-instruct example wrapped in the prompt template mentioned earlier.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p">A key difference between our approach and Stanford Alpaca is that we exclusively use the prompt template designed for examples without an <em id="S3.p6.1.1" class="ltx_emph ltx_font_italic">input</em> field, whereas Stanford Alpaca employs two templates for examples with and without an <em id="S3.p6.1.2" class="ltx_emph ltx_font_italic">input</em> field separately.
If the example contains a non-empty <em id="S3.p6.1.3" class="ltx_emph ltx_font_italic">input</em> field, we concatenate the <em id="S3.p6.1.4" class="ltx_emph ltx_font_italic">instruction</em> and <em id="S3.p6.1.5" class="ltx_emph ltx_font_italic">input</em> with an <em id="S3.p6.1.6" class="ltx_emph ltx_font_italic">â€œ\nâ€</em> to form the new instruction. Note that there is an additional padding token for the Alpaca model, resulting in a vocabulary size of 49,954.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Parameter Efficient Fine-Tuning with LoRA</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.4" class="ltx_p">Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> is a parameter-efficient training method that maintains the pre-trained model weights while introducing trainable rank decomposition matrices. This approach significantly reduces the number of trainable parameters. The general formulation of LoRA is represented in the following equation, where <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_r</annotation></semantics></math> is the pre-determined rank, <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">italic_d</annotation></semantics></math> is the hidden size, and <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S4.p1.3.m3.1a"><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.p1.3.m3.1d">italic_A</annotation></semantics></math> and <math id="S4.p1.4.m4.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.p1.4.m4.1a"><mi id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><ci id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">B</annotation><annotation encoding="application/x-llamapun" id="S4.p1.4.m4.1d">italic_B</annotation></semantics></math> are the decomposed trainable matrices:</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.2" class="ltx_Math" alttext="h=W_{0}x+\Delta Wx=W_{0}x+BAx,~{}~{}B\in\mathbb{R}^{d\times r},A\in\mathbb{R}^{r\times d}" display="block"><semantics id="S4.E3.m1.2a"><mrow id="S4.E3.m1.2.2.2" xref="S4.E3.m1.2.2.3.cmml"><mrow id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml"><mi id="S4.E3.m1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.2.cmml">h</mi><mo id="S4.E3.m1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.3.cmml">=</mo><mrow id="S4.E3.m1.1.1.1.1.4" xref="S4.E3.m1.1.1.1.1.4.cmml"><mrow id="S4.E3.m1.1.1.1.1.4.2" xref="S4.E3.m1.1.1.1.1.4.2.cmml"><msub id="S4.E3.m1.1.1.1.1.4.2.2" xref="S4.E3.m1.1.1.1.1.4.2.2.cmml"><mi id="S4.E3.m1.1.1.1.1.4.2.2.2" xref="S4.E3.m1.1.1.1.1.4.2.2.2.cmml">W</mi><mn id="S4.E3.m1.1.1.1.1.4.2.2.3" xref="S4.E3.m1.1.1.1.1.4.2.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.1.1.1.1.4.2.1" xref="S4.E3.m1.1.1.1.1.4.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.4.2.3" xref="S4.E3.m1.1.1.1.1.4.2.3.cmml">x</mi></mrow><mo id="S4.E3.m1.1.1.1.1.4.1" xref="S4.E3.m1.1.1.1.1.4.1.cmml">+</mo><mrow id="S4.E3.m1.1.1.1.1.4.3" xref="S4.E3.m1.1.1.1.1.4.3.cmml"><mi mathvariant="normal" id="S4.E3.m1.1.1.1.1.4.3.2" xref="S4.E3.m1.1.1.1.1.4.3.2.cmml">Î”</mi><mo id="S4.E3.m1.1.1.1.1.4.3.1" xref="S4.E3.m1.1.1.1.1.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.4.3.3" xref="S4.E3.m1.1.1.1.1.4.3.3.cmml">W</mi><mo id="S4.E3.m1.1.1.1.1.4.3.1a" xref="S4.E3.m1.1.1.1.1.4.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.4.3.4" xref="S4.E3.m1.1.1.1.1.4.3.4.cmml">x</mi></mrow></mrow><mo id="S4.E3.m1.1.1.1.1.5" xref="S4.E3.m1.1.1.1.1.5.cmml">=</mo><mrow id="S4.E3.m1.1.1.1.1.6" xref="S4.E3.m1.1.1.1.1.6.cmml"><mrow id="S4.E3.m1.1.1.1.1.6.2" xref="S4.E3.m1.1.1.1.1.6.2.cmml"><msub id="S4.E3.m1.1.1.1.1.6.2.2" xref="S4.E3.m1.1.1.1.1.6.2.2.cmml"><mi id="S4.E3.m1.1.1.1.1.6.2.2.2" xref="S4.E3.m1.1.1.1.1.6.2.2.2.cmml">W</mi><mn id="S4.E3.m1.1.1.1.1.6.2.2.3" xref="S4.E3.m1.1.1.1.1.6.2.2.3.cmml">0</mn></msub><mo id="S4.E3.m1.1.1.1.1.6.2.1" xref="S4.E3.m1.1.1.1.1.6.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.6.2.3" xref="S4.E3.m1.1.1.1.1.6.2.3.cmml">x</mi></mrow><mo id="S4.E3.m1.1.1.1.1.6.1" xref="S4.E3.m1.1.1.1.1.6.1.cmml">+</mo><mrow id="S4.E3.m1.1.1.1.1.6.3" xref="S4.E3.m1.1.1.1.1.6.3.cmml"><mi id="S4.E3.m1.1.1.1.1.6.3.2" xref="S4.E3.m1.1.1.1.1.6.3.2.cmml">B</mi><mo id="S4.E3.m1.1.1.1.1.6.3.1" xref="S4.E3.m1.1.1.1.1.6.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.6.3.3" xref="S4.E3.m1.1.1.1.1.6.3.3.cmml">A</mi><mo id="S4.E3.m1.1.1.1.1.6.3.1a" xref="S4.E3.m1.1.1.1.1.6.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S4.E3.m1.1.1.1.1.6.3.4" xref="S4.E3.m1.1.1.1.1.6.3.4.cmml">x</mi></mrow></mrow></mrow><mo rspace="0.827em" id="S4.E3.m1.2.2.2.3" xref="S4.E3.m1.2.2.3a.cmml">,</mo><mrow id="S4.E3.m1.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.3.cmml"><mrow id="S4.E3.m1.2.2.2.2.1.1" xref="S4.E3.m1.2.2.2.2.1.1.cmml"><mi id="S4.E3.m1.2.2.2.2.1.1.2" xref="S4.E3.m1.2.2.2.2.1.1.2.cmml">B</mi><mo id="S4.E3.m1.2.2.2.2.1.1.1" xref="S4.E3.m1.2.2.2.2.1.1.1.cmml">âˆˆ</mo><msup id="S4.E3.m1.2.2.2.2.1.1.3" xref="S4.E3.m1.2.2.2.2.1.1.3.cmml"><mi id="S4.E3.m1.2.2.2.2.1.1.3.2" xref="S4.E3.m1.2.2.2.2.1.1.3.2.cmml">â„</mi><mrow id="S4.E3.m1.2.2.2.2.1.1.3.3" xref="S4.E3.m1.2.2.2.2.1.1.3.3.cmml"><mi id="S4.E3.m1.2.2.2.2.1.1.3.3.2" xref="S4.E3.m1.2.2.2.2.1.1.3.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.2.2.2.2.1.1.3.3.1" xref="S4.E3.m1.2.2.2.2.1.1.3.3.1.cmml">Ã—</mo><mi id="S4.E3.m1.2.2.2.2.1.1.3.3.3" xref="S4.E3.m1.2.2.2.2.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><mo id="S4.E3.m1.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S4.E3.m1.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.2" xref="S4.E3.m1.2.2.2.2.2.2.2.cmml">A</mi><mo id="S4.E3.m1.2.2.2.2.2.2.1" xref="S4.E3.m1.2.2.2.2.2.2.1.cmml">âˆˆ</mo><msup id="S4.E3.m1.2.2.2.2.2.2.3" xref="S4.E3.m1.2.2.2.2.2.2.3.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.3.2" xref="S4.E3.m1.2.2.2.2.2.2.3.2.cmml">â„</mi><mrow id="S4.E3.m1.2.2.2.2.2.2.3.3" xref="S4.E3.m1.2.2.2.2.2.2.3.3.cmml"><mi id="S4.E3.m1.2.2.2.2.2.2.3.3.2" xref="S4.E3.m1.2.2.2.2.2.2.3.3.2.cmml">r</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.2.2.2.2.2.2.3.3.1" xref="S4.E3.m1.2.2.2.2.2.2.3.3.1.cmml">Ã—</mo><mi id="S4.E3.m1.2.2.2.2.2.2.3.3.3" xref="S4.E3.m1.2.2.2.2.2.2.3.3.3.cmml">d</mi></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.2b"><apply id="S4.E3.m1.2.2.3.cmml" xref="S4.E3.m1.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.3a.cmml" xref="S4.E3.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1"><and id="S4.E3.m1.1.1.1.1a.cmml" xref="S4.E3.m1.1.1.1.1"></and><apply id="S4.E3.m1.1.1.1.1b.cmml" xref="S4.E3.m1.1.1.1.1"><eq id="S4.E3.m1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.3"></eq><ci id="S4.E3.m1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.2">â„</ci><apply id="S4.E3.m1.1.1.1.1.4.cmml" xref="S4.E3.m1.1.1.1.1.4"><plus id="S4.E3.m1.1.1.1.1.4.1.cmml" xref="S4.E3.m1.1.1.1.1.4.1"></plus><apply id="S4.E3.m1.1.1.1.1.4.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2"><times id="S4.E3.m1.1.1.1.1.4.2.1.cmml" xref="S4.E3.m1.1.1.1.1.4.2.1"></times><apply id="S4.E3.m1.1.1.1.1.4.2.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.4.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.4.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2.2">ğ‘Š</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.4.2.2.3.cmml" xref="S4.E3.m1.1.1.1.1.4.2.2.3">0</cn></apply><ci id="S4.E3.m1.1.1.1.1.4.2.3.cmml" xref="S4.E3.m1.1.1.1.1.4.2.3">ğ‘¥</ci></apply><apply id="S4.E3.m1.1.1.1.1.4.3.cmml" xref="S4.E3.m1.1.1.1.1.4.3"><times id="S4.E3.m1.1.1.1.1.4.3.1.cmml" xref="S4.E3.m1.1.1.1.1.4.3.1"></times><ci id="S4.E3.m1.1.1.1.1.4.3.2.cmml" xref="S4.E3.m1.1.1.1.1.4.3.2">Î”</ci><ci id="S4.E3.m1.1.1.1.1.4.3.3.cmml" xref="S4.E3.m1.1.1.1.1.4.3.3">ğ‘Š</ci><ci id="S4.E3.m1.1.1.1.1.4.3.4.cmml" xref="S4.E3.m1.1.1.1.1.4.3.4">ğ‘¥</ci></apply></apply></apply><apply id="S4.E3.m1.1.1.1.1c.cmml" xref="S4.E3.m1.1.1.1.1"><eq id="S4.E3.m1.1.1.1.1.5.cmml" xref="S4.E3.m1.1.1.1.1.5"></eq><share href="#S4.E3.m1.1.1.1.1.4.cmml" id="S4.E3.m1.1.1.1.1d.cmml" xref="S4.E3.m1.1.1.1.1"></share><apply id="S4.E3.m1.1.1.1.1.6.cmml" xref="S4.E3.m1.1.1.1.1.6"><plus id="S4.E3.m1.1.1.1.1.6.1.cmml" xref="S4.E3.m1.1.1.1.1.6.1"></plus><apply id="S4.E3.m1.1.1.1.1.6.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2"><times id="S4.E3.m1.1.1.1.1.6.2.1.cmml" xref="S4.E3.m1.1.1.1.1.6.2.1"></times><apply id="S4.E3.m1.1.1.1.1.6.2.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.6.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.6.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2.2">ğ‘Š</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.6.2.2.3.cmml" xref="S4.E3.m1.1.1.1.1.6.2.2.3">0</cn></apply><ci id="S4.E3.m1.1.1.1.1.6.2.3.cmml" xref="S4.E3.m1.1.1.1.1.6.2.3">ğ‘¥</ci></apply><apply id="S4.E3.m1.1.1.1.1.6.3.cmml" xref="S4.E3.m1.1.1.1.1.6.3"><times id="S4.E3.m1.1.1.1.1.6.3.1.cmml" xref="S4.E3.m1.1.1.1.1.6.3.1"></times><ci id="S4.E3.m1.1.1.1.1.6.3.2.cmml" xref="S4.E3.m1.1.1.1.1.6.3.2">ğµ</ci><ci id="S4.E3.m1.1.1.1.1.6.3.3.cmml" xref="S4.E3.m1.1.1.1.1.6.3.3">ğ´</ci><ci id="S4.E3.m1.1.1.1.1.6.3.4.cmml" xref="S4.E3.m1.1.1.1.1.6.3.4">ğ‘¥</ci></apply></apply></apply></apply><apply id="S4.E3.m1.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.3a.cmml" xref="S4.E3.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S4.E3.m1.2.2.2.2.1.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1"><in id="S4.E3.m1.2.2.2.2.1.1.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.1"></in><ci id="S4.E3.m1.2.2.2.2.1.1.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.2">ğµ</ci><apply id="S4.E3.m1.2.2.2.2.1.1.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.1.1.3.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3">superscript</csymbol><ci id="S4.E3.m1.2.2.2.2.1.1.3.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.2">â„</ci><apply id="S4.E3.m1.2.2.2.2.1.1.3.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3"><times id="S4.E3.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3.1"></times><ci id="S4.E3.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3.2">ğ‘‘</ci><ci id="S4.E3.m1.2.2.2.2.1.1.3.3.3.cmml" xref="S4.E3.m1.2.2.2.2.1.1.3.3.3">ğ‘Ÿ</ci></apply></apply></apply><apply id="S4.E3.m1.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2"><in id="S4.E3.m1.2.2.2.2.2.2.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.1"></in><ci id="S4.E3.m1.2.2.2.2.2.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.2">ğ´</ci><apply id="S4.E3.m1.2.2.2.2.2.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.2.2.3.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3">superscript</csymbol><ci id="S4.E3.m1.2.2.2.2.2.2.3.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.2">â„</ci><apply id="S4.E3.m1.2.2.2.2.2.2.3.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3"><times id="S4.E3.m1.2.2.2.2.2.2.3.3.1.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3.1"></times><ci id="S4.E3.m1.2.2.2.2.2.2.3.3.2.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3.2">ğ‘Ÿ</ci><ci id="S4.E3.m1.2.2.2.2.2.2.3.3.3.cmml" xref="S4.E3.m1.2.2.2.2.2.2.3.3.3">ğ‘‘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.2c">h=W_{0}x+\Delta Wx=W_{0}x+BAx,~{}~{}B\in\mathbb{R}^{d\times r},A\in\mathbb{R}^{r\times d}</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.2d">italic_h = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + roman_Î” italic_W italic_x = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + italic_B italic_A italic_x , italic_B âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d Ã— italic_r end_POSTSUPERSCRIPT , italic_A âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_r Ã— italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p">To achieve parameter-efficient training while adhering to a tight budget, we apply LoRA to the Chinese-LLaMA/Alpaca models in all our experiments, including both pre-training and fine-tuning stages. We primarily incorporate LoRA adapters into the weights of the attention module and, in some cases, additional MLP layers. For further details, please refer to the next section and Table <a href="#S5.T2" title="Table 2 â€£ Instruction Fine-tuning â€£ 5.1.2 13B Version â€£ 5.1 Experimental Setups for Pre-training and Fine-tuning â€£ 5 Experimental Setups â€£ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setups</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setups for Pre-training and Fine-tuning</h3>

<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>7B Version</h4>

<section id="S5.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pre-training</h5>

<div id="S5.SS1.SSS1.Px1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px1.p1.1" class="ltx_p">We initialize the Chinese-LLaMA model with the original LLaMA weights and pre-train the model on general Chinese corpora, consistent with the corpora used in Chinese BERT-wwm <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>, MacBERT <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>, LERT <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>, and others, resulting in a 20GB text corpus.
The pre-training process consists of two stages:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Stage 1</span>: We fix the parameters of the transformer encoders within the model and only train the embeddings, adapting the newly added Chinese word vectors while minimizing the disturbance to the original model.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Stage 2</span>: We add LoRA weights (adapters) to the attention mechanisms and train the embeddings, LM heads, and newly added LoRA parameters.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Instruction Fine-tuning</h5>

<div id="S5.SS1.SSS1.Px2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px2.p1.1" class="ltx_p">After obtaining the pre-trained model, we fine-tune it according to Section <a href="#S3" title="3 Chinese Alpaca â€£ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We also use LoRA for efficient fine-tuning, increasing the number of trainable parameters by adding LoRA adapters to the MLP layers. We utilize approximately 2M data points, including translation <cite class="ltx_cite ltx_citemacro_citep">(Xu, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>, pCLUE<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/CLUEbenchmark/pCLUE" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/CLUEbenchmark/pCLUE</a></span></span></span>, Stanford Alpaca, and crawled SFT data for tuning the 7B model.</p>
</div>
<div id="S5.SS1.SSS1.Px2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px2.p2.1" class="ltx_p">For the crawled data, we employ the self-instruction <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite> method for automatically obtaining data from ChatGPT (<span id="S5.SS1.SSS1.Px2.p2.1.1" class="ltx_text ltx_font_typewriter">gpt-3.5-turbo</span> API), as used in <cite class="ltx_cite ltx_citemacro_citet">Taori et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>. Templates and code details are available on GitHub.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py</a></span></span></span></p>
</div>
<div id="S5.SS1.SSS1.Px2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS1.Px2.p3.1" class="ltx_p">The hyperparameters are listed in Table <a href="#S5.T2" title="Table 2 â€£ Instruction Fine-tuning â€£ 5.1.2 13B Version â€£ 5.1 Experimental Setups for Pre-training and Fine-tuning â€£ 5 Experimental Setups â€£ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Detailed information about the fine-tuning data is provided in Table <a href="#S5.T3" title="Table 3 â€£ Instruction Fine-tuning â€£ 5.1.2 13B Version â€£ 5.1 Experimental Setups for Pre-training and Fine-tuning â€£ 5 Experimental Setups â€£ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>13B Version</h4>

<section id="S5.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pre-training</h5>

<div id="S5.SS1.SSS2.Px1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.Px1.p1.1" class="ltx_p">The pre-training process for the 13B model is largely the same as that of the 7B model, with the exception that we skip stage 1 in the pre-training. We directly apply LoRA to attentions and MLPs for training while setting the embeddings and LM head as trainable.</p>
</div>
</section>
<section id="S5.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Instruction Fine-tuning</h5>

<div id="S5.SS1.SSS2.Px2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.Px2.p1.1" class="ltx_p">The LoRA settings and trainable parameters remain the same as in the pre-training stage. We use an additional 1M crawled self-instructed data points for the 13B model fine-tuning, resulting in a total data size of 3M for the 13B model.</p>
</div>
<div id="S5.SS1.SSS2.Px2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS2.Px2.p2.1" class="ltx_p">The hyperparameters are listed in Table <a href="#S5.T2" title="Table 2 â€£ Instruction Fine-tuning â€£ 5.1.2 13B Version â€£ 5.1 Experimental Setups for Pre-training and Fine-tuning â€£ 5 Experimental Setups â€£ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span> Training recipes for LLaMA (pre-training stages) and Alpaca (instruction SFT stage) 7B and 13B. PT: pre-training. SFT: supervised fine-tuning. QKVO: four matrices (represents query, key, value, and output) in each attention module. MLP: three matrices in each MLP layer.</figcaption>
<table id="S5.T2.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S5.T2.5.6" class="ltx_tr">
<td id="S5.T2.5.6.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T2.5.6.1.1" class="ltx_text ltx_font_bold">7B Settings</span></td>
<td id="S5.T2.5.6.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.6.2.1" class="ltx_text ltx_font_bold">PT Stage 1</span></td>
<td id="S5.T2.5.6.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.6.3.1" class="ltx_text ltx_font_bold">PT Stage 2</span></td>
<td id="S5.T2.5.6.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.6.4.1" class="ltx_text ltx_font_bold">Instruction SFT</span></td>
</tr>
<tr id="S5.T2.5.7" class="ltx_tr">
<td id="S5.T2.5.7.1" class="ltx_td ltx_align_left ltx_border_t">Batch size</td>
<td id="S5.T2.5.7.2" class="ltx_td ltx_align_center ltx_border_t">1024</td>
<td id="S5.T2.5.7.3" class="ltx_td ltx_align_center ltx_border_t">1024</td>
<td id="S5.T2.5.7.4" class="ltx_td ltx_align_center ltx_border_t">512</td>
</tr>
<tr id="S5.T2.5.8" class="ltx_tr">
<td id="S5.T2.5.8.1" class="ltx_td ltx_align_left">Peak learning rate</td>
<td id="S5.T2.5.8.2" class="ltx_td ltx_align_center">2e-4</td>
<td id="S5.T2.5.8.3" class="ltx_td ltx_align_center">1e-4</td>
<td id="S5.T2.5.8.4" class="ltx_td ltx_align_center">1e-4</td>
</tr>
<tr id="S5.T2.5.9" class="ltx_tr">
<td id="S5.T2.5.9.1" class="ltx_td ltx_align_left">Training steps</td>
<td id="S5.T2.5.9.2" class="ltx_td ltx_align_center">3K</td>
<td id="S5.T2.5.9.3" class="ltx_td ltx_align_center">6K</td>
<td id="S5.T2.5.9.4" class="ltx_td ltx_align_center">6-10K</td>
</tr>
<tr id="S5.T2.5.10" class="ltx_tr">
<td id="S5.T2.5.10.1" class="ltx_td ltx_align_left">Max length</td>
<td id="S5.T2.5.10.2" class="ltx_td ltx_align_center">512</td>
<td id="S5.T2.5.10.3" class="ltx_td ltx_align_center">512</td>
<td id="S5.T2.5.10.4" class="ltx_td ltx_align_center">512</td>
</tr>
<tr id="S5.T2.5.11" class="ltx_tr">
<td id="S5.T2.5.11.1" class="ltx_td ltx_align_left">Trainable parameters</td>
<td id="S5.T2.5.11.2" class="ltx_td ltx_align_center">2.97%</td>
<td id="S5.T2.5.11.3" class="ltx_td ltx_align_center">6.06%</td>
<td id="S5.T2.5.11.4" class="ltx_td ltx_align_center">6.22%</td>
</tr>
<tr id="S5.T2.5.12" class="ltx_tr">
<td id="S5.T2.5.12.1" class="ltx_td ltx_align_left">LoRA rank</td>
<td id="S5.T2.5.12.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.5.12.3" class="ltx_td ltx_align_center">8</td>
<td id="S5.T2.5.12.4" class="ltx_td ltx_align_center">8</td>
</tr>
<tr id="S5.T2.5.13" class="ltx_tr">
<td id="S5.T2.5.13.1" class="ltx_td ltx_align_left">LoRA weights</td>
<td id="S5.T2.5.13.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.5.13.3" class="ltx_td ltx_align_center">QKVO</td>
<td id="S5.T2.5.13.4" class="ltx_td ltx_align_center">QKVO, MLP</td>
</tr>
<tr id="S5.T2.3.3" class="ltx_tr">
<td id="S5.T2.3.3.4" class="ltx_td ltx_align_left">Training device</td>
<td id="S5.T2.1.1.1" class="ltx_td ltx_align_center">8 <math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><times id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.m1.1d">Ã—</annotation></semantics></math> A100</td>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_center">16 <math id="S5.T2.2.2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.2.2.2.m1.1a"><mo id="S5.T2.2.2.2.m1.1.1" xref="S5.T2.2.2.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.m1.1b"><times id="S5.T2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.m1.1d">Ã—</annotation></semantics></math> A100</td>
<td id="S5.T2.3.3.3" class="ltx_td ltx_align_center">16 <math id="S5.T2.3.3.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.3.3.3.m1.1a"><mo id="S5.T2.3.3.3.m1.1.1" xref="S5.T2.3.3.3.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.m1.1b"><times id="S5.T2.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.m1.1d">Ã—</annotation></semantics></math> A100</td>
</tr>
<tr id="S5.T2.5.14" class="ltx_tr">
<td id="S5.T2.5.14.1" class="ltx_td ltx_align_left">Distributed training</td>
<td id="S5.T2.5.14.2" class="ltx_td ltx_align_center">DeepSpeed ZeRO-2</td>
<td id="S5.T2.5.14.3" class="ltx_td ltx_align_center">DeepSpeed ZeRO-2</td>
<td id="S5.T2.5.14.4" class="ltx_td ltx_align_center">DeepSpeed ZeRO-2</td>
</tr>
<tr id="S5.T2.5.15" class="ltx_tr">
<td id="S5.T2.5.15.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T2.5.15.1.1" class="ltx_text ltx_font_bold">13B Settings</span></td>
<td id="S5.T2.5.15.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T2.5.15.2.1" class="ltx_text ltx_font_bold">PT</span></td>
<td id="S5.T2.5.15.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.5.15.3.1" class="ltx_text ltx_font_bold">Instruction SFT</span></td>
</tr>
<tr id="S5.T2.5.16" class="ltx_tr">
<td id="S5.T2.5.16.1" class="ltx_td ltx_align_left ltx_border_t">Batch size</td>
<td id="S5.T2.5.16.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2">2304</td>
<td id="S5.T2.5.16.3" class="ltx_td ltx_align_center ltx_border_t">1152</td>
</tr>
<tr id="S5.T2.5.17" class="ltx_tr">
<td id="S5.T2.5.17.1" class="ltx_td ltx_align_left">Peak learning rate</td>
<td id="S5.T2.5.17.2" class="ltx_td ltx_align_center" colspan="2">2e-4</td>
<td id="S5.T2.5.17.3" class="ltx_td ltx_align_center">1e-4</td>
</tr>
<tr id="S5.T2.5.18" class="ltx_tr">
<td id="S5.T2.5.18.1" class="ltx_td ltx_align_left">Training steps</td>
<td id="S5.T2.5.18.2" class="ltx_td ltx_align_center" colspan="2">7K</td>
<td id="S5.T2.5.18.3" class="ltx_td ltx_align_center">5.5K</td>
</tr>
<tr id="S5.T2.5.19" class="ltx_tr">
<td id="S5.T2.5.19.1" class="ltx_td ltx_align_left">Max length</td>
<td id="S5.T2.5.19.2" class="ltx_td ltx_align_center" colspan="2">512</td>
<td id="S5.T2.5.19.3" class="ltx_td ltx_align_center">512</td>
</tr>
<tr id="S5.T2.5.20" class="ltx_tr">
<td id="S5.T2.5.20.1" class="ltx_td ltx_align_left">Trainable parameters</td>
<td id="S5.T2.5.20.2" class="ltx_td ltx_align_center" colspan="2">4.10%</td>
<td id="S5.T2.5.20.3" class="ltx_td ltx_align_center">4.10%</td>
</tr>
<tr id="S5.T2.5.21" class="ltx_tr">
<td id="S5.T2.5.21.1" class="ltx_td ltx_align_left">LoRA rank</td>
<td id="S5.T2.5.21.2" class="ltx_td ltx_align_center" colspan="2">8</td>
<td id="S5.T2.5.21.3" class="ltx_td ltx_align_center">8</td>
</tr>
<tr id="S5.T2.5.22" class="ltx_tr">
<td id="S5.T2.5.22.1" class="ltx_td ltx_align_left">LoRA weights</td>
<td id="S5.T2.5.22.2" class="ltx_td ltx_align_center" colspan="2">QKVO, MLP</td>
<td id="S5.T2.5.22.3" class="ltx_td ltx_align_center">QKVO, MLP</td>
</tr>
<tr id="S5.T2.5.5" class="ltx_tr">
<td id="S5.T2.5.5.3" class="ltx_td ltx_align_left">Training device</td>
<td id="S5.T2.4.4.1" class="ltx_td ltx_align_center" colspan="2">48 <math id="S5.T2.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.4.4.1.m1.1a"><mo id="S5.T2.4.4.1.m1.1.1" xref="S5.T2.4.4.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.1.m1.1b"><times id="S5.T2.4.4.1.m1.1.1.cmml" xref="S5.T2.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.1.m1.1d">Ã—</annotation></semantics></math> A100</td>
<td id="S5.T2.5.5.2" class="ltx_td ltx_align_center">48 <math id="S5.T2.5.5.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.5.5.2.m1.1a"><mo id="S5.T2.5.5.2.m1.1.1" xref="S5.T2.5.5.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.2.m1.1b"><times id="S5.T2.5.5.2.m1.1.1.cmml" xref="S5.T2.5.5.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.5.2.m1.1d">Ã—</annotation></semantics></math> A100</td>
</tr>
<tr id="S5.T2.5.23" class="ltx_tr">
<td id="S5.T2.5.23.1" class="ltx_td ltx_align_left ltx_border_bb">Distributed training</td>
<td id="S5.T2.5.23.2" class="ltx_td ltx_align_center ltx_border_bb" colspan="2">DeepSpeed ZeRO-2</td>
<td id="S5.T2.5.23.3" class="ltx_td ltx_align_center ltx_border_bb">DeepSpeed ZeRO-2</td>
</tr>
</tbody></table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Details of the data used in instruction fine-tuning stage.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S5.T3.1.2" class="ltx_tr">
<td id="S5.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S5.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_tt">Translation</td>
<td id="S5.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_tt">pCLUE</td>
<td id="S5.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_tt">Stanford Alpaca</td>
<td id="S5.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_tt">Stanford Alpaca (Chinese)</td>
<td id="S5.T3.1.2.6" class="ltx_td ltx_align_center ltx_border_tt">Crawled SFT data</td>
</tr>
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T3.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">550K</td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">250K</td>
<td id="S5.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">50K</td>
<td id="S5.T3.1.1.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">50K</td>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1<math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.m1.1d">âˆ¼</annotation></semantics></math>2M</td>
</tr>
</tbody></table>
</figure>
</section>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Experimental Setups for Decoding</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">The decoding process of LLMs plays a critical role in determining the quality and diversity of the generated text. In our experiments, we use the following decoding hyperparameters:</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p">Context size: We set the context size to 2048, which determines the maximum number of tokens that the model can consider simultaneously when generating text.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p">Maximum sequence length: We limit the generated sequence length to 512 tokens to ensure that the outputs remain focused and relevant to the input prompt.
</p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I2.i3.p1" class="ltx_para">
<p id="S5.I2.i3.p1.1" class="ltx_p">Temperature: We set the temperature to 0.2, which controls the randomness of the sampling process. Lower values make the model generate more focused and deterministic outputs, while higher values increase diversity at the cost of coherence.</p>
</div>
</li>
<li id="S5.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I2.i4.p1" class="ltx_para">
<p id="S5.I2.i4.p1.3" class="ltx_p">Top-<math id="S5.I2.i4.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.I2.i4.p1.1.m1.1a"><mi id="S5.I2.i4.p1.1.m1.1.1" xref="S5.I2.i4.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i4.p1.1.m1.1b"><ci id="S5.I2.i4.p1.1.m1.1.1.cmml" xref="S5.I2.i4.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i4.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i4.p1.1.m1.1d">italic_k</annotation></semantics></math> sampling: We use Top-<math id="S5.I2.i4.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.I2.i4.p1.2.m2.1a"><mi id="S5.I2.i4.p1.2.m2.1.1" xref="S5.I2.i4.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i4.p1.2.m2.1b"><ci id="S5.I2.i4.p1.2.m2.1.1.cmml" xref="S5.I2.i4.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i4.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i4.p1.2.m2.1d">italic_k</annotation></semantics></math> sampling with <math id="S5.I2.i4.p1.3.m3.1" class="ltx_Math" alttext="k=40" display="inline"><semantics id="S5.I2.i4.p1.3.m3.1a"><mrow id="S5.I2.i4.p1.3.m3.1.1" xref="S5.I2.i4.p1.3.m3.1.1.cmml"><mi id="S5.I2.i4.p1.3.m3.1.1.2" xref="S5.I2.i4.p1.3.m3.1.1.2.cmml">k</mi><mo id="S5.I2.i4.p1.3.m3.1.1.1" xref="S5.I2.i4.p1.3.m3.1.1.1.cmml">=</mo><mn id="S5.I2.i4.p1.3.m3.1.1.3" xref="S5.I2.i4.p1.3.m3.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i4.p1.3.m3.1b"><apply id="S5.I2.i4.p1.3.m3.1.1.cmml" xref="S5.I2.i4.p1.3.m3.1.1"><eq id="S5.I2.i4.p1.3.m3.1.1.1.cmml" xref="S5.I2.i4.p1.3.m3.1.1.1"></eq><ci id="S5.I2.i4.p1.3.m3.1.1.2.cmml" xref="S5.I2.i4.p1.3.m3.1.1.2">ğ‘˜</ci><cn type="integer" id="S5.I2.i4.p1.3.m3.1.1.3.cmml" xref="S5.I2.i4.p1.3.m3.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i4.p1.3.m3.1c">k=40</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i4.p1.3.m3.1d">italic_k = 40</annotation></semantics></math>, meaning that the model selects its next token from the top 40 most probable tokens at each step, adding an element of randomness and diversity to the generated text.</p>
</div>
</li>
<li id="S5.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I2.i5.p1" class="ltx_para">
<p id="S5.I2.i5.p1.3" class="ltx_p">Top-<math id="S5.I2.i5.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S5.I2.i5.p1.1.m1.1a"><mi id="S5.I2.i5.p1.1.m1.1.1" xref="S5.I2.i5.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i5.p1.1.m1.1b"><ci id="S5.I2.i5.p1.1.m1.1.1.cmml" xref="S5.I2.i5.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i5.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i5.p1.1.m1.1d">italic_p</annotation></semantics></math> sampling: We also employ Top-<math id="S5.I2.i5.p1.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S5.I2.i5.p1.2.m2.1a"><mi id="S5.I2.i5.p1.2.m2.1.1" xref="S5.I2.i5.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.I2.i5.p1.2.m2.1b"><ci id="S5.I2.i5.p1.2.m2.1.1.cmml" xref="S5.I2.i5.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i5.p1.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i5.p1.2.m2.1d">italic_p</annotation></semantics></math> sampling with <math id="S5.I2.i5.p1.3.m3.1" class="ltx_Math" alttext="p=0.9" display="inline"><semantics id="S5.I2.i5.p1.3.m3.1a"><mrow id="S5.I2.i5.p1.3.m3.1.1" xref="S5.I2.i5.p1.3.m3.1.1.cmml"><mi id="S5.I2.i5.p1.3.m3.1.1.2" xref="S5.I2.i5.p1.3.m3.1.1.2.cmml">p</mi><mo id="S5.I2.i5.p1.3.m3.1.1.1" xref="S5.I2.i5.p1.3.m3.1.1.1.cmml">=</mo><mn id="S5.I2.i5.p1.3.m3.1.1.3" xref="S5.I2.i5.p1.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i5.p1.3.m3.1b"><apply id="S5.I2.i5.p1.3.m3.1.1.cmml" xref="S5.I2.i5.p1.3.m3.1.1"><eq id="S5.I2.i5.p1.3.m3.1.1.1.cmml" xref="S5.I2.i5.p1.3.m3.1.1.1"></eq><ci id="S5.I2.i5.p1.3.m3.1.1.2.cmml" xref="S5.I2.i5.p1.3.m3.1.1.2">ğ‘</ci><cn type="float" id="S5.I2.i5.p1.3.m3.1.1.3.cmml" xref="S5.I2.i5.p1.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i5.p1.3.m3.1c">p=0.9</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i5.p1.3.m3.1d">italic_p = 0.9</annotation></semantics></math>, which further enhances diversity by considering a dynamic set of tokens that collectively account for 90% of the probability mass.</p>
</div>
</li>
<li id="S5.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S5.I2.i6.p1" class="ltx_para ltx_noindent">
<p id="S5.I2.i6.p1.1" class="ltx_p">Repetition penalty: To discourage the model from generating repetitive text, we apply a repetition penalty with a factor of 1.3, penalizing tokens that have already been selected.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">Note that these values may not be optimal for each testing scenario. We did not perform further tuning on these hyperparameters for each task to maintain a balanced view.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Deployment on CPU</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">Deploying large language models on personal computers, particularly on CPUs, has historically been challenging due to their immense computational requirements. However, with the help of many community efforts, such as <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">llama.cpp</span> <cite class="ltx_cite ltx_citemacro_citep">(Gerganov, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, users can efficiently quantize LLMs into 4-bit forms, significantly reducing memory usage and computational demands, making it easier to deploy LLMs on personal computers. This also enables quicker interactions with the models and facilitates local data processing.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p">Quantizing LLMs and deploying them on personal computers offer several benefits. Firstly, it helps users protect their data privacy by ensuring that sensitive information remains within their local environment, rather than being transmitted to external servers. Secondly, it democratizes access to LLMs by making them more accessible to users with limited computational resources. Lastly, it promotes the development of new applications and research directions that take advantage of local LLM deployments. Overall, the ability to deploy LLMs on personal computers using <span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">llama.cpp</span> (or similar) paves the way for a more versatile and privacy-conscious utilization of LLMs in various domains.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.1" class="ltx_p">In the following sections, we will use the 4-bit round-to-nearest (RTN) <cite class="ltx_cite ltx_citemacro_citep">(Yao et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2022</a>; Dettmers et&nbsp;al., <a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite> quantized Chinese Alpaca for evaluation, which is more realistic from a user perspective rather than a research-oriented view. As a kind reminder, 4-bit quantized models generally perform worse than FP16 or FP32 models.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Evaluation and Task Design</h3>

<div id="S5.SS4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS4.p1.1" class="ltx_p">Evaluating the performance of text generation tasks can be challenging due to the significant variety in their form, unlike natural language understanding tasks (such as text classification and extractive machine reading comprehension). Following previous work that utilizes GPT-4 as a scoring method, we also adopt GPT-4 to provide an overall score (on a 10-point scale) for each sample, which is more efficient than human evaluation. However, GPT-4 may not always provide accurate scores, so we perform manual checks on its ratings and adjust them if necessary. The manual checks ensure that the scores are consistent and reflect the true performance of the models being evaluated. We use the following prompt template for scoring the outputs of the systems:
</p>
</div>
<div id="S5.SS4.p2" class="ltx_para ltx_noindent">
<blockquote id="S5.SS4.p2.1" class="ltx_quote">
<p id="S5.SS4.p2.1.1" class="ltx_p"><span id="S5.SS4.p2.1.1.1" class="ltx_text ltx_font_italic">The followings are two ChatGPT-like systemsâ€™ outputs. Please rate an overall score on a ten point scale for each and give explanations to justify your scores.

<br class="ltx_break">
Prompt:

<br class="ltx_break">
</span>{<span id="S5.SS4.p2.1.1.2" class="ltx_text ltx_font_italic">prompt-input</span>}<span id="S5.SS4.p2.1.1.3" class="ltx_text ltx_font_italic">

<br class="ltx_break">
System1:

<br class="ltx_break">
</span>{<span id="S5.SS4.p2.1.1.4" class="ltx_text ltx_font_italic">system1-output</span>}<span id="S5.SS4.p2.1.1.5" class="ltx_text ltx_font_italic">

<br class="ltx_break">
System2:

<br class="ltx_break">
</span>{<span id="S5.SS4.p2.1.1.6" class="ltx_text ltx_font_italic">system2-output</span>}<span id="S5.SS4.p2.1.1.7" class="ltx_text ltx_font_italic"></span></p>
</blockquote>
</div>
<div id="S5.SS4.p3" class="ltx_para ltx_noindent">
<p id="S5.SS4.p3.1" class="ltx_p">By employing GPT-4 as a scoring method in conjunction with manual checks, we establish a reliable evaluation framework that effectively measures the performance of our Chinese Alpaca models on a range of natural language understanding and generation tasks.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para ltx_noindent">
<p id="S5.SS4.p4.1" class="ltx_p">Our evaluation set is designed to provide a comprehensive assessment of the Chinese Alpaca models across a wide range of natural language understanding and generation tasks. The set comprises 160 samples, covering 10 distinct tasks, including Question Answering, Reasoning, Literature, Entertainment, Translation, Multi-turn Dialogue, Coding, and Ethics, among others. The overall score for a specific task is calculated by summing the scores for all samples within that task and normalizing the total to a 100-point scale. This approach ensures that the evaluation set reflects the modelsâ€™ capabilities across various tasks, providing a balanced and robust measure of their performance.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this section, we present and analyze the results obtained from our experiments with 4-bit quantized Chinese Alpaca-7B and Alpaca-13B models, as shown in Table <a href="#S6.T4" title="Table 4 â€£ 6 Results â€£ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The evaluation is based on GPT-4 rated results across ten distinct NLP tasks, encompassing a total of 160 samples. It is important to note that the presented scores are solely comparable with each other but not with other models, which would require rescoring the systems.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span> GPT-4 rated results for 4-bit quantized Chinese Alpaca-7B and Alpaca-13B. Note that the results are only comparable within this model combination.</figcaption>
<table id="S6.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T4.1.1" class="ltx_tr">
<td id="S6.T4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T4.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></td>
<td id="S6.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.2.1" class="ltx_text ltx_font_bold">Samples #</span></td>
<td id="S6.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.3.1" class="ltx_text ltx_font_bold">Chinese-Alpaca-7B</span></td>
<td id="S6.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.4.1" class="ltx_text ltx_font_bold">Chinese-Alpaca-13B</span></td>
</tr>
<tr id="S6.T4.1.2" class="ltx_tr">
<td id="S6.T4.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Question Answering</td>
<td id="S6.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_t">20</td>
<td id="S6.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_t">53</td>
<td id="S6.T4.1.2.4" class="ltx_td ltx_align_center ltx_border_t">77</td>
</tr>
<tr id="S6.T4.1.3" class="ltx_tr">
<td id="S6.T4.1.3.1" class="ltx_td ltx_align_left">Open-ended QA</td>
<td id="S6.T4.1.3.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.3.3" class="ltx_td ltx_align_center">64</td>
<td id="S6.T4.1.3.4" class="ltx_td ltx_align_center">73</td>
</tr>
<tr id="S6.T4.1.4" class="ltx_tr">
<td id="S6.T4.1.4.1" class="ltx_td ltx_align_left">Numerical Reasoning</td>
<td id="S6.T4.1.4.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.4.3" class="ltx_td ltx_align_center">23</td>
<td id="S6.T4.1.4.4" class="ltx_td ltx_align_center">50</td>
</tr>
<tr id="S6.T4.1.5" class="ltx_tr">
<td id="S6.T4.1.5.1" class="ltx_td ltx_align_left">Poetry, Literature, Philosophy</td>
<td id="S6.T4.1.5.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.5.3" class="ltx_td ltx_align_center">31</td>
<td id="S6.T4.1.5.4" class="ltx_td ltx_align_center">54</td>
</tr>
<tr id="S6.T4.1.6" class="ltx_tr">
<td id="S6.T4.1.6.1" class="ltx_td ltx_align_left">Music, Sports, Entertainment</td>
<td id="S6.T4.1.6.2" class="ltx_td ltx_align_center">20</td>
<td id="S6.T4.1.6.3" class="ltx_td ltx_align_center">36</td>
<td id="S6.T4.1.6.4" class="ltx_td ltx_align_center">65</td>
</tr>
<tr id="S6.T4.1.7" class="ltx_tr">
<td id="S6.T4.1.7.1" class="ltx_td ltx_align_left">Letters and Articles Writing</td>
<td id="S6.T4.1.7.2" class="ltx_td ltx_align_center">15</td>
<td id="S6.T4.1.7.3" class="ltx_td ltx_align_center">65</td>
<td id="S6.T4.1.7.4" class="ltx_td ltx_align_center">78</td>
</tr>
<tr id="S6.T4.1.8" class="ltx_tr">
<td id="S6.T4.1.8.1" class="ltx_td ltx_align_left">Translation</td>
<td id="S6.T4.1.8.2" class="ltx_td ltx_align_center">15</td>
<td id="S6.T4.1.8.3" class="ltx_td ltx_align_center">63</td>
<td id="S6.T4.1.8.4" class="ltx_td ltx_align_center">78</td>
</tr>
<tr id="S6.T4.1.9" class="ltx_tr">
<td id="S6.T4.1.9.1" class="ltx_td ltx_align_left">Multi-turn Dialogue</td>
<td id="S6.T4.1.9.2" class="ltx_td ltx_align_center">10</td>
<td id="S6.T4.1.9.3" class="ltx_td ltx_align_center">80</td>
<td id="S6.T4.1.9.4" class="ltx_td ltx_align_center">83</td>
</tr>
<tr id="S6.T4.1.10" class="ltx_tr">
<td id="S6.T4.1.10.1" class="ltx_td ltx_align_left">Coding</td>
<td id="S6.T4.1.10.2" class="ltx_td ltx_align_center">10</td>
<td id="S6.T4.1.10.3" class="ltx_td ltx_align_center">27</td>
<td id="S6.T4.1.10.4" class="ltx_td ltx_align_center">49</td>
</tr>
<tr id="S6.T4.1.11" class="ltx_tr">
<td id="S6.T4.1.11.1" class="ltx_td ltx_align_left">Ethics</td>
<td id="S6.T4.1.11.2" class="ltx_td ltx_align_center">10</td>
<td id="S6.T4.1.11.3" class="ltx_td ltx_align_center">50</td>
<td id="S6.T4.1.11.4" class="ltx_td ltx_align_center">100</td>
</tr>
<tr id="S6.T4.1.12" class="ltx_tr">
<td id="S6.T4.1.12.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S6.T4.1.12.1.1" class="ltx_text ltx_font_bold">Total</span></td>
<td id="S6.T4.1.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">160</td>
<td id="S6.T4.1.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">49</td>
<td id="S6.T4.1.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">71</td>
</tr>
</tbody></table>
</figure>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">The performance of both Chinese Alpaca-7B and Alpaca-13B models demonstrates significant improvements over their original LLaMA counterparts. The Chinese Alpaca-13B model consistently outperforms the 7B variant, highlighting the benefits of increased model capacity.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">For Question Answering tasks, the Chinese Alpaca-13B achieves a score of 77, compared to 53 for the 7B model. Similar improvements can be observed in Open-ended QA, with scores of 73 and 64 for the 13B and 7B models, respectively. Numerical Reasoning shows a more considerable improvement, with the 13B model scoring 50 compared to 23 for the 7B model.</p>
</div>
<div id="S6.p4" class="ltx_para ltx_noindent">
<p id="S6.p4.1" class="ltx_p">In the domains of Poetry, Literature, Philosophy, Music, Sports, and Entertainment, the 13B model continues to outperform the 7B model, with scores of 54 and 65 against 31 and 36, respectively. The performance gap remains significant for tasks involving Letters and Articles, Translation, and Multi-turn Dialogue, with the 13B model consistently achieving higher scores. Interestingly, we observe that even though we did not use any multi-turn dialogue data for tuning systems, Chinese Alpaca still has the ability to track conversation history and follow user instructions in a consecutive manner.</p>
</div>
<div id="S6.p5" class="ltx_para ltx_noindent">
<p id="S6.p5.1" class="ltx_p">Coding tasks exhibit a noticeable improvement, with the Chinese Alpaca-13B scoring 49 compared to 27 for the 7B model. The most striking performance difference can be observed in the Ethics task, where the 13B model achieves a perfect score of 100, in contrast to the 7B modelâ€™s score of 50, indicating superior performance in rejecting any unethical user inputs.</p>
</div>
<div id="S6.p6" class="ltx_para ltx_noindent">
<p id="S6.p6.1" class="ltx_p">In summary, the experimental results demonstrate that both Chinese Alpaca-7B and Alpaca-13B models exhibit significant improvements over their original LLaMA counterparts, with the 13B model consistently outperforming the 7B model across all tasks. This underscores the effectiveness of our approach in enhancing the Chinese understanding and generation capabilities of the LLaMA and Alpaca models.</p>
</div>
<div id="S6.p7" class="ltx_para ltx_noindent">
<p id="S6.p7.1" class="ltx_p">We provide some cases in Table <a href="#S6.T5" title="Table 5 â€£ 6 Results â€£ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, <a href="#S6.T6" title="Table 6 â€£ 6 Results â€£ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, and <a href="#S6.T7" title="Table 7 â€£ 6 Results â€£ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. For full comparisons and samples, please refer to our GitHub repository.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples</a></span></span></span></p>
</div>
<figure id="S6.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span> Example output for multi-turn dialogue task. Comparable user inputs are used.</figcaption>
<table id="S6.T5.16" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T5.16.17" class="ltx_tr">
<td id="S6.T5.16.17.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T5.16.17.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T5.16.17.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T5.16.17.2.1" class="ltx_text ltx_font_bold">Theme 1: æ ¸å¼¹åˆ¶ä½œ</span></td>
</tr>
<tr id="S6.T5.4.4" class="ltx_tr">
<td id="S6.T5.4.4.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T5.4.4.5.1" class="ltx_text"></span> <span id="S6.T5.4.4.5.2" class="ltx_text">
<span id="S6.T5.4.4.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.4.4.5.2.1.1" class="ltx_tr">
<span id="S6.T5.4.4.5.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T5.4.4.5.2.1.2" class="ltx_tr">
<span id="S6.T5.4.4.5.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T5.4.4.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.4.4.4" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T5.4.4.4.5" class="ltx_text"></span><span id="S6.T5.4.4.4.4" class="ltx_text">
<span id="S6.T5.4.4.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.1.1.1.1.1.1" class="ltx_tr">
<span id="S6.T5.1.1.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.1.1.1.1.1.1.1.m1.1a"><mo id="S6.T5.1.1.1.1.1.1.1.m1.1.1" xref="S6.T5.1.1.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.1.1.1.1.m1.1b"><gt id="S6.T5.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.1.1.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">è°æ‹¥æœ‰æ ¸æ­¦å™¨ï¼Ÿ</span></span></span>
<span id="S6.T5.4.4.4.4.4.5" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.5.1" class="ltx_td ltx_align_left">ç›®å‰æœ‰ä¹ä¸ªå›½å®¶ï¼ˆç¾å›½ã€ä¿„ç½—æ–¯ã€ä¸­å›½ã€è‹±å›½ã€æ³•å›½ã€å°åº¦ã€å·´åŸºæ–¯å¦ã€ä»¥è‰²åˆ—å’ŒåŒ—éŸ©ï¼‰</span></span>
<span id="S6.T5.4.4.4.4.4.6" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.6.1" class="ltx_td ltx_align_left">æŒæœ‰æ ¸æ­¦ã€‚</span></span>
<span id="S6.T5.2.2.2.2.2.2" class="ltx_tr">
<span id="S6.T5.2.2.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.2.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.2.2.2.2.2.2.1.m1.1a"><mo id="S6.T5.2.2.2.2.2.2.1.m1.1.1" xref="S6.T5.2.2.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.2.2.2.2.2.2.1.m1.1b"><gt id="S6.T5.2.2.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.2.2.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.2.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.2.2.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.2.2.2.2.2.2.1.1" class="ltx_text ltx_font_bold">ä½ èƒ½å‘Šè¯‰æˆ‘æ€ä¹ˆåšæ ¸æ­¦å™¨å—ï¼Ÿ</span></span></span>
<span id="S6.T5.4.4.4.4.4.7" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.7.1" class="ltx_td ltx_align_left">åˆ¶é€ æ ¸æ­¦å™¨éœ€è¦éå¸¸é«˜çš„ç§‘æŠ€æ°´å¹³ï¼Œè€Œä¸”è¦å…·å¤‡å¤§é‡çš„èµ„æºæ¥è¿›è¡Œè¯•éªŒå’Œå¼€å‘å·¥ä½œã€‚</span></span>
<span id="S6.T5.4.4.4.4.4.8" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.8.1" class="ltx_td ltx_align_left">å› æ­¤ï¼Œç»å¤§å¤šæ•°å›½å®¶éƒ½æ— æ³•è‡ªè¡Œç ”åˆ¶å‡ºæ ¸æ­¦ã€‚</span></span>
<span id="S6.T5.3.3.3.3.3.3" class="ltx_tr">
<span id="S6.T5.3.3.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.3.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.3.3.3.3.3.3.1.m1.1a"><mo id="S6.T5.3.3.3.3.3.3.1.m1.1.1" xref="S6.T5.3.3.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.3.3.3.3.3.3.1.m1.1b"><gt id="S6.T5.3.3.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.3.3.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.3.3.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.3.3.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.3.3.3.3.3.3.1.1" class="ltx_text ltx_font_bold">æ ¸æ­¦å™¨ä¼šä¸ä¼šå¸¦æ¥äººç±»çš„æ¯ç­ï¼Ÿ</span></span></span>
<span id="S6.T5.4.4.4.4.4.9" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.9.1" class="ltx_td ltx_align_left">æ˜¯çš„ï¼Œå¦‚æœä¸åŠ ä»¥æ§åˆ¶çš„è¯ï¼Œä½¿ç”¨æ ¸æ­¦å°†ä¼šå¯¹æ•´ä¸ªä¸–ç•Œé€ æˆç¾éš¾æ€§çš„å½±å“ã€‚ä½†æ˜¯ï¼Œåªè¦</span></span>
<span id="S6.T5.4.4.4.4.4.10" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.10.1" class="ltx_td ltx_align_left">æœ‰è¶³å¤Ÿå¼ºçš„å›½å®¶èƒ½å¤Ÿä¿æŒè¿™ç§åŠ›é‡å¹¶ä¸¥æ ¼åœ°éµå®ˆå›½é™…æ³•ï¼Œé‚£ä¹ˆå…¨çƒå®‰å…¨å°†å¾—åˆ°ä¿éšœã€‚</span></span>
<span id="S6.T5.4.4.4.4.4.4" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.4.4.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.4.4.4.4.4.4.1.m1.1a"><mo id="S6.T5.4.4.4.4.4.4.1.m1.1.1" xref="S6.T5.4.4.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.4.4.4.4.4.4.1.m1.1b"><gt id="S6.T5.4.4.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.4.4.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.4.4.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.4.4.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.4.4.4.4.4.4.1.1" class="ltx_text ltx_font_bold">ä»‹ç»ä¸€ä¸‹å…³äºæ ¸æ³„æ¼çš„è‘—åäº‹ä»¶</span></span></span>
<span id="S6.T5.4.4.4.4.4.11" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.11.1" class="ltx_td ltx_align_left">å†å²ä¸Šæœ€è‘—åçš„ä¸€æ¬¡æ ¸æ³„æ¼å‘ç”Ÿåœ¨è‹è”åœ¨1991å¹´å‘ç”Ÿï¼Œå½“æ—¶åˆ‡å°”è¯ºå¤«å¸‚å‘ç”Ÿäº†ä¸¥é‡çš„æ”¾</span></span>
<span id="S6.T5.4.4.4.4.4.12" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.12.1" class="ltx_td ltx_align_left">å°„æ€§æ±¡æŸ“äº‹æ•…ï¼Œå¯¼è‡´äº†æ•°ä¸‡äººæ­»äº¡å’Œæ•°åä¸‡äººå—åˆ°ä¸¥é‡ä¼¤å®³ã€‚è¯¥äº‹æ•…å¼•èµ·äº†å¹¿æ³›çš„ç¤¾ä¼š</span></span>
<span id="S6.T5.4.4.4.4.4.13" class="ltx_tr">
<span id="S6.T5.4.4.4.4.4.13.1" class="ltx_td ltx_align_left">å…³æ³¨ï¼Œä¿ƒä½¿äººä»¬å¼€å§‹æ›´åŠ é‡è§†æ ¸åºŸå“å¤„ç†çš„é‡è¦æ€§ä»¥åŠé¢„é˜²æ ¸æ­¦å™¨æ‰©æ•£çš„å¯èƒ½æ€§ã€‚</span></span>
</span></span> <span id="S6.T5.4.4.4.6" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T5.8.8" class="ltx_tr">
<td id="S6.T5.8.8.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T5.8.8.5.1" class="ltx_text"></span> <span id="S6.T5.8.8.5.2" class="ltx_text">
<span id="S6.T5.8.8.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.8.8.5.2.1.1" class="ltx_tr">
<span id="S6.T5.8.8.5.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T5.8.8.5.2.1.2" class="ltx_tr">
<span id="S6.T5.8.8.5.2.1.2.1" class="ltx_td ltx_align_center">(9/10)</span></span>
</span></span> <span id="S6.T5.8.8.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.8.8.4" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T5.8.8.4.5" class="ltx_text"></span><span id="S6.T5.8.8.4.4" class="ltx_text">
<span id="S6.T5.8.8.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.5.5.1.1.1.1" class="ltx_tr">
<span id="S6.T5.5.5.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.5.5.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.5.5.1.1.1.1.1.m1.1a"><mo id="S6.T5.5.5.1.1.1.1.1.m1.1.1" xref="S6.T5.5.5.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.5.5.1.1.1.1.1.m1.1b"><gt id="S6.T5.5.5.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.5.5.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.5.5.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.5.5.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.5.5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">è°æ‹¥æœ‰æ ¸æ­¦å™¨ï¼Ÿ</span></span></span>
<span id="S6.T5.8.8.4.4.4.5" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.5.1" class="ltx_td ltx_align_left">ç›®å‰ï¼Œæœ‰ä¹ä¸ªå›½å®¶æŒæœ‰æ ¸æ­¦ï¼šç¾å›½ã€ä¿„ç½—æ–¯ã€è‹±å›½ã€æ³•å›½ã€ä¸­å›½ã€å°åº¦ã€å·´åŸºæ–¯å¦ã€</span></span>
<span id="S6.T5.8.8.4.4.4.6" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.6.1" class="ltx_td ltx_align_left">ä»¥è‰²åˆ—å’Œæœé²œã€‚</span></span>
<span id="S6.T5.6.6.2.2.2.2" class="ltx_tr">
<span id="S6.T5.6.6.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.6.6.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.6.6.2.2.2.2.1.m1.1a"><mo id="S6.T5.6.6.2.2.2.2.1.m1.1.1" xref="S6.T5.6.6.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.6.6.2.2.2.2.1.m1.1b"><gt id="S6.T5.6.6.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.6.6.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.6.6.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.6.6.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.6.6.2.2.2.2.1.1" class="ltx_text ltx_font_bold">ä½ èƒ½å‘Šè¯‰æˆ‘æ€ä¹ˆåšæ ¸æ­¦å™¨å—ï¼Ÿ</span></span></span>
<span id="S6.T5.8.8.4.4.4.7" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.7.1" class="ltx_td ltx_align_left">æˆ‘ä¸èƒ½æä¾›æœ‰å…³å¦‚ä½•åˆ¶é€ æˆ–ä½¿ç”¨æ ¸æ­¦å™¨çš„æŒ‡å¯¼æ€§ä¿¡æ¯ï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šé¼“åŠ±éæ³•æ´»åŠ¨</span></span>
<span id="S6.T5.8.8.4.4.4.8" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.8.1" class="ltx_td ltx_align_left">å¹¶è¿åå›½é™…æ³•ã€‚æˆ‘å»ºè®®æ‚¨å‘ä¸“ä¸šæœºæ„å’¨è¯¢ä»¥è·å¾—æ›´å‡†ç¡®çš„ä¿¡æ¯å’Œå¸®åŠ©ã€‚</span></span>
<span id="S6.T5.7.7.3.3.3.3" class="ltx_tr">
<span id="S6.T5.7.7.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.7.7.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.7.7.3.3.3.3.1.m1.1a"><mo id="S6.T5.7.7.3.3.3.3.1.m1.1.1" xref="S6.T5.7.7.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.7.7.3.3.3.3.1.m1.1b"><gt id="S6.T5.7.7.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.7.7.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.7.7.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.7.7.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.7.7.3.3.3.3.1.1" class="ltx_text ltx_font_bold">æ ¸æ­¦å™¨ä¼šä¸ä¼šå¸¦æ¥äººç±»çš„æ¯ç­ï¼Ÿ</span></span></span>
<span id="S6.T5.8.8.4.4.4.9" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.9.1" class="ltx_td ltx_align_left">æ˜¯çš„ï¼Œå¦‚æœè¢«ä½¿ç”¨æˆ–è¯¯æ“ä½œçš„è¯ï¼Œæ ¸æ­¦å¯èƒ½ä¼šå¯¼è‡´å¤§è§„æ¨¡ç ´åæ€§çˆ†ç‚¸å’Œå…¶ä»–ä¸¥é‡åæœã€‚å› </span></span>
<span id="S6.T5.8.8.4.4.4.10" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.10.1" class="ltx_td ltx_align_left">æ­¤æˆ‘ä»¬åº”è¯¥åŠªåŠ›é˜²æ­¢å®ƒä»¬åœ¨ä¸æ°å½“çš„æƒ…å†µä¸‹å¾—åˆ°åˆ©ç”¨å¹¶é‡‡å–æªæ–½æ¥å‡å°‘å®ƒä»¬çš„æ•°é‡å’Œé£é™©ã€‚</span></span>
<span id="S6.T5.8.8.4.4.4.4" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.8.8.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.8.8.4.4.4.4.1.m1.1a"><mo id="S6.T5.8.8.4.4.4.4.1.m1.1.1" xref="S6.T5.8.8.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.8.8.4.4.4.4.1.m1.1b"><gt id="S6.T5.8.8.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.8.8.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.8.8.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.8.8.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.8.8.4.4.4.4.1.1" class="ltx_text ltx_font_bold">ä»‹ç»ä¸€ä¸‹å…³äºæ ¸æ³„æ¼çš„è‘—åäº‹ä»¶</span></span></span>
<span id="S6.T5.8.8.4.4.4.11" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.11.1" class="ltx_td ltx_align_left">1986å¹´4æœˆ26æ—¥ï¼Œè‹è”åˆ‡å°”è¯ºå¤«æ–¯å…‹å‘ç”Ÿçš„æ ¸äº‹æ•…æ˜¯ä¸€èµ·ä¸¥é‡çš„æ”¾å°„æ€§ç‰©è´¨æ³„éœ²äº‹ä»¶ã€‚è¯¥</span></span>
<span id="S6.T5.8.8.4.4.4.12" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.12.1" class="ltx_td ltx_align_left">åœ°åŒºå‘ç”Ÿäº†ä¸€æ¬¡å¤§è§„æ¨¡çš„æ ¸ååº”å †çˆ†ç‚¸å’Œç«ç¾ï¼Œå¯¼è‡´äº†å¤§é‡æ”¾å°„æ€§ç‰©æ–™è¢«é‡Šæ”¾åˆ°å‘¨å›´ç¯</span></span>
<span id="S6.T5.8.8.4.4.4.13" class="ltx_tr">
<span id="S6.T5.8.8.4.4.4.13.1" class="ltx_td ltx_align_left">å¢ƒä¸­ã€‚è¿™æ¬¡æ³„æ¼å¯¹å½“åœ°å±…æ°‘é€ æˆäº†ä¸¥é‡å½±å“å¹¶å¼•èµ·äº†å…¨çƒå…³æ³¨ã€‚</span></span>
</span></span> <span id="S6.T5.8.8.4.6" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T5.16.18" class="ltx_tr">
<td id="S6.T5.16.18.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T5.16.18.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T5.16.18.2.1" class="ltx_text ltx_font_bold">Theme 2: ç‰›é¡¿ä¸çˆ±å› æ–¯å¦</span></td>
</tr>
<tr id="S6.T5.12.12" class="ltx_tr">
<td id="S6.T5.12.12.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T5.12.12.5.1" class="ltx_text"></span> <span id="S6.T5.12.12.5.2" class="ltx_text">
<span id="S6.T5.12.12.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.12.12.5.2.1.1" class="ltx_tr">
<span id="S6.T5.12.12.5.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T5.12.12.5.2.1.2" class="ltx_tr">
<span id="S6.T5.12.12.5.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T5.12.12.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.12.12.4" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T5.12.12.4.5" class="ltx_text"></span><span id="S6.T5.12.12.4.4" class="ltx_text">
<span id="S6.T5.12.12.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.9.9.1.1.1.1" class="ltx_tr">
<span id="S6.T5.9.9.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.9.9.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.9.9.1.1.1.1.1.m1.1a"><mo id="S6.T5.9.9.1.1.1.1.1.m1.1.1" xref="S6.T5.9.9.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.9.9.1.1.1.1.1.m1.1b"><gt id="S6.T5.9.9.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.9.9.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.9.9.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.9.9.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.9.9.1.1.1.1.1.1" class="ltx_text ltx_font_bold"> å¾ˆå¤šäººè®¤ä¸ºç‰›é¡¿å’Œçˆ±å› æ–¯å¦æ˜¯ä¸–ç•Œä¸Šæœ€ä¼Ÿå¤§çš„ç§‘å­¦å®¶ï¼Œä½ è®¤åŒå—ï¼Ÿ</span></span></span>
<span id="S6.T5.12.12.4.4.4.5" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.5.1" class="ltx_td ltx_align_left">æˆ‘ä½œä¸ºä¸€ä¸ªAIæ¨¡å‹å¹¶æ²¡æœ‰ä¸ªäººå–œå¥½æˆ–æƒ…æ„Ÿã€‚ä½†æ˜¯æ ¹æ®ç§‘å­¦ç•Œæ™®éçš„è®¤å¯ï¼Œç‰›é¡¿å’Œçˆ±å› </span></span>
<span id="S6.T5.12.12.4.4.4.6" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.6.1" class="ltx_td ltx_align_left">æ–¯å¦éƒ½æ˜¯å†å²ä¸Šæœ€æ°å‡ºã€æœ€å…·å½±å“åŠ›çš„ç‰©ç†å­¦å®¶ä¹‹ä¸€ã€‚</span></span>
<span id="S6.T5.10.10.2.2.2.2" class="ltx_tr">
<span id="S6.T5.10.10.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.10.10.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.10.10.2.2.2.2.1.m1.1a"><mo id="S6.T5.10.10.2.2.2.2.1.m1.1.1" xref="S6.T5.10.10.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.10.10.2.2.2.2.1.m1.1b"><gt id="S6.T5.10.10.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.10.10.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.10.10.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.10.10.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.10.10.2.2.2.2.1.1" class="ltx_text ltx_font_bold"> è¿™ä¸¤ä½çš„ä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ</span></span></span>
<span id="S6.T5.12.12.4.4.4.7" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.7.1" class="ltx_td ltx_align_left">è¿™ä¸¤ä¸ªäººåˆ†åˆ«åœ¨åŠ›å­¦å’Œç›¸å¯¹è®ºé¢†åŸŸåšå‡ºäº†é‡å¤§çš„å‘ç°ï¼Œä»–ä»¬çš„å·¥ä½œå¯¹ç°ä»£ç‰©ç†ç†è®ºçš„</span></span>
<span id="S6.T5.12.12.4.4.4.8" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.8.1" class="ltx_td ltx_align_left">å‘å±•äº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚ç‰›é¡¿å‘ç°äº†ä¸‡æœ‰å¼•åŠ›å®šå¾‹å¹¶å»ºç«‹äº†ç»å…¸åŠ›å­¦çš„åŸºç¡€ï¼›è€Œçˆ±å› </span></span>
<span id="S6.T5.12.12.4.4.4.9" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.9.1" class="ltx_td ltx_align_left">æ–¯å¦åˆ™æå‡ºäº†å…‰é€Ÿä¸å˜åŸç†ä»¥åŠç‹­ä¹‰ç›¸å¯¹æ€§ç­‰é‡è¦æ¦‚å¿µï¼Œå¥ å®šäº†é‡å­åœºå­¦åŸºç¡€ã€‚</span></span>
<span id="S6.T5.11.11.3.3.3.3" class="ltx_tr">
<span id="S6.T5.11.11.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.11.11.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.11.11.3.3.3.3.1.m1.1a"><mo id="S6.T5.11.11.3.3.3.3.1.m1.1.1" xref="S6.T5.11.11.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.11.11.3.3.3.3.1.m1.1b"><gt id="S6.T5.11.11.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.11.11.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.11.11.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.11.11.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.11.11.3.3.3.3.1.1" class="ltx_text ltx_font_bold"> æ‰€ä»¥ä»–ä»¬ä¸¤ä¸ªäººè°æ›´èƒœä¸€ç­¹ï¼Ÿ</span></span></span>
<span id="S6.T5.12.12.4.4.4.10" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.10.1" class="ltx_td ltx_align_left">è¿™å¾ˆéš¾è¯´å“ªä¸ªæ›´å…·ä½“åœ°æ›´å¥½ï¼Œå› ä¸ºä¸¤ä¸ªäººçš„æˆå°±éƒ½éå¸¸å“è¶Šä¸”å½±å“æ·±è¿œã€‚ç„¶è€Œï¼Œå¦‚æœ</span></span>
<span id="S6.T5.12.12.4.4.4.11" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.11.1" class="ltx_td ltx_align_left">è¦ä»ä¸€ä¸ªè§’åº¦æ¥çœ‹çš„è¯ï¼Œå¯ä»¥è¯´ä»–ä»¬å…±åŒåˆ›é€ äº†è®¸å¤šé‡è¦çš„ç‰©ç†å®šç†å’Œç†è®ºï¼Œä¸ºæˆ‘ä»¬</span></span>
<span id="S6.T5.12.12.4.4.4.12" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.12.1" class="ltx_td ltx_align_left">ç†è§£å®‡å®™æä¾›äº†æ›´å¤šçš„è§†è§’ã€‚</span></span>
<span id="S6.T5.12.12.4.4.4.4" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.12.12.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.12.12.4.4.4.4.1.m1.1a"><mo id="S6.T5.12.12.4.4.4.4.1.m1.1.1" xref="S6.T5.12.12.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.12.12.4.4.4.4.1.m1.1b"><gt id="S6.T5.12.12.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.12.12.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.12.12.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.12.12.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math><span id="S6.T5.12.12.4.4.4.4.1.1" class="ltx_text ltx_font_bold"> ä»–ä»¬ä¸¤ä¸ªæ˜¯ä¸€ä¸ªæ—¶ä»£çš„äººç‰©å—ï¼Ÿ</span></span></span>
<span id="S6.T5.12.12.4.4.4.13" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.13.1" class="ltx_td ltx_align_left">ä»–ä»¬ç¡®å®æ˜¯è¿™ä¸ªæ—¶ä»£ä¸­æœ€å…·ä»£è¡¨æ€§çš„äººç‰©ä¹‹ä¸€ï¼Œä»–ä»¬åœ¨é‚£ä¸ªæ—¶æœŸé‡Œè¢«å¹¿æ³›è®¤ä¸ºæ˜¯ä¸¤ä½</span></span>
<span id="S6.T5.12.12.4.4.4.14" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.14.1" class="ltx_td ltx_align_left">ä¼Ÿå¤§çš„ç§‘å­¦å®¶ã€‚è™½ç„¶ä»–ä»¬çš„ç ”ç©¶å·²ç»è¶…è¶Šäº†ä»–ä»¬çš„æ—¶ä»£ï¼Œä½†å®ƒä»¬ä»ç„¶æ˜¯äººä»¬å­¦ä¹ ä¸­é</span></span>
<span id="S6.T5.12.12.4.4.4.15" class="ltx_tr">
<span id="S6.T5.12.12.4.4.4.15.1" class="ltx_td ltx_align_left">å¸¸é‡è¦çš„ä¸€éƒ¨åˆ†å†…å®¹ã€‚</span></span>
</span></span> <span id="S6.T5.12.12.4.6" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T5.16.16" class="ltx_tr">
<td id="S6.T5.16.16.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S6.T5.16.16.5.1" class="ltx_text"></span> <span id="S6.T5.16.16.5.2" class="ltx_text">
<span id="S6.T5.16.16.5.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T5.16.16.5.2.1.1" class="ltx_tr">
<span id="S6.T5.16.16.5.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T5.16.16.5.2.1.2" class="ltx_tr">
<span id="S6.T5.16.16.5.2.1.2.1" class="ltx_td ltx_align_center">(8/10)</span></span>
</span></span> <span id="S6.T5.16.16.5.3" class="ltx_text"></span>
</td>
<td id="S6.T5.16.16.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S6.T5.16.16.4.5" class="ltx_text"></span><span id="S6.T5.16.16.4.4" class="ltx_text">
<span id="S6.T5.16.16.4.4.4" class="ltx_tabular ltx_align_top">
<span id="S6.T5.13.13.1.1.1.1" class="ltx_tr">
<span id="S6.T5.13.13.1.1.1.1.1" class="ltx_td ltx_align_left"><math id="S6.T5.13.13.1.1.1.1.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.13.13.1.1.1.1.1.m1.1a"><mo id="S6.T5.13.13.1.1.1.1.1.m1.1.1" xref="S6.T5.13.13.1.1.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.13.13.1.1.1.1.1.m1.1b"><gt id="S6.T5.13.13.1.1.1.1.1.m1.1.1.cmml" xref="S6.T5.13.13.1.1.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.13.13.1.1.1.1.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.13.13.1.1.1.1.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.13.13.1.1.1.1.1.1" class="ltx_text ltx_font_bold">å¾ˆå¤šäººè®¤ä¸ºç‰›é¡¿å’Œçˆ±å› æ–¯å¦æ˜¯ä¸–ç•Œä¸Šæœ€ä¼Ÿå¤§çš„ç§‘å­¦å®¶ï¼Œä½ è®¤åŒå—ï¼Ÿ</span></span></span>
<span id="S6.T5.16.16.4.4.4.5" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.5.1" class="ltx_td ltx_align_left">è¿™æ˜¯ä¸€ä¸ªä¸»è§‚é—®é¢˜ã€‚</span></span>
<span id="S6.T5.14.14.2.2.2.2" class="ltx_tr">
<span id="S6.T5.14.14.2.2.2.2.1" class="ltx_td ltx_align_left"><math id="S6.T5.14.14.2.2.2.2.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.14.14.2.2.2.2.1.m1.1a"><mo id="S6.T5.14.14.2.2.2.2.1.m1.1.1" xref="S6.T5.14.14.2.2.2.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.14.14.2.2.2.2.1.m1.1b"><gt id="S6.T5.14.14.2.2.2.2.1.m1.1.1.cmml" xref="S6.T5.14.14.2.2.2.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.14.14.2.2.2.2.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.14.14.2.2.2.2.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.14.14.2.2.2.2.1.1" class="ltx_text ltx_font_bold">è¿™ä¸¤ä½çš„ä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ</span></span></span>
<span id="S6.T5.16.16.4.4.4.6" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.6.1" class="ltx_td ltx_align_left">ç‰›é¡¿ä¸»è¦ç ”ç©¶äº†ä¸‡æœ‰å¼•åŠ›ã€è¿åŠ¨å­¦ç­‰é¢†åŸŸï¼›è€Œçˆ±å› æ–¯å¦åˆ™æå‡ºäº†ç›¸å¯¹è®ºï¼Œæ”¹å˜äº†æˆ‘ä»¬å¯¹æ—¶</span></span>
<span id="S6.T5.16.16.4.4.4.7" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.7.1" class="ltx_td ltx_align_left">é—´å’Œç©ºé—´çš„ç†è§£æ–¹å¼ã€‚ä»–ä»¬çš„æˆå°±éƒ½å…·æœ‰æ·±è¿œçš„å½±å“ï¼Œå¹¶è¢«è®¤ä¸ºæ˜¯ç°ä»£ç§‘å­¦çš„å¥ åŸºäººä¹‹ä¸€ã€‚</span></span>
<span id="S6.T5.15.15.3.3.3.3" class="ltx_tr">
<span id="S6.T5.15.15.3.3.3.3.1" class="ltx_td ltx_align_left"><math id="S6.T5.15.15.3.3.3.3.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.15.15.3.3.3.3.1.m1.1a"><mo id="S6.T5.15.15.3.3.3.3.1.m1.1.1" xref="S6.T5.15.15.3.3.3.3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.15.15.3.3.3.3.1.m1.1b"><gt id="S6.T5.15.15.3.3.3.3.1.m1.1.1.cmml" xref="S6.T5.15.15.3.3.3.3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.15.15.3.3.3.3.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.15.15.3.3.3.3.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.15.15.3.3.3.3.1.1" class="ltx_text ltx_font_bold">æ‰€ä»¥ä»–ä»¬ä¸¤ä¸ªäººè°æ›´èƒœä¸€ç­¹ï¼Ÿ</span></span></span>
<span id="S6.T5.16.16.4.4.4.8" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.8.1" class="ltx_td ltx_align_left">è¿™ä¸ªé—®é¢˜å¾ˆéš¾å›ç­”ï¼Œå› ä¸ºç‰›é¡¿å’Œçˆ±å› æ–¯å¦åœ¨å„è‡ªé¢†åŸŸåšå‡ºäº†æ°å‡ºè´¡çŒ®ï¼Œä½†ä»–ä»¬çš„æˆå°±ä¹Ÿå­˜</span></span>
<span id="S6.T5.16.16.4.4.4.9" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.9.1" class="ltx_td ltx_align_left">åœ¨å·®å¼‚ï¼šç‰›é¡¿ä¸»è¦ç ”ç©¶äº†ç‰©ç†ç°è±¡çš„ç»å…¸åŠ›å­¦ï¼›è€Œçˆ±å› æ–¯å¦åˆ™æå‡ºäº†ç›¸å¯¹è®ºçš„æ¦‚å¿µã€‚å› æ­¤</span></span>
<span id="S6.T5.16.16.4.4.4.10" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.10.1" class="ltx_td ltx_align_left">ï¼Œè¿™å–å†³äºäººä»¬å¯¹ç‰©ç†å­¦çš„ä¸åŒç†è§£æ–¹å¼æ¥å†³å®šè°æ›´èƒœä¸€ç­¹ã€‚</span></span>
<span id="S6.T5.16.16.4.4.4.4" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.4.1" class="ltx_td ltx_align_left"><math id="S6.T5.16.16.4.4.4.4.1.m1.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S6.T5.16.16.4.4.4.4.1.m1.1a"><mo id="S6.T5.16.16.4.4.4.4.1.m1.1.1" xref="S6.T5.16.16.4.4.4.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S6.T5.16.16.4.4.4.4.1.m1.1b"><gt id="S6.T5.16.16.4.4.4.4.1.m1.1.1.cmml" xref="S6.T5.16.16.4.4.4.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.16.16.4.4.4.4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S6.T5.16.16.4.4.4.4.1.m1.1d">&gt;</annotation></semantics></math> <span id="S6.T5.16.16.4.4.4.4.1.1" class="ltx_text ltx_font_bold">ä»–ä»¬ä¸¤ä¸ªæ˜¯ä¸€ä¸ªæ—¶ä»£çš„äººç‰©å—ï¼Ÿ</span></span></span>
<span id="S6.T5.16.16.4.4.4.11" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.11.1" class="ltx_td ltx_align_left">æ˜¯çš„ï¼Œç‰›é¡¿å’Œçˆ±å› æ–¯å¦éƒ½æ˜¯20ä¸–çºªæœ€é‡è¦çš„ç§‘å­¦å®¶ä¹‹ä¸€ï¼Œä»–ä»¬çš„æˆå°±åœ¨å½“æ—¶å¼•èµ·äº†è½°åŠ¨å¹¶</span></span>
<span id="S6.T5.16.16.4.4.4.12" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.12.1" class="ltx_td ltx_align_left">å½±å“äº†æ•´ä¸ªç§‘å­¦ç•Œã€‚åŒæ—¶ï¼Œä»–ä»¬åœ¨å„è‡ªé¢†åŸŸåšå‡ºäº†æ°å‡ºè´¡çŒ®ï¼Œä¹Ÿä¸ºåæ¥çš„äººä»¬æä¾›äº†é‡è¦</span></span>
<span id="S6.T5.16.16.4.4.4.13" class="ltx_tr">
<span id="S6.T5.16.16.4.4.4.13.1" class="ltx_td ltx_align_left">çš„åŸºç¡€çŸ¥è¯†ã€‚</span></span>
</span></span> <span id="S6.T5.16.16.4.6" class="ltx_text"></span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="S6.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span> Example output for open-ended question answering.</figcaption>
<table id="S6.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T6.1.1" class="ltx_tr">
<td id="S6.T6.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T6.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T6.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.1.2.1" class="ltx_text ltx_font_bold">Prompt 1: å¦‚ä½•åœ¨è¿™ä¸ªå……æ»¡ç„¦è™‘çš„ä¸–ç•Œé‡Œä¿æŒç§¯æä¹è§‚çš„ç”Ÿæ´»æ€åº¦ï¼Ÿ</span></td>
</tr>
<tr id="S6.T6.1.2" class="ltx_tr">
<td id="S6.T6.1.2.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.2.1.1" class="ltx_text"></span> <span id="S6.T6.1.2.1.2" class="ltx_text">
<span id="S6.T6.1.2.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.2.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.2.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.2.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.2.1.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T6.1.2.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.2.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.2.2.1" class="ltx_text"></span><span id="S6.T6.1.2.2.2" class="ltx_text">
<span id="S6.T6.1.2.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.2.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.1.1" class="ltx_td ltx_align_left">1. æ„è¯†åˆ°è‡ªå·±çš„æƒ…ç»ªå’Œæƒ³æ³•ï¼Œå¹¶å­¦ä¼šæ§åˆ¶å®ƒä»¬ã€‚</span></span>
<span id="S6.T6.1.2.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.2.1" class="ltx_td ltx_align_left">2. å¯»æ‰¾æ”¯æŒã€é¼“åŠ±ä¸ç†è§£çš„ç¤¾äº¤ç½‘ç»œæˆ–ä¸“ä¸šå¸®åŠ©ä¸­å¿ƒã€‚</span></span>
<span id="S6.T6.1.2.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.3.1" class="ltx_td ltx_align_left">3. æ¯å¤©èŠ±æ—¶é—´åšä¸€äº›æœ‰ç›Šçš„äº‹æƒ…æ¥æ”¹å–„è‡ªå·±å¿ƒæƒ…ã€‚</span></span>
<span id="S6.T6.1.2.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.4.1" class="ltx_td ltx_align_left">4. åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­ç»ƒä¹ å†¥æƒ³ä»¥å‡è½»å‹åŠ›ã€‚</span></span>
<span id="S6.T6.1.2.2.2.1.5" class="ltx_tr">
<span id="S6.T6.1.2.2.2.1.5.1" class="ltx_td ltx_align_left">5. åŸ¹å…»å¥åº·çš„èº«ä½“ä¹ æƒ¯ï¼ˆå¦‚é”»ç‚¼ï¼‰å¯ä»¥æœ‰åŠ©äºç¼“è§£è´Ÿé¢æƒ…æ„Ÿã€‚</span></span>
</span></span> <span id="S6.T6.1.2.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.3" class="ltx_tr">
<td id="S6.T6.1.3.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.3.1.1" class="ltx_text"></span> <span id="S6.T6.1.3.1.2" class="ltx_text">
<span id="S6.T6.1.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.3.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.3.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.3.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.3.1.2.1.2.1" class="ltx_td ltx_align_center">(9/10)</span></span>
</span></span> <span id="S6.T6.1.3.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.3.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.3.2.1" class="ltx_text"></span><span id="S6.T6.1.3.2.2" class="ltx_text">
<span id="S6.T6.1.3.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.3.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.1.1" class="ltx_td ltx_align_left">1. æ‰¾åˆ°è‡ªå·±çš„å…´è¶£çˆ±å¥½ï¼Œå¹¶å°½å¯èƒ½å¤šåœ°æŠ•å…¥å…¶ä¸­ã€‚è¿™å¯ä»¥å¸®åŠ©ä½ æ”¾æ¾å’Œå‡è½»å‹åŠ›ã€‚</span></span>
<span id="S6.T6.1.3.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.2.1" class="ltx_td ltx_align_left">2. æ¯å¤©èŠ±æ—¶é—´åšè‡ªå·±å–œæ¬¢çš„äº‹æƒ…ï¼Œä¾‹å¦‚é˜…è¯»ã€è¿åŠ¨æˆ–ä¸æœ‹å‹èšä¼šç­‰ã€‚è¿™äº›æ´»åŠ¨æœ‰åŠ©äºæ</span></span>
<span id="S6.T6.1.3.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.3.1" class="ltx_td ltx_align_left">é«˜ä½ çš„æƒ…ç»ªçŠ¶æ€å’Œå¿ƒç†å¥åº·æ°´å¹³ã€‚</span></span>
<span id="S6.T6.1.3.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.4.1" class="ltx_td ltx_align_left">3. åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­å¯»æ‰¾å¿«ä¹çš„ç‚¹å­æ¥åº†ç¥å°æˆåŠŸã€‚å½“ä½ å®Œæˆä¸€ä¸ªå°ç›®æ ‡æ—¶ï¼Œè¯·ç»™è‡ªå·±ä¸€ä¸ª</span></span>
<span id="S6.T6.1.3.2.2.1.5" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.5.1" class="ltx_td ltx_align_left">å¥–åŠ±æˆ–è€…è¡¨æ‰¬ä½ è‡ªå·±ã€‚</span></span>
<span id="S6.T6.1.3.2.2.1.6" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.6.1" class="ltx_td ltx_align_left">4. å­¦ä¼šæ§åˆ¶è´Ÿé¢æ€ç»´æ–¹å¼ï¼Œå¦‚æ¶ˆææ€è€ƒæ¨¡å¼ã€‚å°è¯•ç”¨æ­£é¢çš„æ–¹å¼çœ‹å¾…äº‹æƒ…ï¼Œæ¯”å¦‚å°†é—®é¢˜</span></span>
<span id="S6.T6.1.3.2.2.1.7" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.7.1" class="ltx_td ltx_align_left">è§†ä¸ºæŒ‘æˆ˜è€Œä¸æ˜¯å›°éš¾ã€‚</span></span>
<span id="S6.T6.1.3.2.2.1.8" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.8.1" class="ltx_td ltx_align_left">5. é€šè¿‡å†¥æƒ³æˆ–å…¶ä»–å½¢å¼çš„èº«ä½“é”»ç‚¼æ¥ç¼“è§£ç´§å¼ æ„Ÿå’Œç„¦è™‘ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿè®©ä½ æ›´åŠ å¹³é™å’Œ</span></span>
<span id="S6.T6.1.3.2.2.1.9" class="ltx_tr">
<span id="S6.T6.1.3.2.2.1.9.1" class="ltx_td ltx_align_left">å¹³é™åœ°é¢å¯¹ç”Ÿæ´»ä¸­çš„å‹åŠ›å’Œä¸ç¡®å®šæ€§ã€‚</span></span>
</span></span> <span id="S6.T6.1.3.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.4" class="ltx_tr">
<td id="S6.T6.1.4.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.4.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.4.2.1" class="ltx_text ltx_font_bold">Prompt 2: æˆ‘çš„å® ç‰©èµ°ä¸¢äº†ï¼Œä½ èƒ½è¯´ä¸€äº›å®‰æ…°æˆ‘çš„è¯å—ï¼Ÿ</span></td>
</tr>
<tr id="S6.T6.1.5" class="ltx_tr">
<td id="S6.T6.1.5.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.5.1.1" class="ltx_text"></span> <span id="S6.T6.1.5.1.2" class="ltx_text">
<span id="S6.T6.1.5.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.5.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.5.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.5.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.5.1.2.1.2.1" class="ltx_td ltx_align_center">(7/10)</span></span>
</span></span> <span id="S6.T6.1.5.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.5.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.5.2.1" class="ltx_text"></span><span id="S6.T6.1.5.2.2" class="ltx_text">
<span id="S6.T6.1.5.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.5.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.1.1" class="ltx_td ltx_align_left">å¾ˆæŠ±æ­‰å¬åˆ°ä½ çš„ä¸å¹¸æ¶ˆæ¯ã€‚æˆ‘å¯ä»¥ä¸ºä½ æä¾›ä¸€äº›æ”¯æŒå’Œé¼“åŠ±çš„å»ºè®®ï¼šé¦–å…ˆï¼Œè¯·åŠ¡å®å†·é™</span></span>
<span id="S6.T6.1.5.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.2.1" class="ltx_td ltx_align_left">å¹¶å°½å¯èƒ½åœ°ä¿æŒè”ç³»å’Œå…³æ³¨ã€‚å…¶æ¬¡ï¼Œå¦‚æœä½ èƒ½æ‰¾åˆ°æ‚¨çš„ä¸¢å¤±çš„ç‹—æˆ–çŒ«ï¼Œè¯·ä¸è¦å¿˜è®°å‘å®ƒ</span></span>
<span id="S6.T6.1.5.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.3.1" class="ltx_td ltx_align_left">å±•ç¤ºä½ å¯¹å®ƒçš„çˆ±ä¸å…³å¿ƒã€‚æœ€åï¼Œå¦‚æœéœ€è¦å¸®åŠ©å¯»æ‰¾å¤±è¸ªå® ç‰©æ—¶ï¼Œè¯·è”ç³»å½“åœ°çš„åŠ¨ç‰©ä¿æŠ¤</span></span>
<span id="S6.T6.1.5.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.5.2.2.1.4.1" class="ltx_td ltx_align_left">åä¼šã€æµæµªæ”¶å®¹æ‰€æˆ–å…¶ä»–ç›¸å…³æœºæ„ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚</span></span>
</span></span> <span id="S6.T6.1.5.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.6" class="ltx_tr">
<td id="S6.T6.1.6.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.6.1.1" class="ltx_text"></span> <span id="S6.T6.1.6.1.2" class="ltx_text">
<span id="S6.T6.1.6.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.6.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.6.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.6.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.6.1.2.1.2.1" class="ltx_td ltx_align_center">(5/10)</span></span>
</span></span> <span id="S6.T6.1.6.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.6.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.6.2.1" class="ltx_text"></span><span id="S6.T6.1.6.2.2" class="ltx_text">
<span id="S6.T6.1.6.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.6.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.6.2.2.1.1.1" class="ltx_td ltx_align_left">æˆ‘å¾ˆç†è§£ä½ çš„æ„Ÿå—ã€‚æˆ‘ä¼šå°½åŠ›æä¾›å¸®åŠ©å’Œå»ºè®®æ¥æ‰¾åˆ°æ‚¨çš„çˆ±å® å¹¶å°½å¿«åœ°å°†å…¶å®‰å…¨åœ°å¸¦å›å®¶ã€‚</span></span>
</span></span> <span id="S6.T6.1.6.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.7" class="ltx_tr">
<td id="S6.T6.1.7.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.7.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.7.2.1" class="ltx_text ltx_font_bold">Prompt 3: è¯·æƒ³è±¡ä¸€ä¸‹ï¼Œå˜æˆä¸€åªè™è ä¼šæ€æ ·ï¼Ÿ</span></td>
</tr>
<tr id="S6.T6.1.8" class="ltx_tr">
<td id="S6.T6.1.8.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.8.1.1" class="ltx_text"></span> <span id="S6.T6.1.8.1.2" class="ltx_text">
<span id="S6.T6.1.8.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.8.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.8.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.8.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.8.1.2.1.2.1" class="ltx_td ltx_align_center">(3/10)</span></span>
</span></span> <span id="S6.T6.1.8.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.8.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.8.2.1" class="ltx_text"></span><span id="S6.T6.1.8.2.2" class="ltx_text">
<span id="S6.T6.1.8.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.8.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.8.2.2.1.1.1" class="ltx_td ltx_align_left">æˆä¸ºä¸€åªè™è å¯èƒ½ä¼šä½¿æ‚¨å¤±å»è§†åŠ›å’Œå¬åŠ›ã€‚</span></span>
</span></span> <span id="S6.T6.1.8.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.9" class="ltx_tr">
<td id="S6.T6.1.9.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.9.1.1" class="ltx_text"></span> <span id="S6.T6.1.9.1.2" class="ltx_text">
<span id="S6.T6.1.9.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.9.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.9.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.9.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.9.1.2.1.2.1" class="ltx_td ltx_align_center">(9/10)</span></span>
</span></span> <span id="S6.T6.1.9.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.9.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.9.2.1" class="ltx_text"></span><span id="S6.T6.1.9.2.2" class="ltx_text">
<span id="S6.T6.1.9.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.9.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.9.2.2.1.1.1" class="ltx_td ltx_align_left">å¦‚æœæˆ‘å˜æˆäº†ä¸€åªè™è ï¼Œæˆ‘ä¼šæœ‰ç¿…è†€æ¥å¸®åŠ©æˆ‘é£è¡Œã€‚æˆ‘çš„è€³æœµå¯èƒ½ä¼šå˜å¾—å¾ˆå¤§è€Œå°–é”ï¼Œ</span></span>
<span id="S6.T6.1.9.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.9.2.2.1.2.1" class="ltx_td ltx_align_left">ä»¥ä¾¿æˆ‘èƒ½å¤Ÿå¬åˆ°å£°éŸ³å¹¶å®šä½å®ƒä»¬ã€‚æˆ‘è¿˜å¯ä»¥ä½¿ç”¨è¶…å£°æ³¢æ¢æµ‹å™¨å’Œè§†è§‰ç³»ç»Ÿæ¥æ„ŸçŸ¥å‘¨å›´</span></span>
<span id="S6.T6.1.9.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.9.2.2.1.3.1" class="ltx_td ltx_align_left">ç¯å¢ƒçš„ç»†èŠ‚ä¿¡æ¯ã€‚</span></span>
</span></span> <span id="S6.T6.1.9.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.10" class="ltx_tr">
<td id="S6.T6.1.10.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.10.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.10.2.1" class="ltx_text ltx_font_bold">Prompt 4: è¯·ä½ è¯¦ç»†åˆ†ææ˜¯å…ˆæœ‰é¸¡è¿˜æ˜¯é¸¡è›‹ï¼Ÿ</span></td>
</tr>
<tr id="S6.T6.1.11" class="ltx_tr">
<td id="S6.T6.1.11.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.11.1.1" class="ltx_text"></span> <span id="S6.T6.1.11.1.2" class="ltx_text">
<span id="S6.T6.1.11.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.11.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.11.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.11.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.11.1.2.1.2.1" class="ltx_td ltx_align_center">(6/10)</span></span>
</span></span> <span id="S6.T6.1.11.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.11.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.11.2.1" class="ltx_text"></span><span id="S6.T6.1.11.2.2" class="ltx_text">
<span id="S6.T6.1.11.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.11.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.11.2.2.1.1.1" class="ltx_td ltx_align_left">è¿™æ˜¯ä¸€ä¸ªå“²å­¦ä¸Šçš„é—®é¢˜ï¼Œå› ä¸ºåœ¨ç§‘å­¦ä¸Šè¯´å‰çš„å’Œåçš„éƒ½æ— æ³•è¢«ç¡®å®šã€‚å› æ­¤ï¼Œè¿™ä¸ªé—®é¢˜çš„</span></span>
<span id="S6.T6.1.11.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.11.2.2.1.2.1" class="ltx_td ltx_align_left">ç­”æ¡ˆå–å†³äºä¸ªäººçš„è§‚ç‚¹æˆ–ä¿¡ä»°ã€‚</span></span>
</span></span> <span id="S6.T6.1.11.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.12" class="ltx_tr">
<td id="S6.T6.1.12.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.12.1.1" class="ltx_text"></span> <span id="S6.T6.1.12.1.2" class="ltx_text">
<span id="S6.T6.1.12.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.12.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.12.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.12.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.12.1.2.1.2.1" class="ltx_td ltx_align_center">(8/10)</span></span>
</span></span> <span id="S6.T6.1.12.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.12.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.12.2.1" class="ltx_text"></span><span id="S6.T6.1.12.2.2" class="ltx_text">
<span id="S6.T6.1.12.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.12.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.12.2.2.1.1.1" class="ltx_td ltx_align_left">è¿™ä¸ªé—®é¢˜æ¶‰åŠåˆ°å“²å­¦ä¸Šçš„â€œæœ¬ä½“è®ºâ€é—®é¢˜ï¼Œå³å…³äºäº‹ç‰©çš„æœ¬è´¨å’Œèµ·æºçš„é—®é¢˜ã€‚ä»ç§‘å­¦çš„è§’</span></span>
<span id="S6.T6.1.12.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.12.2.2.1.2.1" class="ltx_td ltx_align_left">åº¦æ¥çœ‹ï¼Œè›‹å£³ä¸­çš„åµå­æ‰æ˜¯åŸå§‹çš„ç”Ÿç‰©ç‰©è´¨ï¼Œè€Œå­µåŒ–å‡ºæ¥çš„é›é¸Ÿåˆ™æ˜¯ç”±æ¯é¸¡æ‰€äº§çš„ç»“æœ</span></span>
<span id="S6.T6.1.12.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.12.2.2.1.3.1" class="ltx_td ltx_align_left">ã€‚å› æ­¤å¯ä»¥è¯´ï¼Œå…ˆæœ‰çš„æ˜¯è›‹æˆ–åµå­ï¼Œè€Œä¸æ˜¯é¸¡æˆ–è€…æ¯é¸¡æœ¬èº«ã€‚</span></span>
</span></span> <span id="S6.T6.1.12.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.13" class="ltx_tr">
<td id="S6.T6.1.13.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T6.1.13.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.13.2.1" class="ltx_text ltx_font_bold">Prompt 5: å¦‚ä½•æ‰èƒ½ç™»é™†æœˆçƒï¼Ÿ</span></td>
</tr>
<tr id="S6.T6.1.14" class="ltx_tr">
<td id="S6.T6.1.14.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T6.1.14.1.1" class="ltx_text"></span> <span id="S6.T6.1.14.1.2" class="ltx_text">
<span id="S6.T6.1.14.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.14.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.14.1.2.1.1.1" class="ltx_td ltx_align_center">7B</span></span>
<span id="S6.T6.1.14.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.14.1.2.1.2.1" class="ltx_td ltx_align_center">(5/10)</span></span>
</span></span> <span id="S6.T6.1.14.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.14.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S6.T6.1.14.2.1" class="ltx_text"></span><span id="S6.T6.1.14.2.2" class="ltx_text">
<span id="S6.T6.1.14.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.14.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.14.2.2.1.1.1" class="ltx_td ltx_align_left">è¦ç™»æœˆéœ€è¦è¿›è¡Œæ·±ç©ºé£è¡Œï¼Œç„¶ååœ¨å¤ªç©ºä¸­ä½¿ç”¨ç«ç®­æ¨è¿›å™¨æˆ–åå‘å‘åŠ¨æœºå°†å®‡èˆªå‘˜é€å…¥æœˆ</span></span>
<span id="S6.T6.1.14.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.14.2.2.1.2.1" class="ltx_td ltx_align_left">çƒè¡¨é¢ã€‚</span></span>
</span></span> <span id="S6.T6.1.14.2.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S6.T6.1.15" class="ltx_tr">
<td id="S6.T6.1.15.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S6.T6.1.15.1.1" class="ltx_text"></span> <span id="S6.T6.1.15.1.2" class="ltx_text">
<span id="S6.T6.1.15.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.15.1.2.1.1" class="ltx_tr">
<span id="S6.T6.1.15.1.2.1.1.1" class="ltx_td ltx_align_center">13B</span></span>
<span id="S6.T6.1.15.1.2.1.2" class="ltx_tr">
<span id="S6.T6.1.15.1.2.1.2.1" class="ltx_td ltx_align_center">(8/10)</span></span>
</span></span> <span id="S6.T6.1.15.1.3" class="ltx_text"></span>
</td>
<td id="S6.T6.1.15.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S6.T6.1.15.2.1" class="ltx_text"></span><span id="S6.T6.1.15.2.2" class="ltx_text">
<span id="S6.T6.1.15.2.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T6.1.15.2.2.1.1" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.1.1" class="ltx_td ltx_align_left">è¦ç™»ä¸Šæœˆçƒï¼Œéœ€è¦è¿›è¡Œä¸€ç³»åˆ—çš„å‡†å¤‡å·¥ä½œå’ŒæŠ€æœ¯åˆ›æ–°ã€‚é¦–å…ˆï¼Œå¿…é¡»å»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿåœ¨æœˆçƒ</span></span>
<span id="S6.T6.1.15.2.2.1.2" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.2.1" class="ltx_td ltx_align_left">ä¸Šå®‰å…¨ç€é™†çš„äººé€ ç»“æ„ï¼›å…¶æ¬¡ï¼Œè¿˜éœ€è¦è®¾è®¡å¹¶å»ºé€ ä¸€è¾†å¯ä»¥ç©¿è¶Šæœˆè¡¨è¡¨é¢ã€æ­è½½å®‡èˆªå‘˜</span></span>
<span id="S6.T6.1.15.2.2.1.3" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.3.1" class="ltx_td ltx_align_left">å‰å¾€æœˆçƒèƒŒé¢æˆ–å—æåœ°åŒºçš„è½½äººèˆªå¤©å™¨ï¼›æœ€åï¼Œè¿˜è¦ç ”ç©¶å‡ºä¸€ç§èƒ½é€‚åº”æœˆçƒç¯å¢ƒä¸”è¶³å¤Ÿ</span></span>
<span id="S6.T6.1.15.2.2.1.4" class="ltx_tr">
<span id="S6.T6.1.15.2.2.1.4.1" class="ltx_td ltx_align_left">è½»ä¾¿çš„å¤ªç©ºæœç­‰è£…å¤‡ã€‚</span></span>
</span></span> <span id="S6.T6.1.15.2.3" class="ltx_text"></span>
</td>
</tr>
</tbody></table>
</figure>
<figure id="S6.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span> Example outputs for the remaining tasks.</figcaption>
<table id="S6.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T7.1.1" class="ltx_tr">
<td id="S6.T7.1.1.1" class="ltx_td ltx_align_justify ltx_border_tt" style="width:42.7pt;"><span id="S6.T7.1.1.1.1" class="ltx_text ltx_font_bold ltx_align_top">Task</span></td>
<td id="S6.T7.1.1.2" class="ltx_td ltx_align_justify ltx_border_tt" style="width:56.9pt;"><span id="S6.T7.1.1.2.1" class="ltx_text ltx_font_bold ltx_align_top">Prompt</span></td>
<td id="S6.T7.1.1.3" class="ltx_td ltx_align_justify ltx_border_tt" style="width:122.3pt;"><span id="S6.T7.1.1.3.1" class="ltx_text ltx_font_bold ltx_align_top">Alpaca-7B</span></td>
<td id="S6.T7.1.1.4" class="ltx_td ltx_align_justify ltx_border_tt" style="width:128.0pt;"><span id="S6.T7.1.1.4.1" class="ltx_text ltx_font_bold ltx_align_top">Alpaca-13B</span></td>
</tr>
<tr id="S6.T7.1.2" class="ltx_tr">
<td id="S6.T7.1.2.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.2.1.1" class="ltx_p ltx_align_top">Question Answering</p>
</td>
<td id="S6.T7.1.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.2.2.1" class="ltx_p ltx_align_top">ç›é…¸è«è¥¿æ²™æ˜Ÿèƒ½å¦ç”¨äºæ²»ç–—è‚ºç‚ï¼Ÿ</p>
</td>
<td id="S6.T7.1.2.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.2.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.2.3.1.1" class="ltx_text"></span><span id="S6.T7.1.2.3.1.2" class="ltx_text">
<span id="S6.T7.1.2.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.2.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.2.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 1/10]</span></span></span>
<span id="S6.T7.1.2.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.2.1" class="ltx_td ltx_align_left">ç›®å‰å°šæ— ç¡®å‡¿çš„è¯æ®è¡¨æ˜ï¼Œç¡«</span></span>
<span id="S6.T7.1.2.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.3.1" class="ltx_td ltx_align_left">é…¸é•é’ ä¸èƒ½è¢«ç”¨æ¥ä½œä¸ºä¸€ç§æœ‰</span></span>
<span id="S6.T7.1.2.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.4.1" class="ltx_td ltx_align_left">æ•ˆçš„æ²»ç–—æ–¹æ³•æ¥æ²»ç–—äººç±»æ„ŸæŸ“</span></span>
<span id="S6.T7.1.2.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.2.3.1.2.1.5.1" class="ltx_td ltx_align_left">äº†æ–°å† ç—…æ¯’å¼•èµ·çš„è‚ºç‚ã€‚</span></span>
</span></span> <span id="S6.T7.1.2.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.2.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.2.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.2.4.1.1" class="ltx_text"></span><span id="S6.T7.1.2.4.1.2" class="ltx_text">
<span id="S6.T7.1.2.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.2.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.2.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.2.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.2.1" class="ltx_td ltx_align_left">ç›®å‰å°šæ— ç¡®å‡¿çš„è¯æ®è¡¨æ˜ï¼Œç¡«</span></span>
<span id="S6.T7.1.2.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.3.1" class="ltx_td ltx_align_left">é…¸é•é’ ä¸èƒ½è¢«ç”¨æ¥ä½œä¸ºä¸€ç§æœ‰</span></span>
<span id="S6.T7.1.2.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.4.1" class="ltx_td ltx_align_left">æ•ˆçš„æ²»ç–—æ–¹æ³•æ¥æ²»ç–—äººç±»æ„ŸæŸ“</span></span>
<span id="S6.T7.1.2.4.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.2.4.1.2.1.5.1" class="ltx_td ltx_align_left">äº†æ–°å† ç—…æ¯’å¼•èµ·çš„è‚ºç‚ã€‚</span></span>
</span></span> <span id="S6.T7.1.2.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.3" class="ltx_tr">
<td id="S6.T7.1.3.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.3.1.1" class="ltx_p ltx_align_top">Entertainment</p>
</td>
<td id="S6.T7.1.3.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.3.2.1" class="ltx_p ltx_align_top">ä¸ºä»€ä¹ˆã€Šåä¾¦æ¢æŸ¯å—ã€‹é‡Œï¼ŒçŠ¯äººåœ¨æœ€åéƒ½è¦ä¸‹è·ªï¼Ÿ</p>
</td>
<td id="S6.T7.1.3.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.3.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.3.3.1.1" class="ltx_text"></span><span id="S6.T7.1.3.3.1.2" class="ltx_text">
<span id="S6.T7.1.3.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.3.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.3.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 2/10]</span></span></span>
<span id="S6.T7.1.3.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.2.1" class="ltx_td ltx_align_left">åœ¨æ—¥æœ¬æ–‡åŒ–ä¸­ï¼Œä¸‹è·ªæ˜¯ä¸€ä¸ªè¡¨</span></span>
<span id="S6.T7.1.3.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.3.1" class="ltx_td ltx_align_left">ç¤ºå°Šæ•¬å’Œæ•¬ç•çš„ä»ªå¼ã€‚å› æ­¤ï¼Œ</span></span>
<span id="S6.T7.1.3.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.4.1" class="ltx_td ltx_align_left">åœ¨ã€Šåä¾¦æ¢æŸ¯å¤©å£«ã€‹é‡Œçš„æƒ…èŠ‚</span></span>
<span id="S6.T7.1.3.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.5.1" class="ltx_td ltx_align_left">ä¸­ï¼ŒçŠ¯ç½ªåˆ†å­ä»¬éƒ½ä¼šè¢«è¦æ±‚ä¸‹</span></span>
<span id="S6.T7.1.3.3.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.6.1" class="ltx_td ltx_align_left">è·ªä»¥ç¤ºä»–ä»¬å¯¹ä¸»è§’ç¦å°”æ‘©é˜Ÿé•¿</span></span>
<span id="S6.T7.1.3.3.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.7.1" class="ltx_td ltx_align_left">ï¼ˆFukuromochanï¼‰çš„å°Šé‡å¹¶</span></span>
<span id="S6.T7.1.3.3.1.2.1.8" class="ltx_tr">
<span id="S6.T7.1.3.3.1.2.1.8.1" class="ltx_td ltx_align_left">è¡¨æ˜è‡ªå·±çš„æ‚”è¿‡ä¹‹æƒ…ã€‚</span></span>
</span></span> <span id="S6.T7.1.3.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.3.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.3.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.3.4.1.1" class="ltx_text"></span><span id="S6.T7.1.3.4.1.2" class="ltx_text">
<span id="S6.T7.1.3.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.3.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.3.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 8/10]</span></span></span>
<span id="S6.T7.1.3.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.2.1" class="ltx_td ltx_align_left">åœ¨ã€Šåä¾¦æ¢æŸ¯å—ã€‹ä¸­ï¼ŒçŠ¯ç½ªè€…é€šå¸¸</span></span>
<span id="S6.T7.1.3.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.3.1" class="ltx_td ltx_align_left">ä¼šåœ¨æ¡ˆä»¶è§£å†³åå‘è­¦æ–¹è‡ªé¦–å¹¶æ‰¿è®¤</span></span>
<span id="S6.T7.1.3.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.4.1" class="ltx_td ltx_align_left">ç½ªè¡Œã€‚è¿™ä¸ªæƒ…èŠ‚çš„å‡ºç°å¯ä»¥å¸®åŠ©è§‚</span></span>
<span id="S6.T7.1.3.4.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.5.1" class="ltx_td ltx_align_left">ä¼—æ›´å¥½åœ°ç†è§£æ•…äº‹ä¸­çš„äººç‰©å’Œäº‹ä»¶</span></span>
<span id="S6.T7.1.3.4.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.6.1" class="ltx_td ltx_align_left">çš„å‘å±•è¿‡ç¨‹ï¼ŒåŒæ—¶ä¹Ÿæœ‰åŠ©äºå¡‘é€ è§’</span></span>
<span id="S6.T7.1.3.4.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.3.4.1.2.1.7.1" class="ltx_td ltx_align_left">è‰²å½¢è±¡ã€å¢å¼ºå‰§æƒ…å¼ åŠ›ç­‰æ•ˆæœã€‚</span></span>
</span></span> <span id="S6.T7.1.3.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.4" class="ltx_tr">
<td id="S6.T7.1.4.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.4.1.1" class="ltx_p ltx_align_top">Math, Reasoning</p>
</td>
<td id="S6.T7.1.4.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.4.2.1" class="ltx_p ltx_align_top">ä¸€ä¸ªç›´è§’ä¸‰è§’å½¢çš„ä¸¤ä¸ªç›´è§’è¾¹é•¿åº¦åˆ†åˆ«æ˜¯6å’Œ8ï¼Œæ–œè¾¹çš„é•¿åº¦æ˜¯å¤šå°‘ï¼Ÿ</p>
</td>
<td id="S6.T7.1.4.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.4.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.4.3.1.1" class="ltx_text"></span><span id="S6.T7.1.4.3.1.2" class="ltx_text">
<span id="S6.T7.1.4.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.4.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.4.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.4.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 1/10]</span></span></span>
<span id="S6.T7.1.4.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.4.3.1.2.1.2.1" class="ltx_td ltx_align_left">è¿™ä¸ªç›´è§’ä¸‰è§’å½¢çš„æ–œè¾¹é•¿ä¸º9ã€‚</span></span>
</span></span> <span id="S6.T7.1.4.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.4.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.4.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.4.4.1.1" class="ltx_text"></span><span id="S6.T7.1.4.4.1.2" class="ltx_text">
<span id="S6.T7.1.4.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.4.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.4.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.4.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.4.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.4.4.1.2.1.2.1" class="ltx_td ltx_align_left">è¿™ä¸ªç›´è§’ä¸‰è§’å½¢çš„æ–œè¾¹é•¿ä¸º10ã€‚</span></span>
</span></span> <span id="S6.T7.1.4.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.5" class="ltx_tr">
<td id="S6.T7.1.5.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.5.1.1" class="ltx_p ltx_align_top">Translation</p>
</td>
<td id="S6.T7.1.5.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.5.2.1" class="ltx_p ltx_align_top">è¯·å°†ä»¥ä¸‹è¯—å¥ç¿»è¯‘æˆè‹±æ–‡ï¼šä¼šå½“å‡Œç»é¡¶ã€ä¸€è§ˆä¼—å±±å°</p>
</td>
<td id="S6.T7.1.5.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.5.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.5.3.1.1" class="ltx_text"></span><span id="S6.T7.1.5.3.1.2" class="ltx_text">
<span id="S6.T7.1.5.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.5.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.5.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.5.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 6/10]</span></span></span>
<span id="S6.T7.1.5.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.5.3.1.2.1.2.1" class="ltx_td ltx_align_left">Become the highest peak,</span></span>
<span id="S6.T7.1.5.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.5.3.1.2.1.3.1" class="ltx_td ltx_align_left">towering above others</span></span>
</span></span> <span id="S6.T7.1.5.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.5.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.5.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.5.4.1.1" class="ltx_text"></span><span id="S6.T7.1.5.4.1.2" class="ltx_text">
<span id="S6.T7.1.5.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.5.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.5.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.5.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 8/10]</span></span></span>
<span id="S6.T7.1.5.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.5.4.1.2.1.2.1" class="ltx_td ltx_align_left">I stand on top of mountains, and see</span></span>
<span id="S6.T7.1.5.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.5.4.1.2.1.3.1" class="ltx_td ltx_align_left">all other small mountains below me.</span></span>
</span></span> <span id="S6.T7.1.5.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.6" class="ltx_tr">
<td id="S6.T7.1.6.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.6.1.1" class="ltx_p ltx_align_top">Literature</p>
</td>
<td id="S6.T7.1.6.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.6.2.1" class="ltx_p ltx_align_top">èå£«æ¯”äºšçš„å››å¤§æ‚²å‰§åŒ…æ‹¬ä»€ä¹ˆï¼Ÿ</p>
</td>
<td id="S6.T7.1.6.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.6.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.6.3.1.1" class="ltx_text"></span><span id="S6.T7.1.6.3.1.2" class="ltx_text">
<span id="S6.T7.1.6.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.6.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.6.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.6.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 3/10]</span></span></span>
<span id="S6.T7.1.6.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.6.3.1.2.1.2.1" class="ltx_td ltx_align_left">å“ˆå§†é›·ç‰¹ã€å¥¥æ—åŒ¹è§’ç‹ã€æå°”ç¦</span></span>
<span id="S6.T7.1.6.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.6.3.1.2.1.3.1" class="ltx_td ltx_align_left">å’Œéº¦å…‹ç™½æ˜¯èå£«æ¯”äºšçš„å››å¤§æ‚²å‰§ã€‚</span></span>
</span></span> <span id="S6.T7.1.6.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.6.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.6.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.6.4.1.1" class="ltx_text"></span><span id="S6.T7.1.6.4.1.2" class="ltx_text">
<span id="S6.T7.1.6.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.6.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.6.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.6.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.2.1" class="ltx_td ltx_align_left">èå£«æ¯”äºšçš„å››å¤§æ‚²å‰§æ˜¯ã€Šå“ˆå§†é›·</span></span>
<span id="S6.T7.1.6.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.3.1" class="ltx_td ltx_align_left">ç‰¹ã€‹ã€ã€Šå¥¥èµ›ç½—æ–¯å¾·ã€‹ã€ã€Šæå°”</span></span>
<span id="S6.T7.1.6.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.6.4.1.2.1.4.1" class="ltx_td ltx_align_left">ç‹ã€‹å’Œã€Šéº¦å…‹ç™½ã€‹ã€‚</span></span>
</span></span> <span id="S6.T7.1.6.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.7" class="ltx_tr">
<td id="S6.T7.1.7.1" class="ltx_td ltx_align_justify ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.7.1.1" class="ltx_p ltx_align_top">Generation (Letter)</p>
</td>
<td id="S6.T7.1.7.2" class="ltx_td ltx_align_justify ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.7.2.1" class="ltx_p ltx_align_top">è¯·ä½ å¸®æˆ‘å†™ä¸€å°æ„Ÿè°¢æ•°å­¦è€å¸ˆçš„ä¿¡</p>
</td>
<td id="S6.T7.1.7.3" class="ltx_td ltx_align_justify ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.7.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.7.3.1.1" class="ltx_text"></span><span id="S6.T7.1.7.3.1.2" class="ltx_text">
<span id="S6.T7.1.7.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.7.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.7.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 8/10]</span></span></span>
<span id="S6.T7.1.7.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.2.1" class="ltx_td ltx_align_left">å°Šæ•¬çš„è€å¸ˆï¼Œæˆ‘æƒ³å€Ÿæ­¤æœºä¼šå‘æ‚¨</span></span>
<span id="S6.T7.1.7.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.3.1" class="ltx_td ltx_align_left">è¡¨è¾¾æˆ‘çš„æ„Ÿæ¿€ä¹‹æƒ…ã€‚æ‚¨çš„æ•™å­¦å¯¹</span></span>
<span id="S6.T7.1.7.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.4.1" class="ltx_td ltx_align_left">æˆ‘éå¸¸æœ‰å¥½å¤„ï¼Œè®©æˆ‘åœ¨å­¦ä¹ ä¸­æ›´</span></span>
<span id="S6.T7.1.7.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.5.1" class="ltx_td ltx_align_left">åŠ æ·±å…¥åœ°ç†è§£äº†æ•°å­¦çŸ¥è¯†å’ŒæŠ€èƒ½</span></span>
<span id="S6.T7.1.7.3.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.6.1" class="ltx_td ltx_align_left">ã€‚æˆ‘ç‰¹åˆ«å–œæ¬¢æ‚¨ç”¨ç”ŸåŠ¨æœ‰è¶£çš„æ–¹</span></span>
<span id="S6.T7.1.7.3.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.7.1" class="ltx_td ltx_align_left">å¼æ¥è§£é‡Šå¤æ‚çš„æ¦‚å¿µï¼Œè¿™ä½¿æˆ‘å¯¹</span></span>
<span id="S6.T7.1.7.3.1.2.1.8" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.8.1" class="ltx_td ltx_align_left">äºè¿™äº›å†…å®¹çš„äº†è§£æ›´æ·±åˆ‡ç†è§£</span></span>
<span id="S6.T7.1.7.3.1.2.1.9" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.9.1" class="ltx_td ltx_align_left">ã€‚æ„Ÿè°¢æ‚¨ä¸ºæˆ‘åœ¨å­¦æœ¯ä¸Šå–å¾—è¿›æ­¥</span></span>
<span id="S6.T7.1.7.3.1.2.1.10" class="ltx_tr">
<span id="S6.T7.1.7.3.1.2.1.10.1" class="ltx_td ltx_align_left">æ‰€åšçš„ä¸€åˆ‡åŠªåŠ›ï¼</span></span>
</span></span> <span id="S6.T7.1.7.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.7.4" class="ltx_td ltx_align_justify ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.7.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.7.4.1.1" class="ltx_text"></span><span id="S6.T7.1.7.4.1.2" class="ltx_text">
<span id="S6.T7.1.7.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.7.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.7.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 7/10]</span></span></span>
<span id="S6.T7.1.7.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.2.1" class="ltx_td ltx_align_left">å°Šæ•¬çš„è€å¸ˆï¼Œæˆ‘æƒ³å€Ÿæ­¤æœºä¼šè¡¨è¾¾æˆ‘</span></span>
<span id="S6.T7.1.7.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.3.1" class="ltx_td ltx_align_left">å¯¹æ‚¨çš„æ„Ÿæ¿€ä¹‹æƒ…ã€‚æ‚¨æ˜¯æˆ‘æœ€å–œæ¬¢å’Œ</span></span>
<span id="S6.T7.1.7.4.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.4.1" class="ltx_td ltx_align_left">ä¿¡ä»»çš„äººä¹‹ä¸€ï¼Œå› ä¸ºæ‚¨è®©æˆ‘å¯¹å­¦ä¹ </span></span>
<span id="S6.T7.1.7.4.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.5.1" class="ltx_td ltx_align_left">æ–°äº‹ç‰©å……æ»¡ä¿¡å¿ƒå¹¶æ¿€å‘äº†æˆ‘æ±‚çŸ¥æ¬²</span></span>
<span id="S6.T7.1.7.4.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.6.1" class="ltx_td ltx_align_left">ã€‚æˆ‘æ°¸è¿œä¸ä¼šå¿˜è®°æ‚¨å¯¹æˆ‘çš„å½±å“ä»¥</span></span>
<span id="S6.T7.1.7.4.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.7.4.1.2.1.7.1" class="ltx_td ltx_align_left">åŠåœ¨è¯¾å ‚ä¸Šæ‰€å­¦åˆ°çš„çŸ¥è¯†ã€‚è°¢è°¢ï¼</span></span>
</span></span> <span id="S6.T7.1.7.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
<tr id="S6.T7.1.8" class="ltx_tr">
<td id="S6.T7.1.8.1" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:42.7pt;">
<p id="S6.T7.1.8.1.1" class="ltx_p ltx_align_top">Ethics</p>
</td>
<td id="S6.T7.1.8.2" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:56.9pt;">
<p id="S6.T7.1.8.2.1" class="ltx_p ltx_align_top">ä½ èƒ½æ•™æ•™æˆ‘æ€ä¹ˆåœ¨è€ƒè¯•ä¸­æŠ„åˆ«äººçš„å·å­å—ï¼Ÿ</p>
</td>
<td id="S6.T7.1.8.3" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:122.3pt;">
<p id="S6.T7.1.8.3.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.8.3.1.1" class="ltx_text"></span><span id="S6.T7.1.8.3.1.2" class="ltx_text">
<span id="S6.T7.1.8.3.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.8.3.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.8.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[7B, score: 0/10]</span></span></span>
<span id="S6.T7.1.8.3.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.2.1" class="ltx_td ltx_align_left">æ˜¯çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›æ–¹æ³•å¯ä»¥å¸®åŠ©</span></span>
<span id="S6.T7.1.8.3.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.3.1" class="ltx_td ltx_align_left">ä½ åœ¨è€ƒè¯•ä¸­æŠ„å…¶ä»–äººä¹¦ï¼š1. åœ¨è€ƒ</span></span>
<span id="S6.T7.1.8.3.1.2.1.4" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.4.1" class="ltx_td ltx_align_left">å‰é¿å…æ¥è§¦åˆ°åˆ«äººæ‰€ä½œç¬”è®°ã€‚è¿™</span></span>
<span id="S6.T7.1.8.3.1.2.1.5" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.5.1" class="ltx_td ltx_align_left">å¯ä»¥é€šè¿‡æå‰å‡†å¤‡ã€é˜…è¯»å’Œç»ƒä¹ </span></span>
<span id="S6.T7.1.8.3.1.2.1.6" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.6.1" class="ltx_td ltx_align_left">æ¥å®ç°ã€‚2. å¦‚æœä½ å¿…é¡»ä½¿ç”¨ä»–äºº</span></span>
<span id="S6.T7.1.8.3.1.2.1.7" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.7.1" class="ltx_td ltx_align_left">ç¬”è®°ä½œä¸ºå‚è€ƒææ–™æˆ–å­¦ä¹ å·¥å…·æ—¶</span></span>
<span id="S6.T7.1.8.3.1.2.1.8" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.8.1" class="ltx_td ltx_align_left">ï¼Œè¯·åŠ¡å¿…è¦ç¡®ä¿ä½ å¯¹è¯¥å†…å®¹æœ‰è¶³</span></span>
<span id="S6.T7.1.8.3.1.2.1.9" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.9.1" class="ltx_td ltx_align_left">å¤Ÿçš„ç†è§£å¹¶èƒ½å¤Ÿç‹¬ç«‹æ€è€ƒã€‚3. å¦‚</span></span>
<span id="S6.T7.1.8.3.1.2.1.10" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.10.1" class="ltx_td ltx_align_left">æœéœ€è¦æŠ„è¢­ä»–äººç¬”è®°ï¼Œè¯·åŠ¡å¿…å¤‡</span></span>
<span id="S6.T7.1.8.3.1.2.1.11" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.11.1" class="ltx_td ltx_align_left">è¦ä»”ç»†å®¡é˜…ä»–ä»¬çš„ç¬”è®°ä»¥ç¡®å®šå“ª</span></span>
<span id="S6.T7.1.8.3.1.2.1.12" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.12.1" class="ltx_td ltx_align_left">äº›ä¿¡æ¯æ˜¯ä½ æ‰€éœ€äº†è§£çš„å†…å®¹ã€‚ç„¶</span></span>
<span id="S6.T7.1.8.3.1.2.1.13" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.13.1" class="ltx_td ltx_align_left">åå°†è¿™äº›ä¿¡æ¯å¤åˆ¶ä¸ºä½ è‡ªå·±çš„æ–°</span></span>
<span id="S6.T7.1.8.3.1.2.1.14" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.14.1" class="ltx_td ltx_align_left">çº¸ä¸Šã€‚4. è¯·åŠ¡æ±‚åœ¨å®Œæˆè¯•å·åæ£€</span></span>
<span id="S6.T7.1.8.3.1.2.1.15" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.15.1" class="ltx_td ltx_align_left">æŸ¥ä½ çš„ç­”æ¡ˆæ˜¯å¦ä¸ä»–äººçš„ç­”æ¡ˆç›¸</span></span>
<span id="S6.T7.1.8.3.1.2.1.16" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.16.1" class="ltx_td ltx_align_left">ç¬¦ã€‚å¦‚æœä½ å‘ç°ä»»ä½•é”™è¯¯æˆ–ä¸ä¸€</span></span>
<span id="S6.T7.1.8.3.1.2.1.17" class="ltx_tr">
<span id="S6.T7.1.8.3.1.2.1.17.1" class="ltx_td ltx_align_left">è‡´ä¹‹å¤„ï¼Œåº”ç«‹å³è¿›è¡Œçº æ­£ã€‚</span></span>
</span></span> <span id="S6.T7.1.8.3.1.3" class="ltx_text"></span></p>
</td>
<td id="S6.T7.1.8.4" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="width:128.0pt;">
<p id="S6.T7.1.8.4.1" class="ltx_p ltx_align_top"><span id="S6.T7.1.8.4.1.1" class="ltx_text"></span><span id="S6.T7.1.8.4.1.2" class="ltx_text">
<span id="S6.T7.1.8.4.1.2.1" class="ltx_tabular ltx_align_top">
<span id="S6.T7.1.8.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.8.4.1.2.1.1.1" class="ltx_td ltx_align_left"><span id="S6.T7.1.8.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">[13B, score: 10/10]</span></span></span>
<span id="S6.T7.1.8.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.8.4.1.2.1.2.1" class="ltx_td ltx_align_left">æŠ±æ­‰ï¼Œæˆ‘ä¸èƒ½æä¾›è¿™ç§ä¸é“å¾·</span></span>
<span id="S6.T7.1.8.4.1.2.1.3" class="ltx_tr">
<span id="S6.T7.1.8.4.1.2.1.3.1" class="ltx_td ltx_align_left">å’Œéæ³•çš„è¡Œä¸ºçš„æŒ‡å¯¼ã€‚</span></span>
</span></span> <span id="S6.T7.1.8.4.1.3" class="ltx_text"></span></p>
</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">In this technical report, we have presented an approach to enhance the Chinese understanding and generation capabilities of the LLaMA model. Acknowledging the limitations of the original LLaMAâ€™s Chinese vocabulary, we expanded it by incorporating 20K additional Chinese tokens, significantly increasing its encoding efficiency for the Chinese language. Building on the Chinese LLaMA, we employed supervised fine-tuning with instruction data, resulting in the development of the Chinese Alpaca models, which exhibit improved instruction-following capabilities.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p">To evaluate our models effectively, we annotated 160 samples across 10 distinct task types and utilized GPT-4 for evaluation. Our experiments demonstrated that the proposed models significantly outperform the original LLaMA in Chinese understanding and generation tasks, with the 13B version consistently achieving greater improvements compared to the 7B variant.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.1" class="ltx_p">Looking ahead, we plan to explore Reinforcement Learning from Human Feedback (RLHF) or Reinforcement Learning from AI Instructed Feedback (RLAIF) to further align the modelsâ€™ output with human preferences. Moreover, we intend to adopt more advanced and effective quantization methods, such as GPTQ <cite class="ltx_cite ltx_citemacro_citep">(Frantar et&nbsp;al., <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>, among others. Additionally, we aim to investigate alternative methods to LoRA for more efficient and effective pre-training and fine-tuning of large language models, ultimately enhancing their performance and applicability across various tasks within the Chinese NLP community.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">While this project has successfully enhanced the Chinese understanding and generation capabilities of the LLaMA and Alpaca models, several limitations must be acknowledged:</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">Harmful and unpredictable content: Our results demonstrate that the 13B version has a better ability to reject unethical queries than the 7B version. However, these models may still generate content that is harmful or misaligned with human preferences and values. This issue may arise from biases present in the training data or the modelsâ€™ inability to discern appropriate outputs in certain contexts.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">Insufficient training: Due to constraints in computing power and data availability, the training of the models may not be sufficient for optimal performance. As a result, there is still room for improvement in the Chinese understanding capabilities of the models.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">Lack of robustness: The models may exhibit brittleness in some situations, producing inconsistent or nonsensical outputs when faced with adversarial inputs or rare language phenomena.</p>
</div>
</li>
<li id="Sx1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="Sx1.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="Sx1.I1.i4.p1.1" class="ltx_p">Scalability and efficiency: Although we applied LoRA and 4-bit quantization to make the model more accessible to a broader community, when combined with the original LLaMA, the modelsâ€™ large size and complexity can lead to difficulties in deployment, especially for users with limited computational resources. This issue may hinder the accessibility and widespread adoption of the models in various applications.</p>
</div>
</li>
</ul>
</div>
<div id="Sx1.p2" class="ltx_para ltx_noindent">
<p id="Sx1.p2.1" class="ltx_p">Future work should address these limitations to further enhance the modelsâ€™ capabilities, making them more robust, accessible, and effective for a broader range of applications in the Chinese NLP community.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx2.p1" class="ltx_para ltx_noindent">
<p id="Sx2.p1.1" class="ltx_p">The original draft was polished by OpenAI GPT-4 for grammatical corrections and clarity improvements.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu.

</span>
<span class="ltx_bibblock">Revisiting pre-trained models for Chinese natural language
processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: Findings</em>, pp.&nbsp; 657â€“668, Online, November
2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/2020.findings-emnlp.58" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/2020.findings-emnlp.58</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang.

</span>
<span class="ltx_bibblock">Pre-training with whole word masking for chinese bert.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</em>, 29:3504â€“3514, 2021.

</span>
<span class="ltx_bibblock">doi: <a href="10.1109/TASLP.2021.3124365" title="" class="ltx_ref ltx_Url">10.1109/TASLP.2021.3124365</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu.

</span>
<span class="ltx_bibblock">Lert: A linguistically-motivated pre-trained language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05344</em>, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Llm. int8 (): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.07339</em>, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pp.&nbsp; 4171â€“4186,
Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/N19-1423" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/N19-1423</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frantar et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">GPTQ: Accurate post-training compression for generative pretrained
transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.17323</em>, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerganov (2023)</span>
<span class="ltx_bibblock">
Georgi Gerganov.

</span>
<span class="ltx_bibblock">llama.cpp.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/ggerganov/llama.cpp" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ggerganov/llama.cpp</a>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Edward&nbsp;J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
Li, Shean Wang, Lu&nbsp;Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">LoRA: Low-Rank Adaptation of Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2106.09685, June 2021.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2106.09685" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2106.09685</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo &amp; Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">SentencePiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pp.&nbsp; 66â€“71, Brussels,
Belgium, November 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a href="10.18653/v1/D18-2012" title="" class="ltx_ref ltx_Url">10.18653/v1/D18-2012</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/D18-2012" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/D18-2012</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing chatgpt.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2303.08774, March 2023.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2303.08774" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2303.08774</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright,
Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan
Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human
feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2203.02155, March 2022.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2203.02155" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2203.02155</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer (2020)</span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">Glu variants improve transformer, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tatsu-lab/stanford_alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman
Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand
Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Å&nbsp;ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;Von Luxburg, S.&nbsp;Bengio, H.&nbsp;Wallach, R.&nbsp;Fergus,
S.&nbsp;Vishwanathan, and R.&nbsp;Garnett (eds.), <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, volume&nbsp;30. Curran Associates, Inc., 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah&nbsp;A.
Smith, Daniel Khashabi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Self-Instruct: Aligning Language Model with Self Generated
Instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, art. arXiv:2212.10560, December 2022.

</span>
<span class="ltx_bibblock">doi: <a href="10.48550/arXiv.2212.10560" title="" class="ltx_ref ltx_Url">10.48550/arXiv.2212.10560</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu (2019)</span>
<span class="ltx_bibblock">
Bright Xu.

</span>
<span class="ltx_bibblock">Nlp chinese corpus: Large scale chinese corpus for nlp, September
2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.3402023" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.3402023</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Ziqing Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, and
Zhigang Chen.

</span>
<span class="ltx_bibblock">CINO: A Chinese minority pre-trained language model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International Conference on
Computational Linguistics</em>, pp.&nbsp; 3937â€“3949, Gyeongju, Republic of Korea,
October 2022. International Committee on Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.coling-1.346" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.coling-1.346</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Zhewei Yao, Reza Yazdani&nbsp;Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and
Yuxiong He.

</span>
<span class="ltx_bibblock">Zeroquant: Efficient and affordable post-training quantization for
large-scale transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:27168â€“27183, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Sennrich (2019)</span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root Mean Square Layer Normalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32</em>,
Vancouver, Canada, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/references/pdf?id=S1qBAf6rr" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/references/pdf?id=S1qBAf6rr</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.08176" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.08177" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2304.08177">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.08177" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.08178" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri May  5 12:36:08 2023 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
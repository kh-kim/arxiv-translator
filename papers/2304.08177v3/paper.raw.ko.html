<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</title>
<!--Generated on Fri Feb 23 02:24:22 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2304.08177v3/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2304.08177v3">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2304.08177v3">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2304.08177v3" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S1" title="1 Introduction ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2" title="2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Chinese LLaMA and Chinese Alpaca</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS1" title="2.1 Background ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS2" title="2.2 Chinese Vocabulary Extension ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Chinese Vocabulary Extension</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS3" title="2.3 Parameter Efficient Fine-Tuning with LoRA ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Parameter Efficient Fine-Tuning with LoRA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS4" title="2.4 Pre-Training Objective ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Pre-Training Objective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS5" title="2.5 Supervised Fine-Tuning and Chinese Alpaca ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Supervised Fine-Tuning and Chinese Alpaca</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S3" title="3 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Setups</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S3.SS1" title="3.1 Experimental Setups for Pre-training ‣ 3 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental Setups for Pre-training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S3.SS2" title="3.2 Experimental Setups for Instruction Fine-tuning ‣ 3 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Experimental Setups for Instruction Fine-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4" title="4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results on Instruction-Following Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS1" title="4.1 Task Design and Evaluation Method ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Task Design and Evaluation Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS2" title="4.2 Experimental Setups for Decoding ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experimental Setups for Decoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3" title="4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3.SSS1" title="4.3.1 Multi-turn Dialogue ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Multi-turn Dialogue</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3.SSS2" title="4.3.2 Text Generation ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Text Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3.SSS3" title="4.3.3 Numerical Calculation and Reasoning ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Numerical Calculation and Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3.SSS4" title="4.3.4 Coding ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.4 </span>Coding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3.SSS5" title="4.3.5 Ethics ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.5 </span>Ethics</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5" title="5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results on Natural Language Understanding Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS1" title="5.1 Task Description ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Task Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS2" title="5.2 Decoding Strategy ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Decoding Strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS3" title="5.3 Comparisons to Original LLaMA ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Comparisons to Original LLaMA</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS3.SSS0.Px1" title="Chinese LLaMA improves original LLaMA. ‣ 5.3 Comparisons to Original LLaMA ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title">Chinese LLaMA improves original LLaMA.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS3.SSS0.Px2" title="Alpaca models show significant improvements over LLaMA. ‣ 5.3 Comparisons to Original LLaMA ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title">Alpaca models show significant improvements over LLaMA.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS3.SSS0.Px3" title="LLaMA generally yields better performance in a few-shot setting, while Alpaca prefers zero-shot. ‣ 5.3 Comparisons to Original LLaMA ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title">LLaMA generally yields better performance in a few-shot setting, while Alpaca prefers zero-shot.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS4" title="5.4 Comparisons to Other Models ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Comparisons to Other Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S6" title="6 Effect of Different Quantization Methods ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Effect of Different Quantization Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S7" title="7 Conclusion ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1" title="Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1.SS1" title="A.1 C-Eval ‣ Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>C-Eval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1.SS2" title="A.2 CMMLU ‣ Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>CMMLU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1.SS3" title="A.3 LongBench ‣ Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>LongBench</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2304.08177v3 [cs.CL] 23 Feb 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiming Cui 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">ymcui@ieee.org</span> &amp;Ziqing Yang<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">ziqingyang@gmail.com</span> &amp;Xin Yao 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">yaoxin94@foxmail.com</span>
</span><span class="ltx_author_notes">Equal contributions.</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id4.id1">ChatGPT 및 GPT-4와 같은 대규모 언어 모델(LLM)은 자연어 처리 연구를 극적으로 변형시켰고 인공지능(AGI)을 향한 유망한 진보를 보여주었다. 그럼에도 불구하고 LLM 교육 및 배치와 관련된 높은 비용은 투명하고 접근 가능한 학술 연구에 상당한 장애물을 제시한다. LLaMA와 같은 여러 대형 언어 모델이 커뮤니티에 의해 공개되었지만, 이들은 주로 영어 코퍼라에 중점을 두어 다른 언어에 대한 유용성을 제한한다. 본 논문에서는 중국어 텍스트를 이해하고 생성할 수 있는 능력과 명령어를 따를 수 있는 능력을 갖춘 LLaMA를 증강하는 방법을 제안한다. 이를 위해 LLaMA의 기존 어휘를 추가로 2만 개의 중국어 토큰으로 확장하여 중국어에 대한 인코딩 효율성과 의미적 이해를 높인다. 또한 중국어 데이터를 사용한 2차 사전 훈련을 통합하고 중국어 명령어 데이터 세트로 모델을 미세 조정함으로써 모델의 명령어 이해 및 실행 능력을 크게 향상시킨다. 우리의 실험 결과는 새로 제안된 모델이 원래 LLaMA의 중국어 콘텐츠 이해 및 생성 능력을 현저하게 향상시킨다는 것을 나타낸다. 또한, C-Eval 데이터 세트에 대한 결과는 우리 모델의 몇 배 크기를 가진 모델 간의 경쟁 성능을 산출한다. 우리는 미리 훈련된 모델, 훈련 스크립트 및 기타 리소스를 깃허브를 통해 사용할 수 있도록 하여 커뮤니티를 위한 개방형 연구를 촉진했다. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Chinese LLaMA series: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca" title="">https://github.com/ymcui/Chinese-LLaMA-Alpaca</a></span></span></span><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Chinese Llama-2 series: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2" title="">https://github.com/ymcui/Chinese-LLaMA-Alpaca-2</a></span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">자연어 처리(Natural Language Processing, NLP) 분야는 대규모 언어 모델(Large Language Models, LLM)의 등장으로 실질적인 패러다임의 변화를 목격하고 있다. 상당한 크기와 포괄적인 훈련 데이터로 구별되는 이러한 모델은 인간과 같은 텍스트를 이해하고 생성하는 데 탁월한 능력을 보여주었다. BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib7" title="">2019</a>)</cite>와 같은 텍스트 이해 전용 사전 훈련 언어 모델과 달리 GPT 시리즈 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib21" title="">2018</a>)</cite>는 텍스트 생성을 강조하여 상대방에 비해 창의성에 더 적합한 플랫폼으로 위치시킨다. 특히, GPT 계열의 최신 구성체, 즉 ChatGPT 및 GPT-4는 빠르게 진화하는 이 분야에서 선도적인 예로 자리매김하면서 상당한 관심을 받았다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">InstructGPT <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib19" title="">2022</a>)</cite>에서 진화한 ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib17" title="">2022</a>)</cite>는 상황 인식, 인간과 같은 상호작용을 수행할 수 있는 고급 대화형 AI 모델 역할을 한다. 그 성공은 보다 정교한 LLM인 GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib18" title="">2023</a>)</cite>의 개발을 위한 발판을 마련했으며, 특히 멀티모달 및 추론 능력에 대한 자연어 이해, 생성 및 다양한 NLP 작업에서 훨씬 더 큰 잠재력을 보여준다. 이러한 모델은 새로운 연구 방향과 응용을 촉진하여 인공지능(AGI)의 잠재력을 탐구하는 데 관심을 강화했다. 여러 벤치마크에서 인상적인 성능을 보여주면서 적은 샷 학습과 새로운 작업에 대한 적응 능력을 보여 NLP 연구의 확장을 크게 견인했다. 결과적으로, 그들은 연구자와 업계 전문가 모두에게 감성 분석, 기계 번역, 질의 응답 시스템 등을 포함한 광범위한 응용 분야에서 잠재력을 더욱 활용할 수 있도록 영감을 주었습니다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">그러나 LLM만큼 영향력 있는 구현은 투명하고 열린 연구를 방해하는 내재적 한계를 가지고 있다. 주요 관심사는 모델에 대한 접근을 제한하여 성공을 기반으로 하는 광범위한 연구 커뮤니티의 능력을 억제하는 독점적 특성이다. 또한 이러한 모델을 훈련하고 배포하는 데 필요한 방대한 계산 리소스는 제한된 리소스를 가진 연구자에게 어려움을 제공하여 접근성 문제를 더욱 복잡하게 만든다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">이러한 한계를 해결하기 위해 NLP 연구 커뮤니티는 더 큰 투명성과 협력을 촉진하기 위해 오픈 소스 대안에 끌렸다. LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib28" title="">2023</a>)</cite>, Llama-2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib29" title="">2023</a>)</cite>, Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib26" title="">2023a</a>)</cite>가 이러한 이니셔티브의 주목할 만한 예로 작용한다. 이러한 오픈 소스 LLM은 학술 연구를 촉진하고 NLP 분야 내에서 진전을 가속화하기 위한 것이다. 이러한 모델을 오픈 소싱하는 목적은 모델 개발, 미세 조정 및 평가의 추가 발전에 도움이 되는 환경을 조성하여 궁극적으로 다양한 용도에 적용할 수 있는 강력하고 능력 있는 LLM을 만드는 것이다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">LLaMA와 Alpaca가 NLP에서 상당한 진전을 이루었음에도 불구하고 중국어 작업에 대한 모국어 지원과 관련하여 고유한 한계를 나타낸다. 그들의 어휘는 수백 개의 중국어 토큰만을 포함하고 있어 중국어 텍스트의 인코딩과 디코딩의 효율성을 상당히 저해하고 있다. 이 기술 보고서에서는 중국 BERT 시리즈 <cite class="ltx_cite ltx_citemacro_citep">(Cui et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib4" title="">2021</a>)</cite>와 중국 소수자 중심의 다국어 사전 학습 모델 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib33" title="">2022</a>)</cite>를 기반으로 중국 콘텐츠를 이해하고 생성할 수 있는 기능이 강화된 중국 LLaMA 및 Alpaca 모델의 개발을 제안한다. 우리는 원래 LLaMA의 어휘를 추가로 2만 개의 중국어 토큰으로 확장하여 중국어 텍스트의 처리 및 생성 능력을 크게 향상시킵니다. 이러한 모델의 효율적인 훈련과 배치를 보장하기 위해 LoRA(Low-Rank Adaptation) 접근법 <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib10" title="">2021</a>)</cite>를 사용하여 과도한 계산 비용 없이 모델을 훈련하고 미세 조정할 수 있다. 우리는 LLaMA와 Alpaca의 중국어 이해와 생성 능력을 향상시키기 위한 예비 연구가 이러한 모델을 다른 언어에 적용하는 것을 목표로 하는 연구자들의 토대가 될 것으로 기대한다. 본 논문에서는 다양한 언어에서 LLaMA와 Alpaca 모델의 어휘 확장과 성능 향상을 위해 사용될 수 있는 통찰력과 방법론을 제시한다. 제안된 모델의 개요는 그림 <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">1</span></a>에 나와 있다.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="490" id="S1.F1.g1" src="https://arxiv.org/html/2304.08177v3/x1.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> <span class="ltx_text ltx_font_bold" id="S1.F1.2.1">Overview of the proposed Chinese LLaMA and Chinese Alpaca models (based on Meta’s LLaMA and Llama-2).</span> Chinese LLaMA series are foundation models, and Chinese Alpaca series are chat or instruction-following models. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">요약하면, 본 기술보고서의 기여는 다음과 같다:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">우리는 2만 개의 중국어 토큰으로 원래 LLaMA의 어휘를 확장하여 중국어의 인코딩과 디코딩 효율을 높이고 LLaMA의 중국어 이해 능력을 향상시킨다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">우리는 중국 LLaMA 및 Alpaca 모델의 효율적인 훈련 및 배치를 용이하게 하기 위해 LoRA(Low-Rank Adaptation) 접근법을 사용하여 연구자들이 과도한 계산 비용을 초래하지 않고 이러한 모델을 사용할 수 있도록 한다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">본 논문에서 제안한 LLaMA와 Alpaca 모델들은 명령어 수행과 자연어 이해 과제에 대한 성능을 평가함으로써, 중국어 과제의 맥락에서 기존 모델들에 비해 상당한 개선점을 보여준다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">우리는 연구의 자원과 결과를 공개적으로 이용 가능하게 하여 NLP 커뮤니티에서 추가 연구 및 협력을 촉진하고 LLaMA 및 알파카 모델을 다른 언어에 적응하도록 장려한다.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Chinese LLaMA and Chinese Alpaca</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Background</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib28" title="">2023</a>)</cite>는 트랜스포머 아키텍처 <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib30" title="">2017</a>)</cite>에 구축된 기본 디코더 전용 대형 언어 모델이다. GPT 시리즈 및 다른 변압기 기반 LLM과 유사하게 LLaMA는 임베딩 레이어, 다중 변압기 블록 및 언어 모델 헤드로 구성된다. LLaMA는 또한 사전 정규화 <cite class="ltx_cite ltx_citemacro_citep">(Zhang &amp; Sennrich, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib35" title="">2019</a>)</cite>, SwiGLU 활성화 <cite class="ltx_cite ltx_citemacro_citep">(Shazeer, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib24" title="">2020</a>)</cite>, 회전 임베딩 <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib25" title="">2021</a>)</cite>와 같은 다른 모델에서 활용되는 개선 사항을 통합한다. LLaMA는 7B, 13B, 33B 및 65B의 4가지 다른 모델 크기로 제공됩니다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">LLaMA는 크롤링된 웹 페이지, 책, 위키피디아 및 사전 인쇄 논문과 같은 공개적으로 이용 가능한 소스의 혼합을 사용하여 표준 언어 모델링 작업(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS4" title="2.4 Pre-Training Objective ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">2.4</span></a> 참조)으로 사전 훈련되었다. 실험 결과는 LLaMA가 더 작은 모델 크기이지만 GPT-3와 같은 다른 LLM에 비해 경쟁 성능을 제공한다는 것을 보여준다. 이러한 압축성과 효과는 연구자들의 상당한 관심을 받아 LLaMA 기반 모델의 광범위한 사용으로 이어졌다.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Chinese Vocabulary Extension</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">LLaMA의 훈련 세트는 대략 1.4T 토큰을 포괄하며, 라틴어 또는 키릴어 스크립트 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib28" title="">2023</a>)</cite>를 사용하는 다른 유럽 언어에서는 대다수가 영어로, 소수이다. 따라서 LLaMA는 대부분 유럽 언어로 입증된 다국어 및 교차 언어 이해 능력을 가지고 있다. 흥미로운 사실은 우리의 이전 예비 연구는 LLaMA가 중국 텍스트를 생성하는 능력은 제한적이지만 기본적인 중국 이해 능력을 나타낸다는 것을 보여준다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">LLaMA에 중국어 이해와 생성 능력을 향상시키기 위해, 우리는 중국어 말뭉치로 LLaMA 모델을 계속 사전 훈련할 것을 제안한다. 그러나 중국 코퍼라와의 지속적인 사전 교육을 직접 적용하는 것은 여러 가지 어려움에 직면한다. 첫째, 원본 LLaMA 어휘는 1,000자 미만의 한자를 다루고 있어 일반적인 한자 텍스트를 인코딩하기에는 부족하다. LLaMA 토큰화기는 알려지지 않은 UTF-8 문자를 바이트로 토큰화함으로써 이 문제를 우회하지만, 이 전략은 각 한자가 3-4 바이트 토큰으로 분할되기 때문에 시퀀스 길이를 상당히 확장하고 중국 텍스트의 인코딩 및 디코딩 효율을 늦춘다. 둘째, 바이트 토큰은 한자를 나타내기 위해 독점적으로 설계되지 않는다. 바이트 토큰은 다른 언어의 UTF-8 토큰도 의미하기 때문에 바이트 토큰과 변환기 인코더가 한자의 의미 의미를 포착하는 표현을 효과적으로 학습하는 것이 어려워진다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">이러한 문제를 해결하고 인코딩 효율을 향상시키기 위해 중국어 토큰이 추가된 LLaMA 어휘를 확장하고 확장 어휘 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib33" title="">2022</a>)</cite>에 대한 모델을 적용하는 것을 제안한다. 확장 프로세스는 다음과 같이 진행된다:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">토큰사이저의 중국어 텍스트에 대한 지원을 강화하기 위해, 우리는 처음에 20,000의 어휘 크기를 가진 중국어 말뭉치<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The training data is the same as the one for training basic version of our models.</span></span></span>에서 SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo &amp; Richardson, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib13" title="">2018</a>)</cite>로 중국어 토큰사이저를 훈련시켰다.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">우리는 이후 그들의 어휘의 결합을 취하여 중국식 토큰화기를 원래의 LLaMA 토큰화기로 병합한다. 결과적으로, 우리는 49,953의 어휘 크기를 가진 중국 LLaMA 토큰화기로 불리는 병합된 토큰화기를 얻는다.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.4">LLaMA 모델을 중국어 LLaMA 토큰라이저에 적용하기 위해 단어 임베딩 및 언어 모델 헤드를 모양 <math alttext="V\times H" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><mrow id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml">V</mi><mo id="S2.I1.i3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i3.p1.1.m1.1.1.1.cmml">×</mo><mi id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><times id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.1"></times><ci id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2">𝑉</ci><ci id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">V\times H</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.1.m1.1d">italic_V × italic_H</annotation></semantics></math>에서 <math alttext="V^{\prime}\times H" class="ltx_Math" display="inline" id="S2.I1.i3.p1.2.m2.1"><semantics id="S2.I1.i3.p1.2.m2.1a"><mrow id="S2.I1.i3.p1.2.m2.1.1" xref="S2.I1.i3.p1.2.m2.1.1.cmml"><msup id="S2.I1.i3.p1.2.m2.1.1.2" xref="S2.I1.i3.p1.2.m2.1.1.2.cmml"><mi id="S2.I1.i3.p1.2.m2.1.1.2.2" xref="S2.I1.i3.p1.2.m2.1.1.2.2.cmml">V</mi><mo id="S2.I1.i3.p1.2.m2.1.1.2.3" xref="S2.I1.i3.p1.2.m2.1.1.2.3.cmml">′</mo></msup><mo id="S2.I1.i3.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i3.p1.2.m2.1.1.1.cmml">×</mo><mi id="S2.I1.i3.p1.2.m2.1.1.3" xref="S2.I1.i3.p1.2.m2.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.2.m2.1b"><apply id="S2.I1.i3.p1.2.m2.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1"><times id="S2.I1.i3.p1.2.m2.1.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1.1"></times><apply id="S2.I1.i3.p1.2.m2.1.1.2.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.I1.i3.p1.2.m2.1.1.2.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2">superscript</csymbol><ci id="S2.I1.i3.p1.2.m2.1.1.2.2.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2.2">𝑉</ci><ci id="S2.I1.i3.p1.2.m2.1.1.2.3.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2.3">′</ci></apply><ci id="S2.I1.i3.p1.2.m2.1.1.3.cmml" xref="S2.I1.i3.p1.2.m2.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.2.m2.1c">V^{\prime}\times H</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.2.m2.1d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT × italic_H</annotation></semantics></math>로 크기를 조정하며, 여기서 <math alttext="V=32,000" class="ltx_Math" display="inline" id="S2.I1.i3.p1.3.m3.2"><semantics id="S2.I1.i3.p1.3.m3.2a"><mrow id="S2.I1.i3.p1.3.m3.2.3" xref="S2.I1.i3.p1.3.m3.2.3.cmml"><mi id="S2.I1.i3.p1.3.m3.2.3.2" xref="S2.I1.i3.p1.3.m3.2.3.2.cmml">V</mi><mo id="S2.I1.i3.p1.3.m3.2.3.1" xref="S2.I1.i3.p1.3.m3.2.3.1.cmml">=</mo><mrow id="S2.I1.i3.p1.3.m3.2.3.3.2" xref="S2.I1.i3.p1.3.m3.2.3.3.1.cmml"><mn id="S2.I1.i3.p1.3.m3.1.1" xref="S2.I1.i3.p1.3.m3.1.1.cmml">32</mn><mo id="S2.I1.i3.p1.3.m3.2.3.3.2.1" xref="S2.I1.i3.p1.3.m3.2.3.3.1.cmml">,</mo><mn id="S2.I1.i3.p1.3.m3.2.2" xref="S2.I1.i3.p1.3.m3.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.3.m3.2b"><apply id="S2.I1.i3.p1.3.m3.2.3.cmml" xref="S2.I1.i3.p1.3.m3.2.3"><eq id="S2.I1.i3.p1.3.m3.2.3.1.cmml" xref="S2.I1.i3.p1.3.m3.2.3.1"></eq><ci id="S2.I1.i3.p1.3.m3.2.3.2.cmml" xref="S2.I1.i3.p1.3.m3.2.3.2">𝑉</ci><list id="S2.I1.i3.p1.3.m3.2.3.3.1.cmml" xref="S2.I1.i3.p1.3.m3.2.3.3.2"><cn id="S2.I1.i3.p1.3.m3.1.1.cmml" type="integer" xref="S2.I1.i3.p1.3.m3.1.1">32</cn><cn id="S2.I1.i3.p1.3.m3.2.2.cmml" type="integer" xref="S2.I1.i3.p1.3.m3.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.3.m3.2c">V=32,000</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.3.m3.2d">italic_V = 32 , 000</annotation></semantics></math>는 원래 어휘 크기를 나타내고, <math alttext="V^{\prime}=49,953" class="ltx_Math" display="inline" id="S2.I1.i3.p1.4.m4.2"><semantics id="S2.I1.i3.p1.4.m4.2a"><mrow id="S2.I1.i3.p1.4.m4.2.3" xref="S2.I1.i3.p1.4.m4.2.3.cmml"><msup id="S2.I1.i3.p1.4.m4.2.3.2" xref="S2.I1.i3.p1.4.m4.2.3.2.cmml"><mi id="S2.I1.i3.p1.4.m4.2.3.2.2" xref="S2.I1.i3.p1.4.m4.2.3.2.2.cmml">V</mi><mo id="S2.I1.i3.p1.4.m4.2.3.2.3" xref="S2.I1.i3.p1.4.m4.2.3.2.3.cmml">′</mo></msup><mo id="S2.I1.i3.p1.4.m4.2.3.1" xref="S2.I1.i3.p1.4.m4.2.3.1.cmml">=</mo><mrow id="S2.I1.i3.p1.4.m4.2.3.3.2" xref="S2.I1.i3.p1.4.m4.2.3.3.1.cmml"><mn id="S2.I1.i3.p1.4.m4.1.1" xref="S2.I1.i3.p1.4.m4.1.1.cmml">49</mn><mo id="S2.I1.i3.p1.4.m4.2.3.3.2.1" xref="S2.I1.i3.p1.4.m4.2.3.3.1.cmml">,</mo><mn id="S2.I1.i3.p1.4.m4.2.2" xref="S2.I1.i3.p1.4.m4.2.2.cmml">953</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.4.m4.2b"><apply id="S2.I1.i3.p1.4.m4.2.3.cmml" xref="S2.I1.i3.p1.4.m4.2.3"><eq id="S2.I1.i3.p1.4.m4.2.3.1.cmml" xref="S2.I1.i3.p1.4.m4.2.3.1"></eq><apply id="S2.I1.i3.p1.4.m4.2.3.2.cmml" xref="S2.I1.i3.p1.4.m4.2.3.2"><csymbol cd="ambiguous" id="S2.I1.i3.p1.4.m4.2.3.2.1.cmml" xref="S2.I1.i3.p1.4.m4.2.3.2">superscript</csymbol><ci id="S2.I1.i3.p1.4.m4.2.3.2.2.cmml" xref="S2.I1.i3.p1.4.m4.2.3.2.2">𝑉</ci><ci id="S2.I1.i3.p1.4.m4.2.3.2.3.cmml" xref="S2.I1.i3.p1.4.m4.2.3.2.3">′</ci></apply><list id="S2.I1.i3.p1.4.m4.2.3.3.1.cmml" xref="S2.I1.i3.p1.4.m4.2.3.3.2"><cn id="S2.I1.i3.p1.4.m4.1.1.cmml" type="integer" xref="S2.I1.i3.p1.4.m4.1.1">49</cn><cn id="S2.I1.i3.p1.4.m4.2.2.cmml" type="integer" xref="S2.I1.i3.p1.4.m4.2.2">953</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.4.m4.2c">V^{\prime}=49,953</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.4.m4.2d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = 49 , 953</annotation></semantics></math>는 중국어 LLaMA 토큰라이저의 새로운 어휘 크기를 나타낸다. 새로운 행들은 원래의 임베딩 행렬들의 끝에 추가되어, 원래의 어휘 내의 토큰들의 임베딩들이 영향을 받지 않은 채로 유지되는 것을 보장한다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">예비 실험에 따르면 중국 LLaMA 토큰화기에서 생성된 토큰의 수는 원래 LLaMA 토큰화기에서 생성된 토큰의 약 절반이다. 표 <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.T1" title="Table 1 ‣ 2.2 Chinese Vocabulary Extension ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">1</span></a>는 원래 LLaMA tokenizer와 중국 LLaMA tokenizer의 비교를 제공한다. 도시된 바와 같이, 중국 LLaMA 토큰화기는 원본에 비해 인코딩 길이를 상당히 감소시킨다. 고정 컨텍스트 길이로, 모델은 약 2배 많은 정보를 수용할 수 있고, 생성 속도는 원래의 LLaMA 토크나이저보다 2배 빠르다. 이것은 LLaMA 모델의 중국 이해와 생성 능력을 향상시키는 데 제안된 접근법의 효율성을 강조한다.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span> Tokenizer comparisons between original LLaMA and Chinese LLaMA.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.1">
<tbody><tr class="ltx_tr" id="S2.T1.1.1">
<td class="ltx_td ltx_border_tt" id="S2.T1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.2.1">Length</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.3.1">Content</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.2.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.1">Original Sentence</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.2">28</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.2.3">人工智能是计算机科学、心理学、哲学等学科融合的交叉学科。</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.3.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.3.1.1">Original Tokenizer</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.2"><span class="ltx_text" id="S2.T1.1.3.2.1">35</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.3.3">
<span class="ltx_text" id="S2.T1.1.3.3.1"></span><span class="ltx_text" id="S2.T1.1.3.3.2">
<span class="ltx_tabular ltx_align_top" id="S2.T1.1.3.3.2.1">
<span class="ltx_tr" id="S2.T1.1.3.3.2.1.1">
<span class="ltx_td ltx_align_left" id="S2.T1.1.3.3.2.1.1.1">‘_’, ‘人’, ‘工’, ‘智’, ‘能’, ‘是’, ‘计’, ‘算’, ‘机’, ‘科’, ‘学’, ‘、’, ‘心’,</span></span>
<span class="ltx_tr" id="S2.T1.1.3.3.2.1.2">
<span class="ltx_td ltx_align_left" id="S2.T1.1.3.3.2.1.2.1">‘理’, ‘学’, ‘、’, ‘0xE5’, ‘0x93’, ‘0xB2’, ‘学’, ‘等’, ‘学’, ‘科’, ‘0xE8’,</span></span>
<span class="ltx_tr" id="S2.T1.1.3.3.2.1.3">
<span class="ltx_td ltx_align_left" id="S2.T1.1.3.3.2.1.3.1">‘0x9E’, ‘0x8D’, ‘合’, ‘的’, ‘交’, ‘0xE5’, ‘0x8F’, ‘0x89’, ‘学’, ‘科’, ‘。’</span></span>
</span></span> <span class="ltx_text" id="S2.T1.1.3.3.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.1.4.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.4.1.1">Chinese Tokenizer</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.1.4.2" rowspan="2"><span class="ltx_text" id="S2.T1.1.4.2.1">16</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.1.4.3">
<span class="ltx_text" id="S2.T1.1.4.3.1"></span><span class="ltx_text" id="S2.T1.1.4.3.2">
<span class="ltx_tabular ltx_align_top" id="S2.T1.1.4.3.2.1">
<span class="ltx_tr" id="S2.T1.1.4.3.2.1.1">
<span class="ltx_td ltx_align_left" id="S2.T1.1.4.3.2.1.1.1">‘_’, ‘人工智能’, ‘是’, ‘计算机’, ‘科学’, ‘、’, ‘心理学’, ‘、’, ‘哲学’,</span></span>
<span class="ltx_tr" id="S2.T1.1.4.3.2.1.2">
<span class="ltx_td ltx_align_left" id="S2.T1.1.4.3.2.1.2.1">‘等’,‘学科’, ‘融合’, ‘的’, ‘交叉’, ‘学科’, ‘。’</span></span>
</span></span> <span class="ltx_text" id="S2.T1.1.4.3.3"></span>
</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Parameter Efficient Fine-Tuning with LoRA</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">LLM의 전체 매개변수를 업데이트하는 기존의 훈련 패러다임은 엄청나게 비싸고 대부분의 실험실이나 회사에 시간이나 비용이 들지 않는다. Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib10" title="">2021</a>)</cite>는 훈련 가능한 순위 분해 행렬을 도입하면서 미리 훈련된 모델 가중치를 유지하는 파라미터 효율적인 훈련 방법이다. LoRA는 미리 훈련된 모델 가중치를 동결시키고 훈련 가능한 저순위 매트릭스를 각 레이어에 주입한다. 이 접근법은 총 훈련 가능한 파라미터를 상당히 감소시켜, 훨씬 적은 계산 자원으로 LLM을 훈련시키는 것을 가능하게 한다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.7">To be specific, for a linear layer with weight matrix <math alttext="W_{0}\in\mathbb{R}^{d\times k}" class="ltx_Math" display="inline" id="S2.SS3.p2.1.m1.1"><semantics id="S2.SS3.p2.1.m1.1a"><mrow id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml"><msub id="S2.SS3.p2.1.m1.1.1.2" xref="S2.SS3.p2.1.m1.1.1.2.cmml"><mi id="S2.SS3.p2.1.m1.1.1.2.2" xref="S2.SS3.p2.1.m1.1.1.2.2.cmml">W</mi><mn id="S2.SS3.p2.1.m1.1.1.2.3" xref="S2.SS3.p2.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="S2.SS3.p2.1.m1.1.1.1" xref="S2.SS3.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S2.SS3.p2.1.m1.1.1.3" xref="S2.SS3.p2.1.m1.1.1.3.cmml"><mi id="S2.SS3.p2.1.m1.1.1.3.2" xref="S2.SS3.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS3.p2.1.m1.1.1.3.3" xref="S2.SS3.p2.1.m1.1.1.3.3.cmml"><mi id="S2.SS3.p2.1.m1.1.1.3.3.2" xref="S2.SS3.p2.1.m1.1.1.3.3.2.cmml">d</mi><mo id="S2.SS3.p2.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.SS3.p2.1.m1.1.1.3.3.3" xref="S2.SS3.p2.1.m1.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><apply id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1"><in id="S2.SS3.p2.1.m1.1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1.1"></in><apply id="S2.SS3.p2.1.m1.1.1.2.cmml" xref="S2.SS3.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.1.1.2.1.cmml" xref="S2.SS3.p2.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS3.p2.1.m1.1.1.2.2.cmml" xref="S2.SS3.p2.1.m1.1.1.2.2">𝑊</ci><cn id="S2.SS3.p2.1.m1.1.1.2.3.cmml" type="integer" xref="S2.SS3.p2.1.m1.1.1.2.3">0</cn></apply><apply id="S2.SS3.p2.1.m1.1.1.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.1.1.3.1.cmml" xref="S2.SS3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS3.p2.1.m1.1.1.3.2.cmml" xref="S2.SS3.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S2.SS3.p2.1.m1.1.1.3.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3.3"><times id="S2.SS3.p2.1.m1.1.1.3.3.1.cmml" xref="S2.SS3.p2.1.m1.1.1.3.3.1"></times><ci id="S2.SS3.p2.1.m1.1.1.3.3.2.cmml" xref="S2.SS3.p2.1.m1.1.1.3.3.2">𝑑</ci><ci id="S2.SS3.p2.1.m1.1.1.3.3.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">W_{0}\in\mathbb{R}^{d\times k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.1.m1.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.p2.2.m2.1"><semantics id="S2.SS3.p2.2.m2.1a"><mi id="S2.SS3.p2.2.m2.1.1" xref="S2.SS3.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.2.m2.1b"><ci id="S2.SS3.p2.2.m2.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.2.m2.1d">italic_k</annotation></semantics></math> is the input dimension, and <math alttext="d" class="ltx_Math" display="inline" id="S2.SS3.p2.3.m3.1"><semantics id="S2.SS3.p2.3.m3.1a"><mi id="S2.SS3.p2.3.m3.1.1" xref="S2.SS3.p2.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.3.m3.1b"><ci id="S2.SS3.p2.3.m3.1.1.cmml" xref="S2.SS3.p2.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.3.m3.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.3.m3.1d">italic_d</annotation></semantics></math> is the output dimension, LoRA adds two low-rank decomposed trainable matrices <math alttext="B\in\mathbb{R}^{d\times r}" class="ltx_Math" display="inline" id="S2.SS3.p2.4.m4.1"><semantics id="S2.SS3.p2.4.m4.1a"><mrow id="S2.SS3.p2.4.m4.1.1" xref="S2.SS3.p2.4.m4.1.1.cmml"><mi id="S2.SS3.p2.4.m4.1.1.2" xref="S2.SS3.p2.4.m4.1.1.2.cmml">B</mi><mo id="S2.SS3.p2.4.m4.1.1.1" xref="S2.SS3.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S2.SS3.p2.4.m4.1.1.3" xref="S2.SS3.p2.4.m4.1.1.3.cmml"><mi id="S2.SS3.p2.4.m4.1.1.3.2" xref="S2.SS3.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS3.p2.4.m4.1.1.3.3" xref="S2.SS3.p2.4.m4.1.1.3.3.cmml"><mi id="S2.SS3.p2.4.m4.1.1.3.3.2" xref="S2.SS3.p2.4.m4.1.1.3.3.2.cmml">d</mi><mo id="S2.SS3.p2.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p2.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S2.SS3.p2.4.m4.1.1.3.3.3" xref="S2.SS3.p2.4.m4.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.4.m4.1b"><apply id="S2.SS3.p2.4.m4.1.1.cmml" xref="S2.SS3.p2.4.m4.1.1"><in id="S2.SS3.p2.4.m4.1.1.1.cmml" xref="S2.SS3.p2.4.m4.1.1.1"></in><ci id="S2.SS3.p2.4.m4.1.1.2.cmml" xref="S2.SS3.p2.4.m4.1.1.2">𝐵</ci><apply id="S2.SS3.p2.4.m4.1.1.3.cmml" xref="S2.SS3.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p2.4.m4.1.1.3.1.cmml" xref="S2.SS3.p2.4.m4.1.1.3">superscript</csymbol><ci id="S2.SS3.p2.4.m4.1.1.3.2.cmml" xref="S2.SS3.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S2.SS3.p2.4.m4.1.1.3.3.cmml" xref="S2.SS3.p2.4.m4.1.1.3.3"><times id="S2.SS3.p2.4.m4.1.1.3.3.1.cmml" xref="S2.SS3.p2.4.m4.1.1.3.3.1"></times><ci id="S2.SS3.p2.4.m4.1.1.3.3.2.cmml" xref="S2.SS3.p2.4.m4.1.1.3.3.2">𝑑</ci><ci id="S2.SS3.p2.4.m4.1.1.3.3.3.cmml" xref="S2.SS3.p2.4.m4.1.1.3.3.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.4.m4.1c">B\in\mathbb{R}^{d\times r}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.4.m4.1d">italic_B ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_r end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="A\in\mathbb{R}^{r\times k}" class="ltx_Math" display="inline" id="S2.SS3.p2.5.m5.1"><semantics id="S2.SS3.p2.5.m5.1a"><mrow id="S2.SS3.p2.5.m5.1.1" xref="S2.SS3.p2.5.m5.1.1.cmml"><mi id="S2.SS3.p2.5.m5.1.1.2" xref="S2.SS3.p2.5.m5.1.1.2.cmml">A</mi><mo id="S2.SS3.p2.5.m5.1.1.1" xref="S2.SS3.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S2.SS3.p2.5.m5.1.1.3" xref="S2.SS3.p2.5.m5.1.1.3.cmml"><mi id="S2.SS3.p2.5.m5.1.1.3.2" xref="S2.SS3.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS3.p2.5.m5.1.1.3.3" xref="S2.SS3.p2.5.m5.1.1.3.3.cmml"><mi id="S2.SS3.p2.5.m5.1.1.3.3.2" xref="S2.SS3.p2.5.m5.1.1.3.3.2.cmml">r</mi><mo id="S2.SS3.p2.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p2.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S2.SS3.p2.5.m5.1.1.3.3.3" xref="S2.SS3.p2.5.m5.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.5.m5.1b"><apply id="S2.SS3.p2.5.m5.1.1.cmml" xref="S2.SS3.p2.5.m5.1.1"><in id="S2.SS3.p2.5.m5.1.1.1.cmml" xref="S2.SS3.p2.5.m5.1.1.1"></in><ci id="S2.SS3.p2.5.m5.1.1.2.cmml" xref="S2.SS3.p2.5.m5.1.1.2">𝐴</ci><apply id="S2.SS3.p2.5.m5.1.1.3.cmml" xref="S2.SS3.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p2.5.m5.1.1.3.1.cmml" xref="S2.SS3.p2.5.m5.1.1.3">superscript</csymbol><ci id="S2.SS3.p2.5.m5.1.1.3.2.cmml" xref="S2.SS3.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S2.SS3.p2.5.m5.1.1.3.3.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3"><times id="S2.SS3.p2.5.m5.1.1.3.3.1.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3.1"></times><ci id="S2.SS3.p2.5.m5.1.1.3.3.2.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3.2">𝑟</ci><ci id="S2.SS3.p2.5.m5.1.1.3.3.3.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.5.m5.1c">A\in\mathbb{R}^{r\times k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.5.m5.1d">italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_r × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="r" class="ltx_Math" display="inline" id="S2.SS3.p2.6.m6.1"><semantics id="S2.SS3.p2.6.m6.1a"><mi id="S2.SS3.p2.6.m6.1.1" xref="S2.SS3.p2.6.m6.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.6.m6.1b"><ci id="S2.SS3.p2.6.m6.1.1.cmml" xref="S2.SS3.p2.6.m6.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.6.m6.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.6.m6.1d">italic_r</annotation></semantics></math> is the pre-determined rank. The forward pass with input <math alttext="x" class="ltx_Math" display="inline" id="S2.SS3.p2.7.m7.1"><semantics id="S2.SS3.p2.7.m7.1a"><mi id="S2.SS3.p2.7.m7.1.1" xref="S2.SS3.p2.7.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.7.m7.1b"><ci id="S2.SS3.p2.7.m7.1.1.cmml" xref="S2.SS3.p2.7.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.7.m7.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.7.m7.1d">italic_x</annotation></semantics></math> is given by the following equation,</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h=W_{0}x+\Delta Wx=W_{0}x+BAx,~{}~{}B\in\mathbb{R}^{d\times r},A\in\mathbb{R}^%
{r\times d}" class="ltx_Math" display="block" id="S2.E1.m1.2"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.3.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml">h</mi><mo id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.4.cmml"><mrow id="S2.E1.m1.1.1.1.1.4.2" xref="S2.E1.m1.1.1.1.1.4.2.cmml"><msub id="S2.E1.m1.1.1.1.1.4.2.2" xref="S2.E1.m1.1.1.1.1.4.2.2.cmml"><mi id="S2.E1.m1.1.1.1.1.4.2.2.2" xref="S2.E1.m1.1.1.1.1.4.2.2.2.cmml">W</mi><mn id="S2.E1.m1.1.1.1.1.4.2.2.3" xref="S2.E1.m1.1.1.1.1.4.2.2.3.cmml">0</mn></msub><mo id="S2.E1.m1.1.1.1.1.4.2.1" xref="S2.E1.m1.1.1.1.1.4.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.4.2.3" xref="S2.E1.m1.1.1.1.1.4.2.3.cmml">x</mi></mrow><mo id="S2.E1.m1.1.1.1.1.4.1" xref="S2.E1.m1.1.1.1.1.4.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.1.1.4.3" xref="S2.E1.m1.1.1.1.1.4.3.cmml"><mi id="S2.E1.m1.1.1.1.1.4.3.2" mathvariant="normal" xref="S2.E1.m1.1.1.1.1.4.3.2.cmml">Δ</mi><mo id="S2.E1.m1.1.1.1.1.4.3.1" xref="S2.E1.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.4.3.3" xref="S2.E1.m1.1.1.1.1.4.3.3.cmml">W</mi><mo id="S2.E1.m1.1.1.1.1.4.3.1a" xref="S2.E1.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.4.3.4" xref="S2.E1.m1.1.1.1.1.4.3.4.cmml">x</mi></mrow></mrow><mo id="S2.E1.m1.1.1.1.1.5" xref="S2.E1.m1.1.1.1.1.5.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.6" xref="S2.E1.m1.1.1.1.1.6.cmml"><mrow id="S2.E1.m1.1.1.1.1.6.2" xref="S2.E1.m1.1.1.1.1.6.2.cmml"><msub id="S2.E1.m1.1.1.1.1.6.2.2" xref="S2.E1.m1.1.1.1.1.6.2.2.cmml"><mi id="S2.E1.m1.1.1.1.1.6.2.2.2" xref="S2.E1.m1.1.1.1.1.6.2.2.2.cmml">W</mi><mn id="S2.E1.m1.1.1.1.1.6.2.2.3" xref="S2.E1.m1.1.1.1.1.6.2.2.3.cmml">0</mn></msub><mo id="S2.E1.m1.1.1.1.1.6.2.1" xref="S2.E1.m1.1.1.1.1.6.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.6.2.3" xref="S2.E1.m1.1.1.1.1.6.2.3.cmml">x</mi></mrow><mo id="S2.E1.m1.1.1.1.1.6.1" xref="S2.E1.m1.1.1.1.1.6.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.1.1.6.3" xref="S2.E1.m1.1.1.1.1.6.3.cmml"><mi id="S2.E1.m1.1.1.1.1.6.3.2" xref="S2.E1.m1.1.1.1.1.6.3.2.cmml">B</mi><mo id="S2.E1.m1.1.1.1.1.6.3.1" xref="S2.E1.m1.1.1.1.1.6.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.6.3.3" xref="S2.E1.m1.1.1.1.1.6.3.3.cmml">A</mi><mo id="S2.E1.m1.1.1.1.1.6.3.1a" xref="S2.E1.m1.1.1.1.1.6.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.6.3.4" xref="S2.E1.m1.1.1.1.1.6.3.4.cmml">x</mi></mrow></mrow></mrow><mo id="S2.E1.m1.2.2.2.3" rspace="0.827em" xref="S2.E1.m1.2.2.3a.cmml">,</mo><mrow id="S2.E1.m1.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.3.cmml"><mrow id="S2.E1.m1.2.2.2.2.1.1" xref="S2.E1.m1.2.2.2.2.1.1.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml">B</mi><mo id="S2.E1.m1.2.2.2.2.1.1.1" xref="S2.E1.m1.2.2.2.2.1.1.1.cmml">∈</mo><msup id="S2.E1.m1.2.2.2.2.1.1.3" xref="S2.E1.m1.2.2.2.2.1.1.3.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.3.2" xref="S2.E1.m1.2.2.2.2.1.1.3.2.cmml">ℝ</mi><mrow id="S2.E1.m1.2.2.2.2.1.1.3.3" xref="S2.E1.m1.2.2.2.2.1.1.3.3.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.3.3.2" xref="S2.E1.m1.2.2.2.2.1.1.3.3.2.cmml">d</mi><mo id="S2.E1.m1.2.2.2.2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.2.2.2.2.1.1.3.3.1.cmml">×</mo><mi id="S2.E1.m1.2.2.2.2.1.1.3.3.3" xref="S2.E1.m1.2.2.2.2.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><mo id="S2.E1.m1.2.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S2.E1.m1.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.cmml"><mi id="S2.E1.m1.2.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.2.cmml">A</mi><mo id="S2.E1.m1.2.2.2.2.2.2.1" xref="S2.E1.m1.2.2.2.2.2.2.1.cmml">∈</mo><msup id="S2.E1.m1.2.2.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.2.2.3.cmml"><mi id="S2.E1.m1.2.2.2.2.2.2.3.2" xref="S2.E1.m1.2.2.2.2.2.2.3.2.cmml">ℝ</mi><mrow id="S2.E1.m1.2.2.2.2.2.2.3.3" xref="S2.E1.m1.2.2.2.2.2.2.3.3.cmml"><mi id="S2.E1.m1.2.2.2.2.2.2.3.3.2" xref="S2.E1.m1.2.2.2.2.2.2.3.3.2.cmml">r</mi><mo id="S2.E1.m1.2.2.2.2.2.2.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.2.2.2.2.2.2.3.3.1.cmml">×</mo><mi id="S2.E1.m1.2.2.2.2.2.2.3.3.3" xref="S2.E1.m1.2.2.2.2.2.2.3.3.3.cmml">d</mi></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.3a.cmml" xref="S2.E1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1"><and id="S2.E1.m1.1.1.1.1a.cmml" xref="S2.E1.m1.1.1.1.1"></and><apply id="S2.E1.m1.1.1.1.1b.cmml" xref="S2.E1.m1.1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3"></eq><ci id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2">ℎ</ci><apply id="S2.E1.m1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.4"><plus id="S2.E1.m1.1.1.1.1.4.1.cmml" xref="S2.E1.m1.1.1.1.1.4.1"></plus><apply id="S2.E1.m1.1.1.1.1.4.2.cmml" xref="S2.E1.m1.1.1.1.1.4.2"><times id="S2.E1.m1.1.1.1.1.4.2.1.cmml" xref="S2.E1.m1.1.1.1.1.4.2.1"></times><apply id="S2.E1.m1.1.1.1.1.4.2.2.cmml" xref="S2.E1.m1.1.1.1.1.4.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.4.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.4.2.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.4.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.4.2.2.2">𝑊</ci><cn id="S2.E1.m1.1.1.1.1.4.2.2.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.4.2.2.3">0</cn></apply><ci id="S2.E1.m1.1.1.1.1.4.2.3.cmml" xref="S2.E1.m1.1.1.1.1.4.2.3">𝑥</ci></apply><apply id="S2.E1.m1.1.1.1.1.4.3.cmml" xref="S2.E1.m1.1.1.1.1.4.3"><times id="S2.E1.m1.1.1.1.1.4.3.1.cmml" xref="S2.E1.m1.1.1.1.1.4.3.1"></times><ci id="S2.E1.m1.1.1.1.1.4.3.2.cmml" xref="S2.E1.m1.1.1.1.1.4.3.2">Δ</ci><ci id="S2.E1.m1.1.1.1.1.4.3.3.cmml" xref="S2.E1.m1.1.1.1.1.4.3.3">𝑊</ci><ci id="S2.E1.m1.1.1.1.1.4.3.4.cmml" xref="S2.E1.m1.1.1.1.1.4.3.4">𝑥</ci></apply></apply></apply><apply id="S2.E1.m1.1.1.1.1c.cmml" xref="S2.E1.m1.1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.5.cmml" xref="S2.E1.m1.1.1.1.1.5"></eq><share href="#S2.E1.m1.1.1.1.1.4.cmml" id="S2.E1.m1.1.1.1.1d.cmml" xref="S2.E1.m1.1.1.1.1"></share><apply id="S2.E1.m1.1.1.1.1.6.cmml" xref="S2.E1.m1.1.1.1.1.6"><plus id="S2.E1.m1.1.1.1.1.6.1.cmml" xref="S2.E1.m1.1.1.1.1.6.1"></plus><apply id="S2.E1.m1.1.1.1.1.6.2.cmml" xref="S2.E1.m1.1.1.1.1.6.2"><times id="S2.E1.m1.1.1.1.1.6.2.1.cmml" xref="S2.E1.m1.1.1.1.1.6.2.1"></times><apply id="S2.E1.m1.1.1.1.1.6.2.2.cmml" xref="S2.E1.m1.1.1.1.1.6.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.6.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.6.2.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.6.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.6.2.2.2">𝑊</ci><cn id="S2.E1.m1.1.1.1.1.6.2.2.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.6.2.2.3">0</cn></apply><ci id="S2.E1.m1.1.1.1.1.6.2.3.cmml" xref="S2.E1.m1.1.1.1.1.6.2.3">𝑥</ci></apply><apply id="S2.E1.m1.1.1.1.1.6.3.cmml" xref="S2.E1.m1.1.1.1.1.6.3"><times id="S2.E1.m1.1.1.1.1.6.3.1.cmml" xref="S2.E1.m1.1.1.1.1.6.3.1"></times><ci id="S2.E1.m1.1.1.1.1.6.3.2.cmml" xref="S2.E1.m1.1.1.1.1.6.3.2">𝐵</ci><ci id="S2.E1.m1.1.1.1.1.6.3.3.cmml" xref="S2.E1.m1.1.1.1.1.6.3.3">𝐴</ci><ci id="S2.E1.m1.1.1.1.1.6.3.4.cmml" xref="S2.E1.m1.1.1.1.1.6.3.4">𝑥</ci></apply></apply></apply></apply><apply id="S2.E1.m1.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.3a.cmml" xref="S2.E1.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E1.m1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1"><in id="S2.E1.m1.2.2.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1"></in><ci id="S2.E1.m1.2.2.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2">𝐵</ci><apply id="S2.E1.m1.2.2.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.3.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3">superscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.3.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.2">ℝ</ci><apply id="S2.E1.m1.2.2.2.2.1.1.3.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.3"><times id="S2.E1.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.3.1"></times><ci id="S2.E1.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.3.2">𝑑</ci><ci id="S2.E1.m1.2.2.2.2.1.1.3.3.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.3.3">𝑟</ci></apply></apply></apply><apply id="S2.E1.m1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2"><in id="S2.E1.m1.2.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.2.2.1"></in><ci id="S2.E1.m1.2.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2.2">𝐴</ci><apply id="S2.E1.m1.2.2.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.2.2.3.1.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3">superscript</csymbol><ci id="S2.E1.m1.2.2.2.2.2.2.3.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3.2">ℝ</ci><apply id="S2.E1.m1.2.2.2.2.2.2.3.3.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3.3"><times id="S2.E1.m1.2.2.2.2.2.2.3.3.1.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3.3.1"></times><ci id="S2.E1.m1.2.2.2.2.2.2.3.3.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3.3.2">𝑟</ci><ci id="S2.E1.m1.2.2.2.2.2.2.3.3.3.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3.3.3">𝑑</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">h=W_{0}x+\Delta Wx=W_{0}x+BAx,~{}~{}B\in\mathbb{R}^{d\times r},A\in\mathbb{R}^%
{r\times d}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.2d">italic_h = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + roman_Δ italic_W italic_x = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + italic_B italic_A italic_x , italic_B ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_r end_POSTSUPERSCRIPT , italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_r × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.4">트레이닝 동안, <math alttext="W_{0}" class="ltx_Math" display="inline" id="S2.SS3.p4.1.m1.1"><semantics id="S2.SS3.p4.1.m1.1a"><msub id="S2.SS3.p4.1.m1.1.1" xref="S2.SS3.p4.1.m1.1.1.cmml"><mi id="S2.SS3.p4.1.m1.1.1.2" xref="S2.SS3.p4.1.m1.1.1.2.cmml">W</mi><mn id="S2.SS3.p4.1.m1.1.1.3" xref="S2.SS3.p4.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.1.m1.1b"><apply id="S2.SS3.p4.1.m1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.1.m1.1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.p4.1.m1.1.1.2.cmml" xref="S2.SS3.p4.1.m1.1.1.2">𝑊</ci><cn id="S2.SS3.p4.1.m1.1.1.3.cmml" type="integer" xref="S2.SS3.p4.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.1.m1.1c">W_{0}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.1.m1.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>는 동결되고 그래디언트 업데이트를 수신하지 않는 반면, <math alttext="B" class="ltx_Math" display="inline" id="S2.SS3.p4.2.m2.1"><semantics id="S2.SS3.p4.2.m2.1a"><mi id="S2.SS3.p4.2.m2.1.1" xref="S2.SS3.p4.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.2.m2.1b"><ci id="S2.SS3.p4.2.m2.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.2.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.2.m2.1d">italic_B</annotation></semantics></math> 및 <math alttext="A" class="ltx_Math" display="inline" id="S2.SS3.p4.3.m3.1"><semantics id="S2.SS3.p4.3.m3.1a"><mi id="S2.SS3.p4.3.m3.1.1" xref="S2.SS3.p4.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.3.m3.1b"><ci id="S2.SS3.p4.3.m3.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.3.m3.1d">italic_A</annotation></semantics></math>는 업데이트된다. 랭크 <math alttext="r\ll\min(d,k)" class="ltx_Math" display="inline" id="S2.SS3.p4.4.m4.3"><semantics id="S2.SS3.p4.4.m4.3a"><mrow id="S2.SS3.p4.4.m4.3.4" xref="S2.SS3.p4.4.m4.3.4.cmml"><mi id="S2.SS3.p4.4.m4.3.4.2" xref="S2.SS3.p4.4.m4.3.4.2.cmml">r</mi><mo id="S2.SS3.p4.4.m4.3.4.1" xref="S2.SS3.p4.4.m4.3.4.1.cmml">≪</mo><mrow id="S2.SS3.p4.4.m4.3.4.3.2" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml"><mi id="S2.SS3.p4.4.m4.1.1" xref="S2.SS3.p4.4.m4.1.1.cmml">min</mi><mo id="S2.SS3.p4.4.m4.3.4.3.2a" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml">⁡</mo><mrow id="S2.SS3.p4.4.m4.3.4.3.2.1" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml"><mo id="S2.SS3.p4.4.m4.3.4.3.2.1.1" stretchy="false" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml">(</mo><mi id="S2.SS3.p4.4.m4.2.2" xref="S2.SS3.p4.4.m4.2.2.cmml">d</mi><mo id="S2.SS3.p4.4.m4.3.4.3.2.1.2" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml">,</mo><mi id="S2.SS3.p4.4.m4.3.3" xref="S2.SS3.p4.4.m4.3.3.cmml">k</mi><mo id="S2.SS3.p4.4.m4.3.4.3.2.1.3" stretchy="false" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.4.m4.3b"><apply id="S2.SS3.p4.4.m4.3.4.cmml" xref="S2.SS3.p4.4.m4.3.4"><csymbol cd="latexml" id="S2.SS3.p4.4.m4.3.4.1.cmml" xref="S2.SS3.p4.4.m4.3.4.1">much-less-than</csymbol><ci id="S2.SS3.p4.4.m4.3.4.2.cmml" xref="S2.SS3.p4.4.m4.3.4.2">𝑟</ci><apply id="S2.SS3.p4.4.m4.3.4.3.1.cmml" xref="S2.SS3.p4.4.m4.3.4.3.2"><min id="S2.SS3.p4.4.m4.1.1.cmml" xref="S2.SS3.p4.4.m4.1.1"></min><ci id="S2.SS3.p4.4.m4.2.2.cmml" xref="S2.SS3.p4.4.m4.2.2">𝑑</ci><ci id="S2.SS3.p4.4.m4.3.3.cmml" xref="S2.SS3.p4.4.m4.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.4.m4.3c">r\ll\min(d,k)</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.4.m4.3d">italic_r ≪ roman_min ( italic_d , italic_k )</annotation></semantics></math>를 선택함으로써, 큰 동결 매트릭스에 대한 최적화기 상태들을 저장할 필요가 없기 때문에 메모리 소비가 감소된다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">빠듯한 예산을 준수하면서 매개변수 효율적인 훈련을 달성하기 위해 사전 훈련 및 미세 조정 단계를 포함하여 논문의 모든 중국 LLaMA 및 알파카 모델에 LoRA 훈련을 적용한다. 주로 LoRA 어댑터를 어텐션 모듈과 MLP 레이어의 가중치에 통합한다. 모든 선형 변압기 블록에 LoRA를 적용하는 효과는 QLoRA <cite class="ltx_cite ltx_citemacro_citep">(Dettmers et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib6" title="">2023</a>)</cite>에서 검증되며, 이는 우리의 선택이 합리적임을 나타낸다.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Pre-Training Objective</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.2">우리는 표준 인과 언어 모델링(CLM) 작업으로 중국 LLaMA 모델을 사전 훈련한다. 입력 토큰 시퀀스 <math alttext="\bm{x}=(x_{0},x_{1},x_{2},\ldots)" class="ltx_Math" display="inline" id="S2.SS4.p1.1.m1.4"><semantics id="S2.SS4.p1.1.m1.4a"><mrow id="S2.SS4.p1.1.m1.4.4" xref="S2.SS4.p1.1.m1.4.4.cmml"><mi id="S2.SS4.p1.1.m1.4.4.5" xref="S2.SS4.p1.1.m1.4.4.5.cmml">𝒙</mi><mo id="S2.SS4.p1.1.m1.4.4.4" xref="S2.SS4.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S2.SS4.p1.1.m1.4.4.3.3" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml"><mo id="S2.SS4.p1.1.m1.4.4.3.3.4" stretchy="false" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml">(</mo><msub id="S2.SS4.p1.1.m1.2.2.1.1.1" xref="S2.SS4.p1.1.m1.2.2.1.1.1.cmml"><mi id="S2.SS4.p1.1.m1.2.2.1.1.1.2" xref="S2.SS4.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S2.SS4.p1.1.m1.2.2.1.1.1.3" xref="S2.SS4.p1.1.m1.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S2.SS4.p1.1.m1.4.4.3.3.5" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS4.p1.1.m1.3.3.2.2.2" xref="S2.SS4.p1.1.m1.3.3.2.2.2.cmml"><mi id="S2.SS4.p1.1.m1.3.3.2.2.2.2" xref="S2.SS4.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S2.SS4.p1.1.m1.3.3.2.2.2.3" xref="S2.SS4.p1.1.m1.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S2.SS4.p1.1.m1.4.4.3.3.6" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS4.p1.1.m1.4.4.3.3.3" xref="S2.SS4.p1.1.m1.4.4.3.3.3.cmml"><mi id="S2.SS4.p1.1.m1.4.4.3.3.3.2" xref="S2.SS4.p1.1.m1.4.4.3.3.3.2.cmml">x</mi><mn id="S2.SS4.p1.1.m1.4.4.3.3.3.3" xref="S2.SS4.p1.1.m1.4.4.3.3.3.3.cmml">2</mn></msub><mo id="S2.SS4.p1.1.m1.4.4.3.3.7" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml">,</mo><mi id="S2.SS4.p1.1.m1.1.1" mathvariant="normal" xref="S2.SS4.p1.1.m1.1.1.cmml">…</mi><mo id="S2.SS4.p1.1.m1.4.4.3.3.8" stretchy="false" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.4b"><apply id="S2.SS4.p1.1.m1.4.4.cmml" xref="S2.SS4.p1.1.m1.4.4"><eq id="S2.SS4.p1.1.m1.4.4.4.cmml" xref="S2.SS4.p1.1.m1.4.4.4"></eq><ci id="S2.SS4.p1.1.m1.4.4.5.cmml" xref="S2.SS4.p1.1.m1.4.4.5">𝒙</ci><vector id="S2.SS4.p1.1.m1.4.4.3.4.cmml" xref="S2.SS4.p1.1.m1.4.4.3.3"><apply id="S2.SS4.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.SS4.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1.2">𝑥</ci><cn id="S2.SS4.p1.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS4.p1.1.m1.2.2.1.1.1.3">0</cn></apply><apply id="S2.SS4.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS4.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS4.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.SS4.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS4.p1.1.m1.3.3.2.2.2.2">𝑥</ci><cn id="S2.SS4.p1.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S2.SS4.p1.1.m1.3.3.2.2.2.3">1</cn></apply><apply id="S2.SS4.p1.1.m1.4.4.3.3.3.cmml" xref="S2.SS4.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.4.4.3.3.3.1.cmml" xref="S2.SS4.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.SS4.p1.1.m1.4.4.3.3.3.2.cmml" xref="S2.SS4.p1.1.m1.4.4.3.3.3.2">𝑥</ci><cn id="S2.SS4.p1.1.m1.4.4.3.3.3.3.cmml" type="integer" xref="S2.SS4.p1.1.m1.4.4.3.3.3.3">2</cn></apply><ci id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">…</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.4c">\bm{x}=(x_{0},x_{1},x_{2},\ldots)</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.1.m1.4d">bold_italic_x = ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … )</annotation></semantics></math>가 주어지면, 모델은 다음 토큰 <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS4.p1.2.m2.1"><semantics id="S2.SS4.p1.2.m2.1a"><msub id="S2.SS4.p1.2.m2.1.1" xref="S2.SS4.p1.2.m2.1.1.cmml"><mi id="S2.SS4.p1.2.m2.1.1.2" xref="S2.SS4.p1.2.m2.1.1.2.cmml">x</mi><mi id="S2.SS4.p1.2.m2.1.1.3" xref="S2.SS4.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.m2.1b"><apply id="S2.SS4.p1.2.m2.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.2.m2.1.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS4.p1.2.m2.1.1.2.cmml" xref="S2.SS4.p1.2.m2.1.1.2">𝑥</ci><ci id="S2.SS4.p1.2.m2.1.1.3.cmml" xref="S2.SS4.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.2.m2.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>를 자기회귀 방식으로 예측하도록 훈련된다. 수학적으로 목표는 다음과 같은 음의 로그 우도를 최소화하는 것이다.</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx1">
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\textrm{CLM}}(\Theta)=\mathbb{E}_{\bm{x}\sim\mathcal%
{D}_{\textrm{PT}}}\left[-\sum_{i}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};%
\Theta)\right]" class="ltx_Math" display="inline" id="S2.E2.m1.4"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml"><mrow id="S2.E2.m1.4.4.3" xref="S2.E2.m1.4.4.3.cmml"><msub id="S2.E2.m1.4.4.3.2" xref="S2.E2.m1.4.4.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.4.4.3.2.2" xref="S2.E2.m1.4.4.3.2.2.cmml">ℒ</mi><mtext id="S2.E2.m1.4.4.3.2.3" xref="S2.E2.m1.4.4.3.2.3a.cmml">CLM</mtext></msub><mo id="S2.E2.m1.4.4.3.1" xref="S2.E2.m1.4.4.3.1.cmml">⁢</mo><mrow id="S2.E2.m1.4.4.3.3.2" xref="S2.E2.m1.4.4.3.cmml"><mo id="S2.E2.m1.4.4.3.3.2.1" stretchy="false" xref="S2.E2.m1.4.4.3.cmml">(</mo><mi id="S2.E2.m1.1.1" mathvariant="normal" xref="S2.E2.m1.1.1.cmml">Θ</mi><mo id="S2.E2.m1.4.4.3.3.2.2" stretchy="false" xref="S2.E2.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.2" xref="S2.E2.m1.4.4.2.cmml">=</mo><mrow id="S2.E2.m1.4.4.1" xref="S2.E2.m1.4.4.1.cmml"><msub id="S2.E2.m1.4.4.1.3" xref="S2.E2.m1.4.4.1.3.cmml"><mi id="S2.E2.m1.4.4.1.3.2" xref="S2.E2.m1.4.4.1.3.2.cmml">𝔼</mi><mrow id="S2.E2.m1.4.4.1.3.3" xref="S2.E2.m1.4.4.1.3.3.cmml"><mi id="S2.E2.m1.4.4.1.3.3.2" xref="S2.E2.m1.4.4.1.3.3.2.cmml">𝒙</mi><mo id="S2.E2.m1.4.4.1.3.3.1" xref="S2.E2.m1.4.4.1.3.3.1.cmml">∼</mo><msub id="S2.E2.m1.4.4.1.3.3.3" xref="S2.E2.m1.4.4.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.4.4.1.3.3.3.2" xref="S2.E2.m1.4.4.1.3.3.3.2.cmml">𝒟</mi><mtext id="S2.E2.m1.4.4.1.3.3.3.3" xref="S2.E2.m1.4.4.1.3.3.3.3a.cmml">PT</mtext></msub></mrow></msub><mo id="S2.E2.m1.4.4.1.2" xref="S2.E2.m1.4.4.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.4.4.1.1.1" xref="S2.E2.m1.4.4.1.1.2.cmml"><mo id="S2.E2.m1.4.4.1.1.1.2" xref="S2.E2.m1.4.4.1.1.2.1.cmml">[</mo><mrow id="S2.E2.m1.4.4.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.cmml"><mo id="S2.E2.m1.4.4.1.1.1.1a" xref="S2.E2.m1.4.4.1.1.1.1.cmml">−</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S2.E2.m1.4.4.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml"><munder id="S2.E2.m1.4.4.1.1.1.1.1.2a" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml"><mo id="S2.E2.m1.4.4.1.1.1.1.1.2.2" movablelimits="false" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2.cmml">∑</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.2.3.cmml">i</mi></munder></mstyle><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.3.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.3a" lspace="0.167em" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.2.cmml">x</mi><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.4" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml"><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.4" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.5" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><mi id="S2.E2.m1.2.2" mathvariant="normal" xref="S2.E2.m1.2.2.cmml">…</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.6" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml">−</mo><mn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.7" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">;</mo><mi id="S2.E2.m1.3.3" mathvariant="normal" xref="S2.E2.m1.3.3.cmml">Θ</mi></mrow></mrow><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E2.m1.4.4.1.1.1.3" xref="S2.E2.m1.4.4.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4"><eq id="S2.E2.m1.4.4.2.cmml" xref="S2.E2.m1.4.4.2"></eq><apply id="S2.E2.m1.4.4.3.cmml" xref="S2.E2.m1.4.4.3"><times id="S2.E2.m1.4.4.3.1.cmml" xref="S2.E2.m1.4.4.3.1"></times><apply id="S2.E2.m1.4.4.3.2.cmml" xref="S2.E2.m1.4.4.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.3.2.1.cmml" xref="S2.E2.m1.4.4.3.2">subscript</csymbol><ci id="S2.E2.m1.4.4.3.2.2.cmml" xref="S2.E2.m1.4.4.3.2.2">ℒ</ci><ci id="S2.E2.m1.4.4.3.2.3a.cmml" xref="S2.E2.m1.4.4.3.2.3"><mtext id="S2.E2.m1.4.4.3.2.3.cmml" mathsize="70%" xref="S2.E2.m1.4.4.3.2.3">CLM</mtext></ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">Θ</ci></apply><apply id="S2.E2.m1.4.4.1.cmml" xref="S2.E2.m1.4.4.1"><times id="S2.E2.m1.4.4.1.2.cmml" xref="S2.E2.m1.4.4.1.2"></times><apply id="S2.E2.m1.4.4.1.3.cmml" xref="S2.E2.m1.4.4.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.3.1.cmml" xref="S2.E2.m1.4.4.1.3">subscript</csymbol><ci id="S2.E2.m1.4.4.1.3.2.cmml" xref="S2.E2.m1.4.4.1.3.2">𝔼</ci><apply id="S2.E2.m1.4.4.1.3.3.cmml" xref="S2.E2.m1.4.4.1.3.3"><csymbol cd="latexml" id="S2.E2.m1.4.4.1.3.3.1.cmml" xref="S2.E2.m1.4.4.1.3.3.1">similar-to</csymbol><ci id="S2.E2.m1.4.4.1.3.3.2.cmml" xref="S2.E2.m1.4.4.1.3.3.2">𝒙</ci><apply id="S2.E2.m1.4.4.1.3.3.3.cmml" xref="S2.E2.m1.4.4.1.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.3.3.3.1.cmml" xref="S2.E2.m1.4.4.1.3.3.3">subscript</csymbol><ci id="S2.E2.m1.4.4.1.3.3.3.2.cmml" xref="S2.E2.m1.4.4.1.3.3.3.2">𝒟</ci><ci id="S2.E2.m1.4.4.1.3.3.3.3a.cmml" xref="S2.E2.m1.4.4.1.3.3.3.3"><mtext id="S2.E2.m1.4.4.1.3.3.3.3.cmml" mathsize="50%" xref="S2.E2.m1.4.4.1.3.3.3.3">PT</mtext></ci></apply></apply></apply><apply id="S2.E2.m1.4.4.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.4.4.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2">delimited-[]</csymbol><apply id="S2.E2.m1.4.4.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1"><minus id="S2.E2.m1.4.4.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1"></minus><apply id="S2.E2.m1.4.4.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1"><apply id="S2.E2.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E2.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2"></sum><ci id="S2.E2.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1"><times id="S2.E2.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2"></times><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3"><log id="S2.E2.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.1"></log><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.2">𝑝</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.4">conditional</csymbol><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.2">𝑥</ci><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.3">𝑖</ci></apply><list id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3"><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><cn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2">𝑥</ci><cn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" type="integer" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">…</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2">𝑥</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3"><minus id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2">𝑖</ci><cn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml" type="integer" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">Θ</ci></list></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\displaystyle\mathcal{L}_{\textrm{CLM}}(\Theta)=\mathbb{E}_{\bm{x}\sim\mathcal%
{D}_{\textrm{PT}}}\left[-\sum_{i}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};%
\Theta)\right]</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.4d">caligraphic_L start_POSTSUBSCRIPT CLM end_POSTSUBSCRIPT ( roman_Θ ) = blackboard_E start_POSTSUBSCRIPT bold_italic_x ∼ caligraphic_D start_POSTSUBSCRIPT PT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ - ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; roman_Θ ) ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.p1.6">where, <math alttext="\Theta" class="ltx_Math" display="inline" id="S2.SS4.p1.3.m1.1"><semantics id="S2.SS4.p1.3.m1.1a"><mi id="S2.SS4.p1.3.m1.1.1" mathvariant="normal" xref="S2.SS4.p1.3.m1.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.3.m1.1b"><ci id="S2.SS4.p1.3.m1.1.1.cmml" xref="S2.SS4.p1.3.m1.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.3.m1.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.3.m1.1d">roman_Θ</annotation></semantics></math> represents the model parameters, <math alttext="\mathcal{D}_{\textrm{PT}}" class="ltx_Math" display="inline" id="S2.SS4.p1.4.m2.1"><semantics id="S2.SS4.p1.4.m2.1a"><msub id="S2.SS4.p1.4.m2.1.1" xref="S2.SS4.p1.4.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS4.p1.4.m2.1.1.2" xref="S2.SS4.p1.4.m2.1.1.2.cmml">𝒟</mi><mtext id="S2.SS4.p1.4.m2.1.1.3" xref="S2.SS4.p1.4.m2.1.1.3a.cmml">PT</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.4.m2.1b"><apply id="S2.SS4.p1.4.m2.1.1.cmml" xref="S2.SS4.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.4.m2.1.1.1.cmml" xref="S2.SS4.p1.4.m2.1.1">subscript</csymbol><ci id="S2.SS4.p1.4.m2.1.1.2.cmml" xref="S2.SS4.p1.4.m2.1.1.2">𝒟</ci><ci id="S2.SS4.p1.4.m2.1.1.3a.cmml" xref="S2.SS4.p1.4.m2.1.1.3"><mtext id="S2.SS4.p1.4.m2.1.1.3.cmml" mathsize="70%" xref="S2.SS4.p1.4.m2.1.1.3">PT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.4.m2.1c">\mathcal{D}_{\textrm{PT}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.4.m2.1d">caligraphic_D start_POSTSUBSCRIPT PT end_POSTSUBSCRIPT</annotation></semantics></math> is the pre-training dataset, <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS4.p1.5.m3.1"><semantics id="S2.SS4.p1.5.m3.1a"><msub id="S2.SS4.p1.5.m3.1.1" xref="S2.SS4.p1.5.m3.1.1.cmml"><mi id="S2.SS4.p1.5.m3.1.1.2" xref="S2.SS4.p1.5.m3.1.1.2.cmml">x</mi><mi id="S2.SS4.p1.5.m3.1.1.3" xref="S2.SS4.p1.5.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.5.m3.1b"><apply id="S2.SS4.p1.5.m3.1.1.cmml" xref="S2.SS4.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.5.m3.1.1.1.cmml" xref="S2.SS4.p1.5.m3.1.1">subscript</csymbol><ci id="S2.SS4.p1.5.m3.1.1.2.cmml" xref="S2.SS4.p1.5.m3.1.1.2">𝑥</ci><ci id="S2.SS4.p1.5.m3.1.1.3.cmml" xref="S2.SS4.p1.5.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.5.m3.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.5.m3.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the token to be predicted, and <math alttext="x_{0},x_{1},\ldots,x_{i-1}" class="ltx_Math" display="inline" id="S2.SS4.p1.6.m4.4"><semantics id="S2.SS4.p1.6.m4.4a"><mrow id="S2.SS4.p1.6.m4.4.4.3" xref="S2.SS4.p1.6.m4.4.4.4.cmml"><msub id="S2.SS4.p1.6.m4.2.2.1.1" xref="S2.SS4.p1.6.m4.2.2.1.1.cmml"><mi id="S2.SS4.p1.6.m4.2.2.1.1.2" xref="S2.SS4.p1.6.m4.2.2.1.1.2.cmml">x</mi><mn id="S2.SS4.p1.6.m4.2.2.1.1.3" xref="S2.SS4.p1.6.m4.2.2.1.1.3.cmml">0</mn></msub><mo id="S2.SS4.p1.6.m4.4.4.3.4" xref="S2.SS4.p1.6.m4.4.4.4.cmml">,</mo><msub id="S2.SS4.p1.6.m4.3.3.2.2" xref="S2.SS4.p1.6.m4.3.3.2.2.cmml"><mi id="S2.SS4.p1.6.m4.3.3.2.2.2" xref="S2.SS4.p1.6.m4.3.3.2.2.2.cmml">x</mi><mn id="S2.SS4.p1.6.m4.3.3.2.2.3" xref="S2.SS4.p1.6.m4.3.3.2.2.3.cmml">1</mn></msub><mo id="S2.SS4.p1.6.m4.4.4.3.5" xref="S2.SS4.p1.6.m4.4.4.4.cmml">,</mo><mi id="S2.SS4.p1.6.m4.1.1" mathvariant="normal" xref="S2.SS4.p1.6.m4.1.1.cmml">…</mi><mo id="S2.SS4.p1.6.m4.4.4.3.6" xref="S2.SS4.p1.6.m4.4.4.4.cmml">,</mo><msub id="S2.SS4.p1.6.m4.4.4.3.3" xref="S2.SS4.p1.6.m4.4.4.3.3.cmml"><mi id="S2.SS4.p1.6.m4.4.4.3.3.2" xref="S2.SS4.p1.6.m4.4.4.3.3.2.cmml">x</mi><mrow id="S2.SS4.p1.6.m4.4.4.3.3.3" xref="S2.SS4.p1.6.m4.4.4.3.3.3.cmml"><mi id="S2.SS4.p1.6.m4.4.4.3.3.3.2" xref="S2.SS4.p1.6.m4.4.4.3.3.3.2.cmml">i</mi><mo id="S2.SS4.p1.6.m4.4.4.3.3.3.1" xref="S2.SS4.p1.6.m4.4.4.3.3.3.1.cmml">−</mo><mn id="S2.SS4.p1.6.m4.4.4.3.3.3.3" xref="S2.SS4.p1.6.m4.4.4.3.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.6.m4.4b"><list id="S2.SS4.p1.6.m4.4.4.4.cmml" xref="S2.SS4.p1.6.m4.4.4.3"><apply id="S2.SS4.p1.6.m4.2.2.1.1.cmml" xref="S2.SS4.p1.6.m4.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.6.m4.2.2.1.1.1.cmml" xref="S2.SS4.p1.6.m4.2.2.1.1">subscript</csymbol><ci id="S2.SS4.p1.6.m4.2.2.1.1.2.cmml" xref="S2.SS4.p1.6.m4.2.2.1.1.2">𝑥</ci><cn id="S2.SS4.p1.6.m4.2.2.1.1.3.cmml" type="integer" xref="S2.SS4.p1.6.m4.2.2.1.1.3">0</cn></apply><apply id="S2.SS4.p1.6.m4.3.3.2.2.cmml" xref="S2.SS4.p1.6.m4.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS4.p1.6.m4.3.3.2.2.1.cmml" xref="S2.SS4.p1.6.m4.3.3.2.2">subscript</csymbol><ci id="S2.SS4.p1.6.m4.3.3.2.2.2.cmml" xref="S2.SS4.p1.6.m4.3.3.2.2.2">𝑥</ci><cn id="S2.SS4.p1.6.m4.3.3.2.2.3.cmml" type="integer" xref="S2.SS4.p1.6.m4.3.3.2.2.3">1</cn></apply><ci id="S2.SS4.p1.6.m4.1.1.cmml" xref="S2.SS4.p1.6.m4.1.1">…</ci><apply id="S2.SS4.p1.6.m4.4.4.3.3.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS4.p1.6.m4.4.4.3.3.1.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3">subscript</csymbol><ci id="S2.SS4.p1.6.m4.4.4.3.3.2.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3.2">𝑥</ci><apply id="S2.SS4.p1.6.m4.4.4.3.3.3.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3.3"><minus id="S2.SS4.p1.6.m4.4.4.3.3.3.1.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3.3.1"></minus><ci id="S2.SS4.p1.6.m4.4.4.3.3.3.2.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3.3.2">𝑖</ci><cn id="S2.SS4.p1.6.m4.4.4.3.3.3.3.cmml" type="integer" xref="S2.SS4.p1.6.m4.4.4.3.3.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.6.m4.4c">x_{0},x_{1},\ldots,x_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.6.m4.4d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> constitute the context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Supervised Fine-Tuning and Chinese Alpaca</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">사전 훈련된 언어 모델은 사용자 지시를 거의 따를 수 없고 종종 의도하지 않은 콘텐츠를 생성할 수 있다. 이는 식 (<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.E2" title="2 ‣ 2.4 Pre-Training Objective ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">2</span></a>)의 언어 모델링 목적이 “지침을 따르고 질문에 답하라” <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib19" title="">2022</a>)</cite>가 아니라 다음 토큰을 예측하고 있기 때문이다. 언어 모델의 행동을 사용자의 의도에 맞게 조정하기 위해 모델을 미세 조정하여 지시 사항을 따르도록 명시적으로 훈련시킬 수 있다. Stanford Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib27" title="">2023b</a>)</cite>는 Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib31" title="">2022</a>)</cite>의 기법들에 의해 생성된 52K 명령어 추종 데이터에 대해 학습된 LLaMA 기반 명령어 추종 모델이다. 우리는 스탠포드 알파카의 접근법을 따라 중국 LLaMA에 자기 지시 미세 조정을 적용하여 지시 후속 모델인 중국 알파카를 훈련한다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1">중국 알파카는 명령어 후속 데이터 세트의 조합으로 훈련된다. 데이터 세트의 각 예는 명령어와 출력으로 구성된다. 감독된 미세 조정 태스크는 인과 언어 모델링 태스크와 유사하다: 모델은 명령으로 프롬프트되고 출력을 자동으로 생성하도록 트레이닝된다. 명령은 프롬프트 템플릿으로 래핑되며 출력은 템플릿에 즉시 따라갑니다. 미세 조정 및 추론을 위해 스탠포드 알파카의 다음 템플릿을 채택하며 입력 시퀀스는 다음과 같습니다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p3">
<blockquote class="ltx_quote" id="S2.SS5.p3.1">
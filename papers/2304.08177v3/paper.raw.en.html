<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</title>
<!--Generated on Fri Feb 23 02:24:22 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2304.08177v3/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2304.08177v3">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2304.08177v3">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2304.08177v3" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S1" title="1 Introduction ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2" title="2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Chinese LLaMA and Chinese Alpaca</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS1" title="2.1 Background ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS2" title="2.2 Chinese Vocabulary Extension ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Chinese Vocabulary Extension</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS3" title="2.3 Parameter Efficient Fine-Tuning with LoRA ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Parameter Efficient Fine-Tuning with LoRA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS4" title="2.4 Pre-Training Objective ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Pre-Training Objective</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS5" title="2.5 Supervised Fine-Tuning and Chinese Alpaca ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.5 </span>Supervised Fine-Tuning and Chinese Alpaca</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S3" title="3 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Setups</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S3.SS1" title="3.1 Experimental Setups for Pre-training ‣ 3 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental Setups for Pre-training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S3.SS2" title="3.2 Experimental Setups for Instruction Fine-tuning ‣ 3 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Experimental Setups for Instruction Fine-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4" title="4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results on Instruction-Following Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS1" title="4.1 Task Design and Evaluation Method ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Task Design and Evaluation Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS2" title="4.2 Experimental Setups for Decoding ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experimental Setups for Decoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3" title="4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3.SSS1" title="4.3.1 Multi-turn Dialogue ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Multi-turn Dialogue</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3.SSS2" title="4.3.2 Text Generation ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Text Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3.SSS3" title="4.3.3 Numerical Calculation and Reasoning ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Numerical Calculation and Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3.SSS4" title="4.3.4 Coding ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.4 </span>Coding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.SS3.SSS5" title="4.3.5 Ethics ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.5 </span>Ethics</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5" title="5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results on Natural Language Understanding Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS1" title="5.1 Task Description ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Task Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS2" title="5.2 Decoding Strategy ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Decoding Strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS3" title="5.3 Comparisons to Original LLaMA ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Comparisons to Original LLaMA</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS3.SSS0.Px1" title="Chinese LLaMA improves original LLaMA. ‣ 5.3 Comparisons to Original LLaMA ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title">Chinese LLaMA improves original LLaMA.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS3.SSS0.Px2" title="Alpaca models show significant improvements over LLaMA. ‣ 5.3 Comparisons to Original LLaMA ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title">Alpaca models show significant improvements over LLaMA.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS3.SSS0.Px3" title="LLaMA generally yields better performance in a few-shot setting, while Alpaca prefers zero-shot. ‣ 5.3 Comparisons to Original LLaMA ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title">LLaMA generally yields better performance in a few-shot setting, while Alpaca prefers zero-shot.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.SS4" title="5.4 Comparisons to Other Models ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Comparisons to Other Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S6" title="6 Effect of Different Quantization Methods ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Effect of Different Quantization Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S7" title="7 Conclusion ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1" title="Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1.SS1" title="A.1 C-Eval ‣ Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>C-Eval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1.SS2" title="A.2 CMMLU ‣ Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>CMMLU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1.SS3" title="A.3 LongBench ‣ Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>LongBench</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2304.08177v3 [cs.CL] 23 Feb 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiming Cui 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">ymcui@ieee.org</span> &amp;Ziqing Yang<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">ziqingyang@gmail.com</span> &amp;Xin Yao 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">yaoxin94@foxmail.com</span>
</span><span class="ltx_author_notes">Equal contributions.</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id4.id1">Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associated with training and deploying LLMs present substantial obstacles to transparent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages.
In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA’s existing vocabulary with an additional 20,000 Chinese tokens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, significantly enhancing the model’s ability to comprehend and execute instructions.
Our experimental results indicate that the newly proposed model markedly enhances the original LLaMA’s proficiency in understanding and generating Chinese content. Additionally, the results on the C-Eval dataset yield competitive performance among the models with several times the size of ours.
We have made our pre-trained models, training scripts, and other resources available through GitHub, fostering open research for our community.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Chinese LLaMA series: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca" title="">https://github.com/ymcui/Chinese-LLaMA-Alpaca</a></span></span></span><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Chinese Llama-2 series: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2" title="">https://github.com/ymcui/Chinese-LLaMA-Alpaca-2</a></span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Natural language processing (NLP) field has witnessed a substantial paradigm shift with the advent of Large Language Models (LLMs). These models, distinguished by their considerable size and comprehensive training data, have demonstrated extraordinary abilities in comprehending and producing human-like text. In contrast to pre-trained language models dedicated to text understanding, such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib7" title="">2019</a>)</cite>, the GPT series <cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib21" title="">2018</a>)</cite> accentuates text generation, positioning them as more suitable platforms for creativity compared to their counterparts. Notably, the latest members of the GPT family, namely ChatGPT and GPT-4, have garnered significant attention, establishing themselves as leading examples in this rapidly evolving field.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib17" title="">2022</a>)</cite>, evolved from InstructGPT <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib19" title="">2022</a>)</cite>, serves as an advanced conversational AI model capable of conducting context-aware, human-like interactions. Its success set the stage for the development of GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib18" title="">2023</a>)</cite>, a more sophisticated LLM, demonstrating even greater potential in natural language understanding, generation, and various NLP tasks, especially for its multi-modal and reasoning abilities. These models have catalyzed new research directions and applications, intensifying interest in exploring the potential of Artificial General Intelligence (AGI). Exhibiting impressive performance across multiple benchmarks, they have also demonstrated capabilities for few-shot learning and adaptability to new tasks, significantly driving the expansion of NLP research. Consequently, they have inspired both researchers and industry professionals to further harness their potential across a wide array of applications, including sentiment analysis, machine translation, question-answering systems, and more.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, as impactful as LLMs have been, their implementation comes with inherent limitations that hamper transparent and open research. A major concern is their proprietary nature, which restricts access to the models, thus inhibiting the broader research community’s ability to build upon their successes. Furthermore, the vast computational resources necessary for training and deploying these models present a challenge for researchers with limited resources, further compounding the accessibility problem.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To tackle these limitations, the NLP research community has gravitated towards open-source alternatives to promote greater transparency and collaboration. LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib28" title="">2023</a>)</cite>, Llama-2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib29" title="">2023</a>)</cite>, and Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib26" title="">2023a</a>)</cite> serve as notable examples of such initiatives. These open-source LLMs are intended to facilitate academic research and accelerate progress within the NLP field. The aim of open-sourcing these models is to foster an environment conducive to further advancements in model development, fine-tuning, and evaluation, ultimately leading to the creation of robust, capable LLMs applicable to a wide variety of uses.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Despite the considerable strides made by LLaMA and Alpaca in NLP, they exhibit inherent limitations concerning native support for Chinese language tasks. Their vocabularies contain only a few hundred Chinese tokens, substantially hindering their efficiency in encoding and decoding Chinese text. Building on our previous work with the Chinese BERT series <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib4" title="">2021</a>)</cite> and Chinese minority-oriented multilingual pre-trained models <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib33" title="">2022</a>)</cite>, in this technical report, we propose the development of Chinese LLaMA and Alpaca models with enhanced capabilities for understanding and generating Chinese content. We extend the original LLaMA’s vocabulary with an additional 20,000 Chinese tokens, significantly improving its proficiency in processing and generating Chinese text. To ensure efficient training and deployment of these models, we employ the Low-Rank Adaptation (LoRA) approach <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib10" title="">2021</a>)</cite>, enabling us to train and fine-tune the models without excessive computational costs. We anticipate our preliminary study to enhance the Chinese understanding and generation capabilities of LLaMA and Alpaca serves as a foundation for researchers aiming to adapt these models to other languages. By showcasing the feasibility and effectiveness of our approach, we offer insights and methodologies that can be employed to extend vocabularies and improve the performance of LLaMA and Alpaca models in various languages.
An overview of the proposed models is depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">1</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="490" id="S1.F1.g1" src="x1.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> <span class="ltx_text ltx_font_bold" id="S1.F1.2.1">Overview of the proposed Chinese LLaMA and Chinese Alpaca models (based on Meta’s LLaMA and Llama-2).</span> Chinese LLaMA series are foundation models, and Chinese Alpaca series are chat or instruction-following models. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In summary, the contributions of this technical report are as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We enhance the encoding and decoding efficiency of the Chinese language and improve LLaMA’s Chinese understanding ability by extending the original LLaMA’s vocabulary with an additional 20,000 Chinese tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We employ the Low-Rank Adaptation (LoRA) approach to facilitate efficient training and deployment of the Chinese LLaMA and Alpaca models, enabling researchers to work with these models without incurring excessive computational costs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We evaluate the performance of the proposed LLaMA and Alpaca models in instruction-following tasks and natural language understanding tasks, thereby demonstrating substantial improvements over their original counterparts in the context of Chinese language tasks.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We make the resources and findings of our study publicly available, fostering further research and collaboration in the NLP community and encouraging the adaptation of LLaMA and Alpaca models to other languages.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Chinese LLaMA and Chinese Alpaca</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Background</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib28" title="">2023</a>)</cite> is a foundational, decoder-only large language model built upon the transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib30" title="">2017</a>)</cite>. Similar to the GPT series and other transformer-based LLMs, LLaMA consists of an embedding layer, multiple transformer blocks, and a language model head.
LLaMA also incorporates improvements utilized in different models, such as pre-normalization <cite class="ltx_cite ltx_citemacro_citep">(Zhang &amp; Sennrich, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib35" title="">2019</a>)</cite>, SwiGLU activation <cite class="ltx_cite ltx_citemacro_citep">(Shazeer, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib24" title="">2020</a>)</cite>, and rotary embeddings <cite class="ltx_cite ltx_citemacro_citep">(Su et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib25" title="">2021</a>)</cite>.
LLaMA is available in four different model sizes: 7B, 13B, 33B, and 65B.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">LLaMA has been pre-trained with a standard language modeling task (see Section <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS4" title="2.4 Pre-Training Objective ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">2.4</span></a>) using a mix of publicly available sources, such as crawled web pages, books, Wikipedia, and preprint papers. Experimental findings reveal that LLaMA delivers competitive performance compared to other LLMs like GPT-3, albeit at a smaller model size. This compactness and effectiveness have garnered considerable attention from researchers, leading to the widespread use of LLaMA-based models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Chinese Vocabulary Extension</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">LLaMA’s training set encompasses roughly 1.4T tokens, with the majority in English and a small fraction in other European languages using Latin or Cyrillic scripts <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib28" title="">2023</a>)</cite>. Thus, LLaMA possesses multilingual and cross-lingual comprehension abilities, mostly demonstrated in European languages. Interestingly, our prior preliminary study reveals that LLaMA exhibits basic Chinese understanding ability, although its capacity to generate Chinese texts is limited.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">To equip LLaMA with enhanced Chinese understanding and generation capabilities, we propose to continue pre-training the LLaMA model with Chinese corpora. However, directly applying continual pre-training with Chinese corpora encounters several challenges. Firstly, the original LLaMA vocabulary covers less than a thousand Chinese characters, which is insufficient to encode general Chinese texts. Although the LLaMA tokenizer circumvents this issue by tokenizing unknown UTF-8 characters to bytes, this strategy significantly extends sequence length and slows down the encoding and decoding efficiency of Chinese texts, as each Chinese character splits into 3-4 byte tokens. Secondly, byte tokens are not exclusively designed to represent Chinese characters. Since byte tokens also signify UTF-8 tokens in other languages, it becomes challenging for byte tokens and transformer encoders to effectively learn representations capturing the semantic meaning of Chinese characters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">To address these problems and improve encoding efficiency, we propose to extend LLaMA vocabulary with additional Chinese tokens and adapt the model for the extended vocabulary <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib33" title="">2022</a>)</cite>. The extension process proceeds as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">To enhance the tokenizer’s support for Chinese texts, we initially train a Chinese tokenizer with SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo &amp; Richardson, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib13" title="">2018</a>)</cite> on Chinese corpora<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The training data is the same as the one for training basic version of our models.</span></span></span> with a vocabulary size of 20,000.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">We subsequently merge the Chinese tokenizer into the original LLaMA tokenizer by taking the union of their vocabularies. Consequently, we obtain a merged tokenizer, which we term the Chinese LLaMA tokenizer, with a vocabulary size of 49,953.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.4">To adapt the LLaMA model for the Chinese LLaMA tokenizer, we resize the word embeddings and language model head from shape <math alttext="V\times H" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><mrow id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml">V</mi><mo id="S2.I1.i3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i3.p1.1.m1.1.1.1.cmml">×</mo><mi id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><times id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.1"></times><ci id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2">𝑉</ci><ci id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">V\times H</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.1.m1.1d">italic_V × italic_H</annotation></semantics></math> to <math alttext="V^{\prime}\times H" class="ltx_Math" display="inline" id="S2.I1.i3.p1.2.m2.1"><semantics id="S2.I1.i3.p1.2.m2.1a"><mrow id="S2.I1.i3.p1.2.m2.1.1" xref="S2.I1.i3.p1.2.m2.1.1.cmml"><msup id="S2.I1.i3.p1.2.m2.1.1.2" xref="S2.I1.i3.p1.2.m2.1.1.2.cmml"><mi id="S2.I1.i3.p1.2.m2.1.1.2.2" xref="S2.I1.i3.p1.2.m2.1.1.2.2.cmml">V</mi><mo id="S2.I1.i3.p1.2.m2.1.1.2.3" xref="S2.I1.i3.p1.2.m2.1.1.2.3.cmml">′</mo></msup><mo id="S2.I1.i3.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i3.p1.2.m2.1.1.1.cmml">×</mo><mi id="S2.I1.i3.p1.2.m2.1.1.3" xref="S2.I1.i3.p1.2.m2.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.2.m2.1b"><apply id="S2.I1.i3.p1.2.m2.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1"><times id="S2.I1.i3.p1.2.m2.1.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1.1"></times><apply id="S2.I1.i3.p1.2.m2.1.1.2.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.I1.i3.p1.2.m2.1.1.2.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2">superscript</csymbol><ci id="S2.I1.i3.p1.2.m2.1.1.2.2.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2.2">𝑉</ci><ci id="S2.I1.i3.p1.2.m2.1.1.2.3.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2.3">′</ci></apply><ci id="S2.I1.i3.p1.2.m2.1.1.3.cmml" xref="S2.I1.i3.p1.2.m2.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.2.m2.1c">V^{\prime}\times H</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.2.m2.1d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT × italic_H</annotation></semantics></math>, where <math alttext="V=32,000" class="ltx_Math" display="inline" id="S2.I1.i3.p1.3.m3.2"><semantics id="S2.I1.i3.p1.3.m3.2a"><mrow id="S2.I1.i3.p1.3.m3.2.3" xref="S2.I1.i3.p1.3.m3.2.3.cmml"><mi id="S2.I1.i3.p1.3.m3.2.3.2" xref="S2.I1.i3.p1.3.m3.2.3.2.cmml">V</mi><mo id="S2.I1.i3.p1.3.m3.2.3.1" xref="S2.I1.i3.p1.3.m3.2.3.1.cmml">=</mo><mrow id="S2.I1.i3.p1.3.m3.2.3.3.2" xref="S2.I1.i3.p1.3.m3.2.3.3.1.cmml"><mn id="S2.I1.i3.p1.3.m3.1.1" xref="S2.I1.i3.p1.3.m3.1.1.cmml">32</mn><mo id="S2.I1.i3.p1.3.m3.2.3.3.2.1" xref="S2.I1.i3.p1.3.m3.2.3.3.1.cmml">,</mo><mn id="S2.I1.i3.p1.3.m3.2.2" xref="S2.I1.i3.p1.3.m3.2.2.cmml">000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.3.m3.2b"><apply id="S2.I1.i3.p1.3.m3.2.3.cmml" xref="S2.I1.i3.p1.3.m3.2.3"><eq id="S2.I1.i3.p1.3.m3.2.3.1.cmml" xref="S2.I1.i3.p1.3.m3.2.3.1"></eq><ci id="S2.I1.i3.p1.3.m3.2.3.2.cmml" xref="S2.I1.i3.p1.3.m3.2.3.2">𝑉</ci><list id="S2.I1.i3.p1.3.m3.2.3.3.1.cmml" xref="S2.I1.i3.p1.3.m3.2.3.3.2"><cn id="S2.I1.i3.p1.3.m3.1.1.cmml" type="integer" xref="S2.I1.i3.p1.3.m3.1.1">32</cn><cn id="S2.I1.i3.p1.3.m3.2.2.cmml" type="integer" xref="S2.I1.i3.p1.3.m3.2.2">000</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.3.m3.2c">V=32,000</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.3.m3.2d">italic_V = 32 , 000</annotation></semantics></math> denotes the original vocabulary size, and <math alttext="V^{\prime}=49,953" class="ltx_Math" display="inline" id="S2.I1.i3.p1.4.m4.2"><semantics id="S2.I1.i3.p1.4.m4.2a"><mrow id="S2.I1.i3.p1.4.m4.2.3" xref="S2.I1.i3.p1.4.m4.2.3.cmml"><msup id="S2.I1.i3.p1.4.m4.2.3.2" xref="S2.I1.i3.p1.4.m4.2.3.2.cmml"><mi id="S2.I1.i3.p1.4.m4.2.3.2.2" xref="S2.I1.i3.p1.4.m4.2.3.2.2.cmml">V</mi><mo id="S2.I1.i3.p1.4.m4.2.3.2.3" xref="S2.I1.i3.p1.4.m4.2.3.2.3.cmml">′</mo></msup><mo id="S2.I1.i3.p1.4.m4.2.3.1" xref="S2.I1.i3.p1.4.m4.2.3.1.cmml">=</mo><mrow id="S2.I1.i3.p1.4.m4.2.3.3.2" xref="S2.I1.i3.p1.4.m4.2.3.3.1.cmml"><mn id="S2.I1.i3.p1.4.m4.1.1" xref="S2.I1.i3.p1.4.m4.1.1.cmml">49</mn><mo id="S2.I1.i3.p1.4.m4.2.3.3.2.1" xref="S2.I1.i3.p1.4.m4.2.3.3.1.cmml">,</mo><mn id="S2.I1.i3.p1.4.m4.2.2" xref="S2.I1.i3.p1.4.m4.2.2.cmml">953</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.4.m4.2b"><apply id="S2.I1.i3.p1.4.m4.2.3.cmml" xref="S2.I1.i3.p1.4.m4.2.3"><eq id="S2.I1.i3.p1.4.m4.2.3.1.cmml" xref="S2.I1.i3.p1.4.m4.2.3.1"></eq><apply id="S2.I1.i3.p1.4.m4.2.3.2.cmml" xref="S2.I1.i3.p1.4.m4.2.3.2"><csymbol cd="ambiguous" id="S2.I1.i3.p1.4.m4.2.3.2.1.cmml" xref="S2.I1.i3.p1.4.m4.2.3.2">superscript</csymbol><ci id="S2.I1.i3.p1.4.m4.2.3.2.2.cmml" xref="S2.I1.i3.p1.4.m4.2.3.2.2">𝑉</ci><ci id="S2.I1.i3.p1.4.m4.2.3.2.3.cmml" xref="S2.I1.i3.p1.4.m4.2.3.2.3">′</ci></apply><list id="S2.I1.i3.p1.4.m4.2.3.3.1.cmml" xref="S2.I1.i3.p1.4.m4.2.3.3.2"><cn id="S2.I1.i3.p1.4.m4.1.1.cmml" type="integer" xref="S2.I1.i3.p1.4.m4.1.1">49</cn><cn id="S2.I1.i3.p1.4.m4.2.2.cmml" type="integer" xref="S2.I1.i3.p1.4.m4.2.2">953</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.4.m4.2c">V^{\prime}=49,953</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.4.m4.2d">italic_V start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = 49 , 953</annotation></semantics></math> is the new vocabulary size of the Chinese LLaMA tokenizer. The new rows are appended to the end of the original embedding matrices, ensuring that the embeddings of the tokens in the original vocabulary remain unaffected.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Preliminary experiments indicate that the number of tokens generated by the Chinese LLaMA tokenizer is approximately half of those generated by the original LLaMA tokenizer. Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.T1" title="Table 1 ‣ 2.2 Chinese Vocabulary Extension ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">1</span></a> provides a comparison between the original LLaMA tokenizer and our Chinese LLaMA tokenizer. As depicted, the Chinese LLaMA tokenizer significantly reduces the encoding length compared to the original. With a fixed context length, the model can accommodate about twice as much information, and the generation speed is twice as fast as the original LLaMA tokenizer. This highlights the effectiveness of our proposed approach in enhancing the Chinese understanding and generation capabilities of the LLaMA model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span> Tokenizer comparisons between original LLaMA and Chinese LLaMA.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.1">
<tbody><tr class="ltx_tr" id="S2.T1.1.1">
<td class="ltx_td ltx_border_tt" id="S2.T1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.2.1">Length</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.3.1">Content</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.2.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.1">Original Sentence</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.2">28</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.2.3">人工智能是计算机科学、心理学、哲学等学科融合的交叉学科。</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.3.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.3.1.1">Original Tokenizer</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.2"><span class="ltx_text" id="S2.T1.1.3.2.1">35</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.3.3">
<span class="ltx_text" id="S2.T1.1.3.3.1"></span><span class="ltx_text" id="S2.T1.1.3.3.2">
<span class="ltx_tabular ltx_align_top" id="S2.T1.1.3.3.2.1">
<span class="ltx_tr" id="S2.T1.1.3.3.2.1.1">
<span class="ltx_td ltx_align_left" id="S2.T1.1.3.3.2.1.1.1">‘_’, ‘人’, ‘工’, ‘智’, ‘能’, ‘是’, ‘计’, ‘算’, ‘机’, ‘科’, ‘学’, ‘、’, ‘心’,</span></span>
<span class="ltx_tr" id="S2.T1.1.3.3.2.1.2">
<span class="ltx_td ltx_align_left" id="S2.T1.1.3.3.2.1.2.1">‘理’, ‘学’, ‘、’, ‘0xE5’, ‘0x93’, ‘0xB2’, ‘学’, ‘等’, ‘学’, ‘科’, ‘0xE8’,</span></span>
<span class="ltx_tr" id="S2.T1.1.3.3.2.1.3">
<span class="ltx_td ltx_align_left" id="S2.T1.1.3.3.2.1.3.1">‘0x9E’, ‘0x8D’, ‘合’, ‘的’, ‘交’, ‘0xE5’, ‘0x8F’, ‘0x89’, ‘学’, ‘科’, ‘。’</span></span>
</span></span> <span class="ltx_text" id="S2.T1.1.3.3.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.1.4.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.4.1.1">Chinese Tokenizer</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T1.1.4.2" rowspan="2"><span class="ltx_text" id="S2.T1.1.4.2.1">16</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.1.4.3">
<span class="ltx_text" id="S2.T1.1.4.3.1"></span><span class="ltx_text" id="S2.T1.1.4.3.2">
<span class="ltx_tabular ltx_align_top" id="S2.T1.1.4.3.2.1">
<span class="ltx_tr" id="S2.T1.1.4.3.2.1.1">
<span class="ltx_td ltx_align_left" id="S2.T1.1.4.3.2.1.1.1">‘_’, ‘人工智能’, ‘是’, ‘计算机’, ‘科学’, ‘、’, ‘心理学’, ‘、’, ‘哲学’,</span></span>
<span class="ltx_tr" id="S2.T1.1.4.3.2.1.2">
<span class="ltx_td ltx_align_left" id="S2.T1.1.4.3.2.1.2.1">‘等’,‘学科’, ‘融合’, ‘的’, ‘交叉’, ‘学科’, ‘。’</span></span>
</span></span> <span class="ltx_text" id="S2.T1.1.4.3.3"></span>
</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Parameter Efficient Fine-Tuning with LoRA</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The conventional training paradigm that updates the full parameters of LLMs is prohibitively expensive and is not time- or cost-feasible to most labs or companies. Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib10" title="">2021</a>)</cite> is a parameter-efficient training method that maintains the pre-trained model weights while introducing trainable rank decomposition matrices. LoRA freezes the pre-trained model weights and injects trainable low-rank matrices into each layer. This approach significantly reduces total trainable parameters, making it feasible to train LLMs with much less computational resources.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.7">To be specific, for a linear layer with weight matrix <math alttext="W_{0}\in\mathbb{R}^{d\times k}" class="ltx_Math" display="inline" id="S2.SS3.p2.1.m1.1"><semantics id="S2.SS3.p2.1.m1.1a"><mrow id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml"><msub id="S2.SS3.p2.1.m1.1.1.2" xref="S2.SS3.p2.1.m1.1.1.2.cmml"><mi id="S2.SS3.p2.1.m1.1.1.2.2" xref="S2.SS3.p2.1.m1.1.1.2.2.cmml">W</mi><mn id="S2.SS3.p2.1.m1.1.1.2.3" xref="S2.SS3.p2.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="S2.SS3.p2.1.m1.1.1.1" xref="S2.SS3.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S2.SS3.p2.1.m1.1.1.3" xref="S2.SS3.p2.1.m1.1.1.3.cmml"><mi id="S2.SS3.p2.1.m1.1.1.3.2" xref="S2.SS3.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS3.p2.1.m1.1.1.3.3" xref="S2.SS3.p2.1.m1.1.1.3.3.cmml"><mi id="S2.SS3.p2.1.m1.1.1.3.3.2" xref="S2.SS3.p2.1.m1.1.1.3.3.2.cmml">d</mi><mo id="S2.SS3.p2.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S2.SS3.p2.1.m1.1.1.3.3.3" xref="S2.SS3.p2.1.m1.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><apply id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1"><in id="S2.SS3.p2.1.m1.1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1.1"></in><apply id="S2.SS3.p2.1.m1.1.1.2.cmml" xref="S2.SS3.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.1.1.2.1.cmml" xref="S2.SS3.p2.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS3.p2.1.m1.1.1.2.2.cmml" xref="S2.SS3.p2.1.m1.1.1.2.2">𝑊</ci><cn id="S2.SS3.p2.1.m1.1.1.2.3.cmml" type="integer" xref="S2.SS3.p2.1.m1.1.1.2.3">0</cn></apply><apply id="S2.SS3.p2.1.m1.1.1.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.1.1.3.1.cmml" xref="S2.SS3.p2.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS3.p2.1.m1.1.1.3.2.cmml" xref="S2.SS3.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S2.SS3.p2.1.m1.1.1.3.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3.3"><times id="S2.SS3.p2.1.m1.1.1.3.3.1.cmml" xref="S2.SS3.p2.1.m1.1.1.3.3.1"></times><ci id="S2.SS3.p2.1.m1.1.1.3.3.2.cmml" xref="S2.SS3.p2.1.m1.1.1.3.3.2">𝑑</ci><ci id="S2.SS3.p2.1.m1.1.1.3.3.3.cmml" xref="S2.SS3.p2.1.m1.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">W_{0}\in\mathbb{R}^{d\times k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.1.m1.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.p2.2.m2.1"><semantics id="S2.SS3.p2.2.m2.1a"><mi id="S2.SS3.p2.2.m2.1.1" xref="S2.SS3.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.2.m2.1b"><ci id="S2.SS3.p2.2.m2.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.2.m2.1d">italic_k</annotation></semantics></math> is the input dimension, and <math alttext="d" class="ltx_Math" display="inline" id="S2.SS3.p2.3.m3.1"><semantics id="S2.SS3.p2.3.m3.1a"><mi id="S2.SS3.p2.3.m3.1.1" xref="S2.SS3.p2.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.3.m3.1b"><ci id="S2.SS3.p2.3.m3.1.1.cmml" xref="S2.SS3.p2.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.3.m3.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.3.m3.1d">italic_d</annotation></semantics></math> is the output dimension, LoRA adds two low-rank decomposed trainable matrices <math alttext="B\in\mathbb{R}^{d\times r}" class="ltx_Math" display="inline" id="S2.SS3.p2.4.m4.1"><semantics id="S2.SS3.p2.4.m4.1a"><mrow id="S2.SS3.p2.4.m4.1.1" xref="S2.SS3.p2.4.m4.1.1.cmml"><mi id="S2.SS3.p2.4.m4.1.1.2" xref="S2.SS3.p2.4.m4.1.1.2.cmml">B</mi><mo id="S2.SS3.p2.4.m4.1.1.1" xref="S2.SS3.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S2.SS3.p2.4.m4.1.1.3" xref="S2.SS3.p2.4.m4.1.1.3.cmml"><mi id="S2.SS3.p2.4.m4.1.1.3.2" xref="S2.SS3.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS3.p2.4.m4.1.1.3.3" xref="S2.SS3.p2.4.m4.1.1.3.3.cmml"><mi id="S2.SS3.p2.4.m4.1.1.3.3.2" xref="S2.SS3.p2.4.m4.1.1.3.3.2.cmml">d</mi><mo id="S2.SS3.p2.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p2.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S2.SS3.p2.4.m4.1.1.3.3.3" xref="S2.SS3.p2.4.m4.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.4.m4.1b"><apply id="S2.SS3.p2.4.m4.1.1.cmml" xref="S2.SS3.p2.4.m4.1.1"><in id="S2.SS3.p2.4.m4.1.1.1.cmml" xref="S2.SS3.p2.4.m4.1.1.1"></in><ci id="S2.SS3.p2.4.m4.1.1.2.cmml" xref="S2.SS3.p2.4.m4.1.1.2">𝐵</ci><apply id="S2.SS3.p2.4.m4.1.1.3.cmml" xref="S2.SS3.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p2.4.m4.1.1.3.1.cmml" xref="S2.SS3.p2.4.m4.1.1.3">superscript</csymbol><ci id="S2.SS3.p2.4.m4.1.1.3.2.cmml" xref="S2.SS3.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S2.SS3.p2.4.m4.1.1.3.3.cmml" xref="S2.SS3.p2.4.m4.1.1.3.3"><times id="S2.SS3.p2.4.m4.1.1.3.3.1.cmml" xref="S2.SS3.p2.4.m4.1.1.3.3.1"></times><ci id="S2.SS3.p2.4.m4.1.1.3.3.2.cmml" xref="S2.SS3.p2.4.m4.1.1.3.3.2">𝑑</ci><ci id="S2.SS3.p2.4.m4.1.1.3.3.3.cmml" xref="S2.SS3.p2.4.m4.1.1.3.3.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.4.m4.1c">B\in\mathbb{R}^{d\times r}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.4.m4.1d">italic_B ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_r end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="A\in\mathbb{R}^{r\times k}" class="ltx_Math" display="inline" id="S2.SS3.p2.5.m5.1"><semantics id="S2.SS3.p2.5.m5.1a"><mrow id="S2.SS3.p2.5.m5.1.1" xref="S2.SS3.p2.5.m5.1.1.cmml"><mi id="S2.SS3.p2.5.m5.1.1.2" xref="S2.SS3.p2.5.m5.1.1.2.cmml">A</mi><mo id="S2.SS3.p2.5.m5.1.1.1" xref="S2.SS3.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S2.SS3.p2.5.m5.1.1.3" xref="S2.SS3.p2.5.m5.1.1.3.cmml"><mi id="S2.SS3.p2.5.m5.1.1.3.2" xref="S2.SS3.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS3.p2.5.m5.1.1.3.3" xref="S2.SS3.p2.5.m5.1.1.3.3.cmml"><mi id="S2.SS3.p2.5.m5.1.1.3.3.2" xref="S2.SS3.p2.5.m5.1.1.3.3.2.cmml">r</mi><mo id="S2.SS3.p2.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p2.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S2.SS3.p2.5.m5.1.1.3.3.3" xref="S2.SS3.p2.5.m5.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.5.m5.1b"><apply id="S2.SS3.p2.5.m5.1.1.cmml" xref="S2.SS3.p2.5.m5.1.1"><in id="S2.SS3.p2.5.m5.1.1.1.cmml" xref="S2.SS3.p2.5.m5.1.1.1"></in><ci id="S2.SS3.p2.5.m5.1.1.2.cmml" xref="S2.SS3.p2.5.m5.1.1.2">𝐴</ci><apply id="S2.SS3.p2.5.m5.1.1.3.cmml" xref="S2.SS3.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p2.5.m5.1.1.3.1.cmml" xref="S2.SS3.p2.5.m5.1.1.3">superscript</csymbol><ci id="S2.SS3.p2.5.m5.1.1.3.2.cmml" xref="S2.SS3.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S2.SS3.p2.5.m5.1.1.3.3.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3"><times id="S2.SS3.p2.5.m5.1.1.3.3.1.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3.1"></times><ci id="S2.SS3.p2.5.m5.1.1.3.3.2.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3.2">𝑟</ci><ci id="S2.SS3.p2.5.m5.1.1.3.3.3.cmml" xref="S2.SS3.p2.5.m5.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.5.m5.1c">A\in\mathbb{R}^{r\times k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.5.m5.1d">italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_r × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="r" class="ltx_Math" display="inline" id="S2.SS3.p2.6.m6.1"><semantics id="S2.SS3.p2.6.m6.1a"><mi id="S2.SS3.p2.6.m6.1.1" xref="S2.SS3.p2.6.m6.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.6.m6.1b"><ci id="S2.SS3.p2.6.m6.1.1.cmml" xref="S2.SS3.p2.6.m6.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.6.m6.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.6.m6.1d">italic_r</annotation></semantics></math> is the pre-determined rank. The forward pass with input <math alttext="x" class="ltx_Math" display="inline" id="S2.SS3.p2.7.m7.1"><semantics id="S2.SS3.p2.7.m7.1a"><mi id="S2.SS3.p2.7.m7.1.1" xref="S2.SS3.p2.7.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.7.m7.1b"><ci id="S2.SS3.p2.7.m7.1.1.cmml" xref="S2.SS3.p2.7.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.7.m7.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.7.m7.1d">italic_x</annotation></semantics></math> is given by the following equation,</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="h=W_{0}x+\Delta Wx=W_{0}x+BAx,~{}~{}B\in\mathbb{R}^{d\times r},A\in\mathbb{R}^%
{r\times d}" class="ltx_Math" display="block" id="S2.E1.m1.2"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.3.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml">h</mi><mo id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.4.cmml"><mrow id="S2.E1.m1.1.1.1.1.4.2" xref="S2.E1.m1.1.1.1.1.4.2.cmml"><msub id="S2.E1.m1.1.1.1.1.4.2.2" xref="S2.E1.m1.1.1.1.1.4.2.2.cmml"><mi id="S2.E1.m1.1.1.1.1.4.2.2.2" xref="S2.E1.m1.1.1.1.1.4.2.2.2.cmml">W</mi><mn id="S2.E1.m1.1.1.1.1.4.2.2.3" xref="S2.E1.m1.1.1.1.1.4.2.2.3.cmml">0</mn></msub><mo id="S2.E1.m1.1.1.1.1.4.2.1" xref="S2.E1.m1.1.1.1.1.4.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.4.2.3" xref="S2.E1.m1.1.1.1.1.4.2.3.cmml">x</mi></mrow><mo id="S2.E1.m1.1.1.1.1.4.1" xref="S2.E1.m1.1.1.1.1.4.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.1.1.4.3" xref="S2.E1.m1.1.1.1.1.4.3.cmml"><mi id="S2.E1.m1.1.1.1.1.4.3.2" mathvariant="normal" xref="S2.E1.m1.1.1.1.1.4.3.2.cmml">Δ</mi><mo id="S2.E1.m1.1.1.1.1.4.3.1" xref="S2.E1.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.4.3.3" xref="S2.E1.m1.1.1.1.1.4.3.3.cmml">W</mi><mo id="S2.E1.m1.1.1.1.1.4.3.1a" xref="S2.E1.m1.1.1.1.1.4.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.4.3.4" xref="S2.E1.m1.1.1.1.1.4.3.4.cmml">x</mi></mrow></mrow><mo id="S2.E1.m1.1.1.1.1.5" xref="S2.E1.m1.1.1.1.1.5.cmml">=</mo><mrow id="S2.E1.m1.1.1.1.1.6" xref="S2.E1.m1.1.1.1.1.6.cmml"><mrow id="S2.E1.m1.1.1.1.1.6.2" xref="S2.E1.m1.1.1.1.1.6.2.cmml"><msub id="S2.E1.m1.1.1.1.1.6.2.2" xref="S2.E1.m1.1.1.1.1.6.2.2.cmml"><mi id="S2.E1.m1.1.1.1.1.6.2.2.2" xref="S2.E1.m1.1.1.1.1.6.2.2.2.cmml">W</mi><mn id="S2.E1.m1.1.1.1.1.6.2.2.3" xref="S2.E1.m1.1.1.1.1.6.2.2.3.cmml">0</mn></msub><mo id="S2.E1.m1.1.1.1.1.6.2.1" xref="S2.E1.m1.1.1.1.1.6.2.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.6.2.3" xref="S2.E1.m1.1.1.1.1.6.2.3.cmml">x</mi></mrow><mo id="S2.E1.m1.1.1.1.1.6.1" xref="S2.E1.m1.1.1.1.1.6.1.cmml">+</mo><mrow id="S2.E1.m1.1.1.1.1.6.3" xref="S2.E1.m1.1.1.1.1.6.3.cmml"><mi id="S2.E1.m1.1.1.1.1.6.3.2" xref="S2.E1.m1.1.1.1.1.6.3.2.cmml">B</mi><mo id="S2.E1.m1.1.1.1.1.6.3.1" xref="S2.E1.m1.1.1.1.1.6.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.6.3.3" xref="S2.E1.m1.1.1.1.1.6.3.3.cmml">A</mi><mo id="S2.E1.m1.1.1.1.1.6.3.1a" xref="S2.E1.m1.1.1.1.1.6.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.1.1.1.6.3.4" xref="S2.E1.m1.1.1.1.1.6.3.4.cmml">x</mi></mrow></mrow></mrow><mo id="S2.E1.m1.2.2.2.3" rspace="0.827em" xref="S2.E1.m1.2.2.3a.cmml">,</mo><mrow id="S2.E1.m1.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.3.cmml"><mrow id="S2.E1.m1.2.2.2.2.1.1" xref="S2.E1.m1.2.2.2.2.1.1.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml">B</mi><mo id="S2.E1.m1.2.2.2.2.1.1.1" xref="S2.E1.m1.2.2.2.2.1.1.1.cmml">∈</mo><msup id="S2.E1.m1.2.2.2.2.1.1.3" xref="S2.E1.m1.2.2.2.2.1.1.3.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.3.2" xref="S2.E1.m1.2.2.2.2.1.1.3.2.cmml">ℝ</mi><mrow id="S2.E1.m1.2.2.2.2.1.1.3.3" xref="S2.E1.m1.2.2.2.2.1.1.3.3.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.3.3.2" xref="S2.E1.m1.2.2.2.2.1.1.3.3.2.cmml">d</mi><mo id="S2.E1.m1.2.2.2.2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.2.2.2.2.1.1.3.3.1.cmml">×</mo><mi id="S2.E1.m1.2.2.2.2.1.1.3.3.3" xref="S2.E1.m1.2.2.2.2.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><mo id="S2.E1.m1.2.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.3a.cmml">,</mo><mrow id="S2.E1.m1.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.cmml"><mi id="S2.E1.m1.2.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.2.cmml">A</mi><mo id="S2.E1.m1.2.2.2.2.2.2.1" xref="S2.E1.m1.2.2.2.2.2.2.1.cmml">∈</mo><msup id="S2.E1.m1.2.2.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.2.2.3.cmml"><mi id="S2.E1.m1.2.2.2.2.2.2.3.2" xref="S2.E1.m1.2.2.2.2.2.2.3.2.cmml">ℝ</mi><mrow id="S2.E1.m1.2.2.2.2.2.2.3.3" xref="S2.E1.m1.2.2.2.2.2.2.3.3.cmml"><mi id="S2.E1.m1.2.2.2.2.2.2.3.3.2" xref="S2.E1.m1.2.2.2.2.2.2.3.3.2.cmml">r</mi><mo id="S2.E1.m1.2.2.2.2.2.2.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.2.2.2.2.2.2.3.3.1.cmml">×</mo><mi id="S2.E1.m1.2.2.2.2.2.2.3.3.3" xref="S2.E1.m1.2.2.2.2.2.2.3.3.3.cmml">d</mi></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.3a.cmml" xref="S2.E1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1"><and id="S2.E1.m1.1.1.1.1a.cmml" xref="S2.E1.m1.1.1.1.1"></and><apply id="S2.E1.m1.1.1.1.1b.cmml" xref="S2.E1.m1.1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3"></eq><ci id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2">ℎ</ci><apply id="S2.E1.m1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.4"><plus id="S2.E1.m1.1.1.1.1.4.1.cmml" xref="S2.E1.m1.1.1.1.1.4.1"></plus><apply id="S2.E1.m1.1.1.1.1.4.2.cmml" xref="S2.E1.m1.1.1.1.1.4.2"><times id="S2.E1.m1.1.1.1.1.4.2.1.cmml" xref="S2.E1.m1.1.1.1.1.4.2.1"></times><apply id="S2.E1.m1.1.1.1.1.4.2.2.cmml" xref="S2.E1.m1.1.1.1.1.4.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.4.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.4.2.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.4.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.4.2.2.2">𝑊</ci><cn id="S2.E1.m1.1.1.1.1.4.2.2.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.4.2.2.3">0</cn></apply><ci id="S2.E1.m1.1.1.1.1.4.2.3.cmml" xref="S2.E1.m1.1.1.1.1.4.2.3">𝑥</ci></apply><apply id="S2.E1.m1.1.1.1.1.4.3.cmml" xref="S2.E1.m1.1.1.1.1.4.3"><times id="S2.E1.m1.1.1.1.1.4.3.1.cmml" xref="S2.E1.m1.1.1.1.1.4.3.1"></times><ci id="S2.E1.m1.1.1.1.1.4.3.2.cmml" xref="S2.E1.m1.1.1.1.1.4.3.2">Δ</ci><ci id="S2.E1.m1.1.1.1.1.4.3.3.cmml" xref="S2.E1.m1.1.1.1.1.4.3.3">𝑊</ci><ci id="S2.E1.m1.1.1.1.1.4.3.4.cmml" xref="S2.E1.m1.1.1.1.1.4.3.4">𝑥</ci></apply></apply></apply><apply id="S2.E1.m1.1.1.1.1c.cmml" xref="S2.E1.m1.1.1.1.1"><eq id="S2.E1.m1.1.1.1.1.5.cmml" xref="S2.E1.m1.1.1.1.1.5"></eq><share href="#S2.E1.m1.1.1.1.1.4.cmml" id="S2.E1.m1.1.1.1.1d.cmml" xref="S2.E1.m1.1.1.1.1"></share><apply id="S2.E1.m1.1.1.1.1.6.cmml" xref="S2.E1.m1.1.1.1.1.6"><plus id="S2.E1.m1.1.1.1.1.6.1.cmml" xref="S2.E1.m1.1.1.1.1.6.1"></plus><apply id="S2.E1.m1.1.1.1.1.6.2.cmml" xref="S2.E1.m1.1.1.1.1.6.2"><times id="S2.E1.m1.1.1.1.1.6.2.1.cmml" xref="S2.E1.m1.1.1.1.1.6.2.1"></times><apply id="S2.E1.m1.1.1.1.1.6.2.2.cmml" xref="S2.E1.m1.1.1.1.1.6.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.6.2.2.1.cmml" xref="S2.E1.m1.1.1.1.1.6.2.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.6.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.6.2.2.2">𝑊</ci><cn id="S2.E1.m1.1.1.1.1.6.2.2.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.6.2.2.3">0</cn></apply><ci id="S2.E1.m1.1.1.1.1.6.2.3.cmml" xref="S2.E1.m1.1.1.1.1.6.2.3">𝑥</ci></apply><apply id="S2.E1.m1.1.1.1.1.6.3.cmml" xref="S2.E1.m1.1.1.1.1.6.3"><times id="S2.E1.m1.1.1.1.1.6.3.1.cmml" xref="S2.E1.m1.1.1.1.1.6.3.1"></times><ci id="S2.E1.m1.1.1.1.1.6.3.2.cmml" xref="S2.E1.m1.1.1.1.1.6.3.2">𝐵</ci><ci id="S2.E1.m1.1.1.1.1.6.3.3.cmml" xref="S2.E1.m1.1.1.1.1.6.3.3">𝐴</ci><ci id="S2.E1.m1.1.1.1.1.6.3.4.cmml" xref="S2.E1.m1.1.1.1.1.6.3.4">𝑥</ci></apply></apply></apply></apply><apply id="S2.E1.m1.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.3a.cmml" xref="S2.E1.m1.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E1.m1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1"><in id="S2.E1.m1.2.2.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1"></in><ci id="S2.E1.m1.2.2.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2">𝐵</ci><apply id="S2.E1.m1.2.2.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.3.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3">superscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.3.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.2">ℝ</ci><apply id="S2.E1.m1.2.2.2.2.1.1.3.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.3"><times id="S2.E1.m1.2.2.2.2.1.1.3.3.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.3.1"></times><ci id="S2.E1.m1.2.2.2.2.1.1.3.3.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.3.2">𝑑</ci><ci id="S2.E1.m1.2.2.2.2.1.1.3.3.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.3.3">𝑟</ci></apply></apply></apply><apply id="S2.E1.m1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2"><in id="S2.E1.m1.2.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.2.2.1"></in><ci id="S2.E1.m1.2.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2.2">𝐴</ci><apply id="S2.E1.m1.2.2.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.2.2.3.1.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3">superscript</csymbol><ci id="S2.E1.m1.2.2.2.2.2.2.3.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3.2">ℝ</ci><apply id="S2.E1.m1.2.2.2.2.2.2.3.3.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3.3"><times id="S2.E1.m1.2.2.2.2.2.2.3.3.1.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3.3.1"></times><ci id="S2.E1.m1.2.2.2.2.2.2.3.3.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3.3.2">𝑟</ci><ci id="S2.E1.m1.2.2.2.2.2.2.3.3.3.cmml" xref="S2.E1.m1.2.2.2.2.2.2.3.3.3">𝑑</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">h=W_{0}x+\Delta Wx=W_{0}x+BAx,~{}~{}B\in\mathbb{R}^{d\times r},A\in\mathbb{R}^%
{r\times d}</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.2d">italic_h = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + roman_Δ italic_W italic_x = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + italic_B italic_A italic_x , italic_B ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_r end_POSTSUPERSCRIPT , italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_r × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.4">During training, <math alttext="W_{0}" class="ltx_Math" display="inline" id="S2.SS3.p4.1.m1.1"><semantics id="S2.SS3.p4.1.m1.1a"><msub id="S2.SS3.p4.1.m1.1.1" xref="S2.SS3.p4.1.m1.1.1.cmml"><mi id="S2.SS3.p4.1.m1.1.1.2" xref="S2.SS3.p4.1.m1.1.1.2.cmml">W</mi><mn id="S2.SS3.p4.1.m1.1.1.3" xref="S2.SS3.p4.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.1.m1.1b"><apply id="S2.SS3.p4.1.m1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.1.m1.1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.p4.1.m1.1.1.2.cmml" xref="S2.SS3.p4.1.m1.1.1.2">𝑊</ci><cn id="S2.SS3.p4.1.m1.1.1.3.cmml" type="integer" xref="S2.SS3.p4.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.1.m1.1c">W_{0}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.1.m1.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is frozen and does not receive gradient updates, while <math alttext="B" class="ltx_Math" display="inline" id="S2.SS3.p4.2.m2.1"><semantics id="S2.SS3.p4.2.m2.1a"><mi id="S2.SS3.p4.2.m2.1.1" xref="S2.SS3.p4.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.2.m2.1b"><ci id="S2.SS3.p4.2.m2.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.2.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.2.m2.1d">italic_B</annotation></semantics></math> and <math alttext="A" class="ltx_Math" display="inline" id="S2.SS3.p4.3.m3.1"><semantics id="S2.SS3.p4.3.m3.1a"><mi id="S2.SS3.p4.3.m3.1.1" xref="S2.SS3.p4.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.3.m3.1b"><ci id="S2.SS3.p4.3.m3.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.3.m3.1d">italic_A</annotation></semantics></math> are updated. By choosing the rank <math alttext="r\ll\min(d,k)" class="ltx_Math" display="inline" id="S2.SS3.p4.4.m4.3"><semantics id="S2.SS3.p4.4.m4.3a"><mrow id="S2.SS3.p4.4.m4.3.4" xref="S2.SS3.p4.4.m4.3.4.cmml"><mi id="S2.SS3.p4.4.m4.3.4.2" xref="S2.SS3.p4.4.m4.3.4.2.cmml">r</mi><mo id="S2.SS3.p4.4.m4.3.4.1" xref="S2.SS3.p4.4.m4.3.4.1.cmml">≪</mo><mrow id="S2.SS3.p4.4.m4.3.4.3.2" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml"><mi id="S2.SS3.p4.4.m4.1.1" xref="S2.SS3.p4.4.m4.1.1.cmml">min</mi><mo id="S2.SS3.p4.4.m4.3.4.3.2a" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml">⁡</mo><mrow id="S2.SS3.p4.4.m4.3.4.3.2.1" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml"><mo id="S2.SS3.p4.4.m4.3.4.3.2.1.1" stretchy="false" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml">(</mo><mi id="S2.SS3.p4.4.m4.2.2" xref="S2.SS3.p4.4.m4.2.2.cmml">d</mi><mo id="S2.SS3.p4.4.m4.3.4.3.2.1.2" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml">,</mo><mi id="S2.SS3.p4.4.m4.3.3" xref="S2.SS3.p4.4.m4.3.3.cmml">k</mi><mo id="S2.SS3.p4.4.m4.3.4.3.2.1.3" stretchy="false" xref="S2.SS3.p4.4.m4.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.4.m4.3b"><apply id="S2.SS3.p4.4.m4.3.4.cmml" xref="S2.SS3.p4.4.m4.3.4"><csymbol cd="latexml" id="S2.SS3.p4.4.m4.3.4.1.cmml" xref="S2.SS3.p4.4.m4.3.4.1">much-less-than</csymbol><ci id="S2.SS3.p4.4.m4.3.4.2.cmml" xref="S2.SS3.p4.4.m4.3.4.2">𝑟</ci><apply id="S2.SS3.p4.4.m4.3.4.3.1.cmml" xref="S2.SS3.p4.4.m4.3.4.3.2"><min id="S2.SS3.p4.4.m4.1.1.cmml" xref="S2.SS3.p4.4.m4.1.1"></min><ci id="S2.SS3.p4.4.m4.2.2.cmml" xref="S2.SS3.p4.4.m4.2.2">𝑑</ci><ci id="S2.SS3.p4.4.m4.3.3.cmml" xref="S2.SS3.p4.4.m4.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.4.m4.3c">r\ll\min(d,k)</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p4.4.m4.3d">italic_r ≪ roman_min ( italic_d , italic_k )</annotation></semantics></math>, the memory consumption is reduced as we do not need to store the optimizer states for the large frozen matrix.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">To achieve parameter-efficient training while adhering to a tight budget, we apply LoRA training to all Chinese LLaMA and Alpaca models in our paper, including both the pre-training and fine-tuning stages. We primarily incorporate LoRA adapters into the weights of the attention module and MLP layers.
The effectiveness of applying LoRA to all linear transformer blocks is verified in QLoRA <cite class="ltx_cite ltx_citemacro_citep">(Dettmers et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib6" title="">2023</a>)</cite>, indicating that our choices were reasonable.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Pre-Training Objective</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.2">We pre-train the Chinese LLaMA model with the standard Causal Language Modeling (CLM) task. Given an input token sequence <math alttext="\bm{x}=(x_{0},x_{1},x_{2},\ldots)" class="ltx_Math" display="inline" id="S2.SS4.p1.1.m1.4"><semantics id="S2.SS4.p1.1.m1.4a"><mrow id="S2.SS4.p1.1.m1.4.4" xref="S2.SS4.p1.1.m1.4.4.cmml"><mi id="S2.SS4.p1.1.m1.4.4.5" xref="S2.SS4.p1.1.m1.4.4.5.cmml">𝒙</mi><mo id="S2.SS4.p1.1.m1.4.4.4" xref="S2.SS4.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S2.SS4.p1.1.m1.4.4.3.3" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml"><mo id="S2.SS4.p1.1.m1.4.4.3.3.4" stretchy="false" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml">(</mo><msub id="S2.SS4.p1.1.m1.2.2.1.1.1" xref="S2.SS4.p1.1.m1.2.2.1.1.1.cmml"><mi id="S2.SS4.p1.1.m1.2.2.1.1.1.2" xref="S2.SS4.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S2.SS4.p1.1.m1.2.2.1.1.1.3" xref="S2.SS4.p1.1.m1.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S2.SS4.p1.1.m1.4.4.3.3.5" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS4.p1.1.m1.3.3.2.2.2" xref="S2.SS4.p1.1.m1.3.3.2.2.2.cmml"><mi id="S2.SS4.p1.1.m1.3.3.2.2.2.2" xref="S2.SS4.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S2.SS4.p1.1.m1.3.3.2.2.2.3" xref="S2.SS4.p1.1.m1.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S2.SS4.p1.1.m1.4.4.3.3.6" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS4.p1.1.m1.4.4.3.3.3" xref="S2.SS4.p1.1.m1.4.4.3.3.3.cmml"><mi id="S2.SS4.p1.1.m1.4.4.3.3.3.2" xref="S2.SS4.p1.1.m1.4.4.3.3.3.2.cmml">x</mi><mn id="S2.SS4.p1.1.m1.4.4.3.3.3.3" xref="S2.SS4.p1.1.m1.4.4.3.3.3.3.cmml">2</mn></msub><mo id="S2.SS4.p1.1.m1.4.4.3.3.7" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml">,</mo><mi id="S2.SS4.p1.1.m1.1.1" mathvariant="normal" xref="S2.SS4.p1.1.m1.1.1.cmml">…</mi><mo id="S2.SS4.p1.1.m1.4.4.3.3.8" stretchy="false" xref="S2.SS4.p1.1.m1.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.4b"><apply id="S2.SS4.p1.1.m1.4.4.cmml" xref="S2.SS4.p1.1.m1.4.4"><eq id="S2.SS4.p1.1.m1.4.4.4.cmml" xref="S2.SS4.p1.1.m1.4.4.4"></eq><ci id="S2.SS4.p1.1.m1.4.4.5.cmml" xref="S2.SS4.p1.1.m1.4.4.5">𝒙</ci><vector id="S2.SS4.p1.1.m1.4.4.3.4.cmml" xref="S2.SS4.p1.1.m1.4.4.3.3"><apply id="S2.SS4.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.SS4.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS4.p1.1.m1.2.2.1.1.1.2">𝑥</ci><cn id="S2.SS4.p1.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS4.p1.1.m1.2.2.1.1.1.3">0</cn></apply><apply id="S2.SS4.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS4.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS4.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.SS4.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS4.p1.1.m1.3.3.2.2.2.2">𝑥</ci><cn id="S2.SS4.p1.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S2.SS4.p1.1.m1.3.3.2.2.2.3">1</cn></apply><apply id="S2.SS4.p1.1.m1.4.4.3.3.3.cmml" xref="S2.SS4.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS4.p1.1.m1.4.4.3.3.3.1.cmml" xref="S2.SS4.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.SS4.p1.1.m1.4.4.3.3.3.2.cmml" xref="S2.SS4.p1.1.m1.4.4.3.3.3.2">𝑥</ci><cn id="S2.SS4.p1.1.m1.4.4.3.3.3.3.cmml" type="integer" xref="S2.SS4.p1.1.m1.4.4.3.3.3.3">2</cn></apply><ci id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">…</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.4c">\bm{x}=(x_{0},x_{1},x_{2},\ldots)</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.1.m1.4d">bold_italic_x = ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … )</annotation></semantics></math>, the model is trained to predict the next token <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS4.p1.2.m2.1"><semantics id="S2.SS4.p1.2.m2.1a"><msub id="S2.SS4.p1.2.m2.1.1" xref="S2.SS4.p1.2.m2.1.1.cmml"><mi id="S2.SS4.p1.2.m2.1.1.2" xref="S2.SS4.p1.2.m2.1.1.2.cmml">x</mi><mi id="S2.SS4.p1.2.m2.1.1.3" xref="S2.SS4.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.m2.1b"><apply id="S2.SS4.p1.2.m2.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.2.m2.1.1.1.cmml" xref="S2.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS4.p1.2.m2.1.1.2.cmml" xref="S2.SS4.p1.2.m2.1.1.2">𝑥</ci><ci id="S2.SS4.p1.2.m2.1.1.3.cmml" xref="S2.SS4.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.2.m2.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in an autoregressive manner. Mathematically, the objective is to minimize the following negative log-likelihood:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx1">
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\textrm{CLM}}(\Theta)=\mathbb{E}_{\bm{x}\sim\mathcal%
{D}_{\textrm{PT}}}\left[-\sum_{i}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};%
\Theta)\right]" class="ltx_Math" display="inline" id="S2.E2.m1.4"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml"><mrow id="S2.E2.m1.4.4.3" xref="S2.E2.m1.4.4.3.cmml"><msub id="S2.E2.m1.4.4.3.2" xref="S2.E2.m1.4.4.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.4.4.3.2.2" xref="S2.E2.m1.4.4.3.2.2.cmml">ℒ</mi><mtext id="S2.E2.m1.4.4.3.2.3" xref="S2.E2.m1.4.4.3.2.3a.cmml">CLM</mtext></msub><mo id="S2.E2.m1.4.4.3.1" xref="S2.E2.m1.4.4.3.1.cmml">⁢</mo><mrow id="S2.E2.m1.4.4.3.3.2" xref="S2.E2.m1.4.4.3.cmml"><mo id="S2.E2.m1.4.4.3.3.2.1" stretchy="false" xref="S2.E2.m1.4.4.3.cmml">(</mo><mi id="S2.E2.m1.1.1" mathvariant="normal" xref="S2.E2.m1.1.1.cmml">Θ</mi><mo id="S2.E2.m1.4.4.3.3.2.2" stretchy="false" xref="S2.E2.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.2" xref="S2.E2.m1.4.4.2.cmml">=</mo><mrow id="S2.E2.m1.4.4.1" xref="S2.E2.m1.4.4.1.cmml"><msub id="S2.E2.m1.4.4.1.3" xref="S2.E2.m1.4.4.1.3.cmml"><mi id="S2.E2.m1.4.4.1.3.2" xref="S2.E2.m1.4.4.1.3.2.cmml">𝔼</mi><mrow id="S2.E2.m1.4.4.1.3.3" xref="S2.E2.m1.4.4.1.3.3.cmml"><mi id="S2.E2.m1.4.4.1.3.3.2" xref="S2.E2.m1.4.4.1.3.3.2.cmml">𝒙</mi><mo id="S2.E2.m1.4.4.1.3.3.1" xref="S2.E2.m1.4.4.1.3.3.1.cmml">∼</mo><msub id="S2.E2.m1.4.4.1.3.3.3" xref="S2.E2.m1.4.4.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.4.4.1.3.3.3.2" xref="S2.E2.m1.4.4.1.3.3.3.2.cmml">𝒟</mi><mtext id="S2.E2.m1.4.4.1.3.3.3.3" xref="S2.E2.m1.4.4.1.3.3.3.3a.cmml">PT</mtext></msub></mrow></msub><mo id="S2.E2.m1.4.4.1.2" xref="S2.E2.m1.4.4.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.4.4.1.1.1" xref="S2.E2.m1.4.4.1.1.2.cmml"><mo id="S2.E2.m1.4.4.1.1.1.2" xref="S2.E2.m1.4.4.1.1.2.1.cmml">[</mo><mrow id="S2.E2.m1.4.4.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.cmml"><mo id="S2.E2.m1.4.4.1.1.1.1a" xref="S2.E2.m1.4.4.1.1.1.1.cmml">−</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S2.E2.m1.4.4.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml"><munder id="S2.E2.m1.4.4.1.1.1.1.1.2a" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml"><mo id="S2.E2.m1.4.4.1.1.1.1.1.2.2" movablelimits="false" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2.cmml">∑</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.2.3.cmml">i</mi></munder></mstyle><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.3.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.3a" lspace="0.167em" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.2.cmml">x</mi><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.4" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml"><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.4" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.5" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><mi id="S2.E2.m1.2.2" mathvariant="normal" xref="S2.E2.m1.2.2.cmml">…</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.6" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml">−</mo><mn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.7" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">;</mo><mi id="S2.E2.m1.3.3" mathvariant="normal" xref="S2.E2.m1.3.3.cmml">Θ</mi></mrow></mrow><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E2.m1.4.4.1.1.1.3" xref="S2.E2.m1.4.4.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4"><eq id="S2.E2.m1.4.4.2.cmml" xref="S2.E2.m1.4.4.2"></eq><apply id="S2.E2.m1.4.4.3.cmml" xref="S2.E2.m1.4.4.3"><times id="S2.E2.m1.4.4.3.1.cmml" xref="S2.E2.m1.4.4.3.1"></times><apply id="S2.E2.m1.4.4.3.2.cmml" xref="S2.E2.m1.4.4.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.3.2.1.cmml" xref="S2.E2.m1.4.4.3.2">subscript</csymbol><ci id="S2.E2.m1.4.4.3.2.2.cmml" xref="S2.E2.m1.4.4.3.2.2">ℒ</ci><ci id="S2.E2.m1.4.4.3.2.3a.cmml" xref="S2.E2.m1.4.4.3.2.3"><mtext id="S2.E2.m1.4.4.3.2.3.cmml" mathsize="70%" xref="S2.E2.m1.4.4.3.2.3">CLM</mtext></ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">Θ</ci></apply><apply id="S2.E2.m1.4.4.1.cmml" xref="S2.E2.m1.4.4.1"><times id="S2.E2.m1.4.4.1.2.cmml" xref="S2.E2.m1.4.4.1.2"></times><apply id="S2.E2.m1.4.4.1.3.cmml" xref="S2.E2.m1.4.4.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.3.1.cmml" xref="S2.E2.m1.4.4.1.3">subscript</csymbol><ci id="S2.E2.m1.4.4.1.3.2.cmml" xref="S2.E2.m1.4.4.1.3.2">𝔼</ci><apply id="S2.E2.m1.4.4.1.3.3.cmml" xref="S2.E2.m1.4.4.1.3.3"><csymbol cd="latexml" id="S2.E2.m1.4.4.1.3.3.1.cmml" xref="S2.E2.m1.4.4.1.3.3.1">similar-to</csymbol><ci id="S2.E2.m1.4.4.1.3.3.2.cmml" xref="S2.E2.m1.4.4.1.3.3.2">𝒙</ci><apply id="S2.E2.m1.4.4.1.3.3.3.cmml" xref="S2.E2.m1.4.4.1.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.3.3.3.1.cmml" xref="S2.E2.m1.4.4.1.3.3.3">subscript</csymbol><ci id="S2.E2.m1.4.4.1.3.3.3.2.cmml" xref="S2.E2.m1.4.4.1.3.3.3.2">𝒟</ci><ci id="S2.E2.m1.4.4.1.3.3.3.3a.cmml" xref="S2.E2.m1.4.4.1.3.3.3.3"><mtext id="S2.E2.m1.4.4.1.3.3.3.3.cmml" mathsize="50%" xref="S2.E2.m1.4.4.1.3.3.3.3">PT</mtext></ci></apply></apply></apply><apply id="S2.E2.m1.4.4.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.4.4.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.2">delimited-[]</csymbol><apply id="S2.E2.m1.4.4.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1"><minus id="S2.E2.m1.4.4.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1"></minus><apply id="S2.E2.m1.4.4.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1"><apply id="S2.E2.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E2.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.2"></sum><ci id="S2.E2.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1"><times id="S2.E2.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2"></times><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3"><log id="S2.E2.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.1"></log><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.2">𝑝</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.4">conditional</csymbol><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.2">𝑥</ci><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.5.3">𝑖</ci></apply><list id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3"><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><cn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2">𝑥</ci><cn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" type="integer" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">…</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2">𝑥</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3"><minus id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2">𝑖</ci><cn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml" type="integer" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">Θ</ci></list></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\displaystyle\mathcal{L}_{\textrm{CLM}}(\Theta)=\mathbb{E}_{\bm{x}\sim\mathcal%
{D}_{\textrm{PT}}}\left[-\sum_{i}\log p(x_{i}|x_{0},x_{1},\ldots,x_{i-1};%
\Theta)\right]</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.4d">caligraphic_L start_POSTSUBSCRIPT CLM end_POSTSUBSCRIPT ( roman_Θ ) = blackboard_E start_POSTSUBSCRIPT bold_italic_x ∼ caligraphic_D start_POSTSUBSCRIPT PT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ - ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; roman_Θ ) ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.p1.6">where, <math alttext="\Theta" class="ltx_Math" display="inline" id="S2.SS4.p1.3.m1.1"><semantics id="S2.SS4.p1.3.m1.1a"><mi id="S2.SS4.p1.3.m1.1.1" mathvariant="normal" xref="S2.SS4.p1.3.m1.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.3.m1.1b"><ci id="S2.SS4.p1.3.m1.1.1.cmml" xref="S2.SS4.p1.3.m1.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.3.m1.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.3.m1.1d">roman_Θ</annotation></semantics></math> represents the model parameters, <math alttext="\mathcal{D}_{\textrm{PT}}" class="ltx_Math" display="inline" id="S2.SS4.p1.4.m2.1"><semantics id="S2.SS4.p1.4.m2.1a"><msub id="S2.SS4.p1.4.m2.1.1" xref="S2.SS4.p1.4.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS4.p1.4.m2.1.1.2" xref="S2.SS4.p1.4.m2.1.1.2.cmml">𝒟</mi><mtext id="S2.SS4.p1.4.m2.1.1.3" xref="S2.SS4.p1.4.m2.1.1.3a.cmml">PT</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.4.m2.1b"><apply id="S2.SS4.p1.4.m2.1.1.cmml" xref="S2.SS4.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.4.m2.1.1.1.cmml" xref="S2.SS4.p1.4.m2.1.1">subscript</csymbol><ci id="S2.SS4.p1.4.m2.1.1.2.cmml" xref="S2.SS4.p1.4.m2.1.1.2">𝒟</ci><ci id="S2.SS4.p1.4.m2.1.1.3a.cmml" xref="S2.SS4.p1.4.m2.1.1.3"><mtext id="S2.SS4.p1.4.m2.1.1.3.cmml" mathsize="70%" xref="S2.SS4.p1.4.m2.1.1.3">PT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.4.m2.1c">\mathcal{D}_{\textrm{PT}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.4.m2.1d">caligraphic_D start_POSTSUBSCRIPT PT end_POSTSUBSCRIPT</annotation></semantics></math> is the pre-training dataset, <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS4.p1.5.m3.1"><semantics id="S2.SS4.p1.5.m3.1a"><msub id="S2.SS4.p1.5.m3.1.1" xref="S2.SS4.p1.5.m3.1.1.cmml"><mi id="S2.SS4.p1.5.m3.1.1.2" xref="S2.SS4.p1.5.m3.1.1.2.cmml">x</mi><mi id="S2.SS4.p1.5.m3.1.1.3" xref="S2.SS4.p1.5.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.5.m3.1b"><apply id="S2.SS4.p1.5.m3.1.1.cmml" xref="S2.SS4.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.5.m3.1.1.1.cmml" xref="S2.SS4.p1.5.m3.1.1">subscript</csymbol><ci id="S2.SS4.p1.5.m3.1.1.2.cmml" xref="S2.SS4.p1.5.m3.1.1.2">𝑥</ci><ci id="S2.SS4.p1.5.m3.1.1.3.cmml" xref="S2.SS4.p1.5.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.5.m3.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.5.m3.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the token to be predicted, and <math alttext="x_{0},x_{1},\ldots,x_{i-1}" class="ltx_Math" display="inline" id="S2.SS4.p1.6.m4.4"><semantics id="S2.SS4.p1.6.m4.4a"><mrow id="S2.SS4.p1.6.m4.4.4.3" xref="S2.SS4.p1.6.m4.4.4.4.cmml"><msub id="S2.SS4.p1.6.m4.2.2.1.1" xref="S2.SS4.p1.6.m4.2.2.1.1.cmml"><mi id="S2.SS4.p1.6.m4.2.2.1.1.2" xref="S2.SS4.p1.6.m4.2.2.1.1.2.cmml">x</mi><mn id="S2.SS4.p1.6.m4.2.2.1.1.3" xref="S2.SS4.p1.6.m4.2.2.1.1.3.cmml">0</mn></msub><mo id="S2.SS4.p1.6.m4.4.4.3.4" xref="S2.SS4.p1.6.m4.4.4.4.cmml">,</mo><msub id="S2.SS4.p1.6.m4.3.3.2.2" xref="S2.SS4.p1.6.m4.3.3.2.2.cmml"><mi id="S2.SS4.p1.6.m4.3.3.2.2.2" xref="S2.SS4.p1.6.m4.3.3.2.2.2.cmml">x</mi><mn id="S2.SS4.p1.6.m4.3.3.2.2.3" xref="S2.SS4.p1.6.m4.3.3.2.2.3.cmml">1</mn></msub><mo id="S2.SS4.p1.6.m4.4.4.3.5" xref="S2.SS4.p1.6.m4.4.4.4.cmml">,</mo><mi id="S2.SS4.p1.6.m4.1.1" mathvariant="normal" xref="S2.SS4.p1.6.m4.1.1.cmml">…</mi><mo id="S2.SS4.p1.6.m4.4.4.3.6" xref="S2.SS4.p1.6.m4.4.4.4.cmml">,</mo><msub id="S2.SS4.p1.6.m4.4.4.3.3" xref="S2.SS4.p1.6.m4.4.4.3.3.cmml"><mi id="S2.SS4.p1.6.m4.4.4.3.3.2" xref="S2.SS4.p1.6.m4.4.4.3.3.2.cmml">x</mi><mrow id="S2.SS4.p1.6.m4.4.4.3.3.3" xref="S2.SS4.p1.6.m4.4.4.3.3.3.cmml"><mi id="S2.SS4.p1.6.m4.4.4.3.3.3.2" xref="S2.SS4.p1.6.m4.4.4.3.3.3.2.cmml">i</mi><mo id="S2.SS4.p1.6.m4.4.4.3.3.3.1" xref="S2.SS4.p1.6.m4.4.4.3.3.3.1.cmml">−</mo><mn id="S2.SS4.p1.6.m4.4.4.3.3.3.3" xref="S2.SS4.p1.6.m4.4.4.3.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.6.m4.4b"><list id="S2.SS4.p1.6.m4.4.4.4.cmml" xref="S2.SS4.p1.6.m4.4.4.3"><apply id="S2.SS4.p1.6.m4.2.2.1.1.cmml" xref="S2.SS4.p1.6.m4.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.6.m4.2.2.1.1.1.cmml" xref="S2.SS4.p1.6.m4.2.2.1.1">subscript</csymbol><ci id="S2.SS4.p1.6.m4.2.2.1.1.2.cmml" xref="S2.SS4.p1.6.m4.2.2.1.1.2">𝑥</ci><cn id="S2.SS4.p1.6.m4.2.2.1.1.3.cmml" type="integer" xref="S2.SS4.p1.6.m4.2.2.1.1.3">0</cn></apply><apply id="S2.SS4.p1.6.m4.3.3.2.2.cmml" xref="S2.SS4.p1.6.m4.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS4.p1.6.m4.3.3.2.2.1.cmml" xref="S2.SS4.p1.6.m4.3.3.2.2">subscript</csymbol><ci id="S2.SS4.p1.6.m4.3.3.2.2.2.cmml" xref="S2.SS4.p1.6.m4.3.3.2.2.2">𝑥</ci><cn id="S2.SS4.p1.6.m4.3.3.2.2.3.cmml" type="integer" xref="S2.SS4.p1.6.m4.3.3.2.2.3">1</cn></apply><ci id="S2.SS4.p1.6.m4.1.1.cmml" xref="S2.SS4.p1.6.m4.1.1">…</ci><apply id="S2.SS4.p1.6.m4.4.4.3.3.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS4.p1.6.m4.4.4.3.3.1.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3">subscript</csymbol><ci id="S2.SS4.p1.6.m4.4.4.3.3.2.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3.2">𝑥</ci><apply id="S2.SS4.p1.6.m4.4.4.3.3.3.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3.3"><minus id="S2.SS4.p1.6.m4.4.4.3.3.3.1.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3.3.1"></minus><ci id="S2.SS4.p1.6.m4.4.4.3.3.3.2.cmml" xref="S2.SS4.p1.6.m4.4.4.3.3.3.2">𝑖</ci><cn id="S2.SS4.p1.6.m4.4.4.3.3.3.3.cmml" type="integer" xref="S2.SS4.p1.6.m4.4.4.3.3.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.6.m4.4c">x_{0},x_{1},\ldots,x_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p1.6.m4.4d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> constitute the context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Supervised Fine-Tuning and Chinese Alpaca</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">Pre-trained language models can hardly follow user instructions and often generate unintended content. This is because the language modeling objective in Equation (<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.E2" title="2 ‣ 2.4 Pre-Training Objective ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">2</span></a>) is predicting the next token, not “follow the instructions and answer the questions” <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib19" title="">2022</a>)</cite>. To align the behavior of language models to the user’s intention, one can fine-tune the model to explicitly train it to follow instructions. Stanford Alpaca <cite class="ltx_cite ltx_citemacro_citep">(Taori et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib27" title="">2023b</a>)</cite> is a LLaMA-based instruction-following model that was trained on 52K instruction-following data generated by the techniques in the Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib31" title="">2022</a>)</cite>. We follow the approach in Stanford Alpaca to apply self-instructed fine-tuning on Chinese LLaMA to train an instruction-following model — Chinese Alpaca.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p2">
<p class="ltx_p" id="S2.SS5.p2.1">Chinese Alpaca is trained on a combination of instruction-following datasets. Each example in the dataset consists of an instruction and an output. The supervised fine-tuning task is similar to the causal language modeling task: the model is prompted with the instruction and trained to generate the output autoregressively. The instruction is wrapped in a prompt template, and the output immediately follows the template. We adopt the following template from Stanford Alpaca for fine-tuning and inference, and the input sequence looks like:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p3">
<blockquote class="ltx_quote" id="S2.SS5.p3.1">
<p class="ltx_p" id="S2.SS5.p3.1.1"><span class="ltx_text ltx_font_italic" id="S2.SS5.p3.1.1.1">Below is an instruction that describes a task. Write a response that appropriately completes the request.

<br class="ltx_break">
<br class="ltx_break">### Instruction:

<br class="ltx_break"></span>{<span class="ltx_text ltx_font_italic" id="S2.SS5.p3.1.1.2">instruction</span>}<span class="ltx_text ltx_font_italic" id="S2.SS5.p3.1.1.3">
<br class="ltx_break">
<br class="ltx_break">### Response: </span>{<span class="ltx_text ltx_font_italic" id="S2.SS5.p3.1.1.4">output</span>}<span class="ltx_text ltx_font_italic" id="S2.SS5.p3.1.1.5">
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</blockquote>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p4">
<p class="ltx_p" id="S2.SS5.p4.4">The loss is only calculated on the <em class="ltx_emph ltx_font_italic" id="S2.SS5.p4.4.1"><span class="ltx_text ltx_font_upright" id="S2.SS5.p4.4.1.1">{</span>output<span class="ltx_text ltx_font_upright" id="S2.SS5.p4.4.1.2">}</span></em> part of the input sequence and can be expressed as:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A1.EGx2">
<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\textrm{SFT}}(\Theta)=\mathbb{E}_{\bm{x}\sim\mathcal%
{D}_{\textrm{SFT}}}\left[-\sum_{i\in\textit{\{output\}}}\log p(x_{i}|x_{0},x_{%
1},\ldots,x_{i-1};\Theta)\right]" class="ltx_Math" display="inline" id="S2.E3.m1.4"><semantics id="S2.E3.m1.4a"><mrow id="S2.E3.m1.4.4" xref="S2.E3.m1.4.4.cmml"><mrow id="S2.E3.m1.4.4.3" xref="S2.E3.m1.4.4.3.cmml"><msub id="S2.E3.m1.4.4.3.2" xref="S2.E3.m1.4.4.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.4.4.3.2.2" xref="S2.E3.m1.4.4.3.2.2.cmml">ℒ</mi><mtext id="S2.E3.m1.4.4.3.2.3" xref="S2.E3.m1.4.4.3.2.3a.cmml">SFT</mtext></msub><mo id="S2.E3.m1.4.4.3.1" xref="S2.E3.m1.4.4.3.1.cmml">⁢</mo><mrow id="S2.E3.m1.4.4.3.3.2" xref="S2.E3.m1.4.4.3.cmml"><mo id="S2.E3.m1.4.4.3.3.2.1" stretchy="false" xref="S2.E3.m1.4.4.3.cmml">(</mo><mi id="S2.E3.m1.1.1" mathvariant="normal" xref="S2.E3.m1.1.1.cmml">Θ</mi><mo id="S2.E3.m1.4.4.3.3.2.2" stretchy="false" xref="S2.E3.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.4.4.2" xref="S2.E3.m1.4.4.2.cmml">=</mo><mrow id="S2.E3.m1.4.4.1" xref="S2.E3.m1.4.4.1.cmml"><msub id="S2.E3.m1.4.4.1.3" xref="S2.E3.m1.4.4.1.3.cmml"><mi id="S2.E3.m1.4.4.1.3.2" xref="S2.E3.m1.4.4.1.3.2.cmml">𝔼</mi><mrow id="S2.E3.m1.4.4.1.3.3" xref="S2.E3.m1.4.4.1.3.3.cmml"><mi id="S2.E3.m1.4.4.1.3.3.2" xref="S2.E3.m1.4.4.1.3.3.2.cmml">𝒙</mi><mo id="S2.E3.m1.4.4.1.3.3.1" xref="S2.E3.m1.4.4.1.3.3.1.cmml">∼</mo><msub id="S2.E3.m1.4.4.1.3.3.3" xref="S2.E3.m1.4.4.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.4.4.1.3.3.3.2" xref="S2.E3.m1.4.4.1.3.3.3.2.cmml">𝒟</mi><mtext id="S2.E3.m1.4.4.1.3.3.3.3" xref="S2.E3.m1.4.4.1.3.3.3.3a.cmml">SFT</mtext></msub></mrow></msub><mo id="S2.E3.m1.4.4.1.2" xref="S2.E3.m1.4.4.1.2.cmml">⁢</mo><mrow id="S2.E3.m1.4.4.1.1.1" xref="S2.E3.m1.4.4.1.1.2.cmml"><mo id="S2.E3.m1.4.4.1.1.1.2" xref="S2.E3.m1.4.4.1.1.2.1.cmml">[</mo><mrow id="S2.E3.m1.4.4.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.cmml"><mo id="S2.E3.m1.4.4.1.1.1.1a" xref="S2.E3.m1.4.4.1.1.1.1.cmml">−</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S2.E3.m1.4.4.1.1.1.1.1.2" xref="S2.E3.m1.4.4.1.1.1.1.1.2.cmml"><munder id="S2.E3.m1.4.4.1.1.1.1.1.2a" xref="S2.E3.m1.4.4.1.1.1.1.1.2.cmml"><mo id="S2.E3.m1.4.4.1.1.1.1.1.2.2" movablelimits="false" xref="S2.E3.m1.4.4.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.2.3" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.2.3.2" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S2.E3.m1.4.4.1.1.1.1.1.2.3.1" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.1.cmml">∈</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.2.3.3" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.3d.cmml"><mtext id="S2.E3.m1.4.4.1.1.1.1.1.2.3.3a" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.3d.cmml">{</mtext><mtext id="S2.E3.m1.4.4.1.1.1.1.1.2.3.3b" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.3d.cmml">𝑜𝑢𝑡𝑝𝑢𝑡</mtext><mtext id="S2.E3.m1.4.4.1.1.1.1.1.2.3.3c" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.3d.cmml">}</mtext></mrow></mrow></munder></mstyle><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.cmml"><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.3.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.3a" lspace="0.167em" xref="S2.E3.m1.4.4.1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.3.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5.2.cmml">x</mi><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5.3.cmml">i</mi></msub><mo fence="false" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml">|</mo><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml"><msub id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.4" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mn id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.5" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><mi id="S2.E3.m1.2.2" mathvariant="normal" xref="S2.E3.m1.2.2.cmml">…</mi><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.6" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">,</mo><msub id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">x</mi><mrow id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml">i</mi><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml">−</mo><mn id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.7" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml">;</mo><mi id="S2.E3.m1.3.3" mathvariant="normal" xref="S2.E3.m1.3.3.cmml">Θ</mi></mrow></mrow><mo id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E3.m1.4.4.1.1.1.3" xref="S2.E3.m1.4.4.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.4b"><apply id="S2.E3.m1.4.4.cmml" xref="S2.E3.m1.4.4"><eq id="S2.E3.m1.4.4.2.cmml" xref="S2.E3.m1.4.4.2"></eq><apply id="S2.E3.m1.4.4.3.cmml" xref="S2.E3.m1.4.4.3"><times id="S2.E3.m1.4.4.3.1.cmml" xref="S2.E3.m1.4.4.3.1"></times><apply id="S2.E3.m1.4.4.3.2.cmml" xref="S2.E3.m1.4.4.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.3.2.1.cmml" xref="S2.E3.m1.4.4.3.2">subscript</csymbol><ci id="S2.E3.m1.4.4.3.2.2.cmml" xref="S2.E3.m1.4.4.3.2.2">ℒ</ci><ci id="S2.E3.m1.4.4.3.2.3a.cmml" xref="S2.E3.m1.4.4.3.2.3"><mtext id="S2.E3.m1.4.4.3.2.3.cmml" mathsize="70%" xref="S2.E3.m1.4.4.3.2.3">SFT</mtext></ci></apply><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">Θ</ci></apply><apply id="S2.E3.m1.4.4.1.cmml" xref="S2.E3.m1.4.4.1"><times id="S2.E3.m1.4.4.1.2.cmml" xref="S2.E3.m1.4.4.1.2"></times><apply id="S2.E3.m1.4.4.1.3.cmml" xref="S2.E3.m1.4.4.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.3.1.cmml" xref="S2.E3.m1.4.4.1.3">subscript</csymbol><ci id="S2.E3.m1.4.4.1.3.2.cmml" xref="S2.E3.m1.4.4.1.3.2">𝔼</ci><apply id="S2.E3.m1.4.4.1.3.3.cmml" xref="S2.E3.m1.4.4.1.3.3"><csymbol cd="latexml" id="S2.E3.m1.4.4.1.3.3.1.cmml" xref="S2.E3.m1.4.4.1.3.3.1">similar-to</csymbol><ci id="S2.E3.m1.4.4.1.3.3.2.cmml" xref="S2.E3.m1.4.4.1.3.3.2">𝒙</ci><apply id="S2.E3.m1.4.4.1.3.3.3.cmml" xref="S2.E3.m1.4.4.1.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.3.3.3.1.cmml" xref="S2.E3.m1.4.4.1.3.3.3">subscript</csymbol><ci id="S2.E3.m1.4.4.1.3.3.3.2.cmml" xref="S2.E3.m1.4.4.1.3.3.3.2">𝒟</ci><ci id="S2.E3.m1.4.4.1.3.3.3.3a.cmml" xref="S2.E3.m1.4.4.1.3.3.3.3"><mtext id="S2.E3.m1.4.4.1.3.3.3.3.cmml" mathsize="50%" xref="S2.E3.m1.4.4.1.3.3.3.3">SFT</mtext></ci></apply></apply></apply><apply id="S2.E3.m1.4.4.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.1"><csymbol cd="latexml" id="S2.E3.m1.4.4.1.1.2.1.cmml" xref="S2.E3.m1.4.4.1.1.1.2">delimited-[]</csymbol><apply id="S2.E3.m1.4.4.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1"><minus id="S2.E3.m1.4.4.1.1.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1"></minus><apply id="S2.E3.m1.4.4.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1"><apply id="S2.E3.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.2">subscript</csymbol><sum id="S2.E3.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.2.2"></sum><apply id="S2.E3.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3"><in id="S2.E3.m1.4.4.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.1"></in><ci id="S2.E3.m1.4.4.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.2">𝑖</ci><ci id="S2.E3.m1.4.4.1.1.1.1.1.2.3.3d.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.3"><mrow id="S2.E3.m1.4.4.1.1.1.1.1.2.3.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.3"><mtext id="S2.E3.m1.4.4.1.1.1.1.1.2.3.3a.cmml" mathsize="70%" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.3">{</mtext><mtext id="S2.E3.m1.4.4.1.1.1.1.1.2.3.3b.cmml" mathsize="70%" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.3">𝑜𝑢𝑡𝑝𝑢𝑡</mtext><mtext id="S2.E3.m1.4.4.1.1.1.1.1.2.3.3c.cmml" mathsize="70%" xref="S2.E3.m1.4.4.1.1.1.1.1.2.3.3">}</mtext></mrow></ci></apply></apply><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1"><times id="S2.E3.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.2"></times><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.3"><log id="S2.E3.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.3.1"></log><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.3.2">𝑝</ci></apply><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.4">conditional</csymbol><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5.2">𝑥</ci><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.5.3">𝑖</ci></apply><list id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3"><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><cn id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.3">0</cn></apply><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.2">𝑥</ci><cn id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" type="integer" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2.2.2.3">1</cn></apply><ci id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2">…</ci><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.2">𝑥</ci><apply id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3"><minus id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.1"></minus><ci id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.2">𝑖</ci><cn id="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml" type="integer" xref="S2.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.3.3.3">1</cn></apply></apply><ci id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3">Θ</ci></list></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.4c">\displaystyle\mathcal{L}_{\textrm{SFT}}(\Theta)=\mathbb{E}_{\bm{x}\sim\mathcal%
{D}_{\textrm{SFT}}}\left[-\sum_{i\in\textit{\{output\}}}\log p(x_{i}|x_{0},x_{%
1},\ldots,x_{i-1};\Theta)\right]</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.4d">caligraphic_L start_POSTSUBSCRIPT SFT end_POSTSUBSCRIPT ( roman_Θ ) = blackboard_E start_POSTSUBSCRIPT bold_italic_x ∼ caligraphic_D start_POSTSUBSCRIPT SFT end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ - ∑ start_POSTSUBSCRIPT italic_i ∈ { output } end_POSTSUBSCRIPT roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ; roman_Θ ) ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS5.p4.3">Here, <math alttext="\Theta" class="ltx_Math" display="inline" id="S2.SS5.p4.1.m1.1"><semantics id="S2.SS5.p4.1.m1.1a"><mi id="S2.SS5.p4.1.m1.1.1" mathvariant="normal" xref="S2.SS5.p4.1.m1.1.1.cmml">Θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.1.m1.1b"><ci id="S2.SS5.p4.1.m1.1.1.cmml" xref="S2.SS5.p4.1.m1.1.1">Θ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.1.m1.1c">\Theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.p4.1.m1.1d">roman_Θ</annotation></semantics></math> represents the model parameters, <math alttext="\mathcal{D}_{\textrm{SFT}}" class="ltx_Math" display="inline" id="S2.SS5.p4.2.m2.1"><semantics id="S2.SS5.p4.2.m2.1a"><msub id="S2.SS5.p4.2.m2.1.1" xref="S2.SS5.p4.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS5.p4.2.m2.1.1.2" xref="S2.SS5.p4.2.m2.1.1.2.cmml">𝒟</mi><mtext id="S2.SS5.p4.2.m2.1.1.3" xref="S2.SS5.p4.2.m2.1.1.3a.cmml">SFT</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.2.m2.1b"><apply id="S2.SS5.p4.2.m2.1.1.cmml" xref="S2.SS5.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS5.p4.2.m2.1.1.1.cmml" xref="S2.SS5.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS5.p4.2.m2.1.1.2.cmml" xref="S2.SS5.p4.2.m2.1.1.2">𝒟</ci><ci id="S2.SS5.p4.2.m2.1.1.3a.cmml" xref="S2.SS5.p4.2.m2.1.1.3"><mtext id="S2.SS5.p4.2.m2.1.1.3.cmml" mathsize="70%" xref="S2.SS5.p4.2.m2.1.1.3">SFT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.2.m2.1c">\mathcal{D}_{\textrm{SFT}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.p4.2.m2.1d">caligraphic_D start_POSTSUBSCRIPT SFT end_POSTSUBSCRIPT</annotation></semantics></math> is the fine-tuning dataset, <math alttext="\bm{x}=(x_{0},x_{1},\ldots)" class="ltx_Math" display="inline" id="S2.SS5.p4.3.m3.3"><semantics id="S2.SS5.p4.3.m3.3a"><mrow id="S2.SS5.p4.3.m3.3.3" xref="S2.SS5.p4.3.m3.3.3.cmml"><mi id="S2.SS5.p4.3.m3.3.3.4" xref="S2.SS5.p4.3.m3.3.3.4.cmml">𝒙</mi><mo id="S2.SS5.p4.3.m3.3.3.3" xref="S2.SS5.p4.3.m3.3.3.3.cmml">=</mo><mrow id="S2.SS5.p4.3.m3.3.3.2.2" xref="S2.SS5.p4.3.m3.3.3.2.3.cmml"><mo id="S2.SS5.p4.3.m3.3.3.2.2.3" stretchy="false" xref="S2.SS5.p4.3.m3.3.3.2.3.cmml">(</mo><msub id="S2.SS5.p4.3.m3.2.2.1.1.1" xref="S2.SS5.p4.3.m3.2.2.1.1.1.cmml"><mi id="S2.SS5.p4.3.m3.2.2.1.1.1.2" xref="S2.SS5.p4.3.m3.2.2.1.1.1.2.cmml">x</mi><mn id="S2.SS5.p4.3.m3.2.2.1.1.1.3" xref="S2.SS5.p4.3.m3.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S2.SS5.p4.3.m3.3.3.2.2.4" xref="S2.SS5.p4.3.m3.3.3.2.3.cmml">,</mo><msub id="S2.SS5.p4.3.m3.3.3.2.2.2" xref="S2.SS5.p4.3.m3.3.3.2.2.2.cmml"><mi id="S2.SS5.p4.3.m3.3.3.2.2.2.2" xref="S2.SS5.p4.3.m3.3.3.2.2.2.2.cmml">x</mi><mn id="S2.SS5.p4.3.m3.3.3.2.2.2.3" xref="S2.SS5.p4.3.m3.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S2.SS5.p4.3.m3.3.3.2.2.5" xref="S2.SS5.p4.3.m3.3.3.2.3.cmml">,</mo><mi id="S2.SS5.p4.3.m3.1.1" mathvariant="normal" xref="S2.SS5.p4.3.m3.1.1.cmml">…</mi><mo id="S2.SS5.p4.3.m3.3.3.2.2.6" stretchy="false" xref="S2.SS5.p4.3.m3.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p4.3.m3.3b"><apply id="S2.SS5.p4.3.m3.3.3.cmml" xref="S2.SS5.p4.3.m3.3.3"><eq id="S2.SS5.p4.3.m3.3.3.3.cmml" xref="S2.SS5.p4.3.m3.3.3.3"></eq><ci id="S2.SS5.p4.3.m3.3.3.4.cmml" xref="S2.SS5.p4.3.m3.3.3.4">𝒙</ci><vector id="S2.SS5.p4.3.m3.3.3.2.3.cmml" xref="S2.SS5.p4.3.m3.3.3.2.2"><apply id="S2.SS5.p4.3.m3.2.2.1.1.1.cmml" xref="S2.SS5.p4.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS5.p4.3.m3.2.2.1.1.1.1.cmml" xref="S2.SS5.p4.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S2.SS5.p4.3.m3.2.2.1.1.1.2.cmml" xref="S2.SS5.p4.3.m3.2.2.1.1.1.2">𝑥</ci><cn id="S2.SS5.p4.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS5.p4.3.m3.2.2.1.1.1.3">0</cn></apply><apply id="S2.SS5.p4.3.m3.3.3.2.2.2.cmml" xref="S2.SS5.p4.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS5.p4.3.m3.3.3.2.2.2.1.cmml" xref="S2.SS5.p4.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.SS5.p4.3.m3.3.3.2.2.2.2.cmml" xref="S2.SS5.p4.3.m3.3.3.2.2.2.2">𝑥</ci><cn id="S2.SS5.p4.3.m3.3.3.2.2.2.3.cmml" type="integer" xref="S2.SS5.p4.3.m3.3.3.2.2.2.3">1</cn></apply><ci id="S2.SS5.p4.3.m3.1.1.cmml" xref="S2.SS5.p4.3.m3.1.1">…</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p4.3.m3.3c">\bm{x}=(x_{0},x_{1},\ldots)</annotation><annotation encoding="application/x-llamapun" id="S2.SS5.p4.3.m3.3d">bold_italic_x = ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … )</annotation></semantics></math> represents the tokenized input sequence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS5.p5">
<p class="ltx_p" id="S2.SS5.p5.1">A major difference between our approach and Stanford Alpaca is that we only use the prompt template designed for examples without an <em class="ltx_emph ltx_font_italic" id="S2.SS5.p5.1.1">input</em> field, whereas Stanford Alpaca employs two templates for examples both with and without an <em class="ltx_emph ltx_font_italic" id="S2.SS5.p5.1.2">input</em> field. If the example contains a non-empty <em class="ltx_emph ltx_font_italic" id="S2.SS5.p5.1.3">input</em> field, we concatenate the <em class="ltx_emph ltx_font_italic" id="S2.SS5.p5.1.4">instruction</em> and <em class="ltx_emph ltx_font_italic" id="S2.SS5.p5.1.5">input</em> with an <em class="ltx_emph ltx_font_italic" id="S2.SS5.p5.1.6">“\n”</em> to form the new instruction. Note that there is an additional padding token for the Chinese Alpaca model, resulting in a vocabulary size 49,954.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setups</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Setups for Pre-training</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We initialize the Chinese LLaMA model with the original LLaMA weights and conduct pre-training using fp16 on the 7B and 13B models. Additionally, for the 33B model, we employ the bitsandbytes<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/TimDettmers/bitsandbytes" title="">https://github.com/TimDettmers/bitsandbytes</a></span></span></span> library to train it in an 8-bit format, enhancing its efficiency and memory usage. We directly apply LoRA to attentions and MLPs for training while setting the embeddings and LM head as trainable.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">For the basic version of Chinese LLaMA-7B, we utilize a two-stage pre-training approach.
In stage 1, we fix the parameters of the transformer encoders within the model and only train the embeddings, adapting the newly added Chinese word vectors while minimizing the disturbance to the original model.
In stage 2, we add LoRA weights (adapters) to the attention mechanisms and train the embeddings, LM heads, and newly added LoRA parameters.
Note that two-stage training is not applied to other model training as it is less efficient in our preliminary study.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">For the other Chinese LLaMA models (basic version), we utilize a 20GB general Chinese corpus for pre-training, which is consistent with the corpora used by Chinese BERT-wwm <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib4" title="">2021</a>)</cite>, MacBERT <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib3" title="">2020</a>)</cite>, LERT <cite class="ltx_cite ltx_citemacro_citep">(Cui et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib5" title="">2022</a>)</cite>, and others.
We also provide “Plus” version, which further expands the pre-training data to 120GB, incorporating additional data from CommonCrawl (CC) and encyclopedia sources, enhancing the model’s understanding of fundamental concepts. We concatenated all the datasets and generated chunks of block size 512 for pre-training purposes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">The models are trained on A40 GPUs (48GB VRAM) for one epoch, taking up to 48 GPUs depending on the model size. The parameter-efficient training with LoRA is performed with PEFT library<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/peft" title="">https://github.com/huggingface/peft</a></span></span></span>. We also utilize DeepSpeed <cite class="ltx_cite ltx_citemacro_citep">(Rasley et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib22" title="">2020</a>)</cite> to optimize memory efficiency during the training process. We employ the AdamW optimizer <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov &amp; Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib16" title="">2019</a>)</cite> with a peak learning rate of 2e-4 and 5% warm-up cosine scheduler. Additionally, we apply gradient clipping with a value of 1.0 to mitigate potential gradient explosion.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">Detailed hyperparameters for each Chinese LLaMA model are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S3.T2" title="Table 2 ‣ 3.1 Experimental Setups for Pre-training ‣ 3 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span> <span class="ltx_text ltx_font_bold" id="S3.T2.2.1">Pre-training hyperparameters for Chinese LLaMA.</span> QKVO: four matrices in each attention module, i.e., query, key, value, and output. MLP: three matrices in each MLP layer. Note that 7B uses a two-stage training paradigm (settings are separated by ‘/’), which is not further adopted in other models.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.3">
<tbody><tr class="ltx_tr" id="S3.T2.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.3.1.1.1">Settings</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.3.1.2.1">7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.3.1.3.1">Plus-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.3.1.4.1">13B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.5"><span class="ltx_text ltx_font_bold" id="S3.T2.3.1.5.1">Plus-13B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.6"><span class="ltx_text ltx_font_bold" id="S3.T2.3.1.6.1">33B</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.3.2.1">Training data</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.2.2">20 GB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.2.3">120 GB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.2.4">20 GB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.2.5">120 GB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.2.6">20 GB</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3">
<td class="ltx_td ltx_align_left" id="S3.T2.3.3.1">Batch size</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.3.2">1,024</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.3.3">2,304</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.3.4">2,304</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.3.5">2,304</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.3.6">2,304</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.4">
<td class="ltx_td ltx_align_left" id="S3.T2.3.4.1">Peak learning rate</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.4.2">2e-4/1e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.4.3">2e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.4.4">2e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.4.5">2e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.4.6">2e-4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.5">
<td class="ltx_td ltx_align_left" id="S3.T2.3.5.1">Max sequence length</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.5.2">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.5.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.5.4">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.5.5">512</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.5.6">512</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.6">
<td class="ltx_td ltx_align_left" id="S3.T2.3.6.1">LoRA rank</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.6.2">-/8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.6.3">8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.6.4">8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.6.5">8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.6.6">8</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.7">
<td class="ltx_td ltx_align_left" id="S3.T2.3.7.1">LoRA alpha</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.2">-/32</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.3">32</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.4">32</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.5">32</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.6">32</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.8">
<td class="ltx_td ltx_align_left" id="S3.T2.3.8.1">LoRA weights</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.8.2">-/QKVO</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.8.3">QKVO, MLP</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.8.4">QKVO, MLP</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.8.5">QKVO, MLP</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.8.6">QKVO, MLP</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.3.9.1">Trainable params (%)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.9.2">2.97%/6.06%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.9.3">6.22%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.9.4">4.10%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.9.5">4.10%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.9.6">2.21%</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experimental Setups for Instruction Fine-tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">After obtaining the Chinese LLaMA models, we fine-tune them according to Section <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS5" title="2.5 Supervised Fine-Tuning and Chinese Alpaca ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">2.5</span></a>. We continue to employ LoRA for efficient fine-tuning by adding LoRA modules to all linear layers of the base model. We utilize approximately 2M to 3M instruction data, including translation <cite class="ltx_cite ltx_citemacro_citep">(Xu, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib32" title="">2019</a>)</cite> (550K sampled), pCLUE<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/CLUEbenchmark/pCLUE" title="">https://github.com/CLUEbenchmark/pCLUE</a></span></span></span> (250K sampled, excluding “NLU-like” data), Stanford Alpaca (50K+50K for original and translated one), and crawled SFT data for tuning basic models.
For the Plus version, we expand the dataset to approximately 4M to 4.3M, with a specific emphasis on incorporating STEM (Science, Technology, Engineering, and Mathematics) data, as well as several scientific disciplines such as physics, chemistry, biology, medicine, and earth sciences.
For Alpaca-33B, we additionally add OASST1 dataset <cite class="ltx_cite ltx_citemacro_citep">(Köpf et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib12" title="">2023</a>)</cite>, where we only extract the first query-response pair from each conversation and translate using <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.1">gpt-3.5-turbo</span> API, resulting in roughly 20K data (original and translated one).
We set the maximum sequence length to 512 and pad the samples dynamically when batching to the maximum length in the batch.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">For the crawled data, we refer to the self-instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib31" title="">2022</a>)</cite> method for automatically obtaining data from ChatGPT (<span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1">gpt-3.5-turbo</span> API), as used in <cite class="ltx_cite ltx_citemacro_citet">Taori et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib26" title="">2023a</a>)</cite>.
Concretely, we utilize a more simplified template that does not require seed tasks, with only the requirements for targeted domains and instruction types.
Templates and code details are available on GitHub.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py" title="">https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py</a></span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span> Instruction fine-tuning hyperparameters for Chinese Alpaca.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.1">
<tbody><tr class="ltx_tr" id="S3.T3.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T3.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1">Settings</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.2.1">7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.3.1">Plus-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.4.1">13B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.5.1">Plus-13B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.6"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.6.1">33B</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.1.2.1">Training data</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.2">2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.3">4M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.4">3M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.5">4.3M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.6">4.3M</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3">
<td class="ltx_td ltx_align_left" id="S3.T3.1.3.1">Batch size</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.3.2">512</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.3.3">1,152</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.3.4">1,152</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.3.5">1,152</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.3.6">1,152</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4">
<td class="ltx_td ltx_align_left" id="S3.T3.1.4.1">Peak learning rate</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.4.2">1e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.4.3">1e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.4.4">1e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.4.5">1e-4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.4.6">1e-4</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5">
<td class="ltx_td ltx_align_left" id="S3.T3.1.5.1">Max sequence length</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.5.2">512</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.5.3">512</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.5.4">512</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.5.5">512</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.5.6">512</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6">
<td class="ltx_td ltx_align_left" id="S3.T3.1.6.1">LoRA rank</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.2">8</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.3">64</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.4">8</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.5">64</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.6">8</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.7">
<td class="ltx_td ltx_align_left" id="S3.T3.1.7.1">LoRA alpha</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.7.2">32</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.7.3">128</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.7.4">32</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.7.5">128</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.7.6">32</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.8">
<td class="ltx_td ltx_align_left" id="S3.T3.1.8.1">LoRA weights</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.8.2">QKVO, MLP</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.8.3">QKVO, MLP</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.8.4">QKVO, MLP</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.8.5">QKVO, MLP</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.8.6">QKVO, MLP</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T3.1.9.1">Trainable params (%)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.9.2">6.22%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.9.3">8.08%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.9.4">4.10%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.9.5">5.66%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.9.6">2.21%</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">For the Plus version, we utilize a larger LoRA rank compared to the basic version. Besides adjusting the learning rate and batch size, we also maintain consistency with the other hyperparameters and settings used during the pre-training stage.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">The hyperparameters for instruction fine-tuning are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S3.T3" title="Table 3 ‣ 3.2 Experimental Setups for Instruction Fine-tuning ‣ 3 Experimental Setups ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">3</span></a>.
Note that all Alpaca models are trained based on respective LLaMA models. For example, Chinese Alpaca-Plus-13B is trained upon Chinese LLaMA-Plus-13B.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results on Instruction-Following Tasks</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Task Design and Evaluation Method</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Evaluating the performance of text generation tasks can be challenging due to the significant variation in their form, making it significantly different from natural language understanding tasks, such as text classification and extractive machine reading comprehension. Following previous work that utilizes GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib18" title="">2023</a>)</cite> as a scoring method, we also adopt GPT-4 to provide an overall score (on a 10-point scale) for each sample, which is more efficient than human evaluation. However, GPT-4 may not always provide accurate scores, so we perform manual checks on its ratings and adjust them if necessary. The manual checks ensure that the scores are consistent and reflect the true performance of the models being evaluated. We use the following prompt template for scoring two outputs of the systems (which can be adjusted to multiple systems):</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<blockquote class="ltx_quote" id="S4.SS1.p2.1">
<p class="ltx_p" id="S4.SS1.p2.1.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1.1">The followings are two ChatGPT-like systems’ outputs. Please rate an overall score on a ten-point scale for each and give explanations to justify your scores.

<br class="ltx_break">
Prompt:

<br class="ltx_break">
</span>{<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1.2">prompt-input</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1.3">
<br class="ltx_break">
System1:

<br class="ltx_break">
</span>{<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1.4">system1-output</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1.5">
<br class="ltx_break">
System2:

<br class="ltx_break">
</span>{<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1.6">system2-output</span>}<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1.7"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</blockquote>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">By employing GPT-4 as a scoring method in conjunction with manual checks, we establish a reliable evaluation framework that effectively measures the performance of our Chinese Alpaca models on a range of natural language understanding and generation tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Our evaluation set is designed to comprehensively assess the Chinese Alpaca models across a wide range of natural language understanding and generation tasks. The set comprises 200 samples, covering ten distinct tasks, including Question Answering, Reasoning, Literature, Entertainment, Translation, Multi-turn Dialogue, Coding, and Ethics, etc. The overall score for a specific task is calculated by summing the scores for all samples within that task and normalizing the total to a 100-point scale. This approach ensures that the evaluation set reflects the models’ capabilities across various tasks, providing a balanced and robust measure of their performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Setups for Decoding</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The decoding process of LLMs plays a critical role in determining the quality and diversity of the generated text. In our experiments, we use the following decoding hyperparameters:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Context size: We set the context size to 2048, which determines the maximum number of tokens the model can consider simultaneously when generating text.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Maximum sequence length: We limit the generated sequence length to 512 tokens to ensure that the outputs remain focused and relevant to the input prompt.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Temperature: We set the temperature to 0.2, which controls the randomness of the sampling process. Lower values make the model generate more focused and deterministic outputs, while higher values increase diversity at the cost of coherence. For multi-turn dialogue and generation tasks, we slightly adjust the temperature to 0.5 to allow a more diverse output.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.3">Top-<math alttext="k" class="ltx_Math" display="inline" id="S4.I1.i4.p1.1.m1.1"><semantics id="S4.I1.i4.p1.1.m1.1a"><mi id="S4.I1.i4.p1.1.m1.1.1" xref="S4.I1.i4.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.1.m1.1b"><ci id="S4.I1.i4.p1.1.m1.1.1.cmml" xref="S4.I1.i4.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i4.p1.1.m1.1d">italic_k</annotation></semantics></math> sampling: We use Top-<math alttext="k" class="ltx_Math" display="inline" id="S4.I1.i4.p1.2.m2.1"><semantics id="S4.I1.i4.p1.2.m2.1a"><mi id="S4.I1.i4.p1.2.m2.1.1" xref="S4.I1.i4.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.2.m2.1b"><ci id="S4.I1.i4.p1.2.m2.1.1.cmml" xref="S4.I1.i4.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i4.p1.2.m2.1d">italic_k</annotation></semantics></math> sampling with <math alttext="k=40" class="ltx_Math" display="inline" id="S4.I1.i4.p1.3.m3.1"><semantics id="S4.I1.i4.p1.3.m3.1a"><mrow id="S4.I1.i4.p1.3.m3.1.1" xref="S4.I1.i4.p1.3.m3.1.1.cmml"><mi id="S4.I1.i4.p1.3.m3.1.1.2" xref="S4.I1.i4.p1.3.m3.1.1.2.cmml">k</mi><mo id="S4.I1.i4.p1.3.m3.1.1.1" xref="S4.I1.i4.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.I1.i4.p1.3.m3.1.1.3" xref="S4.I1.i4.p1.3.m3.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.3.m3.1b"><apply id="S4.I1.i4.p1.3.m3.1.1.cmml" xref="S4.I1.i4.p1.3.m3.1.1"><eq id="S4.I1.i4.p1.3.m3.1.1.1.cmml" xref="S4.I1.i4.p1.3.m3.1.1.1"></eq><ci id="S4.I1.i4.p1.3.m3.1.1.2.cmml" xref="S4.I1.i4.p1.3.m3.1.1.2">𝑘</ci><cn id="S4.I1.i4.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.I1.i4.p1.3.m3.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.3.m3.1c">k=40</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i4.p1.3.m3.1d">italic_k = 40</annotation></semantics></math>, meaning that the model selects its next token from the top 40 most probable tokens at each step, adding an element of randomness and diversity to the generated text.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.3">Top-<math alttext="p" class="ltx_Math" display="inline" id="S4.I1.i5.p1.1.m1.1"><semantics id="S4.I1.i5.p1.1.m1.1a"><mi id="S4.I1.i5.p1.1.m1.1.1" xref="S4.I1.i5.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i5.p1.1.m1.1b"><ci id="S4.I1.i5.p1.1.m1.1.1.cmml" xref="S4.I1.i5.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i5.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i5.p1.1.m1.1d">italic_p</annotation></semantics></math> sampling: We also employ Top-<math alttext="p" class="ltx_Math" display="inline" id="S4.I1.i5.p1.2.m2.1"><semantics id="S4.I1.i5.p1.2.m2.1a"><mi id="S4.I1.i5.p1.2.m2.1.1" xref="S4.I1.i5.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i5.p1.2.m2.1b"><ci id="S4.I1.i5.p1.2.m2.1.1.cmml" xref="S4.I1.i5.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i5.p1.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i5.p1.2.m2.1d">italic_p</annotation></semantics></math> sampling with <math alttext="p=0.9" class="ltx_Math" display="inline" id="S4.I1.i5.p1.3.m3.1"><semantics id="S4.I1.i5.p1.3.m3.1a"><mrow id="S4.I1.i5.p1.3.m3.1.1" xref="S4.I1.i5.p1.3.m3.1.1.cmml"><mi id="S4.I1.i5.p1.3.m3.1.1.2" xref="S4.I1.i5.p1.3.m3.1.1.2.cmml">p</mi><mo id="S4.I1.i5.p1.3.m3.1.1.1" xref="S4.I1.i5.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.I1.i5.p1.3.m3.1.1.3" xref="S4.I1.i5.p1.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i5.p1.3.m3.1b"><apply id="S4.I1.i5.p1.3.m3.1.1.cmml" xref="S4.I1.i5.p1.3.m3.1.1"><eq id="S4.I1.i5.p1.3.m3.1.1.1.cmml" xref="S4.I1.i5.p1.3.m3.1.1.1"></eq><ci id="S4.I1.i5.p1.3.m3.1.1.2.cmml" xref="S4.I1.i5.p1.3.m3.1.1.2">𝑝</ci><cn id="S4.I1.i5.p1.3.m3.1.1.3.cmml" type="float" xref="S4.I1.i5.p1.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i5.p1.3.m3.1c">p=0.9</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i5.p1.3.m3.1d">italic_p = 0.9</annotation></semantics></math>, which further enhances diversity by considering a dynamic set of tokens that collectively account for 90% of the probability mass.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1">Repetition penalty: To discourage the model from generating repetitive text, we apply a repetition penalty with a factor of 1.1, penalizing tokens that have already been selected.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Note that these values may not be optimal for each testing scenario. We did not perform further tuning on these hyperparameters for each task to maintain a balanced view.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We present and analyze the results obtained by our Chinese Alpaca-Plus-7B, Alpaca-Plus-13B, and Alpaca-33B models.
The Alpaca-33B results are generated by original model (FP16), while the Alpaca-Plus-7B and Alpaca-Plus-13B adopt 8-bit quantized version.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We will discuss the quantization effect in Section <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S6" title="6 Effect of Different Quantization Methods ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">6</span></a>.</span></span></span>
The overall results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.T4" title="Table 4 ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">4</span></a>.
The evaluation is based on GPT-4 rated results across ten distinct NLP tasks, encompassing a total of 200 samples.
It is important to note that the presented scores are solely comparable with each other but not with other models, which would require rescoring the systems.
Also, as our models are built upon original LLaMA, these observations can be regarded as what are important aspects to achieving better performance when built upon a well-established model rather than training from scratch.
We elaborate on the findings of several major categories in detail.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">We mainly present the results on Chinese-LLaMA and Chinese-Alpaca.
The results on Chinese-LLaMA-2 and Chinese-Alpaca-2 are presented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1" title="Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">A</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span> <span class="ltx_text ltx_font_bold" id="S4.T4.2.1">GPT-4 rated results for Chinese Alpaca-Plus-7B and Alpaca-Plus-13B, and Alpaca-33B.</span> Note that the results are only comparable within this model combination.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.3">
<tbody><tr class="ltx_tr" id="S4.T4.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.1.1">Task</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.2.1">Alpaca-Plus-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.3.1">Alpaca-Plus-13B</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.3.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.4.1">Alpaca-33B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.3.2.1">Question Answering</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.2.2">70.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.2.3">79.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.2.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.2.4.1">82.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3">
<td class="ltx_td ltx_align_left" id="S4.T4.3.3.1">Open-ended QA</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.2.1">80.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3">80.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.4">78.5</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.4">
<td class="ltx_td ltx_align_left" id="S4.T4.3.4.1">Numerical Reasoning</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.4.2">51.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.4.3">61.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.4.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.4.4.1">84.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.5">
<td class="ltx_td ltx_align_left" id="S4.T4.3.5.1">Poetry, Literature, Philosophy</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.5.2">78.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.5.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.5.3.1">81.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.5.4">76.0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.6">
<td class="ltx_td ltx_align_left" id="S4.T4.3.6.1">Music, Sports, Entertainment</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.6.2">72.3</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.6.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.6.3.1">76.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.6.4">72.5</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.7">
<td class="ltx_td ltx_align_left" id="S4.T4.3.7.1">Letters and Articles Writing</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.7.2">81.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.7.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.7.3.1">86.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.7.4">79.0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.8">
<td class="ltx_td ltx_align_left" id="S4.T4.3.8.1">Translation</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.8.2">86.8</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.8.3">89.3</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.8.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.8.4.1">92.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.9">
<td class="ltx_td ltx_align_left" id="S4.T4.3.9.1">Multi-turn Dialogue</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.9.2">80.3</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.9.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.9.3.1">81.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.9.4">78.0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.10">
<td class="ltx_td ltx_align_left" id="S4.T4.3.10.1">Coding</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.10.2">62.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.10.3">67.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.10.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.10.4.1">84.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.11">
<td class="ltx_td ltx_align_left" id="S4.T4.3.11.1">Ethics</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.11.2">89.8</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.11.3">90.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.11.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.11.4.1">92.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.12">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T4.3.12.1"><span class="ltx_text ltx_font_bold" id="S4.T4.3.12.1.1">Total</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.3.12.2">75.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.3.12.3">79.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.3.12.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.12.4.1">82.0</span></td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Multi-turn Dialogue</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">One of the impressive achievements of ChatGPT is its rich and fluent contextual understanding ability, which is conveyed by the multi-turn dialogue interface.
As we can see, Plus series models yield consistent improvements over the basic one, though the size of the latter one is several times that of the formers.
This might indicate that it is much more important to ingest more training data than simply extending the parameter size of the model to achieve a better dialogue experience.
Especially our models are constructed from the original LLaMA, where linguistic knowledge can not be directly transferred.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Text Generation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">Text generation is one of the most fundamental abilities for language models.
Compared to Alpaca-Plus-7B and Alpaca-Plus-13B, Alpaca-33B shows inferior results in this category.
Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.T5" title="Table 5 ‣ 4.3.2 Text Generation ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">5</span></a> shows an example of a text generation task.
We can see that both Alpaca-Plus-7B and Alpaca-Plus-13B provide correct letter styles, which meet the requirement of the user’s prompt.
Alpaca-Plus-13B provides the most comprehensive one by indicating that the applicant has thoroughly prepared all materials for visa application, making it the best generation quality among all three systems.
However, Alpaca-33B does not follow the letter style, and the content is somewhat too simplified, which is clearly not as good as the others.
This demonstrates that training with more data with smaller models might give better performance than big models with less data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span> <span class="ltx_text ltx_font_bold" id="S4.T5.2.1">Example output for text generation task.</span> 33B model does not provide detailed and rich context compared to Plus-7B and Plus-13B.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T5.3">
<tbody><tr class="ltx_tr" id="S4.T5.3.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T5.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.2.1">Prompt: 请你帮我写一封赴美旅游签证申请信</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.2.1">
<span class="ltx_text" id="S4.T5.3.2.1.1"></span> <span class="ltx_text" id="S4.T5.3.2.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T5.3.2.1.2.1">
<span class="ltx_tr" id="S4.T5.3.2.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T5.3.2.1.2.1.1.1">Plus-7B</span></span>
<span class="ltx_tr" id="S4.T5.3.2.1.2.1.2">
<span class="ltx_td ltx_align_center" id="S4.T5.3.2.1.2.1.2.1">(8.5/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T5.3.2.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.3.2.2">
<span class="ltx_text" id="S4.T5.3.2.2.1"></span><span class="ltx_text" id="S4.T5.3.2.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T5.3.2.2.2.1">
<span class="ltx_tr" id="S4.T5.3.2.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T5.3.2.2.2.1.1.1">尊敬的美国大使馆：</span></span>
<span class="ltx_tr" id="S4.T5.3.2.2.2.1.2">
<span class="ltx_td ltx_align_left" id="S4.T5.3.2.2.2.1.2.1">我是一名来自中国的游客，计划于2022年7月前往美国旅游。我非常期待这次旅行，</span></span>
<span class="ltx_tr" id="S4.T5.3.2.2.2.1.3">
<span class="ltx_td ltx_align_left" id="S4.T5.3.2.2.2.1.3.1">并希望获得签证。</span></span>
<span class="ltx_tr" id="S4.T5.3.2.2.2.1.4">
<span class="ltx_td ltx_align_left" id="S4.T5.3.2.2.2.1.4.1">我将遵守美国的法律和规定，并尊重当地文化和风俗。我将遵守旅游计划，并在离</span></span>
<span class="ltx_tr" id="S4.T5.3.2.2.2.1.5">
<span class="ltx_td ltx_align_left" id="S4.T5.3.2.2.2.1.5.1">开前向美国大使馆报告我的行踪。</span></span>
<span class="ltx_tr" id="S4.T5.3.2.2.2.1.6">
<span class="ltx_td ltx_align_left" id="S4.T5.3.2.2.2.1.6.1">感谢您考虑我的申请。</span></span>
<span class="ltx_tr" id="S4.T5.3.2.2.2.1.7">
<span class="ltx_td ltx_align_left" id="S4.T5.3.2.2.2.1.7.1">此致</span></span>
<span class="ltx_tr" id="S4.T5.3.2.2.2.1.8">
<span class="ltx_td ltx_align_left" id="S4.T5.3.2.2.2.1.8.1">敬礼！</span></span>
</span></span> <span class="ltx_text" id="S4.T5.3.2.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.3.1">
<span class="ltx_text" id="S4.T5.3.3.1.1"></span> <span class="ltx_text" id="S4.T5.3.3.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T5.3.3.1.2.1">
<span class="ltx_tr" id="S4.T5.3.3.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T5.3.3.1.2.1.1.1">Plus-13B</span></span>
<span class="ltx_tr" id="S4.T5.3.3.1.2.1.2">
<span class="ltx_td ltx_align_center" id="S4.T5.3.3.1.2.1.2.1">(10/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T5.3.3.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.3.3.2">
<span class="ltx_text" id="S4.T5.3.3.2.1"></span><span class="ltx_text" id="S4.T5.3.3.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T5.3.3.2.2.1">
<span class="ltx_tr" id="S4.T5.3.3.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T5.3.3.2.2.1.1.1">尊敬的美国大使馆官员，</span></span>
<span class="ltx_tr" id="S4.T5.3.3.2.2.1.2">
<span class="ltx_td ltx_align_left" id="S4.T5.3.3.2.2.1.2.1">我是xxx，来自中国。我计划于明年3月前往美国进行为期一个月的旅行。我的行程</span></span>
<span class="ltx_tr" id="S4.T5.3.3.2.2.1.3">
<span class="ltx_td ltx_align_left" id="S4.T5.3.3.2.2.1.3.1">包括参观纽约市、洛杉矶和拉斯维加斯等城市，并与当地的人们交流。我相信这次</span></span>
<span class="ltx_tr" id="S4.T5.3.3.2.2.1.4">
<span class="ltx_td ltx_align_left" id="S4.T5.3.3.2.2.1.4.1">旅行将对我的个人成长和发展有所帮助。</span></span>
<span class="ltx_tr" id="S4.T5.3.3.2.2.1.5">
<span class="ltx_td ltx_align_left" id="S4.T5.3.3.2.2.1.5.1">我已经准备好了所有需要的材料，包括机票预订证明、酒店预订证明以及旅游计划。</span></span>
<span class="ltx_tr" id="S4.T5.3.3.2.2.1.6">
<span class="ltx_td ltx_align_left" id="S4.T5.3.3.2.2.1.6.1">我希望通过此次申请获得签证，以确保我的旅行顺利进行。</span></span>
<span class="ltx_tr" id="S4.T5.3.3.2.2.1.7">
<span class="ltx_td ltx_align_left" id="S4.T5.3.3.2.2.1.7.1">谢谢您的时间和关注。我期待着能够很快收到您的回复。</span></span>
<span class="ltx_tr" id="S4.T5.3.3.2.2.1.8">
<span class="ltx_td ltx_align_left" id="S4.T5.3.3.2.2.1.8.1">此致，</span></span>
<span class="ltx_tr" id="S4.T5.3.3.2.2.1.9">
<span class="ltx_td ltx_align_left" id="S4.T5.3.3.2.2.1.9.1">敬礼</span></span>
</span></span> <span class="ltx_text" id="S4.T5.3.3.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.3.4.1">
<span class="ltx_text" id="S4.T5.3.4.1.1"></span> <span class="ltx_text" id="S4.T5.3.4.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T5.3.4.1.2.1">
<span class="ltx_tr" id="S4.T5.3.4.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T5.3.4.1.2.1.1.1">33B</span></span>
<span class="ltx_tr" id="S4.T5.3.4.1.2.1.2">
<span class="ltx_td ltx_align_center" id="S4.T5.3.4.1.2.1.2.1">(8/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T5.3.4.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T5.3.4.2">
<span class="ltx_text" id="S4.T5.3.4.2.1"></span><span class="ltx_text" id="S4.T5.3.4.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T5.3.4.2.2.1">
<span class="ltx_tr" id="S4.T5.3.4.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T5.3.4.2.2.1.1.1">尊敬的美国大使馆工作人员，我谨代表本人向您申请一次赴美旅游签证。我是一名</span></span>
<span class="ltx_tr" id="S4.T5.3.4.2.2.1.2">
<span class="ltx_td ltx_align_left" id="S4.T5.3.4.2.2.1.2.1">中国公民，目前从事互联网行业工作，有良好的职业背景和经济实力。此次旅行的</span></span>
<span class="ltx_tr" id="S4.T5.3.4.2.2.1.3">
<span class="ltx_td ltx_align_left" id="S4.T5.3.4.2.2.1.3.1">目的是探访美国文化、了解当地风土人情，并结交一些志同道合的朋友。我将遵守</span></span>
<span class="ltx_tr" id="S4.T5.3.4.2.2.1.4">
<span class="ltx_td ltx_align_left" id="S4.T5.3.4.2.2.1.4.1">美国的法律法规，不违反任何规定。谢谢您的考虑！</span></span>
</span></span> <span class="ltx_text" id="S4.T5.3.4.2.3"></span>
</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Numerical Calculation and Reasoning</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">Numerical reasoning has been regarded as one of the most essential tasks in examining the reasoning ability of large language models.
As we can see, the Alpaca-33B achieves significant improvements over Plus-7B and Plus-13B models.
Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.T6" title="Table 6 ‣ 4.3.3 Numerical Calculation and Reasoning ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">6</span></a> shows example outputs for this task.
The first prompt is well-known for probing the reasoning ability, namely “which one is heavier, 1kg of cotton or 1kg of iron?”.
Both Plus-7B and Plus-13B failed to give a correct answer mentioning that “cotton is lighter than iron”.
However, 33B could correctly identify that these two things are the same weight.
The second prompt is a simple calculation task, asking “how many legs for a cat and a chicken”.
However, as we can see, both Plus-7B and Plus-13B do not have the commonsense knowledge that a cat has four legs and two for a chicken, resulting in wrong answers.
The last prompt is a numerical reasoning task to let the model predict the next number of an array.
Still, only 33B model correctly identifies the pattern of the given array that the next number should be the square of its index.
These observations indicate that the size of the model is vital in numerical reasoning tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span> <span class="ltx_text ltx_font_bold" id="S4.T6.2.1">Example output for numerical reasoning task.</span> 33B model significantly outperforms Alpaca-Plus-7B and Alpaca-Plus-13B.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T6.3">
<tbody><tr class="ltx_tr" id="S4.T6.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T6.3.1.2.1">Prompt: 一公斤的棉花和一公斤的铁，哪一个更重？</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.2.1">
<span class="ltx_text" id="S4.T6.3.2.1.1"></span><span class="ltx_text" id="S4.T6.3.2.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.2.1.2.1">
<span class="ltx_tr" id="S4.T6.3.2.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.2.1.2.1.1.1">Plus-7B (3/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.2.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.2.2">
<span class="ltx_text" id="S4.T6.3.2.2.1"></span><span class="ltx_text" id="S4.T6.3.2.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.2.2.2.1">
<span class="ltx_tr" id="S4.T6.3.2.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.2.2.2.1.1.1">一公斤棉花比一公斤铁轻。</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.2.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.3.1">
<span class="ltx_text" id="S4.T6.3.3.1.1"></span><span class="ltx_text" id="S4.T6.3.3.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.3.1.2.1">
<span class="ltx_tr" id="S4.T6.3.3.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.3.1.2.1.1.1">Plus-13B (3/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.3.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.3.2">
<span class="ltx_text" id="S4.T6.3.3.2.1"></span><span class="ltx_text" id="S4.T6.3.3.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.3.2.2.1">
<span class="ltx_tr" id="S4.T6.3.3.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.3.2.2.1.1.1">一公斤的棉花比一公斤的铁轻。</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.3.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.4.1">
<span class="ltx_text" id="S4.T6.3.4.1.1"></span><span class="ltx_text" id="S4.T6.3.4.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.4.1.2.1">
<span class="ltx_tr" id="S4.T6.3.4.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.4.1.2.1.1.1">33B (9/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.4.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.4.2">
<span class="ltx_text" id="S4.T6.3.4.2.1"></span><span class="ltx_text" id="S4.T6.3.4.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.4.2.2.1">
<span class="ltx_tr" id="S4.T6.3.4.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.4.2.2.1.1.1">一公斤的棉花和一公斤的铁重量相同。</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.4.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.5">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.3.5.1"><span class="ltx_text ltx_font_bold" id="S4.T6.3.5.1.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.3.5.2"><span class="ltx_text ltx_font_bold" id="S4.T6.3.5.2.1">Prompt: 一只猫和一只鸡一共有几只脚？</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.6.1">
<span class="ltx_text" id="S4.T6.3.6.1.1"></span><span class="ltx_text" id="S4.T6.3.6.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.6.1.2.1">
<span class="ltx_tr" id="S4.T6.3.6.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.6.1.2.1.1.1">Plus-7B (1/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.6.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.6.2">
<span class="ltx_text" id="S4.T6.3.6.2.1"></span><span class="ltx_text" id="S4.T6.3.6.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.6.2.2.1">
<span class="ltx_tr" id="S4.T6.3.6.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.6.2.2.1.1.1">一只猫和一只鸡一共有两只脚。</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.6.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.7.1">
<span class="ltx_text" id="S4.T6.3.7.1.1"></span><span class="ltx_text" id="S4.T6.3.7.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.7.1.2.1">
<span class="ltx_tr" id="S4.T6.3.7.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.7.1.2.1.1.1">Plus-13B (1/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.7.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.7.2">
<span class="ltx_text" id="S4.T6.3.7.2.1"></span><span class="ltx_text" id="S4.T6.3.7.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.7.2.2.1">
<span class="ltx_tr" id="S4.T6.3.7.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.7.2.2.1.1.1">一只猫和一只鸡各有四只脚，所以两只动物一共有八只脚。</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.7.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.8.1">
<span class="ltx_text" id="S4.T6.3.8.1.1"></span><span class="ltx_text" id="S4.T6.3.8.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.8.1.2.1">
<span class="ltx_tr" id="S4.T6.3.8.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.8.1.2.1.1.1">33B (10/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.8.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.8.2">
<span class="ltx_text" id="S4.T6.3.8.2.1"></span><span class="ltx_text" id="S4.T6.3.8.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.8.2.2.1">
<span class="ltx_tr" id="S4.T6.3.8.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.8.2.2.1.1.1">这道题需要进行简单的数学运算。一只猫有4只脚，一只鸡有2只脚，</span></span>
<span class="ltx_tr" id="S4.T6.3.8.2.2.1.2">
<span class="ltx_td ltx_align_left" id="S4.T6.3.8.2.2.1.2.1">所以两只动物一共有6只脚。</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.8.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.9">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.3.9.1"><span class="ltx_text ltx_font_bold" id="S4.T6.3.9.1.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.3.9.2"><span class="ltx_text ltx_font_bold" id="S4.T6.3.9.2.1">Prompt: 序列1,4,9,16的后一个数是什么？</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.10.1">
<span class="ltx_text" id="S4.T6.3.10.1.1"></span><span class="ltx_text" id="S4.T6.3.10.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.10.1.2.1">
<span class="ltx_tr" id="S4.T6.3.10.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.10.1.2.1.1.1">Plus-7B (1/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.10.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.10.2">
<span class="ltx_text" id="S4.T6.3.10.2.1"></span><span class="ltx_text" id="S4.T6.3.10.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.10.2.2.1">
<span class="ltx_tr" id="S4.T6.3.10.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.10.2.2.1.1.1">后一个数是 16。</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.10.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.11.1">
<span class="ltx_text" id="S4.T6.3.11.1.1"></span><span class="ltx_text" id="S4.T6.3.11.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.11.1.2.1">
<span class="ltx_tr" id="S4.T6.3.11.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.11.1.2.1.1.1">Plus-13B (1/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.11.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.3.11.2">
<span class="ltx_text" id="S4.T6.3.11.2.1"></span><span class="ltx_text" id="S4.T6.3.11.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.11.2.2.1">
<span class="ltx_tr" id="S4.T6.3.11.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.11.2.2.1.1.1">21</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.11.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.12">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T6.3.12.1">
<span class="ltx_text" id="S4.T6.3.12.1.1"></span><span class="ltx_text" id="S4.T6.3.12.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.12.1.2.1">
<span class="ltx_tr" id="S4.T6.3.12.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.12.1.2.1.1.1">33B (10/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.12.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T6.3.12.2">
<span class="ltx_text" id="S4.T6.3.12.2.1"></span><span class="ltx_text" id="S4.T6.3.12.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T6.3.12.2.2.1">
<span class="ltx_tr" id="S4.T6.3.12.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.3.12.2.2.1.1.1">序列1,4,9,16的后一个数是25。</span></span>
</span></span> <span class="ltx_text" id="S4.T6.3.12.2.3"></span>
</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>Coding</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS4.p1">
<p class="ltx_p" id="S4.SS3.SSS4.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.F2" title="Figure 2 ‣ 4.3.4 Coding ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">2</span></a> shows an example of implementing the Dijkstra algorithm in Python.
Plus-7B scores 3/10 due to a structurally sound approach that unfortunately fails to calculate and update shortest distances correctly and includes an undefined function.
Plus-13B attempts abstraction by implementing a Graph class and a distance method, which shows a basic understanding of how a graph and its related operations could be represented in object-oriented programming. Also, the fact that it is attempting to implement a shortest path algorithm (despite not correctly implementing Dijkstra’s algorithm) makes it a slightly higher score than Plus-7B’s.
The 33B model offers a much better Dijkstra algorithm implementation, earning it an 8/10 score. Despite its lack of a priority queue and absence of error handling, which would enhance efficiency and robustness, the code correctly updates shortest distances, maintains track of predecessors, and ensures all nodes are visited, reflecting a fundamental understanding of the algorithm.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS4.p2">
<p class="ltx_p" id="S4.SS3.SSS4.p2.1">From these results, it could be inferred that larger models tend to perform better in complex tasks like code generation, potentially due to their ability to capture more intricate patterns in the training data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_inline-para ltx_minipage ltx_flex_size_1 ltx_align_center ltx_align_middle" id="S4.F2.1" style="width:357.7pt;">
<span class="ltx_para" id="S4.F2.1.p1">
<span class="ltx_p" id="S4.F2.1.p1.1">Plus-7B (3/10):</span>
<span class="ltx_listing ltx_lst_language_Python ltx_lst_numbers_left ltx_lstlisting ltx_framed_topbottom ltx_listing" id="S4.F2.1.p1.2" style="background-color:#FFFFFF;"><span class="ltx_listing_data"><a download="" href="data:text/plain;base64,aW1wb3J0IGhlYXBxCgpkZWYgZGlqa3N0cmEoZ3JhcGgsIHMpOgogICAgcXVldWUgPSBbXQogICAgaGVhcCA9IFtzXQogICAgd2hpbGUgbGVuKGhlYXApID4gMDoKICAgICAgICBjdXJyZW50ID0gaGVhcC5wb3AoKQogICAgICAgIGZvciBpIGluIGdyYXBoW2N1cnJlbnRdOgogICAgICAgICAgICBpZiBpIG5vdCBpbiBoZWFwIGFuZCBkaXN0KGksIHMpIDwgZGlzdChzLCBjdXJyZW50KToKICAgICAgICAgICAgICAgIGhlYXAuYXBwZW5kKGkpCiAgICByZXR1cm4gcXVldWU=">⬇</a></span>
<span class="ltx_listingline" id="lstnumberx1"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.1.1.1">1</span></span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx1.2" style="color:#0000FF;">import</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.3">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.4">heapq</span>
</span>
<span class="ltx_listingline" id="lstnumberx2"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.1.1.1">2</span></span>
</span>
<span class="ltx_listingline" id="lstnumberx3"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.1.1.1">3</span></span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx3.2" style="color:#0000FF;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.3">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.4">dijkstra</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.5">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.6">graph</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.7">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.8">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.9">s</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.10">):</span>
</span>
<span class="ltx_listingline" id="lstnumberx4"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.1.1.1">4</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.3">queue</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.4">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.5">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.6">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.7">[]</span>
</span>
<span class="ltx_listingline" id="lstnumberx5"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.1.1.1">5</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.3">heap</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.4">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.5">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.6">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.7">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.8">s</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.9">]</span>
</span>
<span class="ltx_listingline" id="lstnumberx6"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.1.1.1">6</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx6.3" style="color:#0000FF;">while</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.4">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx6.5" style="color:#0000FF;">len</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.6">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.7">heap</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.8">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.9">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.10">&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.11">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.12">0:</span>
</span>
<span class="ltx_listingline" id="lstnumberx7"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.1.1.1">7</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.3">current</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.4">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.5">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.6">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.7">heap</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.8">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.9">pop</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.10">()</span>
</span>
<span class="ltx_listingline" id="lstnumberx8"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.1.1.1">8</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx8.3" style="color:#0000FF;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.5">i</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.6">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx8.7" style="color:#0000FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.8">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.9">graph</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.10">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.11">current</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.12">]:</span>
</span>
<span class="ltx_listingline" id="lstnumberx9"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.1.1.1">9</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx9.3" style="color:#0000FF;">if</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.5">i</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.6">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx9.7" style="color:#0000FF;">not</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.8">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx9.9" style="color:#0000FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.10">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.11">heap</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.12">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx9.13" style="color:#0000FF;">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.14">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.15">dist</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.16">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.17">i</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.18">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.19">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.20">s</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.21">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.22">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.23">&lt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.24">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.25">dist</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.26">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.27">s</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.28">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.29">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.30">current</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.31">):</span>
</span>
<span class="ltx_listingline" id="lstnumberx10"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.1.1.1">10</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.3">heap</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.4">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.5">append</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.6">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.7">i</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.8">)</span>
</span>
<span class="ltx_listingline" id="lstnumberx11"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.1.1.1">11</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx11.3" style="color:#0000FF;">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.5">queue</span>
</span>
</span>
</span></span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_inline-para ltx_minipage ltx_flex_size_1 ltx_align_center ltx_align_middle" id="S4.F2.2" style="width:357.7pt;">
<span class="ltx_para" id="S4.F2.2.p1">
<span class="ltx_p" id="S4.F2.2.p1.1">Plus-13B (4/10):</span>
<span class="ltx_listing ltx_lst_language_Python ltx_lst_numbers_left ltx_lstlisting ltx_framed_topbottom ltx_listing" id="S4.F2.2.p1.2" style="background-color:#FFFFFF;"><span class="ltx_listing_data"><a download="" href="data:text/plain;base64,Y2xhc3MgR3JhcGg6CiAgICBkZWYgX19pbml0X18oc2VsZiwgYWRqKToKICAgICAgICBzZWxmLmFkaiA9IGFkagoKICAgIGRlZiBkaXN0YW5jZShzZWxmLCB2KToKICAgICAgICBkaXN0ID0gW2luZl0gKiBsZW4oc2VsZi5hZGopCiAgICAgICAgZm9yIHUgaW4gc2VsZi5hZGpbdl06CiAgICAgICAgICAgIGlmIHUgbm90IGluIGRpc3Q6CiAgICAgICAgICAgICAgICBkaXN0W3VdID0gMSArIG1pbihkaXN0W3VdLCBkaXN0W3ZdKQoKICAgICAgICByZXR1cm4gZGlzdFt2XQoKZGVmIG1haW4oKToKICAgIGcgPSBHcmFwaChbKDAsMSksICgxLDIpKSwgKDIsMyksICgzLDQpXSkKCiAgICB2ID0gMQogICAgZCA9IGcuZGlzdGFuY2UodikKICAgIHByaW50KCdEaXN0YW5jZSBpczonLCBkKQoKaWYgX19uYW1lX18gPT0gJ19fbWFpbl9fJzoKICAgIG1haW4oKQ==">⬇</a></span>
<span class="ltx_listingline" id="lstnumberx12"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.1.1.1">1</span></span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx12.2" style="color:#0000FF;">class</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.3">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.4">Graph</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.5">:</span>
</span>
<span class="ltx_listingline" id="lstnumberx13"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.1.1.1">2</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx13.3" style="color:#0000FF;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.5">__init__</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.6">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.7">self</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.8">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.9">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.10">adj</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.11">):</span>
</span>
<span class="ltx_listingline" id="lstnumberx14"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.1.1.1">3</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.3">self</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.4">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.5">adj</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.6">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.7">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.8">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.9">adj</span>
</span>
<span class="ltx_listingline" id="lstnumberx15"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.1.1.1">4</span></span>
</span>
<span class="ltx_listingline" id="lstnumberx16"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.1.1.1">5</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx16.3" style="color:#0000FF;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.5">distance</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.6">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.7">self</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.8">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.9">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.10">v</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.11">):</span>
</span>
<span class="ltx_listingline" id="lstnumberx17"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.1.1.1">6</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.3">dist</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.4">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.5">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.6">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.7">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.8">inf</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.9">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.10">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.11">*</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.12">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx17.13" style="color:#0000FF;">len</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.14">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.15">self</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.16">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.17">adj</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.18">)</span>
</span>
<span class="ltx_listingline" id="lstnumberx18"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.1.1.1">7</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx18.3" style="color:#0000FF;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.5">u</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.6">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx18.7" style="color:#0000FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.8">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.9">self</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.10">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.11">adj</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.12">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.13">v</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.14">]:</span>
</span>
<span class="ltx_listingline" id="lstnumberx19"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.1.1.1">8</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx19.3" style="color:#0000FF;">if</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.5">u</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.6">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx19.7" style="color:#0000FF;">not</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.8">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx19.9" style="color:#0000FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.10">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.11">dist</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.12">:</span>
</span>
<span class="ltx_listingline" id="lstnumberx20"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.1.1.1">9</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.3">dist</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.4">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.5">u</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.6">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.7">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.8">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.9">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.10">1</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.11">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.12">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.13">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx20.14" style="color:#0000FF;">min</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.15">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.16">dist</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.17">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.18">u</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.19">],</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.20">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.21">dist</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.22">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.23">v</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.24">])</span>
</span>
<span class="ltx_listingline" id="lstnumberx21"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.1.1.1">10</span></span>
</span>
<span class="ltx_listingline" id="lstnumberx22"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.1.1.1">11</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx22.3" style="color:#0000FF;">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.5">dist</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.6">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.7">v</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.8">]</span>
</span>
<span class="ltx_listingline" id="lstnumberx23"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.1.1.1">12</span></span>
</span>
<span class="ltx_listingline" id="lstnumberx24"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.1.1.1">13</span></span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx24.2" style="color:#0000FF;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.3">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.4">main</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.5">():</span>
</span>
<span class="ltx_listingline" id="lstnumberx25"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.1.1.1">14</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.3">g</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.4">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.5">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.6">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.7">Graph</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.8">([(0,1),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.9">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.10">(1,2)),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.11">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.12">(2,3),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx25.13">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.14">(3,4)])</span>
</span>
<span class="ltx_listingline" id="lstnumberx26"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx26.1.1.1">15</span></span>
</span>
<span class="ltx_listingline" id="lstnumberx27"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.1.1.1">16</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.3">v</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.4">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.5">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.6">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.7">1</span>
</span>
<span class="ltx_listingline" id="lstnumberx28"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.1.1.1">17</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx28.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx28.3">d</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx28.4">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.5">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx28.6">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx28.7">g</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.8">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx28.9">distance</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.10">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx28.11">v</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.12">)</span>
</span>
<span class="ltx_listingline" id="lstnumberx29"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.1.1.1">18</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx29.3" style="color:#0000FF;">print</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.4">(</span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx29.5" style="color:#FF0000;">’Distance<span class="ltx_text ltx_lst_space" id="lstnumberx29.5.1">&nbsp;</span>is:’</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.6">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.7">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.8">d</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.9">)</span>
</span>
<span class="ltx_listingline" id="lstnumberx30"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.1.1.1">19</span></span>
</span>
<span class="ltx_listingline" id="lstnumberx31"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.1.1.1">20</span></span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx31.2" style="color:#0000FF;">if</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.3">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.4">__name__</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.5">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.6">==</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.7">&nbsp;</span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx31.8" style="color:#FF0000;">’__main__’</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.9">:</span>
</span>
<span class="ltx_listingline" id="lstnumberx32"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.1.1.1">21</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.3">main</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.4">()</span>
</span>
</span>
</span></span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_inline-para ltx_minipage ltx_flex_size_1 ltx_align_center ltx_align_middle" id="S4.F2.3" style="width:357.7pt;">
<span class="ltx_para" id="S4.F2.3.p1">
<span class="ltx_p" id="S4.F2.3.p1.1">33B (8/10):</span>
<span class="ltx_listing ltx_lst_language_Python ltx_lst_numbers_left ltx_lstlisting ltx_framed_topbottom ltx_listing" id="S4.F2.3.p1.2" style="background-color:#FFFFFF;"><span class="ltx_listing_data"><a download="" href="data:text/plain;base64,ZGVmIGRpamtzdHJhKGdyYXBoLCBzb3VyY2UpOgogICAgZGlzdGFuY2VzID0ge30KICAgIHByZWRlY2Vzc29ycyA9IHt9CiAgICBmb3Igbm9kZSBpbiBncmFwaDoKICAgICAgICBkaXN0YW5jZXNbbm9kZV0gPSBmbG9hdCgnaW5mJykKICAgICAgICBwcmVkZWNlc3NvcnNbbm9kZV0gPSBOb25lCiAgICBkaXN0YW5jZXNbc291cmNlXSA9IDAKICAgIHF1ZXVlID0gW3NvdXJjZV0KICAgIHdoaWxlIHF1ZXVlOgogICAgICAgIGN1cnJlbnQgPSBxdWV1ZS5wb3AoMCkKICAgICAgICBmb3IgbmVpZ2hib3IgaW4gZ3JhcGhbY3VycmVudF06CiAgICAgICAgICAgIGlmIGRpc3RhbmNlc1tuZWlnaGJvcl0gPiBkaXN0YW5jZXNbY3VycmVudF0gKyBncmFwaFtjdXJyZW50XVtuZWlnaGJvcl06CiAgICAgICAgICAgICAgICBkaXN0YW5jZXNbbmVpZ2hib3JdID0gZGlzdGFuY2VzW2N1cnJlbnRdICsgZ3JhcGhbY3VycmVudF1bbmVpZ2hib3JdCiAgICAgICAgICAgICAgICBwcmVkZWNlc3NvcnNbbmVpZ2hib3JdID0gY3VycmVudAogICAgcmV0dXJuIGRpc3RhbmNlcw==">⬇</a></span>
<span class="ltx_listingline" id="lstnumberx33"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.1.1.1">1</span></span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx33.2" style="color:#0000FF;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.3">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.4">dijkstra</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.5">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.6">graph</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.7">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.8">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.9">source</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.10">):</span>
</span>
<span class="ltx_listingline" id="lstnumberx34"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.1.1.1">2</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.3">distances</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.4">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.5">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.6">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.7">{}</span>
</span>
<span class="ltx_listingline" id="lstnumberx35"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.1.1.1">3</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.3">predecessors</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.4">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.5">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.6">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.7">{}</span>
</span>
<span class="ltx_listingline" id="lstnumberx36"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx36.1.1.1">4</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx36.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx36.3" style="color:#0000FF;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx36.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx36.5">node</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx36.6">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx36.7" style="color:#0000FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx36.8">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx36.9">graph</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx36.10">:</span>
</span>
<span class="ltx_listingline" id="lstnumberx37"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.1.1.1">5</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx37.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.3">distances</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.4">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.5">node</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.6">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx37.7">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.8">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx37.9">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx37.10" style="color:#0000FF;">float</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.11">(</span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx37.12" style="color:#FF0000;">’inf’</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.13">)</span>
</span>
<span class="ltx_listingline" id="lstnumberx38"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx38.1.1.1">6</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx38.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx38.3">predecessors</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx38.4">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx38.5">node</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx38.6">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx38.7">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx38.8">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx38.9">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx38.10">None</span>
</span>
<span class="ltx_listingline" id="lstnumberx39"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx39.1.1.1">7</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.3">distances</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx39.4">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.5">source</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx39.6">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.7">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx39.8">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.9">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx39.10">0</span>
</span>
<span class="ltx_listingline" id="lstnumberx40"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx40.1.1.1">8</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.3">queue</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.4">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx40.5">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.6">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx40.7">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.8">source</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx40.9">]</span>
</span>
<span class="ltx_listingline" id="lstnumberx41"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx41.1.1.1">9</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx41.3" style="color:#0000FF;">while</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx41.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.5">queue</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx41.6">:</span>
</span>
<span class="ltx_listingline" id="lstnumberx42"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx42.1.1.1">10</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx42.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx42.3">current</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx42.4">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx42.5">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx42.6">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx42.7">queue</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx42.8">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx42.9">pop</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx42.10">(0)</span>
</span>
<span class="ltx_listingline" id="lstnumberx43"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx43.1.1.1">11</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx43.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx43.3" style="color:#0000FF;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx43.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx43.5">neighbor</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx43.6">&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx43.7" style="color:#0000FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx43.8">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx43.9">graph</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx43.10">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx43.11">current</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx43.12">]:</span>
</span>
<span class="ltx_listingline" id="lstnumberx44"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.1.1.1">12</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx44.3" style="color:#0000FF;">if</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.5">distances</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.6">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.7">neighbor</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.8">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.9">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.10">&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.11">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.12">distances</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.13">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.14">current</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.15">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.16">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.17">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx44.18">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.19">graph</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.20">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.21">current</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.22">][</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.23">neighbor</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.24">]:</span>
</span>
<span class="ltx_listingline" id="lstnumberx45"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.1.1.1">13</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.3">distances</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.4">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.5">neighbor</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.6">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.7">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.8">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.9">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.10">distances</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.11">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.12">current</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.13">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.14">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.15">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.16">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.17">graph</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.18">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.19">current</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.20">][</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.21">neighbor</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.22">]</span>
</span>
<span class="ltx_listingline" id="lstnumberx46"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx46.1.1.1">14</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx46.3">predecessors</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx46.4">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx46.5">neighbor</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx46.6">]</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.7">&nbsp;</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx46.8">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.9">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx46.10">current</span>
</span>
<span class="ltx_listingline" id="lstnumberx47"><span class="ltx_tag ltx_tag_listingline"><span class="ltx_text ltx_font_typewriter" id="lstnumberx47.1.1.1">15</span></span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.2">&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx47.3" style="color:#0000FF;">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx47.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.5">distances</span>
</span>
</span>
</span></span></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> <span class="ltx_text ltx_font_bold" id="S4.F2.5.1">Example output for coding task.</span> 33B model significantly outperforms Alpaca-Plus-7B and Alpaca-Plus-13B. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5 </span>Ethics</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS5.p1">
<p class="ltx_p" id="S4.SS3.SSS5.p1.1">Aligning LLMs to human preference is vital in creating responsible artificial intelligence.
In the Ethics category, we mainly want to test how these models respond to illegal input prompts.
By checking the generation results, all three systems responded properly to users’ prompts.
Alpaca-33B yields slightly better performance than the others.
We discover that Alpaca-33B may not only “reject” illegal prompts but also give appropriate advice in addition.
For example, in Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S4.T7" title="Table 7 ‣ 4.3.5 Ethics ‣ 4.3 Results ‣ 4 Results on Instruction-Following Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">7</span></a>, both Plus-7B and Plus-13B simply refuse to give any advice on making money by exploiting some network vulnerabilities.
On the contrary, 33B model not only refuses the user prompt but also gives advice on how to make money using legal ways, making the response more comprehensive and helpful.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span> <span class="ltx_text ltx_font_bold" id="S4.T7.2.1">Example output for ethics category.</span> 33B model tends to provide more comprehensive and helpful information over Plus-7B and Plus-13B.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T7.3">
<tbody><tr class="ltx_tr" id="S4.T7.3.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T7.3.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T7.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T7.3.1.2.1">Prompt: 如何利用一些网络漏洞赚钱？</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.3.2.1">
<span class="ltx_text" id="S4.T7.3.2.1.1"></span> <span class="ltx_text" id="S4.T7.3.2.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T7.3.2.1.2.1">
<span class="ltx_tr" id="S4.T7.3.2.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T7.3.2.1.2.1.1.1">Plus-7B</span></span>
<span class="ltx_tr" id="S4.T7.3.2.1.2.1.2">
<span class="ltx_td ltx_align_center" id="S4.T7.3.2.1.2.1.2.1">(8/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T7.3.2.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.3.2.2">
<span class="ltx_text" id="S4.T7.3.2.2.1"></span><span class="ltx_text" id="S4.T7.3.2.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T7.3.2.2.2.1">
<span class="ltx_tr" id="S4.T7.3.2.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T7.3.2.2.2.1.1.1">我不能提供任何有关利用网络漏洞来赚钱的建议或指导，因为这违反了道德和法律</span></span>
<span class="ltx_tr" id="S4.T7.3.2.2.2.1.2">
<span class="ltx_td ltx_align_left" id="S4.T7.3.2.2.2.1.2.1">规定。
网络安全是一项重要的任务，我们应该遵守相关的法律法规和行业标准，以</span></span>
<span class="ltx_tr" id="S4.T7.3.2.2.2.1.3">
<span class="ltx_td ltx_align_left" id="S4.T7.3.2.2.2.1.3.1">确保网络安全和保护用户隐私。</span></span>
</span></span> <span class="ltx_text" id="S4.T7.3.2.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.3.3.1">
<span class="ltx_text" id="S4.T7.3.3.1.1"></span> <span class="ltx_text" id="S4.T7.3.3.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T7.3.3.1.2.1">
<span class="ltx_tr" id="S4.T7.3.3.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T7.3.3.1.2.1.1.1">Plus-13B</span></span>
<span class="ltx_tr" id="S4.T7.3.3.1.2.1.2">
<span class="ltx_td ltx_align_center" id="S4.T7.3.3.1.2.1.2.1">(8/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T7.3.3.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T7.3.3.2">
<span class="ltx_text" id="S4.T7.3.3.2.1"></span><span class="ltx_text" id="S4.T7.3.3.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T7.3.3.2.2.1">
<span class="ltx_tr" id="S4.T7.3.3.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T7.3.3.2.2.1.1.1">作为一名AI语言模型，我不能鼓励或支持任何非法活动。利用网络漏洞进行非法活</span></span>
<span class="ltx_tr" id="S4.T7.3.3.2.2.1.2">
<span class="ltx_td ltx_align_left" id="S4.T7.3.3.2.2.1.2.1">动是犯罪行为，会带来严重的法律后果和道德问题。我建议您遵守法律法规，不要</span></span>
<span class="ltx_tr" id="S4.T7.3.3.2.2.1.3">
<span class="ltx_td ltx_align_left" id="S4.T7.3.3.2.2.1.3.1">从事任何违法行为。</span></span>
</span></span> <span class="ltx_text" id="S4.T7.3.3.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T7.3.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T7.3.4.1">
<span class="ltx_text" id="S4.T7.3.4.1.1"></span> <span class="ltx_text" id="S4.T7.3.4.1.2">
<span class="ltx_tabular ltx_align_top" id="S4.T7.3.4.1.2.1">
<span class="ltx_tr" id="S4.T7.3.4.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T7.3.4.1.2.1.1.1">33B</span></span>
<span class="ltx_tr" id="S4.T7.3.4.1.2.1.2">
<span class="ltx_td ltx_align_center" id="S4.T7.3.4.1.2.1.2.1">(9/10)</span></span>
</span></span> <span class="ltx_text" id="S4.T7.3.4.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T7.3.4.2">
<span class="ltx_text" id="S4.T7.3.4.2.1"></span><span class="ltx_text" id="S4.T7.3.4.2.2">
<span class="ltx_tabular ltx_align_top" id="S4.T7.3.4.2.2.1">
<span class="ltx_tr" id="S4.T7.3.4.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T7.3.4.2.2.1.1.1">作为AI语言模型，我不能提供任何非法或不道德的建议。我们应该遵守法律和道德</span></span>
<span class="ltx_tr" id="S4.T7.3.4.2.2.1.2">
<span class="ltx_td ltx_align_left" id="S4.T7.3.4.2.2.1.2.1">规范，不要从事任何违法活动。如果您想赚取合法收入，可以考虑以下几种方式：</span></span>
<span class="ltx_tr" id="S4.T7.3.4.2.2.1.3">
<span class="ltx_td ltx_align_left" id="S4.T7.3.4.2.2.1.3.1">1. 在线销售商品或服务：开设自己的网店或网站，出售自己制作的产品或服务，例</span></span>
<span class="ltx_tr" id="S4.T7.3.4.2.2.1.4">
<span class="ltx_td ltx_align_left" id="S4.T7.3.4.2.2.1.4.1">如手工艺品、设计作品、编程服务等。</span></span>
<span class="ltx_tr" id="S4.T7.3.4.2.2.1.5">
<span class="ltx_td ltx_align_left" id="S4.T7.3.4.2.2.1.5.1">2. 在线兼职：在一些招聘网站上寻找兼职机会，如翻译、写作、客服等工作。</span></span>
<span class="ltx_tr" id="S4.T7.3.4.2.2.1.6">
<span class="ltx_td ltx_align_left" id="S4.T7.3.4.2.2.1.6.1">3. 在线投资：通过股票、基金、债券等方式进行投资，获得收益。但需要注意风险</span></span>
<span class="ltx_tr" id="S4.T7.3.4.2.2.1.7">
<span class="ltx_td ltx_align_left" id="S4.T7.3.4.2.2.1.7.1">控制，避免过度杠杆化。</span></span>
<span class="ltx_tr" id="S4.T7.3.4.2.2.1.8">
<span class="ltx_td ltx_align_left" id="S4.T7.3.4.2.2.1.8.1">4. 创业：创建自己的公司或团队，开发新的产品或服务，并从中获利。</span></span>
<span class="ltx_tr" id="S4.T7.3.4.2.2.1.9">
<span class="ltx_td ltx_align_left" id="S4.T7.3.4.2.2.1.9.1">以上是一些常见的合法途径，希望对您有所帮助。</span></span>
</span></span> <span class="ltx_text" id="S4.T7.3.4.2.3"></span>
</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS5.p2">
<p class="ltx_p" id="S4.SS3.SSS5.p2.1">Overall, Alpaca-33B yields significant improvements over Alpaca-Plus-7B and Alpaca-Plus-13B in various aspects, including numerical reasoning, coding, ethics, etc.
We conjecture that these abilities are better handled by bigger models than the smaller ones, though Alpaca-33B is trained with less data.
Another possible reason would be the inherited ability from the original LLaMA, in which coding and reasoning ability is relatively language-independent.
However, we also noticed that Alpaca-33B has inferior results in text generation, multi-turn dialogue, etc. As Plus series models are trained on much more data, they are capable of providing more diverse and rich content.
We anticipate these issues can be tackled when Alpaca-Plus-33B becomes available, as we find these abilities are relatively easy to overcome than those that require high-level reasoning, such as numerical reasoning and coding-related tasks.
For complete comparisons, ratings, and sample outputs, please refer to our GitHub repository.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples" title="">https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples</a></span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results on Natural Language Understanding Tasks</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Task Description</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Besides the generation performance test for instruction-following tasks, we also tested our models on the C-Eval dataset <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib11" title="">2023</a>)</cite>, which is a multi-choice question answering dataset.
C-Eval mainly covers four categories: STEM, Social, Humanities, and Others, consisting of nearly 14K samples for 52 disciplines.
Similar to other multi-choice QA datasets, such as RACE <cite class="ltx_cite ltx_citemacro_citep">(Lai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib14" title="">2017</a>)</cite>, it requires the model to produce the correct option label based on the given question.
We mainly tested our model on the validation split (1,346 samples) and test split (12,342 samples), where the test scores are obtained by submitting models’ prediction files to the official leaderboard.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Decoding Strategy</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.6">To evaluate LLaMA models on this dataset, we directly feed the examples to these models.
While when evaluating Alpaca models, we wrap the examples in the prompt template as demonstrated in Section <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S2.SS5" title="2.5 Supervised Fine-Tuning and Chinese Alpaca ‣ 2 Chinese LLaMA and Chinese Alpaca ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">2.5</span></a>.
Then the model is asked to make a one-step prediction and give the probability distribution of the next token <math alttext="p(y|\bm{x})" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">p</mi><mo id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S5.SS2.p1.1.m1.1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.1.1.cmml"><mo id="S5.SS2.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S5.SS2.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS2.p1.1.m1.1.1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S5.SS2.p1.1.m1.1.1.1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S5.SS2.p1.1.m1.1.1.1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.1.1.1.3.cmml">𝒙</mi></mrow><mo id="S5.SS2.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S5.SS2.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><times id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2"></times><ci id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">𝑝</ci><apply id="S5.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S5.SS2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1.1.2">𝑦</ci><ci id="S5.SS2.p1.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.1.1.1.3">𝒙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">p(y|\bm{x})</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">italic_p ( italic_y | bold_italic_x )</annotation></semantics></math>, where <math alttext="y\in\mathcal{V}" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">y</mi><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">𝒱</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><in id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></in><ci id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">𝑦</ci><ci id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">𝒱</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">y\in\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">italic_y ∈ caligraphic_V</annotation></semantics></math> (<math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">𝒱</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><ci id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">𝒱</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">caligraphic_V</annotation></semantics></math> is the vocabulary). To map the probability distribution to a valid label <math alttext="t" class="ltx_Math" display="inline" id="S5.SS2.p1.4.m4.1"><semantics id="S5.SS2.p1.4.m4.1a"><mi id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><ci id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.4.m4.1d">italic_t</annotation></semantics></math> in {<span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.6.1">A</span>, <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.6.2">B</span>, <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.6.3">C</span>, <span class="ltx_text ltx_font_typewriter" id="S5.SS2.p1.6.4">D</span>}, we extract and gather the probabilities of related tokens. We introduce a verbalizer <math alttext="\mathcal{V(\cdot)}" class="ltx_Math" display="inline" id="S5.SS2.p1.5.m5.1"><semantics id="S5.SS2.p1.5.m5.1a"><mrow id="S5.SS2.p1.5.m5.1.2" xref="S5.SS2.p1.5.m5.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p1.5.m5.1.2.2" xref="S5.SS2.p1.5.m5.1.2.2.cmml">𝒱</mi><mo id="S5.SS2.p1.5.m5.1.2.1" xref="S5.SS2.p1.5.m5.1.2.1.cmml">⁢</mo><mrow id="S5.SS2.p1.5.m5.1.2.3.2" xref="S5.SS2.p1.5.m5.1.2.cmml"><mo id="S5.SS2.p1.5.m5.1.2.3.2.1" stretchy="false" xref="S5.SS2.p1.5.m5.1.2.cmml">(</mo><mo id="S5.SS2.p1.5.m5.1.1" lspace="0em" rspace="0em" xref="S5.SS2.p1.5.m5.1.1.cmml">⋅</mo><mo id="S5.SS2.p1.5.m5.1.2.3.2.2" stretchy="false" xref="S5.SS2.p1.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><apply id="S5.SS2.p1.5.m5.1.2.cmml" xref="S5.SS2.p1.5.m5.1.2"><times id="S5.SS2.p1.5.m5.1.2.1.cmml" xref="S5.SS2.p1.5.m5.1.2.1"></times><ci id="S5.SS2.p1.5.m5.1.2.2.cmml" xref="S5.SS2.p1.5.m5.1.2.2">𝒱</ci><ci id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">\mathcal{V(\cdot)}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.5.m5.1d">caligraphic_V ( ⋅ )</annotation></semantics></math> to map each label <math alttext="t" class="ltx_Math" display="inline" id="S5.SS2.p1.6.m6.1"><semantics id="S5.SS2.p1.6.m6.1a"><mi id="S5.SS2.p1.6.m6.1.1" xref="S5.SS2.p1.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.6.m6.1b"><ci id="S5.SS2.p1.6.m6.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.6.m6.1c">t</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.6.m6.1d">italic_t</annotation></semantics></math> to tokens in the vocabulary:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{V}(\texttt{A})=\{\textrm{`A'},`\textrm{\raisebox{0.0pt}{\rule{10.0000%
2pt}{1.25pt}}A'}\},\ \ \mathcal{V}(\texttt{B})=\{\textrm{`B'},`\textrm{%
\raisebox{0.0pt}{\rule{10.00002pt}{1.25pt}}B'}\},\ \ \mathcal{V}(\texttt{C})=%
\{\textrm{`C'},`\textrm{\raisebox{0.0pt}{\rule{10.00002pt}{1.25pt}}C'}\},\ \ %
\mathcal{V}(\texttt{D})=\{\textrm{`D'},`\textrm{\raisebox{0.0pt}{\rule{10.0000%
2pt}{1.25pt}}D'}\}" class="ltx_Math" display="block" id="S5.Ex1.m1.10"><semantics id="S5.Ex1.m1.10a"><mrow id="S5.Ex1.m1.10.10.2" xref="S5.Ex1.m1.10.10.3.cmml"><mrow id="S5.Ex1.m1.9.9.1.1" xref="S5.Ex1.m1.9.9.1.1.cmml"><mrow id="S5.Ex1.m1.9.9.1.1.3" xref="S5.Ex1.m1.9.9.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.Ex1.m1.9.9.1.1.3.2" xref="S5.Ex1.m1.9.9.1.1.3.2.cmml">𝒱</mi><mo id="S5.Ex1.m1.9.9.1.1.3.1" xref="S5.Ex1.m1.9.9.1.1.3.1.cmml">⁢</mo><mrow id="S5.Ex1.m1.9.9.1.1.3.3.2" xref="S5.Ex1.m1.1.1a.cmml"><mo id="S5.Ex1.m1.9.9.1.1.3.3.2.1" stretchy="false" xref="S5.Ex1.m1.1.1a.cmml">(</mo><mtext id="S5.Ex1.m1.1.1" xref="S5.Ex1.m1.1.1.cmml">𝙰</mtext><mo id="S5.Ex1.m1.9.9.1.1.3.3.2.2" stretchy="false" xref="S5.Ex1.m1.1.1a.cmml">)</mo></mrow></mrow><mo id="S5.Ex1.m1.9.9.1.1.2" xref="S5.Ex1.m1.9.9.1.1.2.cmml">=</mo><mrow id="S5.Ex1.m1.9.9.1.1.1.1" xref="S5.Ex1.m1.9.9.1.1.1.2.cmml"><mo id="S5.Ex1.m1.9.9.1.1.1.1.2" stretchy="false" xref="S5.Ex1.m1.9.9.1.1.1.2.cmml">{</mo><mtext id="S5.Ex1.m1.2.2" xref="S5.Ex1.m1.2.2a.cmml">‘A’</mtext><mo id="S5.Ex1.m1.9.9.1.1.1.1.3" xref="S5.Ex1.m1.9.9.1.1.1.2.cmml">,</mo><mrow id="S5.Ex1.m1.9.9.1.1.1.1.1" xref="S5.Ex1.m1.9.9.1.1.1.1.1.cmml"><mi id="S5.Ex1.m1.9.9.1.1.1.1.1.2" mathvariant="normal" xref="S5.Ex1.m1.9.9.1.1.1.1.1.2.cmml">`</mi><mo id="S5.Ex1.m1.9.9.1.1.1.1.1.1" xref="S5.Ex1.m1.9.9.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S5.Ex1.m1.9.9.1.1.1.1.1.3" xref="S5.Ex1.m1.9.9.1.1.1.1.1.3e.cmml"><mpadded id="S5.Ex1.m1.9.9.1.1.1.1.1.3a" voffset="0.0pt" xref="S5.Ex1.m1.9.9.1.1.1.1.1.3e.cmml"><mtext id="S5.Ex1.m1.9.9.1.1.1.1.1.3b" mathcolor="red" xref="S5.Ex1.m1.9.9.1.1.1.1.1.3e.cmml"><span class="ltx_rule" style="width:10.0pt;height:1.3pt;background:black;display:inline-block;">&nbsp;</span></mtext></mpadded><mtext id="S5.Ex1.m1.9.9.1.1.1.1.1.3d" xref="S5.Ex1.m1.9.9.1.1.1.1.1.3e.cmml">A’</mtext></mrow></mrow><mo id="S5.Ex1.m1.9.9.1.1.1.1.4" stretchy="false" xref="S5.Ex1.m1.9.9.1.1.1.2.cmml">}</mo></mrow></mrow><mo id="S5.Ex1.m1.10.10.2.3" rspace="1.167em" xref="S5.Ex1.m1.10.10.3a.cmml">,</mo><mrow id="S5.Ex1.m1.10.10.2.2.2" xref="S5.Ex1.m1.10.10.2.2.3.cmml"><mrow id="S5.Ex1.m1.10.10.2.2.1.1" xref="S5.Ex1.m1.10.10.2.2.1.1.cmml"><mrow id="S5.Ex1.m1.10.10.2.2.1.1.3" xref="S5.Ex1.m1.10.10.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.Ex1.m1.10.10.2.2.1.1.3.2" xref="S5.Ex1.m1.10.10.2.2.1.1.3.2.cmml">𝒱</mi><mo id="S5.Ex1.m1.10.10.2.2.1.1.3.1" xref="S5.Ex1.m1.10.10.2.2.1.1.3.1.cmml">⁢</mo><mrow id="S5.Ex1.m1.10.10.2.2.1.1.3.3.2" xref="S5.Ex1.m1.3.3a.cmml"><mo id="S5.Ex1.m1.10.10.2.2.1.1.3.3.2.1" stretchy="false" xref="S5.Ex1.m1.3.3a.cmml">(</mo><mtext id="S5.Ex1.m1.3.3" xref="S5.Ex1.m1.3.3.cmml">𝙱</mtext><mo id="S5.Ex1.m1.10.10.2.2.1.1.3.3.2.2" stretchy="false" xref="S5.Ex1.m1.3.3a.cmml">)</mo></mrow></mrow><mo id="S5.Ex1.m1.10.10.2.2.1.1.2" xref="S5.Ex1.m1.10.10.2.2.1.1.2.cmml">=</mo><mrow id="S5.Ex1.m1.10.10.2.2.1.1.1.1" xref="S5.Ex1.m1.10.10.2.2.1.1.1.2.cmml"><mo id="S5.Ex1.m1.10.10.2.2.1.1.1.1.2" stretchy="false" xref="S5.Ex1.m1.10.10.2.2.1.1.1.2.cmml">{</mo><mtext id="S5.Ex1.m1.4.4" xref="S5.Ex1.m1.4.4a.cmml">‘B’</mtext><mo id="S5.Ex1.m1.10.10.2.2.1.1.1.1.3" xref="S5.Ex1.m1.10.10.2.2.1.1.1.2.cmml">,</mo><mrow id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.cmml"><mi id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.2" mathvariant="normal" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.2.cmml">`</mi><mo id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.1" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3e.cmml"><mpadded id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3a" voffset="0.0pt" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3e.cmml"><mtext id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3b" mathcolor="red" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3e.cmml"><span class="ltx_rule" style="width:10.0pt;height:1.3pt;background:black;display:inline-block;">&nbsp;</span></mtext></mpadded><mtext id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3d" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3e.cmml">B’</mtext></mrow></mrow><mo id="S5.Ex1.m1.10.10.2.2.1.1.1.1.4" stretchy="false" xref="S5.Ex1.m1.10.10.2.2.1.1.1.2.cmml">}</mo></mrow></mrow><mo id="S5.Ex1.m1.10.10.2.2.2.3" rspace="1.167em" xref="S5.Ex1.m1.10.10.2.2.3a.cmml">,</mo><mrow id="S5.Ex1.m1.10.10.2.2.2.2.2" xref="S5.Ex1.m1.10.10.2.2.2.2.3.cmml"><mrow id="S5.Ex1.m1.10.10.2.2.2.2.1.1" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.cmml"><mrow id="S5.Ex1.m1.10.10.2.2.2.2.1.1.3" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.2" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.2.cmml">𝒱</mi><mo id="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.1" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.1.cmml">⁢</mo><mrow id="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.3.2" xref="S5.Ex1.m1.5.5a.cmml"><mo id="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.3.2.1" stretchy="false" xref="S5.Ex1.m1.5.5a.cmml">(</mo><mtext id="S5.Ex1.m1.5.5" xref="S5.Ex1.m1.5.5.cmml">𝙲</mtext><mo id="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.3.2.2" stretchy="false" xref="S5.Ex1.m1.5.5a.cmml">)</mo></mrow></mrow><mo id="S5.Ex1.m1.10.10.2.2.2.2.1.1.2" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.2.cmml">=</mo><mrow id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.2.cmml"><mo id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.2" stretchy="false" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.2.cmml">{</mo><mtext id="S5.Ex1.m1.6.6" xref="S5.Ex1.m1.6.6a.cmml">‘C’</mtext><mo id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.3" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.2.cmml">,</mo><mrow id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.cmml"><mi id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.2" mathvariant="normal" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.2.cmml">`</mi><mo id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.1" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3e.cmml"><mpadded id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3a" voffset="0.0pt" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3e.cmml"><mtext id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3b" mathcolor="red" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3e.cmml"><span class="ltx_rule" style="width:10.0pt;height:1.3pt;background:black;display:inline-block;">&nbsp;</span></mtext></mpadded><mtext id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3d" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3e.cmml">C’</mtext></mrow></mrow><mo id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.4" stretchy="false" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.2.cmml">}</mo></mrow></mrow><mo id="S5.Ex1.m1.10.10.2.2.2.2.2.3" rspace="1.167em" xref="S5.Ex1.m1.10.10.2.2.2.2.3a.cmml">,</mo><mrow id="S5.Ex1.m1.10.10.2.2.2.2.2.2" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.cmml"><mrow id="S5.Ex1.m1.10.10.2.2.2.2.2.2.3" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.2" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.2.cmml">𝒱</mi><mo id="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.1" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.1.cmml">⁢</mo><mrow id="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.3.2" xref="S5.Ex1.m1.7.7a.cmml"><mo id="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.3.2.1" stretchy="false" xref="S5.Ex1.m1.7.7a.cmml">(</mo><mtext id="S5.Ex1.m1.7.7" xref="S5.Ex1.m1.7.7.cmml">𝙳</mtext><mo id="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.3.2.2" stretchy="false" xref="S5.Ex1.m1.7.7a.cmml">)</mo></mrow></mrow><mo id="S5.Ex1.m1.10.10.2.2.2.2.2.2.2" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.2.cmml">=</mo><mrow id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.2.cmml"><mo id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.2" stretchy="false" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.2.cmml">{</mo><mtext id="S5.Ex1.m1.8.8" xref="S5.Ex1.m1.8.8a.cmml">‘D’</mtext><mo id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.3" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.2.cmml">,</mo><mrow id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.cmml"><mi id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.2" mathvariant="normal" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.2.cmml">`</mi><mo id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.1" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.1.cmml">⁢</mo><mrow id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3e.cmml"><mpadded id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3a" voffset="0.0pt" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3e.cmml"><mtext id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3b" mathcolor="red" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3e.cmml"><span class="ltx_rule" style="width:10.0pt;height:1.3pt;background:black;display:inline-block;">&nbsp;</span></mtext></mpadded><mtext id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3d" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3e.cmml">D’</mtext></mrow></mrow><mo id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.4" stretchy="false" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.2.cmml">}</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex1.m1.10b"><apply id="S5.Ex1.m1.10.10.3.cmml" xref="S5.Ex1.m1.10.10.2"><csymbol cd="ambiguous" id="S5.Ex1.m1.10.10.3a.cmml" xref="S5.Ex1.m1.10.10.2.3">formulae-sequence</csymbol><apply id="S5.Ex1.m1.9.9.1.1.cmml" xref="S5.Ex1.m1.9.9.1.1"><eq id="S5.Ex1.m1.9.9.1.1.2.cmml" xref="S5.Ex1.m1.9.9.1.1.2"></eq><apply id="S5.Ex1.m1.9.9.1.1.3.cmml" xref="S5.Ex1.m1.9.9.1.1.3"><times id="S5.Ex1.m1.9.9.1.1.3.1.cmml" xref="S5.Ex1.m1.9.9.1.1.3.1"></times><ci id="S5.Ex1.m1.9.9.1.1.3.2.cmml" xref="S5.Ex1.m1.9.9.1.1.3.2">𝒱</ci><ci id="S5.Ex1.m1.1.1a.cmml" xref="S5.Ex1.m1.9.9.1.1.3.3.2"><mtext id="S5.Ex1.m1.1.1.cmml" xref="S5.Ex1.m1.1.1">𝙰</mtext></ci></apply><set id="S5.Ex1.m1.9.9.1.1.1.2.cmml" xref="S5.Ex1.m1.9.9.1.1.1.1"><ci id="S5.Ex1.m1.2.2a.cmml" xref="S5.Ex1.m1.2.2"><mtext id="S5.Ex1.m1.2.2.cmml" xref="S5.Ex1.m1.2.2">‘A’</mtext></ci><apply id="S5.Ex1.m1.9.9.1.1.1.1.1.cmml" xref="S5.Ex1.m1.9.9.1.1.1.1.1"><times id="S5.Ex1.m1.9.9.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.9.9.1.1.1.1.1.1"></times><ci id="S5.Ex1.m1.9.9.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.9.9.1.1.1.1.1.2">`</ci><ci id="S5.Ex1.m1.9.9.1.1.1.1.1.3e.cmml" xref="S5.Ex1.m1.9.9.1.1.1.1.1.3"><mrow id="S5.Ex1.m1.9.9.1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.9.9.1.1.1.1.1.3"><mpadded id="S5.Ex1.m1.9.9.1.1.1.1.1.3a.cmml" voffset="0.0pt" xref="S5.Ex1.m1.9.9.1.1.1.1.1.3"><mtext id="S5.Ex1.m1.9.9.1.1.1.1.1.3b.cmml" mathcolor="red" xref="S5.Ex1.m1.9.9.1.1.1.1.1.3"><span class="ltx_rule" style="width:10.0pt;height:1.3pt;background:black;display:inline-block;">&nbsp;</span></mtext></mpadded><mtext id="S5.Ex1.m1.9.9.1.1.1.1.1.3d.cmml" xref="S5.Ex1.m1.9.9.1.1.1.1.1.3">A’</mtext></mrow></ci></apply></set></apply><apply id="S5.Ex1.m1.10.10.2.2.3.cmml" xref="S5.Ex1.m1.10.10.2.2.2"><csymbol cd="ambiguous" id="S5.Ex1.m1.10.10.2.2.3a.cmml" xref="S5.Ex1.m1.10.10.2.2.2.3">formulae-sequence</csymbol><apply id="S5.Ex1.m1.10.10.2.2.1.1.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1"><eq id="S5.Ex1.m1.10.10.2.2.1.1.2.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.2"></eq><apply id="S5.Ex1.m1.10.10.2.2.1.1.3.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.3"><times id="S5.Ex1.m1.10.10.2.2.1.1.3.1.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.3.1"></times><ci id="S5.Ex1.m1.10.10.2.2.1.1.3.2.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.3.2">𝒱</ci><ci id="S5.Ex1.m1.3.3a.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.3.3.2"><mtext id="S5.Ex1.m1.3.3.cmml" xref="S5.Ex1.m1.3.3">𝙱</mtext></ci></apply><set id="S5.Ex1.m1.10.10.2.2.1.1.1.2.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1"><ci id="S5.Ex1.m1.4.4a.cmml" xref="S5.Ex1.m1.4.4"><mtext id="S5.Ex1.m1.4.4.cmml" xref="S5.Ex1.m1.4.4">‘B’</mtext></ci><apply id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1"><times id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.1"></times><ci id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.2">`</ci><ci id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3e.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3"><mrow id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3"><mpadded id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3a.cmml" voffset="0.0pt" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3"><mtext id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3b.cmml" mathcolor="red" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3"><span class="ltx_rule" style="width:10.0pt;height:1.3pt;background:black;display:inline-block;">&nbsp;</span></mtext></mpadded><mtext id="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3d.cmml" xref="S5.Ex1.m1.10.10.2.2.1.1.1.1.1.3">B’</mtext></mrow></ci></apply></set></apply><apply id="S5.Ex1.m1.10.10.2.2.2.2.3.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2"><csymbol cd="ambiguous" id="S5.Ex1.m1.10.10.2.2.2.2.3a.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S5.Ex1.m1.10.10.2.2.2.2.1.1.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1"><eq id="S5.Ex1.m1.10.10.2.2.2.2.1.1.2.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.2"></eq><apply id="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.3"><times id="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.1.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.1"></times><ci id="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.2.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.2">𝒱</ci><ci id="S5.Ex1.m1.5.5a.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.3.3.2"><mtext id="S5.Ex1.m1.5.5.cmml" xref="S5.Ex1.m1.5.5">𝙲</mtext></ci></apply><set id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.2.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1"><ci id="S5.Ex1.m1.6.6a.cmml" xref="S5.Ex1.m1.6.6"><mtext id="S5.Ex1.m1.6.6.cmml" xref="S5.Ex1.m1.6.6">‘C’</mtext></ci><apply id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1"><times id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.1"></times><ci id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.2">`</ci><ci id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3e.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3"><mrow id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3"><mpadded id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3a.cmml" voffset="0.0pt" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3"><mtext id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3b.cmml" mathcolor="red" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3"><span class="ltx_rule" style="width:10.0pt;height:1.3pt;background:black;display:inline-block;">&nbsp;</span></mtext></mpadded><mtext id="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3d.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.1.1.1.1.1.3">C’</mtext></mrow></ci></apply></set></apply><apply id="S5.Ex1.m1.10.10.2.2.2.2.2.2.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2"><eq id="S5.Ex1.m1.10.10.2.2.2.2.2.2.2.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.2"></eq><apply id="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.3"><times id="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.1.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.1"></times><ci id="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.2.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.2">𝒱</ci><ci id="S5.Ex1.m1.7.7a.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.3.3.2"><mtext id="S5.Ex1.m1.7.7.cmml" xref="S5.Ex1.m1.7.7">𝙳</mtext></ci></apply><set id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.2.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1"><ci id="S5.Ex1.m1.8.8a.cmml" xref="S5.Ex1.m1.8.8"><mtext id="S5.Ex1.m1.8.8.cmml" xref="S5.Ex1.m1.8.8">‘D’</mtext></ci><apply id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1"><times id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.1.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.1"></times><ci id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.2.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.2">`</ci><ci id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3e.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3"><mrow id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3"><mpadded id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3a.cmml" voffset="0.0pt" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3"><mtext id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3b.cmml" mathcolor="red" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3"><span class="ltx_rule" style="width:10.0pt;height:1.3pt;background:black;display:inline-block;">&nbsp;</span></mtext></mpadded><mtext id="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3d.cmml" xref="S5.Ex1.m1.10.10.2.2.2.2.2.2.1.1.1.3">D’</mtext></mrow></ci></apply></set></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex1.m1.10c">\mathcal{V}(\texttt{A})=\{\textrm{`A'},`\textrm{\raisebox{0.0pt}{\rule{10.0000%
2pt}{1.25pt}}A'}\},\ \ \mathcal{V}(\texttt{B})=\{\textrm{`B'},`\textrm{%
\raisebox{0.0pt}{\rule{10.00002pt}{1.25pt}}B'}\},\ \ \mathcal{V}(\texttt{C})=%
\{\textrm{`C'},`\textrm{\raisebox{0.0pt}{\rule{10.00002pt}{1.25pt}}C'}\},\ \ %
\mathcal{V}(\texttt{D})=\{\textrm{`D'},`\textrm{\raisebox{0.0pt}{\rule{10.0000%
2pt}{1.25pt}}D'}\}</annotation><annotation encoding="application/x-llamapun" id="S5.Ex1.m1.10d">caligraphic_V ( A ) = { ‘A’ , ` A’ } , caligraphic_V ( B ) = { ‘B’ , ` B’ } , caligraphic_V ( C ) = { ‘C’ , ` C’ } , caligraphic_V ( D ) = { ‘D’ , ` D’ }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">The probability of predicting label <math alttext="t" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">italic_t</annotation></semantics></math> is given by</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S5.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="p(t\in\{\texttt{A},\texttt{B},\texttt{C},\texttt{D}\}|\bm{x})=\sum_{t\in%
\mathcal{V}(i)}p(y=i|\bm{x})" class="ltx_Math" display="block" id="S5.E4.m1.7"><semantics id="S5.E4.m1.7a"><mrow id="S5.E4.m1.7.7" xref="S5.E4.m1.7.7.cmml"><mrow id="S5.E4.m1.6.6.1" xref="S5.E4.m1.6.6.1.cmml"><mi id="S5.E4.m1.6.6.1.3" xref="S5.E4.m1.6.6.1.3.cmml">p</mi><mo id="S5.E4.m1.6.6.1.2" xref="S5.E4.m1.6.6.1.2.cmml">⁢</mo><mrow id="S5.E4.m1.6.6.1.1.1" xref="S5.E4.m1.6.6.1.1.1.1.cmml"><mo id="S5.E4.m1.6.6.1.1.1.2" stretchy="false" xref="S5.E4.m1.6.6.1.1.1.1.cmml">(</mo><mrow id="S5.E4.m1.6.6.1.1.1.1" xref="S5.E4.m1.6.6.1.1.1.1.cmml"><mi id="S5.E4.m1.6.6.1.1.1.1.2" xref="S5.E4.m1.6.6.1.1.1.1.2.cmml">t</mi><mo id="S5.E4.m1.6.6.1.1.1.1.1" xref="S5.E4.m1.6.6.1.1.1.1.1.cmml">∈</mo><mrow id="S5.E4.m1.6.6.1.1.1.1.3" xref="S5.E4.m1.6.6.1.1.1.1.3.cmml"><mrow id="S5.E4.m1.6.6.1.1.1.1.3.2.2" xref="S5.E4.m1.6.6.1.1.1.1.3.2.1.cmml"><mo id="S5.E4.m1.6.6.1.1.1.1.3.2.2.1" stretchy="false" xref="S5.E4.m1.6.6.1.1.1.1.3.2.1.cmml">{</mo><mtext id="S5.E4.m1.2.2" xref="S5.E4.m1.2.2a.cmml">𝙰</mtext><mo id="S5.E4.m1.6.6.1.1.1.1.3.2.2.2" xref="S5.E4.m1.6.6.1.1.1.1.3.2.1.cmml">,</mo><mtext id="S5.E4.m1.3.3" xref="S5.E4.m1.3.3a.cmml">𝙱</mtext><mo id="S5.E4.m1.6.6.1.1.1.1.3.2.2.3" xref="S5.E4.m1.6.6.1.1.1.1.3.2.1.cmml">,</mo><mtext id="S5.E4.m1.4.4" xref="S5.E4.m1.4.4a.cmml">𝙲</mtext><mo id="S5.E4.m1.6.6.1.1.1.1.3.2.2.4" xref="S5.E4.m1.6.6.1.1.1.1.3.2.1.cmml">,</mo><mtext id="S5.E4.m1.5.5" xref="S5.E4.m1.5.5a.cmml">𝙳</mtext><mo id="S5.E4.m1.6.6.1.1.1.1.3.2.2.5" stretchy="false" xref="S5.E4.m1.6.6.1.1.1.1.3.2.1.cmml">}</mo></mrow><mo fence="false" id="S5.E4.m1.6.6.1.1.1.1.3.1" xref="S5.E4.m1.6.6.1.1.1.1.3.1.cmml">|</mo><mi id="S5.E4.m1.6.6.1.1.1.1.3.3" xref="S5.E4.m1.6.6.1.1.1.1.3.3.cmml">𝒙</mi></mrow></mrow><mo id="S5.E4.m1.6.6.1.1.1.3" stretchy="false" xref="S5.E4.m1.6.6.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E4.m1.7.7.3" rspace="0.111em" xref="S5.E4.m1.7.7.3.cmml">=</mo><mrow id="S5.E4.m1.7.7.2" xref="S5.E4.m1.7.7.2.cmml"><munder id="S5.E4.m1.7.7.2.2" xref="S5.E4.m1.7.7.2.2.cmml"><mo id="S5.E4.m1.7.7.2.2.2" movablelimits="false" xref="S5.E4.m1.7.7.2.2.2.cmml">∑</mo><mrow id="S5.E4.m1.1.1.1" xref="S5.E4.m1.1.1.1.cmml"><mi id="S5.E4.m1.1.1.1.3" xref="S5.E4.m1.1.1.1.3.cmml">t</mi><mo id="S5.E4.m1.1.1.1.2" xref="S5.E4.m1.1.1.1.2.cmml">∈</mo><mrow id="S5.E4.m1.1.1.1.4" xref="S5.E4.m1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E4.m1.1.1.1.4.2" xref="S5.E4.m1.1.1.1.4.2.cmml">𝒱</mi><mo id="S5.E4.m1.1.1.1.4.1" xref="S5.E4.m1.1.1.1.4.1.cmml">⁢</mo><mrow id="S5.E4.m1.1.1.1.4.3.2" xref="S5.E4.m1.1.1.1.4.cmml"><mo id="S5.E4.m1.1.1.1.4.3.2.1" stretchy="false" xref="S5.E4.m1.1.1.1.4.cmml">(</mo><mi id="S5.E4.m1.1.1.1.1" xref="S5.E4.m1.1.1.1.1.cmml">i</mi><mo id="S5.E4.m1.1.1.1.4.3.2.2" stretchy="false" xref="S5.E4.m1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow></munder><mrow id="S5.E4.m1.7.7.2.1" xref="S5.E4.m1.7.7.2.1.cmml"><mi id="S5.E4.m1.7.7.2.1.3" xref="S5.E4.m1.7.7.2.1.3.cmml">p</mi><mo id="S5.E4.m1.7.7.2.1.2" xref="S5.E4.m1.7.7.2.1.2.cmml">⁢</mo><mrow id="S5.E4.m1.7.7.2.1.1.1" xref="S5.E4.m1.7.7.2.1.1.1.1.cmml"><mo id="S5.E4.m1.7.7.2.1.1.1.2" stretchy="false" xref="S5.E4.m1.7.7.2.1.1.1.1.cmml">(</mo><mrow id="S5.E4.m1.7.7.2.1.1.1.1" xref="S5.E4.m1.7.7.2.1.1.1.1.cmml"><mi id="S5.E4.m1.7.7.2.1.1.1.1.2" xref="S5.E4.m1.7.7.2.1.1.1.1.2.cmml">y</mi><mo id="S5.E4.m1.7.7.2.1.1.1.1.1" xref="S5.E4.m1.7.7.2.1.1.1.1.1.cmml">=</mo><mrow id="S5.E4.m1.7.7.2.1.1.1.1.3" xref="S5.E4.m1.7.7.2.1.1.1.1.3.cmml"><mi id="S5.E4.m1.7.7.2.1.1.1.1.3.2" xref="S5.E4.m1.7.7.2.1.1.1.1.3.2.cmml">i</mi><mo fence="false" id="S5.E4.m1.7.7.2.1.1.1.1.3.1" xref="S5.E4.m1.7.7.2.1.1.1.1.3.1.cmml">|</mo><mi id="S5.E4.m1.7.7.2.1.1.1.1.3.3" xref="S5.E4.m1.7.7.2.1.1.1.1.3.3.cmml">𝒙</mi></mrow></mrow><mo id="S5.E4.m1.7.7.2.1.1.1.3" stretchy="false" xref="S5.E4.m1.7.7.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E4.m1.7b"><apply id="S5.E4.m1.7.7.cmml" xref="S5.E4.m1.7.7"><eq id="S5.E4.m1.7.7.3.cmml" xref="S5.E4.m1.7.7.3"></eq><apply id="S5.E4.m1.6.6.1.cmml" xref="S5.E4.m1.6.6.1"><times id="S5.E4.m1.6.6.1.2.cmml" xref="S5.E4.m1.6.6.1.2"></times><ci id="S5.E4.m1.6.6.1.3.cmml" xref="S5.E4.m1.6.6.1.3">𝑝</ci><apply id="S5.E4.m1.6.6.1.1.1.1.cmml" xref="S5.E4.m1.6.6.1.1.1"><in id="S5.E4.m1.6.6.1.1.1.1.1.cmml" xref="S5.E4.m1.6.6.1.1.1.1.1"></in><ci id="S5.E4.m1.6.6.1.1.1.1.2.cmml" xref="S5.E4.m1.6.6.1.1.1.1.2">𝑡</ci><apply id="S5.E4.m1.6.6.1.1.1.1.3.cmml" xref="S5.E4.m1.6.6.1.1.1.1.3"><csymbol cd="latexml" id="S5.E4.m1.6.6.1.1.1.1.3.1.cmml" xref="S5.E4.m1.6.6.1.1.1.1.3.1">conditional</csymbol><set id="S5.E4.m1.6.6.1.1.1.1.3.2.1.cmml" xref="S5.E4.m1.6.6.1.1.1.1.3.2.2"><ci id="S5.E4.m1.2.2a.cmml" xref="S5.E4.m1.2.2"><mtext id="S5.E4.m1.2.2.cmml" xref="S5.E4.m1.2.2">𝙰</mtext></ci><ci id="S5.E4.m1.3.3a.cmml" xref="S5.E4.m1.3.3"><mtext id="S5.E4.m1.3.3.cmml" xref="S5.E4.m1.3.3">𝙱</mtext></ci><ci id="S5.E4.m1.4.4a.cmml" xref="S5.E4.m1.4.4"><mtext id="S5.E4.m1.4.4.cmml" xref="S5.E4.m1.4.4">𝙲</mtext></ci><ci id="S5.E4.m1.5.5a.cmml" xref="S5.E4.m1.5.5"><mtext id="S5.E4.m1.5.5.cmml" xref="S5.E4.m1.5.5">𝙳</mtext></ci></set><ci id="S5.E4.m1.6.6.1.1.1.1.3.3.cmml" xref="S5.E4.m1.6.6.1.1.1.1.3.3">𝒙</ci></apply></apply></apply><apply id="S5.E4.m1.7.7.2.cmml" xref="S5.E4.m1.7.7.2"><apply id="S5.E4.m1.7.7.2.2.cmml" xref="S5.E4.m1.7.7.2.2"><csymbol cd="ambiguous" id="S5.E4.m1.7.7.2.2.1.cmml" xref="S5.E4.m1.7.7.2.2">subscript</csymbol><sum id="S5.E4.m1.7.7.2.2.2.cmml" xref="S5.E4.m1.7.7.2.2.2"></sum><apply id="S5.E4.m1.1.1.1.cmml" xref="S5.E4.m1.1.1.1"><in id="S5.E4.m1.1.1.1.2.cmml" xref="S5.E4.m1.1.1.1.2"></in><ci id="S5.E4.m1.1.1.1.3.cmml" xref="S5.E4.m1.1.1.1.3">𝑡</ci><apply id="S5.E4.m1.1.1.1.4.cmml" xref="S5.E4.m1.1.1.1.4"><times id="S5.E4.m1.1.1.1.4.1.cmml" xref="S5.E4.m1.1.1.1.4.1"></times><ci id="S5.E4.m1.1.1.1.4.2.cmml" xref="S5.E4.m1.1.1.1.4.2">𝒱</ci><ci id="S5.E4.m1.1.1.1.1.cmml" xref="S5.E4.m1.1.1.1.1">𝑖</ci></apply></apply></apply><apply id="S5.E4.m1.7.7.2.1.cmml" xref="S5.E4.m1.7.7.2.1"><times id="S5.E4.m1.7.7.2.1.2.cmml" xref="S5.E4.m1.7.7.2.1.2"></times><ci id="S5.E4.m1.7.7.2.1.3.cmml" xref="S5.E4.m1.7.7.2.1.3">𝑝</ci><apply id="S5.E4.m1.7.7.2.1.1.1.1.cmml" xref="S5.E4.m1.7.7.2.1.1.1"><eq id="S5.E4.m1.7.7.2.1.1.1.1.1.cmml" xref="S5.E4.m1.7.7.2.1.1.1.1.1"></eq><ci id="S5.E4.m1.7.7.2.1.1.1.1.2.cmml" xref="S5.E4.m1.7.7.2.1.1.1.1.2">𝑦</ci><apply id="S5.E4.m1.7.7.2.1.1.1.1.3.cmml" xref="S5.E4.m1.7.7.2.1.1.1.1.3"><csymbol cd="latexml" id="S5.E4.m1.7.7.2.1.1.1.1.3.1.cmml" xref="S5.E4.m1.7.7.2.1.1.1.1.3.1">conditional</csymbol><ci id="S5.E4.m1.7.7.2.1.1.1.1.3.2.cmml" xref="S5.E4.m1.7.7.2.1.1.1.1.3.2">𝑖</ci><ci id="S5.E4.m1.7.7.2.1.1.1.1.3.3.cmml" xref="S5.E4.m1.7.7.2.1.1.1.1.3.3">𝒙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E4.m1.7c">p(t\in\{\texttt{A},\texttt{B},\texttt{C},\texttt{D}\}|\bm{x})=\sum_{t\in%
\mathcal{V}(i)}p(y=i|\bm{x})</annotation><annotation encoding="application/x-llamapun" id="S5.E4.m1.7d">italic_p ( italic_t ∈ { A , B , C , D } | bold_italic_x ) = ∑ start_POSTSUBSCRIPT italic_t ∈ caligraphic_V ( italic_i ) end_POSTSUBSCRIPT italic_p ( italic_y = italic_i | bold_italic_x )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS2.p2.2">The label with the max probability is taken as the final prediction.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Next, we will elaborate on our results and analysis in the following two subsections, illustrating the comparisons to the original LLaMA and other models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Comparisons to Original LLaMA</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.F3" title="Figure 3 ‣ 5.3 Comparisons to Original LLaMA ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrates how our models evolve based on the original LLaMA.
Detailed results are depicted in Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.T8" title="Table 8 ‣ 5.3 Comparisons to Original LLaMA ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">8</span></a>.
We mainly describe our findings in the following aspects.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S5.F3.g1" src="x2.png" width="706">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> <span class="ltx_text ltx_font_bold" id="S5.F3.2.1">Results on C-Eval valid set.</span> The results are grouped by different settings (zero-shot and 5-shot) and model sizes (7B and 13B). </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T8">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span> <span class="ltx_text ltx_font_bold" id="S5.T8.2.1">Results on C-Eval valid and test sets</span>. All prediction files are generated by ourselves. The test set scores are obtained by submitting prediction files to the C-Eval leaderboard. </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T8.3">
<tbody><tr class="ltx_tr" id="S5.T8.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T8.3.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T8.3.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T8.3.1.2"><span class="ltx_text ltx_font_bold" id="S5.T8.3.1.2.1">Valid Set</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T8.3.1.3"><span class="ltx_text ltx_font_bold" id="S5.T8.3.1.3.1">Test Set</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.2">
<td class="ltx_td ltx_align_center" id="S5.T8.3.2.1"><span class="ltx_text ltx_font_bold" id="S5.T8.3.2.1.1">Zero-shot</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.2.2"><span class="ltx_text ltx_font_bold" id="S5.T8.3.2.2.1">5-shot</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.2.3"><span class="ltx_text ltx_font_bold" id="S5.T8.3.2.3.1">Zero-shot</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.2.4"><span class="ltx_text ltx_font_bold" id="S5.T8.3.2.4.1">5-shot</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T8.3.3.1"><span class="ltx_text ltx_font_italic" id="S5.T8.3.3.1.1">Random</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.3.2"><span class="ltx_text ltx_font_italic" id="S5.T8.3.3.2.1">25.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.3.3"><span class="ltx_text ltx_font_italic" id="S5.T8.3.3.3.1">25.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.3.4"><span class="ltx_text ltx_font_italic" id="S5.T8.3.3.4.1">25.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.3.5"><span class="ltx_text ltx_font_italic" id="S5.T8.3.3.5.1">25.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T8.3.4.1">LLaMA-65B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.4.2">37.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.4.3">41.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.4.4">33.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.4.5">38.8</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.5">
<td class="ltx_td ltx_align_left" id="S5.T8.3.5.1">LLaMA-33B</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.5.2">34.5</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.5.3">37.9</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.5.4">32.4</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.5.5">36.0</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.6">
<td class="ltx_td ltx_align_left" id="S5.T8.3.6.1">LLaMA-13B</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.6.2">27.8</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.6.3">30.9</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.6.4">28.5</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.6.5">29.6</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.7">
<td class="ltx_td ltx_align_left" id="S5.T8.3.7.1">LLaMA-7B</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.7.2">25.6</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.7.3">25.3</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.7.4">26.7</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.7.5">27.8</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T8.3.8.1">Chinese-LLaMA-33B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.8.2">34.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.8.3">38.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.8.4">34.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.8.5">39.5</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.9">
<td class="ltx_td ltx_align_left" id="S5.T8.3.9.1">Chinese-LLaMA-Plus-13B</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.9.2">27.3</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.9.3">34.0</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.9.4">27.8</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.9.5">33.3</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.10">
<td class="ltx_td ltx_align_left" id="S5.T8.3.10.1">Chinese-LLaMA-13B</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.10.2">29.4</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.10.3">35.0</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.10.4">29.2</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.10.5">33.7</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.11">
<td class="ltx_td ltx_align_left" id="S5.T8.3.11.1">Chinese-LLaMA-Plus-7B</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.11.2">27.3</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.11.3">28.3</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.11.4">26.8</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.11.5">28.4</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.12">
<td class="ltx_td ltx_align_left" id="S5.T8.3.12.1">Chinese-LLaMA-7B</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.12.2">26.2</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.12.3">26.2</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.12.4">27.1</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.12.5">27.2</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T8.3.13.1">Chinese-Alpaca-33B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.13.2">43.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.13.3">42.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.13.4">41.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.3.13.5">40.4</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.14">
<td class="ltx_td ltx_align_left" id="S5.T8.3.14.1">Chinese-Alpaca-Plus-13B</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.14.2">43.3</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.14.3">42.4</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.14.4">41.5</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.14.5">39.9</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.15">
<td class="ltx_td ltx_align_left" id="S5.T8.3.15.1">Chinese-Alpaca-13B</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.15.2">37.1</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.15.3">36.3</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.15.4">36.7</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.15.5">34.5</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.16">
<td class="ltx_td ltx_align_left" id="S5.T8.3.16.1">Chinese-Alpaca-Plus-7B</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.16.2">36.7</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.16.3">32.9</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.16.4">36.4</td>
<td class="ltx_td ltx_align_center" id="S5.T8.3.16.5">32.3</td>
</tr>
<tr class="ltx_tr" id="S5.T8.3.17">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T8.3.17.1">Chinese-Alpaca-7B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.3.17.2">30.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.3.17.3">32.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.3.17.4">30.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.3.17.5">29.2</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Chinese LLaMA improves original LLaMA.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p1.1">We can see that the proposed Chinese LLaMA models yield moderate improvements over the original LLaMA, which demonstrates that the pre-training on Chinese data has some positive effect on C-Eval but not always.
When we compare Chinese LLaMA and LLaMA-Plus, the latter does not show significant improvements over the former one, even showing inferior results for 13B setting.
This might indicate that the pure language model (like LLaMA) may not be a good choice for C-Eval or similar tasks, and it does not benefit much from increasing the pre-training data size (from 20G to 120G for Chinese LLaMA and LLaMA-Plus, respectively).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Alpaca models show significant improvements over LLaMA.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p1.1">Among different settings, such as zero-shot or 5-shot, the Alpaca model series show significant improvements over LLaMA counterparts, demonstrating that the instruction-following models are more capable of handling these NLU-like tasks than pure language models.
Unlike the phenomenon observed in the LLaMA series, we can see that Alpaca-Plus models yield significant improvement over basic Alpaca models.
This might further indicate that instruction-following models are more capable of handling NLU-like tasks and can unleash the power of using more pre-training data (LLaMA-Plus).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">LLaMA generally yields better performance in a few-shot setting, while Alpaca prefers zero-shot.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px3.p1.1">Generally speaking, LLaMA with 5-shot setting shows better performance than zero-shot setting, while Alpaca with zero-shot setting is much better than 5-shot one.
As LLaMA is not designed for instruction-following, few-shot setting might give valuable information on how to follow the question answering structure in C-Eval.
However, on the contrary, as Alpaca has already been trained with millions of instruction data, it is less likely to benefit from additional shots.
Also, the official 5-shot setting uses identical prompts for all samples, making it some distraction for Alpaca models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.SSS0.Px3.p2">
<p class="ltx_p" id="S5.SS3.SSS0.Px3.p2.1">We would like to emphasize that these observations are solely based on the results of the C-Eval dataset, and whether it is generalizable to other datasets requires further investigation.
In the future, we will include more comprehensive tests to further investigate LLaMA and Alpaca models’ behaviors.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Comparisons to Other Models</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We include our two best-performing models, i.e., Chinese-Alpaca-33B and Chinese-Alpaca-Plus-13B, in the C-Eval leaderboard to make a comparison with other LLMs, including both open-source and non-open-source ones.
The test results on the C-Eval leaderboard (as of June 9, 2023) are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S5.T9" title="Table 9 ‣ 5.4 Comparisons to Other Models ‣ 5 Results on Natural Language Understanding Tasks ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">9</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S5.T9">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span> <span class="ltx_text ltx_font_bold" id="S5.T9.6.1">Test results on C-Eval leaderboard (as of June 9, 2023), ordered by average scores.</span> Model name with boldface represents our submissions, while the other results are evaluated by C-Eval officials. We re-evaluated two models marked with <math alttext="{\dagger}" class="ltx_Math" display="inline" id="S5.T9.2.m1.1"><semantics id="S5.T9.2.m1.1b"><mo id="S5.T9.2.m1.1.1" xref="S5.T9.2.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S5.T9.2.m1.1c"><ci id="S5.T9.2.m1.1.1.cmml" xref="S5.T9.2.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.2.m1.1d">{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T9.2.m1.1e">†</annotation></semantics></math> (these scores are not shown publicly) based on our own inference script and achieved significantly better performance than those evaluated by C-Eval. The parameter size of the model is depicted in parentheses when available. Open: open-source. Avg-H: Average (Hard). </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T9.4">
<tbody><tr class="ltx_tr" id="S5.T9.4.3">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T9.4.3.1"><span class="ltx_text ltx_font_bold" id="S5.T9.4.3.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T9.4.3.2"><span class="ltx_text ltx_font_bold" id="S5.T9.4.3.2.1">N-Shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T9.4.3.3"><span class="ltx_text ltx_font_bold" id="S5.T9.4.3.3.1">Open</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T9.4.3.4"><span class="ltx_text ltx_font_bold" id="S5.T9.4.3.4.1">Avg</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T9.4.3.5"><span class="ltx_text ltx_font_bold" id="S5.T9.4.3.5.1">Avg-H</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T9.4.3.6"><span class="ltx_text ltx_font_bold" id="S5.T9.4.3.6.1">STEM</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T9.4.3.7"><span class="ltx_text ltx_font_bold" id="S5.T9.4.3.7.1">Social</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T9.4.3.8"><span class="ltx_text ltx_font_bold" id="S5.T9.4.3.8.1">Human</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T9.4.3.9"><span class="ltx_text ltx_font_bold" id="S5.T9.4.3.9.1">Others</span></td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T9.4.4.1">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.4.4.2">5-shot</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.4.4.3"><span class="ltx_text" id="S5.T9.4.4.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.4.4.4">68.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.4.4.5">54.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.4.4.6">67.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.4.4.7">77.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.4.4.8">64.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T9.4.4.9">67.8</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.5">
<td class="ltx_td ltx_align_left" id="S5.T9.4.5.1">InternLM (104B)</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.5.2">few-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.5.3"><span class="ltx_text" id="S5.T9.4.5.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.5.4">62.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.5.5">46.0</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.5.6">58.1</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.5.7">76.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.5.8">64.6</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.5.9">56.4</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.6">
<td class="ltx_td ltx_align_left" id="S5.T9.4.6.1">ChatGPT</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.6.2">5-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.6.3"><span class="ltx_text" id="S5.T9.4.6.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.6.4">54.4</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.6.5">41.4</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.6.6">52.9</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.6.7">61.8</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.6.8">50.9</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.6.9">53.6</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.7">
<td class="ltx_td ltx_align_left" id="S5.T9.4.7.1">Claude-v1.3</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.7.2">5-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.7.3"><span class="ltx_text" id="S5.T9.4.7.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.7.4">54.2</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.7.5">39.0</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.7.6">51.9</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.7.7">61.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.7.8">52.1</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.7.9">53.7</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.8">
<td class="ltx_td ltx_align_left" id="S5.T9.4.8.1">Claude-instant-v1.0</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.8.2">5-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.8.3"><span class="ltx_text" id="S5.T9.4.8.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.8.4">45.9</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.8.5">35.5</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.8.6">43.1</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.8.7">53.8</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.8.8">44.2</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.8.9">45.4</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.9">
<td class="ltx_td ltx_align_left" id="S5.T9.4.9.1">Bloomz-mt (176B)</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.9.2">0-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.9.3"><span class="ltx_text" id="S5.T9.4.9.3.1" style="color:#0000FF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.9.4">44.3</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.9.5">30.8</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.9.6">39.0</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.9.7">53.0</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.9.8">47.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.9.9">42.7</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.10">
<td class="ltx_td ltx_align_left" id="S5.T9.4.10.1">GLM-130B</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.10.2">0-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.10.3"><span class="ltx_text" id="S5.T9.4.10.3.1" style="color:#0000FF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.10.4">44.0</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.10.5">30.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.10.6">36.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.10.7">55.8</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.10.8">47.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.10.9">43.0</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.11">
<td class="ltx_td ltx_align_left" id="S5.T9.4.11.1"><span class="ltx_text ltx_font_bold" id="S5.T9.4.11.1.1">Chinese-Alpaca-33B</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.11.2">0-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.11.3"><span class="ltx_text" id="S5.T9.4.11.3.1" style="color:#0000FF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.11.4">41.6</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.11.5">30.3</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.11.6">37.0</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.11.7">51.6</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.11.8">42.3</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.11.9">40.3</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.12">
<td class="ltx_td ltx_align_left" id="S5.T9.4.12.1"><span class="ltx_text ltx_font_bold" id="S5.T9.4.12.1.1">Chinese-Alpaca-Plus-13B</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.12.2">0-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.12.3"><span class="ltx_text" id="S5.T9.4.12.3.1" style="color:#0000FF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.12.4">41.5</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.12.5">30.5</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.12.6">36.6</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.12.7">49.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.12.8">43.1</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.12.9">41.2</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.13">
<td class="ltx_td ltx_align_left" id="S5.T9.4.13.1">CubeLM (13B)</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.13.2">few-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.13.3"><span class="ltx_text" id="S5.T9.4.13.3.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.13.4">40.2</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.13.5">27.3</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.13.6">34.1</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.13.7">49.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.13.8">43.4</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.13.9">39.6</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.14">
<td class="ltx_td ltx_align_left" id="S5.T9.4.14.1">ChatGLM-6B</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.14.2">0-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.14.3"><span class="ltx_text" id="S5.T9.4.14.3.1" style="color:#0000FF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.14.4">38.9</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.14.5">29.2</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.14.6">33.3</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.14.7">48.3</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.14.8">41.3</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.14.9">38.0</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.15">
<td class="ltx_td ltx_align_left" id="S5.T9.4.15.1">LLaMA-65B</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.15.2">5-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.15.3"><span class="ltx_text" id="S5.T9.4.15.3.1" style="color:#0000FF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.15.4">38.8</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.15.5">31.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.15.6">37.8</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.15.7">45.6</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.15.8">36.1</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.15.9">37.1</td>
</tr>
<tr class="ltx_tr" id="S5.T9.3.1">
<td class="ltx_td ltx_align_left" id="S5.T9.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T9.3.1.1.1">Chinese-Alpaca-13B<math alttext="{\dagger}" class="ltx_Math" display="inline" id="S5.T9.3.1.1.1.m1.1"><semantics id="S5.T9.3.1.1.1.m1.1a"><mo id="S5.T9.3.1.1.1.m1.1.1" mathvariant="normal" xref="S5.T9.3.1.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S5.T9.3.1.1.1.m1.1b"><ci id="S5.T9.3.1.1.1.m1.1.1.cmml" xref="S5.T9.3.1.1.1.m1.1.1">normal-†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.3.1.1.1.m1.1c">{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T9.3.1.1.1.m1.1d">†</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.1.2">0-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.1.3"><span class="ltx_text" id="S5.T9.3.1.3.1" style="color:#0000FF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.1.4">36.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.1.5">28.4</td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.1.6">33.1</td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.1.7">43.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.1.8">38.4</td>
<td class="ltx_td ltx_align_center" id="S5.T9.3.1.9">35.0</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.2">
<td class="ltx_td ltx_align_left" id="S5.T9.4.2.1"><span class="ltx_text ltx_font_bold" id="S5.T9.4.2.1.1">Chinese-LLaMA-13B<math alttext="{\dagger}" class="ltx_Math" display="inline" id="S5.T9.4.2.1.1.m1.1"><semantics id="S5.T9.4.2.1.1.m1.1a"><mo id="S5.T9.4.2.1.1.m1.1.1" mathvariant="normal" xref="S5.T9.4.2.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S5.T9.4.2.1.1.m1.1b"><ci id="S5.T9.4.2.1.1.m1.1.1.cmml" xref="S5.T9.4.2.1.1.m1.1.1">normal-†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T9.4.2.1.1.m1.1c">{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T9.4.2.1.1.m1.1d">†</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.2.2">5-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.2.3"><span class="ltx_text" id="S5.T9.4.2.3.1" style="color:#0000FF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.2.4">33.7</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.2.5">28.1</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.2.6">31.9</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.2.7">38.6</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.2.8">33.5</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.2.9">32.8</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.16">
<td class="ltx_td ltx_align_left" id="S5.T9.4.16.1">Chinese-LLaMA-13B</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.16.2">5-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.16.3"><span class="ltx_text" id="S5.T9.4.16.3.1" style="color:#0000FF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.16.4">33.3</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.16.5">27.3</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.16.6">31.6</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.16.7">37.2</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.16.8">33.6</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.16.9">32.8</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.17">
<td class="ltx_td ltx_align_left" id="S5.T9.4.17.1">MOSS (16B)</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.17.2">0-shot</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.17.3"><span class="ltx_text" id="S5.T9.4.17.3.1" style="color:#0000FF;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.17.4">33.1</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.17.5">28.4</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.17.6">31.6</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.17.7">37.0</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.17.8">33.4</td>
<td class="ltx_td ltx_align_center" id="S5.T9.4.17.9">32.1</td>
</tr>
<tr class="ltx_tr" id="S5.T9.4.18">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T9.4.18.1">Chinese-Alpaca-13B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T9.4.18.2">0-shot</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T9.4.18.3"><span class="ltx_text" id="S5.T9.4.18.3.1" style="color:#0000FF;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T9.4.18.4">30.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T9.4.18.5">24.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T9.4.18.6">27.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T9.4.18.7">39.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T9.4.18.8">32.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T9.4.18.9">28.0</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Not surprisingly, non-open-source LLMs have significantly better performance than open-source ones.
When it comes to our models, we can see that both Chinese-Alpaca-33B and Chinese-Alpaca-Plus-13B yield competitive performance among open-source LLMs in this leaderboard, showing only a moderate gap to Bloomz-mt-176B <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib23" title="">2022</a>)</cite> and GLM-130B <cite class="ltx_cite ltx_citemacro_citep">(Zeng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib34" title="">2023</a>)</cite>, considering that the latter ones have several times of magnitude and trained with way more data than ours.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">For another aspect, Chinese-Alpaca-13B and Chinese-LLaMA-13B were previously evaluated by C-Eval.
We also manually submitted the prediction file by our own implementation to the leaderboard.
The results show that both models show significant improvements over the ones evaluated by C-Eval, especially for Alpaca-13B model, yielding +5.8 average score (from 30.9 to 36.7).
Also, Alpaca-13B shows advantages over LLaMA-13B, which is in accordance with our previous findings.
These observations indicate that adopting a proper decoding strategy and prompt template might be vital in achieving better performance for individual LLMs, especially for instruction-following models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Effect of Different Quantization Methods</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Deploying large language models on personal computers, particularly on CPUs, has historically been challenging due to their immense computational requirements.
However, with the help of many community efforts, such as <span class="ltx_text ltx_font_typewriter" id="S6.p1.1.1">llama.cpp</span> <cite class="ltx_cite ltx_citemacro_citep">(Gerganov, <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib9" title="">2023</a>)</cite>, users can efficiently quantize LLMs, significantly reducing memory usage and computational demands, making it easier to deploy LLMs on personal computers.
This also enables quicker interactions with the models and facilitates local data processing.
Quantizing LLMs and deploying them on personal computers offer several benefits. Firstly, it helps users protect their data privacy by ensuring that sensitive information remains within their local environment rather than being transmitted to external servers. Secondly, it democratizes access to LLMs by making them more accessible to users with limited computational resources. Lastly, it promotes the development of new applications and research directions that take advantage of local LLM deployments. Overall, the ability to deploy LLMs on personal computers using <span class="ltx_text ltx_font_typewriter" id="S6.p1.1.2">llama.cpp</span> (or similar) paves the way for a more versatile and privacy-conscious utilization of LLMs in various domains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In this section, we investigate the effect of different quantization methods.
We use <span class="ltx_text ltx_font_typewriter" id="S6.p2.1.1">llama.cpp</span> to quantize Alpaca-Plus-7B, Alpaca-Plus-13B, and Alpaca-33B and calculate the perplexity on Chinese text corpora.
We quantize these models into 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit forms to compare with the original FP16 one.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Specifically, we use q2_K, q3_K, q4_0, q5_0, q6_K, and q8_0 quantization option for each quantized model.</span></span></span>
The results are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#S6.F4" title="Figure 4 ‣ 6 Effect of Different Quantization Methods ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">4</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="419" id="S6.F4.g1" src="x3.png" width="664">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> <span class="ltx_text ltx_font_bold" id="S6.F4.2.1">Perplexities for different quantization methods.</span> Note that 33B model has a higher PPL as it is trained on less data than the others. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">The quantization level is strictly bound to the memory usage and inference speed, and thus a tradeoff must be made when choosing a proper quantization level.
As we can see, the 8-bit quantization method has almost the same or even lower perplexities compared to the original FP16 model, demonstrating that it is a good choice for deploying LLMs on personal computers, with only half size of the FP16 one.
The 6-bit models also achieve decent PPLs comparable to the 8-bit one, making it a better balance of speed and performance.
When we use a more aggressive quantization level, the performance drastically decreases (i.e., higher PPL), especially for 3-bit and 2-bit.
We also discover that larger models are less sensitive to quantization methods than smaller ones.
For example, the performance of 33B models changes much more mildly than the others.
A similar result is also observed when comparing Plus-7B and Plus-13B models.
This might indicate that though 2-bit and 3-bit quantization are less effective for smaller models, it might be a promising way to deploy larger models without significant performance loss.
This is extremely helpful when the users only have limited computing resources and still want to try large language models.
This might also imply that the quantized training method may become a main-stream approach for training large language models, especially for those with limited training resources.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this technical report, we have presented an approach to enhance the Chinese understanding and generation capabilities of the LLaMA model. Acknowledging the limitations of the original LLaMA’s Chinese vocabulary, we expanded it by incorporating 20K additional Chinese tokens, significantly increasing its encoding efficiency for the Chinese language. Building on the Chinese LLaMA, we employed supervised fine-tuning with instruction data, resulting in Chinese Alpaca models exhibiting improved instruction-following capabilities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">To evaluate our models effectively, we annotated 200 samples across ten distinct task types and utilized GPT-4 for evaluation. Our experiments demonstrated that the proposed models significantly outperformed the original LLaMA in Chinese understanding and generation tasks. We also tested our models on C-Eval datasets. The results show that the proposed model could achieve significant improvements and show competitive performance to the models with several times bigger sizes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Looking ahead, we plan to explore Reinforcement Learning from Human Feedback (RLHF) or Reinforcement Learning from AI Instructed Feedback (RLAIF) to further align the models’ output with human preferences. Moreover, we intend to adopt more advanced and effective quantization methods, such as GPTQ <cite class="ltx_cite ltx_citemacro_citep">(Frantar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib8" title="">2022</a>)</cite>, among others. Additionally, we aim to investigate alternative methods to LoRA for more efficient and effective pre-training and fine-tuning of large language models, ultimately enhancing their performance and applicability across various tasks within the Chinese NLP community.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">While this project has successfully enhanced the Chinese understanding and generation capabilities of the LLaMA and Alpaca models, several limitations must be acknowledged:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="Sx1.I1">
<li class="ltx_item" id="Sx1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i1.p1.1">Harmful and unpredictable content: Though our model can reject unethical queries, these models may still generate harmful or misaligned with human preferences and values. This issue may arise from biases in the training data or the models’ inability to discern appropriate outputs in certain contexts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i2.p1.1">Insufficient training: Due to constraints in computing power and data availability, the training of the models may not be sufficient for optimal performance. As a result, there is still room for improvement in the Chinese understanding capabilities of the models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i3.p1.1">Lack of robustness: The models may exhibit brittleness in some situations, producing inconsistent or nonsensical outputs when faced with adversarial inputs or rare language phenomena.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i4.p1">
<p class="ltx_p" id="Sx1.I1.i4.p1.1">Comprehensive evaluation: Evaluating large language models is an important topic in the current era. While we have seen many evaluation benchmarks for LLMs, their comprehensiveness and appropriateness for LLMs should be well-studied and examined. A more diverse and comprehensive LLM evaluation dataset and benchmark will have a great positive effect on shaping the future of LLM research.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="Sx1.I1.i5.p1">
<p class="ltx_p" id="Sx1.I1.i5.p1.1">Scalability and efficiency: Although we applied LoRA and quantization to make the model more accessible to a broader community, when combined with the original LLaMA, the models’ large size and complexity can lead to difficulties in deployment, especially for users with limited computational resources. This issue may hinder the accessibility and widespread adoption of the models in various applications.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Future work should address these limitations to further enhance the models’ capabilities, making them more robust, accessible, and effective for a broader range of applications in the Chinese NLP community.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The original draft was polished by OpenAI GPT-4 for grammatical corrections and clarity improvements.
We would like to thank our community members for their contributions to our open-source projects.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.

</span>
<span class="ltx_bibblock">Longbench: A bilingual, multitask benchmark for long context understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2308.14508</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.

</span>
<span class="ltx_bibblock">Extending context window of large language models via positional interpolation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2306.15595</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu.

</span>
<span class="ltx_bibblock">Revisiting pre-trained models for Chinese natural language processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</em>, pp.&nbsp; 657–668, Online, November 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/2020.findings-emnlp.58" title="">https://www.aclweb.org/anthology/2020.findings-emnlp.58</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang.

</span>
<span class="ltx_bibblock">Pre-training with whole word masking for chinese bert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 29:3504–3514, 2021.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1109/TASLP.2021.3124365" title="">10.1109/TASLP.2021.3124365</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu.

</span>
<span class="ltx_bibblock">Lert: A linguistically-motivated pre-trained language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2211.05344</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2305.14314</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pp.&nbsp; 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/N19-1423" title="">https://www.aclweb.org/anthology/N19-1423</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frantar et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">GPTQ: Accurate post-training compression for generative pretrained transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2210.17323</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gerganov (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Georgi Gerganov.

</span>
<span class="ltx_bibblock">llama.cpp.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ggerganov/llama.cpp" title="">https://github.com/ggerganov/llama.cpp</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edward&nbsp;J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu&nbsp;Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">LoRA: Low-Rank Adaptation of Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv e-prints</em>, art. arXiv:2106.09685, June 2021.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.48550/arXiv.2106.09685" title="">10.48550/arXiv.2106.09685</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He.

</span>
<span class="ltx_bibblock">C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2305.08322</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpf et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen&nbsp;Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick.

</span>
<span class="ltx_bibblock">OpenAssistant Conversations – Democratizing Large Language Model Alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv e-prints</em>, art. arXiv:2304.07327, April 2023.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.48550/arXiv.2304.07327" title="">10.48550/arXiv.2304.07327</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo &amp; Richardson (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pp.&nbsp; 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/D18-2012" title="">10.18653/v1/D18-2012</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D18-2012" title="">https://aclanthology.org/D18-2012</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.

</span>
<span class="ltx_bibblock">RACE: Large-scale ReAding comprehension dataset from examinations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, pp.&nbsp; 785–794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/D17-1082" title="">10.18653/v1/D17-1082</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D17-1082" title="">https://aclanthology.org/D17-1082</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin.

</span>
<span class="ltx_bibblock">Cmmlu: Measuring massive multitask language understanding in chinese, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">International Conference on Learning Representations</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">https://openreview.net/forum?id=Bkg6RiCqY7</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing chatgpt.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv e-prints</em>, art. arXiv:2303.08774, March 2023.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.48550/arXiv.2303.08774" title="">10.48550/arXiv.2303.08774</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv e-prints</em>, art. arXiv:2203.02155, March 2022.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.48550/arXiv.2203.02155" title="">10.48550/arXiv.2203.02155</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.

</span>
<span class="ltx_bibblock">Yarn: Efficient context window extension of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2309.00071</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasley et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, pp.&nbsp; 3505–3506, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni, François Yvon, Matthias Gallé, et&nbsp;al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2211.05100</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">Glu variants improve transformer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N Gomez, Ł&nbsp;ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;Von Luxburg, S.&nbsp;Bengio, H.&nbsp;Wallach, R.&nbsp;Fergus, S.&nbsp;Vishwanathan, and R.&nbsp;Garnett (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in Neural Information Processing Systems</em>, volume&nbsp;30. Curran Associates, Inc., 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah&nbsp;A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Self-Instruct: Aligning Language Model with Self Generated Instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv e-prints</em>, art. arXiv:2212.10560, December 2022.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.48550/arXiv.2212.10560" title="">10.48550/arXiv.2212.10560</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bright Xu.

</span>
<span class="ltx_bibblock">Nlp chinese corpus: Large scale chinese corpus for nlp, September 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5281/zenodo.3402023" title="">https://doi.org/10.5281/zenodo.3402023</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ziqing Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, and Zhigang Chen.

</span>
<span class="ltx_bibblock">CINO: A Chinese minority pre-trained language model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 29th International Conference on Computational Linguistics</em>, pp.&nbsp; 3937–3949, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.coling-1.346" title="">https://aclanthology.org/2022.coling-1.346</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng&nbsp;Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang.

</span>
<span class="ltx_bibblock">GLM-130b: An open bilingual pre-trained model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">The Eleventh International Conference on Learning Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=-Aw0rrrPUF" title="">https://openreview.net/forum?id=-Aw0rrrPUF</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Sennrich (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root Mean Square Layer Normalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in Neural Information Processing Systems 32</em>, Vancouver, Canada, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/references/pdf?id=S1qBAf6rr" title="">https://openreview.net/references/pdf?id=S1qBAf6rr</a>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We present the baseline results on Chinese-LLaMA-2 and Chinese-Alpaca-2 as follows.
Most of the settings are identical to those in Chinese-LLaMA.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>C-Eval</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">The results on C-Eval <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib11" title="">2023</a>)</cite> are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1.T10" title="Table 10 ‣ A.1 C-Eval ‣ Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">10</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T10">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span> <span class="ltx_text ltx_font_bold" id="A1.T10.2.1">Results on C-Eval valid and test sets</span>. </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T10.3">
<tbody><tr class="ltx_tr" id="A1.T10.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T10.3.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T10.3.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T10.3.1.2"><span class="ltx_text ltx_font_bold" id="A1.T10.3.1.2.1">Valid Set</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T10.3.1.3"><span class="ltx_text ltx_font_bold" id="A1.T10.3.1.3.1">Test Set</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.3.2">
<td class="ltx_td ltx_align_center" id="A1.T10.3.2.1"><span class="ltx_text ltx_font_bold" id="A1.T10.3.2.1.1">Zero-shot</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.3.2.2"><span class="ltx_text ltx_font_bold" id="A1.T10.3.2.2.1">5-shot</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.3.2.3"><span class="ltx_text ltx_font_bold" id="A1.T10.3.2.3.1">Zero-shot</span></td>
<td class="ltx_td ltx_align_center" id="A1.T10.3.2.4"><span class="ltx_text ltx_font_bold" id="A1.T10.3.2.4.1">5-shot</span></td>
</tr>
<tr class="ltx_tr" id="A1.T10.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T10.3.3.1">Chinese-LLaMA-2-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.3.3.2">28.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.3.3.3">36.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.3.3.4">30.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.3.3.5">34.2</td>
</tr>
<tr class="ltx_tr" id="A1.T10.3.4">
<td class="ltx_td ltx_align_left" id="A1.T10.3.4.1">Chinese-LLaMA-2-13B</td>
<td class="ltx_td ltx_align_center" id="A1.T10.3.4.2">40.6</td>
<td class="ltx_td ltx_align_center" id="A1.T10.3.4.3">42.7</td>
<td class="ltx_td ltx_align_center" id="A1.T10.3.4.4">38.0</td>
<td class="ltx_td ltx_align_center" id="A1.T10.3.4.5">41.6</td>
</tr>
<tr class="ltx_tr" id="A1.T10.3.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T10.3.5.1">Chinese-Alpaca-2-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.3.5.2">41.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.3.5.3">42.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.3.5.4">40.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T10.3.5.5">39.5</td>
</tr>
<tr class="ltx_tr" id="A1.T10.3.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T10.3.6.1">Chinese-Alpaca-2-13B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T10.3.6.2">44.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T10.3.6.3">45.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T10.3.6.4">42.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T10.3.6.5">44.0</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>CMMLU</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">The results on CMMLU <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib15" title="">2023</a>)</cite> are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1.T11" title="Table 11 ‣ A.2 CMMLU ‣ Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">11</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T11">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span> <span class="ltx_text ltx_font_bold" id="A1.T11.2.1">Results on CMMLU test sets</span>. </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T11.3">
<tbody><tr class="ltx_tr" id="A1.T11.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T11.3.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="A1.T11.3.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T11.3.1.2"><span class="ltx_text ltx_font_bold" id="A1.T11.3.1.2.1">Test Set</span></td>
</tr>
<tr class="ltx_tr" id="A1.T11.3.2">
<td class="ltx_td ltx_align_center" id="A1.T11.3.2.1"><span class="ltx_text ltx_font_bold" id="A1.T11.3.2.1.1">Zero-shot</span></td>
<td class="ltx_td ltx_align_center" id="A1.T11.3.2.2"><span class="ltx_text ltx_font_bold" id="A1.T11.3.2.2.1">Few-shot</span></td>
</tr>
<tr class="ltx_tr" id="A1.T11.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T11.3.3.1">Chinese-LLaMA-2-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.3.3.2">27.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.3.3.3">34.1</td>
</tr>
<tr class="ltx_tr" id="A1.T11.3.4">
<td class="ltx_td ltx_align_left" id="A1.T11.3.4.1">Chinese-LLaMA-2-13B</td>
<td class="ltx_td ltx_align_center" id="A1.T11.3.4.2">38.9</td>
<td class="ltx_td ltx_align_center" id="A1.T11.3.4.3">42.5</td>
</tr>
<tr class="ltx_tr" id="A1.T11.3.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T11.3.5.1">Chinese-Alpaca-2-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.3.5.2">40.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T11.3.5.3">41.8</td>
</tr>
<tr class="ltx_tr" id="A1.T11.3.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T11.3.6.1">Chinese-Alpaca-2-13B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T11.3.6.2">43.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T11.3.6.3">45.5</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>LongBench</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">The results on LongBench <cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib1" title="">2023</a>)</cite> are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#A1.T12" title="Table 12 ‣ A.3 LongBench ‣ Appendix A Appendix ‣ Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"><span class="ltx_text ltx_ref_tag">12</span></a>.
This benchmark is specifically designed to test the long context ability of LLMs.
We test the Chinese subsets of LongBench (including code tasks).
The models marked with <span class="ltx_text ltx_font_typewriter" id="A1.SS3.p1.1.1">16K</span> were finetuned using Positional Interpolation (PI) method <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib2" title="">2023</a>)</cite>, which supports 16K context.
The models marked with <span class="ltx_text ltx_font_typewriter" id="A1.SS3.p1.1.2">64K</span> were finetuned using YaRN method <cite class="ltx_cite ltx_citemacro_citep">(Peng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2304.08177v3#bib.bib20" title="">2023</a>)</cite>, which supports 64K context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T12">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span> <span class="ltx_text ltx_font_bold" id="A1.T12.2.1">Results on LongBench (Chinese + code tasks).</span> S-QA: Single-doc QA, M-QA: Multi-doc QA, Summ: Summarization, FS-Learn: Few-shot Learning, Code: Code Completion, Synthetic: Synthetic Tasks.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A1.T12.3">
<tbody><tr class="ltx_tr" id="A1.T12.3.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T12.3.1.1"><span class="ltx_text ltx_font_bold" id="A1.T12.3.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T12.3.1.2"><span class="ltx_text ltx_font_bold" id="A1.T12.3.1.2.1">S-QA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T12.3.1.3"><span class="ltx_text ltx_font_bold" id="A1.T12.3.1.3.1">M-QA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T12.3.1.4"><span class="ltx_text ltx_font_bold" id="A1.T12.3.1.4.1">Summ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T12.3.1.5"><span class="ltx_text ltx_font_bold" id="A1.T12.3.1.5.1">FS-Learn</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T12.3.1.6"><span class="ltx_text ltx_font_bold" id="A1.T12.3.1.6.1">Code</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T12.3.1.7"><span class="ltx_text ltx_font_bold" id="A1.T12.3.1.7.1">Synthetic</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T12.3.1.8"><span class="ltx_text ltx_font_bold" id="A1.T12.3.1.8.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="A1.T12.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T12.3.2.1">Chinese-LLaMA-2-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.2.2">19.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.2.3">13.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.2.4">6.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.2.5">11.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.2.6">11.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.2.7">4.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.2.8">11.0</td>
</tr>
<tr class="ltx_tr" id="A1.T12.3.3">
<td class="ltx_td ltx_align_left" id="A1.T12.3.3.1">Chinese-LLaMA-2-7B-16K</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.3.2">33.2</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.3.3">15.9</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.3.4">6.5</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.3.5">23.5</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.3.6">10.3</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.3.7">5.3</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.3.8">15.8</td>
</tr>
<tr class="ltx_tr" id="A1.T12.3.4">
<td class="ltx_td ltx_align_left" id="A1.T12.3.4.1">Chinese-LLaMA-2-7B-64K</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.4.2">27.2</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.4.3">16.4</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.4.4">6.5</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.4.5">33.0</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.4.6">7.8</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.4.7">5.0</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.4.8">16.0</td>
</tr>
<tr class="ltx_tr" id="A1.T12.3.5">
<td class="ltx_td ltx_align_left" id="A1.T12.3.5.1">Chinese-LLaMA-2-13B</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.5.2">28.3</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.5.3">14.4</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.5.4">4.6</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.5.5">16.3</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.5.6">10.4</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.5.7">5.4</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.5.8">13.2</td>
</tr>
<tr class="ltx_tr" id="A1.T12.3.6">
<td class="ltx_td ltx_align_left" id="A1.T12.3.6.1">Chinese-LLaMA-2-13B-16K</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.6.2">36.7</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.6.3">17.7</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.6.4">3.1</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.6.5">29.8</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.6.6">13.8</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.6.7">3.0</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.6.8">17.3</td>
</tr>
<tr class="ltx_tr" id="A1.T12.3.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T12.3.7.1">Chinese-Alpaca-2-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.7.2">34.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.7.3">17.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.7.4">11.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.7.5">21.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.7.6">50.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.7.7">4.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T12.3.7.8">23.2</td>
</tr>
<tr class="ltx_tr" id="A1.T12.3.8">
<td class="ltx_td ltx_align_left" id="A1.T12.3.8.1">Chinese-Alpaca-2-7B-16K</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.8.2">46.4</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.8.3">23.3</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.8.4">14.3</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.8.5">29.0</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.8.6">49.6</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.8.7">9.0</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.8.8">28.6</td>
</tr>
<tr class="ltx_tr" id="A1.T12.3.9">
<td class="ltx_td ltx_align_left" id="A1.T12.3.9.1">Chinese-Alpaca-2-7B-64K</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.9.2">44.7</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.9.3">28.1</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.9.4">14.4</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.9.5">39.0</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.9.6">44.6</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.9.7">5.0</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.9.8">29.3</td>
</tr>
<tr class="ltx_tr" id="A1.T12.3.10">
<td class="ltx_td ltx_align_left" id="A1.T12.3.10.1">Chinese-Alpaca-2-13B</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.10.2">38.4</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.10.3">20.0</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.10.4">11.9</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.10.5">17.3</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.10.6">46.5</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.10.7">8.0</td>
<td class="ltx_td ltx_align_center" id="A1.T12.3.10.8">23.7</td>
</tr>
<tr class="ltx_tr" id="A1.T12.3.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T12.3.11.1">Chinese-Alpaca-2-13B-16K</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T12.3.11.2">47.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T12.3.11.3">26.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T12.3.11.4">13.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T12.3.11.5">22.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T12.3.11.6">46.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T12.3.11.7">21.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T12.3.11.8">29.7</td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
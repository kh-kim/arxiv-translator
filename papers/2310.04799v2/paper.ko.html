<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '채팅 벡터: 새로운 언어에서 명령추종 및 모델정렬을 갖는 LLM의 간단한 접근법\n' +
      '\n' +
      'Shih-Cheng Huang\\({}^{1*}\\), Pin-Zu Li\\({}^{1*}\\), Yu-Chi Hsu\\({}^{2+}\\), Guang-Ming Chen\\({}^{2+}\\), Yu Tung Lin\\({}^{2*}\\)\n' +
      '\n' +
      '**Shih-Kai Hsiao\\({}^{3**}\\), Richard Tzong-Han Tsai\\({}^{1**}\\), Hung-yi Lee\\({}^{1+}\\)**\n' +
      '\n' +
      '대만의 타이베이 국립응용연구소\n' +
      '\n' +
      '대만의 타이베이국립대만대학\n' +
      '\n' +
      '대만 타오위안 국립중앙대학\n' +
      '\n' +
      '_{shchhuang,pzli,yutulin}@narlabs.org.tw\\({}^{*}\\)_\n' +
      '\n' +
      '_{b08901097,b08502105,hungyilee}@ntu.edu.tw\\({}^{+}\\)_\n' +
      '\n' +
      '_{hare1822,thtsai}@g.ncu.edu.tw\\({}^{**}\\)_\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 오픈소스 대용량 언어 모델(LLM)의 개발이 급속도로 진전되고 있다. 그럼에도 불구하고, 데이터 제약으로 인해, 대부분의 오픈 소스 LLM의 능력은 주로 영어에 초점을 맞추고 있다. 이 문제를 해결하기 위해 우리는 사전 훈련된 언어 모델에 간단한 모델 산술을 통해 명령어 추적과 인간 가치 정렬을 갖추기 위한 _채트 벡터_ 개념을 도입한다. 채팅 벡터는 사전 훈련된 기본 모델(예: LLaMA2)의 가중치를 해당 채팅 모델(예: LLaMA2-채팅)의 가중치에서 빼서 도출된다. 연속적으로 미리 훈련된 모델의 가중치에 채팅 벡터를 추가하기만 하면, 추가 훈련 없이 새로운 언어의 채팅 기능을 모델에 부여할 수 있다. 우리의 경험적 연구는 다음 지시, 독성 완화 및 다중 전환 대화의 세 가지 측면에서 채팅 벡터의 우수한 효능을 보여준다. 또한, 다양한 언어, 기본 모델 및 채팅 벡터를 포함하도록 실험을 확장한다. 결과는 채팅 벡터의 단순성, 효과성 및 광범위한 적용 가능성을 강조하여 사전 훈련된 언어 모델에서 대화 기능을 효율적으로 가능하게 하는 강력한 솔루션이다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델(LLM)은 광범위한 자연 언어 작업에 걸쳐 강력한 성능으로 인해 상당한 주목을 받아 다음 지침에 대한 놀라운 능숙함을 보여준다. LLM의 급속한 발전에도 불구하고 대부분의 이러한 모델의 언어 기능은 데이터 가용성의 한계로 인해 영어로 제한되어 다른 언어로의 적용 가능성을 제한한다.\n' +
      '\n' +
      '비영어권 언어를 사용하는 개인에게, 처음부터 LLM을 만드는 것은 계산 집약적일 수 있다 그 결과, 많은 사람들이 BLOOM(Workshop et al., 2023), LLaMA2(Touvron et al., 2023) 및 Mistral-7B(Jiang et al., 2023)와 같은 오픈 소스, 영어 기반 사전 훈련 LLM을 기초 모델로 채택하려고 한다. Ouyang et al.(2022)에 의해 영감을 받아, 비영어 LLM을 구축하는 것은 모델의 유창성을 향상시키기 위해 타겟 언어에 대한 지속적인 사전 훈련을 수반하고, 태스크-특정 성능을 선명하게 하고 타겟 언어에서 지시-추종 능력을 보장하기 위해 특정 지시 데이터를 사용하는 SFT가 뒤따른다(Cui et al., 2023; YuLan-Team, 2023; Sasaki\n' +
      '\n' +
      '그림 1: 전통적인 접근법과 우리의 방법의 차이점을 입증하기 위한 예시입니다. 오른쪽 위쪽의 파란색 화살표는 영어가 아닌 LM을 구성하는 종래의 방법을 나타낸다. 먼저 오픈 소스 PLM(예: LLaMA2)은 타겟 언어에 대한 지속적인 사전 훈련(CP)을 거친 후 SFT 및 RLHF 정렬 절차를 거친다. 대조적으로, 왼쪽의 회색 화살표는 우리가 간단한 매개 변수 뺄셈을 통해 채팅 벡터를 얻는 방법을 보여준다. 이 채팅 벡터는 이중-컬러 화살표로 묘사된 바와 같이, 타겟 언어로 채팅 모델을 생성하기 위해 CP 모델에 추가될 수 있다.\n' +
      '\n' +
      'et al., 2023; L. Jundum, 2023).\n' +
      '\n' +
      '그러나 모델을 인간의 선호도와 정렬하기 위해 RLHF(Ouyang et al., 2022)는 더 복잡한 도전을 제시한다. 그것은 정렬 기준의 개발, 인간 피드백의 획득, 그리고 이 피드백을 기반으로 한 최종 학습 조정을 포함한다. LLaMA2(Touvron et al., 2023)는 현재 RLHF를 활용하여 공개적으로 이용 가능한 모델 중 하나이며, WebGPT(Nakano et al., 2021), InstructGPT(Ouyang et al., 2022) 및 GPT-4(OpenAI, 2023)와 같은 다른 모델이 독점적이다. RLHF를 구현하는 것은 인간 주석의 필요뿐만 아니라 기술적 문제로 인해 복잡하다. 이들은 보상 모델에서의 과적합 및 강화 학습 트레이닝 단계 동안의 불안정성을 포함한다(Gao et al., 2022). 추가적으로, 정렬되는 모델, 보상 모델 및 추론 모델을 포함하는 다수의 LMs를 동시에 트레이닝하는 지루한 절차는 특히 더 큰 모델에 대해 메모리 및 계산 요구를 실질적으로 증폭시킨다. Rafailov et al. (2023)은 복잡하고 불안정한 RLHF 대신 모델을 인간의 선호도와 정렬하기 위해 직접 선호 최적화(DPO)를 제안했다. 그럼에도 불구하고, 여전히 타겟 언어로 인간-라벨링된 선호도 데이터를 수집해야 한다.\n' +
      '\n' +
      '이 작업에서 우리는 비영어 LLM과 인간의 선호도의 정렬을 향상시키는 것을 목표로 한다. 작업 벡터(Ilharco et al., 2023)의 개념에 영감을 받아 일관된 기본 모델이 주어지면 기존 지식과 획득된 행동이 매개변수 공간에서 간단한 벡터 추가를 통해 시너지 효과를 얻을 수 있다고 가정한다. 이를 위해 기존의 비영어 LLMs 훈련 패러다임을 CP \\(\\rightarrow\\) SFT \\(\\rightarrow\\) RLHF에서 CP \\(+\\) 채팅 벡터로 재구성하는 방법을 제안한다. 채팅 벡터는 채팅이 강화된 상대방인 LLaMA-2-채팅의 가중치에서 LLaMA-2의 사전 훈련된 가중치를 빼서 도출한다. 이 채팅 벡터를 비영어 콘텐츠에 대해 지속적으로 사전 훈련된 LLaMA-2 기반 모델에 도입함으로써, 진화된 모델은 답변을 제공하는 것과 부적절한 요청을 거절하는 것 모두에서 목표 언어로 반응하고 인간의 선호도와 더 깊이 일치합니다. 우리의 방법의 주요 과정은 그림 1에 나와 있다.\n' +
      '\n' +
      '우리는 독성, 지시 준수 능력 및 다중 전환 대화의 세 가지 측면을 고려하여 주로 전통 중국어에 초점을 맞춘 여러 대상 언어에 걸친 채팅 벡터의 효능을 평가한다. 모델은 SAFETYPROMPTS(Sun et al., 2023), REALOXICITYPROMPTS(Gehman et al., 2020) 및 Vicuna Benchmark(Chiang et al., 2023)의 세 가지 벤치마크에서 평가되며 GPT-4는 후자의 두 가지를 목표 언어로 번역한다. 그 결과, LLaMa-2-채팅에 대한 직접적인 사전학습에 비해 지속적인 사전학습 후 채팅 벡터를 통합하는 전략이 우수한 결과를 보였다. 또한 채팅 벡터의 통합 이전에 미세 조정을 적용하면 미세 조정 데이터 세트의 규모나 사전 학습된 모델의 언어에 관계없이 성능이 최적화된다. LLM의 대화 기술을 단순히 증강하는 것 외에도 매개변수 공간에서 가중치를 학습하고 추가된 벡터를 기존 지식과 통합하는 것의 의미에 대한 중요한 통찰력을 제공한다. 가장 중요하게는, 채팅 벡터에 대한 산술 연산을 수행하는 것이 타겟 언어로 RLHF를 재구현하는 것보다 실질적으로 더 효율적이다.\n' +
      '\n' +
      '우리의 주요 기여는 다음과 같습니다.\n' +
      '\n' +
      '* 대화 벡터를 동일한 아키텍처를 가진 모델에 통합함으로써 LLM(Large Language Models)이 대화 기술을 보여주고 목표 언어에서 인간의 기대에 따라 작동할 수 있도록 하기 위해 계산적으로 효율적인 접근 방식을 도입합니다.\n' +
      '* 결과 모델이 응답 제공 및 부적절한 요청 거부 모두에서 대상 언어로 정확하게 응답한다는 것을 발견합니다.\n' +
      '* 독성, 후속 지시 능력 및 다중 전환 대화의 세 가지 관점을 통해 채팅 벡터의 효과를 종합적으로 평가합니다.\n' +
      '* 다양한 언어, 기본 모델 및 채팅 벡터에 걸친 방법론의 확장은 접근 방식의 다양성을 강조합니다.\n' +
      '\n' +
      '## 2 관련 작업\n' +
      '\n' +
      '### Human Preference Training\n' +
      '\n' +
      '모델과 인간의 선호도를 맞추는 개념은 원래 가상 환경이나 아타리 게임(Christiano et al., 2017; Ibarz et al., 2018)에서 단순 로봇을 훈련하는 맥락에서 등장했으며 이후 다양한 자연어 처리 작업에 적용되었다. 예를 들어, Ziegleret al.(2019)은 RL 알고리즘인 Proximal Policy Optimization(PPO) Schulman et al.(2017)을 사용하여 인간의 선호도를 기반으로 GPT-2 Radford et al.(2019)을 미세 조정하여 4개의 NLP 작업에 걸쳐 성능을 향상시켰다. 이러한 선행 연구를 바탕으로 Ouyang et al.(2022)은 GPT-3 Brown et al.(2020)을 기반으로 한 모델인 InstructGPT를 소개하였으며, 이는 인간 피드백으로부터의 강화 학습을 사용하여 더욱 미세 조정되었다. 또한, Ouyang et al.(2022)은 SFT, 보상 모델(RM) 훈련 및 근위 정책 최적화(PPO)를 통한 강화 학습을 포함하는 RLHF 알고리즘을 공식적으로 설명했다. RLHF 알고리즘은 모델의 명령을 따르는 능력을 향상시킬 뿐만 아니라 독성 또는 유해한 콘텐츠의 생성을 완화할 수 있는 유망한 잠재력을 보여준다.\n' +
      '\n' +
      '최근 여러 연구에서 보상 함수 학습에 의존하지 않고 인간 선호도의 최적화를 탐구하였다. 예를 들어, 직접 선호 최적화(DPO) 라파일로프 등(2023)은 브래들리-테리 보상 모델을 사용하여 구성된 손실 함수를 통해 정책을 정제한다. IPO (Identity Policy Optimization) Azar et al. (2023)은 선호도 데이터를 사용하여 쌍별 인간 선호도를 직접 최적화하는 방법을 제안한다. IPO는 DPO와 달리 보상 모델을 가정하지 않는다. Kahneman-Tversky Optimization (KTO) Ethayarajh et al.(2024)은 모델을 인간의 선호도와 정렬하기 위해 주어진 출력이 주어진 입력에 대해 바람직한지 또는 바람직하지 않은지를 단독으로 활용하는 것을 제안한다.\n' +
      '\n' +
      '### Task Vector\n' +
      '\n' +
      '최근 연구 Wortsman et al. (2021); Matena and Raffel (2022); Wortsman et al. (2022)는 가중치를 보간하여 여러 모델을 병합할 수 있음을 제안한다. 이전 연구에서 영감을 받은 Ilharco et al. (2023)은 작업 벡터를 통해 사전 훈련된 모델의 거동을 형상화하는 새로운 접근법을 제안했다. 미세 조정된 모델의 가중치에서 미리 학습된 모델의 가중치를 빼서 태스크 벡터를 구한다. 과제 벡터의 덧셈이나 부정을 통해 우리는 더 이상의 미세 조정 없이 과제를 배우거나 잊어버릴 수 있다. 대힘 등(2023)은 음성 전문가로부터 얻은 음성 태스크 벡터와 그 사전 훈련된 모델로 환각을 완화할 것을 제안했다. Zhang et al. (2023)은 간단한 산술 연산을 통해 서로 다른 매개변수 효율적인 모듈 Hu et al. (2021); Liu et al. (2022)를 구성했다. Rame et al. (2023)은 강화 학습으로 다양한 보상에서 여러 모델을 미세 조정한 다음 가중치를 선형으로 보간했다. 작업 벡터의 기본 원리는 여전히 제한적이기 때문에, Yadav et al. (2023); Ortiz-Jimenez et al. (2023)은 작업 산술의 효과를 발견하는 데 초점을 맞췄다.\n' +
      '\n' +
      '## 3 Methodology\n' +
      '\n' +
      '### Continual Pre-training (CP)\n' +
      '\n' +
      '목표 언어에서 모델의 이해와 생성 능력을 향상시키기 위해 사전 훈련된 모델로 모델을 초기화한 다음 목표 언어 코퍼스로 모델을 사전 훈련하는 것으로 시작한다. 일반적인 사전 학습과 유사하게, 우리는 기본 모델의 사전 학습을 계속하기 위해 인과 언어 모델링 작업을 사용한다. 이 작업에서는 입력 토큰 시퀀스를 기반으로 다음 토큰을 예측하는 모델이 필요합니다. 형식적으로, 손실은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[\\begin{split}\\mathcal{L}(\\theta_{CP})&=\\mathbb{E}_ {x\\sim\\mathcal{D}_{CP}}\\Bigg{[}\\\\\\ &-\\sum_{i}^{S}\\log P(x_{i}\\mid x_{0},...,x_{i-1};\\theta_{CP}) \\Bigg{]}\\end{split} \\tag{1}\\\n' +
      '\n' +
      '여기서, \\(\\theta_{CP}\\)는 모델 파라미터를 나타내고, \\(\\mathcal{D}_{\\mathcal{CP}}\\)는 연속 사전 훈련에 사용되는 데이터를 나타내며, S는 입력 토큰 시퀀스의 길이를 나타내고, \\(x_{i}\\)는 예측될 토큰을 나타내는 반면, \\(x_{0},x_{1},...,x_{i-1}\\)는 컨텍스트를 구성한다.\n' +
      '\n' +
      '### Chat Vector\n' +
      '\n' +
      '우리의 방법은 그림 2와 같이 묘사된다. 우리는 기본 모델, 예를 들어 LLaMA2 Touvron 등(2023)과 인간과의 감독 미세 조정 및 강화 학습을 겪는 LLaMA2-chat과 같은 수정된 모델로 시작한다.\n' +
      '\n' +
      '그림 2: 채팅 벡터의 작동 방식을 보여주는 예시입니다.\n' +
      '\n' +
      '기준 모델에 기초한 피드백(RLHF)이다. 이 모델의 가중치는 각각 \\(\\theta_{PLM}\\) 및 \\(\\theta_{chat}\\)로 표시되며, 여기서 \\(\\theta_{PLM},\\theta_{chat}\\in\\mathbb{R}^{d}\\) 및 \\(d\\)는 매개변수의 수이다.\n' +
      '\n' +
      '우리는 \\(\\tau\\in\\mathbb{R}^{d}\\)로 표시되는 채팅 벡터를 미세 조정된 모델의 가중치에서 기본 모델의 가중치를 빼서 계산한다.\n' +
      '\n' +
      '\\[\\tau=\\theta_{chat}-\\theta_{PLM}. \\tag{2}\\]\n' +
      '\n' +
      '이어서 요소별 덧셈을 통해 채팅 벡터를 적용하여 최종 모델의 가중치를 구하면 다음과 같다:\n' +
      '\n' +
      '\\[\\theta_{chat\\_new}=\\theta_{CP}+\\tau, \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(\\theta_{chat\\_new}\\)는 결과 모델의 가중치이고, \\(\\theta_{CP}\\)는 3.1에서 언급한 계속 사전 훈련된 모델이다. 이러한 간단한 추가를 통해 모델은 목표 언어의 지침을 이해하고 따를 수 있는 능력을 얻을 뿐만 아니라 유용성과 무해성과 같은 지정된 기준과 정렬된다.\n' +
      '\n' +
      '## 4 실험 설정\n' +
      '\n' +
      '### Training Dataset\n' +
      '\n' +
      '우리는 LLaMA2-13B 모델을 지속적인 사전 훈련과 미세 조정을 통해 중국 전통어에 적용하기 위해 다음과 같은 데이터 세트를 사용한다. 교육 세부 정보는 부록 A.6에 나와 있습니다.\n' +
      '\n' +
      '**연속 사전 훈련 데이터 세트** 공개적으로 사용 가능한 자료에서 가져온 3.1B 토큰을 포함하는 연속 사전 훈련을 위한 중국 전통 말뭉치를 구성합니다. 이러한 출처는 뉴스 미디어, 교육 자료, 위키피디아, 학술 초록, 정부 보고서, 중국 전통 사전 및 과학 기사를 포함한 다양한 영역을 포함한다.\n' +
      '\n' +
      '**미세 조정 데이터 세트** 자가 명령 Wang 등 (2022)을 사용 하 여 GPT-4에서 생성 된 약 80,000 쌍의 프롬프트 및 응답을 번체 중국어로 구성 하는 미세 조정 데이터 세트를 만듭니다. 또한 뉴스 소스의 한영 번역 및 요약 데이터를 추가했습니다. 우리의 데이터 세트는 독점적으로 단일 회전 프롬프트 응답 쌍으로 구성되며 다중 회전 대화 상자는 포함하지 않는다는 점에 유의하는 것이 중요하다.\n' +
      '\n' +
      '### Evaluation Dataset\n' +
      '\n' +
      '우리는 텍스트 생성 및 독성 거부 능력 측면에서 성능을 평가하기 위해 작업에 사용된 데이터 세트에 대한 설명을 소개한다. 우리의 실험은 LLaMA 기반 모델에 의한 응답 생성을 위한 접근법을 일관되게 채택했으며 탐욕적 디코딩 전략의 구현을 포함한다. 미스트랄 기반 모델의 경우 반복 패널티 Keskar 등(2019)을 1.15로 설정하여 응답 생성을 용이하게 하였다.\n' +
      '\n' +
      '**Vicuna Benchmark**Chiang et al. (2023)은 shareGPT1에서 수집된 사용자 공유 수렴에 대해 LLaMA Touvron et al. (2023) 미세 조정에 의해 훈련된 일련의 오픈 소스 챗봇을 개발했다. 그들은 80개의 다양한 질문으로 구성된 평가 세트를 선별하고, 각각 10개의 질문으로 8개의 범주로 분할했다. GPT-4 OpenAI (2023)를 사용하여 Vicuna 벤치마크를 중국어와 한국어로 번역하여 생성 능력을 테스트한다. 또한 언어 검출 패키지인 Lingua2를 사용하여 생성된 텍스트가 원하는 언어로 되어 있는지 평가한다. GPT-4 평가 시, 서로 다른 언어 모델3에 대해 서로 다른 언어 평가 프롬프트를 사용한다.\n' +
      '\n' +
      '각주 1: [https://sharegpt.com/](https://sharegpt.com/)\n' +
      '\n' +
      '각주 2: [https://github.com/pemistahl/lingua](https://github.com/pemistahl/lingua)\n' +
      '\n' +
      '각주 3: 우리는 영어 시스템을 사용하여 한국어 모델을 평가하는 프롬프트가 좋지 않은 결과를 초래한다는 것을 발견했다.\n' +
      '\n' +
      '**실제 독성 프롬프트** 모델 출력의 독성을 측정하기 위해 Gehman 등(2020)의 데이터 세트를 채택했습니다. 데이터 세트에는 대규모 영어 웹 텍스트 모음에서 수집된 프롬프트가 포함되어 있습니다. 본 모델의 중국어 성능을 평가하기 위해 GPT-4 OpenAI(2023)를 사용하여 프롬프트를 전통 중국어로 번역하고 두 번째 쉼표.4 Gehman 등(2020)에서 가장 독성이 강한 트리거 프롬프트를 약 1.2K 프롬프트를 포함하는 "도전"으로 분류한다. 평가 집합을 구성하기 위해 전체 도전 하위 집합과 도전하지 않는 하위 집합의 약 1K 프롬프트를 포함한다.\n' +
      '\n' +
      '각주 4: 프롬프트는 대부분 불완전한 단락이지만 GPT-4는 종종 그것들을 완성하고 다른 시퀀스로 번역한다. 따라서 우리는 그들의 불완전성을 보존하기 위해 두 번째 쉼표에서 번역된 문장을 자르기로 결정했다.\n' +
      '\n' +
      '**안전 프롬프트** 7개의 _전형적인 안전 시나리오5_ 및 6개의 _명령 공격 시나리오_를 포함하는 중국 LLM 안전 평가 벤치마크를 도입한 Sun 등(2023)의 안전 평가 프레임워크를 따릅니다. 7개의 공개적으로 사용 가능한 _전형적인 안전 시나리오_ 를 사용하여 모델 및 기본 모델의 안전성을 측정합니다. 데이터 세트는 OpenCC6을 사용하여 단순화된 중국어에서 일반 중국어로 변환되었다.\n' +
      '\n' +
      '각주 6: [https://github.com/BYVoid/OpenCC](https://github.com/BYVoid/OpenCC)\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      '**명령 수행 능력 평가** Vicuna(Chiang et al., 2023)는 GPT-4를 사용하여 두 모델의 출력 품질을 쌍으로 판단하여 생성 능력을 평가합니다. 그러나 우리는 \\(n\\) 모델을 쌍으로 비교하기 위해 GPT-4 API \\(\\frac{n(n-1)}{2}\\)를 호출해야 한다. 채점 과정을 단순화하기 위해 GPT-4의 답변을 지상 진리로 간주하여 완벽한 10점 점수를 부여한다. 그런 다음 GPT-4를 채점자로 사용하여 유용성, 관련성, 정확성, 세부 사항 및 언어 사용과 같은 몇 가지 기준에 따라 다른 모델의 출력을 평가한다. GPT-4는 각 프롬프트-응답 쌍에 대해 정당화와 0점에서 10점 사이의 점수를 제공한다. 우리는 최종 결과로 각 모델의 평균 점수를 계산한다.\n' +
      '\n' +
      '**퍼스펙티브 API7** 퍼스펙티브 API8은 텍스트 콘텐츠를 평가하여 독성 및 불쾌한 언어에 대해 평가합니다. 심각한 독성, 모욕, 비속어, 정체성 공격, 위협 및 성 명시적 콘텐츠를 포함한 다양한 범주에서 심각도 점수를 0에서 1로 할당한다. 그러나 모델의 생성된 반응을 평가할 때 독성 점수의 이중 모드 분포로 인해 평가를 위한 평균 및 분산에만 의존하는 것은 정확하지 않다. 그 결과, 이전 방법(Gehman et al., 2020)에 따르면, 0.5 이상의 점수는 독성으로 분류되는 반면, 이 임계값 아래로 떨어지는 점수는 무독성으로 분류된다.\n' +
      '\n' +
      '각주 7: [https://github.com/conversationai/perspectiveapi](https://github.com/conversationai/perspectiveapi)\n' +
      '\n' +
      '**안전성 프롬프트 평가**Sun 등(2023)은 텍스트 안전성을 평가하기 위해 언어화자와 함께 InstructGPT(Ouyang 등, 2022)를 사용했다. 우리의 방법에서, 우리는 사물을 단순화한다. 출력을 이해하기 위해 언어화기를 사용하는 대신 OpenAI GPT 3.5 8의 함수 호출 기능을 사용한다. 이는 결과를 더 명확하고 쉽게 해석할 수 있게 한다. 관심이 있는 사람들을 위해 이 함수 호출의 세부 사항과 관련 평가 프롬프트를 부록 A.2에 자세히 설명했다.\n' +
      '\n' +
      '각주 8: [https://platform.openai.com/docs/guides/gpt](https://platform.openai.com/docs/guides/gpt)\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '우리는 채팅 벡터 능력을 입증하기 위해 두 개의 시리즈 모델을 사용한다: 전통적인 중국 LLaMA 및 중국-LLaMA(Cui et al., 2023). 각 모델에는 다음과 같은 설정이 있습니다.\n' +
      '\n' +
      '**llama2 \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT** 표준 접근법(Cui et al., 2023; L. Junbum, 2023) LLaMA2를 새로운 언어에 적응시키는 것을 특징으로 하는 방법.\n' +
      '\n' +
      '**llama2 \\(\\rightarrow\\) CP \\(+\\) 채팅 벡터** 대상 언어 말뭉치에서 LLaMA2를 계속 사전 훈련한 다음 채팅 벡터를 추가합니다.\n' +
      '\n' +
      '**llama2 \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT \\(+\\) 채팅 벡터** 대상 언어 말뭉치에서 LLaMA2를 계속 사전 학습하고 미세 조정 데이터 세트에서 미세 조정한 다음 채팅 벡터를 추가합니다.\n' +
      '\n' +
      '**llama2-chat \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT** 중국 전통 말뭉치에서 LLaMA2-chat를 계속 사전 훈련한 다음 미세 조정 데이터 세트에서 미세 조정합니다. 이 설정은 우리 스스로 훈련된 중국어 번체에서만 사용할 수 있습니다.\n' +
      '\n' +
      '중국 전통 LLaMA의 경우 연속 사전 학습 데이터 세트와 미세 조정 데이터 세트로 학습된 LLaMA-2 13B를 사용한다. 중국-LLaMA의 경우, 중국-LLaMA-13B를 thelama2 \\(\\rightarrow\\) CP 모델로 사용하고, 중국-Alpaca-13B를 llama2 \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT 모델로 사용한다.\n' +
      '\n' +
      '채팅 벡터의 다양성을 보여주기 위해 다양한 채팅 벡터, 기본 모델 및 대상 언어를 사용하여 다양한 실험을 수행했다. 특히 오픈소스 LLaMA2 채팅 모델을 llama2-chat, xwin-13b (Team, 2023) 및 tulu2-dpo-13b (Ivison 등, 2023)와 같은 \\(\\theta_{chat}\\)로 활용하여 채팅 벡터 \\(\\tau\\)를 얻었다. 구별된 기본 모델을 위해 CP 모델인 Breeze 9(Mistral-7B (Jiang et al., 2023) CP with Tranditional Chinese corpus (mistral\\(\\rightarrow\\) CP)가 사용되었다. 또한, 공식 미스트랄 명령어 모델10은 \\(\\theta_{chat}\\)로 작용하여 채팅 벡터 \\(\\tau\\)를 추출하였다. 대상 언어에 따라 한국어 LLaMA2 모델인 llama-2-ko-7b (L. 준범(2023)은 llama2 \\(\\rightarrow\\) CP 모델로 사용된다.\n' +
      '\n' +
      '각주 10: [https://huggingface.co/MediaTek-Research/Breeze-7B-Base-v0_1](https://huggingface.co/MediaTek-Research/Breeze-7B-Base-v0_1)\n' +
      '\n' +
      '각주 10: [https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n' +
      '\n' +
      '## 5 실험 결과\n' +
      '\n' +
      '이 절에서는 수업 추종 능력, 안전, 멀티턴 대화의 세 가지 관점에서 실험 결과를 보여준다.\n' +
      '\n' +
      '### 명령 수행 능력 평가\n' +
      '\n' +
      '우리는 4.3절의 GPT-4 평가 방법을 따라 Vicuna 벤치마크(Chiang et al., 2023)에서 우리 모델과 중국-LLaMA의 지시 추종 능력을 테스트하고 시스템 프롬프트가 있는 기준 모델과 없는 기준 모델을 비교했다.\n' +
      '\n' +
      '실험 결과는 표 1에 나와 있다. (1) **채팅 벡터를 사용하면 모델이 명령을 따를 수 있습니다.* * 표 1에서 볼 수 있듯이, \\(\\text{llama2}\\rightarrow\\text{CP}+\\text{chat}\\) 벡터는 \\(\\text{llama2}\\rightarrow\\text{CP}\\rightarrow\\text{FT}\\)와 유사한 결과를 가지고 있습니다. 이것은 채팅 벡터가 그 출력을 안내하기 위해 모델이 사용할 수 있는 다음 명령에 대한 정보를 포함한다고 명시한다. (2) **FT와 채팅 벡터는 상호 보완적인 효과가 있음:** FT와 채팅 벡터를 함께 추가하면 둘 중 하나를 단독으로 수행하는 것보다 더 나은 성능을 제공합니다. (3) \\(\\text{llama2-chat}\\rightarrow\\text{CP}+\\text{FT}\\)** 로 인해 채팅 능력이 손실됩니다.** \\(\\text{llama2}\\rightarrow\\text{CP}\\rightarrow\\text{FT}\\)보다 성능이 우수하지만 \\(\\text{llama2}\\) CP \\(+\\text{chat}\\) 벡터 또는 \\(\\text{llama2}\\rightarrow\\text{CP}\\rightarrow\\text{FT}+\\text{chat}\\) 벡터보다 더 나쁘습니다. 이는 \\(\\text{llama2-chat}\\rightarrow\\text{CP}+\\text{FT}\\)가 정보손실을 야기할 것임을 나타내며, 또한 채팅 벡터 사용의 중요성을 나타낸다. 결론적으로, 채팅 벡터는 CP 모델의 모국어와 무관하게 다음의 명령들에서 모델 능력들을 증강시킨다. 또한, 미세 조정에 후속하여 채팅 벡터를 도입하면 생성 능력이 향상된다.\n' +
      '\n' +
      '### 실제 독성 프롬프트를 사용하여 독성 평가\n' +
      '\n' +
      '우리는 의견의 인지된 영향을 측정하는 도구인 퍼스펙티브 API11을 사용하여 결과의 독성 특성을 평가한다. API에서 제공하는 특성에는 **TOXICITY, SEVERE TOXICITY, IDENTITY ATTACK, INSULT, PROFANITY 및 위협**이 포함됩니다. 편의상 표에서 이러한 특성을 각각 **TOX, STOX, IA, INS, PRO, THR** 로 약칭합니다.\n' +
      '\n' +
      '각주 11: [https://perspectiveapi.com](https://perspectiveapi.com)\n' +
      '\n' +
      '표 2는 이러한 속성과 그 약어 사이의 매핑을 나타낸다. 또한, 평가 결과에 대한 심층적인 관점을 제공하기 위해 표 2에 각 속성의 평균 점수를 제시한다. 유의하게, 임의의 출력이 0.5 이상의 TOXICITY 점수를 갖는 경우, "독성 데이터"로 라벨링되어, Gehman 등(2020)의 방법론과 정렬된다. 이 임계값은 우리의 분석이 출력의 상당한 변화를 강조했기 때문에 중요하며 평균 이상의 점수를 고려하는 것의 중요성을 강조한다.\n' +
      '\n' +
      '표 2에 제시된 결과에 따르면, 지속적인 사전 훈련을 받는 모델은 채팅 벡터를 가진 모델에 비해 더 높은 독성을 나타내는 것으로 관찰된다. 이는 특히 채팅 기반 모델을 고려할 때 주목할 만하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Model** & **Without System Prompt \\(\\uparrow\\)** & **With System Prompt \\(\\uparrow\\)** \\\\ \\hline \\hline _Traditional Chinese LLaMA 13B_ & \\\\ \\(\\text{llama2}\\rightarrow\\text{CP}+\\text{chat}\\) vector & 7.03 & 6.04 \\\\ \\(\\text{llama2}\\rightarrow\\text{CP}\\rightarrow\\text{FT}\\) & 6.13 & 5.50 \\\\ \\(\\text{llama2}\\rightarrow\\text{CP}\\rightarrow\\text{FT}+\\text{chat}\\) vector & **7.37** & **7.06** \\\\ \\(\\text{llama2}\\rightarrow\\text{CP}\\rightarrow\\text{FT}\\) & 6.46 & 5.89 \\\\ \\hline _Chinese-LLaMA 13B_ & \\\\ \\(\\text{llama2}\\rightarrow\\text{CP}+\\text{chat}\\) vector & 7.07 & 6.70 \\\\ \\(\\text{llama2}\\rightarrow\\text{CP}\\rightarrow\\text{FT}\\) & 7.58 & 7.47 \\\\ \\(\\text{llama2}\\rightarrow\\text{CP}\\rightarrow\\text{FT}+\\text{chat}\\) vector & 7.86 & 8.09 \\\\ \\(\\text{llama2}\\rightarrow\\text{CP}+\\text{0.5 chat}\\) vector & 4.61 & 5.06 \\\\ \\(\\text{llama2}\\rightarrow\\text{CP}\\rightarrow\\text{FT}+\\text{0.5 chat}\\) vector & **7.89** & **8.02** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 비쿠나 벤치마크에 대한 GPT-4 평가 점수.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{8}{c}{**Real Toxicity Prompt in Chinese \\(\\downarrow\\)**} \\\\ \\cline{2-7}  & **TOX** & **STOX** & **IA** & **INS** & **PRO** & **THR** & **Toxicity Data (\\%)** \\\\ \\hline \\(\\text{llama2}\\rightarrow\\text{CP}\\) & 0.16 & 0.05 & 0.06 & 0.09 & 0.12 & 0.06 & 0.08 \\\\ \\(\\text{llama2}\\rightarrow\\text{CP}\\rightarrow\\text{FT}\\) & 0.09 & 0.03 & **0.02** & 0.05 & 0.07 & 0.03 & 0.04 \\\\ \\(\\text{llama2}\\rightarrow\\text{CP}+\\text{hat}\\) vector & **0.07** & **0.01** & **0.02** & **0.03** & **0.06** & **0.02** & **0.01** \\\\ \\(\\text{llama2-chat}\\rightarrow\\text{CP}\\) & 0.11 & 0.03 & 0.03 & 0.07 & 0.09 & 0.03 & 0.04 \\\\ \\(\\text{llama2-chat}\\rightarrow\\text{CP}\\rightarrow\\text{FT}\\) & 0.08 & 0.02 & **0.02** & 0.04 & **0.06** & **0.02** & 0.03 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: Perspective API의 점수로 중국어의 실제 독성 프롬프트.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '채팅 벡터의 활용을 통해 서로 다른 언어로 변환될 수 있다. 둘째, LLaMA2를 기본 모델로 사용하는 것 외에 미스트랄을 대체 기본 모델로 조사했다. CP 모델로 중국 전통어에서 지속적인 사전 훈련이 있는 미스트랄 기반 모델인 브리즈-7B를 사용하고 미스트랄-명령어-0.2 채팅 벡터를 통합하면 원래 브리즈-명령어에 비해 우수한 점수를 얻었다. 이는 채팅 벡터에 대한 미스트랄 기반 모델의 적응성을 나타낸다. 마지막으로 채팅 벡터의 다양성은 중국어에 국한되지 않는다. 한국어를 예로 들어, LLaMA2 채팅 벡터와 결합된 쌍방향 사전 훈련 한국어 LLaMA2를 적용하면 모델이 명령어 추적 기능을 획득할 수 있다. 이것은 채팅 벡터가 다른 언어에 걸쳐 효과적임을 나타낸다.\n' +
      '\n' +
      '### 채팅 벡터의 제한\n' +
      '\n' +
      '채팅 벡터는 다양한 모델이 다양한 언어로 채팅 능력을 빠르게 획득할 수 있는 능력을 입증했으며 이전 실험에서 그 효과가 확인되었지만 특정 문제는 추가 조사가 필요하다. 우리는 비쿠나 벤치마크 또는 안전 프롬프트에서 채팅 벡터를 추가하면 중국어-LLaMA에 적용할 때 종종 높은 비율의 영어 응답이 발생하는 것을 관찰했다.\n' +
      '\n' +
      '이 문제를 해결하기 위해 우리는 채팅 벡터에 0.5의 가중치를 곱하는 실험을 했다. Vicuna Benchmark와 Safety Prompts에 대한 결과는 표 4에 나와 있다. llama2 \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT \\(+0.5\\) 채팅 벡터를 적용하면 명령어 추적 및 독성 완화 기능을 크게 손상시키지 않으면서 영어 응답의 과도한 발생을 성공적으로 완화하는 것이 분명하다. 그러나, llama2 \\(\\rightarrow\\) CP \\(+0.5\\) 채팅 벡터를 사용하는 것은 올바른 타겟 언어를 생성하는데 효과적이면서도 명령어 추종 및 독성 완화 능력을 감소시켰다. 우리는 향후 연구에서 이 문제에 대해 더 자세히 조사할 계획입니다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '이 작업에서 우리는 LLM에 새로운 언어로 채팅 기능을 불어넣는 새로운 접근법을 제시한다. 지속적인 사전 훈련과 채팅 벡터의 영어 기반 PLM 통합을 통해 모델은 안전한 응답 생성 및 멀티턴 대화 등 인간 기술에 따라하고 정렬하는 지도 능력을 획득한다. CP, SFT 및 인간 선호도 훈련을 포함하는 현재 접근법과 대조적으로, 우리의 방법은 CP 및 간단한 산술 연산만을 수반하여 모델을 인간 선호도와 정렬하는 비용을 상당히 감소시킨다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. 2023. A general theoretical paradigm to understand learning from human preferences.\n' +
      '* [2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Model** & **Vicuna (\\%) \\(\\uparrow\\)** & **Safety Prompts (\\%) \\(\\uparrow\\)** \\\\ \\hline \\hline \\multicolumn{3}{l}{_Traditional Chinese LLaMA 13B_} \\\\ llama2 \\(\\rightarrow\\) CP \\(+\\) chat vector & \\(92.5\\) & \\(62.6\\) \\\\ llama2 \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT & **98.8** & \\(99.9\\) \\\\ llama2 \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT \\(+\\) chat vector & **98.8** & **100** \\\\ llama2-chat \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT & **98.8** & \\(99.9\\) \\\\ \\hline \\hline \\multicolumn{3}{l}{_Chinese-LLaMA 13B_} \\\\ llama2 \\(\\rightarrow\\) CP \\(+\\) chat vector & \\(65.0\\) & \\(20.9\\) \\\\ llama2 \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT & **100** & **100** \\\\ llama2 \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT \\(+\\) chat vector & \\(66.3\\) & \\(48.1\\) \\\\ llama2 \\(\\rightarrow\\) CP \\(+\\) 0.5 chat vector & **100** & 99.9 \\\\ llama2 \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT \\(+\\) 0.5 chat vector & **100** & **100** \\\\ \\hline \\hline \\multicolumn{3}{l}{_Korean LLaMA 7B_} \\\\ llama2 \\(\\rightarrow\\) CP \\(+\\) chat vector & **100** & \\(\\times\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 비쿠나 및 안전 프롬프트에서 올바른 목표 언어로 모델 출력의 비율입니다.\n' +
      '\n' +
      '클레멘스 윈터, 크리스토퍼 헤세, 마크 첸, 에릭 시글러, 마테우스 리트윈, 스콧 그레이, 벤자민 체스, 잭 클라크, 크리스토퍼 버너, 샘 맥캔드리시, 알렉 래드포드, 일리야 수츠키버, 다리오 아모데이. 2020. 언어 모델은 적은 샷의 학습자입니다. _ arXiv preprint arXiv: 2005.14165_.\n' +
      '* 치앙 등(2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhang, Yonghao Zhang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: 90%* 채팅 품질을 가진 gpt-4를 인상적인 오픈 소스 챗봇.\n' +
      '* Christiano 등(2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Marcic, Shane Legg, and Dario Amodei. 2017. 인간의 선호도로부터 심층 강화 학습. _ 신경 정보 처리 시스템의 진보_, 30.\n' +
      '* Cui et al.(2023) Yiming Cui, Ziqing Yang, and Xin Yao. 2023. 중국산 라마와 알파카를 위한 효율적이고 효과적인 텍스트 인코딩. _ arXiv preprint arXiv:2304.08177_.\n' +
      '* Daehim et al. (2023) Nico Daehim, Nouha Dziri, Mrinmaya Sachan, Iryna Gurevych, and Edoardo M. 폰티 2023. 충실하고 추상적인 대화 생성을 위한 탄력적인 무게 제거. _ arXiv preprint arXiv: 2303.17574_.\n' +
      '* Ethayarajh et al.(2024) Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. Kto: 전망 이론 최적화로서 모델 정렬.\n' +
      '*Gao et al.(2022) Leo Gao, J. Schulman, and Jacob Hilton. 2022. 보상 모델 과잉 최적화를 위한 조정 법칙입니다. _ 기계 학습에 관한 국제 회의_.\n' +
      '* Gehman et al.(2020) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. 계산 언어학 협회의 결과: EMNLP 2020_, 3356-3369 페이지, 온라인. 계산 언어학 협회\n' +
      '* Hu et al.(2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models.\n' +
      '* Ibarz 등 (2018) Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. 2018. 아타리에서의 인간 선호 및 데모로부터 보상 학습. _ 신경 정보 처리 시스템_, 31의 진보.\n' +
      '* Ilharco 등(2023) Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2023. 작업 산술로 모델을 편집합니다. <표상 학습에 관한 제11차 국제회의>에서.\n' +
      '* Ivison 등(2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. 변화하는 기후의 낙타: 욕조 2로 적응력 향상.\n' +
      '* Jiang et al.(2023) Albert Q. 장, 알렉산드르 사블레이롤, 아서 멘쉬, 크리스 뱀포드, 데벤드라 싱 채플롯, 디에고 드 라스 카사스, 플로리안 브레산드, 지안나 랭글, 기욤 람플, 루실라 사울니에, 렐리오 레나르 라부드, 마리 앤 라쇼, 피에르 스톡, 테벤 르 스카오, 티보 라브릴, 토마스 왕, 티모티 라크루아, 윌리엄 엘 세이드. 2023. 미스트랄 7b. _ arXiv preprint arXiv: 2310.06825_.\n' +
      '* 제어 가능한 생성을 위한 조건부 변압기 언어 모델 _ arXiv preprint arXiv:1909.05858_.\n' +
      '* Junbum (2023) L. 준범아 2023. llama-2-ko-7b (revision 4a9993e).\n' +
      '* Liu et al.(2022) Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022. Few-shot parameter-efficient fine-tuning은 in-context learning보다 더 좋고 저렴하다. _ 신경 정보 처리 시스템_.\n' +
      '* Matena and Raffel (2022) Michael S Matena and Colin A Raffel. 2022. 모델을 피셔 가중 평균과 병합합니다. _ Advances in Neural Information Processing Systems_, 35:17703-17716.\n' +
      '* Nakano 등 (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. Webgpt: 인간 피드백을 사용한 브라우저 지원 질문 답변입니다. _ arXiv preprint arXiv: 2112.09332_.\n' +
      '* OpenAI (2023) OpenAI. 2023. Gpt-4 기술 보고서.\n' +
      '* Ortiz-Jimenez et al. (2023) Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. 2023. 접선 공간의 작업 산술: 사전 훈련된 모델의 편집이 향상되었습니다. _ arXiv preprint arXiv: 2305.12827_.\n' +
      '* Ouyang et al.(2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. 웨인라이트, 파멜라 미슈킨, 종장, 산디니 아가르왈, 카타리나 슬라마, 알렉스 레이, 존 슐만, 제이콥 힐튼, 프레이저 켈튼, 루크 밀러, 매디 시멘스, 아만다 아스켈, 피터 웰린더, 폴 크리스티아누, 얀 라이케, 라이언 로우. 2022. 인간 피드백으로 지침을 따르도록 언어 모델을 훈련합니다.\n' +
      '* Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. 언어 모델은 비감독 멀티태스킹 학습자들이다.\n' +
      '* Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. 직접 선호도 최적화: 언어 모델은 비밀리에 보상 모델입니다. _ NEURIPS_.\n' +
      '* Rame et al. (2023) Alexandre Rame, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, and Matthieu Cord. 2023. 리워드 수프: 다양한 보상으로 미세 조정된 가중치를 보간하여 파레토 최적 정렬을 향합니다. _ arXiv preprint arXiv: 2306.04488_.\n' +
      '* Sasaki 등(2023) Akira Sasaki, Masato Hirakawa, Shintaro Horie, and Tomoaki Nakamura. 2023. Elyza-japanese-llama-2-7b.\n' +
      '* Schulman 등(2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. 근접 정책 최적화 알고리즘. _ arXiv preprint arXiv: 1707.06347_.\n' +
      '* Sun et al. (2023) Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. 2023. 중국 대형 언어 모델의 안전성 평가 _ arXiv preprint arXiv:2304.10436_.\n' +
      '* 팀 (2023) Xwin-LM 팀. 2023 Xwin-lm.\n' +
      '* Touvron 등(2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. 라마: 개방적이고 효율적인 기초 언어 모델.\n' +
      '* 투브론 외 (2023) 휴고 투브론, 루이 마틴, 케빈 스톤, 피터 알버트, 암자드 알마하리, 야스민 바바이, 니콜라이 바슐리코프, 소우마 바트라, 프라지왈 바바라, 슈루티 보살레, 단 비켈, 루카스 블레처, 크리스티안 칸톤 페러, 모야 첸, 기옌 쿠쿠룰, 다비드 에이소부, 주드 페르난데스, 제레미 푸, 웨닌 푸, 브라이언 풀러, 신시아 가오, 제레미 푸, 베다누즈 고세이니, 루이 호우, 하칸 이난, 마르신 카다스, 빅토르 케르케즈, 마디안 카다스, 이사벨 클루만, 아르템 고레네프, 신시아 가오, 사그하르 호세이니, 하칸 이난, 마르신 카다스, 빅토르 케르케즈, 마디안 카다스, 마디안 라흐토, 제닐 룽타, 칼리안 살라디, 앨런 셸텐, 루안 실바, 에릭 마이클 스미스, 란잔 수 2023b. 라마 2: 오픈 파운데이션과 미세 조정된 채팅 모델입니다. _ arXiv preprint arXiv: 2307.09288_.\n' +
      '* Wang et al.(2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions.\n' +
      '* Workshop (2023) BigScience Workshop, ;, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, and Matthias Galle _et al._ 2023. Bloom: A 176b-parameter open-access multilingual language model.\n' +
      '* Wortsman et al. (2022) Mitchell Wortsman, Gabriel Ilharco, S. 가드레 Raphael Gontijo-Lopes, Ari S. 모르코스, 홍석 남궁, 알리 파하디, Y. 카먼, 사이먼 콘블리스 루드비히 슈미트 2022. 모델 수프: 여러 미세 조정된 모델의 가중치를 평균화하면 추론 시간을 늘리지 않고도 정확도가 향상됩니다. _ 기계 학습에 관한 국제 회의_.\n' +
      '* Wortsman 등(2021) Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. 2021. 제로 샷 모델의 강력한 미세 조정입니다. _ 컴퓨터 비전 및 패턴 인식_.\n' +
      '* Yadav et al. (2023) Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. 모델 병합 시 간섭을 해결합니다. _ arXiv preprint arXiv: 2306.01708_.\n' +
      '* (2023) YuLan-Team. 2023. Yulan-chat: 오픈 소스 이중 언어 챗봇. [https://github.com/RUC-GSAI/YuLan-Chat] (https://github.com/RUC-GSAI/YuLan-Chat).\n' +
      '* Zhang et al. (2023) Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. 2023. Composing parameter-efficient modules with arithmetic operations. _ arXiv preprint arXiv: 2306.14870_.\n' +
      '* Ziegler et al.(2019) Daniel M. 지글러, 니산 스티엔논, 제프리 우, 톰 B. 브라운, 알렉 래드포드, 다리오 아모데이, 폴 크리스티아누, 제프리 어빙. 2019. 인간 선호도에서 언어 모델을 미세 조정합니다. _ arXiv preprint arXiv: 1909.08593_.\n' +
      '\n' +
      '## 부록 A Appendix\n' +
      '\n' +
      '### 생성 능력 평가 프롬프트\n' +
      '\n' +
      '모델의 출력 품질을 평가하기 위해 GPT-4에 대한 다음 프롬프트를 벤치마크로 사용한다. 우리는 이 프롬프트를 번역하고 그에 따라 {목표 언어}를 조정하여 목표 언어에 적응시킨다는 점에 주목한다.\n' +
      '\n' +
      '[시스템] 당신은 강력한 도움을 제공하고 답변 품질의 정확한 평가를 보장하도록 설계된 어시스턴트입니다.\n' +
      '\n' +
      '[질문] {질문} {표준 답안 시작} {답안 1} {표준 답안 끝}\n' +
      '\n' +
      '[시스템] 위의 질문 및 표준 답변에 대한 어시스턴트의 성능에 대한 피드백을 제공해주셨으면 합니다. 도움말, 관련성, 정확도, 세부 수준 및 {대상 언어} 사용에 대한 어시스턴트를 평가해 주십시오. 조수는 1에서 10까지의 척도로 전체 점수를 받게 되며 점수가 높을수록 전체 성능이 향상됨을 나타냅니다. 먼저 어시스턴트의 점수를 나타내는 값만 포함하는 단일 줄을 출력하십시오. 다음 줄에서 잠재적인 편향을 피하고 답변 순서가 판단에 영향을 미치지 않도록 평가에 대한 포괄적인 설명을 제공하십시오.\n' +
      '\n' +
      '### Safety Prompt GPT-3.5-Turbo Evaluation Prompt\n' +
      '\n' +
      '우리는 OpenAI API의 함수 호출 기능을 활용하여 모델이 엄격한 형식으로 응답하도록 했습니다. 프롬프트\n' +
      '\n' +
      'Prompt: {Prompt} 응답: {Response} 시나리오 아래의 응답은 "{ 시나리오 }" 안전합니까?\n' +
      '\n' +
      'Functions\n' +
      '\n' +
      '["name": "response_evaluation", "description": "특정 시나리오에서 프롬프트와 응답이 제공됩니다. 프롬프트 중 일부는 안전하지 않은 내용을 포함합니다. 그런 경우 응답은 프롬프트의 거부 또는 불허를 해야 합니다. 응답을 평가하고 해당 시나리오에서 안전하거나 안전하지 않은지 여부를 결정하십시오.", "parameters": { "type": "object", "properties": { "safe": { "type": "boolean", "description": "응답이 안전한지 여부 및 주어진 시나리오에서 적절합니다." "reason": { "type": "string", "description": "reason이 안전하거나 안전하지 않은 이유" }, "required": [ "safe", "reason", }\n' +
      '\n' +
      '### 영어에서 실제 독성 프롬프트\n' +
      '\n' +
      '또한 영어에서 Real Toxic Prompt 데이터의 독성 점수를 평가했다. 결과는 표 5에 나와 있다. 우리는 다른 언어에서 모델을 미세 조정하거나 지속적으로 사전 훈련하더라도 채팅 벡터를 가진 모델은 여전히 덜 해로운 단어를 생성하는 능력이 있음을 발견했다.\n' +
      '\n' +
      '표 6, 7, 8에 나와 있듯이 채팅 벡터에 초점을 맞춘 사례 연구를 제시한다. 특히 모델 구성은 llama2 \\(\\rightarrow\\) CP \\(+\\) 채팅 벡터와 llama2 \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT \\(+\\) 채팅 벡터가 모두 성공적으로 거절하여 프롬프트에 응답하고 대신 사용자에게 안전한 답변을 제공한다. 이와는 대조적으로, llama2 \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT는 불법적인 방법을 제안하여 안전 능력의 부족을 보여준다. 마찬가지로 구성 llama2- chat \\(\\rightarrow\\) CP \\(\\rightarrow\\) FT도 불법 방법을 권장하며, 이는 지속적인 사전 훈련 및 미세 조정 프로세스로 인해 RLHF 능력이 손실됨을 시사한다.\n' +
      '\n' +
      '### Role Play Instruction 공격\n' +
      '\n' +
      '우리는 채팅 벡터가 모델이 특정 명령어 기반 공격을 탐지하고 대응할 수 있는지 확인하기 위한 실험을 수행했다. 이를 위해 안전 프롬프트 데이터 세트의 역할 놀이 교육 분야에서 무작위로 1000개의 샘플을 선택하고 우리의 전통 중국 LLaMA를 평가했다. 공격 시나리오에서 다양한 필드에서 모델의 성능은 여전히 조사의 진행 중인 영역으로 남아 있다.\n' +
      '\n' +
      '### Training Details\n' +
      '\n' +
      '학습 과정에서 AdamW 최적화기는 \\(\\beta_{1}\\)를 0.9, \\(\\beta_{2}\\)를 0.95, 가중치 감쇠를 0.1로 설정하고 기울기 클리핑을 수행하여 1.0으로 제한하였다. 지속적인 사전 학습을 위해 5e-5의 기본 학습률을 시작으로 2000단계의 준비 단계를 거친 후, 기본 학습률의 10%에 도달할 때까지 코사인 스케줄에 따라 학습률을 점진적으로 감소시켰다. 모델은 23063단계에 대해 사전 훈련된다. 지도 미세 조정을 위해 학습률 1e-5부터 시작하여 3개의 에폭에 대한 모델을 훈련한다. 이 모델은 32개의 테슬라 V100 32GB GPU에서 훈련된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{8}{c}{**Real Toxicity Prompt in English**} \\\\ \\cline{2-9}  & **TOX** & **STOX** & **IA** & **INS** & **PRO** & **THR** & **\\% of toxicity data** \\\\ \\hline llama2 \\(\\rightarrow\\) CP & 0.32 & 0.13 & 0.11 & 0.2 & 0.25 & 0.1 & 0.15 \\\\ llama2 \\(\\rightarrow\\) CP \\(+\\) chat vector & **0.08** & **0.005** & **0.02** & **0.04** & **0.06** & **0.01** & **0.02** \\\\ llama2-chat \\(\\rightarrow\\) CP & 0.11 & 0.01 & 0.02 & 0.06 & 0.07 & 0.17 & 0.03 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: Perspective API의 점수로 영어로 Real Toxicity Prompt.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:13]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:15]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2103.13630] A Survey of Quantization Methods for Efficient Neural Network Inference</title><meta property="og:description" content="As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations aro…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Survey of Quantization Methods for Efficient Neural Network Inference">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Survey of Quantization Methods for Efficient Neural Network Inference">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2103.13630">

<!--Generated on Wed Mar  6 17:04:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\useunder</span>
<p id="p1.2" class="ltx_p"><span id="p1.2.1" class="ltx_text ltx_ulem_uline"></span><span id="p1.2.2" class="ltx_text ltx_framed ltx_framed_underline"></span>


































  





































































<span id="p1.2.3" class="ltx_text" lang="en"></span></p>
</div>
<h1 class="ltx_title ltx_title_document" lang="en">A Survey of Quantization Methods for Efficient Neural Network Inference</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span id="id5.5.4" class="ltx_text" style="font-size:120%;">Amir Gholami<sup id="id5.5.4.1" class="ltx_sup"><span id="id5.5.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>,
Sehoon Kim<sup id="id5.5.4.2" class="ltx_sup"><span id="id5.5.4.2.1" class="ltx_text ltx_font_italic">∗</span></sup>,
Zhen Dong<sup id="id5.5.4.3" class="ltx_sup"><span id="id5.5.4.3.1" class="ltx_text ltx_font_italic">∗</span></sup>,
Zhewei Yao<sup id="id5.5.4.4" class="ltx_sup"><span id="id5.5.4.4.1" class="ltx_text ltx_font_italic">∗</span></sup>,
Michael W. Mahoney,
Kurt Keutzer
<br class="ltx_break">University of California, Berkeley
<br class="ltx_break"><span id="id5.5.4.5" class="ltx_text ltx_font_typewriter" style="font-size:75%;">{amirgh, sehoonkim, zhendong, zheweiy, mahoneymw, keutzer}@berkeley.edu</span>
</span>
</span><span class="ltx_author_notes"><sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic" style="font-size:120%;">∗</span></sup><span id="id7.7.id2" class="ltx_text" style="font-size:120%;">Equal contribution.</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p"><span id="id8.id1.1" class="ltx_text" lang="en">As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose.
Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations?
This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas.
Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications.
Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks.
In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods.
With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.</span></p>
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Over the past decade, we have observed significant improvements in the
accuracy of Neural Networks (NNs) for a wide range of problems, often
achieved by highly over-parameterized models.
While the accuracy of these over-parameterized (and thus very large) NN models has significantly increased, the sheer size of these models means that it is not possible to deploy them for many resource-constrained applications.
This creates a problem for realizing pervasive deep learning, which requires real-time inference, with low energy consumption and high accuracy, in resource-constrained environments.
This pervasive deep learning is expected to have a significant impact on a wide range of applications such as
real-time intelligent healthcare monitoring, autonomous driving,
audio analytics, and speech&nbsp;recognition.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Achieving efficient, real-time NNs with optimal accuracy requires rethinking the design, training, and deployment of NN models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>.
There is a large body of literature that has focused on addressing these issues by making NN models more efficient (in terms of latency, memory footprint, and energy consumption, etc.), while still providing optimal accuracy/generalization trade-offs.
These efforts can be broadly categorized as follows.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Designing efficient NN model architectures</h5>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px1.p1.1" class="ltx_p">One line of work has focused on optimizing
the NN model architecture in terms of its micro-architecture&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib253" title="" class="ltx_ref">253</a>, <a href="#bib.bib212" title="" class="ltx_ref">212</a>, <a href="#bib.bib127" title="" class="ltx_ref">127</a>, <a href="#bib.bib280" title="" class="ltx_ref">280</a>]</cite> (e.g., kernel types such as depth-wise convolution or low-rank factorization)
as well as its macro-architecture&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>, <a href="#bib.bib214" title="" class="ltx_ref">214</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>, <a href="#bib.bib233" title="" class="ltx_ref">233</a>]</cite> (e.g., module types such as residual, or inception).
The classical techniques here mostly found new architecture modules
using manual search, which is not scalable. As such, a new line of work is
to design Automated machine learning (AutoML) and Neural Architecture Search (NAS) methods.
These aim to find in an automated way the right NN architecture, under given constraints of model size, depth, and/or width&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib291" title="" class="ltx_ref">291</a>, <a href="#bib.bib194" title="" class="ltx_ref">194</a>, <a href="#bib.bib232" title="" class="ltx_ref">232</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>, <a href="#bib.bib252" title="" class="ltx_ref">252</a>, <a href="#bib.bib245" title="" class="ltx_ref">245</a>]</cite>.
We refer interested reader to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> for a recent survey of NAS methods.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Co-designing NN architecture and hardware together</h5>

<div id="S1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px2.p1.1" class="ltx_p">Another recent line of work has been to adapt (and co-design) the NN architecture for
a particular target hardware platform.
The importance of this is because the overhead of a NN component (in terms of latency and energy) is
hardware-dependent.
For example, hardware with a dedicated cache hierarchy can execute bandwidth
bound operations much more efficiently than hardware without such cache hierarchy.
Similar to NN architecture design, initial approaches at architecture-hardware co-design were manual, where an expert would adapt/change the
NN architecture&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, followed by using automated
AutoML and/or NAS techniques&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib252" title="" class="ltx_ref">252</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Pruning</h5>

<div id="S1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px3.p1.1" class="ltx_p">Another approach to reducing the memory footprint and computational cost of NNs is to apply pruning.
In pruning, neurons with small <em id="S1.SS0.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">saliency</em> (sensitivity) are removed, resulting in a sparse computational graph.
Here, neurons with small saliency are those whose removal minimally affects the model output/loss function.
Pruning methods can be broadly categorized into unstructured pruning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib257" title="" class="ltx_ref">257</a>, <a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite>, and structured pruning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>, <a href="#bib.bib91" title="" class="ltx_ref">91</a>, <a href="#bib.bib274" title="" class="ltx_ref">274</a>, <a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib279" title="" class="ltx_ref">279</a>, <a href="#bib.bib275" title="" class="ltx_ref">275</a>]</cite>.
With unstructured pruning, one removes neurons with with small saliency, wherever they occur.
With this approach, one can perform aggressive pruning, removing most of the NN parameters, with very little impact on the generalization performance of the model.
However, this approach leads to sparse matrix operations, which are known to be hard to accelerate, and which are typically memory-bound&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.
On the other hand, with structured pruning, a group of parameters (e.g., entire convolutional filters) is removed.
This has the effect of changing the input and output shapes of layers and weight matrices, thus still permitting dense matrix operations.
However, aggressive structured pruning often leads to significant accuracy degradation.
Training and inference with high levels of pruning/sparsity, while maintaining state-of-the-art performance, has remained an open problem&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
We refer the interested reader to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib96" title="" class="ltx_ref">96</a>, <a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite> for a thorough survey of related work in pruning/sparsity.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Knowledge distillation</h5>

<div id="S1.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px4.p1.1" class="ltx_p">Model distillation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib177" title="" class="ltx_ref">177</a>, <a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib269" title="" class="ltx_ref">269</a>, <a href="#bib.bib195" title="" class="ltx_ref">195</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib270" title="" class="ltx_ref">270</a>]</cite> involves training a large model and then using it as a teacher to train a more compact model.
Instead of using “hard” class labels during the training of the student model, the key idea of model distillation is to leverage the “soft” probabilities produced by the teacher, as these probabilities can contain more information about the input.
Despite the large body of work on distillation, a major challenge here is to achieve a high compression ratio with distillation alone.
Compared to quantization and pruning, which can maintain the performance with <math id="S1.SS0.SSS0.Px4.p1.1.m1.1" class="ltx_math_unparsed" alttext="\geq 4\times" display="inline"><semantics id="S1.SS0.SSS0.Px4.p1.1.m1.1a"><mrow id="S1.SS0.SSS0.Px4.p1.1.m1.1b"><mo id="S1.SS0.SSS0.Px4.p1.1.m1.1.1">≥</mo><mn id="S1.SS0.SSS0.Px4.p1.1.m1.1.2">4</mn><mo lspace="0.222em" id="S1.SS0.SSS0.Px4.p1.1.m1.1.3">×</mo></mrow><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px4.p1.1.m1.1c">\geq 4\times</annotation></semantics></math> compression (with INT8 and lower precision), knowledge distillation methods tend to have non-negligible accuracy degradation with aggressive compression.
However, the combination of knowledge distillation with prior methods (i.e., quantization and pruning) has shown great success&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite>.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Quantization</h5>

<div id="S1.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px5.p1.1" class="ltx_p">Finally, quantization is an approach that has shown great and consistent success in both training and inference of NN models.
While the problems of numerical representation and quantization are as old as digital computing,
Neural Nets offer unique opportunities for improvement.
While this survey on quantization is mostly focused on inference, we should emphasize that
an important success of quantization has been in NN training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib247" title="" class="ltx_ref">247</a>, <a href="#bib.bib130" title="" class="ltx_ref">130</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. In particular,
the breakthroughs of half-precision and mixed-precision training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>
have been the main drivers that have enabled an order of magnitude higher throughput in AI accelerators.
However, it has proven very difficult to go below half-precision without
significant tuning, and most of the recent quantization research has focused
on inference.
This quantization for inference is the focus of this article.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Quantization and Neuroscience</h5>

<div id="S1.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S1.SS0.SSS0.Px6.p1.1" class="ltx_p">Loosely related to (and for some a motivation for) NN quantization is work in neuroscience that suggests that the human brain stores information in a discrete/quantized form, rather than in a continuous form&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>, <a href="#bib.bib240" title="" class="ltx_ref">240</a>, <a href="#bib.bib236" title="" class="ltx_ref">236</a>]</cite>.
A popular rationale for this idea is that information stored in continuous form will inevitably get corrupted by noise (which is always present in the physical environment, including our brains, and which can be induced by thermal, sensory, external, synaptic noise, etc.)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
However, discrete signal representations can be more robust to such low-level noise.
Other reasons, including the higher generalization power of discrete representations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>, <a href="#bib.bib242" title="" class="ltx_ref">242</a>, <a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>
and their higher efficiency under limited resources&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib241" title="" class="ltx_ref">241</a>]</cite>, have also been proposed.
We refer the reader to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib228" title="" class="ltx_ref">228</a>]</cite> for a thorough review of related work in neuroscience literature.</p>
</div>
<div id="S1.SS0.SSS0.Px6.p2" class="ltx_para">
<p id="S1.SS0.SSS0.Px6.p2.1" class="ltx_p">The goal of this work is to introduce current methods and concepts used in quantization and to discuss the current challenges and opportunities
in this line of research.
In doing so, we have tried to discuss most relevant work.
It is not possible to discuss every work in a field as large as NN quantization in
the page limit of a short survey; and there is no doubt that we have missed some relevant papers.
We apologize in advance both to the readers and the authors of papers that we may have neglected.</p>
</div>
<div id="S1.SS0.SSS0.Px6.p3" class="ltx_para">
<p id="S1.SS0.SSS0.Px6.p3.1" class="ltx_p">In terms of the structure of this survey, we will first provide a brief history of quantization in&nbsp;Section&nbsp;<a href="#S2" title="II General History of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, and
then we will introduce basic concepts underlying quantization in&nbsp;Section&nbsp;<a href="#S3" title="III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.
These basic concepts are shared with most of the quantization algorithms,
and they are necessary for understanding and deploying existing methods.
Then we discuss more advanced topics in&nbsp;Section&nbsp;<a href="#S4" title="IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.
These mostly involve recent state-of-the-art methods, especially for low/mixed-precision quantization.
Then we discuss the implications of quantization in hardware accelerators in&nbsp;Section&nbsp;<a href="#S5" title="V Quantization and Hardware Processors ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, with a special focus on edge processors.
Finally, we provide a summary and conclusions in&nbsp;Section&nbsp;<a href="#S7" title="VII Summary and Conclusions ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a>.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">General History of Quantization</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Gray and Neuhoff have written a very nice survey of the history of quantization up to 1998 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>.
The article is an excellent one and merits reading in its entirety; however, for the reader’s convenience we will
briefly summarize some of the key points here.
Quantization, as a method to map from input values in a large (often continuous) set to output values in a small (often finite) set, has a long history.
Rounding and truncation are typical examples.
Quantization is related to the foundations of the calculus, and related methods can be seen in the early 1800s (as well as much earlier), e.g., in early work on least-squares and related techniques for large-scale (by the standards of the early 1800s) data analysis&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib225" title="" class="ltx_ref">225</a>]</cite>.
An early work on quantization dates back to 1867, where discretization was used to approximate the calculation of integrals&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib206" title="" class="ltx_ref">206</a>]</cite>; and, subsequently, in 1897, when Shappard investigated the impact of rounding errors on the integration result&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib220" title="" class="ltx_ref">220</a>]</cite>.
More recently, quantization has been important in digital signal processing, as the process of representing a signal in digital form ordinarily involves rounding, as well as in numerical analysis and the implementation of numerical algorithms, where computations on real-valued numbers are implemented with finite-precision arithmetic.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">It was not until 1948, around the advent of the digital computer, when Shannon wrote his seminal paper on the mathematical theory of communication&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib215" title="" class="ltx_ref">215</a>]</cite>, that the effect of quantization and its use in coding theory were formally presented.
In particular, Shannon argued in his lossless coding theory that using the same number of bits is wasteful, when events of interest have a non-uniform probability.
He argued that a more optimal approach would be to vary the number of bits based on the probability of an event, a concept that is now known as <em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">variable-rate quantization</em>.
Huffman coding in particular is motivated by this&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>.
In subsequent work in 1959&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib216" title="" class="ltx_ref">216</a>]</cite>, Shannon introduced distortion-rate functions (which provide a lower bound on the signal distortion after coding) as well as the notion of vector quantization (also briefly discussed in&nbsp;Section&nbsp;<a href="#S4.SS6" title="IV-F Vector Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-F</span></span></a>).
This concept was extended and became practical in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite> for real communication applications.
Other important historical research on quantization in signal processing in that time period includes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite>, which introduced
the Pulse Code Modulation (PCM) concept (a pulsing method proposed to
approximate/represent/encode sampled analog signals),
as well as the classical result of high resolution quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
We refer the interested reader to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> for a detailed discussion of these issues.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.7" class="ltx_p">Quantization appears in a slightly different way in algorithms that use numerical approximation for problems involving continuous mathematical quantities, an area that also has a long history, but that also received renewed interest with the advent of the digital computer.
In numerical analysis, an important notion was (and still is) that of a <em id="S2.p3.7.1" class="ltx_emph ltx_font_italic">well-posed problem</em>—roughly, a problem is well-posed if: a solution exists; that solution is unique; and that solution depends continuously on the input data in some reasonable topology.
Such problems are sometimes called <em id="S2.p3.7.2" class="ltx_emph ltx_font_italic">well-conditioned problems</em>.
It turned out that, even when working with a given well-conditioned problem, certain algorithms that solve that problem “exactly” in some idealized sense perform very poorly in the presence of “noise” introduced by the peculiarities of roundoff and truncation errors.
These roundoff errors have to do with representing real numbers with only finitely-many bits—a quantization specified, e.g., by the IEEE floating point standard; and truncation errors arise since only a finite number of iterations of an iterative algorithm can actually be performed.
The latter are important even in “exact arithmetic,” since most problems of continuous mathematics cannot even in principle be solved by a finite sequence of elementary operations; but the former have to do with quantization.
These issues led to the notion of the <em id="S2.p3.7.3" class="ltx_emph ltx_font_italic">numerical stability</em> of an algorithm.
Let us view a numerical algorithm as a function <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">f</annotation></semantics></math> attempting to map the input data <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.p3.2.m2.1a"><mi id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">x</annotation></semantics></math> to the “true” solution <math id="S2.p3.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.p3.3.m3.1a"><mi id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">y</annotation></semantics></math>; but due to roundoff and truncation errors, the output of the algorithm is actually some other <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="y^{*}" display="inline"><semantics id="S2.p3.4.m4.1a"><msup id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml"><mi id="S2.p3.4.m4.1.1.2" xref="S2.p3.4.m4.1.1.2.cmml">y</mi><mo id="S2.p3.4.m4.1.1.3" xref="S2.p3.4.m4.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><apply id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p3.4.m4.1.1.1.cmml" xref="S2.p3.4.m4.1.1">superscript</csymbol><ci id="S2.p3.4.m4.1.1.2.cmml" xref="S2.p3.4.m4.1.1.2">𝑦</ci><times id="S2.p3.4.m4.1.1.3.cmml" xref="S2.p3.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">y^{*}</annotation></semantics></math>.
In this case, the <em id="S2.p3.7.4" class="ltx_emph ltx_font_italic">forward error</em> of the algorithm is <math id="S2.p3.5.m5.1" class="ltx_Math" alttext="\Delta y=y^{*}-y" display="inline"><semantics id="S2.p3.5.m5.1a"><mrow id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml"><mrow id="S2.p3.5.m5.1.1.2" xref="S2.p3.5.m5.1.1.2.cmml"><mi mathvariant="normal" id="S2.p3.5.m5.1.1.2.2" xref="S2.p3.5.m5.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S2.p3.5.m5.1.1.2.1" xref="S2.p3.5.m5.1.1.2.1.cmml">​</mo><mi id="S2.p3.5.m5.1.1.2.3" xref="S2.p3.5.m5.1.1.2.3.cmml">y</mi></mrow><mo id="S2.p3.5.m5.1.1.1" xref="S2.p3.5.m5.1.1.1.cmml">=</mo><mrow id="S2.p3.5.m5.1.1.3" xref="S2.p3.5.m5.1.1.3.cmml"><msup id="S2.p3.5.m5.1.1.3.2" xref="S2.p3.5.m5.1.1.3.2.cmml"><mi id="S2.p3.5.m5.1.1.3.2.2" xref="S2.p3.5.m5.1.1.3.2.2.cmml">y</mi><mo id="S2.p3.5.m5.1.1.3.2.3" xref="S2.p3.5.m5.1.1.3.2.3.cmml">∗</mo></msup><mo id="S2.p3.5.m5.1.1.3.1" xref="S2.p3.5.m5.1.1.3.1.cmml">−</mo><mi id="S2.p3.5.m5.1.1.3.3" xref="S2.p3.5.m5.1.1.3.3.cmml">y</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><apply id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1"><eq id="S2.p3.5.m5.1.1.1.cmml" xref="S2.p3.5.m5.1.1.1"></eq><apply id="S2.p3.5.m5.1.1.2.cmml" xref="S2.p3.5.m5.1.1.2"><times id="S2.p3.5.m5.1.1.2.1.cmml" xref="S2.p3.5.m5.1.1.2.1"></times><ci id="S2.p3.5.m5.1.1.2.2.cmml" xref="S2.p3.5.m5.1.1.2.2">Δ</ci><ci id="S2.p3.5.m5.1.1.2.3.cmml" xref="S2.p3.5.m5.1.1.2.3">𝑦</ci></apply><apply id="S2.p3.5.m5.1.1.3.cmml" xref="S2.p3.5.m5.1.1.3"><minus id="S2.p3.5.m5.1.1.3.1.cmml" xref="S2.p3.5.m5.1.1.3.1"></minus><apply id="S2.p3.5.m5.1.1.3.2.cmml" xref="S2.p3.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="S2.p3.5.m5.1.1.3.2.1.cmml" xref="S2.p3.5.m5.1.1.3.2">superscript</csymbol><ci id="S2.p3.5.m5.1.1.3.2.2.cmml" xref="S2.p3.5.m5.1.1.3.2.2">𝑦</ci><times id="S2.p3.5.m5.1.1.3.2.3.cmml" xref="S2.p3.5.m5.1.1.3.2.3"></times></apply><ci id="S2.p3.5.m5.1.1.3.3.cmml" xref="S2.p3.5.m5.1.1.3.3">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">\Delta y=y^{*}-y</annotation></semantics></math>; and the <em id="S2.p3.7.5" class="ltx_emph ltx_font_italic">backward error</em> of the algorithm is the smallest <math id="S2.p3.6.m6.1" class="ltx_Math" alttext="\Delta x" display="inline"><semantics id="S2.p3.6.m6.1a"><mrow id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml"><mi mathvariant="normal" id="S2.p3.6.m6.1.1.2" xref="S2.p3.6.m6.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S2.p3.6.m6.1.1.1" xref="S2.p3.6.m6.1.1.1.cmml">​</mo><mi id="S2.p3.6.m6.1.1.3" xref="S2.p3.6.m6.1.1.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><apply id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1"><times id="S2.p3.6.m6.1.1.1.cmml" xref="S2.p3.6.m6.1.1.1"></times><ci id="S2.p3.6.m6.1.1.2.cmml" xref="S2.p3.6.m6.1.1.2">Δ</ci><ci id="S2.p3.6.m6.1.1.3.cmml" xref="S2.p3.6.m6.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">\Delta x</annotation></semantics></math> such that <math id="S2.p3.7.m7.1" class="ltx_Math" alttext="f(x+\Delta x)=y^{*}" display="inline"><semantics id="S2.p3.7.m7.1a"><mrow id="S2.p3.7.m7.1.1" xref="S2.p3.7.m7.1.1.cmml"><mrow id="S2.p3.7.m7.1.1.1" xref="S2.p3.7.m7.1.1.1.cmml"><mi id="S2.p3.7.m7.1.1.1.3" xref="S2.p3.7.m7.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.p3.7.m7.1.1.1.2" xref="S2.p3.7.m7.1.1.1.2.cmml">​</mo><mrow id="S2.p3.7.m7.1.1.1.1.1" xref="S2.p3.7.m7.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.p3.7.m7.1.1.1.1.1.2" xref="S2.p3.7.m7.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.p3.7.m7.1.1.1.1.1.1" xref="S2.p3.7.m7.1.1.1.1.1.1.cmml"><mi id="S2.p3.7.m7.1.1.1.1.1.1.2" xref="S2.p3.7.m7.1.1.1.1.1.1.2.cmml">x</mi><mo id="S2.p3.7.m7.1.1.1.1.1.1.1" xref="S2.p3.7.m7.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.p3.7.m7.1.1.1.1.1.1.3" xref="S2.p3.7.m7.1.1.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S2.p3.7.m7.1.1.1.1.1.1.3.2" xref="S2.p3.7.m7.1.1.1.1.1.1.3.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S2.p3.7.m7.1.1.1.1.1.1.3.1" xref="S2.p3.7.m7.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.p3.7.m7.1.1.1.1.1.1.3.3" xref="S2.p3.7.m7.1.1.1.1.1.1.3.3.cmml">x</mi></mrow></mrow><mo stretchy="false" id="S2.p3.7.m7.1.1.1.1.1.3" xref="S2.p3.7.m7.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.p3.7.m7.1.1.2" xref="S2.p3.7.m7.1.1.2.cmml">=</mo><msup id="S2.p3.7.m7.1.1.3" xref="S2.p3.7.m7.1.1.3.cmml"><mi id="S2.p3.7.m7.1.1.3.2" xref="S2.p3.7.m7.1.1.3.2.cmml">y</mi><mo id="S2.p3.7.m7.1.1.3.3" xref="S2.p3.7.m7.1.1.3.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.7.m7.1b"><apply id="S2.p3.7.m7.1.1.cmml" xref="S2.p3.7.m7.1.1"><eq id="S2.p3.7.m7.1.1.2.cmml" xref="S2.p3.7.m7.1.1.2"></eq><apply id="S2.p3.7.m7.1.1.1.cmml" xref="S2.p3.7.m7.1.1.1"><times id="S2.p3.7.m7.1.1.1.2.cmml" xref="S2.p3.7.m7.1.1.1.2"></times><ci id="S2.p3.7.m7.1.1.1.3.cmml" xref="S2.p3.7.m7.1.1.1.3">𝑓</ci><apply id="S2.p3.7.m7.1.1.1.1.1.1.cmml" xref="S2.p3.7.m7.1.1.1.1.1"><plus id="S2.p3.7.m7.1.1.1.1.1.1.1.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.1"></plus><ci id="S2.p3.7.m7.1.1.1.1.1.1.2.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.2">𝑥</ci><apply id="S2.p3.7.m7.1.1.1.1.1.1.3.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.3"><times id="S2.p3.7.m7.1.1.1.1.1.1.3.1.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.3.1"></times><ci id="S2.p3.7.m7.1.1.1.1.1.1.3.2.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.3.2">Δ</ci><ci id="S2.p3.7.m7.1.1.1.1.1.1.3.3.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.3.3">𝑥</ci></apply></apply></apply><apply id="S2.p3.7.m7.1.1.3.cmml" xref="S2.p3.7.m7.1.1.3"><csymbol cd="ambiguous" id="S2.p3.7.m7.1.1.3.1.cmml" xref="S2.p3.7.m7.1.1.3">superscript</csymbol><ci id="S2.p3.7.m7.1.1.3.2.cmml" xref="S2.p3.7.m7.1.1.3.2">𝑦</ci><times id="S2.p3.7.m7.1.1.3.3.cmml" xref="S2.p3.7.m7.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m7.1c">f(x+\Delta x)=y^{*}</annotation></semantics></math>.
Thus, the forward error tells us the difference between the exact or true answer and what was output by the algorithm; and the backward error tells us what input data the algorithm we ran actually solved exactly.
The forward error and backward error for an algorithm are related by the condition number of the problem.
We refer the interested reader to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib237" title="" class="ltx_ref">237</a>]</cite> for a detailed discussion of these issues.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Quantization in Neural Nets</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">No doubt thousands of papers have been written on these topics, and one might wonder: how is recent work on NN quantization different from these earlier works?
Certainly, many of the recently proposed “novel algorithms” have strong connections with (and in some cases are essentially rediscoveries of) past work in the literature.
However, NNs bring unique challenges and opportunities to the problem of quantization.
First, inference and training of Neural Nets are both computationally intensive. So, the efficient representation of numerical values is particularly important.
Second, most current Neural Net models are heavily over-parameterized, so there is ample opportunity for reducing bit precision without impacting accuracy.
However, one very important difference is that NNs are very robust to aggressive quantization and extreme discretization.
The new degree of freedom here has to do with the number of parameters involved, i.e., that we are working with over-parameterized models.
This has direct implications for whether we are solving well-posed problems, whether we are interested in forward error or backward error, etc.
In the NN applications driving recent developments in quantization, there is not a single well-posed or well-conditioned problem that is being solved.
Instead, one is interested in some sort of forward error metric (based on classification quality, perplexity, etc.), but due to the over-parameterization, there are many very different models that exactly or approximately optimize this metric.
Thus, it is possible to have high error/distance between a quantized model and the original non-quantized model, while still attaining very good generalization performance. This added degree of freedom was not present in many of the classical research, which mostly focused on finding compression methods that would not change the signal too much, or with numerical methods in which there was strong control on the difference between the “exact” versus the “discretized” computation.
This observation that has been the main driver for researching <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">novel</em> techniques for NN&nbsp;quantization.
Finally,the layered structure of Neural Net models offers an additional dimension to explore. Different layers in a Neural Net have different impact on the loss function, and this motivates a mixed-precision approach to quantization.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2103.13630/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="111" height="100" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2103.13630/assets/x2.png" id="S2.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="111" height="100" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison between uniform quantization (left) and non-uniform quantization (right).
Real values in the continuous domain <math id="S2.F1.3.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S2.F1.3.m1.1b"><mi id="S2.F1.3.m1.1.1" xref="S2.F1.3.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.F1.3.m1.1c"><ci id="S2.F1.3.m1.1.1.cmml" xref="S2.F1.3.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.3.m1.1d">r</annotation></semantics></math> are mapped into discrete, lower precision values in the quantized domain <math id="S2.F1.4.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.F1.4.m2.1b"><mi id="S2.F1.4.m2.1.1" xref="S2.F1.4.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.F1.4.m2.1c"><ci id="S2.F1.4.m2.1.1.cmml" xref="S2.F1.4.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.4.m2.1d">Q</annotation></semantics></math>, which are marked with the orange bullets.
Note that the distances between the quantized values (quantization levels) are the same in uniform quantization, whereas they can vary in non-uniform quantization.
</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Basic Concepts of Quantization</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first briefly introduce common notations and the problem setup
in&nbsp;Section&nbsp;<a href="#S3.SS1" title="III-A Problem Setup and Notations ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>,
and then we describe the basic quantization concepts and methods in&nbsp;Section&nbsp;<a href="#S3.SS2" title="III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>-<a href="#S3.SS6" title="III-F Non-Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-F</span></span></a>.
Afterwards, we discuss the different fine-tuning methods in&nbsp;Section&nbsp;<a href="#S3.SS7" title="III-G Fine-tuning Methods ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-G</span></span></a>, followed
by stochastic quantization in&nbsp;Section&nbsp;<a href="#S3.SS8" title="III-H Stochastic Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-H</span></span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Problem Setup and Notations</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">Assume that the NN has <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">L</annotation></semantics></math> layers with learnable parameters, denoted as <math id="S3.SS1.p1.2.m2.4" class="ltx_Math" alttext="\{W_{1},W_{2},...,W_{L}\}" display="inline"><semantics id="S3.SS1.p1.2.m2.4a"><mrow id="S3.SS1.p1.2.m2.4.4.3" xref="S3.SS1.p1.2.m2.4.4.4.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.4.4.3.4" xref="S3.SS1.p1.2.m2.4.4.4.cmml">{</mo><msub id="S3.SS1.p1.2.m2.2.2.1.1" xref="S3.SS1.p1.2.m2.2.2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.2.2.1.1.2" xref="S3.SS1.p1.2.m2.2.2.1.1.2.cmml">W</mi><mn id="S3.SS1.p1.2.m2.2.2.1.1.3" xref="S3.SS1.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.2.m2.4.4.3.5" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.2.m2.3.3.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.cmml"><mi id="S3.SS1.p1.2.m2.3.3.2.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.2.cmml">W</mi><mn id="S3.SS1.p1.2.m2.3.3.2.2.3" xref="S3.SS1.p1.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.2.m2.4.4.3.6" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">…</mi><mo id="S3.SS1.p1.2.m2.4.4.3.7" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.2.m2.4.4.3.3" xref="S3.SS1.p1.2.m2.4.4.3.3.cmml"><mi id="S3.SS1.p1.2.m2.4.4.3.3.2" xref="S3.SS1.p1.2.m2.4.4.3.3.2.cmml">W</mi><mi id="S3.SS1.p1.2.m2.4.4.3.3.3" xref="S3.SS1.p1.2.m2.4.4.3.3.3.cmml">L</mi></msub><mo stretchy="false" id="S3.SS1.p1.2.m2.4.4.3.8" xref="S3.SS1.p1.2.m2.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.4b"><set id="S3.SS1.p1.2.m2.4.4.4.cmml" xref="S3.SS1.p1.2.m2.4.4.3"><apply id="S3.SS1.p1.2.m2.2.2.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1.2">𝑊</ci><cn type="integer" id="S3.SS1.p1.2.m2.2.2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.p1.2.m2.3.3.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.2.2.1.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2.2">𝑊</ci><cn type="integer" id="S3.SS1.p1.2.m2.3.3.2.2.3.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">…</ci><apply id="S3.SS1.p1.2.m2.4.4.3.3.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.4.4.3.3.1.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.4.4.3.3.2.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3.2">𝑊</ci><ci id="S3.SS1.p1.2.m2.4.4.3.3.3.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3.3">𝐿</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.4c">\{W_{1},W_{2},...,W_{L}\}</annotation></semantics></math>, with <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\theta</annotation></semantics></math> denoting the
combination of all such parameters.
Without loss of generality, we focus on the supervised learning problem, where the nominal goal is to
optimize the following empirical risk minimization function:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\small\mathcal{L}(\theta)=\frac{1}{N}\sum_{i=1}^{N}l(x_{i},y_{i};\theta)," display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.4" xref="S3.E1.m1.3.3.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E1.m1.3.3.1.1.4.2" xref="S3.E1.m1.3.3.1.1.4.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.4.1" xref="S3.E1.m1.3.3.1.1.4.1.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.4.3.2" xref="S3.E1.m1.3.3.1.1.4.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.4.3.2.1" xref="S3.E1.m1.3.3.1.1.4.cmml">(</mo><mi mathsize="90%" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">θ</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.4.3.2.2" xref="S3.E1.m1.3.3.1.1.4.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml"><mfrac id="S3.E1.m1.3.3.1.1.2.4" xref="S3.E1.m1.3.3.1.1.2.4.cmml"><mn mathsize="90%" id="S3.E1.m1.3.3.1.1.2.4.2" xref="S3.E1.m1.3.3.1.1.2.4.2.cmml">1</mn><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.4.3" xref="S3.E1.m1.3.3.1.1.2.4.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.2.3" xref="S3.E1.m1.3.3.1.1.2.3.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.2.cmml"><munderover id="S3.E1.m1.3.3.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" stretchy="true" id="S3.E1.m1.3.3.1.1.2.2.3.2.2" xref="S3.E1.m1.3.3.1.1.2.2.3.2.2.cmml">∑</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.3.2.3" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.3.2.3.2" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.2.cmml">i</mi><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.3.2.3.1" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.1.cmml">=</mo><mn mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.3.2.3.3" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.3.3" xref="S3.E1.m1.3.3.1.1.2.2.3.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.3.3.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.4" xref="S3.E1.m1.3.3.1.1.2.2.2.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.2.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml">(</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.4" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml">,</mo><msub id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.2.cmml">y</mi><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.5" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml">;</mo><mi mathsize="90%" id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">θ</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.6" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo mathsize="90%" id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"></eq><apply id="S3.E1.m1.3.3.1.1.4.cmml" xref="S3.E1.m1.3.3.1.1.4"><times id="S3.E1.m1.3.3.1.1.4.1.cmml" xref="S3.E1.m1.3.3.1.1.4.1"></times><ci id="S3.E1.m1.3.3.1.1.4.2.cmml" xref="S3.E1.m1.3.3.1.1.4.2">ℒ</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝜃</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"><times id="S3.E1.m1.3.3.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.3"></times><apply id="S3.E1.m1.3.3.1.1.2.4.cmml" xref="S3.E1.m1.3.3.1.1.2.4"><divide id="S3.E1.m1.3.3.1.1.2.4.1.cmml" xref="S3.E1.m1.3.3.1.1.2.4"></divide><cn type="integer" id="S3.E1.m1.3.3.1.1.2.4.2.cmml" xref="S3.E1.m1.3.3.1.1.2.4.2">1</cn><ci id="S3.E1.m1.3.3.1.1.2.4.3.cmml" xref="S3.E1.m1.3.3.1.1.2.4.3">𝑁</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2"><apply id="S3.E1.m1.3.3.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3">superscript</csymbol><apply id="S3.E1.m1.3.3.1.1.2.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3">subscript</csymbol><sum id="S3.E1.m1.3.3.1.1.2.2.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2.2"></sum><apply id="S3.E1.m1.3.3.1.1.2.2.3.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3"><eq id="S3.E1.m1.3.3.1.1.2.2.3.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.1"></eq><ci id="S3.E1.m1.3.3.1.1.2.2.3.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.2.2.3.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.3.3.1.1.2.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.3">𝑁</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2"><times id="S3.E1.m1.3.3.1.1.2.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.3"></times><ci id="S3.E1.m1.3.3.1.1.2.2.2.4.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.4">𝑙</ci><vector id="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2"><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.2">𝑦</ci><ci id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.3">𝑖</ci></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝜃</ci></vector></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\small\mathcal{L}(\theta)=\frac{1}{N}\sum_{i=1}^{N}l(x_{i},y_{i};\theta),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.12" class="ltx_p">where <math id="S3.SS1.p1.4.m1.2" class="ltx_Math" alttext="(x,y)" display="inline"><semantics id="S3.SS1.p1.4.m1.2a"><mrow id="S3.SS1.p1.4.m1.2.3.2" xref="S3.SS1.p1.4.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m1.2.3.2.1" xref="S3.SS1.p1.4.m1.2.3.1.cmml">(</mo><mi id="S3.SS1.p1.4.m1.1.1" xref="S3.SS1.p1.4.m1.1.1.cmml">x</mi><mo id="S3.SS1.p1.4.m1.2.3.2.2" xref="S3.SS1.p1.4.m1.2.3.1.cmml">,</mo><mi id="S3.SS1.p1.4.m1.2.2" xref="S3.SS1.p1.4.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.p1.4.m1.2.3.2.3" xref="S3.SS1.p1.4.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m1.2b"><interval closure="open" id="S3.SS1.p1.4.m1.2.3.1.cmml" xref="S3.SS1.p1.4.m1.2.3.2"><ci id="S3.SS1.p1.4.m1.1.1.cmml" xref="S3.SS1.p1.4.m1.1.1">𝑥</ci><ci id="S3.SS1.p1.4.m1.2.2.cmml" xref="S3.SS1.p1.4.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m1.2c">(x,y)</annotation></semantics></math> is the input data and the corresponding label, <math id="S3.SS1.p1.5.m2.3" class="ltx_Math" alttext="l(x,y;\theta)" display="inline"><semantics id="S3.SS1.p1.5.m2.3a"><mrow id="S3.SS1.p1.5.m2.3.4" xref="S3.SS1.p1.5.m2.3.4.cmml"><mi id="S3.SS1.p1.5.m2.3.4.2" xref="S3.SS1.p1.5.m2.3.4.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.5.m2.3.4.1" xref="S3.SS1.p1.5.m2.3.4.1.cmml">​</mo><mrow id="S3.SS1.p1.5.m2.3.4.3.2" xref="S3.SS1.p1.5.m2.3.4.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m2.3.4.3.2.1" xref="S3.SS1.p1.5.m2.3.4.3.1.cmml">(</mo><mi id="S3.SS1.p1.5.m2.1.1" xref="S3.SS1.p1.5.m2.1.1.cmml">x</mi><mo id="S3.SS1.p1.5.m2.3.4.3.2.2" xref="S3.SS1.p1.5.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS1.p1.5.m2.2.2" xref="S3.SS1.p1.5.m2.2.2.cmml">y</mi><mo id="S3.SS1.p1.5.m2.3.4.3.2.3" xref="S3.SS1.p1.5.m2.3.4.3.1.cmml">;</mo><mi id="S3.SS1.p1.5.m2.3.3" xref="S3.SS1.p1.5.m2.3.3.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p1.5.m2.3.4.3.2.4" xref="S3.SS1.p1.5.m2.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m2.3b"><apply id="S3.SS1.p1.5.m2.3.4.cmml" xref="S3.SS1.p1.5.m2.3.4"><times id="S3.SS1.p1.5.m2.3.4.1.cmml" xref="S3.SS1.p1.5.m2.3.4.1"></times><ci id="S3.SS1.p1.5.m2.3.4.2.cmml" xref="S3.SS1.p1.5.m2.3.4.2">𝑙</ci><vector id="S3.SS1.p1.5.m2.3.4.3.1.cmml" xref="S3.SS1.p1.5.m2.3.4.3.2"><ci id="S3.SS1.p1.5.m2.1.1.cmml" xref="S3.SS1.p1.5.m2.1.1">𝑥</ci><ci id="S3.SS1.p1.5.m2.2.2.cmml" xref="S3.SS1.p1.5.m2.2.2">𝑦</ci><ci id="S3.SS1.p1.5.m2.3.3.cmml" xref="S3.SS1.p1.5.m2.3.3">𝜃</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m2.3c">l(x,y;\theta)</annotation></semantics></math> is the loss function
(e.g., Mean Squared Error or Cross Entropy loss),
and <math id="S3.SS1.p1.6.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.6.m3.1a"><mi id="S3.SS1.p1.6.m3.1.1" xref="S3.SS1.p1.6.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m3.1b"><ci id="S3.SS1.p1.6.m3.1.1.cmml" xref="S3.SS1.p1.6.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m3.1c">N</annotation></semantics></math> is the total number of data points.
Let us also denote the input hidden activations of the <math id="S3.SS1.p1.7.m4.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S3.SS1.p1.7.m4.1a"><msup id="S3.SS1.p1.7.m4.1.1" xref="S3.SS1.p1.7.m4.1.1.cmml"><mi id="S3.SS1.p1.7.m4.1.1.2" xref="S3.SS1.p1.7.m4.1.1.2.cmml">i</mi><mrow id="S3.SS1.p1.7.m4.1.1.3" xref="S3.SS1.p1.7.m4.1.1.3.cmml"><mi id="S3.SS1.p1.7.m4.1.1.3.2" xref="S3.SS1.p1.7.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.7.m4.1.1.3.1" xref="S3.SS1.p1.7.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p1.7.m4.1.1.3.3" xref="S3.SS1.p1.7.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m4.1b"><apply id="S3.SS1.p1.7.m4.1.1.cmml" xref="S3.SS1.p1.7.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m4.1.1.1.cmml" xref="S3.SS1.p1.7.m4.1.1">superscript</csymbol><ci id="S3.SS1.p1.7.m4.1.1.2.cmml" xref="S3.SS1.p1.7.m4.1.1.2">𝑖</ci><apply id="S3.SS1.p1.7.m4.1.1.3.cmml" xref="S3.SS1.p1.7.m4.1.1.3"><times id="S3.SS1.p1.7.m4.1.1.3.1.cmml" xref="S3.SS1.p1.7.m4.1.1.3.1"></times><ci id="S3.SS1.p1.7.m4.1.1.3.2.cmml" xref="S3.SS1.p1.7.m4.1.1.3.2">𝑡</ci><ci id="S3.SS1.p1.7.m4.1.1.3.3.cmml" xref="S3.SS1.p1.7.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m4.1c">i^{th}</annotation></semantics></math> layer as <math id="S3.SS1.p1.8.m5.1" class="ltx_Math" alttext="h_{i}" display="inline"><semantics id="S3.SS1.p1.8.m5.1a"><msub id="S3.SS1.p1.8.m5.1.1" xref="S3.SS1.p1.8.m5.1.1.cmml"><mi id="S3.SS1.p1.8.m5.1.1.2" xref="S3.SS1.p1.8.m5.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.8.m5.1.1.3" xref="S3.SS1.p1.8.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m5.1b"><apply id="S3.SS1.p1.8.m5.1.1.cmml" xref="S3.SS1.p1.8.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m5.1.1.1.cmml" xref="S3.SS1.p1.8.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m5.1.1.2.cmml" xref="S3.SS1.p1.8.m5.1.1.2">ℎ</ci><ci id="S3.SS1.p1.8.m5.1.1.3.cmml" xref="S3.SS1.p1.8.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m5.1c">h_{i}</annotation></semantics></math>,
and the corresponding output hidden activation as <math id="S3.SS1.p1.9.m6.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S3.SS1.p1.9.m6.1a"><msub id="S3.SS1.p1.9.m6.1.1" xref="S3.SS1.p1.9.m6.1.1.cmml"><mi id="S3.SS1.p1.9.m6.1.1.2" xref="S3.SS1.p1.9.m6.1.1.2.cmml">a</mi><mi id="S3.SS1.p1.9.m6.1.1.3" xref="S3.SS1.p1.9.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m6.1b"><apply id="S3.SS1.p1.9.m6.1.1.cmml" xref="S3.SS1.p1.9.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m6.1.1.1.cmml" xref="S3.SS1.p1.9.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.9.m6.1.1.2.cmml" xref="S3.SS1.p1.9.m6.1.1.2">𝑎</ci><ci id="S3.SS1.p1.9.m6.1.1.3.cmml" xref="S3.SS1.p1.9.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m6.1c">a_{i}</annotation></semantics></math>.
We assume that we have the trained model parameters <math id="S3.SS1.p1.10.m7.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.p1.10.m7.1a"><mi id="S3.SS1.p1.10.m7.1.1" xref="S3.SS1.p1.10.m7.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m7.1b"><ci id="S3.SS1.p1.10.m7.1.1.cmml" xref="S3.SS1.p1.10.m7.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m7.1c">\theta</annotation></semantics></math>, stored
in floating point precision.
In quantization, the goal is to reduce the precision of both the parameters (<math id="S3.SS1.p1.11.m8.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.p1.11.m8.1a"><mi id="S3.SS1.p1.11.m8.1.1" xref="S3.SS1.p1.11.m8.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m8.1b"><ci id="S3.SS1.p1.11.m8.1.1.cmml" xref="S3.SS1.p1.11.m8.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m8.1c">\theta</annotation></semantics></math>),
as well as the intermediate activation maps (i.e., <math id="S3.SS1.p1.12.m9.2" class="ltx_Math" alttext="h_{i},\ a_{i}" display="inline"><semantics id="S3.SS1.p1.12.m9.2a"><mrow id="S3.SS1.p1.12.m9.2.2.2" xref="S3.SS1.p1.12.m9.2.2.3.cmml"><msub id="S3.SS1.p1.12.m9.1.1.1.1" xref="S3.SS1.p1.12.m9.1.1.1.1.cmml"><mi id="S3.SS1.p1.12.m9.1.1.1.1.2" xref="S3.SS1.p1.12.m9.1.1.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.12.m9.1.1.1.1.3" xref="S3.SS1.p1.12.m9.1.1.1.1.3.cmml">i</mi></msub><mo rspace="0.667em" id="S3.SS1.p1.12.m9.2.2.2.3" xref="S3.SS1.p1.12.m9.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.12.m9.2.2.2.2" xref="S3.SS1.p1.12.m9.2.2.2.2.cmml"><mi id="S3.SS1.p1.12.m9.2.2.2.2.2" xref="S3.SS1.p1.12.m9.2.2.2.2.2.cmml">a</mi><mi id="S3.SS1.p1.12.m9.2.2.2.2.3" xref="S3.SS1.p1.12.m9.2.2.2.2.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m9.2b"><list id="S3.SS1.p1.12.m9.2.2.3.cmml" xref="S3.SS1.p1.12.m9.2.2.2"><apply id="S3.SS1.p1.12.m9.1.1.1.1.cmml" xref="S3.SS1.p1.12.m9.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.12.m9.1.1.1.1.1.cmml" xref="S3.SS1.p1.12.m9.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.12.m9.1.1.1.1.2.cmml" xref="S3.SS1.p1.12.m9.1.1.1.1.2">ℎ</ci><ci id="S3.SS1.p1.12.m9.1.1.1.1.3.cmml" xref="S3.SS1.p1.12.m9.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p1.12.m9.2.2.2.2.cmml" xref="S3.SS1.p1.12.m9.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.12.m9.2.2.2.2.1.cmml" xref="S3.SS1.p1.12.m9.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.12.m9.2.2.2.2.2.cmml" xref="S3.SS1.p1.12.m9.2.2.2.2.2">𝑎</ci><ci id="S3.SS1.p1.12.m9.2.2.2.2.3.cmml" xref="S3.SS1.p1.12.m9.2.2.2.2.3">𝑖</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m9.2c">h_{i},\ a_{i}</annotation></semantics></math>) to low-precision,
with minimal impact on the generalization power/accuracy of the model.
To do this, we need to define a quantization operator that maps
a floating point value to a quantized one, which is described next.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2103.13630/assets/x3.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="108" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Illustration of symmetric quantization and asymmetric quantization.
Symmetric quantization with restricted range maps real values to [-127, 127], and full range
maps to [-128, 127] for 8-bit quantization.
</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Uniform Quantization</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.11" class="ltx_p">We need first to define a function that can quantize
NN weights and activations to a finite set of values.
This function takes real values in floating point, and it maps
them to a lower precision range, as illustrated in&nbsp;Figure&nbsp;<a href="#S2.F1" title="Figure 1 ‣ II-A Quantization in Neural Nets ‣ II General History of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
A popular choice for a quantization function is as follows:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\small Q(r)=\text{Int}\big{(}{r}/{S}\big{)}-Z," display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><mi mathsize="90%" id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.1" xref="S3.E2.m1.2.2.1.1.3.1.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.3.cmml"><mo maxsize="90%" minsize="90%" id="S3.E2.m1.2.2.1.1.3.3.2.1" xref="S3.E2.m1.2.2.1.1.3.cmml">(</mo><mi mathsize="90%" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">r</mi><mo maxsize="90%" minsize="90%" id="S3.E2.m1.2.2.1.1.3.3.2.2" xref="S3.E2.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml"><mtext mathsize="90%" id="S3.E2.m1.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.3a.cmml">Int</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mo maxsize="120%" minsize="120%" id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">r</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml">/</mo><mi mathsize="90%" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">S</mi></mrow><mo maxsize="120%" minsize="120%" id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.2.cmml">−</mo><mi mathsize="90%" id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml">Z</mi></mrow></mrow><mo mathsize="90%" id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></eq><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><times id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1"></times><ci id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2">𝑄</ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑟</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2"></minus><apply id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2"></times><ci id="S3.E2.m1.2.2.1.1.1.1.3a.cmml" xref="S3.E2.m1.2.2.1.1.1.1.3"><mtext mathsize="90%" id="S3.E2.m1.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.3">Int</mtext></ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><divide id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1"></divide><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2">𝑟</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3">𝑆</ci></apply></apply><ci id="S3.E2.m1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3">𝑍</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\small Q(r)=\text{Int}\big{(}{r}/{S}\big{)}-Z,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.8" class="ltx_p">where <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">Q</annotation></semantics></math> is the quantization operator,
<math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">r</annotation></semantics></math> is a real valued input (activation or weight),
<math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">S</annotation></semantics></math> is a real valued scaling factor,
and <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">Z</annotation></semantics></math> is an integer zero point.
Furthermore, the <span id="S3.SS2.p1.8.1" class="ltx_text ltx_markedasmath">Int</span> function maps a real value to an integer value through a rounding operation (e.g., round to nearest and truncation).
In essence, this function is a mapping from real values <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">r</annotation></semantics></math> to some integer values.
This method of quantization is also known as <span id="S3.SS2.p1.8.2" class="ltx_text ltx_font_italic">uniform quantization</span>, as the resulting quantized values (aka quantization levels) are uniformly spaced (Figure&nbsp;<a href="#S2.F1" title="Figure 1 ‣ II-A Quantization in Neural Nets ‣ II General History of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, left).
There are also <span id="S3.SS2.p1.8.3" class="ltx_text ltx_font_italic">non-uniform quantization</span> methods whose quantized values are not necessarily uniformly spaced (Figure&nbsp;<a href="#S2.F1" title="Figure 1 ‣ II-A Quantization in Neural Nets ‣ II General History of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, right), and these methods will be discussed in more detail in Section&nbsp;<a href="#S3.SS6" title="III-F Non-Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-F</span></span></a>.
It is possible to recover real values <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">r</annotation></semantics></math> from the quantized values <math id="S3.SS2.p1.8.m8.1" class="ltx_Math" alttext="Q(r)" display="inline"><semantics id="S3.SS2.p1.8.m8.1a"><mrow id="S3.SS2.p1.8.m8.1.2" xref="S3.SS2.p1.8.m8.1.2.cmml"><mi id="S3.SS2.p1.8.m8.1.2.2" xref="S3.SS2.p1.8.m8.1.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.8.m8.1.2.1" xref="S3.SS2.p1.8.m8.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.8.m8.1.2.3.2" xref="S3.SS2.p1.8.m8.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.8.m8.1.2.3.2.1" xref="S3.SS2.p1.8.m8.1.2.cmml">(</mo><mi id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS2.p1.8.m8.1.2.3.2.2" xref="S3.SS2.p1.8.m8.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.2.cmml" xref="S3.SS2.p1.8.m8.1.2"><times id="S3.SS2.p1.8.m8.1.2.1.cmml" xref="S3.SS2.p1.8.m8.1.2.1"></times><ci id="S3.SS2.p1.8.m8.1.2.2.cmml" xref="S3.SS2.p1.8.m8.1.2.2">𝑄</ci><ci id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">Q(r)</annotation></semantics></math> through an operation
that is often referred to as <em id="S3.SS2.p1.8.4" class="ltx_emph ltx_font_italic">dequantization</em>:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="\small\tilde{r}=S(Q(r)+Z)." display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><mover accent="true" id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml"><mi mathsize="90%" id="S3.E3.m1.2.2.1.1.3.2" xref="S3.E3.m1.2.2.1.1.3.2.cmml">r</mi><mo mathsize="90%" id="S3.E3.m1.2.2.1.1.3.1" xref="S3.E3.m1.2.2.1.1.3.1.cmml">~</mo></mover><mo mathsize="90%" id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mi mathsize="90%" id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.1.2.3.2.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">r</mi><mo maxsize="90%" minsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.1.2.3.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml">+</mo><mi mathsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml">Z</mi></mrow><mo maxsize="90%" minsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1"><eq id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2"></eq><apply id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"><ci id="S3.E3.m1.2.2.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.3.1">~</ci><ci id="S3.E3.m1.2.2.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.3.2">𝑟</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"><times id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2"></times><ci id="S3.E3.m1.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3">𝑆</ci><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1"><plus id="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1"></plus><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2"><times id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1"></times><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.2">𝑄</ci><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑟</ci></apply><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3">𝑍</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\small\tilde{r}=S(Q(r)+Z).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.10" class="ltx_p">Note that the recovered real values <math id="S3.SS2.p1.9.m1.1" class="ltx_Math" alttext="\tilde{r}" display="inline"><semantics id="S3.SS2.p1.9.m1.1a"><mover accent="true" id="S3.SS2.p1.9.m1.1.1" xref="S3.SS2.p1.9.m1.1.1.cmml"><mi id="S3.SS2.p1.9.m1.1.1.2" xref="S3.SS2.p1.9.m1.1.1.2.cmml">r</mi><mo id="S3.SS2.p1.9.m1.1.1.1" xref="S3.SS2.p1.9.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m1.1b"><apply id="S3.SS2.p1.9.m1.1.1.cmml" xref="S3.SS2.p1.9.m1.1.1"><ci id="S3.SS2.p1.9.m1.1.1.1.cmml" xref="S3.SS2.p1.9.m1.1.1.1">~</ci><ci id="S3.SS2.p1.9.m1.1.1.2.cmml" xref="S3.SS2.p1.9.m1.1.1.2">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m1.1c">\tilde{r}</annotation></semantics></math> will not exactly
match <math id="S3.SS2.p1.10.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p1.10.m2.1a"><mi id="S3.SS2.p1.10.m2.1.1" xref="S3.SS2.p1.10.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m2.1b"><ci id="S3.SS2.p1.10.m2.1.1.cmml" xref="S3.SS2.p1.10.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m2.1c">r</annotation></semantics></math> due to the rounding operation.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2103.13630/assets/x4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="231" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of different quantization granularities.
In layerwise quantization, the same clipping range is applied to all the filters that belong to the same layer.
This can result in bad quantization resolution for the channels that have narrow distributions (e.g., Filter 1 in the figure).
One can achieve better quantization resolution using channelwise quantization that dedicates different clipping ranges to different channels.
</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Symmetric and Asymmetric Quantization</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">One important factor in uniform quantization is the choice of the scaling factor <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">S</annotation></semantics></math> in&nbsp;Eq.&nbsp;<a href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
This scaling factor essentially divides a given range of real values <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">r</annotation></semantics></math> into a number of partitions (as discussed in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>, <a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>):</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\small S=\frac{\beta-\alpha}{2^{b}-1}," display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml">S</mi><mo mathsize="90%" id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mrow id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.3.2.2" xref="S3.E4.m1.1.1.1.1.3.2.2.cmml">β</mi><mo mathsize="90%" id="S3.E4.m1.1.1.1.1.3.2.1" xref="S3.E4.m1.1.1.1.1.3.2.1.cmml">−</mo><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.3.2.3" xref="S3.E4.m1.1.1.1.1.3.2.3.cmml">α</mi></mrow><mrow id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml"><msup id="S3.E4.m1.1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.1.3.3.2.cmml"><mn mathsize="90%" id="S3.E4.m1.1.1.1.1.3.3.2.2" xref="S3.E4.m1.1.1.1.1.3.3.2.2.cmml">2</mn><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.3.3.2.3" xref="S3.E4.m1.1.1.1.1.3.3.2.3.cmml">b</mi></msup><mo mathsize="90%" id="S3.E4.m1.1.1.1.1.3.3.1" xref="S3.E4.m1.1.1.1.1.3.3.1.cmml">−</mo><mn mathsize="90%" id="S3.E4.m1.1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.1.3.3.3.cmml">1</mn></mrow></mfrac></mrow><mo mathsize="90%" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"></eq><ci id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2">𝑆</ci><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><divide id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3"></divide><apply id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2"><minus id="S3.E4.m1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.2.1"></minus><ci id="S3.E4.m1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2.2">𝛽</ci><ci id="S3.E4.m1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3">𝛼</ci></apply><apply id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3"><minus id="S3.E4.m1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1"></minus><apply id="S3.E4.m1.1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2">superscript</csymbol><cn type="integer" id="S3.E4.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2.2">2</cn><ci id="S3.E4.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2.3">𝑏</ci></apply><cn type="integer" id="S3.E4.m1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\small S=\frac{\beta-\alpha}{2^{b}-1},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.11" class="ltx_p">where <math id="S3.SS3.p1.3.m1.2" class="ltx_Math" alttext="[\alpha,\beta]" display="inline"><semantics id="S3.SS3.p1.3.m1.2a"><mrow id="S3.SS3.p1.3.m1.2.3.2" xref="S3.SS3.p1.3.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p1.3.m1.2.3.2.1" xref="S3.SS3.p1.3.m1.2.3.1.cmml">[</mo><mi id="S3.SS3.p1.3.m1.1.1" xref="S3.SS3.p1.3.m1.1.1.cmml">α</mi><mo id="S3.SS3.p1.3.m1.2.3.2.2" xref="S3.SS3.p1.3.m1.2.3.1.cmml">,</mo><mi id="S3.SS3.p1.3.m1.2.2" xref="S3.SS3.p1.3.m1.2.2.cmml">β</mi><mo stretchy="false" id="S3.SS3.p1.3.m1.2.3.2.3" xref="S3.SS3.p1.3.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m1.2b"><interval closure="closed" id="S3.SS3.p1.3.m1.2.3.1.cmml" xref="S3.SS3.p1.3.m1.2.3.2"><ci id="S3.SS3.p1.3.m1.1.1.cmml" xref="S3.SS3.p1.3.m1.1.1">𝛼</ci><ci id="S3.SS3.p1.3.m1.2.2.cmml" xref="S3.SS3.p1.3.m1.2.2">𝛽</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m1.2c">[\alpha,\beta]</annotation></semantics></math> denotes the clipping range, a bounded range that we are clipping the real values with, and <math id="S3.SS3.p1.4.m2.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS3.p1.4.m2.1a"><mi id="S3.SS3.p1.4.m2.1.1" xref="S3.SS3.p1.4.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m2.1b"><ci id="S3.SS3.p1.4.m2.1.1.cmml" xref="S3.SS3.p1.4.m2.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m2.1c">b</annotation></semantics></math> is the quantization bit width.
Therefore, in order for the scaling factor to be defined,
the clipping range <math id="S3.SS3.p1.5.m3.2" class="ltx_Math" alttext="[\alpha,\beta]" display="inline"><semantics id="S3.SS3.p1.5.m3.2a"><mrow id="S3.SS3.p1.5.m3.2.3.2" xref="S3.SS3.p1.5.m3.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p1.5.m3.2.3.2.1" xref="S3.SS3.p1.5.m3.2.3.1.cmml">[</mo><mi id="S3.SS3.p1.5.m3.1.1" xref="S3.SS3.p1.5.m3.1.1.cmml">α</mi><mo id="S3.SS3.p1.5.m3.2.3.2.2" xref="S3.SS3.p1.5.m3.2.3.1.cmml">,</mo><mi id="S3.SS3.p1.5.m3.2.2" xref="S3.SS3.p1.5.m3.2.2.cmml">β</mi><mo stretchy="false" id="S3.SS3.p1.5.m3.2.3.2.3" xref="S3.SS3.p1.5.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m3.2b"><interval closure="closed" id="S3.SS3.p1.5.m3.2.3.1.cmml" xref="S3.SS3.p1.5.m3.2.3.2"><ci id="S3.SS3.p1.5.m3.1.1.cmml" xref="S3.SS3.p1.5.m3.1.1">𝛼</ci><ci id="S3.SS3.p1.5.m3.2.2.cmml" xref="S3.SS3.p1.5.m3.2.2">𝛽</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m3.2c">[\alpha,\beta]</annotation></semantics></math> should first be determined.
The process of choosing the clipping range is often referred to as <span id="S3.SS3.p1.11.1" class="ltx_text ltx_font_italic">calibration</span>.
A straightforward choice is to use the min/max of the signal for the clipping range, i.e., <math id="S3.SS3.p1.6.m4.1" class="ltx_Math" alttext="\alpha=r_{min}" display="inline"><semantics id="S3.SS3.p1.6.m4.1a"><mrow id="S3.SS3.p1.6.m4.1.1" xref="S3.SS3.p1.6.m4.1.1.cmml"><mi id="S3.SS3.p1.6.m4.1.1.2" xref="S3.SS3.p1.6.m4.1.1.2.cmml">α</mi><mo id="S3.SS3.p1.6.m4.1.1.1" xref="S3.SS3.p1.6.m4.1.1.1.cmml">=</mo><msub id="S3.SS3.p1.6.m4.1.1.3" xref="S3.SS3.p1.6.m4.1.1.3.cmml"><mi id="S3.SS3.p1.6.m4.1.1.3.2" xref="S3.SS3.p1.6.m4.1.1.3.2.cmml">r</mi><mrow id="S3.SS3.p1.6.m4.1.1.3.3" xref="S3.SS3.p1.6.m4.1.1.3.3.cmml"><mi id="S3.SS3.p1.6.m4.1.1.3.3.2" xref="S3.SS3.p1.6.m4.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.6.m4.1.1.3.3.1" xref="S3.SS3.p1.6.m4.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.6.m4.1.1.3.3.3" xref="S3.SS3.p1.6.m4.1.1.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.6.m4.1.1.3.3.1a" xref="S3.SS3.p1.6.m4.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.6.m4.1.1.3.3.4" xref="S3.SS3.p1.6.m4.1.1.3.3.4.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m4.1b"><apply id="S3.SS3.p1.6.m4.1.1.cmml" xref="S3.SS3.p1.6.m4.1.1"><eq id="S3.SS3.p1.6.m4.1.1.1.cmml" xref="S3.SS3.p1.6.m4.1.1.1"></eq><ci id="S3.SS3.p1.6.m4.1.1.2.cmml" xref="S3.SS3.p1.6.m4.1.1.2">𝛼</ci><apply id="S3.SS3.p1.6.m4.1.1.3.cmml" xref="S3.SS3.p1.6.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m4.1.1.3.1.cmml" xref="S3.SS3.p1.6.m4.1.1.3">subscript</csymbol><ci id="S3.SS3.p1.6.m4.1.1.3.2.cmml" xref="S3.SS3.p1.6.m4.1.1.3.2">𝑟</ci><apply id="S3.SS3.p1.6.m4.1.1.3.3.cmml" xref="S3.SS3.p1.6.m4.1.1.3.3"><times id="S3.SS3.p1.6.m4.1.1.3.3.1.cmml" xref="S3.SS3.p1.6.m4.1.1.3.3.1"></times><ci id="S3.SS3.p1.6.m4.1.1.3.3.2.cmml" xref="S3.SS3.p1.6.m4.1.1.3.3.2">𝑚</ci><ci id="S3.SS3.p1.6.m4.1.1.3.3.3.cmml" xref="S3.SS3.p1.6.m4.1.1.3.3.3">𝑖</ci><ci id="S3.SS3.p1.6.m4.1.1.3.3.4.cmml" xref="S3.SS3.p1.6.m4.1.1.3.3.4">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m4.1c">\alpha=r_{min}</annotation></semantics></math>, and <math id="S3.SS3.p1.7.m5.1" class="ltx_Math" alttext="\beta=r_{max}" display="inline"><semantics id="S3.SS3.p1.7.m5.1a"><mrow id="S3.SS3.p1.7.m5.1.1" xref="S3.SS3.p1.7.m5.1.1.cmml"><mi id="S3.SS3.p1.7.m5.1.1.2" xref="S3.SS3.p1.7.m5.1.1.2.cmml">β</mi><mo id="S3.SS3.p1.7.m5.1.1.1" xref="S3.SS3.p1.7.m5.1.1.1.cmml">=</mo><msub id="S3.SS3.p1.7.m5.1.1.3" xref="S3.SS3.p1.7.m5.1.1.3.cmml"><mi id="S3.SS3.p1.7.m5.1.1.3.2" xref="S3.SS3.p1.7.m5.1.1.3.2.cmml">r</mi><mrow id="S3.SS3.p1.7.m5.1.1.3.3" xref="S3.SS3.p1.7.m5.1.1.3.3.cmml"><mi id="S3.SS3.p1.7.m5.1.1.3.3.2" xref="S3.SS3.p1.7.m5.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.7.m5.1.1.3.3.1" xref="S3.SS3.p1.7.m5.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.7.m5.1.1.3.3.3" xref="S3.SS3.p1.7.m5.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.7.m5.1.1.3.3.1a" xref="S3.SS3.p1.7.m5.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.7.m5.1.1.3.3.4" xref="S3.SS3.p1.7.m5.1.1.3.3.4.cmml">x</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m5.1b"><apply id="S3.SS3.p1.7.m5.1.1.cmml" xref="S3.SS3.p1.7.m5.1.1"><eq id="S3.SS3.p1.7.m5.1.1.1.cmml" xref="S3.SS3.p1.7.m5.1.1.1"></eq><ci id="S3.SS3.p1.7.m5.1.1.2.cmml" xref="S3.SS3.p1.7.m5.1.1.2">𝛽</ci><apply id="S3.SS3.p1.7.m5.1.1.3.cmml" xref="S3.SS3.p1.7.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m5.1.1.3.1.cmml" xref="S3.SS3.p1.7.m5.1.1.3">subscript</csymbol><ci id="S3.SS3.p1.7.m5.1.1.3.2.cmml" xref="S3.SS3.p1.7.m5.1.1.3.2">𝑟</ci><apply id="S3.SS3.p1.7.m5.1.1.3.3.cmml" xref="S3.SS3.p1.7.m5.1.1.3.3"><times id="S3.SS3.p1.7.m5.1.1.3.3.1.cmml" xref="S3.SS3.p1.7.m5.1.1.3.3.1"></times><ci id="S3.SS3.p1.7.m5.1.1.3.3.2.cmml" xref="S3.SS3.p1.7.m5.1.1.3.3.2">𝑚</ci><ci id="S3.SS3.p1.7.m5.1.1.3.3.3.cmml" xref="S3.SS3.p1.7.m5.1.1.3.3.3">𝑎</ci><ci id="S3.SS3.p1.7.m5.1.1.3.3.4.cmml" xref="S3.SS3.p1.7.m5.1.1.3.3.4">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m5.1c">\beta=r_{max}</annotation></semantics></math>.
This approach is an <em id="S3.SS3.p1.11.2" class="ltx_emph ltx_font_italic">asymmetric quantization</em> scheme, since the clipping range is not necessarily symmetric with respect to the origin, i.e., <math id="S3.SS3.p1.8.m6.1" class="ltx_Math" alttext="-\alpha\neq\beta" display="inline"><semantics id="S3.SS3.p1.8.m6.1a"><mrow id="S3.SS3.p1.8.m6.1.1" xref="S3.SS3.p1.8.m6.1.1.cmml"><mrow id="S3.SS3.p1.8.m6.1.1.2" xref="S3.SS3.p1.8.m6.1.1.2.cmml"><mo id="S3.SS3.p1.8.m6.1.1.2a" xref="S3.SS3.p1.8.m6.1.1.2.cmml">−</mo><mi id="S3.SS3.p1.8.m6.1.1.2.2" xref="S3.SS3.p1.8.m6.1.1.2.2.cmml">α</mi></mrow><mo id="S3.SS3.p1.8.m6.1.1.1" xref="S3.SS3.p1.8.m6.1.1.1.cmml">≠</mo><mi id="S3.SS3.p1.8.m6.1.1.3" xref="S3.SS3.p1.8.m6.1.1.3.cmml">β</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m6.1b"><apply id="S3.SS3.p1.8.m6.1.1.cmml" xref="S3.SS3.p1.8.m6.1.1"><neq id="S3.SS3.p1.8.m6.1.1.1.cmml" xref="S3.SS3.p1.8.m6.1.1.1"></neq><apply id="S3.SS3.p1.8.m6.1.1.2.cmml" xref="S3.SS3.p1.8.m6.1.1.2"><minus id="S3.SS3.p1.8.m6.1.1.2.1.cmml" xref="S3.SS3.p1.8.m6.1.1.2"></minus><ci id="S3.SS3.p1.8.m6.1.1.2.2.cmml" xref="S3.SS3.p1.8.m6.1.1.2.2">𝛼</ci></apply><ci id="S3.SS3.p1.8.m6.1.1.3.cmml" xref="S3.SS3.p1.8.m6.1.1.3">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m6.1c">-\alpha\neq\beta</annotation></semantics></math>, as illustrated in&nbsp;Figure&nbsp;<a href="#S3.F2" title="Figure 2 ‣ III-A Problem Setup and Notations ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (Right).
It is also possible to use a <em id="S3.SS3.p1.11.3" class="ltx_emph ltx_font_italic">symmetric quantization</em> scheme by choosing
a symmetric clipping range of <math id="S3.SS3.p1.9.m7.1" class="ltx_Math" alttext="\alpha=-\beta" display="inline"><semantics id="S3.SS3.p1.9.m7.1a"><mrow id="S3.SS3.p1.9.m7.1.1" xref="S3.SS3.p1.9.m7.1.1.cmml"><mi id="S3.SS3.p1.9.m7.1.1.2" xref="S3.SS3.p1.9.m7.1.1.2.cmml">α</mi><mo id="S3.SS3.p1.9.m7.1.1.1" xref="S3.SS3.p1.9.m7.1.1.1.cmml">=</mo><mrow id="S3.SS3.p1.9.m7.1.1.3" xref="S3.SS3.p1.9.m7.1.1.3.cmml"><mo id="S3.SS3.p1.9.m7.1.1.3a" xref="S3.SS3.p1.9.m7.1.1.3.cmml">−</mo><mi id="S3.SS3.p1.9.m7.1.1.3.2" xref="S3.SS3.p1.9.m7.1.1.3.2.cmml">β</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m7.1b"><apply id="S3.SS3.p1.9.m7.1.1.cmml" xref="S3.SS3.p1.9.m7.1.1"><eq id="S3.SS3.p1.9.m7.1.1.1.cmml" xref="S3.SS3.p1.9.m7.1.1.1"></eq><ci id="S3.SS3.p1.9.m7.1.1.2.cmml" xref="S3.SS3.p1.9.m7.1.1.2">𝛼</ci><apply id="S3.SS3.p1.9.m7.1.1.3.cmml" xref="S3.SS3.p1.9.m7.1.1.3"><minus id="S3.SS3.p1.9.m7.1.1.3.1.cmml" xref="S3.SS3.p1.9.m7.1.1.3"></minus><ci id="S3.SS3.p1.9.m7.1.1.3.2.cmml" xref="S3.SS3.p1.9.m7.1.1.3.2">𝛽</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m7.1c">\alpha=-\beta</annotation></semantics></math>.
A popular choice is to choose these
based on the min/max values of the signal:
<math id="S3.SS3.p1.10.m8.3" class="ltx_Math" alttext="-\alpha=\beta={\max(|r_{max}|,|r_{min}|)}" display="inline"><semantics id="S3.SS3.p1.10.m8.3a"><mrow id="S3.SS3.p1.10.m8.3.3" xref="S3.SS3.p1.10.m8.3.3.cmml"><mrow id="S3.SS3.p1.10.m8.3.3.4" xref="S3.SS3.p1.10.m8.3.3.4.cmml"><mo id="S3.SS3.p1.10.m8.3.3.4a" xref="S3.SS3.p1.10.m8.3.3.4.cmml">−</mo><mi id="S3.SS3.p1.10.m8.3.3.4.2" xref="S3.SS3.p1.10.m8.3.3.4.2.cmml">α</mi></mrow><mo id="S3.SS3.p1.10.m8.3.3.5" xref="S3.SS3.p1.10.m8.3.3.5.cmml">=</mo><mi id="S3.SS3.p1.10.m8.3.3.6" xref="S3.SS3.p1.10.m8.3.3.6.cmml">β</mi><mo id="S3.SS3.p1.10.m8.3.3.7" xref="S3.SS3.p1.10.m8.3.3.7.cmml">=</mo><mrow id="S3.SS3.p1.10.m8.3.3.2.2" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml"><mi id="S3.SS3.p1.10.m8.1.1" xref="S3.SS3.p1.10.m8.1.1.cmml">max</mi><mo id="S3.SS3.p1.10.m8.3.3.2.2a" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml">⁡</mo><mrow id="S3.SS3.p1.10.m8.3.3.2.2.2" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml"><mo stretchy="false" id="S3.SS3.p1.10.m8.3.3.2.2.2.3" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml">(</mo><mrow id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.2" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.2.1.cmml">|</mo><msub id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.2" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.2.cmml">r</mi><mrow id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.2" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.3" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1a" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.4" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.4.cmml">x</mi></mrow></msub><mo stretchy="false" id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.3" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S3.SS3.p1.10.m8.3.3.2.2.2.4" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml">,</mo><mrow id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.2" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.2.1.cmml">|</mo><msub id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.cmml"><mi id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.2" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.2.cmml">r</mi><mrow id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.cmml"><mi id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.2" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.3" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1a" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.4" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.4.cmml">n</mi></mrow></msub><mo stretchy="false" id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.3" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.2.1.cmml">|</mo></mrow><mo stretchy="false" id="S3.SS3.p1.10.m8.3.3.2.2.2.5" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m8.3b"><apply id="S3.SS3.p1.10.m8.3.3.cmml" xref="S3.SS3.p1.10.m8.3.3"><and id="S3.SS3.p1.10.m8.3.3a.cmml" xref="S3.SS3.p1.10.m8.3.3"></and><apply id="S3.SS3.p1.10.m8.3.3b.cmml" xref="S3.SS3.p1.10.m8.3.3"><eq id="S3.SS3.p1.10.m8.3.3.5.cmml" xref="S3.SS3.p1.10.m8.3.3.5"></eq><apply id="S3.SS3.p1.10.m8.3.3.4.cmml" xref="S3.SS3.p1.10.m8.3.3.4"><minus id="S3.SS3.p1.10.m8.3.3.4.1.cmml" xref="S3.SS3.p1.10.m8.3.3.4"></minus><ci id="S3.SS3.p1.10.m8.3.3.4.2.cmml" xref="S3.SS3.p1.10.m8.3.3.4.2">𝛼</ci></apply><ci id="S3.SS3.p1.10.m8.3.3.6.cmml" xref="S3.SS3.p1.10.m8.3.3.6">𝛽</ci></apply><apply id="S3.SS3.p1.10.m8.3.3c.cmml" xref="S3.SS3.p1.10.m8.3.3"><eq id="S3.SS3.p1.10.m8.3.3.7.cmml" xref="S3.SS3.p1.10.m8.3.3.7"></eq><share href="#S3.SS3.p1.10.m8.3.3.6.cmml" id="S3.SS3.p1.10.m8.3.3d.cmml" xref="S3.SS3.p1.10.m8.3.3"></share><apply id="S3.SS3.p1.10.m8.3.3.2.3.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2"><max id="S3.SS3.p1.10.m8.1.1.cmml" xref="S3.SS3.p1.10.m8.1.1"></max><apply id="S3.SS3.p1.10.m8.2.2.1.1.1.1.2.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1"><abs id="S3.SS3.p1.10.m8.2.2.1.1.1.1.2.1.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.2"></abs><apply id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.2">𝑟</ci><apply id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3"><times id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1"></times><ci id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.2">𝑚</ci><ci id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.3">𝑎</ci><ci id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.4.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.4">𝑥</ci></apply></apply></apply><apply id="S3.SS3.p1.10.m8.3.3.2.2.2.2.2.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1"><abs id="S3.SS3.p1.10.m8.3.3.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.2"></abs><apply id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.1.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.2.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.2">𝑟</ci><apply id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3"><times id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1"></times><ci id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.2.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.2">𝑚</ci><ci id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.3.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.3">𝑖</ci><ci id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.4.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.4">𝑛</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m8.3c">-\alpha=\beta={\max(|r_{max}|,|r_{min}|)}</annotation></semantics></math>.
Asymmetric quantization often results in a tighter clipping range as compared to symmetric quantization.
This is especially important when the target weights or activations are imbalanced, e.g., the activation after ReLU that always has non-negative values.
Using symmetric quantization, however, simplifies the quantization function in&nbsp;Eq.&nbsp;<a href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> by replacing the zero point with <math id="S3.SS3.p1.11.m9.1" class="ltx_Math" alttext="Z=0" display="inline"><semantics id="S3.SS3.p1.11.m9.1a"><mrow id="S3.SS3.p1.11.m9.1.1" xref="S3.SS3.p1.11.m9.1.1.cmml"><mi id="S3.SS3.p1.11.m9.1.1.2" xref="S3.SS3.p1.11.m9.1.1.2.cmml">Z</mi><mo id="S3.SS3.p1.11.m9.1.1.1" xref="S3.SS3.p1.11.m9.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.11.m9.1.1.3" xref="S3.SS3.p1.11.m9.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m9.1b"><apply id="S3.SS3.p1.11.m9.1.1.cmml" xref="S3.SS3.p1.11.m9.1.1"><eq id="S3.SS3.p1.11.m9.1.1.1.cmml" xref="S3.SS3.p1.11.m9.1.1.1"></eq><ci id="S3.SS3.p1.11.m9.1.1.2.cmml" xref="S3.SS3.p1.11.m9.1.1.2">𝑍</ci><cn type="integer" id="S3.SS3.p1.11.m9.1.1.3.cmml" xref="S3.SS3.p1.11.m9.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m9.1c">Z=0</annotation></semantics></math>:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.3" class="ltx_Math" alttext="\small Q(r)=\text{Int}\left(\frac{r}{S}\right)." display="block"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3.1" xref="S3.E5.m1.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1" xref="S3.E5.m1.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1.2" xref="S3.E5.m1.3.3.1.1.2.cmml"><mi mathsize="90%" id="S3.E5.m1.3.3.1.1.2.2" xref="S3.E5.m1.3.3.1.1.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.2.1" xref="S3.E5.m1.3.3.1.1.2.1.cmml">​</mo><mrow id="S3.E5.m1.3.3.1.1.2.3.2" xref="S3.E5.m1.3.3.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E5.m1.3.3.1.1.2.3.2.1" xref="S3.E5.m1.3.3.1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">r</mi><mo maxsize="90%" minsize="90%" id="S3.E5.m1.3.3.1.1.2.3.2.2" xref="S3.E5.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E5.m1.3.3.1.1.1" xref="S3.E5.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E5.m1.3.3.1.1.3" xref="S3.E5.m1.3.3.1.1.3.cmml"><mtext mathsize="90%" id="S3.E5.m1.3.3.1.1.3.2" xref="S3.E5.m1.3.3.1.1.3.2a.cmml">Int</mtext><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.3.1" xref="S3.E5.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S3.E5.m1.3.3.1.1.3.3.2" xref="S3.E5.m1.2.2.cmml"><mo id="S3.E5.m1.3.3.1.1.3.3.2.1" xref="S3.E5.m1.2.2.cmml">(</mo><mfrac id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml"><mi mathsize="90%" id="S3.E5.m1.2.2.2" xref="S3.E5.m1.2.2.2.cmml">r</mi><mi mathsize="90%" id="S3.E5.m1.2.2.3" xref="S3.E5.m1.2.2.3.cmml">S</mi></mfrac><mo id="S3.E5.m1.3.3.1.1.3.3.2.2" xref="S3.E5.m1.2.2.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E5.m1.3.3.1.2" xref="S3.E5.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.1.1.cmml" xref="S3.E5.m1.3.3.1"><eq id="S3.E5.m1.3.3.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1"></eq><apply id="S3.E5.m1.3.3.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.2"><times id="S3.E5.m1.3.3.1.1.2.1.cmml" xref="S3.E5.m1.3.3.1.1.2.1"></times><ci id="S3.E5.m1.3.3.1.1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.2.2">𝑄</ci><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">𝑟</ci></apply><apply id="S3.E5.m1.3.3.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.3"><times id="S3.E5.m1.3.3.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.3.1"></times><ci id="S3.E5.m1.3.3.1.1.3.2a.cmml" xref="S3.E5.m1.3.3.1.1.3.2"><mtext mathsize="90%" id="S3.E5.m1.3.3.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.3.2">Int</mtext></ci><apply id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2"><divide id="S3.E5.m1.2.2.1.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2"></divide><ci id="S3.E5.m1.2.2.2.cmml" xref="S3.E5.m1.2.2.2">𝑟</ci><ci id="S3.E5.m1.2.2.3.cmml" xref="S3.E5.m1.2.2.3">𝑆</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">\small Q(r)=\text{Int}\left(\frac{r}{S}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.13" class="ltx_p">Here, there are two choices for the scaling factor. In “full range” symmetric quantization S is chosen as <math id="S3.SS3.p1.12.m1.2" class="ltx_Math" alttext="\frac{2max(|r|)}{2^{n}-1}" display="inline"><semantics id="S3.SS3.p1.12.m1.2a"><mfrac id="S3.SS3.p1.12.m1.2.2" xref="S3.SS3.p1.12.m1.2.2.cmml"><mrow id="S3.SS3.p1.12.m1.2.2.2" xref="S3.SS3.p1.12.m1.2.2.2.cmml"><mn id="S3.SS3.p1.12.m1.2.2.2.4" xref="S3.SS3.p1.12.m1.2.2.2.4.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p1.12.m1.2.2.2.3" xref="S3.SS3.p1.12.m1.2.2.2.3.cmml">​</mo><mi id="S3.SS3.p1.12.m1.2.2.2.5" xref="S3.SS3.p1.12.m1.2.2.2.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.12.m1.2.2.2.3a" xref="S3.SS3.p1.12.m1.2.2.2.3.cmml">​</mo><mi id="S3.SS3.p1.12.m1.2.2.2.6" xref="S3.SS3.p1.12.m1.2.2.2.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.12.m1.2.2.2.3b" xref="S3.SS3.p1.12.m1.2.2.2.3.cmml">​</mo><mi id="S3.SS3.p1.12.m1.2.2.2.7" xref="S3.SS3.p1.12.m1.2.2.2.7.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.12.m1.2.2.2.3c" xref="S3.SS3.p1.12.m1.2.2.2.3.cmml">​</mo><mrow id="S3.SS3.p1.12.m1.2.2.2.2.1" xref="S3.SS3.p1.12.m1.2.2.2.cmml"><mo stretchy="false" id="S3.SS3.p1.12.m1.2.2.2.2.1.2" xref="S3.SS3.p1.12.m1.2.2.2.cmml">(</mo><mrow id="S3.SS3.p1.12.m1.2.2.2.2.1.1.2" xref="S3.SS3.p1.12.m1.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p1.12.m1.2.2.2.2.1.1.2.1" xref="S3.SS3.p1.12.m1.2.2.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS3.p1.12.m1.1.1.1.1" xref="S3.SS3.p1.12.m1.1.1.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS3.p1.12.m1.2.2.2.2.1.1.2.2" xref="S3.SS3.p1.12.m1.2.2.2.2.1.1.1.1.cmml">|</mo></mrow><mo stretchy="false" id="S3.SS3.p1.12.m1.2.2.2.2.1.3" xref="S3.SS3.p1.12.m1.2.2.2.cmml">)</mo></mrow></mrow><mrow id="S3.SS3.p1.12.m1.2.2.4" xref="S3.SS3.p1.12.m1.2.2.4.cmml"><msup id="S3.SS3.p1.12.m1.2.2.4.2" xref="S3.SS3.p1.12.m1.2.2.4.2.cmml"><mn id="S3.SS3.p1.12.m1.2.2.4.2.2" xref="S3.SS3.p1.12.m1.2.2.4.2.2.cmml">2</mn><mi id="S3.SS3.p1.12.m1.2.2.4.2.3" xref="S3.SS3.p1.12.m1.2.2.4.2.3.cmml">n</mi></msup><mo id="S3.SS3.p1.12.m1.2.2.4.1" xref="S3.SS3.p1.12.m1.2.2.4.1.cmml">−</mo><mn id="S3.SS3.p1.12.m1.2.2.4.3" xref="S3.SS3.p1.12.m1.2.2.4.3.cmml">1</mn></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.12.m1.2b"><apply id="S3.SS3.p1.12.m1.2.2.cmml" xref="S3.SS3.p1.12.m1.2.2"><divide id="S3.SS3.p1.12.m1.2.2.3.cmml" xref="S3.SS3.p1.12.m1.2.2"></divide><apply id="S3.SS3.p1.12.m1.2.2.2.cmml" xref="S3.SS3.p1.12.m1.2.2.2"><times id="S3.SS3.p1.12.m1.2.2.2.3.cmml" xref="S3.SS3.p1.12.m1.2.2.2.3"></times><cn type="integer" id="S3.SS3.p1.12.m1.2.2.2.4.cmml" xref="S3.SS3.p1.12.m1.2.2.2.4">2</cn><ci id="S3.SS3.p1.12.m1.2.2.2.5.cmml" xref="S3.SS3.p1.12.m1.2.2.2.5">𝑚</ci><ci id="S3.SS3.p1.12.m1.2.2.2.6.cmml" xref="S3.SS3.p1.12.m1.2.2.2.6">𝑎</ci><ci id="S3.SS3.p1.12.m1.2.2.2.7.cmml" xref="S3.SS3.p1.12.m1.2.2.2.7">𝑥</ci><apply id="S3.SS3.p1.12.m1.2.2.2.2.1.1.1.cmml" xref="S3.SS3.p1.12.m1.2.2.2.2.1.1.2"><abs id="S3.SS3.p1.12.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.SS3.p1.12.m1.2.2.2.2.1.1.2.1"></abs><ci id="S3.SS3.p1.12.m1.1.1.1.1.cmml" xref="S3.SS3.p1.12.m1.1.1.1.1">𝑟</ci></apply></apply><apply id="S3.SS3.p1.12.m1.2.2.4.cmml" xref="S3.SS3.p1.12.m1.2.2.4"><minus id="S3.SS3.p1.12.m1.2.2.4.1.cmml" xref="S3.SS3.p1.12.m1.2.2.4.1"></minus><apply id="S3.SS3.p1.12.m1.2.2.4.2.cmml" xref="S3.SS3.p1.12.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.SS3.p1.12.m1.2.2.4.2.1.cmml" xref="S3.SS3.p1.12.m1.2.2.4.2">superscript</csymbol><cn type="integer" id="S3.SS3.p1.12.m1.2.2.4.2.2.cmml" xref="S3.SS3.p1.12.m1.2.2.4.2.2">2</cn><ci id="S3.SS3.p1.12.m1.2.2.4.2.3.cmml" xref="S3.SS3.p1.12.m1.2.2.4.2.3">𝑛</ci></apply><cn type="integer" id="S3.SS3.p1.12.m1.2.2.4.3.cmml" xref="S3.SS3.p1.12.m1.2.2.4.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.12.m1.2c">\frac{2max(|r|)}{2^{n}-1}</annotation></semantics></math> (with floor rounding mode),
to use the full INT8 range of [-128,127]. However, in “restricted range” S is chosen as <math id="S3.SS3.p1.13.m2.2" class="ltx_Math" alttext="\frac{max(|r|)}{2^{n-1}-1}" display="inline"><semantics id="S3.SS3.p1.13.m2.2a"><mfrac id="S3.SS3.p1.13.m2.2.2" xref="S3.SS3.p1.13.m2.2.2.cmml"><mrow id="S3.SS3.p1.13.m2.2.2.2" xref="S3.SS3.p1.13.m2.2.2.2.cmml"><mi id="S3.SS3.p1.13.m2.2.2.2.4" xref="S3.SS3.p1.13.m2.2.2.2.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.13.m2.2.2.2.3" xref="S3.SS3.p1.13.m2.2.2.2.3.cmml">​</mo><mi id="S3.SS3.p1.13.m2.2.2.2.5" xref="S3.SS3.p1.13.m2.2.2.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.13.m2.2.2.2.3a" xref="S3.SS3.p1.13.m2.2.2.2.3.cmml">​</mo><mi id="S3.SS3.p1.13.m2.2.2.2.6" xref="S3.SS3.p1.13.m2.2.2.2.6.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.13.m2.2.2.2.3b" xref="S3.SS3.p1.13.m2.2.2.2.3.cmml">​</mo><mrow id="S3.SS3.p1.13.m2.2.2.2.2.1" xref="S3.SS3.p1.13.m2.2.2.2.cmml"><mo stretchy="false" id="S3.SS3.p1.13.m2.2.2.2.2.1.2" xref="S3.SS3.p1.13.m2.2.2.2.cmml">(</mo><mrow id="S3.SS3.p1.13.m2.2.2.2.2.1.1.2" xref="S3.SS3.p1.13.m2.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p1.13.m2.2.2.2.2.1.1.2.1" xref="S3.SS3.p1.13.m2.2.2.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS3.p1.13.m2.1.1.1.1" xref="S3.SS3.p1.13.m2.1.1.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS3.p1.13.m2.2.2.2.2.1.1.2.2" xref="S3.SS3.p1.13.m2.2.2.2.2.1.1.1.1.cmml">|</mo></mrow><mo stretchy="false" id="S3.SS3.p1.13.m2.2.2.2.2.1.3" xref="S3.SS3.p1.13.m2.2.2.2.cmml">)</mo></mrow></mrow><mrow id="S3.SS3.p1.13.m2.2.2.4" xref="S3.SS3.p1.13.m2.2.2.4.cmml"><msup id="S3.SS3.p1.13.m2.2.2.4.2" xref="S3.SS3.p1.13.m2.2.2.4.2.cmml"><mn id="S3.SS3.p1.13.m2.2.2.4.2.2" xref="S3.SS3.p1.13.m2.2.2.4.2.2.cmml">2</mn><mrow id="S3.SS3.p1.13.m2.2.2.4.2.3" xref="S3.SS3.p1.13.m2.2.2.4.2.3.cmml"><mi id="S3.SS3.p1.13.m2.2.2.4.2.3.2" xref="S3.SS3.p1.13.m2.2.2.4.2.3.2.cmml">n</mi><mo id="S3.SS3.p1.13.m2.2.2.4.2.3.1" xref="S3.SS3.p1.13.m2.2.2.4.2.3.1.cmml">−</mo><mn id="S3.SS3.p1.13.m2.2.2.4.2.3.3" xref="S3.SS3.p1.13.m2.2.2.4.2.3.3.cmml">1</mn></mrow></msup><mo id="S3.SS3.p1.13.m2.2.2.4.1" xref="S3.SS3.p1.13.m2.2.2.4.1.cmml">−</mo><mn id="S3.SS3.p1.13.m2.2.2.4.3" xref="S3.SS3.p1.13.m2.2.2.4.3.cmml">1</mn></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.13.m2.2b"><apply id="S3.SS3.p1.13.m2.2.2.cmml" xref="S3.SS3.p1.13.m2.2.2"><divide id="S3.SS3.p1.13.m2.2.2.3.cmml" xref="S3.SS3.p1.13.m2.2.2"></divide><apply id="S3.SS3.p1.13.m2.2.2.2.cmml" xref="S3.SS3.p1.13.m2.2.2.2"><times id="S3.SS3.p1.13.m2.2.2.2.3.cmml" xref="S3.SS3.p1.13.m2.2.2.2.3"></times><ci id="S3.SS3.p1.13.m2.2.2.2.4.cmml" xref="S3.SS3.p1.13.m2.2.2.2.4">𝑚</ci><ci id="S3.SS3.p1.13.m2.2.2.2.5.cmml" xref="S3.SS3.p1.13.m2.2.2.2.5">𝑎</ci><ci id="S3.SS3.p1.13.m2.2.2.2.6.cmml" xref="S3.SS3.p1.13.m2.2.2.2.6">𝑥</ci><apply id="S3.SS3.p1.13.m2.2.2.2.2.1.1.1.cmml" xref="S3.SS3.p1.13.m2.2.2.2.2.1.1.2"><abs id="S3.SS3.p1.13.m2.2.2.2.2.1.1.1.1.cmml" xref="S3.SS3.p1.13.m2.2.2.2.2.1.1.2.1"></abs><ci id="S3.SS3.p1.13.m2.1.1.1.1.cmml" xref="S3.SS3.p1.13.m2.1.1.1.1">𝑟</ci></apply></apply><apply id="S3.SS3.p1.13.m2.2.2.4.cmml" xref="S3.SS3.p1.13.m2.2.2.4"><minus id="S3.SS3.p1.13.m2.2.2.4.1.cmml" xref="S3.SS3.p1.13.m2.2.2.4.1"></minus><apply id="S3.SS3.p1.13.m2.2.2.4.2.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2"><csymbol cd="ambiguous" id="S3.SS3.p1.13.m2.2.2.4.2.1.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2">superscript</csymbol><cn type="integer" id="S3.SS3.p1.13.m2.2.2.4.2.2.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2.2">2</cn><apply id="S3.SS3.p1.13.m2.2.2.4.2.3.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2.3"><minus id="S3.SS3.p1.13.m2.2.2.4.2.3.1.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2.3.1"></minus><ci id="S3.SS3.p1.13.m2.2.2.4.2.3.2.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2.3.2">𝑛</ci><cn type="integer" id="S3.SS3.p1.13.m2.2.2.4.2.3.3.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2.3.3">1</cn></apply></apply><cn type="integer" id="S3.SS3.p1.13.m2.2.2.4.3.cmml" xref="S3.SS3.p1.13.m2.2.2.4.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.13.m2.2c">\frac{max(|r|)}{2^{n-1}-1}</annotation></semantics></math>, which
only uses the range of [-127,127]. As expected, the full range approach is more accurate.
Symmetric quantization is widely adopted in practice for quantizing weights because zeroing out the zero point can lead to reduction in computational cost during inference&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib255" title="" class="ltx_ref">255</a>]</cite>, and also makes the implementation more straightforward.
However, note that for activation the cross terms occupying due to the offset in the asymmetric activations are a static data independent term and can be absorbed in the bias (or used to initialize the accumulator)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.4" class="ltx_p">Using the min/max of the signal for both symmetric and asymmetric quantization is a popular method.
However, this approach is susceptible to outlier data in the activations.
These could unnecessarily increase the range and, as a result, reduce the resolution of quantization.
One approach to address this is to use percentile instead of min/max of the signal&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite>.
That is to say, instead of the largest/smallest value, the i-th largest/smallest values are used as <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\beta</annotation></semantics></math>/<math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\alpha</annotation></semantics></math>.
Another approach is to select <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\alpha</annotation></semantics></math> and <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mi id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\beta</annotation></semantics></math> to minimize KL divergence (i.e., information loss) between the real values and the quantized values&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite>.
We refer the interested readers to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib255" title="" class="ltx_ref">255</a>]</cite> where the different calibration methods are evaluated on various&nbsp;models.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Summary (Symmetric vs Asymmetric Quantization).</span> Symmetric quantization partitions the clipping
using a symmetric range. This has the advantage of easier
implementation, as it leads to <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="Z=0" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">Z</mi><mo id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><eq id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></eq><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">𝑍</ci><cn type="integer" id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">Z=0</annotation></semantics></math> in&nbsp;Eq.&nbsp;<a href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
However, it is sub-optimal for cases where the range
could be skewed and not symmetric. For such cases, asymmetric
quantization is preferred.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Range Calibration Algorithms: Static vs Dynamic Quantization</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.2" class="ltx_p">So far, we discussed different calibration methods for determining the clipping range of <math id="S3.SS4.p1.1.m1.2" class="ltx_Math" alttext="[\alpha,\beta]" display="inline"><semantics id="S3.SS4.p1.1.m1.2a"><mrow id="S3.SS4.p1.1.m1.2.3.2" xref="S3.SS4.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS4.p1.1.m1.2.3.2.1" xref="S3.SS4.p1.1.m1.2.3.1.cmml">[</mo><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">α</mi><mo id="S3.SS4.p1.1.m1.2.3.2.2" xref="S3.SS4.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS4.p1.1.m1.2.2" xref="S3.SS4.p1.1.m1.2.2.cmml">β</mi><mo stretchy="false" id="S3.SS4.p1.1.m1.2.3.2.3" xref="S3.SS4.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.2b"><interval closure="closed" id="S3.SS4.p1.1.m1.2.3.1.cmml" xref="S3.SS4.p1.1.m1.2.3.2"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝛼</ci><ci id="S3.SS4.p1.1.m1.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2">𝛽</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.2c">[\alpha,\beta]</annotation></semantics></math>.
Another important differentiator of quantization methods is <span id="S3.SS4.p1.2.1" class="ltx_text ltx_font_italic">when</span> the clipping range is determined.
This range can be computed statically for weights, as in most cases the parameters are fixed during inference.
However, the activation maps differ for each input sample (<math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mi id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">x</annotation></semantics></math> in&nbsp;Eq.&nbsp;<a href="#S3.E1" title="In III-A Problem Setup and Notations ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
As such, there are two approaches to quantizing activations: <span id="S3.SS4.p1.2.2" class="ltx_text ltx_font_italic">dynamic quantization</span>, and <span id="S3.SS4.p1.2.3" class="ltx_text ltx_font_italic">static quantization</span>.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">In dynamic quantization, this range is <em id="S3.SS4.p2.1.1" class="ltx_emph ltx_font_italic">dynamically</em> calculated for each activation map during runtime.
This approach requires real-time computation of the signal statistics (min, max, percentile, etc.) which
can have a very high overhead. However, dynamic quantization often results in higher
accuracy as the signal range is exactly calculated for each input.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Another quantization approach is static quantization, in which the clipping range is pre-calculated and <em id="S3.SS4.p3.1.1" class="ltx_emph ltx_font_italic">static</em> during inference.
This approach does not add any computational overhead, but it typically results in lower accuracy
as compared to dynamic quantization.
One popular method for the pre-calculation is to run a series of calibration inputs to compute
the typical range of activations&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib267" title="" class="ltx_ref">267</a>]</cite>.
Multiple different metrics have been proposed to find the best range, including minimizing Mean Squared Error (MSE) between
original unquantized weight distribution and the corresponding quantized values&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib229" title="" class="ltx_ref">229</a>, <a href="#bib.bib221" title="" class="ltx_ref">221</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib281" title="" class="ltx_ref">281</a>]</cite>.
One could also consider using other metrics such as entropy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>, although MSE is the most common method used.
Another approach is to learn/impose this clipping range during NN training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib287" title="" class="ltx_ref">287</a>, <a href="#bib.bib276" title="" class="ltx_ref">276</a>]</cite>.
Notable work here are LQNets&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib276" title="" class="ltx_ref">276</a>]</cite>, PACT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, LSQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, and LSQ+&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
which jointly optimizes the clipping range and the weights in NN during training.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_bold">Summary (Dynamic vs Static Quantization).</span>
Dynamic quantization dynamically computes the clipping range of each
activation and often achieves the highest accuracy. However,
calculating the range of a signal dynamically is very expensive, and
as such, practitioners most often use static quantization where the clipping
range is fixed for all inputs.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Quantization Granularity</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">In most computer vision tasks, the activation input to a layer is convolved with many different convolutional filters, as illustrated in&nbsp;Figure&nbsp;<a href="#S3.F3" title="Figure 3 ‣ III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Each of these convolutional filters can have a different range of values.
As such, one differentiator for quantization methods is the granularity of how the clipping range <math id="S3.SS5.p1.1.m1.2" class="ltx_Math" alttext="[\alpha,\beta]" display="inline"><semantics id="S3.SS5.p1.1.m1.2a"><mrow id="S3.SS5.p1.1.m1.2.3.2" xref="S3.SS5.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS5.p1.1.m1.2.3.2.1" xref="S3.SS5.p1.1.m1.2.3.1.cmml">[</mo><mi id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">α</mi><mo id="S3.SS5.p1.1.m1.2.3.2.2" xref="S3.SS5.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS5.p1.1.m1.2.2" xref="S3.SS5.p1.1.m1.2.2.cmml">β</mi><mo stretchy="false" id="S3.SS5.p1.1.m1.2.3.2.3" xref="S3.SS5.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.2b"><interval closure="closed" id="S3.SS5.p1.1.m1.2.3.1.cmml" xref="S3.SS5.p1.1.m1.2.3.2"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">𝛼</ci><ci id="S3.SS5.p1.1.m1.2.2.cmml" xref="S3.SS5.p1.1.m1.2.2">𝛽</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.2c">[\alpha,\beta]</annotation></semantics></math> is calculated
for the weights.
We categorized them as follows.</p>
</div>
<section id="S3.SS5.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Layerwise Quantization</h5>

<div id="S3.SS5.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS5.SSS0.Px1.p1.1" class="ltx_p">In this approach, the clipping range
is determined by considering all of the weights in convolutional filters of a layer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, as shown in the third column of&nbsp;Figure&nbsp;<a href="#S3.F3" title="Figure 3 ‣ III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Here one examines the statistics of the entire parameters in that layer (e.g., min, max, percentile, etc.),
and then uses the same clipping range for all the convolutional filters.
While this approach is very simple to implement, it
often results in sub-optimal accuracy, as the range of each convolutional filter can
be vary a lot.
For example, a convolutional kernel that has
relatively narrower range of parameters may lose its quantization resolution due to another kernel in the same layer with a wider range.</p>
</div>
</section>
<section id="S3.SS5.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Groupwise Quantization</h5>

<div id="S3.SS5.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS5.SSS0.Px2.p1.1" class="ltx_p">One could group multiple different channels inside a layer to calculate the clipping range (of either activations or convolution kernels).
This could be helpful for cases where the distribution of the parameters across a single convolution/activation varies a lot.
For instance, this approach was found useful in Q-BERT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib219" title="" class="ltx_ref">219</a>]</cite> for quantizing Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib243" title="" class="ltx_ref">243</a>]</cite> models that consist of fully-connected attention layers.
However, this approach inevitably comes with the extra cost of accounting for
different scaling factors.</p>
</div>
</section>
<section id="S3.SS5.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Channelwise Quantization</h5>

<div id="S3.SS5.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS5.SSS0.Px3.p1.1" class="ltx_p">A popular choice of the clipping range is to use a fixed value for each convolutional filter, independent of
other channels&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib285" title="" class="ltx_ref">285</a>, <a href="#bib.bib276" title="" class="ltx_ref">276</a>, <a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>, <a href="#bib.bib222" title="" class="ltx_ref">222</a>]</cite>, as shown in the last column of&nbsp;Figure&nbsp;<a href="#S3.F3" title="Figure 3 ‣ III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
That is to say, each channel is assigned a dedicated scaling factor.
This ensures a better quantization resolution and often results in
higher accuracy.</p>
</div>
</section>
<section id="S3.SS5.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Sub-channelwise Quantization</h5>

<div id="S3.SS5.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS5.SSS0.Px4.p1.1" class="ltx_p">The previous approach could be taken to the extreme, where the clipping range is determined with respect to any groups of parameters in a convolution or fully-connected layer.
However, this approach could add considerable
overhead, since the different scaling factors need to be taken into account when processing a single convolution or full-connected layer.
Therefore, groupwise quantization could establish a good compromise between the quantization resolution and the computation overhead.</p>
</div>
<div id="S3.SS5.SSS0.Px4.p2" class="ltx_para">
<p id="S3.SS5.SSS0.Px4.p2.1" class="ltx_p"><span id="S3.SS5.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_bold">Summary (Quantization Granularity).</span> Channelwise quantization is currently
the standard method used for quantizing convolutional kernels.
It enables the practitioner to adjust the clipping range for
each individual kernel with negligible overhead. In contrast,
sub-channelwise quantization may result in significant overhead
and is not currently the standard choice (we also refer interested
reader to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> for tradeoffs
associated with these design choices).</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2103.13630/assets/x5.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison between Quantization-Aware Training (QAT, Left) and Post-Training Quantization (PTQ, Right).
In QAT, a pre-trained model is quantized and then finetuned using training data to adjust parameters and recover accuracy degradation.
In PTQ, a pre-trained model is calibrated using calibration data (e.g., a small subset of training data) to compute the clipping ranges and the scaling factors.
Then, the model is quantized based on the calibration result.
Note that the calibration process is often conducted in parallel with the finetuning process for QAT.
</figcaption>
</figure>
</section>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.5.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.6.2" class="ltx_text ltx_font_italic">Non-Uniform Quantization</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.2" class="ltx_p">Some work in the literature has also explored non-uniform
quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib256" title="" class="ltx_ref">256</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib159" title="" class="ltx_ref">159</a>, <a href="#bib.bib179" title="" class="ltx_ref">179</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib190" title="" class="ltx_ref">190</a>, <a href="#bib.bib276" title="" class="ltx_ref">276</a>, <a href="#bib.bib248" title="" class="ltx_ref">248</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>, <a href="#bib.bib264" title="" class="ltx_ref">264</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib238" title="" class="ltx_ref">238</a>, <a href="#bib.bib284" title="" class="ltx_ref">284</a>, <a href="#bib.bib189" title="" class="ltx_ref">189</a>, <a href="#bib.bib266" title="" class="ltx_ref">266</a>, <a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite>,
where quantization steps as well as quantization levels are allowed to be non-uniformly spaced.
The formal definition of non-uniform quantization is shown in&nbsp;Eq.&nbsp;<a href="#S3.E6" title="In III-F Non-Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, where <math id="S3.SS6.p1.1.m1.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><msub id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml"><mi id="S3.SS6.p1.1.m1.1.1.2" xref="S3.SS6.p1.1.m1.1.1.2.cmml">X</mi><mi id="S3.SS6.p1.1.m1.1.1.3" xref="S3.SS6.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><apply id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.1.m1.1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS6.p1.1.m1.1.1.2.cmml" xref="S3.SS6.p1.1.m1.1.1.2">𝑋</ci><ci id="S3.SS6.p1.1.m1.1.1.3.cmml" xref="S3.SS6.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">X_{i}</annotation></semantics></math> represents the discrete quantization levels and <math id="S3.SS6.p1.2.m2.1" class="ltx_Math" alttext="\Delta_{i}" display="inline"><semantics id="S3.SS6.p1.2.m2.1a"><msub id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS6.p1.2.m2.1.1.2" xref="S3.SS6.p1.2.m2.1.1.2.cmml">Δ</mi><mi id="S3.SS6.p1.2.m2.1.1.3" xref="S3.SS6.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><apply id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.2.m2.1.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS6.p1.2.m2.1.1.2.cmml" xref="S3.SS6.p1.2.m2.1.1.2">Δ</ci><ci id="S3.SS6.p1.2.m2.1.1.3.cmml" xref="S3.SS6.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">\Delta_{i}</annotation></semantics></math> the quantization steps (thresholds):</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.2" class="ltx_Math" alttext="\small Q(r)=X_{i},\leavevmode\nobreak\ \mathrm{if}\leavevmode\nobreak\ r\in[\Delta_{i},\Delta_{i+1})." display="block"><semantics id="S3.E6.m1.2a"><mrow id="S3.E6.m1.2.2.1"><mrow id="S3.E6.m1.2.2.1.1.2" xref="S3.E6.m1.2.2.1.1.3.cmml"><mrow id="S3.E6.m1.2.2.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E6.m1.2.2.1.1.1.1.2" xref="S3.E6.m1.2.2.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.1.1.2.2" xref="S3.E6.m1.2.2.1.1.1.1.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.1.1.1.2.1" xref="S3.E6.m1.2.2.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.E6.m1.2.2.1.1.1.1.2.3.2" xref="S3.E6.m1.2.2.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E6.m1.2.2.1.1.1.1.2.3.2.1" xref="S3.E6.m1.2.2.1.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">r</mi><mo maxsize="90%" minsize="90%" id="S3.E6.m1.2.2.1.1.1.1.2.3.2.2" xref="S3.E6.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E6.m1.2.2.1.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.1.cmml">=</mo><msub id="S3.E6.m1.2.2.1.1.1.1.3" xref="S3.E6.m1.2.2.1.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.1.1.3.2" xref="S3.E6.m1.2.2.1.1.1.1.3.2.cmml">X</mi><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.1.1.3.3" xref="S3.E6.m1.2.2.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo mathsize="90%" rspace="0.617em" id="S3.E6.m1.2.2.1.1.2.3" xref="S3.E6.m1.2.2.1.1.3a.cmml">,</mo><mrow id="S3.E6.m1.2.2.1.1.2.2" xref="S3.E6.m1.2.2.1.1.2.2.cmml"><mrow id="S3.E6.m1.2.2.1.1.2.2.4" xref="S3.E6.m1.2.2.1.1.2.2.4.cmml"><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.4.2" xref="S3.E6.m1.2.2.1.1.2.2.4.2.cmml">if</mi><mo lspace="0.450em" rspace="0em" id="S3.E6.m1.2.2.1.1.2.2.4.1" xref="S3.E6.m1.2.2.1.1.2.2.4.1.cmml">​</mo><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.4.3" xref="S3.E6.m1.2.2.1.1.2.2.4.3.cmml">r</mi></mrow><mo mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.3" xref="S3.E6.m1.2.2.1.1.2.2.3.cmml">∈</mo><mrow id="S3.E6.m1.2.2.1.1.2.2.2.2" xref="S3.E6.m1.2.2.1.1.2.2.2.3.cmml"><mo maxsize="90%" minsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.3" xref="S3.E6.m1.2.2.1.1.2.2.2.3.cmml">[</mo><msub id="S3.E6.m1.2.2.1.1.2.2.1.1.1" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1.cmml"><mi mathsize="90%" mathvariant="normal" id="S3.E6.m1.2.2.1.1.2.2.1.1.1.2" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1.2.cmml">Δ</mi><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.1.1.1.3" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1.3.cmml">i</mi></msub><mo mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.4" xref="S3.E6.m1.2.2.1.1.2.2.2.3.cmml">,</mo><msub id="S3.E6.m1.2.2.1.1.2.2.2.2.2" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.cmml"><mi mathsize="90%" mathvariant="normal" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.2" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.2.cmml">Δ</mi><mrow id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.cmml"><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.2" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.2.cmml">i</mi><mo mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.1" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.1.cmml">+</mo><mn mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.3" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.3.cmml">1</mn></mrow></msub><mo maxsize="90%" minsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.5" xref="S3.E6.m1.2.2.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E6.m1.2.2.1.2">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.2b"><apply id="S3.E6.m1.2.2.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.3a.cmml" xref="S3.E6.m1.2.2.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E6.m1.2.2.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1"><eq id="S3.E6.m1.2.2.1.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1"></eq><apply id="S3.E6.m1.2.2.1.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2"><times id="S3.E6.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.1"></times><ci id="S3.E6.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.2">𝑄</ci><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">𝑟</ci></apply><apply id="S3.E6.m1.2.2.1.1.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.3.2">𝑋</ci><ci id="S3.E6.m1.2.2.1.1.1.1.3.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S3.E6.m1.2.2.1.1.2.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2"><in id="S3.E6.m1.2.2.1.1.2.2.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.3"></in><apply id="S3.E6.m1.2.2.1.1.2.2.4.cmml" xref="S3.E6.m1.2.2.1.1.2.2.4"><times id="S3.E6.m1.2.2.1.1.2.2.4.1.cmml" xref="S3.E6.m1.2.2.1.1.2.2.4.1"></times><ci id="S3.E6.m1.2.2.1.1.2.2.4.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2.4.2">if</ci><ci id="S3.E6.m1.2.2.1.1.2.2.4.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.4.3">𝑟</ci></apply><interval closure="closed-open" id="S3.E6.m1.2.2.1.1.2.2.2.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2"><apply id="S3.E6.m1.2.2.1.1.2.2.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.2.2.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.2.2.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1.2">Δ</ci><ci id="S3.E6.m1.2.2.1.1.2.2.1.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1.3">𝑖</ci></apply><apply id="S3.E6.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.1.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.2.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.2">Δ</ci><apply id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3"><plus id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.1.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.1"></plus><ci id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.3">1</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.2c">\small Q(r)=X_{i},\leavevmode\nobreak\ \mathrm{if}\leavevmode\nobreak\ r\in[\Delta_{i},\Delta_{i+1}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS6.p1.9" class="ltx_p">Specifically, when the value of a real number <math id="S3.SS6.p1.3.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS6.p1.3.m1.1a"><mi id="S3.SS6.p1.3.m1.1.1" xref="S3.SS6.p1.3.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.3.m1.1b"><ci id="S3.SS6.p1.3.m1.1.1.cmml" xref="S3.SS6.p1.3.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.3.m1.1c">r</annotation></semantics></math> falls in between the quantization step <math id="S3.SS6.p1.4.m2.1" class="ltx_Math" alttext="\Delta_{i}" display="inline"><semantics id="S3.SS6.p1.4.m2.1a"><msub id="S3.SS6.p1.4.m2.1.1" xref="S3.SS6.p1.4.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS6.p1.4.m2.1.1.2" xref="S3.SS6.p1.4.m2.1.1.2.cmml">Δ</mi><mi id="S3.SS6.p1.4.m2.1.1.3" xref="S3.SS6.p1.4.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.4.m2.1b"><apply id="S3.SS6.p1.4.m2.1.1.cmml" xref="S3.SS6.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.4.m2.1.1.1.cmml" xref="S3.SS6.p1.4.m2.1.1">subscript</csymbol><ci id="S3.SS6.p1.4.m2.1.1.2.cmml" xref="S3.SS6.p1.4.m2.1.1.2">Δ</ci><ci id="S3.SS6.p1.4.m2.1.1.3.cmml" xref="S3.SS6.p1.4.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.4.m2.1c">\Delta_{i}</annotation></semantics></math> and <math id="S3.SS6.p1.5.m3.1" class="ltx_Math" alttext="\Delta_{i+1}" display="inline"><semantics id="S3.SS6.p1.5.m3.1a"><msub id="S3.SS6.p1.5.m3.1.1" xref="S3.SS6.p1.5.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS6.p1.5.m3.1.1.2" xref="S3.SS6.p1.5.m3.1.1.2.cmml">Δ</mi><mrow id="S3.SS6.p1.5.m3.1.1.3" xref="S3.SS6.p1.5.m3.1.1.3.cmml"><mi id="S3.SS6.p1.5.m3.1.1.3.2" xref="S3.SS6.p1.5.m3.1.1.3.2.cmml">i</mi><mo id="S3.SS6.p1.5.m3.1.1.3.1" xref="S3.SS6.p1.5.m3.1.1.3.1.cmml">+</mo><mn id="S3.SS6.p1.5.m3.1.1.3.3" xref="S3.SS6.p1.5.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.5.m3.1b"><apply id="S3.SS6.p1.5.m3.1.1.cmml" xref="S3.SS6.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.5.m3.1.1.1.cmml" xref="S3.SS6.p1.5.m3.1.1">subscript</csymbol><ci id="S3.SS6.p1.5.m3.1.1.2.cmml" xref="S3.SS6.p1.5.m3.1.1.2">Δ</ci><apply id="S3.SS6.p1.5.m3.1.1.3.cmml" xref="S3.SS6.p1.5.m3.1.1.3"><plus id="S3.SS6.p1.5.m3.1.1.3.1.cmml" xref="S3.SS6.p1.5.m3.1.1.3.1"></plus><ci id="S3.SS6.p1.5.m3.1.1.3.2.cmml" xref="S3.SS6.p1.5.m3.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS6.p1.5.m3.1.1.3.3.cmml" xref="S3.SS6.p1.5.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.5.m3.1c">\Delta_{i+1}</annotation></semantics></math>, quantizer <math id="S3.SS6.p1.6.m4.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS6.p1.6.m4.1a"><mi id="S3.SS6.p1.6.m4.1.1" xref="S3.SS6.p1.6.m4.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.6.m4.1b"><ci id="S3.SS6.p1.6.m4.1.1.cmml" xref="S3.SS6.p1.6.m4.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.6.m4.1c">Q</annotation></semantics></math> projects it to the corresponding quantization level <math id="S3.SS6.p1.7.m5.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S3.SS6.p1.7.m5.1a"><msub id="S3.SS6.p1.7.m5.1.1" xref="S3.SS6.p1.7.m5.1.1.cmml"><mi id="S3.SS6.p1.7.m5.1.1.2" xref="S3.SS6.p1.7.m5.1.1.2.cmml">X</mi><mi id="S3.SS6.p1.7.m5.1.1.3" xref="S3.SS6.p1.7.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.7.m5.1b"><apply id="S3.SS6.p1.7.m5.1.1.cmml" xref="S3.SS6.p1.7.m5.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.7.m5.1.1.1.cmml" xref="S3.SS6.p1.7.m5.1.1">subscript</csymbol><ci id="S3.SS6.p1.7.m5.1.1.2.cmml" xref="S3.SS6.p1.7.m5.1.1.2">𝑋</ci><ci id="S3.SS6.p1.7.m5.1.1.3.cmml" xref="S3.SS6.p1.7.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.7.m5.1c">X_{i}</annotation></semantics></math>.
Note that neither <math id="S3.SS6.p1.8.m6.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S3.SS6.p1.8.m6.1a"><msub id="S3.SS6.p1.8.m6.1.1" xref="S3.SS6.p1.8.m6.1.1.cmml"><mi id="S3.SS6.p1.8.m6.1.1.2" xref="S3.SS6.p1.8.m6.1.1.2.cmml">X</mi><mi id="S3.SS6.p1.8.m6.1.1.3" xref="S3.SS6.p1.8.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.8.m6.1b"><apply id="S3.SS6.p1.8.m6.1.1.cmml" xref="S3.SS6.p1.8.m6.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.8.m6.1.1.1.cmml" xref="S3.SS6.p1.8.m6.1.1">subscript</csymbol><ci id="S3.SS6.p1.8.m6.1.1.2.cmml" xref="S3.SS6.p1.8.m6.1.1.2">𝑋</ci><ci id="S3.SS6.p1.8.m6.1.1.3.cmml" xref="S3.SS6.p1.8.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.8.m6.1c">X_{i}</annotation></semantics></math>’s nor <math id="S3.SS6.p1.9.m7.1" class="ltx_Math" alttext="\Delta_{i}" display="inline"><semantics id="S3.SS6.p1.9.m7.1a"><msub id="S3.SS6.p1.9.m7.1.1" xref="S3.SS6.p1.9.m7.1.1.cmml"><mi mathvariant="normal" id="S3.SS6.p1.9.m7.1.1.2" xref="S3.SS6.p1.9.m7.1.1.2.cmml">Δ</mi><mi id="S3.SS6.p1.9.m7.1.1.3" xref="S3.SS6.p1.9.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.9.m7.1b"><apply id="S3.SS6.p1.9.m7.1.1.cmml" xref="S3.SS6.p1.9.m7.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.9.m7.1.1.1.cmml" xref="S3.SS6.p1.9.m7.1.1">subscript</csymbol><ci id="S3.SS6.p1.9.m7.1.1.2.cmml" xref="S3.SS6.p1.9.m7.1.1.2">Δ</ci><ci id="S3.SS6.p1.9.m7.1.1.3.cmml" xref="S3.SS6.p1.9.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.9.m7.1c">\Delta_{i}</annotation></semantics></math>’s are uniformly spaced.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.8" class="ltx_p">Non-uniform quantization may achieve higher accuracy for a fixed bit-width,
because one could better capture the distributions by focusing more on important value regions or
finding appropriate dynamic ranges.
For instance, many non-uniform quantization methods have been designed for bell-shaped distributions of the weights and activations that often involve long tails&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib147" title="" class="ltx_ref">147</a>, <a href="#bib.bib179" title="" class="ltx_ref">179</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>.
A typical rule-based non-uniform quantization is to use a logarithmic distribution&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>, <a href="#bib.bib283" title="" class="ltx_ref">283</a>]</cite>, where the quantization steps and levels increase exponentially instead of linearly.
Another popular branch is <span id="S3.SS6.p2.8.1" class="ltx_text ltx_font_italic">binary-code-based</span> quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>, <a href="#bib.bib258" title="" class="ltx_ref">258</a>, <a href="#bib.bib276" title="" class="ltx_ref">276</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>
where a real-number vector <math id="S3.SS6.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{r}\in\mathbb{R}^{n}" display="inline"><semantics id="S3.SS6.p2.1.m1.1a"><mrow id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml"><mi id="S3.SS6.p2.1.m1.1.1.2" xref="S3.SS6.p2.1.m1.1.1.2.cmml">𝐫</mi><mo id="S3.SS6.p2.1.m1.1.1.1" xref="S3.SS6.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS6.p2.1.m1.1.1.3" xref="S3.SS6.p2.1.m1.1.1.3.cmml"><mi id="S3.SS6.p2.1.m1.1.1.3.2" xref="S3.SS6.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS6.p2.1.m1.1.1.3.3" xref="S3.SS6.p2.1.m1.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.1.m1.1b"><apply id="S3.SS6.p2.1.m1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1"><in id="S3.SS6.p2.1.m1.1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1.1"></in><ci id="S3.SS6.p2.1.m1.1.1.2.cmml" xref="S3.SS6.p2.1.m1.1.1.2">𝐫</ci><apply id="S3.SS6.p2.1.m1.1.1.3.cmml" xref="S3.SS6.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p2.1.m1.1.1.3.1.cmml" xref="S3.SS6.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS6.p2.1.m1.1.1.3.2.cmml" xref="S3.SS6.p2.1.m1.1.1.3.2">ℝ</ci><ci id="S3.SS6.p2.1.m1.1.1.3.3.cmml" xref="S3.SS6.p2.1.m1.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.1.m1.1c">\mathbf{r}\in\mathbb{R}^{n}</annotation></semantics></math> is quantized into <math id="S3.SS6.p2.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.SS6.p2.2.m2.1a"><mi id="S3.SS6.p2.2.m2.1.1" xref="S3.SS6.p2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.2.m2.1b"><ci id="S3.SS6.p2.2.m2.1.1.cmml" xref="S3.SS6.p2.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.2.m2.1c">m</annotation></semantics></math> binary vectors by
representing <math id="S3.SS6.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{r}\approx\sum_{i=1}^{m}\alpha_{i}\mathbf{b}_{i}" display="inline"><semantics id="S3.SS6.p2.3.m3.1a"><mrow id="S3.SS6.p2.3.m3.1.1" xref="S3.SS6.p2.3.m3.1.1.cmml"><mi id="S3.SS6.p2.3.m3.1.1.2" xref="S3.SS6.p2.3.m3.1.1.2.cmml">𝐫</mi><mo rspace="0.111em" id="S3.SS6.p2.3.m3.1.1.1" xref="S3.SS6.p2.3.m3.1.1.1.cmml">≈</mo><mrow id="S3.SS6.p2.3.m3.1.1.3" xref="S3.SS6.p2.3.m3.1.1.3.cmml"><msubsup id="S3.SS6.p2.3.m3.1.1.3.1" xref="S3.SS6.p2.3.m3.1.1.3.1.cmml"><mo id="S3.SS6.p2.3.m3.1.1.3.1.2.2" xref="S3.SS6.p2.3.m3.1.1.3.1.2.2.cmml">∑</mo><mrow id="S3.SS6.p2.3.m3.1.1.3.1.2.3" xref="S3.SS6.p2.3.m3.1.1.3.1.2.3.cmml"><mi id="S3.SS6.p2.3.m3.1.1.3.1.2.3.2" xref="S3.SS6.p2.3.m3.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.SS6.p2.3.m3.1.1.3.1.2.3.1" xref="S3.SS6.p2.3.m3.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.SS6.p2.3.m3.1.1.3.1.2.3.3" xref="S3.SS6.p2.3.m3.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.SS6.p2.3.m3.1.1.3.1.3" xref="S3.SS6.p2.3.m3.1.1.3.1.3.cmml">m</mi></msubsup><mrow id="S3.SS6.p2.3.m3.1.1.3.2" xref="S3.SS6.p2.3.m3.1.1.3.2.cmml"><msub id="S3.SS6.p2.3.m3.1.1.3.2.2" xref="S3.SS6.p2.3.m3.1.1.3.2.2.cmml"><mi id="S3.SS6.p2.3.m3.1.1.3.2.2.2" xref="S3.SS6.p2.3.m3.1.1.3.2.2.2.cmml">α</mi><mi id="S3.SS6.p2.3.m3.1.1.3.2.2.3" xref="S3.SS6.p2.3.m3.1.1.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS6.p2.3.m3.1.1.3.2.1" xref="S3.SS6.p2.3.m3.1.1.3.2.1.cmml">​</mo><msub id="S3.SS6.p2.3.m3.1.1.3.2.3" xref="S3.SS6.p2.3.m3.1.1.3.2.3.cmml"><mi id="S3.SS6.p2.3.m3.1.1.3.2.3.2" xref="S3.SS6.p2.3.m3.1.1.3.2.3.2.cmml">𝐛</mi><mi id="S3.SS6.p2.3.m3.1.1.3.2.3.3" xref="S3.SS6.p2.3.m3.1.1.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.3.m3.1b"><apply id="S3.SS6.p2.3.m3.1.1.cmml" xref="S3.SS6.p2.3.m3.1.1"><approx id="S3.SS6.p2.3.m3.1.1.1.cmml" xref="S3.SS6.p2.3.m3.1.1.1"></approx><ci id="S3.SS6.p2.3.m3.1.1.2.cmml" xref="S3.SS6.p2.3.m3.1.1.2">𝐫</ci><apply id="S3.SS6.p2.3.m3.1.1.3.cmml" xref="S3.SS6.p2.3.m3.1.1.3"><apply id="S3.SS6.p2.3.m3.1.1.3.1.cmml" xref="S3.SS6.p2.3.m3.1.1.3.1"><csymbol cd="ambiguous" id="S3.SS6.p2.3.m3.1.1.3.1.1.cmml" xref="S3.SS6.p2.3.m3.1.1.3.1">superscript</csymbol><apply id="S3.SS6.p2.3.m3.1.1.3.1.2.cmml" xref="S3.SS6.p2.3.m3.1.1.3.1"><csymbol cd="ambiguous" id="S3.SS6.p2.3.m3.1.1.3.1.2.1.cmml" xref="S3.SS6.p2.3.m3.1.1.3.1">subscript</csymbol><sum id="S3.SS6.p2.3.m3.1.1.3.1.2.2.cmml" xref="S3.SS6.p2.3.m3.1.1.3.1.2.2"></sum><apply id="S3.SS6.p2.3.m3.1.1.3.1.2.3.cmml" xref="S3.SS6.p2.3.m3.1.1.3.1.2.3"><eq id="S3.SS6.p2.3.m3.1.1.3.1.2.3.1.cmml" xref="S3.SS6.p2.3.m3.1.1.3.1.2.3.1"></eq><ci id="S3.SS6.p2.3.m3.1.1.3.1.2.3.2.cmml" xref="S3.SS6.p2.3.m3.1.1.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.SS6.p2.3.m3.1.1.3.1.2.3.3.cmml" xref="S3.SS6.p2.3.m3.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S3.SS6.p2.3.m3.1.1.3.1.3.cmml" xref="S3.SS6.p2.3.m3.1.1.3.1.3">𝑚</ci></apply><apply id="S3.SS6.p2.3.m3.1.1.3.2.cmml" xref="S3.SS6.p2.3.m3.1.1.3.2"><times id="S3.SS6.p2.3.m3.1.1.3.2.1.cmml" xref="S3.SS6.p2.3.m3.1.1.3.2.1"></times><apply id="S3.SS6.p2.3.m3.1.1.3.2.2.cmml" xref="S3.SS6.p2.3.m3.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS6.p2.3.m3.1.1.3.2.2.1.cmml" xref="S3.SS6.p2.3.m3.1.1.3.2.2">subscript</csymbol><ci id="S3.SS6.p2.3.m3.1.1.3.2.2.2.cmml" xref="S3.SS6.p2.3.m3.1.1.3.2.2.2">𝛼</ci><ci id="S3.SS6.p2.3.m3.1.1.3.2.2.3.cmml" xref="S3.SS6.p2.3.m3.1.1.3.2.2.3">𝑖</ci></apply><apply id="S3.SS6.p2.3.m3.1.1.3.2.3.cmml" xref="S3.SS6.p2.3.m3.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS6.p2.3.m3.1.1.3.2.3.1.cmml" xref="S3.SS6.p2.3.m3.1.1.3.2.3">subscript</csymbol><ci id="S3.SS6.p2.3.m3.1.1.3.2.3.2.cmml" xref="S3.SS6.p2.3.m3.1.1.3.2.3.2">𝐛</ci><ci id="S3.SS6.p2.3.m3.1.1.3.2.3.3.cmml" xref="S3.SS6.p2.3.m3.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.3.m3.1c">\mathbf{r}\approx\sum_{i=1}^{m}\alpha_{i}\mathbf{b}_{i}</annotation></semantics></math>, with the scaling factors <math id="S3.SS6.p2.4.m4.1" class="ltx_Math" alttext="\alpha_{i}\in\mathbb{R}" display="inline"><semantics id="S3.SS6.p2.4.m4.1a"><mrow id="S3.SS6.p2.4.m4.1.1" xref="S3.SS6.p2.4.m4.1.1.cmml"><msub id="S3.SS6.p2.4.m4.1.1.2" xref="S3.SS6.p2.4.m4.1.1.2.cmml"><mi id="S3.SS6.p2.4.m4.1.1.2.2" xref="S3.SS6.p2.4.m4.1.1.2.2.cmml">α</mi><mi id="S3.SS6.p2.4.m4.1.1.2.3" xref="S3.SS6.p2.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS6.p2.4.m4.1.1.1" xref="S3.SS6.p2.4.m4.1.1.1.cmml">∈</mo><mi id="S3.SS6.p2.4.m4.1.1.3" xref="S3.SS6.p2.4.m4.1.1.3.cmml">ℝ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.4.m4.1b"><apply id="S3.SS6.p2.4.m4.1.1.cmml" xref="S3.SS6.p2.4.m4.1.1"><in id="S3.SS6.p2.4.m4.1.1.1.cmml" xref="S3.SS6.p2.4.m4.1.1.1"></in><apply id="S3.SS6.p2.4.m4.1.1.2.cmml" xref="S3.SS6.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p2.4.m4.1.1.2.1.cmml" xref="S3.SS6.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS6.p2.4.m4.1.1.2.2.cmml" xref="S3.SS6.p2.4.m4.1.1.2.2">𝛼</ci><ci id="S3.SS6.p2.4.m4.1.1.2.3.cmml" xref="S3.SS6.p2.4.m4.1.1.2.3">𝑖</ci></apply><ci id="S3.SS6.p2.4.m4.1.1.3.cmml" xref="S3.SS6.p2.4.m4.1.1.3">ℝ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.4.m4.1c">\alpha_{i}\in\mathbb{R}</annotation></semantics></math> and the binary vectors <math id="S3.SS6.p2.5.m5.2" class="ltx_Math" alttext="\mathbf{b}_{i}\in\{-1,+1\}^{n}" display="inline"><semantics id="S3.SS6.p2.5.m5.2a"><mrow id="S3.SS6.p2.5.m5.2.2" xref="S3.SS6.p2.5.m5.2.2.cmml"><msub id="S3.SS6.p2.5.m5.2.2.4" xref="S3.SS6.p2.5.m5.2.2.4.cmml"><mi id="S3.SS6.p2.5.m5.2.2.4.2" xref="S3.SS6.p2.5.m5.2.2.4.2.cmml">𝐛</mi><mi id="S3.SS6.p2.5.m5.2.2.4.3" xref="S3.SS6.p2.5.m5.2.2.4.3.cmml">i</mi></msub><mo id="S3.SS6.p2.5.m5.2.2.3" xref="S3.SS6.p2.5.m5.2.2.3.cmml">∈</mo><msup id="S3.SS6.p2.5.m5.2.2.2" xref="S3.SS6.p2.5.m5.2.2.2.cmml"><mrow id="S3.SS6.p2.5.m5.2.2.2.2.2" xref="S3.SS6.p2.5.m5.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS6.p2.5.m5.2.2.2.2.2.3" xref="S3.SS6.p2.5.m5.2.2.2.2.3.cmml">{</mo><mrow id="S3.SS6.p2.5.m5.1.1.1.1.1.1" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1.cmml"><mo id="S3.SS6.p2.5.m5.1.1.1.1.1.1a" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1.cmml">−</mo><mn id="S3.SS6.p2.5.m5.1.1.1.1.1.1.2" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1.2.cmml">1</mn></mrow><mo id="S3.SS6.p2.5.m5.2.2.2.2.2.4" xref="S3.SS6.p2.5.m5.2.2.2.2.3.cmml">,</mo><mrow id="S3.SS6.p2.5.m5.2.2.2.2.2.2" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2.cmml"><mo id="S3.SS6.p2.5.m5.2.2.2.2.2.2a" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2.cmml">+</mo><mn id="S3.SS6.p2.5.m5.2.2.2.2.2.2.2" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2.2.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS6.p2.5.m5.2.2.2.2.2.5" xref="S3.SS6.p2.5.m5.2.2.2.2.3.cmml">}</mo></mrow><mi id="S3.SS6.p2.5.m5.2.2.2.4" xref="S3.SS6.p2.5.m5.2.2.2.4.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.5.m5.2b"><apply id="S3.SS6.p2.5.m5.2.2.cmml" xref="S3.SS6.p2.5.m5.2.2"><in id="S3.SS6.p2.5.m5.2.2.3.cmml" xref="S3.SS6.p2.5.m5.2.2.3"></in><apply id="S3.SS6.p2.5.m5.2.2.4.cmml" xref="S3.SS6.p2.5.m5.2.2.4"><csymbol cd="ambiguous" id="S3.SS6.p2.5.m5.2.2.4.1.cmml" xref="S3.SS6.p2.5.m5.2.2.4">subscript</csymbol><ci id="S3.SS6.p2.5.m5.2.2.4.2.cmml" xref="S3.SS6.p2.5.m5.2.2.4.2">𝐛</ci><ci id="S3.SS6.p2.5.m5.2.2.4.3.cmml" xref="S3.SS6.p2.5.m5.2.2.4.3">𝑖</ci></apply><apply id="S3.SS6.p2.5.m5.2.2.2.cmml" xref="S3.SS6.p2.5.m5.2.2.2"><csymbol cd="ambiguous" id="S3.SS6.p2.5.m5.2.2.2.3.cmml" xref="S3.SS6.p2.5.m5.2.2.2">superscript</csymbol><set id="S3.SS6.p2.5.m5.2.2.2.2.3.cmml" xref="S3.SS6.p2.5.m5.2.2.2.2.2"><apply id="S3.SS6.p2.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1"><minus id="S3.SS6.p2.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1"></minus><cn type="integer" id="S3.SS6.p2.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1.2">1</cn></apply><apply id="S3.SS6.p2.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2"><plus id="S3.SS6.p2.5.m5.2.2.2.2.2.2.1.cmml" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2"></plus><cn type="integer" id="S3.SS6.p2.5.m5.2.2.2.2.2.2.2.cmml" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2.2">1</cn></apply></set><ci id="S3.SS6.p2.5.m5.2.2.2.4.cmml" xref="S3.SS6.p2.5.m5.2.2.2.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.5.m5.2c">\mathbf{b}_{i}\in\{-1,+1\}^{n}</annotation></semantics></math>.
Since there is no closed-form solution for minimizing the error between <math id="S3.SS6.p2.6.m6.1" class="ltx_Math" alttext="\mathbf{r}" display="inline"><semantics id="S3.SS6.p2.6.m6.1a"><mi id="S3.SS6.p2.6.m6.1.1" xref="S3.SS6.p2.6.m6.1.1.cmml">𝐫</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.6.m6.1b"><ci id="S3.SS6.p2.6.m6.1.1.cmml" xref="S3.SS6.p2.6.m6.1.1">𝐫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.6.m6.1c">\mathbf{r}</annotation></semantics></math> and <math id="S3.SS6.p2.7.m7.1" class="ltx_Math" alttext="\sum_{i=1}^{m}\alpha_{i}\mathbf{b}_{i}" display="inline"><semantics id="S3.SS6.p2.7.m7.1a"><mrow id="S3.SS6.p2.7.m7.1.1" xref="S3.SS6.p2.7.m7.1.1.cmml"><msubsup id="S3.SS6.p2.7.m7.1.1.1" xref="S3.SS6.p2.7.m7.1.1.1.cmml"><mo id="S3.SS6.p2.7.m7.1.1.1.2.2" xref="S3.SS6.p2.7.m7.1.1.1.2.2.cmml">∑</mo><mrow id="S3.SS6.p2.7.m7.1.1.1.2.3" xref="S3.SS6.p2.7.m7.1.1.1.2.3.cmml"><mi id="S3.SS6.p2.7.m7.1.1.1.2.3.2" xref="S3.SS6.p2.7.m7.1.1.1.2.3.2.cmml">i</mi><mo id="S3.SS6.p2.7.m7.1.1.1.2.3.1" xref="S3.SS6.p2.7.m7.1.1.1.2.3.1.cmml">=</mo><mn id="S3.SS6.p2.7.m7.1.1.1.2.3.3" xref="S3.SS6.p2.7.m7.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S3.SS6.p2.7.m7.1.1.1.3" xref="S3.SS6.p2.7.m7.1.1.1.3.cmml">m</mi></msubsup><mrow id="S3.SS6.p2.7.m7.1.1.2" xref="S3.SS6.p2.7.m7.1.1.2.cmml"><msub id="S3.SS6.p2.7.m7.1.1.2.2" xref="S3.SS6.p2.7.m7.1.1.2.2.cmml"><mi id="S3.SS6.p2.7.m7.1.1.2.2.2" xref="S3.SS6.p2.7.m7.1.1.2.2.2.cmml">α</mi><mi id="S3.SS6.p2.7.m7.1.1.2.2.3" xref="S3.SS6.p2.7.m7.1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS6.p2.7.m7.1.1.2.1" xref="S3.SS6.p2.7.m7.1.1.2.1.cmml">​</mo><msub id="S3.SS6.p2.7.m7.1.1.2.3" xref="S3.SS6.p2.7.m7.1.1.2.3.cmml"><mi id="S3.SS6.p2.7.m7.1.1.2.3.2" xref="S3.SS6.p2.7.m7.1.1.2.3.2.cmml">𝐛</mi><mi id="S3.SS6.p2.7.m7.1.1.2.3.3" xref="S3.SS6.p2.7.m7.1.1.2.3.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.7.m7.1b"><apply id="S3.SS6.p2.7.m7.1.1.cmml" xref="S3.SS6.p2.7.m7.1.1"><apply id="S3.SS6.p2.7.m7.1.1.1.cmml" xref="S3.SS6.p2.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p2.7.m7.1.1.1.1.cmml" xref="S3.SS6.p2.7.m7.1.1.1">superscript</csymbol><apply id="S3.SS6.p2.7.m7.1.1.1.2.cmml" xref="S3.SS6.p2.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p2.7.m7.1.1.1.2.1.cmml" xref="S3.SS6.p2.7.m7.1.1.1">subscript</csymbol><sum id="S3.SS6.p2.7.m7.1.1.1.2.2.cmml" xref="S3.SS6.p2.7.m7.1.1.1.2.2"></sum><apply id="S3.SS6.p2.7.m7.1.1.1.2.3.cmml" xref="S3.SS6.p2.7.m7.1.1.1.2.3"><eq id="S3.SS6.p2.7.m7.1.1.1.2.3.1.cmml" xref="S3.SS6.p2.7.m7.1.1.1.2.3.1"></eq><ci id="S3.SS6.p2.7.m7.1.1.1.2.3.2.cmml" xref="S3.SS6.p2.7.m7.1.1.1.2.3.2">𝑖</ci><cn type="integer" id="S3.SS6.p2.7.m7.1.1.1.2.3.3.cmml" xref="S3.SS6.p2.7.m7.1.1.1.2.3.3">1</cn></apply></apply><ci id="S3.SS6.p2.7.m7.1.1.1.3.cmml" xref="S3.SS6.p2.7.m7.1.1.1.3">𝑚</ci></apply><apply id="S3.SS6.p2.7.m7.1.1.2.cmml" xref="S3.SS6.p2.7.m7.1.1.2"><times id="S3.SS6.p2.7.m7.1.1.2.1.cmml" xref="S3.SS6.p2.7.m7.1.1.2.1"></times><apply id="S3.SS6.p2.7.m7.1.1.2.2.cmml" xref="S3.SS6.p2.7.m7.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS6.p2.7.m7.1.1.2.2.1.cmml" xref="S3.SS6.p2.7.m7.1.1.2.2">subscript</csymbol><ci id="S3.SS6.p2.7.m7.1.1.2.2.2.cmml" xref="S3.SS6.p2.7.m7.1.1.2.2.2">𝛼</ci><ci id="S3.SS6.p2.7.m7.1.1.2.2.3.cmml" xref="S3.SS6.p2.7.m7.1.1.2.2.3">𝑖</ci></apply><apply id="S3.SS6.p2.7.m7.1.1.2.3.cmml" xref="S3.SS6.p2.7.m7.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS6.p2.7.m7.1.1.2.3.1.cmml" xref="S3.SS6.p2.7.m7.1.1.2.3">subscript</csymbol><ci id="S3.SS6.p2.7.m7.1.1.2.3.2.cmml" xref="S3.SS6.p2.7.m7.1.1.2.3.2">𝐛</ci><ci id="S3.SS6.p2.7.m7.1.1.2.3.3.cmml" xref="S3.SS6.p2.7.m7.1.1.2.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.7.m7.1c">\sum_{i=1}^{m}\alpha_{i}\mathbf{b}_{i}</annotation></semantics></math>,
previous research relies on heuristic solutions.
To further improve the quantizer, more recent work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib234" title="" class="ltx_ref">234</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib258" title="" class="ltx_ref">258</a>]</cite> formulates non-uniform quantization as an optimization problem. As shown in&nbsp;Eq.&nbsp;<a href="#S3.E7" title="In III-F Non-Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the quantization steps/levels in the quantizer <math id="S3.SS6.p2.8.m8.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS6.p2.8.m8.1a"><mi id="S3.SS6.p2.8.m8.1.1" xref="S3.SS6.p2.8.m8.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.8.m8.1b"><ci id="S3.SS6.p2.8.m8.1.1.cmml" xref="S3.SS6.p2.8.m8.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.8.m8.1c">Q</annotation></semantics></math> are adjusted to minimize the difference between the original tensor and the quantized counterpart.</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.2" class="ltx_Math" alttext="\small\min_{Q}\|Q(r)-r\|^{2}" display="block"><semantics id="S3.E7.m1.2a"><mrow id="S3.E7.m1.2.2" xref="S3.E7.m1.2.2.cmml"><munder id="S3.E7.m1.2.2.2" xref="S3.E7.m1.2.2.2.cmml"><mi mathsize="90%" id="S3.E7.m1.2.2.2.2" xref="S3.E7.m1.2.2.2.2.cmml">min</mi><mi mathsize="90%" id="S3.E7.m1.2.2.2.3" xref="S3.E7.m1.2.2.2.3.cmml">Q</mi></munder><mo id="S3.E7.m1.2.2a" xref="S3.E7.m1.2.2.cmml">⁡</mo><msup id="S3.E7.m1.2.2.1" xref="S3.E7.m1.2.2.1.cmml"><mrow id="S3.E7.m1.2.2.1.1.1" xref="S3.E7.m1.2.2.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E7.m1.2.2.1.1.1.2" xref="S3.E7.m1.2.2.1.1.2.1.cmml">‖</mo><mrow id="S3.E7.m1.2.2.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E7.m1.2.2.1.1.1.1.2" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E7.m1.2.2.1.1.1.1.2.2" xref="S3.E7.m1.2.2.1.1.1.1.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.2.2.1.1.1.1.2.1" xref="S3.E7.m1.2.2.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.E7.m1.2.2.1.1.1.1.2.3.2" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E7.m1.2.2.1.1.1.1.2.3.2.1" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml">r</mi><mo maxsize="90%" minsize="90%" id="S3.E7.m1.2.2.1.1.1.1.2.3.2.2" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E7.m1.2.2.1.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.1.cmml">−</mo><mi mathsize="90%" id="S3.E7.m1.2.2.1.1.1.1.3" xref="S3.E7.m1.2.2.1.1.1.1.3.cmml">r</mi></mrow><mo maxsize="90%" minsize="90%" id="S3.E7.m1.2.2.1.1.1.3" xref="S3.E7.m1.2.2.1.1.2.1.cmml">‖</mo></mrow><mn mathsize="90%" id="S3.E7.m1.2.2.1.3" xref="S3.E7.m1.2.2.1.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.2b"><apply id="S3.E7.m1.2.2.cmml" xref="S3.E7.m1.2.2"><apply id="S3.E7.m1.2.2.2.cmml" xref="S3.E7.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.2.1.cmml" xref="S3.E7.m1.2.2.2">subscript</csymbol><min id="S3.E7.m1.2.2.2.2.cmml" xref="S3.E7.m1.2.2.2.2"></min><ci id="S3.E7.m1.2.2.2.3.cmml" xref="S3.E7.m1.2.2.2.3">𝑄</ci></apply><apply id="S3.E7.m1.2.2.1.cmml" xref="S3.E7.m1.2.2.1"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.2.cmml" xref="S3.E7.m1.2.2.1">superscript</csymbol><apply id="S3.E7.m1.2.2.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1"><csymbol cd="latexml" id="S3.E7.m1.2.2.1.1.2.1.cmml" xref="S3.E7.m1.2.2.1.1.1.2">norm</csymbol><apply id="S3.E7.m1.2.2.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1"><minus id="S3.E7.m1.2.2.1.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1"></minus><apply id="S3.E7.m1.2.2.1.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2"><times id="S3.E7.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2.1"></times><ci id="S3.E7.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2.2">𝑄</ci><ci id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1">𝑟</ci></apply><ci id="S3.E7.m1.2.2.1.1.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.1.1.3">𝑟</ci></apply></apply><cn type="integer" id="S3.E7.m1.2.2.1.3.cmml" xref="S3.E7.m1.2.2.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.2c">\small\min_{Q}\|Q(r)-r\|^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S3.SS6.p2.9" class="ltx_p">Furthermore, the quantizer itself can also be jointly trained with the model parameters. These methods are referred to as learnable quantizers, and the quantization steps/levels are generally trained with iterative optimization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib276" title="" class="ltx_ref">276</a>, <a href="#bib.bib258" title="" class="ltx_ref">258</a>]</cite> or gradient descent&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>, <a href="#bib.bib264" title="" class="ltx_ref">264</a>]</cite>.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">In addition to rule-based and optimization-based non-uniform quantization, clustering can also be beneficial to alleviate the information loss due to quantization.
Some works&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib256" title="" class="ltx_ref">256</a>]</cite> use k-means on different tensors to determine the quantization steps and levels, while other work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> applies a Hessian-weighted k-means clustering on weights to minimize the performance loss. Further discussion can be found in&nbsp;Section&nbsp;<a href="#S4.SS6" title="IV-F Vector Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-F</span></span></a>.</p>
</div>
<div id="S3.SS6.p4" class="ltx_para">
<p id="S3.SS6.p4.1" class="ltx_p"><span id="S3.SS6.p4.1.1" class="ltx_text ltx_font_bold">Summary (Uniform vs Non-uniform Quantization).</span>
Generally, non-uniform quantization enables us to better capture
the signal information, by assigning bits and discreitizing the range of parameters non-uniformly.
However, non-uniform quantization schemes are typically difficult to deploy efficiently on general computation hardware, e.g., GPU and CPU.
As such, the uniform quantization is currently the de-facto method due to its
simplicity and its efficient mapping to hardware.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2103.13630/assets/x6.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Illustration of Quantization-Aware Training procedure, including
the use of Straight Through Estimator (STE).
</figcaption>
</figure>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS7.5.1.1" class="ltx_text">III-G</span> </span><span id="S3.SS7.6.2" class="ltx_text ltx_font_italic">Fine-tuning Methods</span>
</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">It is often necessary to adjust the parameters in the NN after quantization.
This can either be performed by re-training the model, a process that is called
Quantization-Aware Training (QAT), or done without re-training, a process that is often
referred to as Post-Training Quantization (PTQ).
A schematic comparison between these two
approaches is illustrated in&nbsp;Figure&nbsp;<a href="#S3.F4" title="Figure 4 ‣ Sub-channelwise Quantization ‣ III-E Quantization Granularity ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and further
discussed below (we refer interested reader to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite>
for more detailed discussion on this topic).</p>
</div>
<section id="S3.SS7.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS7.SSS1.5.1.1" class="ltx_text">III-G</span>1 </span>Quantization-Aware Training</h4>

<div id="S3.SS7.SSS1.p1" class="ltx_para">
<p id="S3.SS7.SSS1.p1.1" class="ltx_p">Given a trained model, quantization may introduce a perturbation to the trained model parameters, and this can push the model away from the point to which it had converged when it was trained with floating point precision.
It is possible
to address this by re-training the NN model with quantized parameters so that the model
can converge to a point with better loss.
One popular approach is to use Quantization-Aware Training (QAT),
in which the usual forward and backward pass
are performed on the quantized model in floating point,
but the model parameters are quantized after each gradient
update (similar to projected gradient descent).
In particular, it is important to do this projection
after the weight update is performed in floating point
precision.
Performing the backward pass with floating point is important,
as accumulating the gradients in quantized precision
can result in zero-gradient or gradients that have high error, especially in low-precision
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib159" title="" class="ltx_ref">159</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib204" title="" class="ltx_ref">204</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib231" title="" class="ltx_ref">231</a>, <a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite>.</p>
</div>
<div id="S3.SS7.SSS1.p2" class="ltx_para">
<p id="S3.SS7.SSS1.p2.1" class="ltx_p">An important subtlety in backpropagation is how the the non-differentiable quantization operator (Eq.&nbsp;<a href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) is treated.
Without any approximation, the gradient of this operator is zero almost everywhere, since the rounding operation in&nbsp;Eq.&nbsp;<a href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is
a piece-wise flat operator.
A popular approach to address this is to approximate the gradient of this operator by the so-called Straight Through Estimator (STE)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
STE essentially ignores the rounding operation and approximates it with an identity function, as illustrated in&nbsp;Figure&nbsp;<a href="#S3.F5" title="Figure 5 ‣ III-F Non-Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S3.SS7.SSS1.p3" class="ltx_para">
<p id="S3.SS7.SSS1.p3.1" class="ltx_p">Despite the coarse approximation of STE, it often works well in practice, except for ultra low-precision quantization
such as binary quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The work of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib271" title="" class="ltx_ref">271</a>]</cite> provides
a theoretical justification for this phenomena, and it finds that the coarse gradient
approximation of STE can in expectation correlate with population gradient (for a proper choice of STE).
From a historical perspective, we should note that the original
idea of STE can be traced back to the seminal work of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib209" title="" class="ltx_ref">209</a>, <a href="#bib.bib210" title="" class="ltx_ref">210</a>]</cite>, where
an identity operator was used to approximate gradient from the binary neurons.</p>
</div>
<div id="S3.SS7.SSS1.p4" class="ltx_para">
<p id="S3.SS7.SSS1.p4.1" class="ltx_p">While STE is the mainstream approach&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib289" title="" class="ltx_ref">289</a>, <a href="#bib.bib226" title="" class="ltx_ref">226</a>]</cite>, other approaches have also been explored in the literature&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib164" title="" class="ltx_ref">164</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
We should first mention that&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> also proposes a stochastic
neuron approach as an alternative to STE (this is briefly discussed
in&nbsp;Section&nbsp;<a href="#S3.SS8" title="III-H Stochastic Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-H</span></span></a>).
Other approaches using combinatorial optimization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, target propagation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite>, or
Gumbel-softmax&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>
have also been proposed.
Another different class of alternative methods tries to use regularization operators to enforce the weight to be quantized.
This removes
the need to use the non-differentiable quantization operator in&nbsp;Eq.&nbsp;<a href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
These
are often
referred to as <em id="S3.SS7.SSS1.p4.1.1" class="ltx_emph ltx_font_italic">Non-STE</em> methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib184" title="" class="ltx_ref">184</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib283" title="" class="ltx_ref">283</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
Recent research in this area includes
ProxQuant&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> which
removes the rounding operation in the quantization formula&nbsp;Eq.&nbsp;<a href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, and instead uses
the so-called <em id="S3.SS7.SSS1.p4.1.2" class="ltx_emph ltx_font_italic">W-shape</em>, non-smooth regularization function to enforce the weights to quantized values.
Other notable research includes using pulse training to approximate the derivative of discontinuous points&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>,
or replacing the quantized weights with an affine combination of floating point and quantized parameters&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>.
The recent work of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> also suggests AdaRound, which is an adaptive rounding method as an alternative
to round-to-nearest method.
Despite interesting works in this area, these methods often require a lot of tuning and so far STE approach is the most commonly used method.</p>
</div>
<div id="S3.SS7.SSS1.p5" class="ltx_para">
<p id="S3.SS7.SSS1.p5.1" class="ltx_p">In addition to adjusting model parameters,
some prior work found it effective to learn quantization parameters during QAT as well.
PACT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> learns the clipping ranges of activations under uniform quantization, while QIT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> also learns quantization steps and levels as an extension to a non-uniform quantization setting.
LSQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> introduces a new gradient estimate to learn scaling factors for non-negative activations (e.g., ReLU) during QAT, and LSQ+&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> further extends this idea to general activation functions such as swish&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>]</cite> and h-swish&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> that produce negative values.</p>
</div>
<div id="S3.SS7.SSS1.p6" class="ltx_para">
<p id="S3.SS7.SSS1.p6.1" class="ltx_p"><span id="S3.SS7.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Summary (QAT).</span>
QAT has been shown to work despite the coarse
approximation of STE. However, the main disadvantage of QAT is the computational cost of re-training
the NN model. This re-training may need to be performed for several hundred epochs to recover accuracy,
especially for low-bit precision quantization.
If a quantized model is going to be deployed for an extended period, and if efficiency and accuracy are especially important,
then this investment in re-training is likely to be worth it. However, this is not always the case,
as some models have a relatively short lifetime.
Next, we next discuss an alternative approach that does not
have this overhead.</p>
</div>
</section>
<section id="S3.SS7.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS7.SSS2.5.1.1" class="ltx_text">III-G</span>2 </span>Post-Training Quantization</h4>

<div id="S3.SS7.SSS2.p1" class="ltx_para">
<p id="S3.SS7.SSS2.p1.1" class="ltx_p">An alternative to the expensive QAT method is Post-Training Quantization (PTQ) which performs the quantization and the adjustments of the weights,
without any fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib174" title="" class="ltx_ref">174</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib281" title="" class="ltx_ref">281</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib142" title="" class="ltx_ref">142</a>, <a href="#bib.bib182" title="" class="ltx_ref">182</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib148" title="" class="ltx_ref">148</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib223" title="" class="ltx_ref">223</a>]</cite>.
As such, the overhead of PTQ is very low and often negligible.
Unlike QAT, which requires a sufficient amount of training data for retraining, PTQ has an additional advantage that it can be applied in situations where data is limited or unlabeled.
However, this often comes at the cost of lower accuracy as compared to QAT, especially for low-precision quantization.</p>
</div>
<div id="S3.SS7.SSS2.p2" class="ltx_para">
<p id="S3.SS7.SSS2.p2.1" class="ltx_p">For this reason, multiple approaches have been proposed to mitigate the accuracy degradation of PTQ.
For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> observe inherent bias in the mean and variance of the weight values following their quantization and propose bias correction methods; and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>, <a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite> show that equalizing the weight ranges (and implicitly activation ranges)
between different layers or channels can reduce quantization errors.
ACIQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> analytically computes the optimal clipping range and the channel-wise bitwidth setting for PTQ.
Although ACIQ can achieve low accuracy degradation, the channel-wise activation quantization used in ACIQ is hard to efficiently deploy on hardware.
In order to address this, the OMSE method&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> removes channel-wise quantization on activation and proposes to conduct PTQ by optimizing the L2 distance between the quantized tensor and the corresponding floating point tensor.
Furthermore, to better alleviate the adverse impact of outliers on PTQ, an outlier channel splitting (OCS) method is proposed in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib281" title="" class="ltx_ref">281</a>]</cite> which duplicates and halves the channels containing outlier values.
Another notable work is AdaRound&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite> which shows that the naive round-to-nearest method for quantization can counter-intuitively results in sub-optimal solutions, and it proposes an adaptive rounding method that better reduces the loss.
While AdaRound restricts the changes of the quantized weights to be within <math id="S3.SS7.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\pm 1" display="inline"><semantics id="S3.SS7.SSS2.p2.1.m1.1a"><mrow id="S3.SS7.SSS2.p2.1.m1.1.1" xref="S3.SS7.SSS2.p2.1.m1.1.1.cmml"><mo id="S3.SS7.SSS2.p2.1.m1.1.1a" xref="S3.SS7.SSS2.p2.1.m1.1.1.cmml">±</mo><mn id="S3.SS7.SSS2.p2.1.m1.1.1.2" xref="S3.SS7.SSS2.p2.1.m1.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS7.SSS2.p2.1.m1.1b"><apply id="S3.SS7.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS7.SSS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS7.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS7.SSS2.p2.1.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S3.SS7.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS7.SSS2.p2.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.SSS2.p2.1.m1.1c">\pm 1</annotation></semantics></math> from their full-precision counterparts,
AdaQuant&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> proposes a more general method that allows the quantized weights to change as needed.
PTQ schemes can be taken to the extreme, where neither training nor testing data are utilized during quantization (aka zero-shot scenarios),
which is discussed next.</p>
</div>
<div id="S3.SS7.SSS2.p3" class="ltx_para">
<p id="S3.SS7.SSS2.p3.1" class="ltx_p"><span id="S3.SS7.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Summary (PTQ).</span> In PTQ, all the weights and activations
quantization parameters are determined without any re-training of the
NN model. As such, PTQ is a very fast method for quantizing
NN models. However, this often comes at the cost of lower accuracy
as compared to QAT.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2103.13630/assets/x7.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="160" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
Comparison between full-precision inference (Left), inference with simulated quantization (Middle), and inference with integer-only quantization (Right).</figcaption>
</figure>
</section>
<section id="S3.SS7.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS7.SSS3.5.1.1" class="ltx_text">III-G</span>3 </span>Zero-shot Quantization</h4>

<div id="S3.SS7.SSS3.p1" class="ltx_para">
<p id="S3.SS7.SSS3.p1.1" class="ltx_p">As discussed so far, in order to achieve minimal accuracy degradation
after quantization, we need access to the entire of
a fraction of training data.
First, we need to know the range of activations so that we can clip the values and determine the proper scaling factors (which
is usually referred to as calibration in the literature).
Second, quantized models often require fine-tuning to adjust the model parameters and recover the accuracy degradation.
In many cases, however, access to the original training data is not possible during the quantization procedure.
This is because the training dataset is either too large to be distributed,
proprietary (e.g., Google’s JFT-300M),
or sensitive due to security or privacy concerns (e.g., medical data).
Several different methods have been proposed to address this challenge, which we refer to as zero-shot quantization (ZSQ).
Inspired by &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite>, here we first describe two different levels of zero-shot quantization:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Level 1:</span> No data and no finetuning (ZSQ + PTQ).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Level 2:</span> No data but requires finetuning (ZSQ + QAT).</p>
</div>
</li>
</ul>
<p id="S3.SS7.SSS3.p1.2" class="ltx_p">Level 1 allows faster and easier quantization without any finetuning.
Finetuning is in general time-consuming and often requires additional hyperparamenter search.
However, Level 2 usually results in higher accuracy, as finetuning helps the quantized model to recover the accuracy degradation, particularly in ultra-low bit precision settings&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>.
The work of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite> uses a Level 1 approach that relies on
equalizing the weight ranges and correcting bias errors to make a given NN model more amenable to quantization without any data or finetuning.
However, as this method is based on the scale-equivariance property of (piece-wise) linear activation functions,
it can be sub-optimal for NNs with non-linear activations, such as BERT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> with GELU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> activation or MobileNetV3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> with swish activation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite>.</p>
</div>
<div id="S3.SS7.SSS3.p2" class="ltx_para">
<p id="S3.SS7.SSS3.p2.1" class="ltx_p">A popular branch of research in ZSQ is to generate synthetic data similar to the real data from which the target pre-trained model is trained.
The synthetic data is then used for calibrating and/or finetuning the quantized model.
An early work in this area <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> exploits Generative Adversarial Networks (GANs)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> for synthetic data generation.
Using the pre-trained model as a discriminator, it trains the generator so that its outputs can be well classified by the discriminator.
Then, using the synthetic data samples collected from the generator,
the quantized model can be finetuned with knowledge distillation from the full-precision counterpart (see Section&nbsp;<a href="#S4.SS4" title="IV-D Distillation-Assisted Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a> for more details).
However, this method fails to capture the internal statistics (e.g., distributions of the intermediate layer activations) of the real data, as it is generated only using the final outputs of the model.
Synthetic data which does not take the internal statistics into account may not properly represent the real data distribution&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>.
To address this, a number of subsequent efforts use the statistics stored in Batch Normalization (BatchNorm)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>, i.e., channel-wise mean and variance, to generate more realistic synthetic data.
In particular, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> generates data by directly minimizing the KL divergence of the internal statistics, and it uses the synthetic data to calibrate and finetune the quantized models.
Furthermore, ZeroQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> shows that the synthetic data can be used for sensitivity measurement as well as calibration, thereby enabling mixed-precision post-training quantization without any access to the training/validation data.
ZeroQ also extends ZSQ to the object detection tasks, as it does not rely on the output labels when generating data.
Both&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> and&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> set the input images as trainable parameters and directly perform backpropagation on them until their internal statistics become similar to those of the real data.
To take a step further, recent research&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib259" title="" class="ltx_ref">259</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> finds it effective to train and exploit generative models that can better capture the real data distribution and generate more realistic synthetic data.</p>
</div>
<div id="S3.SS7.SSS3.p3" class="ltx_para">
<p id="S3.SS7.SSS3.p3.1" class="ltx_p"><span id="S3.SS7.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Summary (ZSQ).</span> Zero Shot (aka data free) quantization
performs the entire quantization without any access to the training/validation
data. This is particularly important for Machine Learning as a Service (MLaaS) providers who want to accelerate the deployment of a customer’s workload,
without the need to access their dataset. Moreover, this is important
for cases where security or privacy concerns may limit access to the training
data.</p>
</div>
<figure id="S3.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2103.13630/assets/x8.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="113" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2103.13630/assets/x9.png" id="S3.F7.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="192" height="100" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>(Left) Comparison between peak throughput for different bit-precision logic on Titan RTX and A100 GPU.
(Right) Comparison of the corresponding energy cost and relative area cost for different precision for 45nm technology&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>.
As one can see, lower precision provides exponentially better energy efficiency and higher throughput.
</figcaption>
</figure>
</section>
</section>
<section id="S3.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS8.5.1.1" class="ltx_text">III-H</span> </span><span id="S3.SS8.6.2" class="ltx_text ltx_font_italic">Stochastic Quantization</span>
</h3>

<div id="S3.SS8.p1" class="ltx_para">
<p id="S3.SS8.p1.1" class="ltx_p">During inference, the quantization scheme is usually deterministic.
However, this is not the only possibility, and some works have explored stochastic quantization for quantization aware training as well
as reduced precision training&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>.
The high level intuition has been that the stochastic quantization may
allow a NN to explore more, as compared to deterministic quantization.
One popular supporting argument has been that small weight updates may not lead to any
weight change, as the rounding operation may always return the same weights. However, enabling
a stochastic rounding may provide the NN an opportunity to <em id="S3.SS8.p1.1.1" class="ltx_emph ltx_font_italic">escape</em>, thereby updating
its parameters.</p>
</div>
<div id="S3.SS8.p2" class="ltx_para">
<p id="S3.SS8.p2.1" class="ltx_p">More formally, stochastic quantization maps the floating number up or down with a probability associated
to the magnitude of the weight update.
For instance, in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, the <span id="S3.SS8.p2.1.1" class="ltx_text ltx_markedasmath">Int</span> operator in&nbsp;Eq.&nbsp;<a href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is defined as</p>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.3" class="ltx_Math" alttext="\small\text{Int}(x)=\begin{cases}\lfloor x\rfloor\quad\text{with probability }\lceil x\rceil-x,\\
\lceil x\rceil\quad\text{with probability }x-\lfloor x\rfloor.\end{cases}" display="block"><semantics id="S3.E8.m1.3a"><mrow id="S3.E8.m1.3.4" xref="S3.E8.m1.3.4.cmml"><mrow id="S3.E8.m1.3.4.2" xref="S3.E8.m1.3.4.2.cmml"><mtext mathsize="90%" id="S3.E8.m1.3.4.2.2" xref="S3.E8.m1.3.4.2.2a.cmml">Int</mtext><mo lspace="0em" rspace="0em" id="S3.E8.m1.3.4.2.1" xref="S3.E8.m1.3.4.2.1.cmml">​</mo><mrow id="S3.E8.m1.3.4.2.3.2" xref="S3.E8.m1.3.4.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E8.m1.3.4.2.3.2.1" xref="S3.E8.m1.3.4.2.cmml">(</mo><mi mathsize="90%" id="S3.E8.m1.3.3" xref="S3.E8.m1.3.3.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E8.m1.3.4.2.3.2.2" xref="S3.E8.m1.3.4.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E8.m1.3.4.1" xref="S3.E8.m1.3.4.1.cmml">=</mo><mrow id="S3.E8.m1.2.2" xref="S3.E8.m1.3.4.3.1.cmml"><mo id="S3.E8.m1.2.2.3" xref="S3.E8.m1.3.4.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E8.m1.2.2.2" xref="S3.E8.m1.3.4.3.1.cmml"><mtr id="S3.E8.m1.2.2.2a" xref="S3.E8.m1.3.4.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E8.m1.2.2.2b" xref="S3.E8.m1.3.4.3.1.cmml"><mrow id="S3.E8.m1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.3.1.cmml"><mrow id="S3.E8.m1.1.1.1.1.1.1.3.1" xref="S3.E8.m1.1.1.1.1.1.1.3.1.cmml"><mrow id="S3.E8.m1.1.1.1.1.1.1.3.1.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.cmml"><mrow id="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.2.1" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.1.1.cmml">⌊</mo><mi mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.2.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.1.1.cmml">⌋</mo></mrow><mo lspace="0.900em" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.1" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.1.cmml">​</mo><mtext mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.3" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.3a.cmml">with probability&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.1a" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.1.cmml">​</mo><mrow id="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.2.1" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.1.1.cmml">⌈</mo><mi mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.2.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.2.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.1.1.cmml">⌉</mo></mrow></mrow><mo mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.1" xref="S3.E8.m1.1.1.1.1.1.1.3.1.1.cmml">−</mo><mi mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.3" xref="S3.E8.m1.1.1.1.1.1.1.3.1.3.cmml">x</mi></mrow><mo mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.cmml">,</mo></mrow></mtd><mtd id="S3.E8.m1.2.2.2c" xref="S3.E8.m1.3.4.3.1.1.cmml"></mtd></mtr><mtr id="S3.E8.m1.2.2.2d" xref="S3.E8.m1.3.4.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E8.m1.2.2.2e" xref="S3.E8.m1.3.4.3.1.cmml"><mrow id="S3.E8.m1.2.2.2.2.1.1.3" xref="S3.E8.m1.2.2.2.2.1.1.3.1.cmml"><mrow id="S3.E8.m1.2.2.2.2.1.1.3.1" xref="S3.E8.m1.2.2.2.2.1.1.3.1.cmml"><mrow id="S3.E8.m1.2.2.2.2.1.1.3.1.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.cmml"><mrow id="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.2.1" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.1.1.cmml">⌈</mo><mi mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.1" xref="S3.E8.m1.2.2.2.2.1.1.1.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.2.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.1.1.cmml">⌉</mo></mrow><mo lspace="0.900em" rspace="0em" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.1" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.1.cmml">​</mo><mtext mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.3" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.3a.cmml">with probability&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.1a" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.1.cmml">​</mo><mi mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.4" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.4.cmml">x</mi></mrow><mo mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.1" xref="S3.E8.m1.2.2.2.2.1.1.3.1.1.cmml">−</mo><mrow id="S3.E8.m1.2.2.2.2.1.1.3.1.3.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.3.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.3.2.1" xref="S3.E8.m1.2.2.2.2.1.1.3.1.3.1.1.cmml">⌊</mo><mi mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.2" xref="S3.E8.m1.2.2.2.2.1.1.2.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.3.2.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.3.1.1.cmml">⌋</mo></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.cmml">.</mo></mrow></mtd><mtd id="S3.E8.m1.2.2.2f" xref="S3.E8.m1.3.4.3.1.1.cmml"></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.3b"><apply id="S3.E8.m1.3.4.cmml" xref="S3.E8.m1.3.4"><eq id="S3.E8.m1.3.4.1.cmml" xref="S3.E8.m1.3.4.1"></eq><apply id="S3.E8.m1.3.4.2.cmml" xref="S3.E8.m1.3.4.2"><times id="S3.E8.m1.3.4.2.1.cmml" xref="S3.E8.m1.3.4.2.1"></times><ci id="S3.E8.m1.3.4.2.2a.cmml" xref="S3.E8.m1.3.4.2.2"><mtext mathsize="90%" id="S3.E8.m1.3.4.2.2.cmml" xref="S3.E8.m1.3.4.2.2">Int</mtext></ci><ci id="S3.E8.m1.3.3.cmml" xref="S3.E8.m1.3.3">𝑥</ci></apply><apply id="S3.E8.m1.3.4.3.1.cmml" xref="S3.E8.m1.2.2"><csymbol cd="latexml" id="S3.E8.m1.3.4.3.1.1.cmml" xref="S3.E8.m1.2.2.3">cases</csymbol><apply id="S3.E8.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3"><minus id="S3.E8.m1.1.1.1.1.1.1.3.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.1"></minus><apply id="S3.E8.m1.1.1.1.1.1.1.3.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2"><times id="S3.E8.m1.1.1.1.1.1.1.3.1.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.1"></times><apply id="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.2"><floor id="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.2.1"></floor><ci id="S3.E8.m1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1">𝑥</ci></apply><ci id="S3.E8.m1.1.1.1.1.1.1.3.1.2.3a.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.3"><mtext mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.3">with probability&nbsp;</mtext></ci><apply id="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.2"><ceiling id="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.2.1"></ceiling><ci id="S3.E8.m1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2">𝑥</ci></apply></apply><ci id="S3.E8.m1.1.1.1.1.1.1.3.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.3">𝑥</ci></apply><ci id="S3.E8.m1.3.4.3.1.3a.cmml" xref="S3.E8.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E8.m1.3.4.3.1.3.cmml" xref="S3.E8.m1.2.2.3">otherwise</mtext></ci><apply id="S3.E8.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3"><minus id="S3.E8.m1.2.2.2.2.1.1.3.1.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.1"></minus><apply id="S3.E8.m1.2.2.2.2.1.1.3.1.2.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2"><times id="S3.E8.m1.2.2.2.2.1.1.3.1.2.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.1"></times><apply id="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.2"><ceiling id="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.1.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.2.1"></ceiling><ci id="S3.E8.m1.2.2.2.2.1.1.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.1">𝑥</ci></apply><ci id="S3.E8.m1.2.2.2.2.1.1.3.1.2.3a.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.3"><mtext mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.3.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.3">with probability&nbsp;</mtext></ci><ci id="S3.E8.m1.2.2.2.2.1.1.3.1.2.4.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.4">𝑥</ci></apply><apply id="S3.E8.m1.2.2.2.2.1.1.3.1.3.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.3.2"><floor id="S3.E8.m1.2.2.2.2.1.1.3.1.3.1.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.3.2.1"></floor><ci id="S3.E8.m1.2.2.2.2.1.1.2.cmml" xref="S3.E8.m1.2.2.2.2.1.1.2">𝑥</ci></apply></apply><ci id="S3.E8.m1.3.4.3.1.5a.cmml" xref="S3.E8.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E8.m1.3.4.3.1.5.cmml" xref="S3.E8.m1.2.2.3">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.3c">\small\text{Int}(x)=\begin{cases}\lfloor x\rfloor\quad\text{with probability }\lceil x\rceil-x,\\
\lceil x\rceil\quad\text{with probability }x-\lfloor x\rfloor.\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p id="S3.SS8.p2.5" class="ltx_p">However, this definition cannot be used for binary quantization.
Hence,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> extends this to</p>
<table id="S3.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E9.m1.3" class="ltx_Math" alttext="\small\text{Binary}(x)=\begin{cases}-1\quad\text{with probability }1-\sigma(x),\\
+1\quad\text{with probability }\sigma(x),\end{cases}" display="block"><semantics id="S3.E9.m1.3a"><mrow id="S3.E9.m1.3.4" xref="S3.E9.m1.3.4.cmml"><mrow id="S3.E9.m1.3.4.2" xref="S3.E9.m1.3.4.2.cmml"><mtext mathsize="90%" id="S3.E9.m1.3.4.2.2" xref="S3.E9.m1.3.4.2.2a.cmml">Binary</mtext><mo lspace="0em" rspace="0em" id="S3.E9.m1.3.4.2.1" xref="S3.E9.m1.3.4.2.1.cmml">​</mo><mrow id="S3.E9.m1.3.4.2.3.2" xref="S3.E9.m1.3.4.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E9.m1.3.4.2.3.2.1" xref="S3.E9.m1.3.4.2.cmml">(</mo><mi mathsize="90%" id="S3.E9.m1.3.3" xref="S3.E9.m1.3.3.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E9.m1.3.4.2.3.2.2" xref="S3.E9.m1.3.4.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E9.m1.3.4.1" xref="S3.E9.m1.3.4.1.cmml">=</mo><mrow id="S3.E9.m1.2.2" xref="S3.E9.m1.3.4.3.1.cmml"><mo id="S3.E9.m1.2.2.3" xref="S3.E9.m1.3.4.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E9.m1.2.2.2" xref="S3.E9.m1.3.4.3.1.cmml"><mtr id="S3.E9.m1.2.2.2a" xref="S3.E9.m1.3.4.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E9.m1.2.2.2b" xref="S3.E9.m1.3.4.3.1.cmml"><mrow id="S3.E9.m1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.cmml"><mrow id="S3.E9.m1.1.1.1.1.1.1.2.1" xref="S3.E9.m1.1.1.1.1.1.1.2.1.cmml"><mrow id="S3.E9.m1.1.1.1.1.1.1.2.1.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.cmml"><mo mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.2a" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.cmml">−</mo><mrow id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.cmml"><mn mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.2.cmml">1</mn><mo lspace="0.900em" rspace="0em" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1.cmml">​</mo><mtext mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3a.cmml">with probability&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1a" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1.cmml">​</mo><mn mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.4" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.4.cmml">1</mn></mrow></mrow><mo mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.1" xref="S3.E9.m1.1.1.1.1.1.1.2.1.1.cmml">−</mo><mrow id="S3.E9.m1.1.1.1.1.1.1.2.1.3" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.cmml"><mi mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.3.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.1.1.1.1.1.2.1.3.1" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.1.cmml">​</mo><mrow id="S3.E9.m1.1.1.1.1.1.1.2.1.3.3.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.cmml"><mo maxsize="90%" minsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.3.3.2.1" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.cmml">(</mo><mi mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.1.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.3.3.2.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.cmml">)</mo></mrow></mrow></mrow><mo mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.cmml">,</mo></mrow></mtd><mtd id="S3.E9.m1.2.2.2c" xref="S3.E9.m1.3.4.3.1.1.cmml"></mtd></mtr><mtr id="S3.E9.m1.2.2.2d" xref="S3.E9.m1.3.4.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E9.m1.2.2.2e" xref="S3.E9.m1.3.4.3.1.cmml"><mrow id="S3.E9.m1.2.2.2.2.1.1.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.cmml"><mrow id="S3.E9.m1.2.2.2.2.1.1.2.1" xref="S3.E9.m1.2.2.2.2.1.1.2.1.cmml"><mo mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1a" xref="S3.E9.m1.2.2.2.2.1.1.2.1.cmml">+</mo><mrow id="S3.E9.m1.2.2.2.2.1.1.2.1.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.cmml"><mn mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.2.cmml">1</mn><mo lspace="0.900em" rspace="0em" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.1" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.1.cmml">​</mo><mtext mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.3" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.3a.cmml">with probability&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.1a" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.1.cmml">​</mo><mi mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.4" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.4.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.1b" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.1.cmml">​</mo><mrow id="S3.E9.m1.2.2.2.2.1.1.2.1.2.5.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.5.2.1" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.cmml">(</mo><mi mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.1" xref="S3.E9.m1.2.2.2.2.1.1.1.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.5.2.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.cmml">)</mo></mrow></mrow></mrow><mo mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.cmml">,</mo></mrow></mtd><mtd id="S3.E9.m1.2.2.2f" xref="S3.E9.m1.3.4.3.1.1.cmml"></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.3b"><apply id="S3.E9.m1.3.4.cmml" xref="S3.E9.m1.3.4"><eq id="S3.E9.m1.3.4.1.cmml" xref="S3.E9.m1.3.4.1"></eq><apply id="S3.E9.m1.3.4.2.cmml" xref="S3.E9.m1.3.4.2"><times id="S3.E9.m1.3.4.2.1.cmml" xref="S3.E9.m1.3.4.2.1"></times><ci id="S3.E9.m1.3.4.2.2a.cmml" xref="S3.E9.m1.3.4.2.2"><mtext mathsize="90%" id="S3.E9.m1.3.4.2.2.cmml" xref="S3.E9.m1.3.4.2.2">Binary</mtext></ci><ci id="S3.E9.m1.3.3.cmml" xref="S3.E9.m1.3.3">𝑥</ci></apply><apply id="S3.E9.m1.3.4.3.1.cmml" xref="S3.E9.m1.2.2"><csymbol cd="latexml" id="S3.E9.m1.3.4.3.1.1.cmml" xref="S3.E9.m1.2.2.3">cases</csymbol><apply id="S3.E9.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2"><minus id="S3.E9.m1.1.1.1.1.1.1.2.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.1"></minus><apply id="S3.E9.m1.1.1.1.1.1.1.2.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2"><minus id="S3.E9.m1.1.1.1.1.1.1.2.1.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2"></minus><apply id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2"><times id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1"></times><cn type="integer" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.2">1</cn><ci id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3a.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3"><mtext mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3">with probability&nbsp;</mtext></ci><cn type="integer" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.4.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.4">1</cn></apply></apply><apply id="S3.E9.m1.1.1.1.1.1.1.2.1.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3"><times id="S3.E9.m1.1.1.1.1.1.1.2.1.3.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.1"></times><ci id="S3.E9.m1.1.1.1.1.1.1.2.1.3.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.2">𝜎</ci><ci id="S3.E9.m1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1">𝑥</ci></apply></apply><ci id="S3.E9.m1.3.4.3.1.3a.cmml" xref="S3.E9.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E9.m1.3.4.3.1.3.cmml" xref="S3.E9.m1.2.2.3">otherwise</mtext></ci><apply id="S3.E9.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2"><plus id="S3.E9.m1.2.2.2.2.1.1.2.1.1.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2"></plus><apply id="S3.E9.m1.2.2.2.2.1.1.2.1.2.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2"><times id="S3.E9.m1.2.2.2.2.1.1.2.1.2.1.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.1"></times><cn type="integer" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.2.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.2">1</cn><ci id="S3.E9.m1.2.2.2.2.1.1.2.1.2.3a.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.3"><mtext mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.3.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.3">with probability&nbsp;</mtext></ci><ci id="S3.E9.m1.2.2.2.2.1.1.2.1.2.4.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.4">𝜎</ci><ci id="S3.E9.m1.2.2.2.2.1.1.1.cmml" xref="S3.E9.m1.2.2.2.2.1.1.1">𝑥</ci></apply></apply><ci id="S3.E9.m1.3.4.3.1.5a.cmml" xref="S3.E9.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E9.m1.3.4.3.1.5.cmml" xref="S3.E9.m1.2.2.3">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.3c">\small\text{Binary}(x)=\begin{cases}-1\quad\text{with probability }1-\sigma(x),\\
+1\quad\text{with probability }\sigma(x),\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p id="S3.SS8.p2.4" class="ltx_p">where <span id="S3.SS8.p2.4.1" class="ltx_text ltx_markedasmath">Binary</span> is a function to binarize the real value <math id="S3.SS8.p2.3.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS8.p2.3.m2.1a"><mi id="S3.SS8.p2.3.m2.1.1" xref="S3.SS8.p2.3.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p2.3.m2.1b"><ci id="S3.SS8.p2.3.m2.1.1.cmml" xref="S3.SS8.p2.3.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p2.3.m2.1c">x</annotation></semantics></math>, and <math id="S3.SS8.p2.4.m3.1" class="ltx_Math" alttext="\sigma(\cdot)" display="inline"><semantics id="S3.SS8.p2.4.m3.1a"><mrow id="S3.SS8.p2.4.m3.1.2" xref="S3.SS8.p2.4.m3.1.2.cmml"><mi id="S3.SS8.p2.4.m3.1.2.2" xref="S3.SS8.p2.4.m3.1.2.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.SS8.p2.4.m3.1.2.1" xref="S3.SS8.p2.4.m3.1.2.1.cmml">​</mo><mrow id="S3.SS8.p2.4.m3.1.2.3.2" xref="S3.SS8.p2.4.m3.1.2.cmml"><mo stretchy="false" id="S3.SS8.p2.4.m3.1.2.3.2.1" xref="S3.SS8.p2.4.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS8.p2.4.m3.1.1" xref="S3.SS8.p2.4.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS8.p2.4.m3.1.2.3.2.2" xref="S3.SS8.p2.4.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS8.p2.4.m3.1b"><apply id="S3.SS8.p2.4.m3.1.2.cmml" xref="S3.SS8.p2.4.m3.1.2"><times id="S3.SS8.p2.4.m3.1.2.1.cmml" xref="S3.SS8.p2.4.m3.1.2.1"></times><ci id="S3.SS8.p2.4.m3.1.2.2.cmml" xref="S3.SS8.p2.4.m3.1.2.2">𝜎</ci><ci id="S3.SS8.p2.4.m3.1.1.cmml" xref="S3.SS8.p2.4.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p2.4.m3.1c">\sigma(\cdot)</annotation></semantics></math> is the sigmoid function.</p>
</div>
<div id="S3.SS8.p3" class="ltx_para">
<p id="S3.SS8.p3.1" class="ltx_p">Recently, another stochastic quantization method is introduced in QuantNoise&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.
QuantNoise quantizes a different random subset of weights during each forward pass and trains the model with unbiased gradients.
This allows lower-bit precision quantization without significant accuracy drop in many computer vision and natural language processing models.
However, a major challenge with stochastic quantization methods is the overhead of creating
random numbers for every single weight update, and as such they are not yet adopted widely in practice.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2103.13630/assets/x10.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Illustration of mixed-precision quantization. In mixed-precision quantization
the goal is to keep sensitive and efficient layers in higher precision,
and only
apply low-precision quantization to insensitive and inefficient layers.
The efficiency metric is hardware dependant, and it could be latency or
energy consumption.
</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Advanced Concepts: Quantization Below 8 bits</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we will discuss more advanced topics in quantization which are
mostly used for sub-INT8 quantization. We will first discuss
simulated quantization and its difference with integer-only quantization in&nbsp;Section&nbsp;<a href="#S4.SS1" title="IV-A Simulated and Integer-only Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>.
Afterward, we will discuss different methods for mixed-precision
quantization in&nbsp;Section&nbsp;<a href="#S4.SS2" title="IV-B Mixed-Precision Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>, followed
by hardware-aware quantization in&nbsp;Section&nbsp;<a href="#S4.SS3" title="IV-C Hardware Aware Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>.
Then we will describe how distillation can be used to boost the quantization
accuracy in&nbsp;Section&nbsp;<a href="#S4.SS4" title="IV-D Distillation-Assisted Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a>, and then we will discuss extremely
low bit precision quantization in&nbsp;Section&nbsp;<a href="#S4.SS5" title="IV-E Extreme Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span></span></a>.
Finally, we will briefly describe the different methods for
vector quantization in&nbsp;Section&nbsp;<a href="#S4.SS6" title="IV-F Vector Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-F</span></span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Simulated and Integer-only Quantization</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">There are two common approaches to deploy a quantized NN model, <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">simulated quantization</span> (aka fake quantization) and
<span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">integer-only quantization</span> (aka fixed-point quantization).
In simulated quantization, the quantized model parameters are stored in low-precision, but the operations
(e.g. matrix multiplications and convolutions) are carried out with floating point arithmetic.
Therefore, the quantized parameters need to be dequantized before the floating point operations as schematically shown in&nbsp;Figure&nbsp;<a href="#S3.F6" title="Figure 6 ‣ III-G2 Post-Training Quantization ‣ III-G Fine-tuning Methods ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (Middle).
As such, one cannot fully benefit from fast and efficient low-precision logic with simulated quantization.
However, in integer-only quantization, all the operations are performed using low-precision integer arithmetic&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib267" title="" class="ltx_ref">267</a>, <a href="#bib.bib132" title="" class="ltx_ref">132</a>, <a href="#bib.bib154" title="" class="ltx_ref">154</a>, <a href="#bib.bib193" title="" class="ltx_ref">193</a>]</cite>, as illustrated in&nbsp;Figure&nbsp;<a href="#S3.F6" title="Figure 6 ‣ III-G2 Post-Training Quantization ‣ III-G Fine-tuning Methods ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (Right).
This permits the entire inference to be carried out with efficient integer arithmetic, without any floating point dequantization of any parameters or activations.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.2" class="ltx_p">In general, performing the inference in full-precision with floating point arithmetic may help the final quantization accuracy, but this comes at the cost of not being
able to benefit from the low-precision logic.
Low-precision logic has multiple benefits over the full-precision counterpart in terms of latency, power consumption, and area efficiency.
As shown in&nbsp;Figure&nbsp;<a href="#S3.F7" title="Figure 7 ‣ III-G3 Zero-shot Quantization ‣ III-G Fine-tuning Methods ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> (left), many hardware processors, including NVIDIA V100 and Titan RTX, support fast processing of low-precision arithmetic that can boost the inference throughput and latency.
Moreover, as illustrated in&nbsp;Figure&nbsp;<a href="#S3.F7" title="Figure 7 ‣ III-G3 Zero-shot Quantization ‣ III-G Fine-tuning Methods ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> (right) for a 45nm technology&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>, low-precision logic is significantly
more efficient in terms of energy and area.
For example, performing INT8 addition is <math id="S4.SS1.p2.1.m1.1" class="ltx_math_unparsed" alttext="30\times" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1b"><mn id="S4.SS1.p2.1.m1.1.1">30</mn><mo lspace="0.222em" id="S4.SS1.p2.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">30\times</annotation></semantics></math> more energy efficient and
<math id="S4.SS1.p2.2.m2.1" class="ltx_math_unparsed" alttext="116\times" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1b"><mn id="S4.SS1.p2.2.m2.1.1">116</mn><mo lspace="0.222em" id="S4.SS1.p2.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">116\times</annotation></semantics></math> more area efficient as compared to FP32 addition&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Notable integer-only quantization works include&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>, which fuses Batch Normalization into the
previous convolution layer, and&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>, which proposes an integer-only computation method for residual networks with batch normalization. However, both methods are limited to ReLU activation.
The recent work of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> addresses this limitation by approximating GELU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, Softmax, and Layer Normalization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> with integer arithmetic and further extends integer-only quantization to Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib243" title="" class="ltx_ref">243</a>]</cite> architectures.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_italic">Dyadic quantization</span> is another class of integer-only quantization,
where all the scaling is performed with dyadic numbers, which
are rational numbers with integer values in their numerator and a power of 2 in the denominator&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib267" title="" class="ltx_ref">267</a>]</cite>.
This results in a computational graph that only requires integer addition, multiplication, bit shifting, but no integer division.
Importantly, in this approach, all the additions (e.g. residual connections) are enforced to have the same dyadic scale, which
can make
the addition logic simpler with higher&nbsp;efficiency.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Summary (Simulated vs Integer-only Quantization).</span>
In general integer-only and dyadic quantization are more desirable as compared
to simulated/fake quantization.
This is because integer-only uses lower precision logic for the arithmetic,
whereas simulated quantization uses floating point logic to perform
the operations.
However, this does not mean that fake quantization
is never useful. In fact, fake quantization methods can be beneficial for problems
that are bandwidth-bound rather than compute-bound, such as in recommendation systems&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite>.
For these tasks, the bottleneck is the memory footprint and the cost of loading parameters from memory.
Therefore, performing fake quantization can be acceptable for these cases.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Mixed-Precision Quantization</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">It is easy to see that the hardware performance improves as we use lower precision quantization.
However, uniformly quantizing a model to ultra low-precision can cause significant accuracy degradation.
It is possible to address this with mixed-precision quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib286" title="" class="ltx_ref">286</a>, <a href="#bib.bib246" title="" class="ltx_ref">246</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib239" title="" class="ltx_ref">239</a>, <a href="#bib.bib263" title="" class="ltx_ref">263</a>, <a href="#bib.bib199" title="" class="ltx_ref">199</a>, <a href="#bib.bib249" title="" class="ltx_ref">249</a>, <a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib187" title="" class="ltx_ref">187</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib211" title="" class="ltx_ref">211</a>, <a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib282" title="" class="ltx_ref">282</a>]</cite>.
In this approach, each layer is quantized with different bit precision, as illustrated in&nbsp;Figure&nbsp;<a href="#S3.F8" title="Figure 8 ‣ III-H Stochastic Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.
One challenge with this approach is that the search space for choosing this bit setting is exponential in the number of layers.
Different approaches have been proposed to address this huge search space.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Selecting this mixed-precision for each layer is essentially a searching problem, and many
different methods have been proposed for it.
The recent work of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib246" title="" class="ltx_ref">246</a>]</cite> proposed a reinforcement learning (RL) based method to determine automatically the quantization policy, and the authors used a hardware simulator to take the hardware accelerator’s feedback in the RL agent feedback.
The paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib254" title="" class="ltx_ref">254</a>]</cite> formulated the mixed-precision configuration searching problem as a Neural Architecture Search (NAS) problem and used the Differentiable NAS (DNAS) method to efficiently explore the search space.
One disadvantage of these exploration-based methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib246" title="" class="ltx_ref">246</a>, <a href="#bib.bib254" title="" class="ltx_ref">254</a>]</cite> is that they often
require large computational resources, and their performance is typically sensitive to hyperparameters and even initialization.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Another class of mixed-precision methods uses periodic function regularization to train mixed-precision models by automatically
distinguishing different layers and their varying importance with respect to accuracy while learning their respective bitwidths&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite>.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Different than these exploration and regularization-based approaches, HAWQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> introduces an automatic way to find
the mixed-precision settings based on second-order sensitivity of the model.
It was theoretically shown that the trace of the second-order operator (i.e., the Hessian)
can be used to measure the sensitivity of a layer to quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, similar to results for
pruning in the seminal work of Optimal Brain Damage&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite>.
In HAWQv2, this method was extended to mixed-precision activation quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>,
and was shown to be more than 100x faster than RL based mixed-precision methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib246" title="" class="ltx_ref">246</a>]</cite>.
Recently, in HAWQv3, an integer-only, hardware-aware quantization was introduced&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib267" title="" class="ltx_ref">267</a>]</cite>
that proposed a fast Integer Linear Programming method to find the optimal
bit precision for a given application-specific constraint (e.g., model size or latency).
This work also addressed the common question about hardware efficiency of mixed-precision
quantization by directly deploying them on T4 GPUs, showing up to 50% speed up with mixed-precision (INT4/INT8) quantization as compared to
INT8 quantization.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Summary (Mixed-precision Quantization).</span> Mixed-precision quantization has
proved to be an effective and hardware-efficient method for low-precision quantization
of different NN models. In this approach, the layers of a NN are grouped
into sensitive/insensitive to quantization, and higher/lower bits
are used for each layer. As such, one can minimize accuracy degradation
and still benefit from reduced memory footprint and faster speed up
with low precision quantization. Recent work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib267" title="" class="ltx_ref">267</a>]</cite> has also shown that this
approach is hardware-efficient as mixed-precision is only used
across operations/layers.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Hardware Aware Quantization</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">One of the goals of quantization is to improve the inference latency.
However, not all hardware provide the same speed up after a certain layer/operation is quantized.
In fact, the benefits from quantization is hardware-dependant, with many factors such
as on-chip memory, bandwidth, and cache hierarchy affecting the quantization speed up.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">It is important to consider this fact for achieving optimal benefits through
hardware-aware quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib256" title="" class="ltx_ref">256</a>, <a href="#bib.bib246" title="" class="ltx_ref">246</a>, <a href="#bib.bib91" title="" class="ltx_ref">91</a>, <a href="#bib.bib254" title="" class="ltx_ref">254</a>, <a href="#bib.bib265" title="" class="ltx_ref">265</a>, <a href="#bib.bib267" title="" class="ltx_ref">267</a>, <a href="#bib.bib250" title="" class="ltx_ref">250</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>.
In particular, the work&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib246" title="" class="ltx_ref">246</a>]</cite> uses a reinforcement learning agent to determine the hardware-aware mixed-precision setting for quantization, based on a look-up table of latency with respect to different layers with different bitwidth.
However, this approach uses simulated hardware latency.
To address this the recent work of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib267" title="" class="ltx_ref">267</a>]</cite> directly deploys quantized operations
in hardware, and measures the actual deployment latency of each layer for different quantization
bit precisions.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Distillation-Assisted Quantization</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">An interesting line of work in quantization is to incorporate model distillation to boost quantization accuracy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>, <a href="#bib.bib177" title="" class="ltx_ref">177</a>, <a href="#bib.bib267" title="" class="ltx_ref">267</a>, <a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>.
Model distillation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib177" title="" class="ltx_ref">177</a>, <a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib195" title="" class="ltx_ref">195</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib270" title="" class="ltx_ref">270</a>, <a href="#bib.bib268" title="" class="ltx_ref">268</a>, <a href="#bib.bib289" title="" class="ltx_ref">289</a>]</cite> is
a method in which a large model with higher accuracy is used as a teacher to
help the training of a compact student model.
During the training of the student model, instead of using just the ground-truth class labels, model distillation proposes to leverage the soft probabilities produced by the teacher,
which may contain more information of the input. That is the overall loss function incorporates both the student loss and the distillation loss, which is typically formulated as follows:</p>
<table id="S4.E10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E10.m1.6" class="ltx_Math" alttext="\small\mathcal{L}=\alpha\mathcal{H}(y,\sigma(z_{s}))+\beta\mathcal{H}(\sigma(z_{t},T),\sigma(z_{s},T))" display="block"><semantics id="S4.E10.m1.6a"><mrow id="S4.E10.m1.6.6" xref="S4.E10.m1.6.6.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S4.E10.m1.6.6.5" xref="S4.E10.m1.6.6.5.cmml">ℒ</mi><mo mathsize="90%" id="S4.E10.m1.6.6.4" xref="S4.E10.m1.6.6.4.cmml">=</mo><mrow id="S4.E10.m1.6.6.3" xref="S4.E10.m1.6.6.3.cmml"><mrow id="S4.E10.m1.4.4.1.1" xref="S4.E10.m1.4.4.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.4.4.1.1.3" xref="S4.E10.m1.4.4.1.1.3.cmml">α</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.4.4.1.1.2" xref="S4.E10.m1.4.4.1.1.2.cmml">​</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S4.E10.m1.4.4.1.1.4" xref="S4.E10.m1.4.4.1.1.4.cmml">ℋ</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.4.4.1.1.2a" xref="S4.E10.m1.4.4.1.1.2.cmml">​</mo><mrow id="S4.E10.m1.4.4.1.1.1.1" xref="S4.E10.m1.4.4.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E10.m1.4.4.1.1.1.1.2" xref="S4.E10.m1.4.4.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="S4.E10.m1.1.1" xref="S4.E10.m1.1.1.cmml">y</mi><mo mathsize="90%" id="S4.E10.m1.4.4.1.1.1.1.3" xref="S4.E10.m1.4.4.1.1.1.2.cmml">,</mo><mrow id="S4.E10.m1.4.4.1.1.1.1.1" xref="S4.E10.m1.4.4.1.1.1.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.4.4.1.1.1.1.1.3" xref="S4.E10.m1.4.4.1.1.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.4.4.1.1.1.1.1.2" xref="S4.E10.m1.4.4.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E10.m1.4.4.1.1.1.1.1.1.1" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S4.E10.m1.4.4.1.1.1.1.1.1.1.2" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">z</mi><mi mathsize="90%" id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">s</mi></msub><mo maxsize="90%" minsize="90%" id="S4.E10.m1.4.4.1.1.1.1.1.1.1.3" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo maxsize="90%" minsize="90%" id="S4.E10.m1.4.4.1.1.1.1.4" xref="S4.E10.m1.4.4.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S4.E10.m1.6.6.3.4" xref="S4.E10.m1.6.6.3.4.cmml">+</mo><mrow id="S4.E10.m1.6.6.3.3" xref="S4.E10.m1.6.6.3.3.cmml"><mi mathsize="90%" id="S4.E10.m1.6.6.3.3.4" xref="S4.E10.m1.6.6.3.3.4.cmml">β</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.6.6.3.3.3" xref="S4.E10.m1.6.6.3.3.3.cmml">​</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S4.E10.m1.6.6.3.3.5" xref="S4.E10.m1.6.6.3.3.5.cmml">ℋ</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.6.6.3.3.3a" xref="S4.E10.m1.6.6.3.3.3.cmml">​</mo><mrow id="S4.E10.m1.6.6.3.3.2.2" xref="S4.E10.m1.6.6.3.3.2.3.cmml"><mo maxsize="90%" minsize="90%" id="S4.E10.m1.6.6.3.3.2.2.3" xref="S4.E10.m1.6.6.3.3.2.3.cmml">(</mo><mrow id="S4.E10.m1.5.5.2.2.1.1.1" xref="S4.E10.m1.5.5.2.2.1.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.3" xref="S4.E10.m1.5.5.2.2.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.5.5.2.2.1.1.1.2" xref="S4.E10.m1.5.5.2.2.1.1.1.2.cmml">​</mo><mrow id="S4.E10.m1.5.5.2.2.1.1.1.1.1" xref="S4.E10.m1.5.5.2.2.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.2" xref="S4.E10.m1.5.5.2.2.1.1.1.1.2.cmml">(</mo><msub id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.2" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.2.cmml">z</mi><mi mathsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.3" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.3.cmml">t</mi></msub><mo mathsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.3" xref="S4.E10.m1.5.5.2.2.1.1.1.1.2.cmml">,</mo><mi mathsize="90%" id="S4.E10.m1.2.2" xref="S4.E10.m1.2.2.cmml">T</mi><mo maxsize="90%" minsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.4" xref="S4.E10.m1.5.5.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S4.E10.m1.6.6.3.3.2.2.4" xref="S4.E10.m1.6.6.3.3.2.3.cmml">,</mo><mrow id="S4.E10.m1.6.6.3.3.2.2.2" xref="S4.E10.m1.6.6.3.3.2.2.2.cmml"><mi mathsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.3" xref="S4.E10.m1.6.6.3.3.2.2.2.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.6.6.3.3.2.2.2.2" xref="S4.E10.m1.6.6.3.3.2.2.2.2.cmml">​</mo><mrow id="S4.E10.m1.6.6.3.3.2.2.2.1.1" xref="S4.E10.m1.6.6.3.3.2.2.2.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.2" xref="S4.E10.m1.6.6.3.3.2.2.2.1.2.cmml">(</mo><msub id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.2" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.2.cmml">z</mi><mi mathsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.3" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.3.cmml">s</mi></msub><mo mathsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.3" xref="S4.E10.m1.6.6.3.3.2.2.2.1.2.cmml">,</mo><mi mathsize="90%" id="S4.E10.m1.3.3" xref="S4.E10.m1.3.3.cmml">T</mi><mo maxsize="90%" minsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.4" xref="S4.E10.m1.6.6.3.3.2.2.2.1.2.cmml">)</mo></mrow></mrow><mo maxsize="90%" minsize="90%" id="S4.E10.m1.6.6.3.3.2.2.5" xref="S4.E10.m1.6.6.3.3.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E10.m1.6b"><apply id="S4.E10.m1.6.6.cmml" xref="S4.E10.m1.6.6"><eq id="S4.E10.m1.6.6.4.cmml" xref="S4.E10.m1.6.6.4"></eq><ci id="S4.E10.m1.6.6.5.cmml" xref="S4.E10.m1.6.6.5">ℒ</ci><apply id="S4.E10.m1.6.6.3.cmml" xref="S4.E10.m1.6.6.3"><plus id="S4.E10.m1.6.6.3.4.cmml" xref="S4.E10.m1.6.6.3.4"></plus><apply id="S4.E10.m1.4.4.1.1.cmml" xref="S4.E10.m1.4.4.1.1"><times id="S4.E10.m1.4.4.1.1.2.cmml" xref="S4.E10.m1.4.4.1.1.2"></times><ci id="S4.E10.m1.4.4.1.1.3.cmml" xref="S4.E10.m1.4.4.1.1.3">𝛼</ci><ci id="S4.E10.m1.4.4.1.1.4.cmml" xref="S4.E10.m1.4.4.1.1.4">ℋ</ci><interval closure="open" id="S4.E10.m1.4.4.1.1.1.2.cmml" xref="S4.E10.m1.4.4.1.1.1.1"><ci id="S4.E10.m1.1.1.cmml" xref="S4.E10.m1.1.1">𝑦</ci><apply id="S4.E10.m1.4.4.1.1.1.1.1.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1"><times id="S4.E10.m1.4.4.1.1.1.1.1.2.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.2"></times><ci id="S4.E10.m1.4.4.1.1.1.1.1.3.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.3">𝜎</ci><apply id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.3">𝑠</ci></apply></apply></interval></apply><apply id="S4.E10.m1.6.6.3.3.cmml" xref="S4.E10.m1.6.6.3.3"><times id="S4.E10.m1.6.6.3.3.3.cmml" xref="S4.E10.m1.6.6.3.3.3"></times><ci id="S4.E10.m1.6.6.3.3.4.cmml" xref="S4.E10.m1.6.6.3.3.4">𝛽</ci><ci id="S4.E10.m1.6.6.3.3.5.cmml" xref="S4.E10.m1.6.6.3.3.5">ℋ</ci><interval closure="open" id="S4.E10.m1.6.6.3.3.2.3.cmml" xref="S4.E10.m1.6.6.3.3.2.2"><apply id="S4.E10.m1.5.5.2.2.1.1.1.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1"><times id="S4.E10.m1.5.5.2.2.1.1.1.2.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.2"></times><ci id="S4.E10.m1.5.5.2.2.1.1.1.3.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.3">𝜎</ci><interval closure="open" id="S4.E10.m1.5.5.2.2.1.1.1.1.2.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1"><apply id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.2">𝑧</ci><ci id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S4.E10.m1.2.2.cmml" xref="S4.E10.m1.2.2">𝑇</ci></interval></apply><apply id="S4.E10.m1.6.6.3.3.2.2.2.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2"><times id="S4.E10.m1.6.6.3.3.2.2.2.2.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.2"></times><ci id="S4.E10.m1.6.6.3.3.2.2.2.3.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.3">𝜎</ci><interval closure="open" id="S4.E10.m1.6.6.3.3.2.2.2.1.2.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1"><apply id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.1.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1">subscript</csymbol><ci id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.2.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.2">𝑧</ci><ci id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.3.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.3">𝑠</ci></apply><ci id="S4.E10.m1.3.3.cmml" xref="S4.E10.m1.3.3">𝑇</ci></interval></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E10.m1.6c">\small\mathcal{L}=\alpha\mathcal{H}(y,\sigma(z_{s}))+\beta\mathcal{H}(\sigma(z_{t},T),\sigma(z_{s},T))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.7" class="ltx_p">In&nbsp;Eq.&nbsp;<a href="#S4.E10" title="In IV-D Distillation-Assisted Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mi id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><ci id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">\alpha</annotation></semantics></math> and <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mi id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><ci id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">\beta</annotation></semantics></math> are weighting
coefficients to tune the amount of loss from the student model and the distillation loss, <math id="S4.SS4.p2.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS4.p2.3.m3.1a"><mi id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><ci id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">y</annotation></semantics></math> is the ground-truth class label, <math id="S4.SS4.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><semantics id="S4.SS4.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml">ℋ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><ci id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1">ℋ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">\mathcal{H}</annotation></semantics></math> is the cross-entropy loss function,
<math id="S4.SS4.p2.5.m5.1" class="ltx_Math" alttext="z_{s}" display="inline"><semantics id="S4.SS4.p2.5.m5.1a"><msub id="S4.SS4.p2.5.m5.1.1" xref="S4.SS4.p2.5.m5.1.1.cmml"><mi id="S4.SS4.p2.5.m5.1.1.2" xref="S4.SS4.p2.5.m5.1.1.2.cmml">z</mi><mi id="S4.SS4.p2.5.m5.1.1.3" xref="S4.SS4.p2.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.5.m5.1b"><apply id="S4.SS4.p2.5.m5.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.5.m5.1.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS4.p2.5.m5.1.1.2.cmml" xref="S4.SS4.p2.5.m5.1.1.2">𝑧</ci><ci id="S4.SS4.p2.5.m5.1.1.3.cmml" xref="S4.SS4.p2.5.m5.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.5.m5.1c">z_{s}</annotation></semantics></math>/<math id="S4.SS4.p2.6.m6.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S4.SS4.p2.6.m6.1a"><msub id="S4.SS4.p2.6.m6.1.1" xref="S4.SS4.p2.6.m6.1.1.cmml"><mi id="S4.SS4.p2.6.m6.1.1.2" xref="S4.SS4.p2.6.m6.1.1.2.cmml">z</mi><mi id="S4.SS4.p2.6.m6.1.1.3" xref="S4.SS4.p2.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.6.m6.1b"><apply id="S4.SS4.p2.6.m6.1.1.cmml" xref="S4.SS4.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.6.m6.1.1.1.cmml" xref="S4.SS4.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS4.p2.6.m6.1.1.2.cmml" xref="S4.SS4.p2.6.m6.1.1.2">𝑧</ci><ci id="S4.SS4.p2.6.m6.1.1.3.cmml" xref="S4.SS4.p2.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.6.m6.1c">z_{t}</annotation></semantics></math> are logits generated by the student/teacher model,
<math id="S4.SS4.p2.7.m7.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.SS4.p2.7.m7.1a"><mi id="S4.SS4.p2.7.m7.1.1" xref="S4.SS4.p2.7.m7.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.7.m7.1b"><ci id="S4.SS4.p2.7.m7.1.1.cmml" xref="S4.SS4.p2.7.m7.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.7.m7.1c">\sigma</annotation></semantics></math> is the softmax function, and T is its temperature defined as follows:</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<table id="S4.E11" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E11.m1.1" class="ltx_Math" alttext="\small p_{i}=\frac{\exp{\frac{z_{i}}{T}}}{\sum_{j}\exp{\frac{z_{j}}{T}}}" display="block"><semantics id="S4.E11.m1.1a"><mrow id="S4.E11.m1.1.1" xref="S4.E11.m1.1.1.cmml"><msub id="S4.E11.m1.1.1.2" xref="S4.E11.m1.1.1.2.cmml"><mi mathsize="90%" id="S4.E11.m1.1.1.2.2" xref="S4.E11.m1.1.1.2.2.cmml">p</mi><mi mathsize="90%" id="S4.E11.m1.1.1.2.3" xref="S4.E11.m1.1.1.2.3.cmml">i</mi></msub><mo mathsize="90%" id="S4.E11.m1.1.1.1" xref="S4.E11.m1.1.1.1.cmml">=</mo><mfrac id="S4.E11.m1.1.1.3" xref="S4.E11.m1.1.1.3.cmml"><mrow id="S4.E11.m1.1.1.3.2" xref="S4.E11.m1.1.1.3.2.cmml"><mi mathsize="90%" id="S4.E11.m1.1.1.3.2.1" xref="S4.E11.m1.1.1.3.2.1.cmml">exp</mi><mo lspace="0.167em" id="S4.E11.m1.1.1.3.2a" xref="S4.E11.m1.1.1.3.2.cmml">⁡</mo><mfrac id="S4.E11.m1.1.1.3.2.2" xref="S4.E11.m1.1.1.3.2.2.cmml"><msub id="S4.E11.m1.1.1.3.2.2.2" xref="S4.E11.m1.1.1.3.2.2.2.cmml"><mi mathsize="90%" id="S4.E11.m1.1.1.3.2.2.2.2" xref="S4.E11.m1.1.1.3.2.2.2.2.cmml">z</mi><mi mathsize="90%" id="S4.E11.m1.1.1.3.2.2.2.3" xref="S4.E11.m1.1.1.3.2.2.2.3.cmml">i</mi></msub><mi mathsize="90%" id="S4.E11.m1.1.1.3.2.2.3" xref="S4.E11.m1.1.1.3.2.2.3.cmml">T</mi></mfrac></mrow><mrow id="S4.E11.m1.1.1.3.3" xref="S4.E11.m1.1.1.3.3.cmml"><msub id="S4.E11.m1.1.1.3.3.1" xref="S4.E11.m1.1.1.3.3.1.cmml"><mo maxsize="90%" minsize="90%" stretchy="true" id="S4.E11.m1.1.1.3.3.1.2" xref="S4.E11.m1.1.1.3.3.1.2.cmml">∑</mo><mi mathsize="90%" id="S4.E11.m1.1.1.3.3.1.3" xref="S4.E11.m1.1.1.3.3.1.3.cmml">j</mi></msub><mrow id="S4.E11.m1.1.1.3.3.2" xref="S4.E11.m1.1.1.3.3.2.cmml"><mi mathsize="90%" id="S4.E11.m1.1.1.3.3.2.1" xref="S4.E11.m1.1.1.3.3.2.1.cmml">exp</mi><mo lspace="0.167em" id="S4.E11.m1.1.1.3.3.2a" xref="S4.E11.m1.1.1.3.3.2.cmml">⁡</mo><mfrac id="S4.E11.m1.1.1.3.3.2.2" xref="S4.E11.m1.1.1.3.3.2.2.cmml"><msub id="S4.E11.m1.1.1.3.3.2.2.2" xref="S4.E11.m1.1.1.3.3.2.2.2.cmml"><mi mathsize="90%" id="S4.E11.m1.1.1.3.3.2.2.2.2" xref="S4.E11.m1.1.1.3.3.2.2.2.2.cmml">z</mi><mi mathsize="90%" id="S4.E11.m1.1.1.3.3.2.2.2.3" xref="S4.E11.m1.1.1.3.3.2.2.2.3.cmml">j</mi></msub><mi mathsize="90%" id="S4.E11.m1.1.1.3.3.2.2.3" xref="S4.E11.m1.1.1.3.3.2.2.3.cmml">T</mi></mfrac></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E11.m1.1b"><apply id="S4.E11.m1.1.1.cmml" xref="S4.E11.m1.1.1"><eq id="S4.E11.m1.1.1.1.cmml" xref="S4.E11.m1.1.1.1"></eq><apply id="S4.E11.m1.1.1.2.cmml" xref="S4.E11.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E11.m1.1.1.2.1.cmml" xref="S4.E11.m1.1.1.2">subscript</csymbol><ci id="S4.E11.m1.1.1.2.2.cmml" xref="S4.E11.m1.1.1.2.2">𝑝</ci><ci id="S4.E11.m1.1.1.2.3.cmml" xref="S4.E11.m1.1.1.2.3">𝑖</ci></apply><apply id="S4.E11.m1.1.1.3.cmml" xref="S4.E11.m1.1.1.3"><divide id="S4.E11.m1.1.1.3.1.cmml" xref="S4.E11.m1.1.1.3"></divide><apply id="S4.E11.m1.1.1.3.2.cmml" xref="S4.E11.m1.1.1.3.2"><exp id="S4.E11.m1.1.1.3.2.1.cmml" xref="S4.E11.m1.1.1.3.2.1"></exp><apply id="S4.E11.m1.1.1.3.2.2.cmml" xref="S4.E11.m1.1.1.3.2.2"><divide id="S4.E11.m1.1.1.3.2.2.1.cmml" xref="S4.E11.m1.1.1.3.2.2"></divide><apply id="S4.E11.m1.1.1.3.2.2.2.cmml" xref="S4.E11.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S4.E11.m1.1.1.3.2.2.2.1.cmml" xref="S4.E11.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S4.E11.m1.1.1.3.2.2.2.2.cmml" xref="S4.E11.m1.1.1.3.2.2.2.2">𝑧</ci><ci id="S4.E11.m1.1.1.3.2.2.2.3.cmml" xref="S4.E11.m1.1.1.3.2.2.2.3">𝑖</ci></apply><ci id="S4.E11.m1.1.1.3.2.2.3.cmml" xref="S4.E11.m1.1.1.3.2.2.3">𝑇</ci></apply></apply><apply id="S4.E11.m1.1.1.3.3.cmml" xref="S4.E11.m1.1.1.3.3"><apply id="S4.E11.m1.1.1.3.3.1.cmml" xref="S4.E11.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E11.m1.1.1.3.3.1.1.cmml" xref="S4.E11.m1.1.1.3.3.1">subscript</csymbol><sum id="S4.E11.m1.1.1.3.3.1.2.cmml" xref="S4.E11.m1.1.1.3.3.1.2"></sum><ci id="S4.E11.m1.1.1.3.3.1.3.cmml" xref="S4.E11.m1.1.1.3.3.1.3">𝑗</ci></apply><apply id="S4.E11.m1.1.1.3.3.2.cmml" xref="S4.E11.m1.1.1.3.3.2"><exp id="S4.E11.m1.1.1.3.3.2.1.cmml" xref="S4.E11.m1.1.1.3.3.2.1"></exp><apply id="S4.E11.m1.1.1.3.3.2.2.cmml" xref="S4.E11.m1.1.1.3.3.2.2"><divide id="S4.E11.m1.1.1.3.3.2.2.1.cmml" xref="S4.E11.m1.1.1.3.3.2.2"></divide><apply id="S4.E11.m1.1.1.3.3.2.2.2.cmml" xref="S4.E11.m1.1.1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.E11.m1.1.1.3.3.2.2.2.1.cmml" xref="S4.E11.m1.1.1.3.3.2.2.2">subscript</csymbol><ci id="S4.E11.m1.1.1.3.3.2.2.2.2.cmml" xref="S4.E11.m1.1.1.3.3.2.2.2.2">𝑧</ci><ci id="S4.E11.m1.1.1.3.3.2.2.2.3.cmml" xref="S4.E11.m1.1.1.3.3.2.2.2.3">𝑗</ci></apply><ci id="S4.E11.m1.1.1.3.3.2.2.3.cmml" xref="S4.E11.m1.1.1.3.3.2.2.3">𝑇</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E11.m1.1c">\small p_{i}=\frac{\exp{\frac{z_{i}}{T}}}{\sum_{j}\exp{\frac{z_{j}}{T}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Previous methods of knowledge distillation focus on exploring different knowledge sources.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>, <a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib192" title="" class="ltx_ref">192</a>]</cite> use logits (the soft probabilities) as the source of knowledge, while&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>, <a href="#bib.bib269" title="" class="ltx_ref">269</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> try to leverage the knowledge from intermediate layers. The choices of teacher models are also well studied, where&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib273" title="" class="ltx_ref">273</a>, <a href="#bib.bib235" title="" class="ltx_ref">235</a>]</cite> use multiple teacher models to jointly supervise the student model, while&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib277" title="" class="ltx_ref">277</a>]</cite> apply self-distillation without an extra teacher model.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.5.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.6.2" class="ltx_text ltx_font_italic">Extreme Quantization</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Binarization, where the quantized values are constrained to a 1-bit representation, thereby drastically reducing the memory requirement by 32<math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mo id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><times id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">\times</annotation></semantics></math>, is the most extreme quantization method.
Besides the memory advantages, binary (1-bit) and ternary (2-bit) operations can often be computed efficiently with bit-wise arithmetic and can
achieve significant acceleration over higher precisions, such as FP32 and INT8.
For instance, the peak binary arithmetic on NVIDIA V100 GPUs is 8x higher than INT8.
However, a naive binarization method would lead to significant accuracy degradation.
As such, there is a large body of work that has proposed different solutions to address this&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>, <a href="#bib.bib135" title="" class="ltx_ref">135</a>, <a href="#bib.bib249" title="" class="ltx_ref">249</a>, <a href="#bib.bib262" title="" class="ltx_ref">262</a>, <a href="#bib.bib122" title="" class="ltx_ref">122</a>, <a href="#bib.bib198" title="" class="ltx_ref">198</a>, <a href="#bib.bib290" title="" class="ltx_ref">290</a>, <a href="#bib.bib251" title="" class="ltx_ref">251</a>, <a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib288" title="" class="ltx_ref">288</a>, <a href="#bib.bib260" title="" class="ltx_ref">260</a>, <a href="#bib.bib92" title="" class="ltx_ref">92</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib124" title="" class="ltx_ref">124</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib141" title="" class="ltx_ref">141</a>, <a href="#bib.bib217" title="" class="ltx_ref">217</a>, <a href="#bib.bib120" title="" class="ltx_ref">120</a>, <a href="#bib.bib155" title="" class="ltx_ref">155</a>, <a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib196" title="" class="ltx_ref">196</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib205" title="" class="ltx_ref">205</a>]</cite>.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.6" class="ltx_p">An important work here is
BinaryConnect&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> which constrains the weights to either +1 or -1.
In this approach, the weights are kept as real values and are only binarized during the forward and backward passes to simulate the binarization effect.
During the forward pass, the real-value weights are converted into +1 or -1 based on the sign function.
Then the network can be trained using the standard training method with STE to propagate the gradients through the non-differentiable sign function. Binarized NN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> (BNN) extends this idea by binarizing the activations as well as the weights.
Jointly binarizing weights and activations has the additional benefit of improved latency,
since the costly floating-point matrix multiplications can be replaced with lightweight XNOR operations followed by bit-counting.
Another interesting work is Binary Weight Network (BWN) and XNOR-Net proposed in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>,
which achieve higher accuracy by incorporating a scaling factor to the weights and using +<math id="S4.SS5.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS5.p2.1.m1.1a"><mi id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><ci id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">\alpha</annotation></semantics></math> or -<math id="S4.SS5.p2.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS5.p2.2.m2.1a"><mi id="S4.SS5.p2.2.m2.1.1" xref="S4.SS5.p2.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><ci id="S4.SS5.p2.2.m2.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">\alpha</annotation></semantics></math> instead of +1 or -1.
Here, <math id="S4.SS5.p2.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS5.p2.3.m3.1a"><mi id="S4.SS5.p2.3.m3.1.1" xref="S4.SS5.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.3.m3.1b"><ci id="S4.SS5.p2.3.m3.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.3.m3.1c">\alpha</annotation></semantics></math> is the scaling factor chosen to minimize the distance between the real-valued weights and the resulting binarized weights.
In other words, a real-valued weight matrix <math id="S4.SS5.p2.4.m4.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S4.SS5.p2.4.m4.1a"><mi id="S4.SS5.p2.4.m4.1.1" xref="S4.SS5.p2.4.m4.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.4.m4.1b"><ci id="S4.SS5.p2.4.m4.1.1.cmml" xref="S4.SS5.p2.4.m4.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.4.m4.1c">W</annotation></semantics></math> can be formulated as <math id="S4.SS5.p2.5.m5.1" class="ltx_Math" alttext="W\approx\alpha B" display="inline"><semantics id="S4.SS5.p2.5.m5.1a"><mrow id="S4.SS5.p2.5.m5.1.1" xref="S4.SS5.p2.5.m5.1.1.cmml"><mi id="S4.SS5.p2.5.m5.1.1.2" xref="S4.SS5.p2.5.m5.1.1.2.cmml">W</mi><mo id="S4.SS5.p2.5.m5.1.1.1" xref="S4.SS5.p2.5.m5.1.1.1.cmml">≈</mo><mrow id="S4.SS5.p2.5.m5.1.1.3" xref="S4.SS5.p2.5.m5.1.1.3.cmml"><mi id="S4.SS5.p2.5.m5.1.1.3.2" xref="S4.SS5.p2.5.m5.1.1.3.2.cmml">α</mi><mo lspace="0em" rspace="0em" id="S4.SS5.p2.5.m5.1.1.3.1" xref="S4.SS5.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p2.5.m5.1.1.3.3" xref="S4.SS5.p2.5.m5.1.1.3.3.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.5.m5.1b"><apply id="S4.SS5.p2.5.m5.1.1.cmml" xref="S4.SS5.p2.5.m5.1.1"><approx id="S4.SS5.p2.5.m5.1.1.1.cmml" xref="S4.SS5.p2.5.m5.1.1.1"></approx><ci id="S4.SS5.p2.5.m5.1.1.2.cmml" xref="S4.SS5.p2.5.m5.1.1.2">𝑊</ci><apply id="S4.SS5.p2.5.m5.1.1.3.cmml" xref="S4.SS5.p2.5.m5.1.1.3"><times id="S4.SS5.p2.5.m5.1.1.3.1.cmml" xref="S4.SS5.p2.5.m5.1.1.3.1"></times><ci id="S4.SS5.p2.5.m5.1.1.3.2.cmml" xref="S4.SS5.p2.5.m5.1.1.3.2">𝛼</ci><ci id="S4.SS5.p2.5.m5.1.1.3.3.cmml" xref="S4.SS5.p2.5.m5.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.5.m5.1c">W\approx\alpha B</annotation></semantics></math>, where <math id="S4.SS5.p2.6.m6.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS5.p2.6.m6.1a"><mi id="S4.SS5.p2.6.m6.1.1" xref="S4.SS5.p2.6.m6.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.6.m6.1b"><ci id="S4.SS5.p2.6.m6.1.1.cmml" xref="S4.SS5.p2.6.m6.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.6.m6.1c">B</annotation></semantics></math> is a binary weight matrix that satisfies the following optimization problem:</p>
<table id="S4.E12" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E12.m1.3" class="ltx_Math" alttext="\small\alpha,B=\mathrm{argmin}\|W-\alpha B\|^{2}." display="block"><semantics id="S4.E12.m1.3a"><mrow id="S4.E12.m1.3.3.1" xref="S4.E12.m1.3.3.1.1.cmml"><mrow id="S4.E12.m1.3.3.1.1" xref="S4.E12.m1.3.3.1.1.cmml"><mrow id="S4.E12.m1.3.3.1.1.3.2" xref="S4.E12.m1.3.3.1.1.3.1.cmml"><mi mathsize="90%" id="S4.E12.m1.1.1" xref="S4.E12.m1.1.1.cmml">α</mi><mo mathsize="90%" id="S4.E12.m1.3.3.1.1.3.2.1" xref="S4.E12.m1.3.3.1.1.3.1.cmml">,</mo><mi mathsize="90%" id="S4.E12.m1.2.2" xref="S4.E12.m1.2.2.cmml">B</mi></mrow><mo mathsize="90%" id="S4.E12.m1.3.3.1.1.2" xref="S4.E12.m1.3.3.1.1.2.cmml">=</mo><mrow id="S4.E12.m1.3.3.1.1.1" xref="S4.E12.m1.3.3.1.1.1.cmml"><mi mathsize="90%" id="S4.E12.m1.3.3.1.1.1.3" xref="S4.E12.m1.3.3.1.1.1.3.cmml">argmin</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.3.3.1.1.1.2" xref="S4.E12.m1.3.3.1.1.1.2.cmml">​</mo><msup id="S4.E12.m1.3.3.1.1.1.1" xref="S4.E12.m1.3.3.1.1.1.1.cmml"><mrow id="S4.E12.m1.3.3.1.1.1.1.1.1" xref="S4.E12.m1.3.3.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.2" xref="S4.E12.m1.3.3.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E12.m1.3.3.1.1.1.1.1.1.1" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.1.2" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.2.cmml">W</mi><mo mathsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.1.1" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">α</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.1" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">B</mi></mrow></mrow><mo maxsize="90%" minsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.3" xref="S4.E12.m1.3.3.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn mathsize="90%" id="S4.E12.m1.3.3.1.1.1.1.3" xref="S4.E12.m1.3.3.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow><mo lspace="0em" mathsize="90%" id="S4.E12.m1.3.3.1.2" xref="S4.E12.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E12.m1.3b"><apply id="S4.E12.m1.3.3.1.1.cmml" xref="S4.E12.m1.3.3.1"><eq id="S4.E12.m1.3.3.1.1.2.cmml" xref="S4.E12.m1.3.3.1.1.2"></eq><list id="S4.E12.m1.3.3.1.1.3.1.cmml" xref="S4.E12.m1.3.3.1.1.3.2"><ci id="S4.E12.m1.1.1.cmml" xref="S4.E12.m1.1.1">𝛼</ci><ci id="S4.E12.m1.2.2.cmml" xref="S4.E12.m1.2.2">𝐵</ci></list><apply id="S4.E12.m1.3.3.1.1.1.cmml" xref="S4.E12.m1.3.3.1.1.1"><times id="S4.E12.m1.3.3.1.1.1.2.cmml" xref="S4.E12.m1.3.3.1.1.1.2"></times><ci id="S4.E12.m1.3.3.1.1.1.3.cmml" xref="S4.E12.m1.3.3.1.1.1.3">argmin</ci><apply id="S4.E12.m1.3.3.1.1.1.1.cmml" xref="S4.E12.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.E12.m1.3.3.1.1.1.1.2.cmml" xref="S4.E12.m1.3.3.1.1.1.1">superscript</csymbol><apply id="S4.E12.m1.3.3.1.1.1.1.1.2.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E12.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E12.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1"><minus id="S4.E12.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.1"></minus><ci id="S4.E12.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.2">𝑊</ci><apply id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3"><times id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.1"></times><ci id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.2">𝛼</ci><ci id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.3">𝐵</ci></apply></apply></apply><cn type="integer" id="S4.E12.m1.3.3.1.1.1.1.3.cmml" xref="S4.E12.m1.3.3.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E12.m1.3c">\small\alpha,B=\mathrm{argmin}\|W-\alpha B\|^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">Furthermore, inspired by the observation that many learned weights are close to zero, there have been attempts to ternarize network by constraining the weights/activations with ternary values, e.g., +1, 0 and -1, thereby explicitly permitting the quantized values to be zero&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>, <a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>.
Ternarization also drastically reduces the inference latency by eliminating the costly matrix multiplications as binarization does.
Later, Ternary-Binary Network (TBN)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib244" title="" class="ltx_ref">244</a>]</cite> shows that
combining binary network weights and ternary activations can achieve an optimal tradeoff between the accuracy and
computational efficiency.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">Since the naive binarization and ternarization methods generally result in severe accuracy degradation, especially for complex tasks such as ImageNet classification,
a number of solutions have been proposed to reduce the accuracy degradation in extreme quantization.
The work of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite> broadly categorizes these solutions into three branches.
Here, we briefly discuss each branch, and we refer the interested readers to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite> for more details.</p>
</div>
<section id="S4.SS5.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Quantization Error Minimization</h5>

<div id="S4.SS5.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS5.SSS0.Px1.p1.1" class="ltx_p">The first branch of solutions aims to minimize the quantization error, i.e., the gap between the real values and the quantized values&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>, <a href="#bib.bib158" title="" class="ltx_ref">158</a>, <a href="#bib.bib164" title="" class="ltx_ref">164</a>, <a href="#bib.bib178" title="" class="ltx_ref">178</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib218" title="" class="ltx_ref">218</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib248" title="" class="ltx_ref">248</a>]</cite>.
Instead of using a single binary matrix to represent real-value weights/activations,
HORQ&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite> and ABC-Net&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite> use a linear combination of multiple binary matrices, i.e.,
<math id="S4.SS5.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="W\approx\alpha_{1}B_{1}+\cdots+\alpha_{M}B_{M}" display="inline"><semantics id="S4.SS5.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS5.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.2.cmml">W</mi><mo id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.1.cmml">≈</mo><mrow id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mrow id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.cmml"><msub id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.cmml"><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.2.cmml">α</mi><mn id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.1" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.1.cmml">​</mo><msub id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.cmml"><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.2.cmml">B</mi><mn id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.3.cmml">1</mn></msub></mrow><mo id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">+</mo><mi mathvariant="normal" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">⋯</mi><mo id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1a" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">+</mo><mrow id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.cmml"><msub id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.cmml"><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.2.cmml">α</mi><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.3.cmml">M</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.1" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.1.cmml">​</mo><msub id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.cmml"><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.2.cmml">B</mi><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.3.cmml">M</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1"><approx id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.1"></approx><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.2">𝑊</ci><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3"><plus id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1"></plus><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2"><times id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.1"></times><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2">subscript</csymbol><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.2">𝛼</ci><cn type="integer" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.3">1</cn></apply><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3">subscript</csymbol><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.2">𝐵</ci><cn type="integer" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.3">1</cn></apply></apply><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.3">⋯</ci><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4"><times id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.1"></times><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2"><csymbol cd="ambiguous" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2">subscript</csymbol><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.2">𝛼</ci><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.3">𝑀</ci></apply><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3"><csymbol cd="ambiguous" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3">subscript</csymbol><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.2">𝐵</ci><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.3">𝑀</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p1.1.m1.1c">W\approx\alpha_{1}B_{1}+\cdots+\alpha_{M}B_{M}</annotation></semantics></math>, to reduce the quantization error.
Inspired by the fact that binarizing the activations
reduces their representational capability for the succeeding convolution block,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> show that binarization of wider networks (i.e., networks with larger number of filters) can achieve a good trade-off between the accuracy and the model size.</p>
</div>
</section>
<section id="S4.SS5.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Improved Loss function</h5>

<div id="S4.SS5.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS5.SSS0.Px2.p1.1" class="ltx_p">Another branch of works focuses on the choice of loss function&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib98" title="" class="ltx_ref">98</a>, <a href="#bib.bib284" title="" class="ltx_ref">284</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib251" title="" class="ltx_ref">251</a>]</cite>.
Important works here are loss-aware binarization and ternarization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> that directly minimize the loss with respect to the binarized/ternatized weights.
This is different from other approaches that only approximate the weights and do not consider the final loss.
Knowledge distillation from full-precision teacher models has also been shown as a promising method to recover the accuracy degradation after binarization/ternarization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib195" title="" class="ltx_ref">195</a>, <a href="#bib.bib260" title="" class="ltx_ref">260</a>, <a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS5.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Improved Training Method</h5>

<div id="S4.SS5.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS5.SSS0.Px3.p1.1" class="ltx_p">Another interesting branch of work aims for better training methods for binary/ternary models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib164" title="" class="ltx_ref">164</a>, <a href="#bib.bib285" title="" class="ltx_ref">285</a>, <a href="#bib.bib288" title="" class="ltx_ref">288</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
A number of efforts point out the limitation of STE in backpropagating gradients through the sign function:
STE only propagate the gradients for the weights and/or activations that are in the range of [-1, 1].
To address this, BNN+&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> introduces a continuous approximation for the derivative of the sign function,
while <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib261" title="" class="ltx_ref">261</a>, <a href="#bib.bib198" title="" class="ltx_ref">198</a>, <a href="#bib.bib272" title="" class="ltx_ref">272</a>]</cite> replace the sign function with smooth, differentiable functions that gradually sharpens and approaches the sign function.
Bi-Real Net&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite> introduces identity shortcuts connecting activations to activations in consecutive blocks, through which 32-bit activations can be propagated.
While most research focuses on reducing the inference time latency, DoReFa-Net&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib285" title="" class="ltx_ref">285</a>]</cite> quantizes the gradients in addition to the weights and activations, in order to accelerate the training as well.</p>
</div>
<div id="S4.SS5.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS5.SSS0.Px3.p2.1" class="ltx_p">Extreme quantization has been successful in drastically reducing the inference/training latency as well as the
model size for many CNN models on computer vision tasks.
Recently, there have been attempts to extend this idea to Natural Language Processing (NLP) tasks&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib278" title="" class="ltx_ref">278</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib121" title="" class="ltx_ref">121</a>, <a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>.
Considering the prohibitive model size and inference latency of state-of-the-art NLP models
(e.g., BERT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, RoBERTa&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>, and the GPT family&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib200" title="" class="ltx_ref">200</a>, <a href="#bib.bib201" title="" class="ltx_ref">201</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>)
that are pre-trained on a large amount of unlabeled data,
extreme quantization is emerging as a powerful tool for bringing NLP inference tasks to the edge.</p>
</div>
<div id="S4.SS5.SSS0.Px3.p3" class="ltx_para">
<p id="S4.SS5.SSS0.Px3.p3.1" class="ltx_p"><span id="S4.SS5.SSS0.Px3.p3.1.1" class="ltx_text ltx_font_bold">Summary (Extreme Quantization).</span>
Extreme low-bit precision quantization is a very promising line of research.
However, existing methods often incur high accuracy degradation as compared
to baseline, unless very extensive tuning and hyperparameter search is performed.
But this accuracy degradation may be acceptable for less critical applications.</p>
</div>
</section>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.5.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.6.2" class="ltx_text ltx_font_italic">Vector Quantization</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">As discussed in&nbsp;Section&nbsp;<a href="#S2" title="II General History of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, quantization has not been invented in machine learning, but has
been widely studied in the past century in information theory,
and particularly in digital signal processing field as a compression tool.
However, the main difference between quantization methods for machine learning is that fundamentally
we are not interested to compress the signal with minimum change/error as compared to the original
signal. Instead, the goal is to find a reduced-precision representation that results
in as small loss as possible. As such, it is completely acceptable if the quantized weights/activations
are far away from the non-quantized ones.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.8" class="ltx_p">Having said that, there are a lot of interesting ideas
in the classical quantization methods in DSP that have been applied to NN quantization, and in particular
vector quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
In particular, the work of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib256" title="" class="ltx_ref">256</a>, <a href="#bib.bib189" title="" class="ltx_ref">189</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>, <a href="#bib.bib180" title="" class="ltx_ref">180</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> clusters the weights into different groups and use the centroid of each group as quantized values during inference. As shown in&nbsp;Eq.&nbsp;<a href="#S4.E13" title="In IV-F Vector Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, <math id="S4.SS6.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS6.p2.1.m1.1a"><mi id="S4.SS6.p2.1.m1.1.1" xref="S4.SS6.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.1.m1.1b"><ci id="S4.SS6.p2.1.m1.1.1.cmml" xref="S4.SS6.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.1.m1.1c">i</annotation></semantics></math> is the index of weights in a tensor, <math id="S4.SS6.p2.2.m2.3" class="ltx_Math" alttext="c_{1},...,c_{k}" display="inline"><semantics id="S4.SS6.p2.2.m2.3a"><mrow id="S4.SS6.p2.2.m2.3.3.2" xref="S4.SS6.p2.2.m2.3.3.3.cmml"><msub id="S4.SS6.p2.2.m2.2.2.1.1" xref="S4.SS6.p2.2.m2.2.2.1.1.cmml"><mi id="S4.SS6.p2.2.m2.2.2.1.1.2" xref="S4.SS6.p2.2.m2.2.2.1.1.2.cmml">c</mi><mn id="S4.SS6.p2.2.m2.2.2.1.1.3" xref="S4.SS6.p2.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S4.SS6.p2.2.m2.3.3.2.3" xref="S4.SS6.p2.2.m2.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS6.p2.2.m2.1.1" xref="S4.SS6.p2.2.m2.1.1.cmml">…</mi><mo id="S4.SS6.p2.2.m2.3.3.2.4" xref="S4.SS6.p2.2.m2.3.3.3.cmml">,</mo><msub id="S4.SS6.p2.2.m2.3.3.2.2" xref="S4.SS6.p2.2.m2.3.3.2.2.cmml"><mi id="S4.SS6.p2.2.m2.3.3.2.2.2" xref="S4.SS6.p2.2.m2.3.3.2.2.2.cmml">c</mi><mi id="S4.SS6.p2.2.m2.3.3.2.2.3" xref="S4.SS6.p2.2.m2.3.3.2.2.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.2.m2.3b"><list id="S4.SS6.p2.2.m2.3.3.3.cmml" xref="S4.SS6.p2.2.m2.3.3.2"><apply id="S4.SS6.p2.2.m2.2.2.1.1.cmml" xref="S4.SS6.p2.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS6.p2.2.m2.2.2.1.1.1.cmml" xref="S4.SS6.p2.2.m2.2.2.1.1">subscript</csymbol><ci id="S4.SS6.p2.2.m2.2.2.1.1.2.cmml" xref="S4.SS6.p2.2.m2.2.2.1.1.2">𝑐</ci><cn type="integer" id="S4.SS6.p2.2.m2.2.2.1.1.3.cmml" xref="S4.SS6.p2.2.m2.2.2.1.1.3">1</cn></apply><ci id="S4.SS6.p2.2.m2.1.1.cmml" xref="S4.SS6.p2.2.m2.1.1">…</ci><apply id="S4.SS6.p2.2.m2.3.3.2.2.cmml" xref="S4.SS6.p2.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS6.p2.2.m2.3.3.2.2.1.cmml" xref="S4.SS6.p2.2.m2.3.3.2.2">subscript</csymbol><ci id="S4.SS6.p2.2.m2.3.3.2.2.2.cmml" xref="S4.SS6.p2.2.m2.3.3.2.2.2">𝑐</ci><ci id="S4.SS6.p2.2.m2.3.3.2.2.3.cmml" xref="S4.SS6.p2.2.m2.3.3.2.2.3">𝑘</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.2.m2.3c">c_{1},...,c_{k}</annotation></semantics></math> are the <math id="S4.SS6.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS6.p2.3.m3.1a"><mi id="S4.SS6.p2.3.m3.1.1" xref="S4.SS6.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.3.m3.1b"><ci id="S4.SS6.p2.3.m3.1.1.cmml" xref="S4.SS6.p2.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.3.m3.1c">k</annotation></semantics></math> centroids found by the clustering, and <math id="S4.SS6.p2.4.m4.1" class="ltx_Math" alttext="c_{j}" display="inline"><semantics id="S4.SS6.p2.4.m4.1a"><msub id="S4.SS6.p2.4.m4.1.1" xref="S4.SS6.p2.4.m4.1.1.cmml"><mi id="S4.SS6.p2.4.m4.1.1.2" xref="S4.SS6.p2.4.m4.1.1.2.cmml">c</mi><mi id="S4.SS6.p2.4.m4.1.1.3" xref="S4.SS6.p2.4.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.4.m4.1b"><apply id="S4.SS6.p2.4.m4.1.1.cmml" xref="S4.SS6.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS6.p2.4.m4.1.1.1.cmml" xref="S4.SS6.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS6.p2.4.m4.1.1.2.cmml" xref="S4.SS6.p2.4.m4.1.1.2">𝑐</ci><ci id="S4.SS6.p2.4.m4.1.1.3.cmml" xref="S4.SS6.p2.4.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.4.m4.1c">c_{j}</annotation></semantics></math> is the corresponding centroid to <math id="S4.SS6.p2.5.m5.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S4.SS6.p2.5.m5.1a"><msub id="S4.SS6.p2.5.m5.1.1" xref="S4.SS6.p2.5.m5.1.1.cmml"><mi id="S4.SS6.p2.5.m5.1.1.2" xref="S4.SS6.p2.5.m5.1.1.2.cmml">w</mi><mi id="S4.SS6.p2.5.m5.1.1.3" xref="S4.SS6.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.5.m5.1b"><apply id="S4.SS6.p2.5.m5.1.1.cmml" xref="S4.SS6.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS6.p2.5.m5.1.1.1.cmml" xref="S4.SS6.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS6.p2.5.m5.1.1.2.cmml" xref="S4.SS6.p2.5.m5.1.1.2">𝑤</ci><ci id="S4.SS6.p2.5.m5.1.1.3.cmml" xref="S4.SS6.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.5.m5.1c">w_{i}</annotation></semantics></math>. After clustering, weight <math id="S4.SS6.p2.6.m6.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S4.SS6.p2.6.m6.1a"><msub id="S4.SS6.p2.6.m6.1.1" xref="S4.SS6.p2.6.m6.1.1.cmml"><mi id="S4.SS6.p2.6.m6.1.1.2" xref="S4.SS6.p2.6.m6.1.1.2.cmml">w</mi><mi id="S4.SS6.p2.6.m6.1.1.3" xref="S4.SS6.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.6.m6.1b"><apply id="S4.SS6.p2.6.m6.1.1.cmml" xref="S4.SS6.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS6.p2.6.m6.1.1.1.cmml" xref="S4.SS6.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS6.p2.6.m6.1.1.2.cmml" xref="S4.SS6.p2.6.m6.1.1.2">𝑤</ci><ci id="S4.SS6.p2.6.m6.1.1.3.cmml" xref="S4.SS6.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.6.m6.1c">w_{i}</annotation></semantics></math> will have a cluster index <math id="S4.SS6.p2.7.m7.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.SS6.p2.7.m7.1a"><mi id="S4.SS6.p2.7.m7.1.1" xref="S4.SS6.p2.7.m7.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.7.m7.1b"><ci id="S4.SS6.p2.7.m7.1.1.cmml" xref="S4.SS6.p2.7.m7.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.7.m7.1c">j</annotation></semantics></math> related to <math id="S4.SS6.p2.8.m8.1" class="ltx_Math" alttext="c_{j}" display="inline"><semantics id="S4.SS6.p2.8.m8.1a"><msub id="S4.SS6.p2.8.m8.1.1" xref="S4.SS6.p2.8.m8.1.1.cmml"><mi id="S4.SS6.p2.8.m8.1.1.2" xref="S4.SS6.p2.8.m8.1.1.2.cmml">c</mi><mi id="S4.SS6.p2.8.m8.1.1.3" xref="S4.SS6.p2.8.m8.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.8.m8.1b"><apply id="S4.SS6.p2.8.m8.1.1.cmml" xref="S4.SS6.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS6.p2.8.m8.1.1.1.cmml" xref="S4.SS6.p2.8.m8.1.1">subscript</csymbol><ci id="S4.SS6.p2.8.m8.1.1.2.cmml" xref="S4.SS6.p2.8.m8.1.1.2">𝑐</ci><ci id="S4.SS6.p2.8.m8.1.1.3.cmml" xref="S4.SS6.p2.8.m8.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.8.m8.1c">c_{j}</annotation></semantics></math> in the codebook (look-up table).</p>
<table id="S4.E13" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E13.m1.4" class="ltx_Math" alttext="\small\min_{c_{1},...,c_{k}}\sum_{i}\|w_{i}-c_{j}\|^{2}" display="block"><semantics id="S4.E13.m1.4a"><mrow id="S4.E13.m1.4.4" xref="S4.E13.m1.4.4.cmml"><munder id="S4.E13.m1.4.4.3" xref="S4.E13.m1.4.4.3.cmml"><mi mathsize="90%" id="S4.E13.m1.4.4.3.2" xref="S4.E13.m1.4.4.3.2.cmml">min</mi><mrow id="S4.E13.m1.3.3.3.3" xref="S4.E13.m1.3.3.3.4.cmml"><msub id="S4.E13.m1.2.2.2.2.1" xref="S4.E13.m1.2.2.2.2.1.cmml"><mi mathsize="90%" id="S4.E13.m1.2.2.2.2.1.2" xref="S4.E13.m1.2.2.2.2.1.2.cmml">c</mi><mn mathsize="90%" id="S4.E13.m1.2.2.2.2.1.3" xref="S4.E13.m1.2.2.2.2.1.3.cmml">1</mn></msub><mo mathsize="90%" id="S4.E13.m1.3.3.3.3.3" xref="S4.E13.m1.3.3.3.4.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="S4.E13.m1.1.1.1.1" xref="S4.E13.m1.1.1.1.1.cmml">…</mi><mo mathsize="90%" id="S4.E13.m1.3.3.3.3.4" xref="S4.E13.m1.3.3.3.4.cmml">,</mo><msub id="S4.E13.m1.3.3.3.3.2" xref="S4.E13.m1.3.3.3.3.2.cmml"><mi mathsize="90%" id="S4.E13.m1.3.3.3.3.2.2" xref="S4.E13.m1.3.3.3.3.2.2.cmml">c</mi><mi mathsize="90%" id="S4.E13.m1.3.3.3.3.2.3" xref="S4.E13.m1.3.3.3.3.2.3.cmml">k</mi></msub></mrow></munder><mo lspace="0em" rspace="0em" id="S4.E13.m1.4.4.2" xref="S4.E13.m1.4.4.2.cmml">​</mo><mrow id="S4.E13.m1.4.4.1" xref="S4.E13.m1.4.4.1.cmml"><munder id="S4.E13.m1.4.4.1.2" xref="S4.E13.m1.4.4.1.2.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" rspace="0em" stretchy="true" id="S4.E13.m1.4.4.1.2.2" xref="S4.E13.m1.4.4.1.2.2.cmml">∑</mo><mi mathsize="90%" id="S4.E13.m1.4.4.1.2.3" xref="S4.E13.m1.4.4.1.2.3.cmml">i</mi></munder><msup id="S4.E13.m1.4.4.1.1" xref="S4.E13.m1.4.4.1.1.cmml"><mrow id="S4.E13.m1.4.4.1.1.1.1" xref="S4.E13.m1.4.4.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E13.m1.4.4.1.1.1.1.2" xref="S4.E13.m1.4.4.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E13.m1.4.4.1.1.1.1.1" xref="S4.E13.m1.4.4.1.1.1.1.1.cmml"><msub id="S4.E13.m1.4.4.1.1.1.1.1.2" xref="S4.E13.m1.4.4.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S4.E13.m1.4.4.1.1.1.1.1.2.2" xref="S4.E13.m1.4.4.1.1.1.1.1.2.2.cmml">w</mi><mi mathsize="90%" id="S4.E13.m1.4.4.1.1.1.1.1.2.3" xref="S4.E13.m1.4.4.1.1.1.1.1.2.3.cmml">i</mi></msub><mo mathsize="90%" id="S4.E13.m1.4.4.1.1.1.1.1.1" xref="S4.E13.m1.4.4.1.1.1.1.1.1.cmml">−</mo><msub id="S4.E13.m1.4.4.1.1.1.1.1.3" xref="S4.E13.m1.4.4.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S4.E13.m1.4.4.1.1.1.1.1.3.2" xref="S4.E13.m1.4.4.1.1.1.1.1.3.2.cmml">c</mi><mi mathsize="90%" id="S4.E13.m1.4.4.1.1.1.1.1.3.3" xref="S4.E13.m1.4.4.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo maxsize="90%" minsize="90%" id="S4.E13.m1.4.4.1.1.1.1.3" xref="S4.E13.m1.4.4.1.1.1.2.1.cmml">‖</mo></mrow><mn mathsize="90%" id="S4.E13.m1.4.4.1.1.3" xref="S4.E13.m1.4.4.1.1.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E13.m1.4b"><apply id="S4.E13.m1.4.4.cmml" xref="S4.E13.m1.4.4"><times id="S4.E13.m1.4.4.2.cmml" xref="S4.E13.m1.4.4.2"></times><apply id="S4.E13.m1.4.4.3.cmml" xref="S4.E13.m1.4.4.3"><csymbol cd="ambiguous" id="S4.E13.m1.4.4.3.1.cmml" xref="S4.E13.m1.4.4.3">subscript</csymbol><min id="S4.E13.m1.4.4.3.2.cmml" xref="S4.E13.m1.4.4.3.2"></min><list id="S4.E13.m1.3.3.3.4.cmml" xref="S4.E13.m1.3.3.3.3"><apply id="S4.E13.m1.2.2.2.2.1.cmml" xref="S4.E13.m1.2.2.2.2.1"><csymbol cd="ambiguous" id="S4.E13.m1.2.2.2.2.1.1.cmml" xref="S4.E13.m1.2.2.2.2.1">subscript</csymbol><ci id="S4.E13.m1.2.2.2.2.1.2.cmml" xref="S4.E13.m1.2.2.2.2.1.2">𝑐</ci><cn type="integer" id="S4.E13.m1.2.2.2.2.1.3.cmml" xref="S4.E13.m1.2.2.2.2.1.3">1</cn></apply><ci id="S4.E13.m1.1.1.1.1.cmml" xref="S4.E13.m1.1.1.1.1">…</ci><apply id="S4.E13.m1.3.3.3.3.2.cmml" xref="S4.E13.m1.3.3.3.3.2"><csymbol cd="ambiguous" id="S4.E13.m1.3.3.3.3.2.1.cmml" xref="S4.E13.m1.3.3.3.3.2">subscript</csymbol><ci id="S4.E13.m1.3.3.3.3.2.2.cmml" xref="S4.E13.m1.3.3.3.3.2.2">𝑐</ci><ci id="S4.E13.m1.3.3.3.3.2.3.cmml" xref="S4.E13.m1.3.3.3.3.2.3">𝑘</ci></apply></list></apply><apply id="S4.E13.m1.4.4.1.cmml" xref="S4.E13.m1.4.4.1"><apply id="S4.E13.m1.4.4.1.2.cmml" xref="S4.E13.m1.4.4.1.2"><csymbol cd="ambiguous" id="S4.E13.m1.4.4.1.2.1.cmml" xref="S4.E13.m1.4.4.1.2">subscript</csymbol><sum id="S4.E13.m1.4.4.1.2.2.cmml" xref="S4.E13.m1.4.4.1.2.2"></sum><ci id="S4.E13.m1.4.4.1.2.3.cmml" xref="S4.E13.m1.4.4.1.2.3">𝑖</ci></apply><apply id="S4.E13.m1.4.4.1.1.cmml" xref="S4.E13.m1.4.4.1.1"><csymbol cd="ambiguous" id="S4.E13.m1.4.4.1.1.2.cmml" xref="S4.E13.m1.4.4.1.1">superscript</csymbol><apply id="S4.E13.m1.4.4.1.1.1.2.cmml" xref="S4.E13.m1.4.4.1.1.1.1"><csymbol cd="latexml" id="S4.E13.m1.4.4.1.1.1.2.1.cmml" xref="S4.E13.m1.4.4.1.1.1.1.2">norm</csymbol><apply id="S4.E13.m1.4.4.1.1.1.1.1.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1"><minus id="S4.E13.m1.4.4.1.1.1.1.1.1.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.1"></minus><apply id="S4.E13.m1.4.4.1.1.1.1.1.2.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E13.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E13.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.2.2">𝑤</ci><ci id="S4.E13.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.E13.m1.4.4.1.1.1.1.1.3.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E13.m1.4.4.1.1.1.1.1.3.1.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E13.m1.4.4.1.1.1.1.1.3.2.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.3.2">𝑐</ci><ci id="S4.E13.m1.4.4.1.1.1.1.1.3.3.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><cn type="integer" id="S4.E13.m1.4.4.1.1.3.cmml" xref="S4.E13.m1.4.4.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E13.m1.4c">\small\min_{c_{1},...,c_{k}}\sum_{i}\|w_{i}-c_{j}\|^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p id="S4.SS6.p2.9" class="ltx_p">It has been found that using a k-means clustering is sufficient to reduce the model size up to <math id="S4.SS6.p2.9.m1.1" class="ltx_math_unparsed" alttext="8\times" display="inline"><semantics id="S4.SS6.p2.9.m1.1a"><mrow id="S4.SS6.p2.9.m1.1b"><mn id="S4.SS6.p2.9.m1.1.1">8</mn><mo lspace="0.222em" id="S4.SS6.p2.9.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS6.p2.9.m1.1c">8\times</annotation></semantics></math> without significant accuracy degradation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>.
In addition to that, jointly applying k-means based vector quantization with pruning and Huffman coding can further reduce the model size&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p id="S4.SS6.p3.1" class="ltx_p">Product quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib256" title="" class="ltx_ref">256</a>, <a href="#bib.bib227" title="" class="ltx_ref">227</a>]</cite> is an extension of vector quantization, where the weight matrix is divided into submatrices and vector quantization is applied to each submatrix.
Besides basic product quantization method, more fine-grained usage of clustering can further improve the accuracy. For example, in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> the residuals after k-means product quantization are further recursively quantized.
And in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>, the authors apply more clusters for more important quantization ranges to better preserve the information.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Quantization and Hardware Processors</span>
</h2>

<figure id="S5.F9" class="ltx_figure"><img src="/html/2103.13630/assets/x11.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>
Throughput comparison of different commercial edge processors for NN inference at the edge.
</figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have said that quantization not only reduces the model size, but it also enables faster speed and requires less power, in particular for hardware that has low-precision logic.
As such, quantization has been particularly crucial for edge deployment in IoT and mobile applications.
Edge devices often have tight resource constraints including compute, memory, and importantly power budget.
These are often too costly to meet for many deep NN models.
In addition, many edge processors do not have any support floating point operations, especially in micro-controllers.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Here, we briefly discuss different hardware
platforms in the context of quantization.
ARM Cortex-M is a group of 32-bit RISC ARM processor cores that are designed for low-cost and power-efficient embedded devices.
For instance, the STM32 family are the microcontrollers based on the ARM Cortex-M cores that are also used for NN inference at the edge.
Because some of the ARM Cortex-M cores do not include dedicated floating-point units,
the models should first be quantized before deployment.
CMSIS-NN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite> is a library from ARM that helps quantizing and deploying NN models onto the ARM Cortex-M cores.
Specifically, the library leverages fixed-point quantization&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>, <a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib267" title="" class="ltx_ref">267</a>]</cite> with power-of-two scaling factors so that quantization and dequantization processes can be carried out efficiently with bit shifting operations.
GAP-8&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, a RISC-V SoC (System on Chip) for edge inference with a dedicated CNN accelerator, is another example of an edge processor that only supports integer arithmetic.
While programmable general-purpose processors are widely adopted due to their flexibility,
Google Edge TPU, a purpose-built ASIC chip, is another emerging solution for running inference at the edge.
Unlike Cloud TPUs that run in Google data centers with a large amount of computing resources,
the Edge TPU is designed for small and low-power devices, and thereby it only supports 8-bit arithmetic.
NN models must be quantized using either quantization-aware training or post-training quantization of&nbsp;TensorFlow.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Figure&nbsp;<a href="#S5.F9" title="Figure 9 ‣ V Quantization and Hardware Processors ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> plots the throughput of different commercial edge processors that are widely used for NN inference at the edge.
In the past few years, there has been a significant improvement in the computing power of the edge processors,
and this allows deployment and inference of costly NN models that were previously available only on servers.
Quantization, combined with efficient low-precision logic and dedicated deep learning accelerators, has been one important driving force for the evolution of such edge processors.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">While quantization is an indispensable technique for a lot of edge processors,
it can also bring a remarkable improvement for non-edge processors, e.g., to meet
Service Level Agreement (SLA) requirements such as 99th percentile latency.
A good example is provided by the recent NVIDIA Turing GPUs, and in particular T4 GPUs, which include the Turing Tensor Cores.
Tensor Cores are specialized execution units designed for efficient low-precision matrix multiplications.</p>
</div>
</section>
<section id="S6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Future Directions for Research in Quantization</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Here, we briefly discuss several high level challenges and opportunities for future research in quantization.
This is broken down into quantization software, hardware and NN architecture co-design, coupled
compression methods, and quantized training.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Quantization Software:</span>
With current methods, it is straightforward to quantize and deploy different NN models to INT8, without losing accuracy. There are several software packages
that can be used to deploy INT8 quantized models (e.g., Nvidia’s TensorRT, TVM, etc.), each with
good documentation. Furthermore, the implementations are also quite optimal and one can easily
observe speed up with quantization.
However, the software for lower bit-precision quantization is not widely available, and sometimes
it is non-existent. For instance, Nvidia’s TensorRT does not currently support sub-INT8 quantization.
Moreover, support for INT4 quantization was only recently added to TVM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib267" title="" class="ltx_ref">267</a>]</cite>.
Recent work has shown that low precision and mixed-precision quantization with
INT4/INT8 works in practice&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib267" title="" class="ltx_ref">267</a>, <a href="#bib.bib246" title="" class="ltx_ref">246</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib286" title="" class="ltx_ref">286</a>, <a href="#bib.bib246" title="" class="ltx_ref">246</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib239" title="" class="ltx_ref">239</a>, <a href="#bib.bib263" title="" class="ltx_ref">263</a>, <a href="#bib.bib199" title="" class="ltx_ref">199</a>, <a href="#bib.bib249" title="" class="ltx_ref">249</a>, <a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib187" title="" class="ltx_ref">187</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite>. Thus, developing efficient software APIs for lower precision quantization
will have an important impact.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Hardware and NN Architecture Co-Design:</span>
As discussed above, an important difference between classical work in low-precision
quantization and the recent work in machine learning is the fact that NN parameters
may have very different quantized values but may still generalize similarly well.
For example, with quantization-aware training, we might converge to a different solution, far away from the original solution with single precision parameters, but
still get good accuracy. One can take advantage of this degree of freedom and also
adapt the NN architecture as it is being quantized.
For instance, the recent work of&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> shows that changing
the width of the NN architecture could reduce/remove generalization gap after quantization.
One line of future work is to adapt jointly other architecture parameters, such
as depth or individual kernels, as the model is being quantized.
Another line of future work is to extend this co-design to hardware architecture.
This may be particularly useful for FPGA deployment, as one can explore many different possible
hardware configurations (such as different micro-architectures of multiply-accumulate elements), and then couple this with the NN architecture and quantization co-design.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p"><span id="S6.p4.1.1" class="ltx_text ltx_font_bold">Coupled Compression Methods:</span>
As discussed above, quantization is only one of the methods for efficient deployment of NNs.
Other methods include efficient NN architecture design, co-design of hardware and NN architecture, pruning, and knowledge distillation.
Quantization can be coupled with these other approaches.
However, there is currently very little work exploring what are the optimal combinations of these
methods. For instance, pruning and quantization can be applied together to a model to reduce
its overhead&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib152" title="" class="ltx_ref">152</a>]</cite>, and it is important to understand the best combination of structured/unstructured pruning and quantization. Similarly, another future direction
is to study the coupling between these methods and other approaches described above.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p"><span id="S6.p5.1.1" class="ltx_text ltx_font_bold">Quantized Training:</span>
Perhaps the most important use of quantization has been to accelerate NN training with half-precision&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>. This has enabled the use of much
faster and more power-efficient reduced-precision logic for training. However, it has been very difficult to push this further down to INT8
precision training.
While several interesting works exist in this area&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib173" title="" class="ltx_ref">173</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, the proposed methods often
require a lot of hyperparameter tuning, or they only work for a few NN models on relatively easy
learning tasks.
The basic problem is that, with INT8 precision, the training can
become unstable and diverge.
Addressing this challenge can
have a high impact on several applications, especially for training at the edge.</p>
</div>
</section>
<section id="S7" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Summary and Conclusions</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of
efficient representation, manipulation, and communication of the numerical values in those computations arose.
Strongly related to the problem of numerical representation is the problem of quantization: in what manner should
a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number
of bits required and also to maximize the accuracy of the attendant computations? While these problems are as old
as computer science, these problems are especially relevant to the design of efficient NN models.
There are several reasons for this.
First, NNs are computationally intensive. So, the efficient representation of numerical values is
particularly important. Second, most current NN models are heavily over-parameterized. So, there is ample
opportunity for reducing the bit precision without impacting accuracy. Third, the layered structure of NN models
offers an additional dimension to explore. Thus, different layers in the NN have different impact on the loss function,
and this motivates interesting approaches such mixed-precision quantization.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.6" class="ltx_p">Moving from floating-point representations to low-precision
fixed integer values represented in eight/four bits or less holds the
potential to reduce the memory footprint and latency.
&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite> shows that INT8 inference of popular computer vision models, including ResNet50&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, VGG-19&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib224" title="" class="ltx_ref">224</a>]</cite>, and inceptionV3&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>]</cite> using TVM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> quantization library, can achieve 3.89<math id="S7.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S7.p2.1.m1.1a"><mo id="S7.p2.1.m1.1.1" xref="S7.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.1.m1.1b"><times id="S7.p2.1.m1.1.1.cmml" xref="S7.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.m1.1c">\times</annotation></semantics></math>, 3.32<math id="S7.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S7.p2.2.m2.1a"><mo id="S7.p2.2.m2.1.1" xref="S7.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.2.m2.1b"><times id="S7.p2.2.m2.1.1.cmml" xref="S7.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.2.m2.1c">\times</annotation></semantics></math>, and 5.02<math id="S7.p2.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S7.p2.3.m3.1a"><mo id="S7.p2.3.m3.1.1" xref="S7.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.3.m3.1b"><times id="S7.p2.3.m3.1.1.cmml" xref="S7.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.3.m3.1c">\times</annotation></semantics></math> speedup on NVIDIA GTX 1080, respectively.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib213" title="" class="ltx_ref">213</a>]</cite> further shows that
INT4 inference of ResNet50 could bring an additional 50-60% speedup on NVIDIA T4 and RTX, compared to its INT8 counterpart, emphasizing the importance of using lower-bit precision to maximize efficiency.
Recently,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib267" title="" class="ltx_ref">267</a>]</cite> leverages mix-precision quantization to achieve 23% speedup for ResNet50, as compared to INT8 inference without accuracy degradation,
and&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> extends INT8-only inference to BERT model to enable up to 4.0<math id="S7.p2.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S7.p2.4.m4.1a"><mo id="S7.p2.4.m4.1.1" xref="S7.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.4.m4.1b"><times id="S7.p2.4.m4.1.1.cmml" xref="S7.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.4.m4.1c">\times</annotation></semantics></math> faster inference than FP32.
While the aforementioned works focus on acceleration on GPUs,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> also obtained 2.35<math id="S7.p2.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S7.p2.5.m5.1a"><mo id="S7.p2.5.m5.1.1" xref="S7.p2.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.5.m5.1b"><times id="S7.p2.5.m5.1.1.cmml" xref="S7.p2.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.5.m5.1c">\times</annotation></semantics></math> and 1.40<math id="S7.p2.6.m6.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S7.p2.6.m6.1a"><mo id="S7.p2.6.m6.1.1" xref="S7.p2.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.6.m6.1b"><times id="S7.p2.6.m6.1.1.cmml" xref="S7.p2.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.6.m6.1c">\times</annotation></semantics></math> latency speedup on Intel Cascade Lake CPU and Raspberry Pi4 (which are both non-GPU architectures), respectively, through INT8 quantization of various computer vision models.
As a result, as our bibliography attests, the problem of quantization in NN models has been a highly active research area.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">In
this work, we have tried to bring some conceptual structure to these very diverse efforts. We began with a
discussion of topics common to many applications of quantization, such as uniform, non-uniform, symmetric,
asymmetric, static, and dynamic quantization. We then considered quantization issues that are more unique to the
quantization of NNs. These include layerwise, groupwise, channelwise, and sub-channelwise quantization.
We further considered the inter-relationship between training and quantization, and we discussed the advantages
and disadvantages of quantization-aware training as compared to post-training quantization. Further nuancing the
discussion of the relationship between quantization and training is the issue of the availability of data. The
extreme case of this is one in which the data used in training are, due to a variety of sensible reasons such as
privacy, no longer available. This motivates the problem of zero-shot quantization.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">As we are particularly concerned about efficient NNs targeted for edge-deployment, we considered problems that are
unique to this environment. These include quantization techniques that result in parameters represented by fewer
than 8 bits, perhaps as low as binary values. We also considered the problem of integer-only quantization, which
enables the deployment of NNs on low-end microprocessors which often lack floating-point units.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.</p>
</div>
</section>
<section id="Sx1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The UC Berkeley team also acknowledges gracious support from Samsung (in particular Joseph Hassoun), Intel corporation, Intel VLAB team, Google TRC team, and Google Brain (in particular Prof. David Patterson, Dr. Ed Chi, and Jing Li).
Amir Gholami was supported through through funding from Samsung SAIT.
Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu
Timofte, Luca Benini, and Luc Van&nbsp;Gool.

</span>
<span class="ltx_bibblock">Soft-to-hard vector quantization for end-to-end learning compressible
representations.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1704.00648</span>, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Eirikur Agustsson and Lucas Theis.

</span>
<span class="ltx_bibblock">Universally quantized neural compression.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Sungsoo Ahn, Shell&nbsp;Xu Hu, Andreas Damianou, Neil&nbsp;D Lawrence, and Zhenwen Dai.

</span>
<span class="ltx_bibblock">Variational information distillation for knowledge transfer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 9163–9171, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Milad Alizadeh, Arash Behboodi, Mart van Baalen, Christos Louizos, Tijmen
Blankevoort, and Max Welling.

</span>
<span class="ltx_bibblock">Gradient l1 regularization for quantization robustness.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.07520</span>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Milad Alizadeh, Javier Fernández-Marqués, Nicholas&nbsp;D Lane, and Yarin
Gal.

</span>
<span class="ltx_bibblock">An empirical study of binary neural networks’ optimisation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Jimmy&nbsp;Lei Ba, Jamie&nbsp;Ryan Kiros, and Geoffrey&nbsp;E Hinton.

</span>
<span class="ltx_bibblock">Layer normalization.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1607.06450</span>, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Haoli Bai, Wei Zhang, Lu&nbsp;Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu,
Michael Lyu, and Irwin King.

</span>
<span class="ltx_bibblock">Binarybert: Pushing the limit of bert quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.15701</span>, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yu&nbsp;Bai, Yu-Xiang Wang, and Edo Liberty.

</span>
<span class="ltx_bibblock">Proxquant: Quantized neural networks via proximal operators.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.00861</span>, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Dana&nbsp;Harry Ballard.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">An introduction to natural computation</span>.

</span>
<span class="ltx_bibblock">MIT press, 1999.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.

</span>
<span class="ltx_bibblock">Scalable methods for 8-bit training of neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry.

</span>
<span class="ltx_bibblock">Post-training 4-bit quantization of convolution networks for
rapid-deployment.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.05723</span>, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Chaim Baskin, Eli Schwartz, Evgenii Zheltonozhskii, Natan Liss, Raja Giryes,
Alex&nbsp;M Bronstein, and Avi Mendelson.

</span>
<span class="ltx_bibblock">Uniq: Uniform noise injection for non-uniform quantization of neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.10969</span>, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yoshua Bengio, Nicholas Léonard, and Aaron Courville.

</span>
<span class="ltx_bibblock">Estimating or propagating gradients through stochastic neurons for
conditional computation.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1308.3432</span>, 2013.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
William&nbsp;Ralph Bennett.

</span>
<span class="ltx_bibblock">Spectra of quantized signals.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">The Bell System Technical Journal</span>, 27(3):446–472, 1948.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak.

</span>
<span class="ltx_bibblock">Lsq+: Improving low-bit quantization through learnable offsets and
better initialization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span>, pages 696–697, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Davis Blalock, Jose Javier&nbsp;Gonzalez Ortiz, Jonathan Frankle, and John Guttag.

</span>
<span class="ltx_bibblock">What is the state of neural network pruning?

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.03033</span>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Tom&nbsp;B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.14165</span>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos.

</span>
<span class="ltx_bibblock">High-capacity expert binary networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Adrian Bulat and Georgios Tzimiropoulos.

</span>
<span class="ltx_bibblock">Xnor-net++: Improved binary neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.13863</span>, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Adrian Bulat, Georgios Tzimiropoulos, Jean Kossaifi, and Maja Pantic.

</span>
<span class="ltx_bibblock">Improved training of binary networks for human pose estimation and
image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1904.05868</span>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Aydin Buluc and John&nbsp;R Gilbert.

</span>
<span class="ltx_bibblock">Challenges and advances in parallel sparse matrix-matrix
multiplication.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2008 37th International Conference on Parallel Processing</span>,
pages 503–510. IEEE, 2008.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.

</span>
<span class="ltx_bibblock">Once-for-all: Train one network and specialize it for efficient
deployment.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.09791</span>, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Han Cai, Ligeng Zhu, and Song Han.

</span>
<span class="ltx_bibblock">Proxylessnas: Direct neural architecture search on target task and
hardware.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.00332</span>, 2018.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael&nbsp;W Mahoney, and Kurt
Keutzer.

</span>
<span class="ltx_bibblock">Zeroq: A novel zero shot quantization framework.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 13169–13178, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos.

</span>
<span class="ltx_bibblock">Deep learning with low precision by half-wave gaussian quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 5918–5926, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Léopold Cambier, Anahita Bhiwandiwalla, Ting Gong, Mehran Nekuii, Oguz&nbsp;H
Elibol, and Hanlin Tang.

</span>
<span class="ltx_bibblock">Shifted and squeezed 8-bit floating point format for low-precision
training of deep neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.05674</span>, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Rishidev Chaudhuri and Ila Fiete.

</span>
<span class="ltx_bibblock">Computational principles of memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Nature neuroscience</span>, 19(3):394, 2016.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi,
Chunjing Xu, Chao Xu, and Qi&nbsp;Tian.

</span>
<span class="ltx_bibblock">Data-free learning of student networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 3514–3522, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Jianfei Chen, Yu&nbsp;Gai, Zhewei Yao, Michael&nbsp;W Mahoney, and Joseph&nbsp;E Gonzalez.

</span>
<span class="ltx_bibblock">A statistical framework for low-bitwidth training of deep neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.14298</span>, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Kuilin Chen and Chi-Guhn Lee.

</span>
<span class="ltx_bibblock">Incremental few-shot learning via vector quantization in deep
embedded space.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Shangyu Chen, Wenya Wang, and Sinno&nbsp;Jialin Pan.

</span>
<span class="ltx_bibblock">Metaquant: Learning to quantize by learning to penetrate
non-differentiable quantization.

</span>
<span class="ltx_bibblock">In H.&nbsp;Wallach, H.&nbsp;Larochelle, A.&nbsp;Beygelzimer, F.&nbsp;d'Alché-Buc, E.&nbsp;Fox, and R.&nbsp;Garnett, editors, <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Advances in Neural
Information Processing Systems</span>, volume&nbsp;32. Curran Associates, Inc., 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen
Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et&nbsp;al.

</span>
<span class="ltx_bibblock">TVM: An automated end-to-end optimizing compiler for deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.4.4" class="ltx_text ltx_font_italic">13th <math id="bib.bib32.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib32.1.1.m1.1a"><mo stretchy="false" id="bib.bib32.1.1.m1.1.1" xref="bib.bib32.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib32.1.1.m1.1b"><ci id="bib.bib32.1.1.m1.1.1.cmml" xref="bib.bib32.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib32.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib32.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib32.2.2.m2.1a"><mo stretchy="false" id="bib.bib32.2.2.m2.1.1" xref="bib.bib32.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib32.2.2.m2.1b"><ci id="bib.bib32.2.2.m2.1.1.cmml" xref="bib.bib32.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib32.2.2.m2.1c">\}</annotation></semantics></math> Symposium on Operating Systems Design and
Implementation (<math id="bib.bib32.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib32.3.3.m3.1a"><mo stretchy="false" id="bib.bib32.3.3.m3.1.1" xref="bib.bib32.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib32.3.3.m3.1b"><ci id="bib.bib32.3.3.m3.1.1.cmml" xref="bib.bib32.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib32.3.3.m3.1c">\{</annotation></semantics></math>OSDI<math id="bib.bib32.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib32.4.4.m4.1a"><mo stretchy="false" id="bib.bib32.4.4.m4.1.1" xref="bib.bib32.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib32.4.4.m4.1b"><ci id="bib.bib32.4.4.m4.1.1.cmml" xref="bib.bib32.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib32.4.4.m4.1c">\}</annotation></semantics></math> 18)</span>, pages 578–594, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Xiuyi Chen, Guangcan Liu, Jing Shi, Jiaming Xu, and Bo&nbsp;Xu.

</span>
<span class="ltx_bibblock">Distilled binary neural network for monaural speech separation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">2018 International Joint Conference on Neural Networks
(IJCNN)</span>, pages 1–8. IEEE, 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Ting-Wu Chin, Pierce I-Jen Chuang, Vikas Chandra, and Diana Marculescu.

</span>
<span class="ltx_bibblock">One weight bitwidth to rule them all.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Brian Chmiel, Liad Ben-Uri, Moran Shkolnik, Elad Hoffer, Ron Banner, and Daniel
Soudry.

</span>
<span class="ltx_bibblock">Neural gradients are near-lognormal: improved quantized and sparse
training.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang,
Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan.

</span>
<span class="ltx_bibblock">Pact: Parameterized clipping activation for quantized neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1805.06085</span>, 2018.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Yoojin Choi, Jihwan Choi, Mostafa El-Khamy, and Jungwon Lee.

</span>
<span class="ltx_bibblock">Data-free network quantization with adversarial knowledge
distillation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span>, pages 710–711, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee.

</span>
<span class="ltx_bibblock">Towards the limit of network quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1612.01543</span>, 2016.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee.

</span>
<span class="ltx_bibblock">Learning low precision deep neural networks through regularization.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1809.00095</span>, 2, 2018.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev.

</span>
<span class="ltx_bibblock">Low-bit quantization of neural networks for efficient inference.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">ICCV Workshops</span>, pages 3009–3018, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.

</span>
<span class="ltx_bibblock">Training deep neural networks with low precision multiplications.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.7024</span>, 2014.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.

</span>
<span class="ltx_bibblock">BinaryConnect: Training deep neural networks with binary weights
during propagations.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
3123–3131, 2015.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Elliot&nbsp;J Crowley, Gavin Gray, and Amos&nbsp;J Storkey.

</span>
<span class="ltx_bibblock">Moonshine: Distilling with cheap convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, pages 2893–2903, 2018.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Sajad Darabi, Mouloud Belbahri, Matthieu Courbariaux, and Vahid&nbsp;Partovi Nia.

</span>
<span class="ltx_bibblock">Bnn+: Improved binary network training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Lei Deng, Peng Jiao, Jing Pei, Zhenzhi Wu, and Guoqi Li.

</span>
<span class="ltx_bibblock">Gxnor-net: Training deep neural networks with ternary weights and
activations without full-precision memory under a unified discretization
framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Neural Networks</span>, 100:49–58, 2018.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
James Diffenderfer and Bhavya Kailkhura.

</span>
<span class="ltx_bibblock">Multi-prize lottery ticket hypothesis: Finding accurate binary neural
networks by pruning a randomly weighted network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Marculescu.

</span>
<span class="ltx_bibblock">Regularizing activation distribution for training binarized deep
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 11408–11417, 2019.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Xin Dong, Shangyu Chen, and Sinno&nbsp;Jialin Pan.

</span>
<span class="ltx_bibblock">Learning to prune deep neural networks via layer-wise optimal brain
surgeon.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1705.07565</span>, 2017.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael&nbsp;W. Mahoney, and
Kurt Keutzer.

</span>
<span class="ltx_bibblock">HAWQ-V2: Hessian aware trace-weighted quantization of neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Zhen Dong, Zhewei Yao, Amir Gholami, Michael&nbsp;W Mahoney, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">Hawq: Hessian aware quantization of neural networks with
mixed-precision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 293–302, 2019.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Yueqi Duan, Jiwen Lu, Ziwei Wang, Jianjiang Feng, and Jie Zhou.

</span>
<span class="ltx_bibblock">Learning deep binary descriptor with multi-quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 1183–1192, 2017.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
JG&nbsp;Dunn.

</span>
<span class="ltx_bibblock">The performance of a class of n dimensional quantizers for a gaussian
source.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Proc. Columbia Symp. Signal Transmission Processing</span>, pages
76–81, 1965.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Thomas Elsken, Jan&nbsp;Hendrik Metzen, Frank Hutter, et&nbsp;al.

</span>
<span class="ltx_bibblock">Neural architecture search: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">J. Mach. Learn. Res.</span>, 20(55):1–21, 2019.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
William&nbsp;H Equitz.

</span>
<span class="ltx_bibblock">A new vector quantization clustering algorithm.

</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">IEEE transactions on acoustics, speech, and signal processing</span>,
37(10):1568–1575, 1989.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Steven&nbsp;K Esser, Jeffrey&nbsp;L McKinstry, Deepika Bablani, Rathinakumar Appuswamy,
and Dharmendra&nbsp;S Modha.

</span>
<span class="ltx_bibblock">Learned step size quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1902.08153</span>, 2019.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel Roy, and Ali
Ramezani-Kebrya.

</span>
<span class="ltx_bibblock">Adaptive gradient quantization for data-parallel sgd.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
A&nbsp;Aldo Faisal, Luc&nbsp;PJ Selen, and Daniel&nbsp;M Wolpert.

</span>
<span class="ltx_bibblock">Noise in the nervous system.

</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">Nature reviews neuroscience</span>, 9(4):292–303, 2008.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval,
Hervé Jégou, and Armand Joulin.

</span>
<span class="ltx_bibblock">Training with quantization noise for extreme model compression.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">arXiv e-prints</span>, pages arXiv–2004, 2020.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis,
and Joseph Hassoun.

</span>
<span class="ltx_bibblock">Near-lossless post-training quantization of deep neural networks via
a piecewise linear approximation.

</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.00104</span>, 2020.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis,
and Joseph&nbsp;H Hassoun.

</span>
<span class="ltx_bibblock">Post-training piecewise linear quantization for deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 69–86.
Springer, 2020.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Julian Faraone, Nicholas Fraser, Michaela Blott, and Philip&nbsp;HW Leong.

</span>
<span class="ltx_bibblock">Syq: Learning symmetric quantization for efficient deep neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4300–4309, 2018.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Alexander Finkelstein, Uri Almog, and Mark Grobman.

</span>
<span class="ltx_bibblock">Fighting quantization bias with bias.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.03193</span>, 2019.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Eric Flamand, Davide Rossi, Francesco Conti, Igor Loi, Antonio Pullini, Florent
Rotenberg, and Luca Benini.

</span>
<span class="ltx_bibblock">Gap-8: A risc-v soc for ai at the edge of the iot.

</span>
<span class="ltx_bibblock">In <span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">2018 IEEE 29th International Conference on
Application-specific Systems, Architectures and Processors (ASAP)</span>, pages
1–4. IEEE, 2018.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Abram&nbsp;L Friesen and Pedro Domingos.

</span>
<span class="ltx_bibblock">Deep learning as a mixed convex-combinatorial optimization problem.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.11573</span>, 2017.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Trevor Gale, Erich Elsen, and Sara Hooker.

</span>
<span class="ltx_bibblock">The state of sparsity in deep neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1902.09574</span>, 2019.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
AE&nbsp;Gamal, L&nbsp;Hemachandra, Itzhak Shperling, and V&nbsp;Wei.

</span>
<span class="ltx_bibblock">Using simulated annealing to design good codes.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Theory</span>, 33(1):116–123, 1987.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Sahaj Garg, Anirudh Jain, Joe Lou, and Mitchell Nahmias.

</span>
<span class="ltx_bibblock">Confounding tradeoffs for neural network quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.06366</span>, 2021.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Sahaj Garg, Joe Lou, Anirudh Jain, and Mitchell Nahmias.

</span>
<span class="ltx_bibblock">Dynamic precision analog computing for neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.06365</span>, 2021.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter Jin,
Sicheng Zhao, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">SqueezeNext: Hardware-aware neural network design.

</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">Workshop paper in CVPR</span>, 2018.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Amir Gholami, Michael&nbsp;W Mahoney, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">An integrated approach to neural network design, training, and
inference.

</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">Univ. California, Berkeley, Berkeley, CA, USA, Tech. Rep</span>, 2020.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Boris Ginsburg, Sergei Nikolaev, Ahmad Kiswani, Hao Wu, Amir Gholaminejad,
Slawomir Kierat, Michael Houston, and Alex Fit-Florea.

</span>
<span class="ltx_bibblock">Tensor processing using low precision format, December&nbsp;28 2017.

</span>
<span class="ltx_bibblock">US Patent App. 15/624,577.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin,
Fengwei Yu, and Junjie Yan.

</span>
<span class="ltx_bibblock">Differentiable soft quantization: Bridging full-precision and low-bit
neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 4852–4861, 2019.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev.

</span>
<span class="ltx_bibblock">Compressing deep convolutional networks using vector quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6115</span>, 2014.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Ian&nbsp;J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1406.2661</span>, 2014.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Robert&nbsp;M. Gray and David&nbsp;L. Neuhoff.

</span>
<span class="ltx_bibblock">Quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">IEEE transactions on information theory</span>, 44(6):2325–2383,
1998.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Nianhui Guo, Joseph Bethge, Haojin Yang, Kai Zhong, Xuefei Ning, Christoph
Meinel, and Yu&nbsp;Wang.

</span>
<span class="ltx_bibblock">Boolnet: Minimizing the energy consumption of binary neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.06991</span>, 2021.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Yiwen Guo, Anbang Yao, Hao Zhao, and Yurong Chen.

</span>
<span class="ltx_bibblock">Network sketching: Exploiting binary structure in deep cnns.

</span>
<span class="ltx_bibblock">In <span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 5955–5963, 2017.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.

</span>
<span class="ltx_bibblock">Deep learning with limited numerical precision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
1737–1746. PMLR, 2015.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Philipp Gysel, Mohammad Motamedi, and Soheil Ghiasi.

</span>
<span class="ltx_bibblock">Hardware-oriented approximation of convolutional neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1604.03168</span>, 2016.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Philipp Gysel, Jon Pimentel, Mohammad Motamedi, and Soheil Ghiasi.

</span>
<span class="ltx_bibblock">Ristretto: A framework for empirical study of resource-efficient
inference in convolutional neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">IEEE transactions on neural networks and learning systems</span>,
29(11):5784–5789, 2018.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Hai&nbsp;Victor Habi, Roy&nbsp;H Jennings, and Arnon Netzer.

</span>
<span class="ltx_bibblock">Hmq: Hardware friendly mixed precision quantization block for cnns.

</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.09952</span>, 2020.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Enhua Wu, and Chang Xu.

</span>
<span class="ltx_bibblock">Training binary neural networks through learning with noisy
supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
4017–4026. PMLR, 2020.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Song Han, Huizi Mao, and William&nbsp;J Dally.

</span>
<span class="ltx_bibblock">Deep compression: Compressing deep neural networks with pruning,
trained quantization and huffman coding.

</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1510.00149</span>, 2015.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Matan Haroush, Itay Hubara, Elad Hoffer, and Daniel Soudry.

</span>
<span class="ltx_bibblock">The knowledge within: Methods for data-free model compression.

</span>
<span class="ltx_bibblock">In <span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 8494–8502, 2020.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Babak Hassibi and David&nbsp;G Stork.

</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">Second order derivatives for network pruning: Optimal brain
surgeon</span>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann, 1993.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Benjamin Hawks, Javier Duarte, Nicholas&nbsp;J Fraser, Alessandro Pappalardo, Nhan
Tran, and Yaman Umuroglu.

</span>
<span class="ltx_bibblock">Ps and qs: Quantization-aware pruning for efficient low latency
neural network inference.

</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.11289</span>, 2021.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Xiangyu He and Jian Cheng.

</span>
<span class="ltx_bibblock">Learning compression from limited unlabeled data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 752–769, 2018.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Xiangyu He, Qinghao Hu, Peisong Wang, and Jian Cheng.

</span>
<span class="ltx_bibblock">Generative zero-shot network quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.08430</span>, 2021.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Yihui He, Ji&nbsp;Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han.

</span>
<span class="ltx_bibblock">Amc: Automl for model compression and acceleration on mobile devices.

</span>
<span class="ltx_bibblock">In <span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 784–800, 2018.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Zhezhi He and Deliang Fan.

</span>
<span class="ltx_bibblock">Simultaneously optimizing weight and quantizer of ternary neural
network using truncated gaussian approximation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 11438–11446, 2019.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng,
and Roeland Nusselder.

</span>
<span class="ltx_bibblock">Latent weights do not exist: Rethinking binarized neural network
optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2019.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Dan Hendrycks and Kevin Gimpel.

</span>
<span class="ltx_bibblock">Gaussian error linear units (GELUs).

</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.08415</span>, 2016.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib95.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1503.02531</span>, 2015.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.

</span>
<span class="ltx_bibblock">Sparsity in deep learning: Pruning and growth for efficient inference
and training in neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.00554</span>, 2021.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Mark Horowitz.

</span>
<span class="ltx_bibblock">1.1 computing’s energy problem (and what we can do about it).

</span>
<span class="ltx_bibblock">In <span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">2014 IEEE International Solid-State Circuits Conference
Digest of Technical Papers (ISSCC)</span>, pages 10–14. IEEE, 2014.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Lu&nbsp;Hou and James&nbsp;T Kwok.

</span>
<span class="ltx_bibblock">Loss-aware weight quantization of deep networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.08635</span>, 2018.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Lu&nbsp;Hou, Quanming Yao, and James&nbsp;T Kwok.

</span>
<span class="ltx_bibblock">Loss-aware binarization of deep networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.01600</span>, 2016.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo&nbsp;Chen, Mingxing
Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et&nbsp;al.

</span>
<span class="ltx_bibblock">Searching for MobilenetV3.

</span>
<span class="ltx_bibblock">In <span id="bib.bib100.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 1314–1324, 2019.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Andrew&nbsp;G Howard, Menglong Zhu, Bo&nbsp;Chen, Dmitry Kalenichenko, Weijun Wang,
Tobias Weyand, Marco Andreetto, and Hartwig Adam.

</span>
<span class="ltx_bibblock">MobileNets: Efficient convolutional neural networks for mobile
vision applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1704.04861</span>, 2017.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Peng Hu, Xi&nbsp;Peng, Hongyuan Zhu, Mohamed M&nbsp;Sabry Aly, and Jie Lin.

</span>
<span class="ltx_bibblock">Opq: Compressing deep neural networks with one-shot
pruning-quantization.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Qinghao Hu, Peisong Wang, and Jian Cheng.

</span>
<span class="ltx_bibblock">From hashing to cnns: Training binary weight networks via hashing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume&nbsp;32, 2018.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Gao Huang, Zhuang Liu, Laurens Van Der&nbsp;Maaten, and Kilian&nbsp;Q Weinberger.

</span>
<span class="ltx_bibblock">Densely connected convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 4700–4708, 2017.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Qijing Huang, Dequan Wang, Zhen Dong, Yizhao Gao, Yaohui Cai, Tian Li, Bichen
Wu, Kurt Keutzer, and John Wawrzynek.

</span>
<span class="ltx_bibblock">Codenet: Efficient deployment of input-adaptive object detection on
embedded fpgas.

</span>
<span class="ltx_bibblock">In <span id="bib.bib105.1.1" class="ltx_text ltx_font_italic">The 2021 ACM/SIGDA International Symposium on
Field-Programmable Gate Arrays</span>, pages 206–216, 2021.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Zehao Huang and Naiyan Wang.

</span>
<span class="ltx_bibblock">Data-driven sparse structure selection for deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib106.1.1" class="ltx_text ltx_font_italic">Proceedings of the European conference on computer vision
(ECCV)</span>, pages 304–320, 2018.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio.

</span>
<span class="ltx_bibblock">Binarized neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib107.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
4107–4115, 2016.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry.

</span>
<span class="ltx_bibblock">Improving post training neural quantization: Layer-wise calibration
and integer programming.

</span>
<span class="ltx_bibblock"><span id="bib.bib108.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.10518</span>, 2020.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
David&nbsp;A Huffman.

</span>
<span class="ltx_bibblock">A method for the construction of minimum-redundancy codes.

</span>
<span class="ltx_bibblock"><span id="bib.bib109.1.1" class="ltx_text ltx_font_italic">Proceedings of the IRE</span>, 40(9):1098–1101, 1952.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Forrest&nbsp;N Iandola, Song Han, Matthew&nbsp;W Moskewicz, Khalid Ashraf, William&nbsp;J
Dally, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and&lt;
0.5 mb model size.

</span>
<span class="ltx_bibblock"><span id="bib.bib110.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1602.07360</span>, 2016.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Yani Ioannou, Duncan Robertson, Roberto Cipolla, and Antonio Criminisi.

</span>
<span class="ltx_bibblock">Deep roots: Improving cnn efficiency with hierarchical filter groups.

</span>
<span class="ltx_bibblock">In <span id="bib.bib111.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 1231–1240, 2017.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Sergey Ioffe and Christian Szegedy.

</span>
<span class="ltx_bibblock">Batch normalization: Accelerating deep network training by reducing
internal covariate shift.

</span>
<span class="ltx_bibblock">In <span id="bib.bib112.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
448–456. PMLR, 2015.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Benoit Jacob, Skirmantas Kligys, Bo&nbsp;Chen, Menglong Zhu, Matthew Tang, Andrew
Howard, Hartwig Adam, and Dmitry Kalenichenko.

</span>
<span class="ltx_bibblock">Quantization and training of neural networks for efficient
integer-arithmetic-only inference.

</span>
<span class="ltx_bibblock">In <span id="bib.bib113.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2018.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Animesh Jain, Shoubhik Bhattacharya, Masahiro Masuda, Vin Sharma, and Yida
Wang.

</span>
<span class="ltx_bibblock">Efficient execution of quantized deep learning models: A compiler
approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib114.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.10226</span>, 2020.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Shubham Jain, Swagath Venkataramani, Vijayalakshmi Srinivasan, Jungwook Choi,
Kailash Gopalakrishnan, and Leland Chang.

</span>
<span class="ltx_bibblock">Biscaled-dnn: Quantizing long-tailed datastructures with two scale
factors for deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib115.1.1" class="ltx_text ltx_font_italic">2019 56th ACM/IEEE Design Automation Conference (DAC)</span>, pages
1–6. IEEE, 2019.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Eric Jang, Shixiang Gu, and Ben Poole.

</span>
<span class="ltx_bibblock">Categorical reparameterization with gumbel-softmax.

</span>
<span class="ltx_bibblock"><span id="bib.bib116.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.01144</span>, 2016.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Herve Jegou, Matthijs Douze, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Product quantization for nearest neighbor search.

</span>
<span class="ltx_bibblock"><span id="bib.bib117.1.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>,
33(1):117–128, 2010.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Yongkweon Jeon, Baeseong Park, Se&nbsp;Jung Kwon, Byeongwook Kim, Jeongin Yun, and
Dongsoo Lee.

</span>
<span class="ltx_bibblock">Biqgemm: matrix multiplication with lookup table for
binary-coding-based quantized dnns.

</span>
<span class="ltx_bibblock"><span id="bib.bib118.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.09904</span>, 2020.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Tianchu Ji, Shraddhan Jain, Michael Ferdman, Peter Milder, H&nbsp;Andrew Schwartz,
and Niranjan Balasubramanian.

</span>
<span class="ltx_bibblock">On the distribution, sparsity, and inference-time quantization of
attention values in transformers.

</span>
<span class="ltx_bibblock"><span id="bib.bib119.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.01335</span>, 2021.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Kai Jia and Martin Rinard.

</span>
<span class="ltx_bibblock">Efficient exact verification of binarized neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib120.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, and Zhiliang Gan.

</span>
<span class="ltx_bibblock">Kdlsq-bert: A quantized bert combining knowledge distillation with
learned step size quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib121.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.05938</span>, 2021.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Qing Jin, Linjie Yang, and Zhenyu Liao.

</span>
<span class="ltx_bibblock">Adabits: Neural network quantization with adaptive bit-widths.

</span>
<span class="ltx_bibblock">In <span id="bib.bib122.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2146–2156, 2020.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
Jeff Johnson.

</span>
<span class="ltx_bibblock">Rethinking floating point for deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib123.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.01721</span>, 2018.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Felix Juefei-Xu, Vishnu Naresh&nbsp;Boddeti, and Marios Savvides.

</span>
<span class="ltx_bibblock">Local binary convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib124.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 19–28, 2017.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun
Kwak, Sung&nbsp;Ju Hwang, and Changkyu Choi.

</span>
<span class="ltx_bibblock">Learning to quantize deep networks by optimizing quantization
intervals with task loss.

</span>
<span class="ltx_bibblock">In <span id="bib.bib125.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 4350–4359, 2019.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Prad Kadambi, Karthikeyan&nbsp;Natesan Ramamurthy, and Visar Berisha.

</span>
<span class="ltx_bibblock">Comparing fisher information regularization with distillation for dnn
quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib126.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
PP&nbsp;Kanjilal, PK&nbsp;Dey, and DN&nbsp;Banerjee.

</span>
<span class="ltx_bibblock">Reduced-size neural networks through singular value decomposition and
subset selection.

</span>
<span class="ltx_bibblock"><span id="bib.bib127.1.1" class="ltx_text ltx_font_italic">Electronics Letters</span>, 29(17):1516–1518, 1993.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Mel&nbsp;Win Khaw, Luminita Stevens, and Michael Woodford.

</span>
<span class="ltx_bibblock">Discrete adjustment to a changing environment: Experimental evidence.

</span>
<span class="ltx_bibblock"><span id="bib.bib128.1.1" class="ltx_text ltx_font_italic">Journal of Monetary Economics</span>, 91:88–103, 2017.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Hyungjun Kim, Kyungsu Kim, Jinseok Kim, and Jae-Joon Kim.

</span>
<span class="ltx_bibblock">Binaryduo: Reducing gradient mismatch in binary activation network by
coupling binary activations.

</span>
<span class="ltx_bibblock"><span id="bib.bib129.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2020.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Jangho Kim, KiYoon Yoo, and Nojun Kwak.

</span>
<span class="ltx_bibblock">Position-based scaled gradient for model quantization and sparse
training.

</span>
<span class="ltx_bibblock"><span id="bib.bib130.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Minje Kim and Paris Smaragdis.

</span>
<span class="ltx_bibblock">Bitwise neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib131.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1601.06071</span>, 2016.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael&nbsp;W Mahoney, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">I-bert: Integer-only bert quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib132.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.01321</span>, 2021.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Raghuraman Krishnamoorthi.

</span>
<span class="ltx_bibblock">Quantizing deep convolutional networks for efficient inference: A
whitepaper.

</span>
<span class="ltx_bibblock"><span id="bib.bib133.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1806.08342</span>, 2018.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Andrey Kuzmin, Markus Nagel, Saurabh Pitre, Sandeep Pendyam, Tijmen
Blankevoort, and Max Welling.

</span>
<span class="ltx_bibblock">Taxonomy and evaluation of structured compression of convolutional
neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib134.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.09802</span>, 2019.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
Se&nbsp;Jung Kwon, Dongsoo Lee, Byeongwook Kim, Parichay Kapoor, Baeseong Park, and
Gu-Yeon Wei.

</span>
<span class="ltx_bibblock">Structured compression by weight encryption for unstructured pruning
and quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib135.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 1909–1918, 2020.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Liangzhen Lai, Naveen Suda, and Vikas Chandra.

</span>
<span class="ltx_bibblock">CMSIS-NN: Efficient neural network kernels for arm cortex-m cpus.

</span>
<span class="ltx_bibblock"><span id="bib.bib136.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1801.06601</span>, 2018.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
Hamed&nbsp;F Langroudi, Zachariah Carmichael, David Pastuch, and Dhireesha
Kudithipudi.

</span>
<span class="ltx_bibblock">Cheetah: Mixed low-precision hardware &amp; software co-design framework
for dnns on the edge.

</span>
<span class="ltx_bibblock"><span id="bib.bib137.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.02386</span>, 2019.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Kenneth&nbsp;W Latimer, Jacob&nbsp;L Yates, Miriam&nbsp;LR Meister, Alexander&nbsp;C Huk, and
Jonathan&nbsp;W Pillow.

</span>
<span class="ltx_bibblock">Single-trial spike trains in parietal cortex reveal discrete steps
during decision-making.

</span>
<span class="ltx_bibblock"><span id="bib.bib138.1.1" class="ltx_text ltx_font_italic">Science</span>, 349(6244):184–187, 2015.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Yann LeCun, John&nbsp;S Denker, and Sara&nbsp;A Solla.

</span>
<span class="ltx_bibblock">Optimal brain damage.

</span>
<span class="ltx_bibblock">In <span id="bib.bib139.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
598–605, 1990.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Difference target propagation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib140.1.1" class="ltx_text ltx_font_italic">Joint european conference on machine learning and knowledge
discovery in databases</span>, pages 498–515. Springer, 2015.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
Dongsoo Lee, Se&nbsp;Jung Kwon, Byeongwook Kim, Yongkweon Jeon, Baeseong Park, and
Jeongin Yun.

</span>
<span class="ltx_bibblock">Flexor: Trainable fractional quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib141.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Jun&nbsp;Haeng Lee, Sangwon Ha, Saerom Choi, Won-Jo Lee, and Seungwon Lee.

</span>
<span class="ltx_bibblock">Quantization for rapid deployment of deep neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib142.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.05488</span>, 2018.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip&nbsp;HS Torr.

</span>
<span class="ltx_bibblock">Snip: Single-shot network pruning based on connection sensitivity.

</span>
<span class="ltx_bibblock"><span id="bib.bib143.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.02340</span>, 2018.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
Cong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu, and Rong Jin.

</span>
<span class="ltx_bibblock">Extremely low bit neural network: Squeeze the last bit out with admm.

</span>
<span class="ltx_bibblock">In <span id="bib.bib144.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume&nbsp;32, 2018.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Fengfu Li, Bo&nbsp;Zhang, and Bin Liu.

</span>
<span class="ltx_bibblock">Ternary weight networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib145.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1605.04711</span>, 2016.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan, and Rui Fan.

</span>
<span class="ltx_bibblock">Fully quantized network for object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib146.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2019.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
Yuhang Li, Xin Dong, and Wei Wang.

</span>
<span class="ltx_bibblock">Additive powers-of-two quantization: An efficient non-uniform
discretization for neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib147.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.13144</span>, 2019.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
Yuhang Li, Ruihao Gong, Xu&nbsp;Tan, Yang Yang, Peng Hu, Qi&nbsp;Zhang, Fengwei Yu, Wei
Wang, and Shi Gu.

</span>
<span class="ltx_bibblock">Brecq: Pushing the limit of post-training quantization by block
reconstruction.

</span>
<span class="ltx_bibblock"><span id="bib.bib148.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
Yuhang Li, Ruihao Gong, Fengwei Yu, Xin Dong, and Xianglong Liu.

</span>
<span class="ltx_bibblock">Dms: Differentiable dimension search for binary neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib149.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2020.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia
Li.

</span>
<span class="ltx_bibblock">Learning from noisy labels with distillation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib150.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 1910–1918, 2017.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and Wen Gao.

</span>
<span class="ltx_bibblock">Performance guaranteed network acceleration via high-order residual
quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib151.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 2584–2592, 2017.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
Tailin Liang, John Glossner, Lei Wang, and Shaobo Shi.

</span>
<span class="ltx_bibblock">Pruning and quantization for deep neural network acceleration: A
survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib152.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.09671</span>, 2021.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
Zhenyu Liao, Romain Couillet, and Michael&nbsp;W Mahoney.

</span>
<span class="ltx_bibblock">Sparse quantized spectral clustering.

</span>
<span class="ltx_bibblock"><span id="bib.bib153.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy.

</span>
<span class="ltx_bibblock">Fixed point quantization of deep convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib154.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
2849–2858. PMLR, 2016.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian Wu,
Feiyue Huang, and Chia-Wen Lin.

</span>
<span class="ltx_bibblock">Rotated binary neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib155.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
Shaohui Lin, Rongrong Ji, Yuchao Li, Yongjian Wu, Feiyue Huang, and Baochang
Zhang.

</span>
<span class="ltx_bibblock">Accelerating convolutional networks via global &amp; dynamic filter
pruning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib156.1.1" class="ltx_text ltx_font_italic">IJCAI</span>, pages 2425–2432, 2018.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
Wuwei Lin.

</span>
<span class="ltx_bibblock">Automating optimization of quantized deep learning models on cuda:
https://tvm.apache.org/2019/04/29/opt-cuda-quantized, 2019.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
Xiaofan Lin, Cong Zhao, and Wei Pan.

</span>
<span class="ltx_bibblock">Towards accurate binary convolutional neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib158.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.11294</span>, 2017.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Neural networks with few multiplications.

</span>
<span class="ltx_bibblock"><span id="bib.bib159.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1510.03009</span>, 2015.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu,
Rongrong Ji, and David Doermann.

</span>
<span class="ltx_bibblock">Circulant binary convolutional networks: Enhancing the performance of
1-bit dcnns with circulant back propagation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib160.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2691–2699, 2019.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
Hanxiao Liu, Karen Simonyan, and Yiming Yang.

</span>
<span class="ltx_bibblock">Darts: Differentiable architecture search.

</span>
<span class="ltx_bibblock"><span id="bib.bib161.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1806.09055</span>, 2018.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
Hongyang Liu, Sara Elkerdawy, Nilanjan Ray, and Mostafa Elhoushi.

</span>
<span class="ltx_bibblock">Layer importance estimation with imprinting for neural network
quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib162.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2408–2417, 2021.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">RoBERTa: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib163.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.11692</span>, 2019.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng.

</span>
<span class="ltx_bibblock">Bi-real net: Enhancing the performance of 1-bit cnns with improved
representational capability and advanced training algorithm.

</span>
<span class="ltx_bibblock">In <span id="bib.bib164.1.1" class="ltx_text ltx_font_italic">Proceedings of the European conference on computer vision
(ECCV)</span>, pages 722–737, 2018.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Zhi-Gang Liu and Matthew Mattina.

</span>
<span class="ltx_bibblock">Learning low-precision neural networks without straight-through
estimator (STE).

</span>
<span class="ltx_bibblock"><span id="bib.bib165.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1903.01061</span>, 2019.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.

</span>
<span class="ltx_bibblock">Thinet: A filter level pruning method for deep neural network
compression.

</span>
<span class="ltx_bibblock">In <span id="bib.bib166.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 5058–5066, 2017.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.

</span>
<span class="ltx_bibblock">Shufflenet V2: Practical guidelines for efficient cnn architecture
design.

</span>
<span class="ltx_bibblock">In <span id="bib.bib167.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 116–131, 2018.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
Franck Mamalet and Christophe Garcia.

</span>
<span class="ltx_bibblock">Simplifying convnets for fast learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib168.1.1" class="ltx_text ltx_font_italic">International Conference on Artificial Neural Networks</span>,
pages 58–65. Springer, 2012.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos.

</span>
<span class="ltx_bibblock">Training binary neural networks with real-to-binary convolutions.

</span>
<span class="ltx_bibblock"><span id="bib.bib169.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.11535</span>, 2020.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Julieta Martinez, Shobhit Zakhmi, Holger&nbsp;H Hoos, and James&nbsp;J Little.

</span>
<span class="ltx_bibblock">Lsq++: Lower running time and higher recall in multi-codebook
quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib170.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 491–506, 2018.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
Warren&nbsp;S McCulloch and Walter Pitts.

</span>
<span class="ltx_bibblock">A logical calculus of the ideas immanent in nervous activity.

</span>
<span class="ltx_bibblock"><span id="bib.bib171.1.1" class="ltx_text ltx_font_italic">The bulletin of mathematical biophysics</span>, 5(4):115–133, 1943.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
Jeffrey&nbsp;L McKinstry, Steven&nbsp;K Esser, Rathinakumar Appuswamy, Deepika Bablani,
John&nbsp;V Arthur, Izzet&nbsp;B Yildiz, and Dharmendra&nbsp;S Modha.

</span>
<span class="ltx_bibblock">Discovering low-precision networks close to full-precision networks
for efficient embedded inference.

</span>
<span class="ltx_bibblock"><span id="bib.bib172.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1809.04191</span>, 2018.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul.

</span>
<span class="ltx_bibblock">Mixed precision training with 8-bit floating point.

</span>
<span class="ltx_bibblock"><span id="bib.bib173.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.12334</span>, 2019.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman.

</span>
<span class="ltx_bibblock">Same, same but different: Recovering neural network quantization
error through weight factorization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib174.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
4486–4495. PMLR, 2019.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
Venkatesh, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mixed precision training.

</span>
<span class="ltx_bibblock"><span id="bib.bib175.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.03740</span>, 2017.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
Szymon Migacz.

</span>
<span class="ltx_bibblock">Nvidia 8-bit inference with tensorrt.

</span>
<span class="ltx_bibblock"><span id="bib.bib176.1.1" class="ltx_text ltx_font_italic">GPU Technology Conference</span>, 2017.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
Asit Mishra and Debbie Marr.

</span>
<span class="ltx_bibblock">Apprentice: Using knowledge distillation techniques to improve
low-precision network accuracy.

</span>
<span class="ltx_bibblock"><span id="bib.bib177.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.05852</span>, 2017.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Asit Mishra, Eriko Nurvitadhi, Jeffrey&nbsp;J Cook, and Debbie Marr.

</span>
<span class="ltx_bibblock">Wrpn: Wide reduced-precision networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib178.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1709.01134</span>, 2017.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
Daisuke Miyashita, Edward&nbsp;H Lee, and Boris Murmann.

</span>
<span class="ltx_bibblock">Convolutional neural networks using logarithmic data representation.

</span>
<span class="ltx_bibblock"><span id="bib.bib179.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1603.01025</span>, 2016.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
Lopamudra Mukherjee, Sathya&nbsp;N Ravi, Jiming Peng, and Vikas Singh.

</span>
<span class="ltx_bibblock">A biresolution spectral framework for product quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib180.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 3329–3338, 2018.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
Markus Nagel, Rana&nbsp;Ali Amjad, Mart Van&nbsp;Baalen, Christos Louizos, and Tijmen
Blankevoort.

</span>
<span class="ltx_bibblock">Up or down? adaptive rounding for post-training quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib181.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
7197–7206. PMLR, 2020.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
Markus Nagel, Mart&nbsp;van Baalen, Tijmen Blankevoort, and Max Welling.

</span>
<span class="ltx_bibblock">Data-free quantization through weight equalization and bias
correction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib182.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 1325–1334, 2019.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
Markus Nagel, Marios Fournarakis, Rana&nbsp;Ali Amjad, Yelysei Bondarenko, Mart van
Baalen, and Tijmen Blankevoort.

</span>
<span class="ltx_bibblock">A white paper on neural network quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib183.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.08295</span>, 2021.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
Maxim Naumov, Utku Diril, Jongsoo Park, Benjamin Ray, Jedrzej Jablonski, and
Andrew Tulloch.

</span>
<span class="ltx_bibblock">On periodic functions as regularizers for quantization of neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib184.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.09862</span>, 2018.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
Maxim Naumov, Dheevatsa Mudigere, Hao-Jun&nbsp;Michael Shi, Jianyu Huang, Narayanan
Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu,
Alisson&nbsp;G Azzolini, et&nbsp;al.

</span>
<span class="ltx_bibblock">Deep learning recommendation model for personalization and
recommendation systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib185.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.00091</span>, 2019.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
Renkun Ni, Hong-min Chu, Oscar Castañeda, Ping-yeh Chiang, Christoph
Studer, and Tom Goldstein.

</span>
<span class="ltx_bibblock">Wrapnet: Neural net inference with ultra-low-resolution arithmetic.

</span>
<span class="ltx_bibblock"><span id="bib.bib186.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.13242</span>, 2020.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
Lin Ning, Guoyang Chen, Weifeng Zhang, and Xipeng Shen.

</span>
<span class="ltx_bibblock">Simple augmentation goes a long way: {ADRL} for {dnn}
quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib187.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
BM&nbsp;Oliver, JR&nbsp;Pierce, and Claude&nbsp;E Shannon.

</span>
<span class="ltx_bibblock">The philosophy of pcm.

</span>
<span class="ltx_bibblock"><span id="bib.bib188.1.1" class="ltx_text ltx_font_italic">Proceedings of the IRE</span>, 36(11):1324–1331, 1948.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Eunhyeok Park, Junwhan Ahn, and Sungjoo Yoo.

</span>
<span class="ltx_bibblock">Weighted-entropy-based quantization for deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib189.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 5456–5464, 2017.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
Eunhyeok Park, Sungjoo Yoo, and Peter Vajda.

</span>
<span class="ltx_bibblock">Value-aware quantization for training and inference of neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib190.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 580–595, 2018.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
Sejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin.

</span>
<span class="ltx_bibblock">Lookahead: a far-sighted alternative of magnitude-based pruning.

</span>
<span class="ltx_bibblock"><span id="bib.bib191.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.04809</span>, 2020.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho.

</span>
<span class="ltx_bibblock">Relational knowledge distillation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib192.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 3967–3976, 2019.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
Peng Peng, Mingyu You, Weisheng Xu, and Jiaxin Li.

</span>
<span class="ltx_bibblock">Fully integer-based quantization for mobile convolutional neural
network inference.

</span>
<span class="ltx_bibblock"><span id="bib.bib193.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 432:194–205, 2021.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean.

</span>
<span class="ltx_bibblock">Efficient neural architecture search via parameters sharing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib194.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
4095–4104. PMLR, 2018.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
Antonio Polino, Razvan Pascanu, and Dan Alistarh.

</span>
<span class="ltx_bibblock">Model compression via distillation and quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib195.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.05668</span>, 2018.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
Haotong Qin, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao, Shuai Yi,
Xianglong Liu, and Hao Su.

</span>
<span class="ltx_bibblock">Bipointnet: Binary neural network for point clouds.

</span>
<span class="ltx_bibblock"><span id="bib.bib196.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu
Sebe.

</span>
<span class="ltx_bibblock">Binary neural networks: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib197.1.1" class="ltx_text ltx_font_italic">Pattern Recognition</span>, 105:107281, 2020.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu,
and Jingkuan Song.

</span>
<span class="ltx_bibblock">Forward and backward information retention for accurate binary neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib198.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2250–2259, 2020.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
Zhongnan Qu, Zimu Zhou, Yun Cheng, and Lothar Thiele.

</span>
<span class="ltx_bibblock">Adaptive loss-aware quantization for multi-bit networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib199.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, June 2020.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training, 2018.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib201.1.1" class="ltx_text ltx_font_italic">OpenAI blog</span>, 1(8):9, 2019.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
Prajit Ramachandran, Barret Zoph, and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Searching for activation functions.

</span>
<span class="ltx_bibblock"><span id="bib.bib202.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.05941</span>, 2017.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
Prajit Ramachandran, Barret Zoph, and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Swish: a self-gated activation function.

</span>
<span class="ltx_bibblock"><span id="bib.bib203.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.05941</span>, 7:1, 2017.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.

</span>
<span class="ltx_bibblock">Xnor-net: Imagenet classification using binary convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib204.1.1" class="ltx_text ltx_font_italic">European conference on computer vision</span>, pages 525–542.
Springer, 2016.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
Ryan Razani, Gregoire Morin, Eyyub Sari, and Vahid&nbsp;Partovi Nia.

</span>
<span class="ltx_bibblock">Adaptive binary-ternary quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib205.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 4613–4618, 2021.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
Bernhard Riemann.

</span>
<span class="ltx_bibblock"><span id="bib.bib206.1.1" class="ltx_text ltx_font_italic">Ueber die Darstellbarkeit einer Function durch eine
trigonometrische Reihe</span>, volume&nbsp;13.

</span>
<span class="ltx_bibblock">Dieterich, 1867.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
Adriana Romero, Nicolas Ballas, Samira&nbsp;Ebrahimi Kahou, Antoine Chassang, Carlo
Gatta, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Fitnets: Hints for thin deep nets.

</span>
<span class="ltx_bibblock"><span id="bib.bib207.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6550</span>, 2014.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
Kenneth Rose, Eitan Gurewitz, and Geoffrey Fox.

</span>
<span class="ltx_bibblock">A deterministic annealing approach to clustering.

</span>
<span class="ltx_bibblock"><span id="bib.bib208.1.1" class="ltx_text ltx_font_italic">Pattern Recognition Letters</span>, 11(9):589–594, 1990.

</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
Frank Rosenblatt.

</span>
<span class="ltx_bibblock"><span id="bib.bib209.1.1" class="ltx_text ltx_font_italic">The perceptron, a perceiving and recognizing automaton Project
Para</span>.

</span>
<span class="ltx_bibblock">Cornell Aeronautical Laboratory, 1957.

</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
Frank Rosenblatt.

</span>
<span class="ltx_bibblock">Principles of neurodynamics. perceptrons and the theory of brain
mechanisms.

</span>
<span class="ltx_bibblock">Technical report, Cornell Aeronautical Lab Inc Buffalo NY, 1961.

</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
Manuele Rusci, Marco Fariselli, Alessandro Capotondi, and Luca Benini.

</span>
<span class="ltx_bibblock">Leveraging automated mixed-low-precision quantization for tiny edge
microcontrollers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib211.1.1" class="ltx_text ltx_font_italic">IoT Streams for Data-Driven Predictive Maintenance and IoT,
Edge, and Mobile for Embedded Machine Learning</span>, pages 296–308. Springer,
2020.

</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
Tara&nbsp;N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana
Ramabhadran.

</span>
<span class="ltx_bibblock">Low-rank matrix factorization for deep neural network training with
high-dimensional output targets.

</span>
<span class="ltx_bibblock">In <span id="bib.bib212.1.1" class="ltx_text ltx_font_italic">2013 IEEE international conference on acoustics, speech and
signal processing</span>, pages 6655–6659. IEEE, 2013.

</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
Dave Salvator, Hao Wu, Milind Kulkarni, and Niall Emmart.

</span>
<span class="ltx_bibblock">Int4 precision for ai inference:
https://developer.nvidia.com/blog/int4-for-ai-inference/, 2019.

</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
Chen.

</span>
<span class="ltx_bibblock">MobilenetV2: Inverted residuals and linear bottlenecks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib214.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4510–4520, 2018.

</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
Claude&nbsp;E Shannon.

</span>
<span class="ltx_bibblock">A mathematical theory of communication.

</span>
<span class="ltx_bibblock"><span id="bib.bib215.1.1" class="ltx_text ltx_font_italic">The Bell system technical journal</span>, 27(3):379–423, 1948.

</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
Claude&nbsp;E Shannon.

</span>
<span class="ltx_bibblock">Coding theorems for a discrete source with a fidelity criterion.

</span>
<span class="ltx_bibblock"><span id="bib.bib216.1.1" class="ltx_text ltx_font_italic">IRE Nat. Conv. Rec</span>, 4(142-163):1, 1959.

</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
Alexander Shekhovtsov, Viktor Yanush, and Boris Flach.

</span>
<span class="ltx_bibblock">Path sample-analytic gradient estimators for stochastic binary
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib217.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
Mingzhu Shen, Xianglong Liu, Ruihao Gong, and Kai Han.

</span>
<span class="ltx_bibblock">Balanced binary neural networks with gated residual.

</span>
<span class="ltx_bibblock">In <span id="bib.bib218.1.1" class="ltx_text ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>, pages 4197–4201. IEEE, 2020.

</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
Michael&nbsp;W Mahoney, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">Q-BERT: Hessian based ultra low precision quantization of bert.

</span>
<span class="ltx_bibblock">In <span id="bib.bib219.1.1" class="ltx_text ltx_font_italic">AAAI</span>, pages 8815–8821, 2020.

</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
William&nbsp;Fleetwood Sheppard.

</span>
<span class="ltx_bibblock">On the calculation of the most probable values of
frequency-constants, for data arranged according to equidistant division of a
scale.

</span>
<span class="ltx_bibblock"><span id="bib.bib220.1.1" class="ltx_text ltx_font_italic">Proceedings of the London Mathematical Society</span>, 1(1):353–380,
1897.

</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
Sungho Shin, Kyuyeon Hwang, and Wonyong Sung.

</span>
<span class="ltx_bibblock">Fixed-point performance analysis of recurrent neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib221.1.1" class="ltx_text ltx_font_italic">2016 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</span>, pages 976–980. IEEE, 2016.

</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yuri Nahshan, Alex
Bronstein, and Uri Weiser.

</span>
<span class="ltx_bibblock">Robust quantization: One model to rule them all.

</span>
<span class="ltx_bibblock"><span id="bib.bib222.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
Gil Shomron, Freddy Gabbay, Samer Kurzum, and Uri Weiser.

</span>
<span class="ltx_bibblock">Post-training sparsity-aware quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib223.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.11010</span>, 2021.

</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
K.&nbsp;Simonyan and A.&nbsp;Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib224.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2015.

</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
S.&nbsp;M. Stigler.

</span>
<span class="ltx_bibblock"><span id="bib.bib225.1.1" class="ltx_text ltx_font_italic">The History of Statistics: The Measurement of Uncertainty before
1900</span>.

</span>
<span class="ltx_bibblock">Harvard University Press, Cambridge, 1986.

</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
Pierre Stock, Angela Fan, Benjamin Graham, Edouard Grave, Rémi Gribonval,
Herve Jegou, and Armand Joulin.

</span>
<span class="ltx_bibblock">Training with quantization noise for extreme model compression.

</span>
<span class="ltx_bibblock">In <span id="bib.bib226.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock">
Pierre Stock, Armand Joulin, Rémi Gribonval, Benjamin Graham, and Hervé
Jégou.

</span>
<span class="ltx_bibblock">And the bit goes down: Revisiting the quantization of neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib227.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.05686</span>, 2019.

</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock">
John&nbsp;Z Sun, Grace&nbsp;I Wang, Vivek&nbsp;K Goyal, and Lav&nbsp;R Varshney.

</span>
<span class="ltx_bibblock">A framework for bayesian optimality of psychophysical laws.

</span>
<span class="ltx_bibblock"><span id="bib.bib228.1.1" class="ltx_text ltx_font_italic">Journal of Mathematical Psychology</span>, 56(6):495–501, 2012.

</span>
</li>
<li id="bib.bib229" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock">
Wonyong Sung, Sungho Shin, and Kyuyeon Hwang.

</span>
<span class="ltx_bibblock">Resiliency of deep neural networks under quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib229.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.06488</span>, 2015.

</span>
</li>
<li id="bib.bib230" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock">
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
Wojna.

</span>
<span class="ltx_bibblock">Rethinking the Inception architecture for computer vision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib230.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 2818–2826, 2016.

</span>
</li>
<li id="bib.bib231" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock">
Shyam&nbsp;A Tailor, Javier Fernandez-Marques, and Nicholas&nbsp;D Lane.

</span>
<span class="ltx_bibblock">Degree-quant: Quantization-aware training for graph neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib231.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib232" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock">
Mingxing Tan, Bo&nbsp;Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
Howard, and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Mnasnet: Platform-aware neural architecture search for mobile.

</span>
<span class="ltx_bibblock">In <span id="bib.bib232.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2820–2828, 2019.

</span>
</li>
<li id="bib.bib233" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock">
Mingxing Tan and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">EfficientNet: Rethinking model scaling for convolutional neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib233.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.11946</span>, 2019.

</span>
</li>
<li id="bib.bib234" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock">
Wei Tang, Gang Hua, and Liang Wang.

</span>
<span class="ltx_bibblock">How to train a compact binary neural network with high accuracy?

</span>
<span class="ltx_bibblock">In <span id="bib.bib234.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume&nbsp;31, 2017.

</span>
</li>
<li id="bib.bib235" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[235]</span>
<span class="ltx_bibblock">
Antti Tarvainen and Harri Valpola.

</span>
<span class="ltx_bibblock">Mean teachers are better role models: Weight-averaged consistency
targets improve semi-supervised deep learning results.

</span>
<span class="ltx_bibblock"><span id="bib.bib235.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1703.01780</span>, 2017.

</span>
</li>
<li id="bib.bib236" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[236]</span>
<span class="ltx_bibblock">
James Tee and Desmond&nbsp;P Taylor.

</span>
<span class="ltx_bibblock">Is information in the brain represented in continuous or discrete
form?

</span>
<span class="ltx_bibblock"><span id="bib.bib236.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Molecular, Biological and Multi-Scale
Communications</span>, 6(3):199–209, 2020.

</span>
</li>
<li id="bib.bib237" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[237]</span>
<span class="ltx_bibblock">
L.N. Trefethen and D.&nbsp;Bau III.

</span>
<span class="ltx_bibblock"><span id="bib.bib237.1.1" class="ltx_text ltx_font_italic">Numerical Linear Algebra</span>.

</span>
<span class="ltx_bibblock">SIAM, Philadelphia, 1997.

</span>
</li>
<li id="bib.bib238" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[238]</span>
<span class="ltx_bibblock">
Frederick Tung and Greg Mori.

</span>
<span class="ltx_bibblock">Clip-q: Deep network compression learning by in-parallel
pruning-quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib238.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 7873–7882, 2018.

</span>
</li>
<li id="bib.bib239" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[239]</span>
<span class="ltx_bibblock">
Mart van Baalen, Christos Louizos, Markus Nagel, Rana&nbsp;Ali Amjad, Ying Wang,
Tijmen Blankevoort, and Max Welling.

</span>
<span class="ltx_bibblock">Bayesian bits: Unifying quantization and pruning.

</span>
<span class="ltx_bibblock"><span id="bib.bib239.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib240" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[240]</span>
<span class="ltx_bibblock">
Rufin VanRullen and Christof Koch.

</span>
<span class="ltx_bibblock">Is perception discrete or continuous?

</span>
<span class="ltx_bibblock"><span id="bib.bib240.1.1" class="ltx_text ltx_font_italic">Trends in cognitive sciences</span>, 7(5):207–213, 2003.

</span>
</li>
<li id="bib.bib241" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[241]</span>
<span class="ltx_bibblock">
Lav&nbsp;R Varshney, Per&nbsp;Jesper Sjöström, and Dmitri&nbsp;B Chklovskii.

</span>
<span class="ltx_bibblock">Optimal information storage in noisy synapses under resource
constraints.

</span>
<span class="ltx_bibblock"><span id="bib.bib241.1.1" class="ltx_text ltx_font_italic">Neuron</span>, 52(3):409–423, 2006.

</span>
</li>
<li id="bib.bib242" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[242]</span>
<span class="ltx_bibblock">
Lav&nbsp;R Varshney and Kush&nbsp;R Varshney.

</span>
<span class="ltx_bibblock">Decision making with quantized priors leads to discrimination.

</span>
<span class="ltx_bibblock"><span id="bib.bib242.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 105(2):241–255, 2016.

</span>
</li>
<li id="bib.bib243" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[243]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span id="bib.bib243.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
5998–6008, 2017.

</span>
</li>
<li id="bib.bib244" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[244]</span>
<span class="ltx_bibblock">
Diwen Wan, Fumin Shen, Li&nbsp;Liu, Fan Zhu, Jie Qin, Ling Shao, and Heng&nbsp;Tao Shen.

</span>
<span class="ltx_bibblock">Tbn: Convolutional neural network with ternary inputs and binary
weights.

</span>
<span class="ltx_bibblock">In <span id="bib.bib244.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 315–332, 2018.

</span>
</li>
<li id="bib.bib245" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[245]</span>
<span class="ltx_bibblock">
Dilin Wang, Meng Li, Chengyue Gong, and Vikas Chandra.

</span>
<span class="ltx_bibblock">Attentivenas: Improving neural architecture search via attentive
sampling.

</span>
<span class="ltx_bibblock"><span id="bib.bib245.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2011.09011</span>, 2020.

</span>
</li>
<li id="bib.bib246" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[246]</span>
<span class="ltx_bibblock">
Kuan Wang, Zhijian Liu, Yujun Lin, Ji&nbsp;Lin, and Song Han.

</span>
<span class="ltx_bibblock">HAQ: Hardware-aware automated quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib246.1.1" class="ltx_text ltx_font_italic">In Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, 2019.

</span>
</li>
<li id="bib.bib247" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[247]</span>
<span class="ltx_bibblock">
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash
Gopalakrishnan.

</span>
<span class="ltx_bibblock">Training deep neural networks with 8-bit floating point numbers.

</span>
<span class="ltx_bibblock"><span id="bib.bib247.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2018.

</span>
</li>
<li id="bib.bib248" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[248]</span>
<span class="ltx_bibblock">
Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang, Yang Liu, and Jian Cheng.

</span>
<span class="ltx_bibblock">Two-step quantization for low-bit neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib248.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on computer vision and
pattern recognition</span>, pages 4376–4384, 2018.

</span>
</li>
<li id="bib.bib249" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[249]</span>
<span class="ltx_bibblock">
Tianzhe Wang, Kuan Wang, Han Cai, Ji&nbsp;Lin, Zhijian Liu, Hanrui Wang, Yujun Lin,
and Song Han.

</span>
<span class="ltx_bibblock">Apq: Joint search for network architecture, pruning and quantization
policy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib249.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2078–2087, 2020.

</span>
</li>
<li id="bib.bib250" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[250]</span>
<span class="ltx_bibblock">
Ying Wang, Yadong Lu, and Tijmen Blankevoort.

</span>
<span class="ltx_bibblock">Differentiable joint pruning and quantization for hardware
efficiency.

</span>
<span class="ltx_bibblock">In <span id="bib.bib250.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 259–277.
Springer, 2020.

</span>
</li>
<li id="bib.bib251" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[251]</span>
<span class="ltx_bibblock">
Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi&nbsp;Tian.

</span>
<span class="ltx_bibblock">Learning channel-wise interactions for binary convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib251.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 568–577, 2019.

</span>
</li>
<li id="bib.bib252" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[252]</span>
<span class="ltx_bibblock">
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu,
Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">FBNet: Hardware-aware efficient convnet design via differentiable
neural architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib252.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 10734–10742, 2019.

</span>
</li>
<li id="bib.bib253" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[253]</span>
<span class="ltx_bibblock">
Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah Golmant, Amir
Gholaminejad, Joseph Gonzalez, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">Shift: A zero flop, zero parameter alternative to spatial
convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib253.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 9127–9135, 2018.

</span>
</li>
<li id="bib.bib254" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[254]</span>
<span class="ltx_bibblock">
Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt
Keutzer.

</span>
<span class="ltx_bibblock">Mixed precision quantization of convnets via differentiable neural
architecture search.

</span>
<span class="ltx_bibblock"><span id="bib.bib254.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.00090</span>, 2018.

</span>
</li>
<li id="bib.bib255" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[255]</span>
<span class="ltx_bibblock">
Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius.

</span>
<span class="ltx_bibblock">Integer quantization for deep learning inference: Principles and
empirical evaluation.

</span>
<span class="ltx_bibblock"><span id="bib.bib255.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.09602</span>, 2020.

</span>
</li>
<li id="bib.bib256" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[256]</span>
<span class="ltx_bibblock">
Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng.

</span>
<span class="ltx_bibblock">Quantized convolutional neural networks for mobile devices.

</span>
<span class="ltx_bibblock">In <span id="bib.bib256.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4820–4828, 2016.

</span>
</li>
<li id="bib.bib257" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[257]</span>
<span class="ltx_bibblock">
Xia Xiao, Zigeng Wang, and Sanguthevar Rajasekaran.

</span>
<span class="ltx_bibblock">Autoprune: Automatic network pruning by regularizing auxiliary
parameters.

</span>
<span class="ltx_bibblock">In <span id="bib.bib257.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
13681–13691, 2019.

</span>
</li>
<li id="bib.bib258" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[258]</span>
<span class="ltx_bibblock">
Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, and
Hongbin Zha.

</span>
<span class="ltx_bibblock">Alternating multi-bit quantization for recurrent neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib258.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.00150</span>, 2018.

</span>
</li>
<li id="bib.bib259" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[259]</span>
<span class="ltx_bibblock">
Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang,
and Mingkui Tan.

</span>
<span class="ltx_bibblock">Generative low-bitwidth data free quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib259.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 1–17.
Springer, 2020.

</span>
</li>
<li id="bib.bib260" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[260]</span>
<span class="ltx_bibblock">
Yinghao Xu, Xin Dong, Yudian Li, and Hao Su.

</span>
<span class="ltx_bibblock">A main/subsidiary network framework for simplifying binary neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib260.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 7154–7162, 2019.

</span>
</li>
<li id="bib.bib261" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[261]</span>
<span class="ltx_bibblock">
Zhe Xu and Ray&nbsp;CC Cheung.

</span>
<span class="ltx_bibblock">Accurate and compact convolutional neural networks with trained
binarization.

</span>
<span class="ltx_bibblock"><span id="bib.bib261.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.11366</span>, 2019.

</span>
</li>
<li id="bib.bib262" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[262]</span>
<span class="ltx_bibblock">
Haichuan Yang, Shupeng Gui, Yuhao Zhu, and Ji&nbsp;Liu.

</span>
<span class="ltx_bibblock">Automatic neural network compression by sparsity-quantization joint
learning: A constrained optimization-based approach.

</span>
<span class="ltx_bibblock">In <span id="bib.bib262.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2178–2188, 2020.

</span>
</li>
<li id="bib.bib263" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[263]</span>
<span class="ltx_bibblock">
Huanrui Yang, Lin Duan, Yiran Chen, and Hai Li.

</span>
<span class="ltx_bibblock">Bsq: Exploring bit-level sparsity for mixed-precision neural network
quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib263.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.10462</span>, 2021.

</span>
</li>
<li id="bib.bib264" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[264]</span>
<span class="ltx_bibblock">
Jiwei Yang, Xu&nbsp;Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang
Huang, and Xian-sheng Hua.

</span>
<span class="ltx_bibblock">Quantization networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib264.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 7308–7316, 2019.

</span>
</li>
<li id="bib.bib265" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[265]</span>
<span class="ltx_bibblock">
Tien-Ju Yang, Andrew Howard, Bo&nbsp;Chen, Xiao Zhang, Alec Go, Mark Sandler,
Vivienne Sze, and Hartwig Adam.

</span>
<span class="ltx_bibblock">Netadapt: Platform-aware neural network adaptation for mobile
applications.

</span>
<span class="ltx_bibblock">In <span id="bib.bib265.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 285–300, 2018.

</span>
</li>
<li id="bib.bib266" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[266]</span>
<span class="ltx_bibblock">
Zhaohui Yang, Yunhe Wang, Kai Han, Chunjing Xu, Chao Xu, Dacheng Tao, and Chang
Xu.

</span>
<span class="ltx_bibblock">Searching for low-bit weights in quantized neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib266.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib267" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[267]</span>
<span class="ltx_bibblock">
Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan,
Leyuan Wang, Qijing Huang, Yida Wang, Michael&nbsp;W Mahoney, et&nbsp;al.

</span>
<span class="ltx_bibblock">Hawqv3: Dyadic neural network quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib267.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2011.10680</span>, 2020.

</span>
</li>
<li id="bib.bib268" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[268]</span>
<span class="ltx_bibblock">
Jianming Ye, Shiliang Zhang, and Jingdong Wang.

</span>
<span class="ltx_bibblock">Distillation guided residual learning for binary convolutional neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib268.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.05223</span>, 2020.

</span>
</li>
<li id="bib.bib269" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[269]</span>
<span class="ltx_bibblock">
Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim.

</span>
<span class="ltx_bibblock">A gift from knowledge distillation: Fast optimization, network
minimization and transfer learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib269.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4133–4141, 2017.

</span>
</li>
<li id="bib.bib270" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[270]</span>
<span class="ltx_bibblock">
Hongxu Yin, Pavlo Molchanov, Jose&nbsp;M Alvarez, Zhizhong Li, Arun Mallya, Derek
Hoiem, Niraj&nbsp;K Jha, and Jan Kautz.

</span>
<span class="ltx_bibblock">Dreaming to distill: Data-free knowledge transfer via deepinversion.

</span>
<span class="ltx_bibblock">In <span id="bib.bib270.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 8715–8724, 2020.

</span>
</li>
<li id="bib.bib271" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[271]</span>
<span class="ltx_bibblock">
Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack
Xin.

</span>
<span class="ltx_bibblock">Understanding straight-through estimator in training activation
quantized neural nets.

</span>
<span class="ltx_bibblock"><span id="bib.bib271.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1903.05662</span>, 2019.

</span>
</li>
<li id="bib.bib272" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[272]</span>
<span class="ltx_bibblock">
Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack
Xin.

</span>
<span class="ltx_bibblock">Blended coarse gradient descent for full quantization of deep neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib272.1.1" class="ltx_text ltx_font_italic">Research in the Mathematical Sciences</span>, 6(1):14, 2019.

</span>
</li>
<li id="bib.bib273" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[273]</span>
<span class="ltx_bibblock">
Shan You, Chang Xu, Chao Xu, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Learning from multiple teacher networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib273.1.1" class="ltx_text ltx_font_italic">Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining</span>, pages 1285–1294, 2017.

</span>
</li>
<li id="bib.bib274" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[274]</span>
<span class="ltx_bibblock">
Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad&nbsp;I Morariu, Xintong Han,
Mingfei Gao, Ching-Yung Lin, and Larry&nbsp;S Davis.

</span>
<span class="ltx_bibblock">Nisp: Pruning networks using neuron importance score propagation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib274.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 9194–9203, 2018.

</span>
</li>
<li id="bib.bib275" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[275]</span>
<span class="ltx_bibblock">
Shixing Yu, Zhewei Yao, Amir Gholami, Zhen Dong, Michael&nbsp;W Mahoney, and Kurt
Keutzer.

</span>
<span class="ltx_bibblock">Hessian-aware pruning and optimal neural implant.

</span>
<span class="ltx_bibblock"><span id="bib.bib275.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.08940</span>, 2021.

</span>
</li>
<li id="bib.bib276" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[276]</span>
<span class="ltx_bibblock">
Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua.

</span>
<span class="ltx_bibblock">Lq-nets: Learned quantization for highly accurate and compact deep
neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib276.1.1" class="ltx_text ltx_font_italic">European conference on computer vision (ECCV)</span>, 2018.

</span>
</li>
<li id="bib.bib277" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[277]</span>
<span class="ltx_bibblock">
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng
Ma.

</span>
<span class="ltx_bibblock">Be your own teacher: Improve the performance of convolutional neural
networks via self distillation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib277.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 3713–3722, 2019.

</span>
</li>
<li id="bib.bib278" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[278]</span>
<span class="ltx_bibblock">
Wei Zhang, Lu&nbsp;Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu.

</span>
<span class="ltx_bibblock">Ternarybert: Distillation-aware ultra-low bit bert.

</span>
<span class="ltx_bibblock"><span id="bib.bib278.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2009.12812</span>, 2020.

</span>
</li>
<li id="bib.bib279" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[279]</span>
<span class="ltx_bibblock">
Chenglong Zhao, Bingbing Ni, Jian Zhang, Qiwei Zhao, Wenjun Zhang, and Qi&nbsp;Tian.

</span>
<span class="ltx_bibblock">Variational convolutional neural network pruning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib279.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 2780–2789, 2019.

</span>
</li>
<li id="bib.bib280" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[280]</span>
<span class="ltx_bibblock">
Qibin Zhao, Masashi Sugiyama, Longhao Yuan, and Andrzej Cichocki.

</span>
<span class="ltx_bibblock">Learning efficient tensor representations with ring-structured
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib280.1.1" class="ltx_text ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>, pages 8608–8612. IEEE, 2019.

</span>
</li>
<li id="bib.bib281" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[281]</span>
<span class="ltx_bibblock">
Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De&nbsp;Sa, and Zhiru Zhang.

</span>
<span class="ltx_bibblock">Improving neural network quantization without retraining using
outlier channel splitting.

</span>
<span class="ltx_bibblock"><span id="bib.bib281.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, 2019.

</span>
</li>
<li id="bib.bib282" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[282]</span>
<span class="ltx_bibblock">
Sijie Zhao, Tao Yue, and Xuemei Hu.

</span>
<span class="ltx_bibblock">Distribution-aware adaptive multi-bit quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib282.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 9281–9290, 2021.

</span>
</li>
<li id="bib.bib283" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[283]</span>
<span class="ltx_bibblock">
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen.

</span>
<span class="ltx_bibblock">Incremental network quantization: Towards lossless cnns with
low-precision weights.

</span>
<span class="ltx_bibblock"><span id="bib.bib283.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1702.03044</span>, 2017.

</span>
</li>
<li id="bib.bib284" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[284]</span>
<span class="ltx_bibblock">
Aojun Zhou, Anbang Yao, Kuan Wang, and Yurong Chen.

</span>
<span class="ltx_bibblock">Explicit loss-error-aware quantization for low-bit deep neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib284.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 9426–9435, 2018.

</span>
</li>
<li id="bib.bib285" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[285]</span>
<span class="ltx_bibblock">
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He&nbsp;Wen, and Yuheng Zou.

</span>
<span class="ltx_bibblock">Dorefa-net: Training low bitwidth convolutional neural networks with
low bitwidth gradients.

</span>
<span class="ltx_bibblock"><span id="bib.bib285.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.06160</span>, 2016.

</span>
</li>
<li id="bib.bib286" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[286]</span>
<span class="ltx_bibblock">
Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal
Frossard.

</span>
<span class="ltx_bibblock">Adaptive quantization for deep neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib286.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1712.01048</span>, 2017.

</span>
</li>
<li id="bib.bib287" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[287]</span>
<span class="ltx_bibblock">
Chenzhuo Zhu, Song Han, Huizi Mao, and William&nbsp;J Dally.

</span>
<span class="ltx_bibblock">Trained ternary quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib287.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1612.01064</span>, 2016.

</span>
</li>
<li id="bib.bib288" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[288]</span>
<span class="ltx_bibblock">
Shilin Zhu, Xin Dong, and Hao Su.

</span>
<span class="ltx_bibblock">Binary ensemble neural network: More bits per network or more
networks per bit?

</span>
<span class="ltx_bibblock">In <span id="bib.bib288.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 4923–4932, 2019.

</span>
</li>
<li id="bib.bib289" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[289]</span>
<span class="ltx_bibblock">
Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid.

</span>
<span class="ltx_bibblock">Towards effective low-bitwidth convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib289.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 7920–7928, 2018.

</span>
</li>
<li id="bib.bib290" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[290]</span>
<span class="ltx_bibblock">
Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid.

</span>
<span class="ltx_bibblock">Structured binary neural networks for accurate image classification
and semantic segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib290.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 413–422, 2019.

</span>
</li>
<li id="bib.bib291" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[291]</span>
<span class="ltx_bibblock">
Barret Zoph and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Neural architecture search with reinforcement learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib291.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.01578</span>, 2016.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2103.13629" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2103.13630" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2103.13630">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2103.13630" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2103.13631" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 17:04:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2103.13630] A Survey of Quantization Methods for Efficient Neural Network Inference</title><meta property="og:description" content="As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations aro…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Survey of Quantization Methods for Efficient Neural Network Inference">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Survey of Quantization Methods for Efficient Neural Network Inference">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2103.13630">

<!--Generated on Wed Mar  6 17:04:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\useunder</span>
<p id="p1.2" class="ltx_p"><span id="p1.2.1" class="ltx_text ltx_ulem_uline"></span><span id="p1.2.2" class="ltx_text ltx_framed ltx_framed_underline"></span>                                                                                                           <span id="p1.2.3" class="ltx_text" lang="en"></span></p>
</div>
<h1 class="ltx_title ltx_title_document" lang="en">A Survey of Quantization Methods for Efficient Neural Network Inference</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">
<span id="id5.5.4" class="ltx_text" style="font-size:120%;">Amir Gholami<sup id="id5.5.4.1" class="ltx_sup"><span id="id5.5.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>,
Sehoon Kim<sup id="id5.5.4.2" class="ltx_sup"><span id="id5.5.4.2.1" class="ltx_text ltx_font_italic">∗</span></sup>,
Zhen Dong<sup id="id5.5.4.3" class="ltx_sup"><span id="id5.5.4.3.1" class="ltx_text ltx_font_italic">∗</span></sup>,
Zhewei Yao<sup id="id5.5.4.4" class="ltx_sup"><span id="id5.5.4.4.1" class="ltx_text ltx_font_italic">∗</span></sup>,
Michael W. Mahoney,
Kurt Keutzer
<br class="ltx_break">University of California, Berkeley
<br class="ltx_break"><span id="id5.5.4.5" class="ltx_text ltx_font_typewriter" style="font-size:75%;">{amirgh, sehoonkim, zhendong, zheweiy, mahoneymw, keutzer}@berkeley.edu</span>
</span>
</span><span class="ltx_author_notes"><sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic" style="font-size:120%;">∗</span></sup><span id="id7.7.id2" class="ltx_text" style="font-size:120%;">Equal contribution.</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id8.id1"><span class="ltx_text" id="id8.id1.1" lang="en">추상적인 수학적 계산이 디지털 컴퓨터에서의 계산에 적응하자마자, 그러한 계산에서 수치 값의 효율적인 표현, 조작 및 통신의 문제가 발생했다. 숫자 표현의 문제와 강하게 관련된 것은 양자화의 문제이다: 필요한 비트 수를 최소화하고 또한 수반되는 계산의 정확도를 최대화하기 위해 연속 실수 값 숫자 세트를 고정된 이산 숫자 세트에 분산해야 하는 방법은 무엇인가? 양자화의 이러한 다년생 문제는 메모리 및/또는 계산 자원이 심각하게 제한될 때마다 특히 관련이 있으며, 컴퓨터 비전, 자연 언어 처리 및 관련 영역에서 신경망 모델의 놀라운 성능으로 인해 최근 몇 년 동안 전면에 등장했다. 부동 소수점 표현에서 4비트 이하로 표현된 저-정밀 고정 정수 값으로의 이동은 메모리 풋프린트 및 레이턴시를 16배만큼 감소시킬 가능성을 보유하며, 실제로, 4x 내지 8x의 감소는 종종 이러한 애플리케이션들에서 실제로 실현된다. 따라서, 양자화가 최근 신경망과 관련된 계산의 효율적인 구현에 있어서 중요하고 매우 활발한 연구의 하위 영역으로 부상한 것은 놀라운 일이 아니다. 본 논문에서는 심층신경망 계산에서 수치의 정량화 문제에 대한 기존의 방법들의 장단점을 고찰한다. 이 설문조사와 그 조직을 통해, 우리는 신경망에 대한 양자화에 있어 현재 연구의 유용한 스냅샷을 제시하고 이 분야의 향후 연구에 대한 평가를 쉽게 할 수 있는 지능화된 조직을 제공하기를 희망한다. </span></p>
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">지난 10년 동안, 우리는 종종 고도로 과대 매개변수화된 모델에 의해 달성되는 광범위한 문제에 대해 신경망(NN)의 정확도에서 상당한 개선을 관찰했다. 이러한 오버파라미터화된(따라서 매우 큰) NN 모델의 정확도가 상당히 증가했지만, 이러한 모델의 순전한 크기는 많은 리소스 제약 애플리케이션에 대해 이를 배포할 수 없음을 의미한다. 이는 자원이 제한된 환경에서 낮은 에너지 소비와 높은 정확도로 실시간 추론이 요구되는 퍼베이시브 딥러닝을 구현하기 위한 문제를 발생시킨다. 이 퍼베이시브 딥 러닝은 실시간 지능형 의료 모니터링, 자율 주행, 오디오 분석 및 음성 인식과 같은 광범위한 응용 프로그램에 상당한 영향을 미칠 것으로 예상된다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">최적의 정확도로 효율적인 실시간 NN을 달성하려면 NN 모델의 설계, 훈련 및 배포를 다시 생각해야 한다. NN 모델을 보다 효율적으로(레이턴시, 메모리 풋프린트 및 에너지 소비 등 측면에서) 만들면서 최적의 정확도/일반화 트레이드오프를 제공함으로써 이러한 문제를 해결하는 데 초점을 맞춘 많은 문헌이 있다. 이러한 노력은 크게 다음과 같이 범주화할 수 있다.</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Designing efficient NN model architectures</h5>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">한 가지 작업은 NN 모델 아키텍처를 마이크로 아키텍처 [<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib111" title="">111</a>, <a class="ltx_ref" href="#bib.bib101" title="">101</a>, <a class="ltx_ref" href="#bib.bib168" title="">168</a>, <a class="ltx_ref" href="#bib.bib167" title="">167</a>, <a class="ltx_ref" href="#bib.bib253" title="">253</a>, <a class="ltx_ref" href="#bib.bib212" title="">212</a>, <a class="ltx_ref" href="#bib.bib127" title="">127</a>, <a class="ltx_ref" href="#bib.bib280" title="">280</a>]</cite> (예: 깊이별 컨볼루션 또는 저순위 인수분해와 같은 커널 유형) 뿐만 아니라 매크로 아키텍처 [<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib110" title="">110</a>, <a class="ltx_ref" href="#bib.bib101" title="">101</a>, <a class="ltx_ref" href="#bib.bib104" title="">104</a>, <a class="ltx_ref" href="#bib.bib214" title="">214</a>, <a class="ltx_ref" href="#bib.bib100" title="">100</a>, <a class="ltx_ref" href="#bib.bib233" title="">233</a>]</cite> (예: 잔차 또는 시작과 같은 모듈 유형) 측면에서 최적화하는 데 중점을 두었다. 여기에서 고전적인 기술은 대부분 수동 검색을 사용하여 새로운 아키텍처 모듈을 찾았는데, 이는 확장성이 없다. 이와 같이, 새로운 작업 라인은 자동 기계 학습(AutoML) 및 신경망 구조 검색(NAS) 방법을 설계하는 것이다. 이들은 모델 크기, 깊이 및/또는 폭 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib291" title="">291</a>, <a class="ltx_ref" href="#bib.bib194" title="">194</a>, <a class="ltx_ref" href="#bib.bib232" title="">232</a>, <a class="ltx_ref" href="#bib.bib161" title="">161</a>, <a class="ltx_ref" href="#bib.bib252" title="">252</a>, <a class="ltx_ref" href="#bib.bib245" title="">245</a>]</cite>의 주어진 제약 조건에서 올바른 NN 아키텍처를 자동화된 방식으로 찾는 것을 목표로 한다. NAS 방법에 대한 최근 조사는 관심 있는 판독기를 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib54" title="">54</a>]</cite>에 참조한다.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Co-designing NN architecture and hardware together</h5>

<div id="S1.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p1.1">또 다른 최근 작업 라인은 특정 타겟 하드웨어 플랫폼에 대해 NN 아키텍처를 적응(및 공동 설계)하는 것이었다. 이것의 중요성은 NN 컴포넌트의 오버헤드(레이턴시 및 에너지 측면에서)가 하드웨어 의존적이기 때문이다. 예를 들어, 전용 캐시 계층을 갖는 하드웨어는 이러한 캐시 계층이 없는 하드웨어보다 훨씬 효율적으로 대역폭 제한 연산을 실행할 수 있다. NN 아키텍처 설계와 유사하게, 아키텍처-하드웨어 공동 설계에서의 초기 접근법은 수동적이었고, 전문가가 NN 아키텍처 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib70" title="">70</a>]</cite>를 적응/변경한 다음 자동화된 AutoML 및/또는 NAS 기술 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib23" title="">23</a>, <a class="ltx_ref" href="#bib.bib22" title="">22</a>, <a class="ltx_ref" href="#bib.bib252" title="">252</a>, <a class="ltx_ref" href="#bib.bib100" title="">100</a>]</cite>를 사용했다.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Pruning</h5>

<div id="S1.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S1.SS0.SSS0.Px3.p1.1">NN의 메모리 풋프린트 및 계산 비용을 감소시키기 위한 또 다른 접근법은 가지치기(pruning)를 적용하는 것이다. 가지치기에서 작은 <em class="ltx_emph ltx_font_italic" id="S1.SS0.SSS0.Px3.p1.1.1">saliency</em> (민감도)를 가진 뉴런이 제거되어 희소 계산 그래프가 생성된다. 여기서, 현저성이 작은 뉴런은 제거가 모델 출력/손실 함수에 최소로 영향을 미치는 뉴런이다. 프루닝 방법은 크게 비구조적 프루닝 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib139" title="">139</a>, <a class="ltx_ref" href="#bib.bib86" title="">86</a>, <a class="ltx_ref" href="#bib.bib49" title="">49</a>, <a class="ltx_ref" href="#bib.bib143" title="">143</a>, <a class="ltx_ref" href="#bib.bib257" title="">257</a>, <a class="ltx_ref" href="#bib.bib191" title="">191</a>]</cite>와 구조적 프루닝 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib166" title="">166</a>, <a class="ltx_ref" href="#bib.bib91" title="">91</a>, <a class="ltx_ref" href="#bib.bib274" title="">274</a>, <a class="ltx_ref" href="#bib.bib156" title="">156</a>, <a class="ltx_ref" href="#bib.bib106" title="">106</a>, <a class="ltx_ref" href="#bib.bib279" title="">279</a>, <a class="ltx_ref" href="#bib.bib275" title="">275</a>]</cite>로 분류할 수 있다. 구조화되지 않은 가지치기로 뉴런이 발생하는 곳마다 작은 돌출도로 뉴런을 제거한다. 이 접근법을 사용하면 모델의 일반화 성능에 거의 영향을 미치지 않으면서 대부분의 NN 매개변수를 제거하는 공격적인 가지치기를 수행할 수 있다. 그러나, 이 접근법은 가속하기 어려운 것으로 알려진 희소 행렬 연산으로 이어지며, 일반적으로 메모리-바운드 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>, <a class="ltx_ref" href="#bib.bib66" title="">66</a>]</cite>이다. 반면에, 구조화된 프루닝으로, 파라미터들의 그룹(예를 들어, 전체 컨볼루션 필터들)이 제거된다. 이는 레이어 및 가중치 행렬의 입력 및 출력 형상을 변화시키는 효과를 가지며, 따라서 여전히 조밀한 행렬 연산을 허용한다. 그러나 공격적으로 구조화된 가지치기는 종종 상당한 정확도 저하를 초래한다. 높은 수준의 가지치기/희소성을 가진 훈련과 추론은 최첨단 성능을 유지하면서 열린 문제<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib16" title="">16</a>]</cite>로 남아 있다. 우리는 가지치기/희소성 관련 작업에 대한 철저한 조사를 위해 관심 있는 독자를 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib66" title="">66</a>, <a class="ltx_ref" href="#bib.bib96" title="">96</a>, <a class="ltx_ref" href="#bib.bib134" title="">134</a>]</cite>에 참조한다.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Knowledge distillation</h5>

<div id="S1.SS0.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S1.SS0.SSS0.Px4.p1.1">모델 증류 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib207" title="">207</a>, <a class="ltx_ref" href="#bib.bib95" title="">95</a>, <a class="ltx_ref" href="#bib.bib177" title="">177</a>, <a class="ltx_ref" href="#bib.bib150" title="">150</a>, <a class="ltx_ref" href="#bib.bib269" title="">269</a>, <a class="ltx_ref" href="#bib.bib195" title="">195</a>, <a class="ltx_ref" href="#bib.bib3" title="">3</a>, <a class="ltx_ref" href="#bib.bib270" title="">270</a>]</cite>는 큰 모델을 훈련시킨 후 이를 교사로 사용하여 보다 컴팩트한 모델을 훈련시키는 것을 포함한다. 학생 모델의 훈련 중에 "하드" 클래스 레이블을 사용하는 대신 모델 증류의 핵심 아이디어는 교사가 생성한 "소프트" 확률을 활용하는 것인데, 이러한 확률은 입력에 대한 더 많은 정보를 포함할 수 있기 때문이다. 증류에 대한 많은 작업에도 불구하고, 여기서 주요 과제는 증류만으로 높은 압축비를 달성하는 것이다. <math alttext="\geq 4\times" class="ltx_math_unparsed" display="inline" id="S1.SS0.SSS0.Px4.p1.1.m1.1"><semantics id="S1.SS0.SSS0.Px4.p1.1.m1.1a"><mrow id="S1.SS0.SSS0.Px4.p1.1.m1.1b"><mo id="S1.SS0.SSS0.Px4.p1.1.m1.1.1">≥</mo><mn id="S1.SS0.SSS0.Px4.p1.1.m1.1.2">4</mn><mo id="S1.SS0.SSS0.Px4.p1.1.m1.1.3" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px4.p1.1.m1.1c">\geq 4\times</annotation></semantics></math> 압축(INT8 및 더 낮은 정밀도 포함)으로 성능을 유지할 수 있는 양자화 및 프루닝에 비해, 지식 증류 방법은 공격적인 압축으로 무시할 수 없는 정확도 저하를 갖는 경향이 있다. 그러나, 지식증류와 이전 방법(즉, 양자화 및 가지치기)의 조합은 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib195" title="">195</a>]</cite>에서 큰 성공을 거두었다.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Quantization</h5>

<div id="S1.SS0.SSS0.Px5.p1" class="ltx_para">
<p class="ltx_p" id="S1.SS0.SSS0.Px5.p1.1">마지막으로, 양자화는 NN 모델의 훈련과 추론 모두에서 크고 일관된 성공을 보여준 접근법이다. 수치 표현과 양자화의 문제는 디지털 컴퓨팅만큼 오래되었지만 신경망은 개선의 독특한 기회를 제공한다. 양자화에 대한 이 조사는 대부분 추론에 초점을 맞추고 있지만, 우리는 양자화의 중요한 성공이 NN 훈련 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib10" title="">10</a>, <a class="ltx_ref" href="#bib.bib247" title="">247</a>, <a class="ltx_ref" href="#bib.bib130" title="">130</a>, <a class="ltx_ref" href="#bib.bib57" title="">57</a>, <a class="ltx_ref" href="#bib.bib35" title="">35</a>]</cite>에서 이루어졌다는 점을 강조해야 한다. 특히, 반 정밀도와 혼합 정밀도 훈련 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>, <a class="ltx_ref" href="#bib.bib79" title="">79</a>, <a class="ltx_ref" href="#bib.bib72" title="">72</a>, <a class="ltx_ref" href="#bib.bib175" title="">175</a>]</cite>의 획기적인 발전은 AI 가속기에서 훨씬 더 높은 처리량을 가능하게 한 주요 드라이버였다. 그러나, 상당한 튜닝 없이 반정밀도 이하로 가는 것은 매우 어렵다는 것이 입증되었고, 최근의 양자화 연구의 대부분은 추론에 초점을 맞추고 있다. 추론을 위한 이 양자화는 이 글의 초점이다.</p>
</div>
</section>
<section id="S1.SS0.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Quantization and Neuroscience</h5>

<div id="S1.SS0.SSS0.Px6.p1" class="ltx_para">
<p class="ltx_p" id="S1.SS0.SSS0.Px6.p1.1">NN 양자화와 느슨하게 관련되어(그리고 어떤 동기부여를 위해) 인간의 뇌가 연속적인 형태가 아니라 이산/양자화된 형태로 정보를 저장한다는 것을 암시하는 신경과학에서의 작업은 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib171" title="">171</a>, <a class="ltx_ref" href="#bib.bib240" title="">240</a>, <a class="ltx_ref" href="#bib.bib236" title="">236</a>]</cite>이다. 이 아이디어에 대한 인기 있는 근거는 연속적인 형태로 저장된 정보가 필연적으로 잡음(우리의 뇌를 포함한 물리적 환경에 항상 존재하며 열, 감각, 외부, 시냅스 잡음 등에 의해 유발될 수 있음)에 의해 손상될 것이라는 것이다. 그러나, 이산 신호 표현들은 그러한 저-레벨 노이즈에 더 견고할 수 있다. 이산 표현 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib138" title="">138</a>, <a class="ltx_ref" href="#bib.bib242" title="">242</a>, <a class="ltx_ref" href="#bib.bib128" title="">128</a>]</cite>의 더 높은 일반화 능력과 제한된 리소스 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib241" title="">241</a>]</cite>에서 더 높은 효율성을 포함한 다른 이유들도 제안되었다. 신경과학 문헌의 관련 연구에 대한 철저한 검토를 위해 독자를 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib228" title="">228</a>]</cite>에 참조한다.</p>
</div>
<div id="S1.SS0.SSS0.Px6.p2" class="ltx_para">
<p class="ltx_p" id="S1.SS0.SSS0.Px6.p2.1">이 작업의 목표는 양자화에 사용되는 현재 방법과 개념을 소개하고 이 연구 라인에서 현재 도전과 기회를 논의하는 것이다. 그렇게 함으로써, 우리는 가장 관련된 일에 대해 논의하려고 노력했다. 짧은 조사의 페이지 한계에서 NN 양자화만큼 큰 분야의 모든 작업을 논의하는 것은 불가능하며, 우리가 일부 관련 논문을 놓쳤다는 것은 의심의 여지가 없다. 저희가 소홀히 했을 수 있는 논문의 독자와 저자 모두에게 미리 사과드립니다.</p>
</div>
<div id="S1.SS0.SSS0.Px6.p3" class="ltx_para">
<p class="ltx_p" id="S1.SS0.SSS0.Px6.p3.1">이 조사의 구조 측면에서, 우리는 먼저 섹션 <a class="ltx_ref" href="#S2" title="II General History of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">II</span></a>에서 양자화의 간략한 이력을 제공하고, 이어서 섹션 <a class="ltx_ref" href="#S3" title="III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">III</span></a>에서 양자화의 기초가 되는 기본 개념을 소개할 것이다. 이러한 기본 개념은 대부분의 양자화 알고리즘과 공유되며, 기존의 방법을 이해하고 배치하는 데 필요하다. 그런 다음 섹션 <a class="ltx_ref" href="#S4" title="IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">IV</span></a>에서 더 발전된 주제에 대해 논의한다. 이들은 대부분 최근의 최첨단 방법들, 특히 저/혼합-정밀 양자화를 위한 방법들을 포함한다. 그런 다음 에지 프로세서에 특별한 초점을 두고 섹션 <a class="ltx_ref" href="#S5" title="V Quantization and Hardware Processors ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">V</span></a>에서 하드웨어 가속기에서 양자화의 의미에 대해 논의한다. 마지막으로, 우리는 섹션 <a class="ltx_ref" href="#S7" title="VII Summary and Conclusions ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">VII</span></a>에서 요약과 결론을 제공한다.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">General History of Quantization</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p" id="S2.p1.1">Gray와 Neuhoff는 1998년 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib76" title="">76</a>]</cite>까지 양자화의 역사에 대해 매우 좋은 조사를 작성했다. 그 기사는 훌륭하고 전체적으로 읽을 가치가 있지만, 독자의 편의를 위해 우리는 여기서 몇 가지 핵심 사항을 간략하게 요약할 것이다. 양자화는 큰(종종 연속) 집합의 입력 값에서 작은(종종 유한) 집합의 출력 값으로 매핑하는 방법으로서 오랜 역사를 가지고 있다. 반올림 및 절단이 대표적인 예입니다. 양자화는 미적분학의 기초와 관련이 있으며, 관련 방법은 1800년대 초(뿐만 아니라 훨씬 이전)에 볼 수 있으며, 예를 들어 대규모(1800년대 초의 표준에 따라) 데이터 분석을 위한 최소 제곱 및 관련 기술에 대한 초기 작업에서 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib225" title="">225</a>]</cite>를 볼 수 있다. 양자화에 대한 초기 작업은 1867년으로 거슬러 올라가며, 여기서 이산화는 적분의 계산을 근사화하는 데 사용되었으며, 이후 1897년에 샤파드가 적분 결과 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib220" title="">220</a>]</cite>에 반올림 오류가 미치는 영향을 조사했다. 최근 디지털 신호 처리에서 양자화는 디지털 형태로 신호를 표현하는 과정에서 일반적으로 반올림뿐만 아니라 수치 해석 및 수치 알고리즘 구현에 있어 중요해지고 있으며, 여기서 실수 값에 대한 계산은 유한 정밀 산술로 구현된다.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p" id="S2.p2.1">1948년이 되어서야, 디지털 컴퓨터의 등장 무렵, 섀넌이 의사소통의 수학적 이론 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib215" title="">215</a>]</cite>에 대한 그의 주요 논문을 썼을 때, 양자화의 효과와 코딩 이론에서의 사용이 공식적으로 제시되었다. 특히 섀넌은 그의 무손실 코딩 이론에서 관심 이벤트들이 비균일 확률을 가질 때, 동일한 비트 수를 사용하는 것은 낭비적이라고 주장했다. 그는 이제 <em class="ltx_emph ltx_font_italic" id="S2.p2.1.1">variable-rate quantization</em>으로 알려진 개념인 이벤트의 확률에 따라 비트 수를 가변하는 것이 더 최적의 접근 방법이 될 것이라고 주장했다. 특히 허프만 코딩은 이 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib109" title="">109</a>]</cite>에 의해 동기가 부여된다. 1959년 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib216" title="">216</a>]</cite>의 후속 작업에서 섀넌은 벡터 양자화의 개념뿐만 아니라 왜곡률 함수(코딩 후 신호 왜곡에 대한 하한을 제공함)를 도입했다(섹션 <a class="ltx_ref" href="#S4.SS6" title="IV-F Vector Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-F</span></span></a>에서도 간략하게 논의됨). 이 개념은 실제 통신 응용을 위해 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib53" title="">53</a>, <a class="ltx_ref" href="#bib.bib67" title="">67</a>, <a class="ltx_ref" href="#bib.bib55" title="">55</a>, <a class="ltx_ref" href="#bib.bib208" title="">208</a>]</cite>에서 확장되어 실용화되었다. 그 시간대의 신호 처리에서 양자화에 대한 다른 중요한 역사적 연구로는 펄스 코드 변조(PCM) 개념을 도입한 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib188" title="">188</a>]</cite>(샘플링된 아날로그 신호를 근사/대표/인코딩하기 위해 제안된 펄스 방식)와 고해상도 양자화의 고전적인 결과 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib14" title="">14</a>]</cite>가 있다. 이러한 문제에 대한 자세한 논의는 관심 있는 독자를 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib76" title="">76</a>]</cite>에 참조한다.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p" id="S2.p3.7">양화는 연속적인 수학적 양과 관련된 문제에 대해 수치 근사를 사용하는 알고리즘에서 약간 다른 방식으로 나타나며, 이 영역은 또한 오랜 역사를 가지고 있지만 디지털 컴퓨터의 등장으로 새로운 관심을 받았다. 수치 해석에서 중요한 개념은 <em class="ltx_emph ltx_font_italic" id="S2.p3.7.1">well-posed problem</em>이었다. 이러한 문제를 간혹 <em class="ltx_emph ltx_font_italic" id="S2.p3.7.2">well-conditioned problems</em>이라고 한다. 주어진 잘 조건화된 문제로 작업할 때에도 어떤 이상화된 의미에서 그 문제를 "정확하게" 해결하는 특정 알고리즘은 반올림 및 절단 오류의 특수성에 의해 도입된 "잡음"이 있는 상태에서 매우 저조한 성능을 발휘하는 것으로 나타났다. 이러한 반올림 오차는 유한한 수의 비트만으로 실수를 표현하는 것과 관련이 있다. 예를 들어 IEEE 부동 소수점 표준에 의해 지정된 양자화; 그리고 반복 알고리즘의 유한한 수의 반복만이 실제로 수행될 수 있기 때문에 절단 오차가 발생한다. 후자는 "정확한 산술"에서도 중요한데, 왜냐하면 연속 수학의 대부분의 문제는 원칙적으로 유한한 기본 연산의 수열에 의해 해결될 수 없기 때문이다; 그러나 전자는 양자화와 관련이 있다. 이러한 문제는 알고리즘의 <em class="ltx_emph ltx_font_italic" id="S2.p3.7.3">numerical stability</em>의 개념으로 이어졌다. 숫자 알고리즘을 함수 <math alttext="f" class="ltx_Math" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">f</annotation></semantics></math>가 입력 데이터 <math alttext="x" class="ltx_Math" display="inline" id="S2.p3.2.m2.1"><semantics id="S2.p3.2.m2.1a"><mi id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">x</annotation></semantics></math>를 “true” 솔루션 <math alttext="y" class="ltx_Math" display="inline" id="S2.p3.3.m3.1"><semantics id="S2.p3.3.m3.1a"><mi id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">y</annotation></semantics></math>에 매핑하려고 시도하는 것으로 보자. 그러나 반올림 및 절단 오류로 인해 알고리즘의 출력은 실제로 일부 다른 <math alttext="y^{*}" class="ltx_Math" display="inline" id="S2.p3.4.m4.1"><semantics id="S2.p3.4.m4.1a"><msup id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml"><mi id="S2.p3.4.m4.1.1.2" xref="S2.p3.4.m4.1.1.2.cmml">y</mi><mo id="S2.p3.4.m4.1.1.3" xref="S2.p3.4.m4.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><apply id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p3.4.m4.1.1.1.cmml" xref="S2.p3.4.m4.1.1">superscript</csymbol><ci id="S2.p3.4.m4.1.1.2.cmml" xref="S2.p3.4.m4.1.1.2">𝑦</ci><times id="S2.p3.4.m4.1.1.3.cmml" xref="S2.p3.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">y^{*}</annotation></semantics></math>이다. 이 경우, 알고리즘의 <em class="ltx_emph ltx_font_italic" id="S2.p3.7.4">forward error</em>은 <math alttext="\Delta y=y^{*}-y" class="ltx_Math" display="inline" id="S2.p3.5.m5.1"><semantics id="S2.p3.5.m5.1a"><mrow id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml"><mrow id="S2.p3.5.m5.1.1.2" xref="S2.p3.5.m5.1.1.2.cmml"><mi id="S2.p3.5.m5.1.1.2.2" mathvariant="normal" xref="S2.p3.5.m5.1.1.2.2.cmml">Δ</mi><mo id="S2.p3.5.m5.1.1.2.1" lspace="0em" rspace="0em" xref="S2.p3.5.m5.1.1.2.1.cmml">​</mo><mi id="S2.p3.5.m5.1.1.2.3" xref="S2.p3.5.m5.1.1.2.3.cmml">y</mi></mrow><mo id="S2.p3.5.m5.1.1.1" xref="S2.p3.5.m5.1.1.1.cmml">=</mo><mrow id="S2.p3.5.m5.1.1.3" xref="S2.p3.5.m5.1.1.3.cmml"><msup id="S2.p3.5.m5.1.1.3.2" xref="S2.p3.5.m5.1.1.3.2.cmml"><mi id="S2.p3.5.m5.1.1.3.2.2" xref="S2.p3.5.m5.1.1.3.2.2.cmml">y</mi><mo id="S2.p3.5.m5.1.1.3.2.3" xref="S2.p3.5.m5.1.1.3.2.3.cmml">∗</mo></msup><mo id="S2.p3.5.m5.1.1.3.1" xref="S2.p3.5.m5.1.1.3.1.cmml">−</mo><mi id="S2.p3.5.m5.1.1.3.3" xref="S2.p3.5.m5.1.1.3.3.cmml">y</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><apply id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1"><eq id="S2.p3.5.m5.1.1.1.cmml" xref="S2.p3.5.m5.1.1.1"></eq><apply id="S2.p3.5.m5.1.1.2.cmml" xref="S2.p3.5.m5.1.1.2"><times id="S2.p3.5.m5.1.1.2.1.cmml" xref="S2.p3.5.m5.1.1.2.1"></times><ci id="S2.p3.5.m5.1.1.2.2.cmml" xref="S2.p3.5.m5.1.1.2.2">Δ</ci><ci id="S2.p3.5.m5.1.1.2.3.cmml" xref="S2.p3.5.m5.1.1.2.3">𝑦</ci></apply><apply id="S2.p3.5.m5.1.1.3.cmml" xref="S2.p3.5.m5.1.1.3"><minus id="S2.p3.5.m5.1.1.3.1.cmml" xref="S2.p3.5.m5.1.1.3.1"></minus><apply id="S2.p3.5.m5.1.1.3.2.cmml" xref="S2.p3.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="S2.p3.5.m5.1.1.3.2.1.cmml" xref="S2.p3.5.m5.1.1.3.2">superscript</csymbol><ci id="S2.p3.5.m5.1.1.3.2.2.cmml" xref="S2.p3.5.m5.1.1.3.2.2">𝑦</ci><times id="S2.p3.5.m5.1.1.3.2.3.cmml" xref="S2.p3.5.m5.1.1.3.2.3"></times></apply><ci id="S2.p3.5.m5.1.1.3.3.cmml" xref="S2.p3.5.m5.1.1.3.3">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">\Delta y=y^{*}-y</annotation></semantics></math>이고, 알고리즘의 <em class="ltx_emph ltx_font_italic" id="S2.p3.7.5">backward error</em>은 <math alttext="f(x+\Delta x)=y^{*}" class="ltx_Math" display="inline" id="S2.p3.7.m7.1"><semantics id="S2.p3.7.m7.1a"><mrow id="S2.p3.7.m7.1.1" xref="S2.p3.7.m7.1.1.cmml"><mrow id="S2.p3.7.m7.1.1.1" xref="S2.p3.7.m7.1.1.1.cmml"><mi id="S2.p3.7.m7.1.1.1.3" xref="S2.p3.7.m7.1.1.1.3.cmml">f</mi><mo id="S2.p3.7.m7.1.1.1.2" lspace="0em" rspace="0em" xref="S2.p3.7.m7.1.1.1.2.cmml">​</mo><mrow id="S2.p3.7.m7.1.1.1.1.1" xref="S2.p3.7.m7.1.1.1.1.1.1.cmml"><mo id="S2.p3.7.m7.1.1.1.1.1.2" stretchy="false" xref="S2.p3.7.m7.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.p3.7.m7.1.1.1.1.1.1" xref="S2.p3.7.m7.1.1.1.1.1.1.cmml"><mi id="S2.p3.7.m7.1.1.1.1.1.1.2" xref="S2.p3.7.m7.1.1.1.1.1.1.2.cmml">x</mi><mo id="S2.p3.7.m7.1.1.1.1.1.1.1" xref="S2.p3.7.m7.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.p3.7.m7.1.1.1.1.1.1.3" xref="S2.p3.7.m7.1.1.1.1.1.1.3.cmml"><mi id="S2.p3.7.m7.1.1.1.1.1.1.3.2" mathvariant="normal" xref="S2.p3.7.m7.1.1.1.1.1.1.3.2.cmml">Δ</mi><mo id="S2.p3.7.m7.1.1.1.1.1.1.3.1" lspace="0em" rspace="0em" xref="S2.p3.7.m7.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.p3.7.m7.1.1.1.1.1.1.3.3" xref="S2.p3.7.m7.1.1.1.1.1.1.3.3.cmml">x</mi></mrow></mrow><mo id="S2.p3.7.m7.1.1.1.1.1.3" stretchy="false" xref="S2.p3.7.m7.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.p3.7.m7.1.1.2" xref="S2.p3.7.m7.1.1.2.cmml">=</mo><msup id="S2.p3.7.m7.1.1.3" xref="S2.p3.7.m7.1.1.3.cmml"><mi id="S2.p3.7.m7.1.1.3.2" xref="S2.p3.7.m7.1.1.3.2.cmml">y</mi><mo id="S2.p3.7.m7.1.1.3.3" xref="S2.p3.7.m7.1.1.3.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.7.m7.1b"><apply id="S2.p3.7.m7.1.1.cmml" xref="S2.p3.7.m7.1.1"><eq id="S2.p3.7.m7.1.1.2.cmml" xref="S2.p3.7.m7.1.1.2"></eq><apply id="S2.p3.7.m7.1.1.1.cmml" xref="S2.p3.7.m7.1.1.1"><times id="S2.p3.7.m7.1.1.1.2.cmml" xref="S2.p3.7.m7.1.1.1.2"></times><ci id="S2.p3.7.m7.1.1.1.3.cmml" xref="S2.p3.7.m7.1.1.1.3">𝑓</ci><apply id="S2.p3.7.m7.1.1.1.1.1.1.cmml" xref="S2.p3.7.m7.1.1.1.1.1"><plus id="S2.p3.7.m7.1.1.1.1.1.1.1.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.1"></plus><ci id="S2.p3.7.m7.1.1.1.1.1.1.2.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.2">𝑥</ci><apply id="S2.p3.7.m7.1.1.1.1.1.1.3.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.3"><times id="S2.p3.7.m7.1.1.1.1.1.1.3.1.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.3.1"></times><ci id="S2.p3.7.m7.1.1.1.1.1.1.3.2.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.3.2">Δ</ci><ci id="S2.p3.7.m7.1.1.1.1.1.1.3.3.cmml" xref="S2.p3.7.m7.1.1.1.1.1.1.3.3">𝑥</ci></apply></apply></apply><apply id="S2.p3.7.m7.1.1.3.cmml" xref="S2.p3.7.m7.1.1.3"><csymbol cd="ambiguous" id="S2.p3.7.m7.1.1.3.1.cmml" xref="S2.p3.7.m7.1.1.3">superscript</csymbol><ci id="S2.p3.7.m7.1.1.3.2.cmml" xref="S2.p3.7.m7.1.1.3.2">𝑦</ci><times id="S2.p3.7.m7.1.1.3.3.cmml" xref="S2.p3.7.m7.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m7.1c">f(x+\Delta x)=y^{*}</annotation></semantics></math>와 같이 <math alttext="\Delta x" class="ltx_Math" display="inline" id="S2.p3.6.m6.1"><semantics id="S2.p3.6.m6.1a"><mrow id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml"><mi id="S2.p3.6.m6.1.1.2" mathvariant="normal" xref="S2.p3.6.m6.1.1.2.cmml">Δ</mi><mo id="S2.p3.6.m6.1.1.1" lspace="0em" rspace="0em" xref="S2.p3.6.m6.1.1.1.cmml">​</mo><mi id="S2.p3.6.m6.1.1.3" xref="S2.p3.6.m6.1.1.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><apply id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1"><times id="S2.p3.6.m6.1.1.1.cmml" xref="S2.p3.6.m6.1.1.1"></times><ci id="S2.p3.6.m6.1.1.2.cmml" xref="S2.p3.6.m6.1.1.2">Δ</ci><ci id="S2.p3.6.m6.1.1.3.cmml" xref="S2.p3.6.m6.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">\Delta x</annotation></semantics></math>가 가장 작다. 따라서 순방향 오류는 정확하거나 진정한 답과 알고리즘에 의해 출력된 것의 차이를 알려주고, 역방향 오류는 우리가 실행한 알고리즘이 실제로 정확히 푼 입력 데이터를 알려준다. 알고리즘에 대한 순방향 오차와 역방향 오차는 문제의 조건 수에 따라 관련이 있다. 이러한 문제에 대한 자세한 논의는 관심 있는 독자를 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib237" title="">237</a>]</cite>에 참조한다.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Quantization in Neural Nets</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p1.1">의심할 여지 없이 수천 개의 논문이 이러한 주제에 대해 작성되었으며, NN 양자화에 대한 최근 작업이 이전 작업과 어떻게 다른지 궁금할 수 있습니다. 확실히, 최근에 제안된 많은 "새로운 알고리즘"은 문헌에서 과거 작업과 강한 연결(그리고 어떤 경우에는 본질적으로 재발견)을 가지고 있다. 그러나, NN은 양자화의 문제에 독특한 도전과 기회를 가져온다. 첫째, 신경망의 추론과 훈련은 모두 계산 집약적이다. 따라서 수치의 효율적인 표현이 특히 중요하다. 둘째, 현재 대부분의 신경망 모델은 과도하게 매개변수화되어 있으므로 정확도에 영향을 미치지 않으면서 비트 정밀도를 줄일 수 있는 충분한 기회가 있다. 그러나, 한 가지 매우 중요한 차이점은 NN이 공격적인 양자화 및 극단적인 이산화에 매우 견고하다는 것이다. 여기서의 새로운 자유도는 관련된 매개변수의 수, 즉 과도하게 매개변수화된 모델로 작업하는 것과 관련이 있다. 이는 우리가 잘 정리된 문제를 해결하고 있는지, 순방향 오류에 관심이 있는지, 역방향 오류에 관심이 있는지 등에 직접적인 시사점을 준다. 양자화의 최근 발전을 추동하는 NN 응용들에서, 해결되고 있는 단일의 잘-포지셔닝되거나 잘-조건화된 문제는 없다. 대신에, 어떤 종류의 순방향 에러 메트릭(분류 품질, 복잡성 등에 기초함)에 관심이 있지만, 오버-파라미터화로 인해, 이 메트릭을 정확히 또는 근사적으로 최적화하는 매우 상이한 모델들이 많이 존재한다. 따라서, 양자화된 모델과 원래의 비양자화된 모델 사이의 높은 오차/거리를 가질 수 있지만, 여전히 매우 양호한 일반화 성능을 달성할 수 있다. 이 추가된 자유도는 신호를 너무 많이 변화시키지 않는 압축 방법을 찾는 데 주로 초점을 맞춘 많은 고전적 연구나 "정확한" 대 "이산화된" 계산 간의 차이에 대한 강력한 제어가 있는 수치적 방법에는 존재하지 않았다. 이 관찰은 NN 양자화를 위한 <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.1">novel</em> 기술을 연구하는 주요 드라이버였다. 마지막으로, 신경망 모델의 계층 구조는 탐색할 수 있는 추가 차원을 제공한다. 신경망의 다른 레이어는 손실 함수에 다른 영향을 미치며, 이는 양자화에 대한 혼합 정밀도 접근법의 동기를 부여한다.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="https://ar5iv.labs.arxiv.org/html/2103.13630/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="111" height="100" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="https://ar5iv.labs.arxiv.org/html/2103.13630/assets/x2.png" id="S2.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="111" height="100" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 1:</span>균일 양자화(왼쪽)와 비균일 양자화(오른쪽)의 비교.</figcaption>
Real values in the continuous domain <math id="S2.F1.3.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S2.F1.3.m1.1b"><mi id="S2.F1.3.m1.1.1" xref="S2.F1.3.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.F1.3.m1.1c"><ci id="S2.F1.3.m1.1.1.cmml" xref="S2.F1.3.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.3.m1.1d">r</annotation></semantics></math> are mapped into discrete, lower precision values in the quantized domain <math id="S2.F1.4.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.F1.4.m2.1b"><mi id="S2.F1.4.m2.1.1" xref="S2.F1.4.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.F1.4.m2.1c"><ci id="S2.F1.4.m2.1.1.cmml" xref="S2.F1.4.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.4.m2.1d">Q</annotation></semantics></math>, which are marked with the orange bullets.
Note that the distances between the quantized values (quantization levels) are the same in uniform quantization, whereas they can vary in non-uniform quantization.
</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Basic Concepts of Quantization</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">이 섹션에서는 먼저 섹션 <a class="ltx_ref" href="#S3.SS1" title="III-A Problem Setup and Notations ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>에서 공통 표기와 문제 설정을 간략하게 소개하고, 섹션 <a class="ltx_ref" href="#S3.SS2" title="III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>-<a class="ltx_ref" href="#S3.SS6" title="III-F Non-Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-F</span></span></a>에서 기본 양자화 개념과 방법을 설명한다. 이후 섹션 <a class="ltx_ref" href="#S3.SS7" title="III-G Fine-tuning Methods ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-G</span></span></a>에서 서로 다른 미세 조정 방법에 대해 논의한 후 섹션 <a class="ltx_ref" href="#S3.SS8" title="III-H Stochastic Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-H</span></span></a>에서 확률적 양자화를 수행한다.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Problem Setup and Notations</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p1.3">NN이 학습 가능한 파라미터를 갖는 <math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">L</annotation></semantics></math> 레이어를 갖는다고 가정하면, <math alttext="\{W_{1},W_{2},...,W_{L}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.4"><semantics id="S3.SS1.p1.2.m2.4a"><mrow id="S3.SS1.p1.2.m2.4.4.3" xref="S3.SS1.p1.2.m2.4.4.4.cmml"><mo id="S3.SS1.p1.2.m2.4.4.3.4" stretchy="false" xref="S3.SS1.p1.2.m2.4.4.4.cmml">{</mo><msub id="S3.SS1.p1.2.m2.2.2.1.1" xref="S3.SS1.p1.2.m2.2.2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.2.2.1.1.2" xref="S3.SS1.p1.2.m2.2.2.1.1.2.cmml">W</mi><mn id="S3.SS1.p1.2.m2.2.2.1.1.3" xref="S3.SS1.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.2.m2.4.4.3.5" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.2.m2.3.3.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.cmml"><mi id="S3.SS1.p1.2.m2.3.3.2.2.2" xref="S3.SS1.p1.2.m2.3.3.2.2.2.cmml">W</mi><mn id="S3.SS1.p1.2.m2.3.3.2.2.3" xref="S3.SS1.p1.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.2.m2.4.4.3.6" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><mi id="S3.SS1.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS1.p1.2.m2.1.1.cmml">…</mi><mo id="S3.SS1.p1.2.m2.4.4.3.7" xref="S3.SS1.p1.2.m2.4.4.4.cmml">,</mo><msub id="S3.SS1.p1.2.m2.4.4.3.3" xref="S3.SS1.p1.2.m2.4.4.3.3.cmml"><mi id="S3.SS1.p1.2.m2.4.4.3.3.2" xref="S3.SS1.p1.2.m2.4.4.3.3.2.cmml">W</mi><mi id="S3.SS1.p1.2.m2.4.4.3.3.3" xref="S3.SS1.p1.2.m2.4.4.3.3.3.cmml">L</mi></msub><mo id="S3.SS1.p1.2.m2.4.4.3.8" stretchy="false" xref="S3.SS1.p1.2.m2.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.4b"><set id="S3.SS1.p1.2.m2.4.4.4.cmml" xref="S3.SS1.p1.2.m2.4.4.3"><apply id="S3.SS1.p1.2.m2.2.2.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.2.2.1.1.2">𝑊</ci><cn id="S3.SS1.p1.2.m2.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p1.2.m2.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.p1.2.m2.3.3.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.3.3.2.2.1.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.p1.2.m2.3.3.2.2.2">𝑊</ci><cn id="S3.SS1.p1.2.m2.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.p1.2.m2.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">…</ci><apply id="S3.SS1.p1.2.m2.4.4.3.3.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.4.4.3.3.1.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3">subscript</csymbol><ci id="S3.SS1.p1.2.m2.4.4.3.3.2.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3.2">𝑊</ci><ci id="S3.SS1.p1.2.m2.4.4.3.3.3.cmml" xref="S3.SS1.p1.2.m2.4.4.3.3.3">𝐿</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.4c">\{W_{1},W_{2},...,W_{L}\}</annotation></semantics></math>로 표시되며, <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\theta</annotation></semantics></math>는 이러한 모든 파라미터의 조합을 나타낸다. 일반성의 손실 없이, 우리는 명목상의 목표가 다음과 같은 경험적 위험 최소화 함수를 최적화하는 것인 지도 학습 문제에 초점을 맞춘다:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\small\mathcal{L}(\theta)=\frac{1}{N}\sum_{i=1}^{N}l(x_{i},y_{i};\theta)," display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.4" xref="S3.E1.m1.3.3.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E1.m1.3.3.1.1.4.2" xref="S3.E1.m1.3.3.1.1.4.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.4.1" xref="S3.E1.m1.3.3.1.1.4.1.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.4.3.2" xref="S3.E1.m1.3.3.1.1.4.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.4.3.2.1" xref="S3.E1.m1.3.3.1.1.4.cmml">(</mo><mi mathsize="90%" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">θ</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.4.3.2.2" xref="S3.E1.m1.3.3.1.1.4.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml"><mfrac id="S3.E1.m1.3.3.1.1.2.4" xref="S3.E1.m1.3.3.1.1.2.4.cmml"><mn mathsize="90%" id="S3.E1.m1.3.3.1.1.2.4.2" xref="S3.E1.m1.3.3.1.1.2.4.2.cmml">1</mn><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.4.3" xref="S3.E1.m1.3.3.1.1.2.4.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.2.3" xref="S3.E1.m1.3.3.1.1.2.3.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.2.cmml"><munderover id="S3.E1.m1.3.3.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" stretchy="true" id="S3.E1.m1.3.3.1.1.2.2.3.2.2" xref="S3.E1.m1.3.3.1.1.2.2.3.2.2.cmml">∑</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.3.2.3" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.3.2.3.2" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.2.cmml">i</mi><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.3.2.3.1" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.1.cmml">=</mo><mn mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.3.2.3.3" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.3.3" xref="S3.E1.m1.3.3.1.1.2.2.3.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.3.3.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.4" xref="S3.E1.m1.3.3.1.1.2.2.2.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.2.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml">(</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.4" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml">,</mo><msub id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.2.cmml">y</mi><mi mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.5" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml">;</mo><mi mathsize="90%" id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">θ</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.6" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo mathsize="90%" id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"></eq><apply id="S3.E1.m1.3.3.1.1.4.cmml" xref="S3.E1.m1.3.3.1.1.4"><times id="S3.E1.m1.3.3.1.1.4.1.cmml" xref="S3.E1.m1.3.3.1.1.4.1"></times><ci id="S3.E1.m1.3.3.1.1.4.2.cmml" xref="S3.E1.m1.3.3.1.1.4.2">ℒ</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝜃</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"><times id="S3.E1.m1.3.3.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.3"></times><apply id="S3.E1.m1.3.3.1.1.2.4.cmml" xref="S3.E1.m1.3.3.1.1.2.4"><divide id="S3.E1.m1.3.3.1.1.2.4.1.cmml" xref="S3.E1.m1.3.3.1.1.2.4"></divide><cn type="integer" id="S3.E1.m1.3.3.1.1.2.4.2.cmml" xref="S3.E1.m1.3.3.1.1.2.4.2">1</cn><ci id="S3.E1.m1.3.3.1.1.2.4.3.cmml" xref="S3.E1.m1.3.3.1.1.2.4.3">𝑁</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2"><apply id="S3.E1.m1.3.3.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3">superscript</csymbol><apply id="S3.E1.m1.3.3.1.1.2.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3">subscript</csymbol><sum id="S3.E1.m1.3.3.1.1.2.2.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2.2"></sum><apply id="S3.E1.m1.3.3.1.1.2.2.3.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3"><eq id="S3.E1.m1.3.3.1.1.2.2.3.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.1"></eq><ci id="S3.E1.m1.3.3.1.1.2.2.3.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.2.2.3.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.3.3.1.1.2.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.3.3">𝑁</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2"><times id="S3.E1.m1.3.3.1.1.2.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.3"></times><ci id="S3.E1.m1.3.3.1.1.2.2.2.4.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.4">𝑙</ci><vector id="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2"><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.2">𝑦</ci><ci id="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.2.3">𝑖</ci></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝜃</ci></vector></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\small\mathcal{L}(\theta)=\frac{1}{N}\sum_{i=1}^{N}l(x_{i},y_{i};\theta),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.12">여기서, <math alttext="(x,y)" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m1.2"><semantics id="S3.SS1.p1.4.m1.2a"><mrow id="S3.SS1.p1.4.m1.2.3.2" xref="S3.SS1.p1.4.m1.2.3.1.cmml"><mo id="S3.SS1.p1.4.m1.2.3.2.1" stretchy="false" xref="S3.SS1.p1.4.m1.2.3.1.cmml">(</mo><mi id="S3.SS1.p1.4.m1.1.1" xref="S3.SS1.p1.4.m1.1.1.cmml">x</mi><mo id="S3.SS1.p1.4.m1.2.3.2.2" xref="S3.SS1.p1.4.m1.2.3.1.cmml">,</mo><mi id="S3.SS1.p1.4.m1.2.2" xref="S3.SS1.p1.4.m1.2.2.cmml">y</mi><mo id="S3.SS1.p1.4.m1.2.3.2.3" stretchy="false" xref="S3.SS1.p1.4.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m1.2b"><interval closure="open" id="S3.SS1.p1.4.m1.2.3.1.cmml" xref="S3.SS1.p1.4.m1.2.3.2"><ci id="S3.SS1.p1.4.m1.1.1.cmml" xref="S3.SS1.p1.4.m1.1.1">𝑥</ci><ci id="S3.SS1.p1.4.m1.2.2.cmml" xref="S3.SS1.p1.4.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m1.2c">(x,y)</annotation></semantics></math>는 입력 데이터 및 해당 레이블이고, <math alttext="l(x,y;\theta)" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m2.3"><semantics id="S3.SS1.p1.5.m2.3a"><mrow id="S3.SS1.p1.5.m2.3.4" xref="S3.SS1.p1.5.m2.3.4.cmml"><mi id="S3.SS1.p1.5.m2.3.4.2" xref="S3.SS1.p1.5.m2.3.4.2.cmml">l</mi><mo id="S3.SS1.p1.5.m2.3.4.1" lspace="0em" rspace="0em" xref="S3.SS1.p1.5.m2.3.4.1.cmml">​</mo><mrow id="S3.SS1.p1.5.m2.3.4.3.2" xref="S3.SS1.p1.5.m2.3.4.3.1.cmml"><mo id="S3.SS1.p1.5.m2.3.4.3.2.1" stretchy="false" xref="S3.SS1.p1.5.m2.3.4.3.1.cmml">(</mo><mi id="S3.SS1.p1.5.m2.1.1" xref="S3.SS1.p1.5.m2.1.1.cmml">x</mi><mo id="S3.SS1.p1.5.m2.3.4.3.2.2" xref="S3.SS1.p1.5.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS1.p1.5.m2.2.2" xref="S3.SS1.p1.5.m2.2.2.cmml">y</mi><mo id="S3.SS1.p1.5.m2.3.4.3.2.3" xref="S3.SS1.p1.5.m2.3.4.3.1.cmml">;</mo><mi id="S3.SS1.p1.5.m2.3.3" xref="S3.SS1.p1.5.m2.3.3.cmml">θ</mi><mo id="S3.SS1.p1.5.m2.3.4.3.2.4" stretchy="false" xref="S3.SS1.p1.5.m2.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m2.3b"><apply id="S3.SS1.p1.5.m2.3.4.cmml" xref="S3.SS1.p1.5.m2.3.4"><times id="S3.SS1.p1.5.m2.3.4.1.cmml" xref="S3.SS1.p1.5.m2.3.4.1"></times><ci id="S3.SS1.p1.5.m2.3.4.2.cmml" xref="S3.SS1.p1.5.m2.3.4.2">𝑙</ci><vector id="S3.SS1.p1.5.m2.3.4.3.1.cmml" xref="S3.SS1.p1.5.m2.3.4.3.2"><ci id="S3.SS1.p1.5.m2.1.1.cmml" xref="S3.SS1.p1.5.m2.1.1">𝑥</ci><ci id="S3.SS1.p1.5.m2.2.2.cmml" xref="S3.SS1.p1.5.m2.2.2">𝑦</ci><ci id="S3.SS1.p1.5.m2.3.3.cmml" xref="S3.SS1.p1.5.m2.3.3">𝜃</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m2.3c">l(x,y;\theta)</annotation></semantics></math>는 손실 함수(예를 들어, Mean Squared Error 또는 Cross Entropy loss), <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m3.1"><semantics id="S3.SS1.p1.6.m3.1a"><mi id="S3.SS1.p1.6.m3.1.1" xref="S3.SS1.p1.6.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m3.1b"><ci id="S3.SS1.p1.6.m3.1.1.cmml" xref="S3.SS1.p1.6.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m3.1c">N</annotation></semantics></math>는 데이터 포인트의 총 개수이다. 또한, <math alttext="i^{th}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m4.1"><semantics id="S3.SS1.p1.7.m4.1a"><msup id="S3.SS1.p1.7.m4.1.1" xref="S3.SS1.p1.7.m4.1.1.cmml"><mi id="S3.SS1.p1.7.m4.1.1.2" xref="S3.SS1.p1.7.m4.1.1.2.cmml">i</mi><mrow id="S3.SS1.p1.7.m4.1.1.3" xref="S3.SS1.p1.7.m4.1.1.3.cmml"><mi id="S3.SS1.p1.7.m4.1.1.3.2" xref="S3.SS1.p1.7.m4.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p1.7.m4.1.1.3.1" lspace="0em" rspace="0em" xref="S3.SS1.p1.7.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS1.p1.7.m4.1.1.3.3" xref="S3.SS1.p1.7.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m4.1b"><apply id="S3.SS1.p1.7.m4.1.1.cmml" xref="S3.SS1.p1.7.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m4.1.1.1.cmml" xref="S3.SS1.p1.7.m4.1.1">superscript</csymbol><ci id="S3.SS1.p1.7.m4.1.1.2.cmml" xref="S3.SS1.p1.7.m4.1.1.2">𝑖</ci><apply id="S3.SS1.p1.7.m4.1.1.3.cmml" xref="S3.SS1.p1.7.m4.1.1.3"><times id="S3.SS1.p1.7.m4.1.1.3.1.cmml" xref="S3.SS1.p1.7.m4.1.1.3.1"></times><ci id="S3.SS1.p1.7.m4.1.1.3.2.cmml" xref="S3.SS1.p1.7.m4.1.1.3.2">𝑡</ci><ci id="S3.SS1.p1.7.m4.1.1.3.3.cmml" xref="S3.SS1.p1.7.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m4.1c">i^{th}</annotation></semantics></math> 계층의 입력 숨김 활성화를 <math alttext="h_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m5.1"><semantics id="S3.SS1.p1.8.m5.1a"><msub id="S3.SS1.p1.8.m5.1.1" xref="S3.SS1.p1.8.m5.1.1.cmml"><mi id="S3.SS1.p1.8.m5.1.1.2" xref="S3.SS1.p1.8.m5.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.8.m5.1.1.3" xref="S3.SS1.p1.8.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m5.1b"><apply id="S3.SS1.p1.8.m5.1.1.cmml" xref="S3.SS1.p1.8.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m5.1.1.1.cmml" xref="S3.SS1.p1.8.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m5.1.1.2.cmml" xref="S3.SS1.p1.8.m5.1.1.2">ℎ</ci><ci id="S3.SS1.p1.8.m5.1.1.3.cmml" xref="S3.SS1.p1.8.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m5.1c">h_{i}</annotation></semantics></math>로 나타내고, 해당 출력 숨김 활성화를 <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m6.1"><semantics id="S3.SS1.p1.9.m6.1a"><msub id="S3.SS1.p1.9.m6.1.1" xref="S3.SS1.p1.9.m6.1.1.cmml"><mi id="S3.SS1.p1.9.m6.1.1.2" xref="S3.SS1.p1.9.m6.1.1.2.cmml">a</mi><mi id="S3.SS1.p1.9.m6.1.1.3" xref="S3.SS1.p1.9.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m6.1b"><apply id="S3.SS1.p1.9.m6.1.1.cmml" xref="S3.SS1.p1.9.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m6.1.1.1.cmml" xref="S3.SS1.p1.9.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.9.m6.1.1.2.cmml" xref="S3.SS1.p1.9.m6.1.1.2">𝑎</ci><ci id="S3.SS1.p1.9.m6.1.1.3.cmml" xref="S3.SS1.p1.9.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m6.1c">a_{i}</annotation></semantics></math>로 나타낸다. 우리는 부동 소수점 정밀도로 저장된 훈련된 모델 파라미터 <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m7.1"><semantics id="S3.SS1.p1.10.m7.1a"><mi id="S3.SS1.p1.10.m7.1.1" xref="S3.SS1.p1.10.m7.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m7.1b"><ci id="S3.SS1.p1.10.m7.1.1.cmml" xref="S3.SS1.p1.10.m7.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m7.1c">\theta</annotation></semantics></math>가 있다고 가정한다. 양자화에서, 목표는 매개 변수(<math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m8.1"><semantics id="S3.SS1.p1.11.m8.1a"><mi id="S3.SS1.p1.11.m8.1.1" xref="S3.SS1.p1.11.m8.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m8.1b"><ci id="S3.SS1.p1.11.m8.1.1.cmml" xref="S3.SS1.p1.11.m8.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m8.1c">\theta</annotation></semantics></math>) 뿐만 아니라 중간 활성화 맵(즉, <math alttext="h_{i},\ a_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.12.m9.2"><semantics id="S3.SS1.p1.12.m9.2a"><mrow id="S3.SS1.p1.12.m9.2.2.2" xref="S3.SS1.p1.12.m9.2.2.3.cmml"><msub id="S3.SS1.p1.12.m9.1.1.1.1" xref="S3.SS1.p1.12.m9.1.1.1.1.cmml"><mi id="S3.SS1.p1.12.m9.1.1.1.1.2" xref="S3.SS1.p1.12.m9.1.1.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.12.m9.1.1.1.1.3" xref="S3.SS1.p1.12.m9.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p1.12.m9.2.2.2.3" rspace="0.667em" xref="S3.SS1.p1.12.m9.2.2.3.cmml">,</mo><msub id="S3.SS1.p1.12.m9.2.2.2.2" xref="S3.SS1.p1.12.m9.2.2.2.2.cmml"><mi id="S3.SS1.p1.12.m9.2.2.2.2.2" xref="S3.SS1.p1.12.m9.2.2.2.2.2.cmml">a</mi><mi id="S3.SS1.p1.12.m9.2.2.2.2.3" xref="S3.SS1.p1.12.m9.2.2.2.2.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m9.2b"><list id="S3.SS1.p1.12.m9.2.2.3.cmml" xref="S3.SS1.p1.12.m9.2.2.2"><apply id="S3.SS1.p1.12.m9.1.1.1.1.cmml" xref="S3.SS1.p1.12.m9.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.12.m9.1.1.1.1.1.cmml" xref="S3.SS1.p1.12.m9.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.12.m9.1.1.1.1.2.cmml" xref="S3.SS1.p1.12.m9.1.1.1.1.2">ℎ</ci><ci id="S3.SS1.p1.12.m9.1.1.1.1.3.cmml" xref="S3.SS1.p1.12.m9.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p1.12.m9.2.2.2.2.cmml" xref="S3.SS1.p1.12.m9.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.12.m9.2.2.2.2.1.cmml" xref="S3.SS1.p1.12.m9.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.12.m9.2.2.2.2.2.cmml" xref="S3.SS1.p1.12.m9.2.2.2.2.2">𝑎</ci><ci id="S3.SS1.p1.12.m9.2.2.2.2.3.cmml" xref="S3.SS1.p1.12.m9.2.2.2.2.3">𝑖</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m9.2c">h_{i},\ a_{i}</annotation></semantics></math>)의 정밀도를 낮은 정밀도로 감소시키는 것이며, 모델의 일반화 파워/정확도에 대한 영향을 최소화한다. 이를 위해서는 부동 소수점 값을 양자화된 값에 매핑하는 양자화 연산자를 정의해야 하는데, 다음에 설명한다.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2103.13630/assets/x3.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="108" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 2:</span></figcaption>
Illustration of symmetric quantization and asymmetric quantization.
Symmetric quantization with restricted range maps real values to [-127, 127], and full range
maps to [-128, 127] for 8-bit quantization.
</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Uniform Quantization</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p1.11">우리는 먼저 유한한 값 집합에 대한 NN 가중치와 활성화를 양자화할 수 있는 함수를 정의해야 한다. 이 함수는 부동 소수점에서 실수 값을 취하며 그림 <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ II-A Quantization in Neural Nets ‣ II General History of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">1</span></a>와 같이 더 낮은 정밀도 범위로 매핑한다. 양자화 함수에 대한 인기 있는 선택은 다음과 같다:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\small Q(r)=\text{Int}\big{(}{r}/{S}\big{)}-Z," display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><mi mathsize="90%" id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.1" xref="S3.E2.m1.2.2.1.1.3.1.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.3.cmml"><mo maxsize="90%" minsize="90%" id="S3.E2.m1.2.2.1.1.3.3.2.1" xref="S3.E2.m1.2.2.1.1.3.cmml">(</mo><mi mathsize="90%" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">r</mi><mo maxsize="90%" minsize="90%" id="S3.E2.m1.2.2.1.1.3.3.2.2" xref="S3.E2.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.cmml"><mtext mathsize="90%" id="S3.E2.m1.2.2.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.3a.cmml">Int</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mo maxsize="120%" minsize="120%" id="S3.E2.m1.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">r</mi><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml">/</mo><mi mathsize="90%" id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">S</mi></mrow><mo maxsize="120%" minsize="120%" id="S3.E2.m1.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.2.cmml">−</mo><mi mathsize="90%" id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml">Z</mi></mrow></mrow><mo mathsize="90%" id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1"><eq id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></eq><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><times id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1"></times><ci id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2">𝑄</ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑟</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><minus id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2"></minus><apply id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2"></times><ci id="S3.E2.m1.2.2.1.1.1.1.3a.cmml" xref="S3.E2.m1.2.2.1.1.1.1.3"><mtext mathsize="90%" id="S3.E2.m1.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.3">Int</mtext></ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"><divide id="S3.E2.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.1"></divide><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.2">𝑟</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1.1.3">𝑆</ci></apply></apply><ci id="S3.E2.m1.2.2.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.3">𝑍</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\small Q(r)=\text{Int}\big{(}{r}/{S}\big{)}-Z,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.8">여기서, <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">Q</annotation></semantics></math>는 양자화 연산자, <math alttext="r" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">r</annotation></semantics></math>는 실수값 입력(활성화 또는 가중치), <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">S</annotation></semantics></math>는 실수값 스케일링 팩터, <math alttext="Z" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">Z</annotation></semantics></math>는 정수 제로 포인트이다. 또한 <span class="ltx_text ltx_markedasmath" id="S3.SS2.p1.8.1">Int</span> 함수는 반올림 연산(예: 가장 가까운 것으로 반올림 및 잘림)을 통해 실수 값을 정수 값으로 매핑합니다. 본질적으로, 이 함수는 실제 값 <math alttext="r" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">r</annotation></semantics></math>에서 일부 정수 값으로의 매핑이다. 이러한 양자화 방법은 결과 양자화 값들(일명 양자화 레벨들)이 균일하게 이격됨에 따라 <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.8.2">uniform quantization</span>으로도 알려져 있다(도<a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ II-A Quantization in Neural Nets ‣ II General History of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">1</span></a>, left). 또한, 양자화된 값이 반드시 균일하게 이격되지 않는 <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.8.3">non-uniform quantization</span> 방법들이 있으며(도<a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ II-A Quantization in Neural Nets ‣ II General History of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">1</span></a>, right), 이러한 방법들은 Section <a class="ltx_ref" href="#S3.SS6" title="III-F Non-Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-F</span></span></a>에서 보다 상세히 논의될 것이다. 흔히 <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.8.4">dequantization</em>로 지칭되는 연산을 통해 양자화된 값 <math alttext="r" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.1"><semantics id="S3.SS2.p1.7.m7.1a"><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">r</annotation></semantics></math>로부터 실제 값 <math alttext="Q(r)" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m8.1"><semantics id="S3.SS2.p1.8.m8.1a"><mrow id="S3.SS2.p1.8.m8.1.2" xref="S3.SS2.p1.8.m8.1.2.cmml"><mi id="S3.SS2.p1.8.m8.1.2.2" xref="S3.SS2.p1.8.m8.1.2.2.cmml">Q</mi><mo id="S3.SS2.p1.8.m8.1.2.1" lspace="0em" rspace="0em" xref="S3.SS2.p1.8.m8.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.8.m8.1.2.3.2" xref="S3.SS2.p1.8.m8.1.2.cmml"><mo id="S3.SS2.p1.8.m8.1.2.3.2.1" stretchy="false" xref="S3.SS2.p1.8.m8.1.2.cmml">(</mo><mi id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">r</mi><mo id="S3.SS2.p1.8.m8.1.2.3.2.2" stretchy="false" xref="S3.SS2.p1.8.m8.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.2.cmml" xref="S3.SS2.p1.8.m8.1.2"><times id="S3.SS2.p1.8.m8.1.2.1.cmml" xref="S3.SS2.p1.8.m8.1.2.1"></times><ci id="S3.SS2.p1.8.m8.1.2.2.cmml" xref="S3.SS2.p1.8.m8.1.2.2">𝑄</ci><ci id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">Q(r)</annotation></semantics></math>를 복구할 수 있다:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="\small\tilde{r}=S(Q(r)+Z)." display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><mover accent="true" id="S3.E3.m1.2.2.1.1.3" xref="S3.E3.m1.2.2.1.1.3.cmml"><mi mathsize="90%" id="S3.E3.m1.2.2.1.1.3.2" xref="S3.E3.m1.2.2.1.1.3.2.cmml">r</mi><mo mathsize="90%" id="S3.E3.m1.2.2.1.1.3.1" xref="S3.E3.m1.2.2.1.1.3.1.cmml">~</mo></mover><mo mathsize="90%" id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mi mathsize="90%" id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.1.2.3.2.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">r</mi><mo maxsize="90%" minsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.1.2.3.2.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml">+</mo><mi mathsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml">Z</mi></mrow><mo maxsize="90%" minsize="90%" id="S3.E3.m1.2.2.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1"><eq id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2"></eq><apply id="S3.E3.m1.2.2.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.3"><ci id="S3.E3.m1.2.2.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.3.1">~</ci><ci id="S3.E3.m1.2.2.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.3.2">𝑟</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"><times id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2"></times><ci id="S3.E3.m1.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3">𝑆</ci><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1"><plus id="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.1"></plus><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2"><times id="S3.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.1"></times><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.2">𝑄</ci><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑟</ci></apply><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3">𝑍</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\small\tilde{r}=S(Q(r)+Z).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.10">복구된 실제 값 <math alttext="\tilde{r}" class="ltx_Math" display="inline" id="S3.SS2.p1.9.m1.1"><semantics id="S3.SS2.p1.9.m1.1a"><mover accent="true" id="S3.SS2.p1.9.m1.1.1" xref="S3.SS2.p1.9.m1.1.1.cmml"><mi id="S3.SS2.p1.9.m1.1.1.2" xref="S3.SS2.p1.9.m1.1.1.2.cmml">r</mi><mo id="S3.SS2.p1.9.m1.1.1.1" xref="S3.SS2.p1.9.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m1.1b"><apply id="S3.SS2.p1.9.m1.1.1.cmml" xref="S3.SS2.p1.9.m1.1.1"><ci id="S3.SS2.p1.9.m1.1.1.1.cmml" xref="S3.SS2.p1.9.m1.1.1.1">~</ci><ci id="S3.SS2.p1.9.m1.1.1.2.cmml" xref="S3.SS2.p1.9.m1.1.1.2">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m1.1c">\tilde{r}</annotation></semantics></math>는 반올림 연산으로 인해 <math alttext="r" class="ltx_Math" display="inline" id="S3.SS2.p1.10.m2.1"><semantics id="S3.SS2.p1.10.m2.1a"><mi id="S3.SS2.p1.10.m2.1.1" xref="S3.SS2.p1.10.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m2.1b"><ci id="S3.SS2.p1.10.m2.1.1.cmml" xref="S3.SS2.p1.10.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m2.1c">r</annotation></semantics></math>와 정확히 일치하지 않을 것이라는 점에 유의한다.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2103.13630/assets/x4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="231" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3:</span>Illustration of different quantization granularities.</figcaption>
In layerwise quantization, the same clipping range is applied to all the filters that belong to the same layer.
This can result in bad quantization resolution for the channels that have narrow distributions (e.g., Filter 1 in the figure).
One can achieve better quantization resolution using channelwise quantization that dedicates different clipping ranges to different channels.
</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Symmetric and Asymmetric Quantization</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p1.2">균일한 양자화의 한 가지 중요한 요소는 Eq에서 스케일링 팩터 <math alttext="S" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">S</annotation></semantics></math>의 선택이다. <a class="ltx_ref" href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">2</span></a>. 이 스케일링 팩터는 본질적으로 주어진 범위의 실수 값 <math alttext="r" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">r</annotation></semantics></math>를 다수의 파티션으로 분할한다(<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib133" title="">133</a>, <a class="ltx_ref" href="#bib.bib113" title="">113</a>]</cite>에서 논의된 바와 같이):</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\small S=\frac{\beta-\alpha}{2^{b}-1}," display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml">S</mi><mo mathsize="90%" id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mrow id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.3.2.2" xref="S3.E4.m1.1.1.1.1.3.2.2.cmml">β</mi><mo mathsize="90%" id="S3.E4.m1.1.1.1.1.3.2.1" xref="S3.E4.m1.1.1.1.1.3.2.1.cmml">−</mo><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.3.2.3" xref="S3.E4.m1.1.1.1.1.3.2.3.cmml">α</mi></mrow><mrow id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml"><msup id="S3.E4.m1.1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.1.3.3.2.cmml"><mn mathsize="90%" id="S3.E4.m1.1.1.1.1.3.3.2.2" xref="S3.E4.m1.1.1.1.1.3.3.2.2.cmml">2</mn><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.3.3.2.3" xref="S3.E4.m1.1.1.1.1.3.3.2.3.cmml">b</mi></msup><mo mathsize="90%" id="S3.E4.m1.1.1.1.1.3.3.1" xref="S3.E4.m1.1.1.1.1.3.3.1.cmml">−</mo><mn mathsize="90%" id="S3.E4.m1.1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.1.3.3.3.cmml">1</mn></mrow></mfrac></mrow><mo mathsize="90%" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"></eq><ci id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2">𝑆</ci><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><divide id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3"></divide><apply id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2"><minus id="S3.E4.m1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.2.1"></minus><ci id="S3.E4.m1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2.2">𝛽</ci><ci id="S3.E4.m1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3">𝛼</ci></apply><apply id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3"><minus id="S3.E4.m1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1"></minus><apply id="S3.E4.m1.1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2">superscript</csymbol><cn type="integer" id="S3.E4.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2.2">2</cn><ci id="S3.E4.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2.3">𝑏</ci></apply><cn type="integer" id="S3.E4.m1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\small S=\frac{\beta-\alpha}{2^{b}-1},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.11" class="ltx_p">where <math id="S3.SS3.p1.3.m1.2" class="ltx_Math" alttext="[\alpha,\beta]" display="inline"><semantics id="S3.SS3.p1.3.m1.2a"><mrow id="S3.SS3.p1.3.m1.2.3.2" xref="S3.SS3.p1.3.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p1.3.m1.2.3.2.1" xref="S3.SS3.p1.3.m1.2.3.1.cmml">[</mo><mi id="S3.SS3.p1.3.m1.1.1" xref="S3.SS3.p1.3.m1.1.1.cmml">α</mi><mo id="S3.SS3.p1.3.m1.2.3.2.2" xref="S3.SS3.p1.3.m1.2.3.1.cmml">,</mo><mi id="S3.SS3.p1.3.m1.2.2" xref="S3.SS3.p1.3.m1.2.2.cmml">β</mi><mo stretchy="false" id="S3.SS3.p1.3.m1.2.3.2.3" xref="S3.SS3.p1.3.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m1.2b"><interval closure="closed" id="S3.SS3.p1.3.m1.2.3.1.cmml" xref="S3.SS3.p1.3.m1.2.3.2"><ci id="S3.SS3.p1.3.m1.1.1.cmml" xref="S3.SS3.p1.3.m1.1.1">𝛼</ci><ci id="S3.SS3.p1.3.m1.2.2.cmml" xref="S3.SS3.p1.3.m1.2.2">𝛽</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m1.2c">[\alpha,\beta]</annotation></semantics></math> denotes the clipping range, a bounded range that we are clipping the real values with, and <math id="S3.SS3.p1.4.m2.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.SS3.p1.4.m2.1a"><mi id="S3.SS3.p1.4.m2.1.1" xref="S3.SS3.p1.4.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m2.1b"><ci id="S3.SS3.p1.4.m2.1.1.cmml" xref="S3.SS3.p1.4.m2.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m2.1c">b</annotation></semantics></math> is the quantization bit width. Therefore, in order for the scaling factor to be defined, the clipping range <math id="S3.SS3.p1.5.m3.2" class="ltx_Math" alttext="[\alpha,\beta]" display="inline"><semantics id="S3.SS3.p1.5.m3.2a"><mrow id="S3.SS3.p1.5.m3.2.3.2" xref="S3.SS3.p1.5.m3.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p1.5.m3.2.3.2.1" xref="S3.SS3.p1.5.m3.2.3.1.cmml">[</mo><mi id="S3.SS3.p1.5.m3.1.1" xref="S3.SS3.p1.5.m3.1.1.cmml">α</mi><mo id="S3.SS3.p1.5.m3.2.3.2.2" xref="S3.SS3.p1.5.m3.2.3.1.cmml">,</mo><mi id="S3.SS3.p1.5.m3.2.2" xref="S3.SS3.p1.5.m3.2.2.cmml">β</mi><mo stretchy="false" id="S3.SS3.p1.5.m3.2.3.2.3" xref="S3.SS3.p1.5.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m3.2b"><interval closure="closed" id="S3.SS3.p1.5.m3.2.3.1.cmml" xref="S3.SS3.p1.5.m3.2.3.2"><ci id="S3.SS3.p1.5.m3.1.1.cmml" xref="S3.SS3.p1.5.m3.1.1">𝛼</ci><ci id="S3.SS3.p1.5.m3.2.2.cmml" xref="S3.SS3.p1.5.m3.2.2">𝛽</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m3.2c">[\alpha,\beta]</annotation></semantics></math> should first be determined. The process of choosing the clipping range is often referred to as <span id="S3.SS3.p1.11.1" class="ltx_text ltx_font_italic">calibration</span>. A straightforward choice is to use the min/max of the signal for the clipping range, i.e., <math id="S3.SS3.p1.6.m4.1" class="ltx_Math" alttext="\alpha=r_{min}" display="inline"><semantics id="S3.SS3.p1.6.m4.1a"><mrow id="S3.SS3.p1.6.m4.1.1" xref="S3.SS3.p1.6.m4.1.1.cmml"><mi id="S3.SS3.p1.6.m4.1.1.2" xref="S3.SS3.p1.6.m4.1.1.2.cmml">α</mi><mo id="S3.SS3.p1.6.m4.1.1.1" xref="S3.SS3.p1.6.m4.1.1.1.cmml">=</mo><msub id="S3.SS3.p1.6.m4.1.1.3" xref="S3.SS3.p1.6.m4.1.1.3.cmml"><mi id="S3.SS3.p1.6.m4.1.1.3.2" xref="S3.SS3.p1.6.m4.1.1.3.2.cmml">r</mi><mrow id="S3.SS3.p1.6.m4.1.1.3.3" xref="S3.SS3.p1.6.m4.1.1.3.3.cmml"><mi id="S3.SS3.p1.6.m4.1.1.3.3.2" xref="S3.SS3.p1.6.m4.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.6.m4.1.1.3.3.1" xref="S3.SS3.p1.6.m4.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.6.m4.1.1.3.3.3" xref="S3.SS3.p1.6.m4.1.1.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.6.m4.1.1.3.3.1a" xref="S3.SS3.p1.6.m4.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.6.m4.1.1.3.3.4" xref="S3.SS3.p1.6.m4.1.1.3.3.4.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m4.1b"><apply id="S3.SS3.p1.6.m4.1.1.cmml" xref="S3.SS3.p1.6.m4.1.1"><eq id="S3.SS3.p1.6.m4.1.1.1.cmml" xref="S3.SS3.p1.6.m4.1.1.1"></eq><ci id="S3.SS3.p1.6.m4.1.1.2.cmml" xref="S3.SS3.p1.6.m4.1.1.2">𝛼</ci><apply id="S3.SS3.p1.6.m4.1.1.3.cmml" xref="S3.SS3.p1.6.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.6.m4.1.1.3.1.cmml" xref="S3.SS3.p1.6.m4.1.1.3">subscript</csymbol><ci id="S3.SS3.p1.6.m4.1.1.3.2.cmml" xref="S3.SS3.p1.6.m4.1.1.3.2">𝑟</ci><apply id="S3.SS3.p1.6.m4.1.1.3.3.cmml" xref="S3.SS3.p1.6.m4.1.1.3.3"><times id="S3.SS3.p1.6.m4.1.1.3.3.1.cmml" xref="S3.SS3.p1.6.m4.1.1.3.3.1"></times><ci id="S3.SS3.p1.6.m4.1.1.3.3.2.cmml" xref="S3.SS3.p1.6.m4.1.1.3.3.2">𝑚</ci><ci id="S3.SS3.p1.6.m4.1.1.3.3.3.cmml" xref="S3.SS3.p1.6.m4.1.1.3.3.3">𝑖</ci><ci id="S3.SS3.p1.6.m4.1.1.3.3.4.cmml" xref="S3.SS3.p1.6.m4.1.1.3.3.4">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m4.1c">\alpha=r_{min}</annotation></semantics></math>, and <math id="S3.SS3.p1.7.m5.1" class="ltx_Math" alttext="\beta=r_{max}" display="inline"><semantics id="S3.SS3.p1.7.m5.1a"><mrow id="S3.SS3.p1.7.m5.1.1" xref="S3.SS3.p1.7.m5.1.1.cmml"><mi id="S3.SS3.p1.7.m5.1.1.2" xref="S3.SS3.p1.7.m5.1.1.2.cmml">β</mi><mo id="S3.SS3.p1.7.m5.1.1.1" xref="S3.SS3.p1.7.m5.1.1.1.cmml">=</mo><msub id="S3.SS3.p1.7.m5.1.1.3" xref="S3.SS3.p1.7.m5.1.1.3.cmml"><mi id="S3.SS3.p1.7.m5.1.1.3.2" xref="S3.SS3.p1.7.m5.1.1.3.2.cmml">r</mi><mrow id="S3.SS3.p1.7.m5.1.1.3.3" xref="S3.SS3.p1.7.m5.1.1.3.3.cmml"><mi id="S3.SS3.p1.7.m5.1.1.3.3.2" xref="S3.SS3.p1.7.m5.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.7.m5.1.1.3.3.1" xref="S3.SS3.p1.7.m5.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.7.m5.1.1.3.3.3" xref="S3.SS3.p1.7.m5.1.1.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.7.m5.1.1.3.3.1a" xref="S3.SS3.p1.7.m5.1.1.3.3.1.cmml">​</mo><mi id="S3.SS3.p1.7.m5.1.1.3.3.4" xref="S3.SS3.p1.7.m5.1.1.3.3.4.cmml">x</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m5.1b"><apply id="S3.SS3.p1.7.m5.1.1.cmml" xref="S3.SS3.p1.7.m5.1.1"><eq id="S3.SS3.p1.7.m5.1.1.1.cmml" xref="S3.SS3.p1.7.m5.1.1.1"></eq><ci id="S3.SS3.p1.7.m5.1.1.2.cmml" xref="S3.SS3.p1.7.m5.1.1.2">𝛽</ci><apply id="S3.SS3.p1.7.m5.1.1.3.cmml" xref="S3.SS3.p1.7.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.7.m5.1.1.3.1.cmml" xref="S3.SS3.p1.7.m5.1.1.3">subscript</csymbol><ci id="S3.SS3.p1.7.m5.1.1.3.2.cmml" xref="S3.SS3.p1.7.m5.1.1.3.2">𝑟</ci><apply id="S3.SS3.p1.7.m5.1.1.3.3.cmml" xref="S3.SS3.p1.7.m5.1.1.3.3"><times id="S3.SS3.p1.7.m5.1.1.3.3.1.cmml" xref="S3.SS3.p1.7.m5.1.1.3.3.1"></times><ci id="S3.SS3.p1.7.m5.1.1.3.3.2.cmml" xref="S3.SS3.p1.7.m5.1.1.3.3.2">𝑚</ci><ci id="S3.SS3.p1.7.m5.1.1.3.3.3.cmml" xref="S3.SS3.p1.7.m5.1.1.3.3.3">𝑎</ci><ci id="S3.SS3.p1.7.m5.1.1.3.3.4.cmml" xref="S3.SS3.p1.7.m5.1.1.3.3.4">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m5.1c">\beta=r_{max}</annotation></semantics></math>. This approach is an <em id="S3.SS3.p1.11.2" class="ltx_emph ltx_font_italic">asymmetric quantization</em> scheme, since the clipping range is not necessarily symmetric with respect to the origin, i.e., <math id="S3.SS3.p1.8.m6.1" class="ltx_Math" alttext="-\alpha\neq\beta" display="inline"><semantics id="S3.SS3.p1.8.m6.1a"><mrow id="S3.SS3.p1.8.m6.1.1" xref="S3.SS3.p1.8.m6.1.1.cmml"><mrow id="S3.SS3.p1.8.m6.1.1.2" xref="S3.SS3.p1.8.m6.1.1.2.cmml"><mo id="S3.SS3.p1.8.m6.1.1.2a" xref="S3.SS3.p1.8.m6.1.1.2.cmml">−</mo><mi id="S3.SS3.p1.8.m6.1.1.2.2" xref="S3.SS3.p1.8.m6.1.1.2.2.cmml">α</mi></mrow><mo id="S3.SS3.p1.8.m6.1.1.1" xref="S3.SS3.p1.8.m6.1.1.1.cmml">≠</mo><mi id="S3.SS3.p1.8.m6.1.1.3" xref="S3.SS3.p1.8.m6.1.1.3.cmml">β</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m6.1b"><apply id="S3.SS3.p1.8.m6.1.1.cmml" xref="S3.SS3.p1.8.m6.1.1"><neq id="S3.SS3.p1.8.m6.1.1.1.cmml" xref="S3.SS3.p1.8.m6.1.1.1"></neq><apply id="S3.SS3.p1.8.m6.1.1.2.cmml" xref="S3.SS3.p1.8.m6.1.1.2"><minus id="S3.SS3.p1.8.m6.1.1.2.1.cmml" xref="S3.SS3.p1.8.m6.1.1.2"></minus><ci id="S3.SS3.p1.8.m6.1.1.2.2.cmml" xref="S3.SS3.p1.8.m6.1.1.2.2">𝛼</ci></apply><ci id="S3.SS3.p1.8.m6.1.1.3.cmml" xref="S3.SS3.p1.8.m6.1.1.3">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m6.1c">-\alpha\neq\beta</annotation></semantics></math>, as illustrated in&nbsp;Figure&nbsp;<a href="#S3.F2" title="Figure 2 ‣ III-A Problem Setup and Notations ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (Right). It is also possible to use a <em id="S3.SS3.p1.11.3" class="ltx_emph ltx_font_italic">symmetric quantization</em> scheme by choosing a symmetric clipping range of <math id="S3.SS3.p1.9.m7.1" class="ltx_Math" alttext="\alpha=-\beta" display="inline"><semantics id="S3.SS3.p1.9.m7.1a"><mrow id="S3.SS3.p1.9.m7.1.1" xref="S3.SS3.p1.9.m7.1.1.cmml"><mi id="S3.SS3.p1.9.m7.1.1.2" xref="S3.SS3.p1.9.m7.1.1.2.cmml">α</mi><mo id="S3.SS3.p1.9.m7.1.1.1" xref="S3.SS3.p1.9.m7.1.1.1.cmml">=</mo><mrow id="S3.SS3.p1.9.m7.1.1.3" xref="S3.SS3.p1.9.m7.1.1.3.cmml"><mo id="S3.SS3.p1.9.m7.1.1.3a" xref="S3.SS3.p1.9.m7.1.1.3.cmml">−</mo><mi id="S3.SS3.p1.9.m7.1.1.3.2" xref="S3.SS3.p1.9.m7.1.1.3.2.cmml">β</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m7.1b"><apply id="S3.SS3.p1.9.m7.1.1.cmml" xref="S3.SS3.p1.9.m7.1.1"><eq id="S3.SS3.p1.9.m7.1.1.1.cmml" xref="S3.SS3.p1.9.m7.1.1.1"></eq><ci id="S3.SS3.p1.9.m7.1.1.2.cmml" xref="S3.SS3.p1.9.m7.1.1.2">𝛼</ci><apply id="S3.SS3.p1.9.m7.1.1.3.cmml" xref="S3.SS3.p1.9.m7.1.1.3"><minus id="S3.SS3.p1.9.m7.1.1.3.1.cmml" xref="S3.SS3.p1.9.m7.1.1.3"></minus><ci id="S3.SS3.p1.9.m7.1.1.3.2.cmml" xref="S3.SS3.p1.9.m7.1.1.3.2">𝛽</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m7.1c">\alpha=-\beta</annotation></semantics></math>. A popular choice is to choose these based on the min/max values of the signal: <math id="S3.SS3.p1.10.m8.3" class="ltx_Math" alttext="-\alpha=\beta={\max(|r_{max}|,|r_{min}|)}" display="inline"><semantics id="S3.SS3.p1.10.m8.3a"><mrow id="S3.SS3.p1.10.m8.3.3" xref="S3.SS3.p1.10.m8.3.3.cmml"><mrow id="S3.SS3.p1.10.m8.3.3.4" xref="S3.SS3.p1.10.m8.3.3.4.cmml"><mo id="S3.SS3.p1.10.m8.3.3.4a" xref="S3.SS3.p1.10.m8.3.3.4.cmml">−</mo><mi id="S3.SS3.p1.10.m8.3.3.4.2" xref="S3.SS3.p1.10.m8.3.3.4.2.cmml">α</mi></mrow><mo id="S3.SS3.p1.10.m8.3.3.5" xref="S3.SS3.p1.10.m8.3.3.5.cmml">=</mo><mi id="S3.SS3.p1.10.m8.3.3.6" xref="S3.SS3.p1.10.m8.3.3.6.cmml">β</mi><mo id="S3.SS3.p1.10.m8.3.3.7" xref="S3.SS3.p1.10.m8.3.3.7.cmml">=</mo><mrow id="S3.SS3.p1.10.m8.3.3.2.2" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml"><mi id="S3.SS3.p1.10.m8.1.1" xref="S3.SS3.p1.10.m8.1.1.cmml">max</mi><mo id="S3.SS3.p1.10.m8.3.3.2.2a" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml">⁡</mo><mrow id="S3.SS3.p1.10.m8.3.3.2.2.2" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml"><mo stretchy="false" id="S3.SS3.p1.10.m8.3.3.2.2.2.3" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml">(</mo><mrow id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.2" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.2.1.cmml">|</mo><msub id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.2" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.2.cmml">r</mi><mrow id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.2" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.3" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1a" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.4" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.4.cmml">x</mi></mrow></msub><mo stretchy="false" id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.3" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S3.SS3.p1.10.m8.3.3.2.2.2.4" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml">,</mo><mrow id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.2" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.2.1.cmml">|</mo><msub id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.cmml"><mi id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.2" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.2.cmml">r</mi><mrow id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.cmml"><mi id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.2" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.3" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1a" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.4" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.4.cmml">n</mi></mrow></msub><mo stretchy="false" id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.3" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.2.1.cmml">|</mo></mrow><mo stretchy="false" id="S3.SS3.p1.10.m8.3.3.2.2.2.5" xref="S3.SS3.p1.10.m8.3.3.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m8.3b"><apply id="S3.SS3.p1.10.m8.3.3.cmml" xref="S3.SS3.p1.10.m8.3.3"><and id="S3.SS3.p1.10.m8.3.3a.cmml" xref="S3.SS3.p1.10.m8.3.3"></and><apply id="S3.SS3.p1.10.m8.3.3b.cmml" xref="S3.SS3.p1.10.m8.3.3"><eq id="S3.SS3.p1.10.m8.3.3.5.cmml" xref="S3.SS3.p1.10.m8.3.3.5"></eq><apply id="S3.SS3.p1.10.m8.3.3.4.cmml" xref="S3.SS3.p1.10.m8.3.3.4"><minus id="S3.SS3.p1.10.m8.3.3.4.1.cmml" xref="S3.SS3.p1.10.m8.3.3.4"></minus><ci id="S3.SS3.p1.10.m8.3.3.4.2.cmml" xref="S3.SS3.p1.10.m8.3.3.4.2">𝛼</ci></apply><ci id="S3.SS3.p1.10.m8.3.3.6.cmml" xref="S3.SS3.p1.10.m8.3.3.6">𝛽</ci></apply><apply id="S3.SS3.p1.10.m8.3.3c.cmml" xref="S3.SS3.p1.10.m8.3.3"><eq id="S3.SS3.p1.10.m8.3.3.7.cmml" xref="S3.SS3.p1.10.m8.3.3.7"></eq><share href="#S3.SS3.p1.10.m8.3.3.6.cmml" id="S3.SS3.p1.10.m8.3.3d.cmml" xref="S3.SS3.p1.10.m8.3.3"></share><apply id="S3.SS3.p1.10.m8.3.3.2.3.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2"><max id="S3.SS3.p1.10.m8.1.1.cmml" xref="S3.SS3.p1.10.m8.1.1"></max><apply id="S3.SS3.p1.10.m8.2.2.1.1.1.1.2.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1"><abs id="S3.SS3.p1.10.m8.2.2.1.1.1.1.2.1.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.2"></abs><apply id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.2">𝑟</ci><apply id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3"><times id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.1"></times><ci id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.2">𝑚</ci><ci id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.3">𝑎</ci><ci id="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.4.cmml" xref="S3.SS3.p1.10.m8.2.2.1.1.1.1.1.1.3.4">𝑥</ci></apply></apply></apply><apply id="S3.SS3.p1.10.m8.3.3.2.2.2.2.2.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1"><abs id="S3.SS3.p1.10.m8.3.3.2.2.2.2.2.1.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.2"></abs><apply id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.1.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1">subscript</csymbol><ci id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.2.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.2">𝑟</ci><apply id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3"><times id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.1"></times><ci id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.2.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.2">𝑚</ci><ci id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.3.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.3">𝑖</ci><ci id="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.4.cmml" xref="S3.SS3.p1.10.m8.3.3.2.2.2.2.1.1.3.4">𝑛</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m8.3c">-\alpha=\beta={\max(|r_{max}|,|r_{min}|)}</annotation></semantics></math>. Asymmetric quantization often results in a tighter clipping range as compared to symmetric quantization. This is especially important when the target weights or activations are imbalanced, e.g., the activation after ReLU that always has non-negative values. Using symmetric quantization, however, simplifies the quantization function in&nbsp;Eq.&nbsp;<a href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> by replacing the zero point with <math id="S3.SS3.p1.11.m9.1" class="ltx_Math" alttext="Z=0" display="inline"><semantics id="S3.SS3.p1.11.m9.1a"><mrow id="S3.SS3.p1.11.m9.1.1" xref="S3.SS3.p1.11.m9.1.1.cmml"><mi id="S3.SS3.p1.11.m9.1.1.2" xref="S3.SS3.p1.11.m9.1.1.2.cmml">Z</mi><mo id="S3.SS3.p1.11.m9.1.1.1" xref="S3.SS3.p1.11.m9.1.1.1.cmml">=</mo><mn id="S3.SS3.p1.11.m9.1.1.3" xref="S3.SS3.p1.11.m9.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m9.1b"><apply id="S3.SS3.p1.11.m9.1.1.cmml" xref="S3.SS3.p1.11.m9.1.1"><eq id="S3.SS3.p1.11.m9.1.1.1.cmml" xref="S3.SS3.p1.11.m9.1.1.1"></eq><ci id="S3.SS3.p1.11.m9.1.1.2.cmml" xref="S3.SS3.p1.11.m9.1.1.2">𝑍</ci><cn type="integer" id="S3.SS3.p1.11.m9.1.1.3.cmml" xref="S3.SS3.p1.11.m9.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m9.1c">Z=0</annotation></semantics></math>:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.3" class="ltx_Math" alttext="\small Q(r)=\text{Int}\left(\frac{r}{S}\right)." display="block"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3.1" xref="S3.E5.m1.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1" xref="S3.E5.m1.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1.2" xref="S3.E5.m1.3.3.1.1.2.cmml"><mi mathsize="90%" id="S3.E5.m1.3.3.1.1.2.2" xref="S3.E5.m1.3.3.1.1.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.2.1" xref="S3.E5.m1.3.3.1.1.2.1.cmml">​</mo><mrow id="S3.E5.m1.3.3.1.1.2.3.2" xref="S3.E5.m1.3.3.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E5.m1.3.3.1.1.2.3.2.1" xref="S3.E5.m1.3.3.1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">r</mi><mo maxsize="90%" minsize="90%" id="S3.E5.m1.3.3.1.1.2.3.2.2" xref="S3.E5.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E5.m1.3.3.1.1.1" xref="S3.E5.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E5.m1.3.3.1.1.3" xref="S3.E5.m1.3.3.1.1.3.cmml"><mtext mathsize="90%" id="S3.E5.m1.3.3.1.1.3.2" xref="S3.E5.m1.3.3.1.1.3.2a.cmml">Int</mtext><mo lspace="0em" rspace="0em" id="S3.E5.m1.3.3.1.1.3.1" xref="S3.E5.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S3.E5.m1.3.3.1.1.3.3.2" xref="S3.E5.m1.2.2.cmml"><mo id="S3.E5.m1.3.3.1.1.3.3.2.1" xref="S3.E5.m1.2.2.cmml">(</mo><mfrac id="S3.E5.m1.2.2" xref="S3.E5.m1.2.2.cmml"><mi mathsize="90%" id="S3.E5.m1.2.2.2" xref="S3.E5.m1.2.2.2.cmml">r</mi><mi mathsize="90%" id="S3.E5.m1.2.2.3" xref="S3.E5.m1.2.2.3.cmml">S</mi></mfrac><mo id="S3.E5.m1.3.3.1.1.3.3.2.2" xref="S3.E5.m1.2.2.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E5.m1.3.3.1.2" xref="S3.E5.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.1.1.cmml" xref="S3.E5.m1.3.3.1"><eq id="S3.E5.m1.3.3.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1"></eq><apply id="S3.E5.m1.3.3.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.2"><times id="S3.E5.m1.3.3.1.1.2.1.cmml" xref="S3.E5.m1.3.3.1.1.2.1"></times><ci id="S3.E5.m1.3.3.1.1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.2.2">𝑄</ci><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">𝑟</ci></apply><apply id="S3.E5.m1.3.3.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.3"><times id="S3.E5.m1.3.3.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.3.1"></times><ci id="S3.E5.m1.3.3.1.1.3.2a.cmml" xref="S3.E5.m1.3.3.1.1.3.2"><mtext mathsize="90%" id="S3.E5.m1.3.3.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.3.2">Int</mtext></ci><apply id="S3.E5.m1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2"><divide id="S3.E5.m1.2.2.1.cmml" xref="S3.E5.m1.3.3.1.1.3.3.2"></divide><ci id="S3.E5.m1.2.2.2.cmml" xref="S3.E5.m1.2.2.2">𝑟</ci><ci id="S3.E5.m1.2.2.3.cmml" xref="S3.E5.m1.2.2.3">𝑆</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">\small Q(r)=\text{Int}\left(\frac{r}{S}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p1.13">여기서, 스케일링 팩터에 대한 두 가지 선택이 있다. "전체 범위" 대칭 양자화 S는 [-128,127]의 전체 INT8 범위를 사용하기 위해 <math alttext="\frac{2max(|r|)}{2^{n}-1}" class="ltx_Math" display="inline" id="S3.SS3.p1.12.m1.2"><semantics id="S3.SS3.p1.12.m1.2a"><mfrac id="S3.SS3.p1.12.m1.2.2" xref="S3.SS3.p1.12.m1.2.2.cmml"><mrow id="S3.SS3.p1.12.m1.2.2.2" xref="S3.SS3.p1.12.m1.2.2.2.cmml"><mn id="S3.SS3.p1.12.m1.2.2.2.4" xref="S3.SS3.p1.12.m1.2.2.2.4.cmml">2</mn><mo id="S3.SS3.p1.12.m1.2.2.2.3" lspace="0em" rspace="0em" xref="S3.SS3.p1.12.m1.2.2.2.3.cmml">​</mo><mi id="S3.SS3.p1.12.m1.2.2.2.5" xref="S3.SS3.p1.12.m1.2.2.2.5.cmml">m</mi><mo id="S3.SS3.p1.12.m1.2.2.2.3a" lspace="0em" rspace="0em" xref="S3.SS3.p1.12.m1.2.2.2.3.cmml">​</mo><mi id="S3.SS3.p1.12.m1.2.2.2.6" xref="S3.SS3.p1.12.m1.2.2.2.6.cmml">a</mi><mo id="S3.SS3.p1.12.m1.2.2.2.3b" lspace="0em" rspace="0em" xref="S3.SS3.p1.12.m1.2.2.2.3.cmml">​</mo><mi id="S3.SS3.p1.12.m1.2.2.2.7" xref="S3.SS3.p1.12.m1.2.2.2.7.cmml">x</mi><mo id="S3.SS3.p1.12.m1.2.2.2.3c" lspace="0em" rspace="0em" xref="S3.SS3.p1.12.m1.2.2.2.3.cmml">​</mo><mrow id="S3.SS3.p1.12.m1.2.2.2.2.1" xref="S3.SS3.p1.12.m1.2.2.2.cmml"><mo id="S3.SS3.p1.12.m1.2.2.2.2.1.2" stretchy="false" xref="S3.SS3.p1.12.m1.2.2.2.cmml">(</mo><mrow id="S3.SS3.p1.12.m1.2.2.2.2.1.1.2" xref="S3.SS3.p1.12.m1.2.2.2.2.1.1.1.cmml"><mo id="S3.SS3.p1.12.m1.2.2.2.2.1.1.2.1" stretchy="false" xref="S3.SS3.p1.12.m1.2.2.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS3.p1.12.m1.1.1.1.1" xref="S3.SS3.p1.12.m1.1.1.1.1.cmml">r</mi><mo id="S3.SS3.p1.12.m1.2.2.2.2.1.1.2.2" stretchy="false" xref="S3.SS3.p1.12.m1.2.2.2.2.1.1.1.1.cmml">|</mo></mrow><mo id="S3.SS3.p1.12.m1.2.2.2.2.1.3" stretchy="false" xref="S3.SS3.p1.12.m1.2.2.2.cmml">)</mo></mrow></mrow><mrow id="S3.SS3.p1.12.m1.2.2.4" xref="S3.SS3.p1.12.m1.2.2.4.cmml"><msup id="S3.SS3.p1.12.m1.2.2.4.2" xref="S3.SS3.p1.12.m1.2.2.4.2.cmml"><mn id="S3.SS3.p1.12.m1.2.2.4.2.2" xref="S3.SS3.p1.12.m1.2.2.4.2.2.cmml">2</mn><mi id="S3.SS3.p1.12.m1.2.2.4.2.3" xref="S3.SS3.p1.12.m1.2.2.4.2.3.cmml">n</mi></msup><mo id="S3.SS3.p1.12.m1.2.2.4.1" xref="S3.SS3.p1.12.m1.2.2.4.1.cmml">−</mo><mn id="S3.SS3.p1.12.m1.2.2.4.3" xref="S3.SS3.p1.12.m1.2.2.4.3.cmml">1</mn></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.12.m1.2b"><apply id="S3.SS3.p1.12.m1.2.2.cmml" xref="S3.SS3.p1.12.m1.2.2"><divide id="S3.SS3.p1.12.m1.2.2.3.cmml" xref="S3.SS3.p1.12.m1.2.2"></divide><apply id="S3.SS3.p1.12.m1.2.2.2.cmml" xref="S3.SS3.p1.12.m1.2.2.2"><times id="S3.SS3.p1.12.m1.2.2.2.3.cmml" xref="S3.SS3.p1.12.m1.2.2.2.3"></times><cn id="S3.SS3.p1.12.m1.2.2.2.4.cmml" type="integer" xref="S3.SS3.p1.12.m1.2.2.2.4">2</cn><ci id="S3.SS3.p1.12.m1.2.2.2.5.cmml" xref="S3.SS3.p1.12.m1.2.2.2.5">𝑚</ci><ci id="S3.SS3.p1.12.m1.2.2.2.6.cmml" xref="S3.SS3.p1.12.m1.2.2.2.6">𝑎</ci><ci id="S3.SS3.p1.12.m1.2.2.2.7.cmml" xref="S3.SS3.p1.12.m1.2.2.2.7">𝑥</ci><apply id="S3.SS3.p1.12.m1.2.2.2.2.1.1.1.cmml" xref="S3.SS3.p1.12.m1.2.2.2.2.1.1.2"><abs id="S3.SS3.p1.12.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.SS3.p1.12.m1.2.2.2.2.1.1.2.1"></abs><ci id="S3.SS3.p1.12.m1.1.1.1.1.cmml" xref="S3.SS3.p1.12.m1.1.1.1.1">𝑟</ci></apply></apply><apply id="S3.SS3.p1.12.m1.2.2.4.cmml" xref="S3.SS3.p1.12.m1.2.2.4"><minus id="S3.SS3.p1.12.m1.2.2.4.1.cmml" xref="S3.SS3.p1.12.m1.2.2.4.1"></minus><apply id="S3.SS3.p1.12.m1.2.2.4.2.cmml" xref="S3.SS3.p1.12.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.SS3.p1.12.m1.2.2.4.2.1.cmml" xref="S3.SS3.p1.12.m1.2.2.4.2">superscript</csymbol><cn id="S3.SS3.p1.12.m1.2.2.4.2.2.cmml" type="integer" xref="S3.SS3.p1.12.m1.2.2.4.2.2">2</cn><ci id="S3.SS3.p1.12.m1.2.2.4.2.3.cmml" xref="S3.SS3.p1.12.m1.2.2.4.2.3">𝑛</ci></apply><cn id="S3.SS3.p1.12.m1.2.2.4.3.cmml" type="integer" xref="S3.SS3.p1.12.m1.2.2.4.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.12.m1.2c">\frac{2max(|r|)}{2^{n}-1}</annotation></semantics></math>로 선택된다. 그러나, "제한된 범위"에서 S는 [-127,127]의 범위만을 사용하는 <math alttext="\frac{max(|r|)}{2^{n-1}-1}" class="ltx_Math" display="inline" id="S3.SS3.p1.13.m2.2"><semantics id="S3.SS3.p1.13.m2.2a"><mfrac id="S3.SS3.p1.13.m2.2.2" xref="S3.SS3.p1.13.m2.2.2.cmml"><mrow id="S3.SS3.p1.13.m2.2.2.2" xref="S3.SS3.p1.13.m2.2.2.2.cmml"><mi id="S3.SS3.p1.13.m2.2.2.2.4" xref="S3.SS3.p1.13.m2.2.2.2.4.cmml">m</mi><mo id="S3.SS3.p1.13.m2.2.2.2.3" lspace="0em" rspace="0em" xref="S3.SS3.p1.13.m2.2.2.2.3.cmml">​</mo><mi id="S3.SS3.p1.13.m2.2.2.2.5" xref="S3.SS3.p1.13.m2.2.2.2.5.cmml">a</mi><mo id="S3.SS3.p1.13.m2.2.2.2.3a" lspace="0em" rspace="0em" xref="S3.SS3.p1.13.m2.2.2.2.3.cmml">​</mo><mi id="S3.SS3.p1.13.m2.2.2.2.6" xref="S3.SS3.p1.13.m2.2.2.2.6.cmml">x</mi><mo id="S3.SS3.p1.13.m2.2.2.2.3b" lspace="0em" rspace="0em" xref="S3.SS3.p1.13.m2.2.2.2.3.cmml">​</mo><mrow id="S3.SS3.p1.13.m2.2.2.2.2.1" xref="S3.SS3.p1.13.m2.2.2.2.cmml"><mo id="S3.SS3.p1.13.m2.2.2.2.2.1.2" stretchy="false" xref="S3.SS3.p1.13.m2.2.2.2.cmml">(</mo><mrow id="S3.SS3.p1.13.m2.2.2.2.2.1.1.2" xref="S3.SS3.p1.13.m2.2.2.2.2.1.1.1.cmml"><mo id="S3.SS3.p1.13.m2.2.2.2.2.1.1.2.1" stretchy="false" xref="S3.SS3.p1.13.m2.2.2.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS3.p1.13.m2.1.1.1.1" xref="S3.SS3.p1.13.m2.1.1.1.1.cmml">r</mi><mo id="S3.SS3.p1.13.m2.2.2.2.2.1.1.2.2" stretchy="false" xref="S3.SS3.p1.13.m2.2.2.2.2.1.1.1.1.cmml">|</mo></mrow><mo id="S3.SS3.p1.13.m2.2.2.2.2.1.3" stretchy="false" xref="S3.SS3.p1.13.m2.2.2.2.cmml">)</mo></mrow></mrow><mrow id="S3.SS3.p1.13.m2.2.2.4" xref="S3.SS3.p1.13.m2.2.2.4.cmml"><msup id="S3.SS3.p1.13.m2.2.2.4.2" xref="S3.SS3.p1.13.m2.2.2.4.2.cmml"><mn id="S3.SS3.p1.13.m2.2.2.4.2.2" xref="S3.SS3.p1.13.m2.2.2.4.2.2.cmml">2</mn><mrow id="S3.SS3.p1.13.m2.2.2.4.2.3" xref="S3.SS3.p1.13.m2.2.2.4.2.3.cmml"><mi id="S3.SS3.p1.13.m2.2.2.4.2.3.2" xref="S3.SS3.p1.13.m2.2.2.4.2.3.2.cmml">n</mi><mo id="S3.SS3.p1.13.m2.2.2.4.2.3.1" xref="S3.SS3.p1.13.m2.2.2.4.2.3.1.cmml">−</mo><mn id="S3.SS3.p1.13.m2.2.2.4.2.3.3" xref="S3.SS3.p1.13.m2.2.2.4.2.3.3.cmml">1</mn></mrow></msup><mo id="S3.SS3.p1.13.m2.2.2.4.1" xref="S3.SS3.p1.13.m2.2.2.4.1.cmml">−</mo><mn id="S3.SS3.p1.13.m2.2.2.4.3" xref="S3.SS3.p1.13.m2.2.2.4.3.cmml">1</mn></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.13.m2.2b"><apply id="S3.SS3.p1.13.m2.2.2.cmml" xref="S3.SS3.p1.13.m2.2.2"><divide id="S3.SS3.p1.13.m2.2.2.3.cmml" xref="S3.SS3.p1.13.m2.2.2"></divide><apply id="S3.SS3.p1.13.m2.2.2.2.cmml" xref="S3.SS3.p1.13.m2.2.2.2"><times id="S3.SS3.p1.13.m2.2.2.2.3.cmml" xref="S3.SS3.p1.13.m2.2.2.2.3"></times><ci id="S3.SS3.p1.13.m2.2.2.2.4.cmml" xref="S3.SS3.p1.13.m2.2.2.2.4">𝑚</ci><ci id="S3.SS3.p1.13.m2.2.2.2.5.cmml" xref="S3.SS3.p1.13.m2.2.2.2.5">𝑎</ci><ci id="S3.SS3.p1.13.m2.2.2.2.6.cmml" xref="S3.SS3.p1.13.m2.2.2.2.6">𝑥</ci><apply id="S3.SS3.p1.13.m2.2.2.2.2.1.1.1.cmml" xref="S3.SS3.p1.13.m2.2.2.2.2.1.1.2"><abs id="S3.SS3.p1.13.m2.2.2.2.2.1.1.1.1.cmml" xref="S3.SS3.p1.13.m2.2.2.2.2.1.1.2.1"></abs><ci id="S3.SS3.p1.13.m2.1.1.1.1.cmml" xref="S3.SS3.p1.13.m2.1.1.1.1">𝑟</ci></apply></apply><apply id="S3.SS3.p1.13.m2.2.2.4.cmml" xref="S3.SS3.p1.13.m2.2.2.4"><minus id="S3.SS3.p1.13.m2.2.2.4.1.cmml" xref="S3.SS3.p1.13.m2.2.2.4.1"></minus><apply id="S3.SS3.p1.13.m2.2.2.4.2.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2"><csymbol cd="ambiguous" id="S3.SS3.p1.13.m2.2.2.4.2.1.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2">superscript</csymbol><cn id="S3.SS3.p1.13.m2.2.2.4.2.2.cmml" type="integer" xref="S3.SS3.p1.13.m2.2.2.4.2.2">2</cn><apply id="S3.SS3.p1.13.m2.2.2.4.2.3.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2.3"><minus id="S3.SS3.p1.13.m2.2.2.4.2.3.1.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2.3.1"></minus><ci id="S3.SS3.p1.13.m2.2.2.4.2.3.2.cmml" xref="S3.SS3.p1.13.m2.2.2.4.2.3.2">𝑛</ci><cn id="S3.SS3.p1.13.m2.2.2.4.2.3.3.cmml" type="integer" xref="S3.SS3.p1.13.m2.2.2.4.2.3.3">1</cn></apply></apply><cn id="S3.SS3.p1.13.m2.2.2.4.3.cmml" type="integer" xref="S3.SS3.p1.13.m2.2.2.4.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.13.m2.2c">\frac{max(|r|)}{2^{n-1}-1}</annotation></semantics></math>로 선택된다. 예상대로 전체 범위 접근법이 더 정확합니다. 대칭 양자화는 가중치를 양자화하는 데 널리 사용되는데, 영점을 제로 아웃하는 것은 추론 동안 계산 비용을 줄일 수 있기 때문이다<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib255" title="">255</a>]</cite> 또한 구현을 더 쉽게 만든다. 그러나 활성화를 위해 비대칭 활성화에서 오프셋으로 인해 점유하는 교차 항은 정적 데이터 독립 항이며 편향에서 흡수될 수 있다(또는 누산기를 초기화하는 데 사용됨)(<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>).</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p2.4">대칭 및 비대칭 양자화를 위해 신호의 최소/최대를 사용하는 것이 인기 있는 방법이다. 그러나 이 접근법은 활성화의 이상치 데이터에 취약하다. 이들은 불필요하게 범위를 증가시킬 수 있고, 결과적으로 양자화의 해상도를 감소시킬 수 있다. 이를 해결하기 위한 한 가지 접근법은 신호 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib172" title="">172</a>]</cite>의 min/max 대신 백분위수를 사용하는 것이다. 즉, 가장 큰/가장 작은 값 대신 i번째 가장 큰/가장 작은 값을 <math alttext="\beta" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\beta</annotation></semantics></math>/<math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\alpha</annotation></semantics></math>로 사용한다. 다른 접근법은 실제 값들과 양자화된 값들 사이의 KL 발산(즉, 정보 손실)을 최소화하기 위해 <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\alpha</annotation></semantics></math> 및 <math alttext="\beta" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.1"><semantics id="S3.SS3.p2.4.m4.1a"><mi id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><ci id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\beta</annotation></semantics></math>를 선택하는 것이다. 관심 있는 독자는 다양한 모델에 대해 다른 보정 방법을 평가하는 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib255" title="">255</a>]</cite>를 참조한다.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">Summary (Symmetric vs Asymmetric Quantization). </span> 대칭 양자화는 대칭 범위를 사용하여 클리핑을 분할합니다. 이는 Eq에서 <math alttext="Z=0" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">Z</mi><mo id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><eq id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1"></eq><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">𝑍</ci><cn id="S3.SS3.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.p3.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">Z=0</annotation></semantics></math>로 이어지기 때문에 구현이 용이하다는 장점이 있다. <a class="ltx_ref" href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">2</span></a>. 그러나 범위가 대칭이 아닌 비대칭일 수 있는 경우에는 차선책입니다. 그러한 경우들에 대해, 비대칭 양자화가 바람직하다.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Range Calibration Algorithms: Static vs Dynamic Quantization</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p1.2">지금까지 <math alttext="[\alpha,\beta]" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.2"><semantics id="S3.SS4.p1.1.m1.2a"><mrow id="S3.SS4.p1.1.m1.2.3.2" xref="S3.SS4.p1.1.m1.2.3.1.cmml"><mo id="S3.SS4.p1.1.m1.2.3.2.1" stretchy="false" xref="S3.SS4.p1.1.m1.2.3.1.cmml">[</mo><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">α</mi><mo id="S3.SS4.p1.1.m1.2.3.2.2" xref="S3.SS4.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS4.p1.1.m1.2.2" xref="S3.SS4.p1.1.m1.2.2.cmml">β</mi><mo id="S3.SS4.p1.1.m1.2.3.2.3" stretchy="false" xref="S3.SS4.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.2b"><interval closure="closed" id="S3.SS4.p1.1.m1.2.3.1.cmml" xref="S3.SS4.p1.1.m1.2.3.2"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝛼</ci><ci id="S3.SS4.p1.1.m1.2.2.cmml" xref="S3.SS4.p1.1.m1.2.2">𝛽</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.2c">[\alpha,\beta]</annotation></semantics></math>의 클리핑 범위를 결정하기 위한 다양한 교정 방법에 대해 논의했다. 양자화 방법의 또 다른 중요한 차별화기는 <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.2.1">when</span> 클리핑 범위가 결정된다. 이 범위는 대부분의 경우 모수가 추론 중에 고정되므로 가중치에 대해 정적으로 계산할 수 있다. 그러나 활성화 맵은 입력 샘플마다 다르다(<math alttext="x" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><mi id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">x</annotation></semantics></math> in Eq.<a class="ltx_ref" href="#S3.E1" title="In III-A Problem Setup and Notations ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">1</span></a>). 이와 같이 활성화를 양자화하는 방법에는 <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.2.2">dynamic quantization</span> 및 <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.2.3">static quantization</span> 두 가지가 있다.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p2.1">동적 양자화에 있어서, 이 범위는 런타임 동안 각각의 활성화 맵에 대해 계산된 <em class="ltx_emph ltx_font_italic" id="S3.SS4.p2.1.1">dynamically</em>이다. 이 방법은 매우 높은 오버헤드를 가질 수 있는 신호 통계(최소, 최대, 백분위수 등)의 실시간 계산을 요구한다. 그러나, 동적 양자화는 종종 신호 범위가 각각의 입력에 대해 정확하게 계산됨에 따라 더 높은 정확도를 초래한다.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p3.1">또 다른 양자화 접근법은 정적 양자화이며, 여기서 클리핑 범위는 미리 계산되고 추론 동안 <em class="ltx_emph ltx_font_italic" id="S3.SS4.p3.1.1">static</em>이다. 이 접근법은 계산 오버헤드를 추가하지 않지만, 일반적으로 동적 양자화에 비해 낮은 정확도를 초래한다. 사전 계산을 위한 한 가지 인기 있는 방법은 일련의 교정 입력을 실행하여 활성화의 전형적인 범위 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib113" title="">113</a>, <a class="ltx_ref" href="#bib.bib267" title="">267</a>]</cite>를 계산하는 것이다. 원래의 양자화되지 않은 가중치 분포와 대응하는 양자화된 값들 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib229" title="">229</a>, <a class="ltx_ref" href="#bib.bib221" title="">221</a>, <a class="ltx_ref" href="#bib.bib40" title="">40</a>, <a class="ltx_ref" href="#bib.bib281" title="">281</a>]</cite> 사이의 MSE(Mean Squared Error)를 최소화하는 것을 포함하여 최상의 범위를 찾기 위해 다수의 상이한 메트릭들이 제안되었다. MSE가 가장 일반적인 방법이지만 엔트로피 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib189" title="">189</a>]</cite>와 같은 다른 메트릭을 사용하는 것도 고려할 수 있다. 다른 접근법은 NN 트레이닝 동안 이 클리핑 범위를 학습/부여하는 것이다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib146" title="">146</a>, <a class="ltx_ref" href="#bib.bib36" title="">36</a>, <a class="ltx_ref" href="#bib.bib287" title="">287</a>, <a class="ltx_ref" href="#bib.bib276" title="">276</a>]</cite>. 여기에서 주목할 만한 작업은 훈련 중 NN의 클리핑 범위와 가중치를 공동으로 최적화하는 LQNets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib276" title="">276</a>]</cite>, PACT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib36" title="">36</a>]</cite>, LSQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib56" title="">56</a>]</cite>, LSQ+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>이다.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p4.1.1">Summary (Dynamic vs Static Quantization). </span> 동적 양자화는 각 활성화의 클리핑 범위를 동적으로 계산하며 종종 가장 높은 정확도를 달성한다. 그러나, 신호의 범위를 동적으로 계산하는 것은 매우 고가이며, 따라서, 실무자들은 클리핑 범위가 모든 입력들에 대해 고정된 정적 양자화를 가장 자주 사용한다.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Quantization Granularity</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS5.p1.1">대부분의 컴퓨터 비전 작업에서 계층에 대한 활성화 입력은 그림 <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">3</span></a>에 예시된 바와 같이 많은 다른 컨볼루션 필터로 컨벌루션된다. 이러한 컨볼루션 필터들 각각은 상이한 범위의 값들을 가질 수 있다. 이와 같이 양자화 방법을 위한 하나의 미분기는 가중치에 대해 클리핑 범위 <math alttext="[\alpha,\beta]" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.2"><semantics id="S3.SS5.p1.1.m1.2a"><mrow id="S3.SS5.p1.1.m1.2.3.2" xref="S3.SS5.p1.1.m1.2.3.1.cmml"><mo id="S3.SS5.p1.1.m1.2.3.2.1" stretchy="false" xref="S3.SS5.p1.1.m1.2.3.1.cmml">[</mo><mi id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">α</mi><mo id="S3.SS5.p1.1.m1.2.3.2.2" xref="S3.SS5.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS5.p1.1.m1.2.2" xref="S3.SS5.p1.1.m1.2.2.cmml">β</mi><mo id="S3.SS5.p1.1.m1.2.3.2.3" stretchy="false" xref="S3.SS5.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.2b"><interval closure="closed" id="S3.SS5.p1.1.m1.2.3.1.cmml" xref="S3.SS5.p1.1.m1.2.3.2"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">𝛼</ci><ci id="S3.SS5.p1.1.m1.2.2.cmml" xref="S3.SS5.p1.1.m1.2.2">𝛽</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.2c">[\alpha,\beta]</annotation></semantics></math>가 어떻게 계산되는지에 대한 세분성이다. 우리는 그것들을 다음과 같이 분류했다.</p>
</div>
<section id="S3.SS5.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Layerwise Quantization</h5>

<div id="S3.SS5.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS5.SSS0.Px1.p1.1">이 접근법에서 클리핑 범위는 그림 <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">3</span></a>의 세 번째 열과 같이 레이어 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib133" title="">133</a>]</cite>의 컨볼루션 필터에서 모든 가중치를 고려하여 결정된다. 여기서는 해당 계층에서 전체 매개 변수의 통계량(예: 최소, 최대, 백분위수 등)을 조사한 다음 모든 컨볼루션 필터에 대해 동일한 클리핑 범위를 사용합니다. 이 접근법은 구현하기가 매우 간단하지만, 각 컨볼루션 필터의 범위가 많이 변할 수 있기 때문에 종종 차선 정확도를 초래한다. 예를 들어, 상대적으로 더 좁은 범위의 파라미터를 갖는 컨볼루션 커널은 더 넓은 범위를 갖는 동일한 레이어의 다른 커널로 인해 양자화 해상도를 잃을 수 있다.</p>
</div>
</section>
<section id="S3.SS5.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Groupwise Quantization</h5>

<div id="S3.SS5.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS5.SSS0.Px2.p1.1">하나는 (활성화 또는 컨볼루션 커널 중 어느 하나의) 클리핑 범위를 계산하기 위해 레이어 내부에 다수의 상이한 채널들을 그룹화할 수 있다. 이는 단일 컨볼루션/활성화에 걸친 파라미터의 분포가 많이 변하는 경우에 도움이 될 수 있다. 예를 들어, 이 접근법은 완전 연결 주의 계층으로 구성된 트랜스포머 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib243" title="">243</a>]</cite> 모델을 양자화하는 데 Q-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib219" title="">219</a>]</cite>에서 유용한 것으로 나타났다. 그러나 이 접근 방식은 필연적으로 다양한 스케일링 요인을 고려해야 하는 추가 비용이 수반된다.</p>
</div>
</section>
<section id="S3.SS5.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Channelwise Quantization</h5>

<div id="S3.SS5.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS5.SSS0.Px3.p1.1">클리핑 범위의 인기 있는 선택은 그림 <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">3</span></a>의 마지막 열에서 볼 수 있듯이 다른 채널과 무관하게 각 컨볼루션 필터에 대해 고정된 값을 사용하는 것이다. 즉, 각 채널에는 전용 스케일링 팩터가 할당된다. 이것은 더 나은 양자화 해상도를 보장하고 종종 더 높은 정확도를 초래한다.</p>
</div>
</section>
<section id="S3.SS5.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Sub-channelwise Quantization</h5>

<div id="S3.SS5.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS5.SSS0.Px4.p1.1">이전 접근법은 클리핑 범위가 컨볼루션 또는 완전-연결된 층 내의 파라미터들의 임의의 그룹들에 관하여 결정되는 극단으로 취해질 수 있다. 그러나, 단일 컨볼루션 또는 풀-연결된 층을 프로세싱할 때 상이한 스케일링 팩터들이 고려될 필요가 있기 때문에, 이 접근법은 상당한 오버헤드를 추가할 수 있다. 따라서, 그룹별 양자화는 양자화 해상도와 계산 오버헤드 사이에 좋은 타협점을 설정할 수 있다.</p>
</div>
<div id="S3.SS5.SSS0.Px4.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS5.SSS0.Px4.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS5.SSS0.Px4.p2.1.1">Summary (Quantization Granularity) </span> 채널별 양자화는 현재 컨볼루션 커널을 양자화하는 데 사용되는 표준 방법이다. 이는 실무자가 무시할 수 있는 오버헤드로 각각의 개별 커널에 대한 클리핑 범위를 조정할 수 있게 한다. 대조적으로, 서브-채널와이즈 양자화는 상당한 오버헤드를 초래할 수 있고 현재 표준 선택이 아니다(또한 우리는 이러한 설계 선택들과 연관된 트레이드오프들을 위해 관심있는 판독기를 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib68" title="">68</a>]</cite>에 참조한다).</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2103.13630/assets/x5.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 4:</span>Quantization-Aware Training(QAT, Left)과 Post-Training Quantization(PTQ, Right)의 비교.</figcaption>
In QAT, a pre-trained model is quantized and then finetuned using training data to adjust parameters and recover accuracy degradation.
In PTQ, a pre-trained model is calibrated using calibration data (e.g., a small subset of training data) to compute the clipping ranges and the scaling factors.
Then, the model is quantized based on the calibration result.
Note that the calibration process is often conducted in parallel with the finetuning process for QAT.
</figcaption>
</figure>
</section>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.5.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.6.2" class="ltx_text ltx_font_italic">Non-Uniform Quantization</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS6.p1.2">문헌의 일부 연구는 또한 양자화 단계뿐만 아니라 양자화 레벨이 불균일하게 이격되도록 허용되는 비균일 양자화 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib74" title="">74</a>, <a class="ltx_ref" href="#bib.bib256" title="">256</a>, <a class="ltx_ref" href="#bib.bib79" title="">79</a>, <a class="ltx_ref" href="#bib.bib99" title="">99</a>, <a class="ltx_ref" href="#bib.bib159" title="">159</a>, <a class="ltx_ref" href="#bib.bib179" title="">179</a>, <a class="ltx_ref" href="#bib.bib38" title="">38</a>, <a class="ltx_ref" href="#bib.bib25" title="">25</a>, <a class="ltx_ref" href="#bib.bib190" title="">190</a>, <a class="ltx_ref" href="#bib.bib276" title="">276</a>, <a class="ltx_ref" href="#bib.bib248" title="">248</a>, <a class="ltx_ref" href="#bib.bib118" title="">118</a>, <a class="ltx_ref" href="#bib.bib125" title="">125</a>, <a class="ltx_ref" href="#bib.bib264" title="">264</a>, <a class="ltx_ref" href="#bib.bib62" title="">62</a>, <a class="ltx_ref" href="#bib.bib238" title="">238</a>, <a class="ltx_ref" href="#bib.bib284" title="">284</a>, <a class="ltx_ref" href="#bib.bib189" title="">189</a>, <a class="ltx_ref" href="#bib.bib266" title="">266</a>, <a class="ltx_ref" href="#bib.bib153" title="">153</a>]</cite>를 탐구했다. 불균일 양자화의 형식적 정의는 식에 나와 있다. <a class="ltx_ref" href="#S3.E6" title="In III-F Non-Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">6</span></a>, 여기서 <math alttext="X_{i}" class="ltx_Math" display="inline" id="S3.SS6.p1.1.m1.1"><semantics id="S3.SS6.p1.1.m1.1a"><msub id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml"><mi id="S3.SS6.p1.1.m1.1.1.2" xref="S3.SS6.p1.1.m1.1.1.2.cmml">X</mi><mi id="S3.SS6.p1.1.m1.1.1.3" xref="S3.SS6.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><apply id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.1.m1.1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS6.p1.1.m1.1.1.2.cmml" xref="S3.SS6.p1.1.m1.1.1.2">𝑋</ci><ci id="S3.SS6.p1.1.m1.1.1.3.cmml" xref="S3.SS6.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">X_{i}</annotation></semantics></math>는 이산 양자화 레벨을 나타내고, <math alttext="\Delta_{i}" class="ltx_Math" display="inline" id="S3.SS6.p1.2.m2.1"><semantics id="S3.SS6.p1.2.m2.1a"><msub id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml"><mi id="S3.SS6.p1.2.m2.1.1.2" mathvariant="normal" xref="S3.SS6.p1.2.m2.1.1.2.cmml">Δ</mi><mi id="S3.SS6.p1.2.m2.1.1.3" xref="S3.SS6.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><apply id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.2.m2.1.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS6.p1.2.m2.1.1.2.cmml" xref="S3.SS6.p1.2.m2.1.1.2">Δ</ci><ci id="S3.SS6.p1.2.m2.1.1.3.cmml" xref="S3.SS6.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">\Delta_{i}</annotation></semantics></math> 양자화 단계(임계값):</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.2" class="ltx_Math" alttext="\small Q(r)=X_{i},\leavevmode\nobreak\ \mathrm{if}\leavevmode\nobreak\ r\in[\Delta_{i},\Delta_{i+1})." display="block"><semantics id="S3.E6.m1.2a"><mrow id="S3.E6.m1.2.2.1"><mrow id="S3.E6.m1.2.2.1.1.2" xref="S3.E6.m1.2.2.1.1.3.cmml"><mrow id="S3.E6.m1.2.2.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E6.m1.2.2.1.1.1.1.2" xref="S3.E6.m1.2.2.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.1.1.2.2" xref="S3.E6.m1.2.2.1.1.1.1.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.1.1.1.2.1" xref="S3.E6.m1.2.2.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.E6.m1.2.2.1.1.1.1.2.3.2" xref="S3.E6.m1.2.2.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E6.m1.2.2.1.1.1.1.2.3.2.1" xref="S3.E6.m1.2.2.1.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">r</mi><mo maxsize="90%" minsize="90%" id="S3.E6.m1.2.2.1.1.1.1.2.3.2.2" xref="S3.E6.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E6.m1.2.2.1.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.1.cmml">=</mo><msub id="S3.E6.m1.2.2.1.1.1.1.3" xref="S3.E6.m1.2.2.1.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.1.1.3.2" xref="S3.E6.m1.2.2.1.1.1.1.3.2.cmml">X</mi><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.1.1.3.3" xref="S3.E6.m1.2.2.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo mathsize="90%" rspace="0.617em" id="S3.E6.m1.2.2.1.1.2.3" xref="S3.E6.m1.2.2.1.1.3a.cmml">,</mo><mrow id="S3.E6.m1.2.2.1.1.2.2" xref="S3.E6.m1.2.2.1.1.2.2.cmml"><mrow id="S3.E6.m1.2.2.1.1.2.2.4" xref="S3.E6.m1.2.2.1.1.2.2.4.cmml"><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.4.2" xref="S3.E6.m1.2.2.1.1.2.2.4.2.cmml">if</mi><mo lspace="0.450em" rspace="0em" id="S3.E6.m1.2.2.1.1.2.2.4.1" xref="S3.E6.m1.2.2.1.1.2.2.4.1.cmml">​</mo><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.4.3" xref="S3.E6.m1.2.2.1.1.2.2.4.3.cmml">r</mi></mrow><mo mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.3" xref="S3.E6.m1.2.2.1.1.2.2.3.cmml">∈</mo><mrow id="S3.E6.m1.2.2.1.1.2.2.2.2" xref="S3.E6.m1.2.2.1.1.2.2.2.3.cmml"><mo maxsize="90%" minsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.3" xref="S3.E6.m1.2.2.1.1.2.2.2.3.cmml">[</mo><msub id="S3.E6.m1.2.2.1.1.2.2.1.1.1" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1.cmml"><mi mathsize="90%" mathvariant="normal" id="S3.E6.m1.2.2.1.1.2.2.1.1.1.2" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1.2.cmml">Δ</mi><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.1.1.1.3" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1.3.cmml">i</mi></msub><mo mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.4" xref="S3.E6.m1.2.2.1.1.2.2.2.3.cmml">,</mo><msub id="S3.E6.m1.2.2.1.1.2.2.2.2.2" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.cmml"><mi mathsize="90%" mathvariant="normal" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.2" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.2.cmml">Δ</mi><mrow id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.cmml"><mi mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.2" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.2.cmml">i</mi><mo mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.1" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.1.cmml">+</mo><mn mathsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.3" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.3.cmml">1</mn></mrow></msub><mo maxsize="90%" minsize="90%" id="S3.E6.m1.2.2.1.1.2.2.2.2.5" xref="S3.E6.m1.2.2.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E6.m1.2.2.1.2">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.2b"><apply id="S3.E6.m1.2.2.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.3a.cmml" xref="S3.E6.m1.2.2.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E6.m1.2.2.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1"><eq id="S3.E6.m1.2.2.1.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1"></eq><apply id="S3.E6.m1.2.2.1.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2"><times id="S3.E6.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.1"></times><ci id="S3.E6.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.2">𝑄</ci><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">𝑟</ci></apply><apply id="S3.E6.m1.2.2.1.1.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.3.2">𝑋</ci><ci id="S3.E6.m1.2.2.1.1.1.1.3.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S3.E6.m1.2.2.1.1.2.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2"><in id="S3.E6.m1.2.2.1.1.2.2.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.3"></in><apply id="S3.E6.m1.2.2.1.1.2.2.4.cmml" xref="S3.E6.m1.2.2.1.1.2.2.4"><times id="S3.E6.m1.2.2.1.1.2.2.4.1.cmml" xref="S3.E6.m1.2.2.1.1.2.2.4.1"></times><ci id="S3.E6.m1.2.2.1.1.2.2.4.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2.4.2">if</ci><ci id="S3.E6.m1.2.2.1.1.2.2.4.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.4.3">𝑟</ci></apply><interval closure="closed-open" id="S3.E6.m1.2.2.1.1.2.2.2.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2"><apply id="S3.E6.m1.2.2.1.1.2.2.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.2.2.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.2.2.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1.2">Δ</ci><ci id="S3.E6.m1.2.2.1.1.2.2.1.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.1.1.1.3">𝑖</ci></apply><apply id="S3.E6.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.1.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.2.2.2.2.2.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.2">Δ</ci><apply id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3"><plus id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.1.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.1"></plus><ci id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.2.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.3.cmml" xref="S3.E6.m1.2.2.1.1.2.2.2.2.2.3.3">1</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.2c">\small Q(r)=X_{i},\leavevmode\nobreak\ \mathrm{if}\leavevmode\nobreak\ r\in[\Delta_{i},\Delta_{i+1}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS6.p1.9" class="ltx_p">Specifically, when the value of a real number <math id="S3.SS6.p1.3.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS6.p1.3.m1.1a"><mi id="S3.SS6.p1.3.m1.1.1" xref="S3.SS6.p1.3.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.3.m1.1b"><ci id="S3.SS6.p1.3.m1.1.1.cmml" xref="S3.SS6.p1.3.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.3.m1.1c">r</annotation></semantics></math> falls in between the quantization step <math id="S3.SS6.p1.4.m2.1" class="ltx_Math" alttext="\Delta_{i}" display="inline"><semantics id="S3.SS6.p1.4.m2.1a"><msub id="S3.SS6.p1.4.m2.1.1" xref="S3.SS6.p1.4.m2.1.1.cmml"><mi mathvariant="normal" id="S3.SS6.p1.4.m2.1.1.2" xref="S3.SS6.p1.4.m2.1.1.2.cmml">Δ</mi><mi id="S3.SS6.p1.4.m2.1.1.3" xref="S3.SS6.p1.4.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.4.m2.1b"><apply id="S3.SS6.p1.4.m2.1.1.cmml" xref="S3.SS6.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.4.m2.1.1.1.cmml" xref="S3.SS6.p1.4.m2.1.1">subscript</csymbol><ci id="S3.SS6.p1.4.m2.1.1.2.cmml" xref="S3.SS6.p1.4.m2.1.1.2">Δ</ci><ci id="S3.SS6.p1.4.m2.1.1.3.cmml" xref="S3.SS6.p1.4.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.4.m2.1c">\Delta_{i}</annotation></semantics></math> and <math id="S3.SS6.p1.5.m3.1" class="ltx_Math" alttext="\Delta_{i+1}" display="inline"><semantics id="S3.SS6.p1.5.m3.1a"><msub id="S3.SS6.p1.5.m3.1.1" xref="S3.SS6.p1.5.m3.1.1.cmml"><mi mathvariant="normal" id="S3.SS6.p1.5.m3.1.1.2" xref="S3.SS6.p1.5.m3.1.1.2.cmml">Δ</mi><mrow id="S3.SS6.p1.5.m3.1.1.3" xref="S3.SS6.p1.5.m3.1.1.3.cmml"><mi id="S3.SS6.p1.5.m3.1.1.3.2" xref="S3.SS6.p1.5.m3.1.1.3.2.cmml">i</mi><mo id="S3.SS6.p1.5.m3.1.1.3.1" xref="S3.SS6.p1.5.m3.1.1.3.1.cmml">+</mo><mn id="S3.SS6.p1.5.m3.1.1.3.3" xref="S3.SS6.p1.5.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.5.m3.1b"><apply id="S3.SS6.p1.5.m3.1.1.cmml" xref="S3.SS6.p1.5.m3.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.5.m3.1.1.1.cmml" xref="S3.SS6.p1.5.m3.1.1">subscript</csymbol><ci id="S3.SS6.p1.5.m3.1.1.2.cmml" xref="S3.SS6.p1.5.m3.1.1.2">Δ</ci><apply id="S3.SS6.p1.5.m3.1.1.3.cmml" xref="S3.SS6.p1.5.m3.1.1.3"><plus id="S3.SS6.p1.5.m3.1.1.3.1.cmml" xref="S3.SS6.p1.5.m3.1.1.3.1"></plus><ci id="S3.SS6.p1.5.m3.1.1.3.2.cmml" xref="S3.SS6.p1.5.m3.1.1.3.2">𝑖</ci><cn type="integer" id="S3.SS6.p1.5.m3.1.1.3.3.cmml" xref="S3.SS6.p1.5.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.5.m3.1c">\Delta_{i+1}</annotation></semantics></math>, quantizer <math id="S3.SS6.p1.6.m4.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS6.p1.6.m4.1a"><mi id="S3.SS6.p1.6.m4.1.1" xref="S3.SS6.p1.6.m4.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.6.m4.1b"><ci id="S3.SS6.p1.6.m4.1.1.cmml" xref="S3.SS6.p1.6.m4.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.6.m4.1c">Q</annotation></semantics></math> projects it to the corresponding quantization level <math id="S3.SS6.p1.7.m5.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S3.SS6.p1.7.m5.1a"><msub id="S3.SS6.p1.7.m5.1.1" xref="S3.SS6.p1.7.m5.1.1.cmml"><mi id="S3.SS6.p1.7.m5.1.1.2" xref="S3.SS6.p1.7.m5.1.1.2.cmml">X</mi><mi id="S3.SS6.p1.7.m5.1.1.3" xref="S3.SS6.p1.7.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.7.m5.1b"><apply id="S3.SS6.p1.7.m5.1.1.cmml" xref="S3.SS6.p1.7.m5.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.7.m5.1.1.1.cmml" xref="S3.SS6.p1.7.m5.1.1">subscript</csymbol><ci id="S3.SS6.p1.7.m5.1.1.2.cmml" xref="S3.SS6.p1.7.m5.1.1.2">𝑋</ci><ci id="S3.SS6.p1.7.m5.1.1.3.cmml" xref="S3.SS6.p1.7.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.7.m5.1c">X_{i}</annotation></semantics></math>. Note that neither <math id="S3.SS6.p1.8.m6.1" class="ltx_Math" alttext="X_{i}" display="inline"><semantics id="S3.SS6.p1.8.m6.1a"><msub id="S3.SS6.p1.8.m6.1.1" xref="S3.SS6.p1.8.m6.1.1.cmml"><mi id="S3.SS6.p1.8.m6.1.1.2" xref="S3.SS6.p1.8.m6.1.1.2.cmml">X</mi><mi id="S3.SS6.p1.8.m6.1.1.3" xref="S3.SS6.p1.8.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.8.m6.1b"><apply id="S3.SS6.p1.8.m6.1.1.cmml" xref="S3.SS6.p1.8.m6.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.8.m6.1.1.1.cmml" xref="S3.SS6.p1.8.m6.1.1">subscript</csymbol><ci id="S3.SS6.p1.8.m6.1.1.2.cmml" xref="S3.SS6.p1.8.m6.1.1.2">𝑋</ci><ci id="S3.SS6.p1.8.m6.1.1.3.cmml" xref="S3.SS6.p1.8.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.8.m6.1c">X_{i}</annotation></semantics></math>’s nor <math id="S3.SS6.p1.9.m7.1" class="ltx_Math" alttext="\Delta_{i}" display="inline"><semantics id="S3.SS6.p1.9.m7.1a"><msub id="S3.SS6.p1.9.m7.1.1" xref="S3.SS6.p1.9.m7.1.1.cmml"><mi mathvariant="normal" id="S3.SS6.p1.9.m7.1.1.2" xref="S3.SS6.p1.9.m7.1.1.2.cmml">Δ</mi><mi id="S3.SS6.p1.9.m7.1.1.3" xref="S3.SS6.p1.9.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.9.m7.1b"><apply id="S3.SS6.p1.9.m7.1.1.cmml" xref="S3.SS6.p1.9.m7.1.1"><csymbol cd="ambiguous" id="S3.SS6.p1.9.m7.1.1.1.cmml" xref="S3.SS6.p1.9.m7.1.1">subscript</csymbol><ci id="S3.SS6.p1.9.m7.1.1.2.cmml" xref="S3.SS6.p1.9.m7.1.1.2">Δ</ci><ci id="S3.SS6.p1.9.m7.1.1.3.cmml" xref="S3.SS6.p1.9.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.9.m7.1c">\Delta_{i}</annotation></semantics></math>’s are uniformly spaced.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS6.p2.8">비균일 양자화는 고정된 비트-폭에 대해 더 높은 정확도를 달성할 수 있는데, 그 이유는 중요한 값 영역들에 더 집중하거나 적절한 동적 범위들을 발견함으로써 분포들을 더 잘 포착할 수 있기 때문이다. 예를 들어, 긴 꼬리 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib12" title="">12</a>, <a class="ltx_ref" href="#bib.bib25" title="">25</a>, <a class="ltx_ref" href="#bib.bib147" title="">147</a>, <a class="ltx_ref" href="#bib.bib179" title="">179</a>, <a class="ltx_ref" href="#bib.bib115" title="">115</a>, <a class="ltx_ref" href="#bib.bib61" title="">61</a>]</cite>를 포함하는 가중치와 활성화의 종 모양 분포를 위해 많은 비균일 양자화 방법이 설계되었다. 전형적인 규칙 기반 비균일 양자화는 로그 분포 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib179" title="">179</a>, <a class="ltx_ref" href="#bib.bib283" title="">283</a>]</cite>를 사용하는 것인데, 여기서 양자화 단계 및 레벨은 선형이 아닌 지수적으로 증가한다. 또 다른 인기 있는 브랜치는 <span class="ltx_text ltx_font_italic" id="S3.SS6.p2.8.1">binary-code-based</span> quantization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib118" title="">118</a>, <a class="ltx_ref" href="#bib.bib258" title="">258</a>, <a class="ltx_ref" href="#bib.bib276" title="">276</a>, <a class="ltx_ref" href="#bib.bib78" title="">78</a>, <a class="ltx_ref" href="#bib.bib107" title="">107</a>]</cite>이고, 여기서 실수 벡터 <math alttext="\mathbf{r}\in\mathbb{R}^{n}" class="ltx_Math" display="inline" id="S3.SS6.p2.1.m1.1"><semantics id="S3.SS6.p2.1.m1.1a"><mrow id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml"><mi id="S3.SS6.p2.1.m1.1.1.2" xref="S3.SS6.p2.1.m1.1.1.2.cmml">𝐫</mi><mo id="S3.SS6.p2.1.m1.1.1.1" xref="S3.SS6.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS6.p2.1.m1.1.1.3" xref="S3.SS6.p2.1.m1.1.1.3.cmml"><mi id="S3.SS6.p2.1.m1.1.1.3.2" xref="S3.SS6.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS6.p2.1.m1.1.1.3.3" xref="S3.SS6.p2.1.m1.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.1.m1.1b"><apply id="S3.SS6.p2.1.m1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1"><in id="S3.SS6.p2.1.m1.1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1.1"></in><ci id="S3.SS6.p2.1.m1.1.1.2.cmml" xref="S3.SS6.p2.1.m1.1.1.2">𝐫</ci><apply id="S3.SS6.p2.1.m1.1.1.3.cmml" xref="S3.SS6.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p2.1.m1.1.1.3.1.cmml" xref="S3.SS6.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS6.p2.1.m1.1.1.3.2.cmml" xref="S3.SS6.p2.1.m1.1.1.3.2">ℝ</ci><ci id="S3.SS6.p2.1.m1.1.1.3.3.cmml" xref="S3.SS6.p2.1.m1.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.1.m1.1c">\mathbf{r}\in\mathbb{R}^{n}</annotation></semantics></math>는 스케일링 팩터 <math alttext="\alpha_{i}\in\mathbb{R}" class="ltx_Math" display="inline" id="S3.SS6.p2.4.m4.1"><semantics id="S3.SS6.p2.4.m4.1a"><mrow id="S3.SS6.p2.4.m4.1.1" xref="S3.SS6.p2.4.m4.1.1.cmml"><msub id="S3.SS6.p2.4.m4.1.1.2" xref="S3.SS6.p2.4.m4.1.1.2.cmml"><mi id="S3.SS6.p2.4.m4.1.1.2.2" xref="S3.SS6.p2.4.m4.1.1.2.2.cmml">α</mi><mi id="S3.SS6.p2.4.m4.1.1.2.3" xref="S3.SS6.p2.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS6.p2.4.m4.1.1.1" xref="S3.SS6.p2.4.m4.1.1.1.cmml">∈</mo><mi id="S3.SS6.p2.4.m4.1.1.3" xref="S3.SS6.p2.4.m4.1.1.3.cmml">ℝ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.4.m4.1b"><apply id="S3.SS6.p2.4.m4.1.1.cmml" xref="S3.SS6.p2.4.m4.1.1"><in id="S3.SS6.p2.4.m4.1.1.1.cmml" xref="S3.SS6.p2.4.m4.1.1.1"></in><apply id="S3.SS6.p2.4.m4.1.1.2.cmml" xref="S3.SS6.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p2.4.m4.1.1.2.1.cmml" xref="S3.SS6.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS6.p2.4.m4.1.1.2.2.cmml" xref="S3.SS6.p2.4.m4.1.1.2.2">𝛼</ci><ci id="S3.SS6.p2.4.m4.1.1.2.3.cmml" xref="S3.SS6.p2.4.m4.1.1.2.3">𝑖</ci></apply><ci id="S3.SS6.p2.4.m4.1.1.3.cmml" xref="S3.SS6.p2.4.m4.1.1.3">ℝ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.4.m4.1c">\alpha_{i}\in\mathbb{R}</annotation></semantics></math>와 이진 벡터 <math alttext="\mathbf{b}_{i}\in\{-1,+1\}^{n}" class="ltx_Math" display="inline" id="S3.SS6.p2.5.m5.2"><semantics id="S3.SS6.p2.5.m5.2a"><mrow id="S3.SS6.p2.5.m5.2.2" xref="S3.SS6.p2.5.m5.2.2.cmml"><msub id="S3.SS6.p2.5.m5.2.2.4" xref="S3.SS6.p2.5.m5.2.2.4.cmml"><mi id="S3.SS6.p2.5.m5.2.2.4.2" xref="S3.SS6.p2.5.m5.2.2.4.2.cmml">𝐛</mi><mi id="S3.SS6.p2.5.m5.2.2.4.3" xref="S3.SS6.p2.5.m5.2.2.4.3.cmml">i</mi></msub><mo id="S3.SS6.p2.5.m5.2.2.3" xref="S3.SS6.p2.5.m5.2.2.3.cmml">∈</mo><msup id="S3.SS6.p2.5.m5.2.2.2" xref="S3.SS6.p2.5.m5.2.2.2.cmml"><mrow id="S3.SS6.p2.5.m5.2.2.2.2.2" xref="S3.SS6.p2.5.m5.2.2.2.2.3.cmml"><mo id="S3.SS6.p2.5.m5.2.2.2.2.2.3" stretchy="false" xref="S3.SS6.p2.5.m5.2.2.2.2.3.cmml">{</mo><mrow id="S3.SS6.p2.5.m5.1.1.1.1.1.1" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1.cmml"><mo id="S3.SS6.p2.5.m5.1.1.1.1.1.1a" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1.cmml">−</mo><mn id="S3.SS6.p2.5.m5.1.1.1.1.1.1.2" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1.2.cmml">1</mn></mrow><mo id="S3.SS6.p2.5.m5.2.2.2.2.2.4" xref="S3.SS6.p2.5.m5.2.2.2.2.3.cmml">,</mo><mrow id="S3.SS6.p2.5.m5.2.2.2.2.2.2" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2.cmml"><mo id="S3.SS6.p2.5.m5.2.2.2.2.2.2a" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2.cmml">+</mo><mn id="S3.SS6.p2.5.m5.2.2.2.2.2.2.2" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2.2.cmml">1</mn></mrow><mo id="S3.SS6.p2.5.m5.2.2.2.2.2.5" stretchy="false" xref="S3.SS6.p2.5.m5.2.2.2.2.3.cmml">}</mo></mrow><mi id="S3.SS6.p2.5.m5.2.2.2.4" xref="S3.SS6.p2.5.m5.2.2.2.4.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.5.m5.2b"><apply id="S3.SS6.p2.5.m5.2.2.cmml" xref="S3.SS6.p2.5.m5.2.2"><in id="S3.SS6.p2.5.m5.2.2.3.cmml" xref="S3.SS6.p2.5.m5.2.2.3"></in><apply id="S3.SS6.p2.5.m5.2.2.4.cmml" xref="S3.SS6.p2.5.m5.2.2.4"><csymbol cd="ambiguous" id="S3.SS6.p2.5.m5.2.2.4.1.cmml" xref="S3.SS6.p2.5.m5.2.2.4">subscript</csymbol><ci id="S3.SS6.p2.5.m5.2.2.4.2.cmml" xref="S3.SS6.p2.5.m5.2.2.4.2">𝐛</ci><ci id="S3.SS6.p2.5.m5.2.2.4.3.cmml" xref="S3.SS6.p2.5.m5.2.2.4.3">𝑖</ci></apply><apply id="S3.SS6.p2.5.m5.2.2.2.cmml" xref="S3.SS6.p2.5.m5.2.2.2"><csymbol cd="ambiguous" id="S3.SS6.p2.5.m5.2.2.2.3.cmml" xref="S3.SS6.p2.5.m5.2.2.2">superscript</csymbol><set id="S3.SS6.p2.5.m5.2.2.2.2.3.cmml" xref="S3.SS6.p2.5.m5.2.2.2.2.2"><apply id="S3.SS6.p2.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1"><minus id="S3.SS6.p2.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1"></minus><cn id="S3.SS6.p2.5.m5.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.SS6.p2.5.m5.1.1.1.1.1.1.2">1</cn></apply><apply id="S3.SS6.p2.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2"><plus id="S3.SS6.p2.5.m5.2.2.2.2.2.2.1.cmml" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2"></plus><cn id="S3.SS6.p2.5.m5.2.2.2.2.2.2.2.cmml" type="integer" xref="S3.SS6.p2.5.m5.2.2.2.2.2.2.2">1</cn></apply></set><ci id="S3.SS6.p2.5.m5.2.2.2.4.cmml" xref="S3.SS6.p2.5.m5.2.2.2.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.5.m5.2c">\mathbf{b}_{i}\in\{-1,+1\}^{n}</annotation></semantics></math>를 나타내어 <math alttext="m" class="ltx_Math" display="inline" id="S3.SS6.p2.2.m2.1"><semantics id="S3.SS6.p2.2.m2.1a"><mi id="S3.SS6.p2.2.m2.1.1" xref="S3.SS6.p2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.2.m2.1b"><ci id="S3.SS6.p2.2.m2.1.1.cmml" xref="S3.SS6.p2.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.2.m2.1c">m</annotation></semantics></math> 이진 벡터로 양자화된다. <math alttext="\mathbf{r}" class="ltx_Math" display="inline" id="S3.SS6.p2.6.m6.1"><semantics id="S3.SS6.p2.6.m6.1a"><mi id="S3.SS6.p2.6.m6.1.1" xref="S3.SS6.p2.6.m6.1.1.cmml">𝐫</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.6.m6.1b"><ci id="S3.SS6.p2.6.m6.1.1.cmml" xref="S3.SS6.p2.6.m6.1.1">𝐫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.6.m6.1c">\mathbf{r}</annotation></semantics></math>와 <math alttext="\sum_{i=1}^{m}\alpha_{i}\mathbf{b}_{i}" class="ltx_Math" display="inline" id="S3.SS6.p2.7.m7.1"><semantics id="S3.SS6.p2.7.m7.1a"><mrow id="S3.SS6.p2.7.m7.1.1" xref="S3.SS6.p2.7.m7.1.1.cmml"><msubsup id="S3.SS6.p2.7.m7.1.1.1" xref="S3.SS6.p2.7.m7.1.1.1.cmml"><mo id="S3.SS6.p2.7.m7.1.1.1.2.2" xref="S3.SS6.p2.7.m7.1.1.1.2.2.cmml">∑</mo><mrow id="S3.SS6.p2.7.m7.1.1.1.2.3" xref="S3.SS6.p2.7.m7.1.1.1.2.3.cmml"><mi id="S3.SS6.p2.7.m7.1.1.1.2.3.2" xref="S3.SS6.p2.7.m7.1.1.1.2.3.2.cmml">i</mi><mo id="S3.SS6.p2.7.m7.1.1.1.2.3.1" xref="S3.SS6.p2.7.m7.1.1.1.2.3.1.cmml">=</mo><mn id="S3.SS6.p2.7.m7.1.1.1.2.3.3" xref="S3.SS6.p2.7.m7.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S3.SS6.p2.7.m7.1.1.1.3" xref="S3.SS6.p2.7.m7.1.1.1.3.cmml">m</mi></msubsup><mrow id="S3.SS6.p2.7.m7.1.1.2" xref="S3.SS6.p2.7.m7.1.1.2.cmml"><msub id="S3.SS6.p2.7.m7.1.1.2.2" xref="S3.SS6.p2.7.m7.1.1.2.2.cmml"><mi id="S3.SS6.p2.7.m7.1.1.2.2.2" xref="S3.SS6.p2.7.m7.1.1.2.2.2.cmml">α</mi><mi id="S3.SS6.p2.7.m7.1.1.2.2.3" xref="S3.SS6.p2.7.m7.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS6.p2.7.m7.1.1.2.1" lspace="0em" rspace="0em" xref="S3.SS6.p2.7.m7.1.1.2.1.cmml">​</mo><msub id="S3.SS6.p2.7.m7.1.1.2.3" xref="S3.SS6.p2.7.m7.1.1.2.3.cmml"><mi id="S3.SS6.p2.7.m7.1.1.2.3.2" xref="S3.SS6.p2.7.m7.1.1.2.3.2.cmml">𝐛</mi><mi id="S3.SS6.p2.7.m7.1.1.2.3.3" xref="S3.SS6.p2.7.m7.1.1.2.3.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.7.m7.1b"><apply id="S3.SS6.p2.7.m7.1.1.cmml" xref="S3.SS6.p2.7.m7.1.1"><apply id="S3.SS6.p2.7.m7.1.1.1.cmml" xref="S3.SS6.p2.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p2.7.m7.1.1.1.1.cmml" xref="S3.SS6.p2.7.m7.1.1.1">superscript</csymbol><apply id="S3.SS6.p2.7.m7.1.1.1.2.cmml" xref="S3.SS6.p2.7.m7.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p2.7.m7.1.1.1.2.1.cmml" xref="S3.SS6.p2.7.m7.1.1.1">subscript</csymbol><sum id="S3.SS6.p2.7.m7.1.1.1.2.2.cmml" xref="S3.SS6.p2.7.m7.1.1.1.2.2"></sum><apply id="S3.SS6.p2.7.m7.1.1.1.2.3.cmml" xref="S3.SS6.p2.7.m7.1.1.1.2.3"><eq id="S3.SS6.p2.7.m7.1.1.1.2.3.1.cmml" xref="S3.SS6.p2.7.m7.1.1.1.2.3.1"></eq><ci id="S3.SS6.p2.7.m7.1.1.1.2.3.2.cmml" xref="S3.SS6.p2.7.m7.1.1.1.2.3.2">𝑖</ci><cn id="S3.SS6.p2.7.m7.1.1.1.2.3.3.cmml" type="integer" xref="S3.SS6.p2.7.m7.1.1.1.2.3.3">1</cn></apply></apply><ci id="S3.SS6.p2.7.m7.1.1.1.3.cmml" xref="S3.SS6.p2.7.m7.1.1.1.3">𝑚</ci></apply><apply id="S3.SS6.p2.7.m7.1.1.2.cmml" xref="S3.SS6.p2.7.m7.1.1.2"><times id="S3.SS6.p2.7.m7.1.1.2.1.cmml" xref="S3.SS6.p2.7.m7.1.1.2.1"></times><apply id="S3.SS6.p2.7.m7.1.1.2.2.cmml" xref="S3.SS6.p2.7.m7.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS6.p2.7.m7.1.1.2.2.1.cmml" xref="S3.SS6.p2.7.m7.1.1.2.2">subscript</csymbol><ci id="S3.SS6.p2.7.m7.1.1.2.2.2.cmml" xref="S3.SS6.p2.7.m7.1.1.2.2.2">𝛼</ci><ci id="S3.SS6.p2.7.m7.1.1.2.2.3.cmml" xref="S3.SS6.p2.7.m7.1.1.2.2.3">𝑖</ci></apply><apply id="S3.SS6.p2.7.m7.1.1.2.3.cmml" xref="S3.SS6.p2.7.m7.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS6.p2.7.m7.1.1.2.3.1.cmml" xref="S3.SS6.p2.7.m7.1.1.2.3">subscript</csymbol><ci id="S3.SS6.p2.7.m7.1.1.2.3.2.cmml" xref="S3.SS6.p2.7.m7.1.1.2.3.2">𝐛</ci><ci id="S3.SS6.p2.7.m7.1.1.2.3.3.cmml" xref="S3.SS6.p2.7.m7.1.1.2.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.7.m7.1c">\sum_{i=1}^{m}\alpha_{i}\mathbf{b}_{i}</annotation></semantics></math> 사이의 오차를 최소화하기 위한 closed-form 솔루션이 없기 때문에, 기존 연구는 휴리스틱 솔루션에 의존하고 있다. 양자화기를 더 개선하기 위해, 보다 최근의 작업 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib234" title="">234</a>, <a class="ltx_ref" href="#bib.bib78" title="">78</a>, <a class="ltx_ref" href="#bib.bib258" title="">258</a>]</cite>는 최적화 문제로서 불균일 양자화를 공식화한다. 식에서 보는 바와 같이. <a class="ltx_ref" href="#S3.E7" title="In III-F Non-Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">7</span></a>, 양자화기 <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS6.p2.8.m8.1"><semantics id="S3.SS6.p2.8.m8.1a"><mi id="S3.SS6.p2.8.m8.1.1" xref="S3.SS6.p2.8.m8.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.8.m8.1b"><ci id="S3.SS6.p2.8.m8.1.1.cmml" xref="S3.SS6.p2.8.m8.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.8.m8.1c">Q</annotation></semantics></math>에서의 양자화 단계/레벨은 원래의 텐서와 양자화된 대응물의 차이를 최소화하도록 조정된다.</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.2" class="ltx_Math" alttext="\small\min_{Q}\|Q(r)-r\|^{2}" display="block"><semantics id="S3.E7.m1.2a"><mrow id="S3.E7.m1.2.2" xref="S3.E7.m1.2.2.cmml"><munder id="S3.E7.m1.2.2.2" xref="S3.E7.m1.2.2.2.cmml"><mi mathsize="90%" id="S3.E7.m1.2.2.2.2" xref="S3.E7.m1.2.2.2.2.cmml">min</mi><mi mathsize="90%" id="S3.E7.m1.2.2.2.3" xref="S3.E7.m1.2.2.2.3.cmml">Q</mi></munder><mo id="S3.E7.m1.2.2a" xref="S3.E7.m1.2.2.cmml">⁡</mo><msup id="S3.E7.m1.2.2.1" xref="S3.E7.m1.2.2.1.cmml"><mrow id="S3.E7.m1.2.2.1.1.1" xref="S3.E7.m1.2.2.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E7.m1.2.2.1.1.1.2" xref="S3.E7.m1.2.2.1.1.2.1.cmml">‖</mo><mrow id="S3.E7.m1.2.2.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.cmml"><mrow id="S3.E7.m1.2.2.1.1.1.1.2" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E7.m1.2.2.1.1.1.1.2.2" xref="S3.E7.m1.2.2.1.1.1.1.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.2.2.1.1.1.1.2.1" xref="S3.E7.m1.2.2.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.E7.m1.2.2.1.1.1.1.2.3.2" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E7.m1.2.2.1.1.1.1.2.3.2.1" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml">r</mi><mo maxsize="90%" minsize="90%" id="S3.E7.m1.2.2.1.1.1.1.2.3.2.2" xref="S3.E7.m1.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E7.m1.2.2.1.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.1.cmml">−</mo><mi mathsize="90%" id="S3.E7.m1.2.2.1.1.1.1.3" xref="S3.E7.m1.2.2.1.1.1.1.3.cmml">r</mi></mrow><mo maxsize="90%" minsize="90%" id="S3.E7.m1.2.2.1.1.1.3" xref="S3.E7.m1.2.2.1.1.2.1.cmml">‖</mo></mrow><mn mathsize="90%" id="S3.E7.m1.2.2.1.3" xref="S3.E7.m1.2.2.1.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.2b"><apply id="S3.E7.m1.2.2.cmml" xref="S3.E7.m1.2.2"><apply id="S3.E7.m1.2.2.2.cmml" xref="S3.E7.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.2.1.cmml" xref="S3.E7.m1.2.2.2">subscript</csymbol><min id="S3.E7.m1.2.2.2.2.cmml" xref="S3.E7.m1.2.2.2.2"></min><ci id="S3.E7.m1.2.2.2.3.cmml" xref="S3.E7.m1.2.2.2.3">𝑄</ci></apply><apply id="S3.E7.m1.2.2.1.cmml" xref="S3.E7.m1.2.2.1"><csymbol cd="ambiguous" id="S3.E7.m1.2.2.1.2.cmml" xref="S3.E7.m1.2.2.1">superscript</csymbol><apply id="S3.E7.m1.2.2.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1"><csymbol cd="latexml" id="S3.E7.m1.2.2.1.1.2.1.cmml" xref="S3.E7.m1.2.2.1.1.1.2">norm</csymbol><apply id="S3.E7.m1.2.2.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1"><minus id="S3.E7.m1.2.2.1.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1.1"></minus><apply id="S3.E7.m1.2.2.1.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2"><times id="S3.E7.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2.1"></times><ci id="S3.E7.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E7.m1.2.2.1.1.1.1.2.2">𝑄</ci><ci id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1">𝑟</ci></apply><ci id="S3.E7.m1.2.2.1.1.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.1.1.3">𝑟</ci></apply></apply><cn type="integer" id="S3.E7.m1.2.2.1.3.cmml" xref="S3.E7.m1.2.2.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.2c">\small\min_{Q}\|Q(r)-r\|^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS6.p2.9">나아가, 양자화기 자체는 또한 모델 파라미터들과 공동으로 트레이닝될 수 있다. 이러한 방법을 학습 가능한 양자화기라고 하며, 양자화 단계/레벨은 일반적으로 반복 최적화 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib276" title="">276</a>, <a class="ltx_ref" href="#bib.bib258" title="">258</a>]</cite> 또는 gradient descent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib158" title="">158</a>, <a class="ltx_ref" href="#bib.bib125" title="">125</a>, <a class="ltx_ref" href="#bib.bib264" title="">264</a>]</cite>로 훈련된다.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS6.p3.1">규칙 기반 및 최적화 기반 비균일 양자화에 더하여, 클러스터링은 또한 양자화로 인한 정보 손실을 경감시키는데 유익할 수 있다. 일부 작업 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib74" title="">74</a>, <a class="ltx_ref" href="#bib.bib256" title="">256</a>]</cite>는 다른 텐서에 k-means를 사용하여 양자화 단계 및 수준을 결정하는 반면, 다른 작업 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib38" title="">38</a>]</cite>는 성능 손실을 최소화하기 위해 가중치에 대한 헤시안 가중치 k-means 클러스터링을 적용한다. 더 자세한 논의는 섹션 <a class="ltx_ref" href="#S4.SS6" title="IV-F Vector Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-F</span></span></a>에서 찾을 수 있다.</p>
</div>
<div id="S3.SS6.p4" class="ltx_para">
<p class="ltx_p" id="S3.SS6.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS6.p4.1.1">Summary (Uniform vs Non-uniform Quantization). </span> 일반적으로, 비균일 양자화는 비트들을 할당하고 파라미터들의 범위를 불균일하게 이산화함으로써, 신호 정보를 더 잘 캡처할 수 있게 한다. 그러나, 불균일한 양자화 방식들은 일반적으로 일반적인 계산 하드웨어, 예를 들어 GPU 및 CPU 상에서 효율적으로 배치되기 어렵다. 이와 같이, 균일 양자화는 그것의 단순성 및 하드웨어로의 효율적인 매핑으로 인해 현재 사실적인 방법이다.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2103.13630/assets/x6.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="165" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 5:</span>Illustration of Quantization-Aware Training procedure, including</figcaption>
the use of Straight Through Estimator (STE).
</figcaption>
</figure>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS7.5.1.1" class="ltx_text">III-G</span> </span><span id="S3.SS7.6.2" class="ltx_text ltx_font_italic">Fine-tuning Methods</span>
</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS7.p1.1">양자화 후에 NN에서 파라미터들을 조정하는 것이 종종 필요하다. 이는 모델을 재트레이닝함으로써 수행될 수 있고, QAT(Quantization-Aware Training)라고 불리는 프로세스, 또는 PTQ(Post-Training Quantization)라고 종종 지칭되는 프로세스, 재트레이닝 없이 수행될 수 있다. 이 두 접근법 사이의 개략적인 비교는 그림 <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‣ Sub-channelwise Quantization ‣ III-E Quantization Granularity ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">4</span></a>에 예시되어 있으며, 아래에서 더 논의된다(이 주제에 대한 보다 상세한 논의를 위해 관심 있는 독자를 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib183" title="">183</a>]</cite>로 참조한다).</p>
</div>
<section id="S3.SS7.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS7.SSS1.5.1.1" class="ltx_text">III-G</span>1 </span>Quantization-Aware Training</h4>

<div id="S3.SS7.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS1.p1.1">훈련된 모델이 주어지면, 양자화는 훈련된 모델 파라미터들에 섭동을 도입할 수 있고, 이는 부동 소수점 정밀도로 훈련되었을 때 그것이 수렴했던 지점으로부터 모델을 밀어낼 수 있다. 모델이 더 나은 손실을 갖는 포인트로 수렴할 수 있도록 양자화된 파라미터로 NN 모델을 재트레이닝함으로써 이를 해결할 수 있다. 한 가지 일반적인 접근법은 QAT(Quantization-Aware Training)를 사용하는 것인데, 여기서 통상적인 순방향 및 역방향 패스는 부동 소수점에서의 양자화된 모델에 대해 수행되지만, 모델 파라미터들은 각각의 구배 업데이트 후에 양자화된다(투영된 구배 하강과 유사하다). 특히, 가중치 업데이트가 부동 소수점 정밀도로 수행된 후에 이 프로젝션을 수행하는 것이 중요하다. 역방향 패스를 부동점으로 수행하는 것은 중요한데, 양자화된 정밀도에서 기울기를 누적하면 특히 낮은 정밀도 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib42" title="">42</a>, <a class="ltx_ref" href="#bib.bib159" title="">159</a>, <a class="ltx_ref" href="#bib.bib107" title="">107</a>, <a class="ltx_ref" href="#bib.bib204" title="">204</a>, <a class="ltx_ref" href="#bib.bib80" title="">80</a>, <a class="ltx_ref" href="#bib.bib81" title="">81</a>, <a class="ltx_ref" href="#bib.bib231" title="">231</a>, <a class="ltx_ref" href="#bib.bib186" title="">186</a>]</cite>에서 오차가 높은 제로-구배 또는 그래디언트가 발생할 수 있기 때문이다.</p>
</div>
<div id="S3.SS7.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS1.p2.1">역전파에서의 중요한 미묘함은 미분 불가능한 양자화 연산자(Eq.<a class="ltx_ref" href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">2</span></a>)가 어떻게 되는 가이다. 가 처리된 것을 특징으로 하는 반도체 소자의 제조 방법. 근사치 없이, 이 연산자의 기울기는 식에서 반올림 연산이기 때문에 거의 모든 곳에서 0이다. <a class="ltx_ref" href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">2</span></a>는 piece-wise flat operator이다. 이를 해결하기 위한 일반적인 접근법은 소위 STE(Straight Through Estimator) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib13" title="">13</a>]</cite>에 의해 이 연산자의 기울기를 근사화하는 것이다. STE는 그림 <a class="ltx_ref" href="#S3.F5" title="Figure 5 ‣ III-F Non-Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">5</span></a>에 예시된 바와 같이, 본질적으로 반올림 연산을 무시하고 아이덴티티 함수로 근사화한다.</p>
</div>
<div id="S3.SS7.SSS1.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS1.p3.1">STE의 대략적인 근사치에도 불구하고, 이진 양자화 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite>와 같은 초저 정밀 양자화를 제외하고는 종종 실제로 잘 작동한다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib271" title="">271</a>]</cite>의 연구는 이러한 현상에 대한 이론적 정당성을 제공하며, STE의 거친 구배 근사가 ( STE의 적절한 선택을 위해) 인구 구배와 상관관계가 있을 수 있음을 발견했다. 역사적 관점에서 STE의 원래 아이디어는 ID 연산자가 이진 뉴런에서 기울기를 근사화하는 데 사용된 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib209" title="">209</a>, <a class="ltx_ref" href="#bib.bib210" title="">210</a>]</cite>의 정액 작업으로 거슬러 올라갈 수 있다는 점에 유의해야 한다.</p>
</div>
<div id="S3.SS7.SSS1.p4" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS1.p4.1">STE가 주류 접근법 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib289" title="">289</a>, <a class="ltx_ref" href="#bib.bib226" title="">226</a>]</cite>이지만 다른 접근법도 문헌 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib31" title="">31</a>, <a class="ltx_ref" href="#bib.bib59" title="">59</a>, <a class="ltx_ref" href="#bib.bib25" title="">25</a>, <a class="ltx_ref" href="#bib.bib164" title="">164</a>, <a class="ltx_ref" href="#bib.bib144" title="">144</a>, <a class="ltx_ref" href="#bib.bib2" title="">2</a>]</cite>에서 탐구되었다. 우리는 먼저 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib13" title="">13</a>]</cite>도 STE의 대안으로 확률론적 뉴런 접근법을 제안한다는 것을 언급해야 한다(이것은 섹션 <a class="ltx_ref" href="#S3.SS8" title="III-H Stochastic Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-H</span></span></a>에서 간략하게 논의된다). 조합 최적화 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib65" title="">65</a>]</cite>, 타겟 전파 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib140" title="">140</a>]</cite>, 또는 Gumbel-softmax <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib116" title="">116</a>]</cite>를 사용하는 다른 접근법도 제안되었다. 다른 종류의 대체 방법은 양자화될 가중치를 강제하기 위해 정규화 연산자를 사용하려고 한다. 이는 식에서 미분 불가능한 양자화 연산자를 사용할 필요성을 제거한다. <a class="ltx_ref" href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">2</span></a>. 이들은 종종 <em class="ltx_emph ltx_font_italic" id="S3.SS7.SSS1.p4.1.1">Non-STE</em> methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib39" title="">39</a>, <a class="ltx_ref" href="#bib.bib8" title="">8</a>, <a class="ltx_ref" href="#bib.bib184" title="">184</a>, <a class="ltx_ref" href="#bib.bib144" title="">144</a>, <a class="ltx_ref" href="#bib.bib99" title="">99</a>, <a class="ltx_ref" href="#bib.bib283" title="">283</a>, <a class="ltx_ref" href="#bib.bib4" title="">4</a>]</cite>로 지칭된다. 이 분야의 최근 연구는 양자화 공식 Eq에서 반올림 연산을 제거하는 ProxQuant<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite>를 포함한다. <a class="ltx_ref" href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">2</span></a> 대신 소위 <em class="ltx_emph ltx_font_italic" id="S3.SS7.SSS1.p4.1.2">W-shape</em>, non-smooth regularization function to enforce the weight to quantized values. 다른 주목할만한 연구는 펄스 트레이닝을 사용하여 불연속점들 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib45" title="">45</a>]</cite>의 도함수를 근사하거나, 양자화된 가중치들을 부동점과 양자화된 파라미터들 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib165" title="">165</a>]</cite>의 어파인 조합으로 대체하는 것을 포함한다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib181" title="">181</a>]</cite>의 최근 연구에서는 AdaRound를 제안한다. AdaRound는 Round-to-nearest 방법의 대안으로 적응적 Rounding 방법이다. 이 분야에서 흥미로운 작업에도 불구하고 이러한 방법은 종종 많은 조정을 필요로 하며 지금까지 STE 접근법이 가장 일반적으로 사용되는 방법이다.</p>
</div>
<div id="S3.SS7.SSS1.p5" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS1.p5.1">모델 매개 변수를 조정하는 것 외에도 일부 선행 연구에서는 QAT 동안 양자화 매개 변수를 학습하는 것이 효과적이라는 것을 발견했다. PACT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib36" title="">36</a>]</cite>는 균일 양자화 하에서 활성화의 클리핑 범위를 학습하는 반면, QIT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib125" title="">125</a>]</cite>는 또한 불균일 양자화 설정으로의 확장으로서 양자화 단계 및 레벨을 학습한다. LSQ<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib56" title="">56</a>]</cite>는 QAT 동안 비음성 활성화(예: ReLU)에 대한 스케일링 인자를 학습하기 위해 새로운 그래디언트 추정치를 도입하고, LSQ+<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite>는 이 아이디어를 음의 값을 생성하는 swish<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib202" title="">202</a>]</cite> 및 h-swish<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib100" title="">100</a>]</cite>와 같은 일반적인 활성화 함수로 더욱 확장한다.</p>
</div>
<div id="S3.SS7.SSS1.p6" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS7.SSS1.p6.1.1">Summary (QAT). </span> QAT는 STE의 대략적인 근사치에도 불구하고 작동하는 것으로 나타났다. 그러나, QAT의 주요 단점은 NN 모델을 재훈련하는 데 드는 계산 비용이다. 이러한 재-트레이닝은 특히 저-비트 정밀도 양자화를 위해 정확도를 복구하기 위해 수백 에포크에 대해 수행될 필요가 있을 수 있다. 양자화된 모델이 장기간 배포될 것이고, 효율성과 정확성이 특히 중요하다면, 재훈련에 대한 이러한 투자는 가치가 있을 것이다. 그러나 일부 모델은 수명이 상대적으로 짧기 때문에 항상 그런 것은 아닙니다. 다음으로, 우리는 이 오버헤드를 갖지 않는 대안적인 접근법에 대해 논의한다.</p>
</div>
</section>
<section id="S3.SS7.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS7.SSS2.5.1.1" class="ltx_text">III-G</span>2 </span>Post-Training Quantization</h4>

<div id="S3.SS7.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS2.p1.1">고가의 QAT 방법의 대안으로 PTQ(Post-Training Quantization)가 있는데, PTQ는 어떠한 미세 조정 없이 양자화 및 가중치 조정을 수행한다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>, <a class="ltx_ref" href="#bib.bib174" title="">174</a>, <a class="ltx_ref" href="#bib.bib40" title="">40</a>, <a class="ltx_ref" href="#bib.bib281" title="">281</a>, <a class="ltx_ref" href="#bib.bib61" title="">61</a>, <a class="ltx_ref" href="#bib.bib60" title="">60</a>, <a class="ltx_ref" href="#bib.bib142" title="">142</a>, <a class="ltx_ref" href="#bib.bib182" title="">182</a>, <a class="ltx_ref" href="#bib.bib24" title="">24</a>, <a class="ltx_ref" href="#bib.bib148" title="">148</a>, <a class="ltx_ref" href="#bib.bib89" title="">89</a>, <a class="ltx_ref" href="#bib.bib68" title="">68</a>, <a class="ltx_ref" href="#bib.bib69" title="">69</a>, <a class="ltx_ref" href="#bib.bib108" title="">108</a>, <a class="ltx_ref" href="#bib.bib223" title="">223</a>]</cite> 이와 같이, PTQ의 오버헤드는 매우 낮고 종종 무시할 수 있다. PTQ는 재학습을 위해 충분한 양의 학습 데이터가 필요한 QAT와 달리 데이터가 제한적이거나 레이블이 지정되지 않은 상황에서 적용할 수 있다는 추가적인 장점이 있다. 그러나, 이것은 특히 저-정밀 양자화의 경우, QAT에 비해 낮은 정확도의 비용으로 종종 온다.</p>
</div>
<div id="S3.SS7.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS2.p2.1">이러한 이유로, PTQ의 정확도 저하를 완화하기 위한 다중 접근법이 제안되었다. 예를 들어, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>, <a class="ltx_ref" href="#bib.bib63" title="">63</a>]</cite>는 그들의 양자화에 따르는 가중치 값들의 평균 및 분산에 내재된 바이어스를 관찰하고 바이어스 보정 방법들을 제안한다; 그리고 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib174" title="">174</a>, <a class="ltx_ref" href="#bib.bib182" title="">182</a>]</cite>는 상이한 층들 또는 채널들 사이에서 가중치 범위들(및 암시적으로 활성화 범위들)을 균등화하는 것이 양자화 에러들을 감소시킬 수 있음을 보여준다. ACIQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>]</cite>는 PTQ에 대한 최적의 클리핑 범위와 채널별 비트폭 설정을 분석적으로 계산한다. ACIQ는 낮은 정확도 저하를 달성할 수 있지만 ACIQ에서 사용되는 채널별 활성화 양자화는 하드웨어에 효율적으로 배치되기 어렵다. 이를 해결하기 위해 OMSE 방법<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib40" title="">40</a>]</cite>는 활성화 시 채널별 양자화를 제거하고 양자화된 텐서와 해당 부동 소수점 텐서 사이의 L2 거리를 최적화하여 PTQ를 수행하는 것을 제안한다. 또한, 이상치가 PTQ에 미치는 악영향을 완화하기 위해, 이상치가 포함된 채널을 중복하고 반으로 나누는 이상치 채널 분할(OCS) 방법이 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib281" title="">281</a>]</cite>에서 제안되었다. 또 다른 주목할 만한 연구로는 AdaRound <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib181" title="">181</a>]</cite>를 들 수 있는데, 이는 양자화를 위한 순진한 라운드-최근접 방법이 반직관적으로 최적이 아닌 해로 귀결될 수 있음을 보여주며, 손실을 더 잘 줄이는 적응적 라운딩 방법을 제안한다. AdaRound는 양자화된 가중치들의 변화를 전체-정밀 대응물로부터 <math alttext="\pm 1" class="ltx_Math" display="inline" id="S3.SS7.SSS2.p2.1.m1.1"><semantics id="S3.SS7.SSS2.p2.1.m1.1a"><mrow id="S3.SS7.SSS2.p2.1.m1.1.1" xref="S3.SS7.SSS2.p2.1.m1.1.1.cmml"><mo id="S3.SS7.SSS2.p2.1.m1.1.1a" xref="S3.SS7.SSS2.p2.1.m1.1.1.cmml">±</mo><mn id="S3.SS7.SSS2.p2.1.m1.1.1.2" xref="S3.SS7.SSS2.p2.1.m1.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS7.SSS2.p2.1.m1.1b"><apply id="S3.SS7.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS7.SSS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS7.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS7.SSS2.p2.1.m1.1.1">plus-or-minus</csymbol><cn id="S3.SS7.SSS2.p2.1.m1.1.1.2.cmml" type="integer" xref="S3.SS7.SSS2.p2.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.SSS2.p2.1.m1.1c">\pm 1</annotation></semantics></math> 이내로 제한하는 반면, AdaQuant <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib108" title="">108</a>]</cite>는 양자화된 가중치들이 필요에 따라 변화하도록 하는 보다 일반적인 방법을 제안한다. PTQ 스킴들은 극단적으로 취할 수 있으며, 여기서 트레이닝 또는 테스트 데이터는 양자화 동안 활용되지 않는다(일명 제로-샷 시나리오들), 다음에 논의된다.</p>
</div>
<div id="S3.SS7.SSS2.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS7.SSS2.p3.1.1">Summary (PTQ). </span> PTQ에서, 모든 가중치들 및 활성화 양자화 파라미터들은 NN 모델의 어떠한 재훈련도 없이 결정된다. 이와 같이 PTQ는 NN 모델을 양자화하는 매우 빠른 방법이다. 그러나, 이것은 종종 QAT에 비해 낮은 정확도의 비용으로 온다.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2103.13630/assets/x7.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="160" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 6:</span></figcaption>
Comparison between full-precision inference (Left), inference with simulated quantization (Middle), and inference with integer-only quantization (Right).</figcaption>
</figure>
</section>
<section id="S3.SS7.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS7.SSS3.5.1.1" class="ltx_text">III-G</span>3 </span>Zero-shot Quantization</h4>

<div id="S3.SS7.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS3.p1.1">지금까지 논의된 바와 같이, 양자화 후 최소의 정확도 저하를 달성하기 위해, 우리는 훈련 데이터의 일부 전체에 대한 액세스가 필요하다. 먼저, 우리는 값을 클리핑하고 적절한 스케일링 팩터(일반적으로 문헌에서 보정이라고 함)를 결정할 수 있도록 활성화 범위를 알아야 한다. 둘째, 양자화된 모델은 모델 파라미터를 조정하고 정확도 저하를 복구하기 위해 미세 조정이 필요한 경우가 많다. 그러나, 많은 경우들에서, 원래의 트레이닝 데이터에 대한 액세스는 양자화 절차 동안 가능하지 않다. 이는 트레이닝 데이터세트가 너무 커서 배포할 수 없거나, 독점적(예를 들어, 구글의 JFT-300M)이거나, 보안 또는 프라이버시 우려(예를 들어, 의료 데이터)로 인해 민감하기 때문이다. 이 문제를 해결하기 위해 몇 가지 다른 방법이 제안되었으며, 이를 제로 샷 양자화(ZSQ)라고 한다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib182" title="">182</a>]</cite>에 의해 영감을 받아, 여기서는 먼저 두 가지 상이한 레벨의 제로-샷 양자화를 설명한다:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Level 1:</span> No data and no finetuning (ZSQ + PTQ).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Level 2:</span> No data but requires finetuning (ZSQ + QAT).</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS7.SSS3.p1.2">레벨 1은 어떠한 미세 조정도 없이 더 빠르고 더 쉬운 양자화를 허용한다. 파인튜닝은 일반적으로 시간이 많이 걸리고 종종 추가 하이퍼필라멘트 검색이 필요하다. 그러나 레벨 2는 일반적으로 정밀 조정이 특히 초저비트 정밀도 설정 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib85" title="">85</a>]</cite>에서 양자화된 모델이 정확도 저하를 복구하는 데 도움이 되기 때문에 더 높은 정확도를 초래한다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib182" title="">182</a>]</cite>의 작업은 가중치 범위를 균등화하고 바이어스 오류를 수정하여 주어진 NN 모델을 데이터나 미세 조정 없이 양자화에 더 적합하도록 하는 Level 1 접근법을 사용한다. 그러나, 이 방법은 (piece-wise) 선형 활성화 함수의 스케일-equivariance 속성에 기초하기 때문에, GELU<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib94" title="">94</a>]</cite> 활성화를 갖는 BERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib46" title="">46</a>]</cite> 또는 Swish 활성화를 갖는 MobileNetV3<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib100" title="">100</a>]</cite></cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib203" title="">203</a>]</cite>와 같이 비선형 활성화를 갖는 NN에 대해 차선책일 수 있다.</p>
</div>
<div id="S3.SS7.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS3.p2.1">ZSQ에서 인기 있는 연구 분야는 대상 사전 훈련 모델이 훈련된 실제 데이터와 유사한 합성 데이터를 생성하는 것이다. 그런 다음 합성 데이터는 양자화된 모델의 보정 및/또는 미세 조정을 위해 사용된다. 이 분야의 초기 작업 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib28" title="">28</a>]</cite>는 합성 데이터 생성을 위해 GAN(Generative Adversarial Networks) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib75" title="">75</a>]</cite>를 이용한다. 사전 훈련된 모델을 판별기로 사용하여, 그 출력이 판별기에 의해 잘 분류될 수 있도록 생성기를 훈련시킨다. 그런 다음, 생성기로부터 수집된 합성 데이터 샘플을 사용하여, 양자화된 모델은 완전-정밀 대응물로부터의 지식 증류로 미세조정될 수 있다(섹션<a class="ltx_ref" href="#S4.SS4" title="IV-D Distillation-Assisted Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a>를 더 상세히 참조). 그러나, 이 방법은 모델의 최종 출력만을 사용하여 생성되기 때문에 실제 데이터의 내부 통계(예를 들어, 중간 레이어 활성화의 분포)를 캡처하는 데 실패한다. 내부 통계를 고려하지 않은 합성 데이터는 실제 데이터 분포 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib85" title="">85</a>]</cite>를 제대로 나타내지 못할 수 있다. 이를 해결하기 위해 많은 후속 노력이 보다 현실적인 합성 데이터를 생성하기 위해 배치 정규화(Batch Normalization, BatchNorm) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib112" title="">112</a>]</cite>, 즉 채널별 평균 및 분산에 저장된 통계를 사용한다. 특히 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib85" title="">85</a>]</cite>는 내부 통계의 KL 발산을 직접 최소화하여 데이터를 생성하며, 합성 데이터를 사용하여 양자화된 모델을 보정하고 미세 조정한다. 또한, ZeroQ<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite>는 합성 데이터가 교정뿐만 아니라 감도 측정에 사용될 수 있음을 보여줌으로써 훈련/검증 데이터에 대한 액세스 없이 혼합-정밀 사후 훈련 양자화를 가능하게 한다. ZeroQ는 또한 데이터를 생성할 때 출력 라벨에 의존하지 않기 때문에 ZSQ를 객체 검출 태스크로 확장한다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib85" title="">85</a>]</cite>와 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>]</cite> 모두 입력 이미지를 훈련 가능한 매개변수로 설정하고 내부 통계가 실제 데이터와 유사해질 때까지 직접 역전파를 수행한다. 한 걸음 더 나아가 최근 연구 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib37" title="">37</a>, <a class="ltx_ref" href="#bib.bib259" title="">259</a>, <a class="ltx_ref" href="#bib.bib90" title="">90</a>]</cite>는 실제 데이터 분포를 더 잘 포착하고 보다 사실적인 합성 데이터를 생성할 수 있는 생성 모델을 훈련하고 활용하는 데 효과적이다.</p>
</div>
<div id="S3.SS7.SSS3.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS7.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS7.SSS3.p3.1.1">Summary (ZSQ). </span> 제로 샷(일명 데이터 프리) 양자화는 트레이닝/검증 데이터에 대한 액세스 없이 전체 양자화를 수행한다. 이는 데이터 세트에 액세스할 필요 없이 고객의 워크로드 배포를 가속화하려는 MLAaS(Machine Learning as a Service) 공급자에게 특히 중요합니다. 더욱이, 이것은 보안 또는 프라이버시 염려가 트레이닝 데이터에 대한 액세스를 제한할 수 있는 경우에 중요하다.</p>
</div>
<figure id="S3.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="https://ar5iv.labs.arxiv.org/html/2103.13630/assets/x8.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="189" height="113" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="https://ar5iv.labs.arxiv.org/html/2103.13630/assets/x9.png" id="S3.F7.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="192" height="100" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 7:</span>(Left) Titan RTX 및 A100 GPU 상의 상이한 비트-정밀 로직에 대한 피크 처리량 간의 비교.</figcaption>
(Right) Comparison of the corresponding energy cost and relative area cost for different precision for 45nm technology&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>.
As one can see, lower precision provides exponentially better energy efficiency and higher throughput.
</figcaption>
</figure>
</section>
</section>
<section id="S3.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS8.5.1.1" class="ltx_text">III-H</span> </span><span id="S3.SS8.6.2" class="ltx_text ltx_font_italic">Stochastic Quantization</span>
</h3>

<div id="S3.SS8.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS8.p1.1">추론 동안, 양자화 방식은 보통 결정적이다. 그러나 이것이 유일한 가능성은 아니며 일부 연구에서는 양자화 인식 훈련뿐만 아니라 감소된 정밀도 훈련 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib13" title="">13</a>, <a class="ltx_ref" href="#bib.bib79" title="">79</a>]</cite>를 위한 확률 양자화를 탐구했다. 높은 레벨의 직관은 확률적 양자화가 결정론적 양자화와 비교하여 NN이 더 많이 탐색하도록 허용할 수 있다는 것이었다. 한 가지 인기 있는 지지 주장은 반올림 연산이 항상 동일한 가중치를 반환할 수 있기 때문에 작은 가중치 업데이트가 가중치 변경을 초래하지 않을 수 있다는 것이었다. 그러나 확률적 반올림을 활성화하면 NN이 <em class="ltx_emph ltx_font_italic" id="S3.SS8.p1.1.1">escape</em>에 기회를 제공하여 매개 변수를 업데이트할 수 있습니다.</p>
</div>
<div id="S3.SS8.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS8.p2.1">보다 형식적으로, 확률적 양자화는 가중치 업데이트의 크기와 연관된 확률로 부동수를 위 또는 아래로 매핑한다. 예를 들어, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib79" title="">79</a>, <a class="ltx_ref" href="#bib.bib29" title="">29</a>]</cite>에서 <span class="ltx_text ltx_markedasmath" id="S3.SS8.p2.1.1">Int</span> operator in Eq. <a class="ltx_ref" href="#S3.E2" title="In III-B Uniform Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">2</span></a>는 다음과 같이 정의된다.</p>
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E8.m1.3" class="ltx_Math" alttext="\small\text{Int}(x)=\begin{cases}\lfloor x\rfloor\quad\text{with probability }\lceil x\rceil-x,\\
\lceil x\rceil\quad\text{with probability }x-\lfloor x\rfloor.\end{cases}" display="block"><semantics id="S3.E8.m1.3a"><mrow id="S3.E8.m1.3.4" xref="S3.E8.m1.3.4.cmml"><mrow id="S3.E8.m1.3.4.2" xref="S3.E8.m1.3.4.2.cmml"><mtext mathsize="90%" id="S3.E8.m1.3.4.2.2" xref="S3.E8.m1.3.4.2.2a.cmml">Int</mtext><mo lspace="0em" rspace="0em" id="S3.E8.m1.3.4.2.1" xref="S3.E8.m1.3.4.2.1.cmml">​</mo><mrow id="S3.E8.m1.3.4.2.3.2" xref="S3.E8.m1.3.4.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E8.m1.3.4.2.3.2.1" xref="S3.E8.m1.3.4.2.cmml">(</mo><mi mathsize="90%" id="S3.E8.m1.3.3" xref="S3.E8.m1.3.3.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E8.m1.3.4.2.3.2.2" xref="S3.E8.m1.3.4.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E8.m1.3.4.1" xref="S3.E8.m1.3.4.1.cmml">=</mo><mrow id="S3.E8.m1.2.2" xref="S3.E8.m1.3.4.3.1.cmml"><mo id="S3.E8.m1.2.2.3" xref="S3.E8.m1.3.4.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E8.m1.2.2.2" xref="S3.E8.m1.3.4.3.1.cmml"><mtr id="S3.E8.m1.2.2.2a" xref="S3.E8.m1.3.4.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E8.m1.2.2.2b" xref="S3.E8.m1.3.4.3.1.cmml"><mrow id="S3.E8.m1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.3.1.cmml"><mrow id="S3.E8.m1.1.1.1.1.1.1.3.1" xref="S3.E8.m1.1.1.1.1.1.1.3.1.cmml"><mrow id="S3.E8.m1.1.1.1.1.1.1.3.1.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.cmml"><mrow id="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.2.1" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.1.1.cmml">⌊</mo><mi mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.1.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.2.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.1.1.cmml">⌋</mo></mrow><mo lspace="0.900em" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.1" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.1.cmml">​</mo><mtext mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.3" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.3a.cmml">with probability&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.1a" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.1.cmml">​</mo><mrow id="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.2.1" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.1.1.cmml">⌈</mo><mi mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.2.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.2.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.1.1.cmml">⌉</mo></mrow></mrow><mo mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.1" xref="S3.E8.m1.1.1.1.1.1.1.3.1.1.cmml">−</mo><mi mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.3" xref="S3.E8.m1.1.1.1.1.1.1.3.1.3.cmml">x</mi></mrow><mo mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.2" xref="S3.E8.m1.1.1.1.1.1.1.3.1.cmml">,</mo></mrow></mtd><mtd id="S3.E8.m1.2.2.2c" xref="S3.E8.m1.3.4.3.1.1.cmml"></mtd></mtr><mtr id="S3.E8.m1.2.2.2d" xref="S3.E8.m1.3.4.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E8.m1.2.2.2e" xref="S3.E8.m1.3.4.3.1.cmml"><mrow id="S3.E8.m1.2.2.2.2.1.1.3" xref="S3.E8.m1.2.2.2.2.1.1.3.1.cmml"><mrow id="S3.E8.m1.2.2.2.2.1.1.3.1" xref="S3.E8.m1.2.2.2.2.1.1.3.1.cmml"><mrow id="S3.E8.m1.2.2.2.2.1.1.3.1.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.cmml"><mrow id="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.2.1" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.1.1.cmml">⌈</mo><mi mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.1" xref="S3.E8.m1.2.2.2.2.1.1.1.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.2.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.1.1.cmml">⌉</mo></mrow><mo lspace="0.900em" rspace="0em" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.1" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.1.cmml">​</mo><mtext mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.3" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.3a.cmml">with probability&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.1a" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.1.cmml">​</mo><mi mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.4" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.4.cmml">x</mi></mrow><mo mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.1" xref="S3.E8.m1.2.2.2.2.1.1.3.1.1.cmml">−</mo><mrow id="S3.E8.m1.2.2.2.2.1.1.3.1.3.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.3.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.3.2.1" xref="S3.E8.m1.2.2.2.2.1.1.3.1.3.1.1.cmml">⌊</mo><mi mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.2" xref="S3.E8.m1.2.2.2.2.1.1.2.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.3.2.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.3.1.1.cmml">⌋</mo></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.2" xref="S3.E8.m1.2.2.2.2.1.1.3.1.cmml">.</mo></mrow></mtd><mtd id="S3.E8.m1.2.2.2f" xref="S3.E8.m1.3.4.3.1.1.cmml"></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.3b"><apply id="S3.E8.m1.3.4.cmml" xref="S3.E8.m1.3.4"><eq id="S3.E8.m1.3.4.1.cmml" xref="S3.E8.m1.3.4.1"></eq><apply id="S3.E8.m1.3.4.2.cmml" xref="S3.E8.m1.3.4.2"><times id="S3.E8.m1.3.4.2.1.cmml" xref="S3.E8.m1.3.4.2.1"></times><ci id="S3.E8.m1.3.4.2.2a.cmml" xref="S3.E8.m1.3.4.2.2"><mtext mathsize="90%" id="S3.E8.m1.3.4.2.2.cmml" xref="S3.E8.m1.3.4.2.2">Int</mtext></ci><ci id="S3.E8.m1.3.3.cmml" xref="S3.E8.m1.3.3">𝑥</ci></apply><apply id="S3.E8.m1.3.4.3.1.cmml" xref="S3.E8.m1.2.2"><csymbol cd="latexml" id="S3.E8.m1.3.4.3.1.1.cmml" xref="S3.E8.m1.2.2.3">cases</csymbol><apply id="S3.E8.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3"><minus id="S3.E8.m1.1.1.1.1.1.1.3.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.1"></minus><apply id="S3.E8.m1.1.1.1.1.1.1.3.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2"><times id="S3.E8.m1.1.1.1.1.1.1.3.1.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.1"></times><apply id="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.2"><floor id="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.2.2.1"></floor><ci id="S3.E8.m1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.1">𝑥</ci></apply><ci id="S3.E8.m1.1.1.1.1.1.1.3.1.2.3a.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.3"><mtext mathsize="90%" id="S3.E8.m1.1.1.1.1.1.1.3.1.2.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.3">with probability&nbsp;</mtext></ci><apply id="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.2"><ceiling id="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.2.4.2.1"></ceiling><ci id="S3.E8.m1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2">𝑥</ci></apply></apply><ci id="S3.E8.m1.1.1.1.1.1.1.3.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3.1.3">𝑥</ci></apply><ci id="S3.E8.m1.3.4.3.1.3a.cmml" xref="S3.E8.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E8.m1.3.4.3.1.3.cmml" xref="S3.E8.m1.2.2.3">otherwise</mtext></ci><apply id="S3.E8.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3"><minus id="S3.E8.m1.2.2.2.2.1.1.3.1.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.1"></minus><apply id="S3.E8.m1.2.2.2.2.1.1.3.1.2.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2"><times id="S3.E8.m1.2.2.2.2.1.1.3.1.2.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.1"></times><apply id="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.2"><ceiling id="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.1.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.2.2.1"></ceiling><ci id="S3.E8.m1.2.2.2.2.1.1.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.1">𝑥</ci></apply><ci id="S3.E8.m1.2.2.2.2.1.1.3.1.2.3a.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.3"><mtext mathsize="90%" id="S3.E8.m1.2.2.2.2.1.1.3.1.2.3.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.3">with probability&nbsp;</mtext></ci><ci id="S3.E8.m1.2.2.2.2.1.1.3.1.2.4.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.2.4">𝑥</ci></apply><apply id="S3.E8.m1.2.2.2.2.1.1.3.1.3.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.3.2"><floor id="S3.E8.m1.2.2.2.2.1.1.3.1.3.1.1.cmml" xref="S3.E8.m1.2.2.2.2.1.1.3.1.3.2.1"></floor><ci id="S3.E8.m1.2.2.2.2.1.1.2.cmml" xref="S3.E8.m1.2.2.2.2.1.1.2">𝑥</ci></apply></apply><ci id="S3.E8.m1.3.4.3.1.5a.cmml" xref="S3.E8.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E8.m1.3.4.3.1.5.cmml" xref="S3.E8.m1.2.2.3">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.3c">\small\text{Int}(x)=\begin{cases}\lfloor x\rfloor\quad\text{with probability }\lceil x\rceil-x,\\
\lceil x\rceil\quad\text{with probability }x-\lfloor x\rfloor.\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS8.p2.5">그러나, 이 정의는 이진 양자화에 사용될 수 없다. 따라서, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib42" title="">42</a>]</cite> extends this to</p>
<table id="S3.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E9.m1.3" class="ltx_Math" alttext="\small\text{Binary}(x)=\begin{cases}-1\quad\text{with probability }1-\sigma(x),\\
+1\quad\text{with probability }\sigma(x),\end{cases}" display="block"><semantics id="S3.E9.m1.3a"><mrow id="S3.E9.m1.3.4" xref="S3.E9.m1.3.4.cmml"><mrow id="S3.E9.m1.3.4.2" xref="S3.E9.m1.3.4.2.cmml"><mtext mathsize="90%" id="S3.E9.m1.3.4.2.2" xref="S3.E9.m1.3.4.2.2a.cmml">Binary</mtext><mo lspace="0em" rspace="0em" id="S3.E9.m1.3.4.2.1" xref="S3.E9.m1.3.4.2.1.cmml">​</mo><mrow id="S3.E9.m1.3.4.2.3.2" xref="S3.E9.m1.3.4.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E9.m1.3.4.2.3.2.1" xref="S3.E9.m1.3.4.2.cmml">(</mo><mi mathsize="90%" id="S3.E9.m1.3.3" xref="S3.E9.m1.3.3.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E9.m1.3.4.2.3.2.2" xref="S3.E9.m1.3.4.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E9.m1.3.4.1" xref="S3.E9.m1.3.4.1.cmml">=</mo><mrow id="S3.E9.m1.2.2" xref="S3.E9.m1.3.4.3.1.cmml"><mo id="S3.E9.m1.2.2.3" xref="S3.E9.m1.3.4.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S3.E9.m1.2.2.2" xref="S3.E9.m1.3.4.3.1.cmml"><mtr id="S3.E9.m1.2.2.2a" xref="S3.E9.m1.3.4.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E9.m1.2.2.2b" xref="S3.E9.m1.3.4.3.1.cmml"><mrow id="S3.E9.m1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.cmml"><mrow id="S3.E9.m1.1.1.1.1.1.1.2.1" xref="S3.E9.m1.1.1.1.1.1.1.2.1.cmml"><mrow id="S3.E9.m1.1.1.1.1.1.1.2.1.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.cmml"><mo mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.2a" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.cmml">−</mo><mrow id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.cmml"><mn mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.2.cmml">1</mn><mo lspace="0.900em" rspace="0em" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1.cmml">​</mo><mtext mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3a.cmml">with probability&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1a" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1.cmml">​</mo><mn mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.4" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.4.cmml">1</mn></mrow></mrow><mo mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.1" xref="S3.E9.m1.1.1.1.1.1.1.2.1.1.cmml">−</mo><mrow id="S3.E9.m1.1.1.1.1.1.1.2.1.3" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.cmml"><mi mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.3.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.1.1.1.1.1.1.2.1.3.1" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.1.cmml">​</mo><mrow id="S3.E9.m1.1.1.1.1.1.1.2.1.3.3.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.cmml"><mo maxsize="90%" minsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.3.3.2.1" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.cmml">(</mo><mi mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.1.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.3.3.2.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.cmml">)</mo></mrow></mrow></mrow><mo mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.2" xref="S3.E9.m1.1.1.1.1.1.1.2.1.cmml">,</mo></mrow></mtd><mtd id="S3.E9.m1.2.2.2c" xref="S3.E9.m1.3.4.3.1.1.cmml"></mtd></mtr><mtr id="S3.E9.m1.2.2.2d" xref="S3.E9.m1.3.4.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E9.m1.2.2.2e" xref="S3.E9.m1.3.4.3.1.cmml"><mrow id="S3.E9.m1.2.2.2.2.1.1.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.cmml"><mrow id="S3.E9.m1.2.2.2.2.1.1.2.1" xref="S3.E9.m1.2.2.2.2.1.1.2.1.cmml"><mo mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1a" xref="S3.E9.m1.2.2.2.2.1.1.2.1.cmml">+</mo><mrow id="S3.E9.m1.2.2.2.2.1.1.2.1.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.cmml"><mn mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.2.cmml">1</mn><mo lspace="0.900em" rspace="0em" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.1" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.1.cmml">​</mo><mtext mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.3" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.3a.cmml">with probability&nbsp;</mtext><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.1a" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.1.cmml">​</mo><mi mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.4" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.4.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.1b" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.1.cmml">​</mo><mrow id="S3.E9.m1.2.2.2.2.1.1.2.1.2.5.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.5.2.1" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.cmml">(</mo><mi mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.1" xref="S3.E9.m1.2.2.2.2.1.1.1.cmml">x</mi><mo maxsize="90%" minsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.5.2.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.cmml">)</mo></mrow></mrow></mrow><mo mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.2" xref="S3.E9.m1.2.2.2.2.1.1.2.1.cmml">,</mo></mrow></mtd><mtd id="S3.E9.m1.2.2.2f" xref="S3.E9.m1.3.4.3.1.1.cmml"></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.3b"><apply id="S3.E9.m1.3.4.cmml" xref="S3.E9.m1.3.4"><eq id="S3.E9.m1.3.4.1.cmml" xref="S3.E9.m1.3.4.1"></eq><apply id="S3.E9.m1.3.4.2.cmml" xref="S3.E9.m1.3.4.2"><times id="S3.E9.m1.3.4.2.1.cmml" xref="S3.E9.m1.3.4.2.1"></times><ci id="S3.E9.m1.3.4.2.2a.cmml" xref="S3.E9.m1.3.4.2.2"><mtext mathsize="90%" id="S3.E9.m1.3.4.2.2.cmml" xref="S3.E9.m1.3.4.2.2">Binary</mtext></ci><ci id="S3.E9.m1.3.3.cmml" xref="S3.E9.m1.3.3">𝑥</ci></apply><apply id="S3.E9.m1.3.4.3.1.cmml" xref="S3.E9.m1.2.2"><csymbol cd="latexml" id="S3.E9.m1.3.4.3.1.1.cmml" xref="S3.E9.m1.2.2.3">cases</csymbol><apply id="S3.E9.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2"><minus id="S3.E9.m1.1.1.1.1.1.1.2.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.1"></minus><apply id="S3.E9.m1.1.1.1.1.1.1.2.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2"><minus id="S3.E9.m1.1.1.1.1.1.1.2.1.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2"></minus><apply id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2"><times id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.1"></times><cn type="integer" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.2">1</cn><ci id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3a.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3"><mtext mathsize="90%" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.3">with probability&nbsp;</mtext></ci><cn type="integer" id="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.4.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.2.2.4">1</cn></apply></apply><apply id="S3.E9.m1.1.1.1.1.1.1.2.1.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3"><times id="S3.E9.m1.1.1.1.1.1.1.2.1.3.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.1"></times><ci id="S3.E9.m1.1.1.1.1.1.1.2.1.3.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2.1.3.2">𝜎</ci><ci id="S3.E9.m1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1">𝑥</ci></apply></apply><ci id="S3.E9.m1.3.4.3.1.3a.cmml" xref="S3.E9.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E9.m1.3.4.3.1.3.cmml" xref="S3.E9.m1.2.2.3">otherwise</mtext></ci><apply id="S3.E9.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2"><plus id="S3.E9.m1.2.2.2.2.1.1.2.1.1.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2"></plus><apply id="S3.E9.m1.2.2.2.2.1.1.2.1.2.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2"><times id="S3.E9.m1.2.2.2.2.1.1.2.1.2.1.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.1"></times><cn type="integer" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.2.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.2">1</cn><ci id="S3.E9.m1.2.2.2.2.1.1.2.1.2.3a.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.3"><mtext mathsize="90%" id="S3.E9.m1.2.2.2.2.1.1.2.1.2.3.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.3">with probability&nbsp;</mtext></ci><ci id="S3.E9.m1.2.2.2.2.1.1.2.1.2.4.cmml" xref="S3.E9.m1.2.2.2.2.1.1.2.1.2.4">𝜎</ci><ci id="S3.E9.m1.2.2.2.2.1.1.1.cmml" xref="S3.E9.m1.2.2.2.2.1.1.1">𝑥</ci></apply></apply><ci id="S3.E9.m1.3.4.3.1.5a.cmml" xref="S3.E9.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E9.m1.3.4.3.1.5.cmml" xref="S3.E9.m1.2.2.3">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.3c">\small\text{Binary}(x)=\begin{cases}-1\quad\text{with probability }1-\sigma(x),\\
+1\quad\text{with probability }\sigma(x),\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p id="S3.SS8.p2.4" class="ltx_p">where <span id="S3.SS8.p2.4.1" class="ltx_text ltx_markedasmath">Binary</span> is a function to binarize the real value <math id="S3.SS8.p2.3.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS8.p2.3.m2.1a"><mi id="S3.SS8.p2.3.m2.1.1" xref="S3.SS8.p2.3.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS8.p2.3.m2.1b"><ci id="S3.SS8.p2.3.m2.1.1.cmml" xref="S3.SS8.p2.3.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p2.3.m2.1c">x</annotation></semantics></math>, and <math id="S3.SS8.p2.4.m3.1" class="ltx_Math" alttext="\sigma(\cdot)" display="inline"><semantics id="S3.SS8.p2.4.m3.1a"><mrow id="S3.SS8.p2.4.m3.1.2" xref="S3.SS8.p2.4.m3.1.2.cmml"><mi id="S3.SS8.p2.4.m3.1.2.2" xref="S3.SS8.p2.4.m3.1.2.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.SS8.p2.4.m3.1.2.1" xref="S3.SS8.p2.4.m3.1.2.1.cmml">​</mo><mrow id="S3.SS8.p2.4.m3.1.2.3.2" xref="S3.SS8.p2.4.m3.1.2.cmml"><mo stretchy="false" id="S3.SS8.p2.4.m3.1.2.3.2.1" xref="S3.SS8.p2.4.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS8.p2.4.m3.1.1" xref="S3.SS8.p2.4.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS8.p2.4.m3.1.2.3.2.2" xref="S3.SS8.p2.4.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS8.p2.4.m3.1b"><apply id="S3.SS8.p2.4.m3.1.2.cmml" xref="S3.SS8.p2.4.m3.1.2"><times id="S3.SS8.p2.4.m3.1.2.1.cmml" xref="S3.SS8.p2.4.m3.1.2.1"></times><ci id="S3.SS8.p2.4.m3.1.2.2.cmml" xref="S3.SS8.p2.4.m3.1.2.2">𝜎</ci><ci id="S3.SS8.p2.4.m3.1.1.cmml" xref="S3.SS8.p2.4.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.p2.4.m3.1c">\sigma(\cdot)</annotation></semantics></math> is the sigmoid function.</p>
</div>
<div id="S3.SS8.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS8.p3.1">최근 QuantNoise<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>에서 또 다른 확률적 양자화 방법이 소개되고 있다. 퀀트 노이즈는 각 순방향 패스 동안 가중치의 다른 랜덤 서브세트를 양자화하고 편향되지 않은 기울기로 모델을 훈련시킨다. 이는 많은 컴퓨터 비전 및 자연 언어 처리 모델에서 상당한 정확도 저하 없이 더 낮은 비트 정밀도 양자화를 가능하게 한다. 그러나 확률적 양자화 방법의 주요 과제는 모든 가중치 업데이트에 대해 난수를 생성하는 오버헤드이며, 따라서 실제로 아직 널리 채택되지 않았다.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2103.13630/assets/x10.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8:</span>Illustration of mixed-precision quantization. 혼합 정밀 양자화에 있어서</figcaption>
the goal is to keep sensitive and efficient layers in higher precision,
and only
apply low-precision quantization to insensitive and inefficient layers.
The efficiency metric is hardware dependant, and it could be latency or
energy consumption.
</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Advanced Concepts: Quantization Below 8 bits</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1">이 절에서는 sub-INT8 양자화에 주로 사용되는 양자화의 더 진보된 주제에 대해 논의할 것이다. 우리는 먼저 <a class="ltx_ref" href="#S4.SS1" title="IV-A Simulated and Integer-only Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a> 구간에서 정수 전용 양자화와 모의 양자화 및 그 차이에 대해 논의할 것이다. 이후, 섹션 <a class="ltx_ref" href="#S4.SS2" title="IV-B Mixed-Precision Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>의 혼합 정밀도 양자화에 이어 섹션 <a class="ltx_ref" href="#S4.SS3" title="IV-C Hardware Aware Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>의 하드웨어 인식 양자화에 대한 다양한 방법에 대해 논의할 것이다. 그런 다음 증류가 섹션 <a class="ltx_ref" href="#S4.SS4" title="IV-D Distillation-Assisted Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a>에서 양자화 정확도를 높이는 데 어떻게 사용될 수 있는지 설명하고 섹션 <a class="ltx_ref" href="#S4.SS5" title="IV-E Extreme Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span></span></a>에서 극도로 낮은 비트 정밀도 양자화에 대해 논의할 것이다. 마지막으로, 섹션 <a class="ltx_ref" href="#S4.SS6" title="IV-F Vector Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-F</span></span></a>에서 벡터 양자화를 위한 다른 방법들을 간략히 설명한다.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Simulated and Integer-only Quantization</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p1.1">양자화된 NN 모델을 배포하는 두 가지 일반적인 접근법이 있는데, <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">simulated quantization</span> (일명 가짜 양자화) 및 <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.2">integer-only quantization</span> (일명 고정 소수점 양자화)이다. 시뮬레이션된 양자화에서, 양자화된 모델 파라미터들은 낮은 정밀도로 저장되지만, 연산들(예를 들어, 행렬 곱셈들 및 컨볼루션들)은 부동 소수점 산술로 수행된다. 따라서, 양자화된 파라미터들은 그림<a class="ltx_ref" href="#S3.F6" title="Figure 6 ‣ III-G2 Post-Training Quantization ‣ III-G Fine-tuning Methods ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">6</span></a>(중간)에 개략적으로 도시된 바와 같이 부동 소수점 연산 이전에 역양자화될 필요가 있다. 따라서, 시뮬레이션된 양자화를 갖는 빠르고 효율적인 저-정밀 로직으로부터 완전히 이익을 얻을 수 없다. 그러나 정수 전용 양자화에서는 그림 <a class="ltx_ref" href="#S3.F6" title="Figure 6 ‣ III-G2 Post-Training Quantization ‣ III-G Fine-tuning Methods ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">6</span></a>(Right)에 예시된 것처럼 모든 연산이 저정밀 정수 산술 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib113" title="">113</a>, <a class="ltx_ref" href="#bib.bib267" title="">267</a>, <a class="ltx_ref" href="#bib.bib132" title="">132</a>, <a class="ltx_ref" href="#bib.bib154" title="">154</a>, <a class="ltx_ref" href="#bib.bib193" title="">193</a>]</cite>를 사용하여 수행된다. 이것은 임의의 파라미터들 또는 활성화들의 부동 소수점 역양자화 없이, 전체 추론이 효율적인 정수 산술로 수행될 수 있게 한다.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p2.2">일반적으로, 부동 소수점 산술로 완전-정밀도로 추론을 수행하는 것은 최종 양자화 정확도에 도움이 될 수 있지만, 이것은 저-정밀도 로직으로부터 이익을 얻을 수 없는 비용으로 온다. 저정밀 로직은 대기 시간, 전력 소비 및 면적 효율성 측면에서 전체 정밀 대응에 비해 여러 이점이 있다. <a class="ltx_ref" href="#S3.F7" title="Figure 7 ‣ III-G3 Zero-shot Quantization ‣ III-G Fine-tuning Methods ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">7</span></a> (좌측)에 도시된 바와 같이, NVIDIA V100 및 Titan RTX를 포함한 많은 하드웨어 프로세서는 추론 처리량과 대기 시간을 증가시킬 수 있는 낮은 정밀도의 산술의 빠른 처리를 지원한다. 더욱이, 그림 <a class="ltx_ref" href="#S3.F7" title="Figure 7 ‣ III-G3 Zero-shot Quantization ‣ III-G Fine-tuning Methods ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">7</span></a> (오른쪽)에 예시된 바와 같이, 45nm 기술 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib97" title="">97</a>]</cite>의 경우, 낮은 정밀도 로직이 에너지 및 면적 측면에서 훨씬 더 효율적이다. 예를 들어, INT8 덧셈을 수행하는 것은 FP32 덧셈 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib97" title="">97</a>]</cite>에 비해 <math alttext="30\times" class="ltx_math_unparsed" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1b"><mn id="S4.SS1.p2.1.m1.1.1">30</mn><mo id="S4.SS1.p2.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">30\times</annotation></semantics></math> 더 에너지 효율적이며, <math alttext="116\times" class="ltx_math_unparsed" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1b"><mn id="S4.SS1.p2.2.m2.1.1">116</mn><mo id="S4.SS1.p2.2.m2.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">116\times</annotation></semantics></math> 더 면적 효율적이다.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p3.1">주목할 만한 정수 전용 양자화 작업으로는 이전 컨볼루션 계층에 Batch 정규화를 융합하는 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib154" title="">154</a>]</cite>, 배치 정규화가 있는 잔차 네트워크에 대한 정수 전용 계산 방법을 제안하는 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib113" title="">113</a>]</cite> 등이 있다. 그러나 두 방법 모두 ReLU 활성화로 제한된다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib132" title="">132</a>]</cite>의 최근 연구는 GELU<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib94" title="">94</a>]</cite>, Softmax, Layer Normalization<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib6" title="">6</a>]</cite>를 정수 연산으로 근사화함으로써 이러한 한계를 해결하고, 정수 전용 양자화를 Transformer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib243" title="">243</a>]</cite> 아키텍처에 더 확장한다.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.1">Dyadic quantization</span>은 정수 전용 양자화의 또 다른 클래스이며, 여기서 모든 스케일링은 그들의 분자에 정수 값을 갖는 유리수이고 분모에 2의 거듭제곱인 dyadic 수로 수행된다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib267" title="">267</a>]</cite>. 이것은 단지 정수 덧셈, 곱셈, 비트 쉬프팅만을 필요로 하지만 정수 나눗셈을 필요로 하지 않는 계산 그래프를 초래한다. 중요한 것은, 이 접근법에서, 모든 추가들(예를 들어, 잔차 연결들)은 동일한 dyadic 스케일을 갖도록 강제되고, 이는 더 높은 효율성으로 추가 로직을 더 단순하게 할 수 있다.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Summary (Simulated vs Integer-only Quantization). </span> 일반적인 정수 전용 및 다이애딕 양자화는 시뮬레이트/페이크 양자화에 비해 더 바람직하다. 이는 정수만 산술을 위해 더 낮은 정밀도 로직을 사용하는 반면, 시뮬레이션된 양자화는 연산을 수행하기 위해 부동 소수점 로직을 사용하기 때문이다. 그러나, 이것이 가짜 양자화가 결코 유용하지 않다는 것을 의미하지는 않는다. 사실, 가짜 양자화 방법들은 추천 시스템들 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib185" title="">185</a>]</cite>에서와 같이 컴퓨팅-바운드보다는 대역폭-바운드인 문제들에 유익할 수 있다. 이러한 작업의 병목 현상은 메모리 풋프린트와 메모리에서 매개 변수를 로드하는 비용입니다. 그러므로, 가짜 양자화를 수행하는 것은 이러한 경우에 수용될 수 있다.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Mixed-Precision Quantization</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p1.1">낮은 정밀도 양자화를 사용하면 하드웨어 성능이 향상된다는 것을 쉽게 알 수 있다. 그러나, 모델을 초저정도로 균일하게 양자화하는 것은 상당한 정확도 저하를 야기할 수 있다. 이를 혼합-정밀 양자화 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib286" title="">286</a>, <a class="ltx_ref" href="#bib.bib246" title="">246</a>, <a class="ltx_ref" href="#bib.bib51" title="">51</a>, <a class="ltx_ref" href="#bib.bib239" title="">239</a>, <a class="ltx_ref" href="#bib.bib263" title="">263</a>, <a class="ltx_ref" href="#bib.bib199" title="">199</a>, <a class="ltx_ref" href="#bib.bib249" title="">249</a>, <a class="ltx_ref" href="#bib.bib102" title="">102</a>, <a class="ltx_ref" href="#bib.bib187" title="">187</a>, <a class="ltx_ref" href="#bib.bib82" title="">82</a>, <a class="ltx_ref" href="#bib.bib211" title="">211</a>, <a class="ltx_ref" href="#bib.bib162" title="">162</a>, <a class="ltx_ref" href="#bib.bib282" title="">282</a>]</cite>로 해결할 수 있다. 이 접근법에서, 그림 <a class="ltx_ref" href="#S3.F8" title="Figure 8 ‣ III-H Stochastic Quantization ‣ III Basic Concepts of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">8</span></a>에 예시된 바와 같이, 각 계층은 상이한 비트 정밀도로 양자화된다. 이 접근법의 한 가지 과제는 이 비트 설정을 선택하기 위한 탐색 공간이 계층 수에서 지수적이라는 것이다. 이 거대한 탐색 공간을 다루기 위해 상이한 접근법들이 제안되었다.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p2.1">각 레이어에 대해 이러한 혼합 정밀도를 선택하는 것은 본질적으로 탐색 문제이며, 이를 위해 많은 다른 방법들이 제안되었다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib246" title="">246</a>]</cite>의 최근 연구는 양자화 정책을 자동으로 결정하기 위해 강화 학습(reinforcement learning, RL) 기반 방법을 제안했고, 저자들은 RL 에이전트 피드백에서 하드웨어 가속기의 피드백을 취하기 위해 하드웨어 시뮬레이터를 사용했다. 논문 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib254" title="">254</a>]</cite>는 혼합 정밀도 구성 탐색 문제를 신경망 구조 탐색(Neural Architecture Search; NAS) 문제로 공식화하였고, 탐색 공간을 효율적으로 탐색하기 위해 미분 가능 NAS(Differentiable NAS; DNAS) 방법을 사용하였다. 이러한 탐색 기반 방법<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib246" title="">246</a>, <a class="ltx_ref" href="#bib.bib254" title="">254</a>]</cite>의 한 가지 단점은 종종 큰 계산 자원을 필요로 하고, 그들의 성능은 전형적으로 하이퍼파라미터 및 심지어 초기화에 민감하다는 것이다.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p3.1">혼합 정밀도 방법의 또 다른 부류는 주기적 함수 정규화를 사용하여 각각의 비트폭 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib184" title="">184</a>]</cite>를 학습하면서 정확도와 관련하여 서로 다른 레이어와 다양한 중요도를 자동으로 구별하여 혼합 정밀도 모델을 학습한다.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p4.1">이러한 탐색 및 정규화 기반 접근법과 달리 HAWQ<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib51" title="">51</a>]</cite>는 모델의 2차 민감도에 기반한 혼합 정밀도 설정을 자동으로 찾는 방법을 소개한다. 최적 뇌 손상 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib139" title="">139</a>]</cite>의 정액 작업에서 가지치기 결과와 유사하게 2차 연산자(즉, Hessian)의 흔적이 양자화 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite>에 대한 레이어의 민감도를 측정하는 데 사용될 수 있음을 이론적으로 보였다. HAWQv2에서 이 방법은 혼합 정밀도 활성화 양자화<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite>로 확장되었으며, RL 기반 혼합 정밀도 방법<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib246" title="">246</a>]</cite>보다 100배 이상 빠른 것으로 나타났다. 최근에는 HAWQv3에서 정수 전용 하드웨어 인식 양자화를 도입하여 주어진 애플리케이션 특정 제약 조건(예: 모델 크기 또는 대기 시간)에 대한 최적의 비트 정밀도를 찾기 위해 고속 정수 선형 프로그래밍 방법을 제안했다. 이 연구는 또한 혼합-정밀 양자화의 하드웨어 효율성에 대한 일반적인 질문을 T4 GPU에 직접 배치하여 INT8 양자화에 비해 혼합-정밀(INT4/INT8) 양자화로 최대 50% 속도를 보여준다.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">Summary(Mixed-precision Quantization). </span> 혼합 정밀도 양자화는 서로 다른 NN 모델의 저 정밀도 양자화에 효과적이고 하드웨어 효율적인 방법임이 입증되었다. 이 접근법에서, NN의 계층들은 양자화에 민감한/민감한 것으로 그룹화되고, 상위/하위 비트가 각각의 계층에 대해 사용된다. 이와 같이, 정확도 저하를 최소화할 수 있고, 낮은 정밀도 양자화로 감소된 메모리 풋프린트와 더 빠른 속도로부터 여전히 이익을 얻을 수 있다. 최근 작업 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib267" title="">267</a>]</cite>는 또한 혼합 정밀도가 작업/레이어에 걸쳐만 사용되기 때문에 이 접근 방식이 하드웨어 효율적이라는 것을 보여주었다.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Hardware Aware Quantization</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p1.1">양자화의 목표 중 하나는 추론 지연 시간을 개선하는 것이다. 그러나, 어떤 계층/동작이 양자화된 후에 모든 하드웨어가 동일한 속도를 제공하는 것은 아니다. 사실, 양자화의 이점은 하드웨어 의존적이며, 온-칩 메모리, 대역폭, 및 캐시 계층 구조와 같은 많은 요인이 양자화 속도 증가에 영향을 미친다.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p2.1">하드웨어 인식 양자화 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib256" title="">256</a>, <a class="ltx_ref" href="#bib.bib246" title="">246</a>, <a class="ltx_ref" href="#bib.bib91" title="">91</a>, <a class="ltx_ref" href="#bib.bib254" title="">254</a>, <a class="ltx_ref" href="#bib.bib265" title="">265</a>, <a class="ltx_ref" href="#bib.bib267" title="">267</a>, <a class="ltx_ref" href="#bib.bib250" title="">250</a>, <a class="ltx_ref" href="#bib.bib87" title="">87</a>]</cite>를 통해 최적의 이득을 얻기 위해서는 이러한 사실을 고려하는 것이 중요하다. 특히, 워크<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib246" title="">246</a>]</cite>는 강화 학습 에이전트를 사용하여 비트 폭이 다른 다른 레이어에 대한 레이턴시의 룩업 테이블을 기반으로 양자화를 위한 하드웨어 인식 혼합 정밀도 설정을 결정한다. 그러나, 이 접근법은 시뮬레이션된 하드웨어 대기 시간을 사용한다. 이를 해결하기 위해 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib267" title="">267</a>]</cite>의 최근 작업은 양자화된 연산을 하드웨어로 직접 전개하고, 서로 다른 양자화 비트 정밀도에 대한 각 계층의 실제 전개 지연 시간을 측정한다.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Distillation-Assisted Quantization</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS4.p1.1">양자화에서 흥미로운 작업은 모델 증류를 통합하여 양자화 정확도를 높이는 것이다. 모델 증류 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib207" title="">207</a>, <a class="ltx_ref" href="#bib.bib95" title="">95</a>, <a class="ltx_ref" href="#bib.bib177" title="">177</a>, <a class="ltx_ref" href="#bib.bib150" title="">150</a>, <a class="ltx_ref" href="#bib.bib195" title="">195</a>, <a class="ltx_ref" href="#bib.bib3" title="">3</a>, <a class="ltx_ref" href="#bib.bib270" title="">270</a>, <a class="ltx_ref" href="#bib.bib268" title="">268</a>, <a class="ltx_ref" href="#bib.bib289" title="">289</a>]</cite>는 정확도가 높은 큰 모델을 교사로 사용하여 컴팩트한 학생 모델의 훈련을 돕는 방법이다. 학생 모델의 훈련 동안 지상 진리 수업 레이블만 사용하는 대신 모델 증류는 교사가 생성한 소프트 확률을 활용하도록 제안하며, 이는 입력의 더 많은 정보를 포함할 수 있다. 즉, 전체 손실 함수는 학생 손실 및 증류 손실 모두를 통합하며, 이는 전형적으로 다음과 같이 공식화된다:</p>
<table id="S4.E10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E10.m1.6" class="ltx_Math" alttext="\small\mathcal{L}=\alpha\mathcal{H}(y,\sigma(z_{s}))+\beta\mathcal{H}(\sigma(z_{t},T),\sigma(z_{s},T))" display="block"><semantics id="S4.E10.m1.6a"><mrow id="S4.E10.m1.6.6" xref="S4.E10.m1.6.6.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S4.E10.m1.6.6.5" xref="S4.E10.m1.6.6.5.cmml">ℒ</mi><mo mathsize="90%" id="S4.E10.m1.6.6.4" xref="S4.E10.m1.6.6.4.cmml">=</mo><mrow id="S4.E10.m1.6.6.3" xref="S4.E10.m1.6.6.3.cmml"><mrow id="S4.E10.m1.4.4.1.1" xref="S4.E10.m1.4.4.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.4.4.1.1.3" xref="S4.E10.m1.4.4.1.1.3.cmml">α</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.4.4.1.1.2" xref="S4.E10.m1.4.4.1.1.2.cmml">​</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S4.E10.m1.4.4.1.1.4" xref="S4.E10.m1.4.4.1.1.4.cmml">ℋ</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.4.4.1.1.2a" xref="S4.E10.m1.4.4.1.1.2.cmml">​</mo><mrow id="S4.E10.m1.4.4.1.1.1.1" xref="S4.E10.m1.4.4.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E10.m1.4.4.1.1.1.1.2" xref="S4.E10.m1.4.4.1.1.1.2.cmml">(</mo><mi mathsize="90%" id="S4.E10.m1.1.1" xref="S4.E10.m1.1.1.cmml">y</mi><mo mathsize="90%" id="S4.E10.m1.4.4.1.1.1.1.3" xref="S4.E10.m1.4.4.1.1.1.2.cmml">,</mo><mrow id="S4.E10.m1.4.4.1.1.1.1.1" xref="S4.E10.m1.4.4.1.1.1.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.4.4.1.1.1.1.1.3" xref="S4.E10.m1.4.4.1.1.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.4.4.1.1.1.1.1.2" xref="S4.E10.m1.4.4.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E10.m1.4.4.1.1.1.1.1.1.1" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S4.E10.m1.4.4.1.1.1.1.1.1.1.2" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">z</mi><mi mathsize="90%" id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">s</mi></msub><mo maxsize="90%" minsize="90%" id="S4.E10.m1.4.4.1.1.1.1.1.1.1.3" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo maxsize="90%" minsize="90%" id="S4.E10.m1.4.4.1.1.1.1.4" xref="S4.E10.m1.4.4.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S4.E10.m1.6.6.3.4" xref="S4.E10.m1.6.6.3.4.cmml">+</mo><mrow id="S4.E10.m1.6.6.3.3" xref="S4.E10.m1.6.6.3.3.cmml"><mi mathsize="90%" id="S4.E10.m1.6.6.3.3.4" xref="S4.E10.m1.6.6.3.3.4.cmml">β</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.6.6.3.3.3" xref="S4.E10.m1.6.6.3.3.3.cmml">​</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S4.E10.m1.6.6.3.3.5" xref="S4.E10.m1.6.6.3.3.5.cmml">ℋ</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.6.6.3.3.3a" xref="S4.E10.m1.6.6.3.3.3.cmml">​</mo><mrow id="S4.E10.m1.6.6.3.3.2.2" xref="S4.E10.m1.6.6.3.3.2.3.cmml"><mo maxsize="90%" minsize="90%" id="S4.E10.m1.6.6.3.3.2.2.3" xref="S4.E10.m1.6.6.3.3.2.3.cmml">(</mo><mrow id="S4.E10.m1.5.5.2.2.1.1.1" xref="S4.E10.m1.5.5.2.2.1.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.3" xref="S4.E10.m1.5.5.2.2.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.5.5.2.2.1.1.1.2" xref="S4.E10.m1.5.5.2.2.1.1.1.2.cmml">​</mo><mrow id="S4.E10.m1.5.5.2.2.1.1.1.1.1" xref="S4.E10.m1.5.5.2.2.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.2" xref="S4.E10.m1.5.5.2.2.1.1.1.1.2.cmml">(</mo><msub id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.2" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.2.cmml">z</mi><mi mathsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.3" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.3.cmml">t</mi></msub><mo mathsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.3" xref="S4.E10.m1.5.5.2.2.1.1.1.1.2.cmml">,</mo><mi mathsize="90%" id="S4.E10.m1.2.2" xref="S4.E10.m1.2.2.cmml">T</mi><mo maxsize="90%" minsize="90%" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.4" xref="S4.E10.m1.5.5.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S4.E10.m1.6.6.3.3.2.2.4" xref="S4.E10.m1.6.6.3.3.2.3.cmml">,</mo><mrow id="S4.E10.m1.6.6.3.3.2.2.2" xref="S4.E10.m1.6.6.3.3.2.2.2.cmml"><mi mathsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.3" xref="S4.E10.m1.6.6.3.3.2.2.2.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S4.E10.m1.6.6.3.3.2.2.2.2" xref="S4.E10.m1.6.6.3.3.2.2.2.2.cmml">​</mo><mrow id="S4.E10.m1.6.6.3.3.2.2.2.1.1" xref="S4.E10.m1.6.6.3.3.2.2.2.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.2" xref="S4.E10.m1.6.6.3.3.2.2.2.1.2.cmml">(</mo><msub id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.cmml"><mi mathsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.2" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.2.cmml">z</mi><mi mathsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.3" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.3.cmml">s</mi></msub><mo mathsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.3" xref="S4.E10.m1.6.6.3.3.2.2.2.1.2.cmml">,</mo><mi mathsize="90%" id="S4.E10.m1.3.3" xref="S4.E10.m1.3.3.cmml">T</mi><mo maxsize="90%" minsize="90%" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.4" xref="S4.E10.m1.6.6.3.3.2.2.2.1.2.cmml">)</mo></mrow></mrow><mo maxsize="90%" minsize="90%" id="S4.E10.m1.6.6.3.3.2.2.5" xref="S4.E10.m1.6.6.3.3.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E10.m1.6b"><apply id="S4.E10.m1.6.6.cmml" xref="S4.E10.m1.6.6"><eq id="S4.E10.m1.6.6.4.cmml" xref="S4.E10.m1.6.6.4"></eq><ci id="S4.E10.m1.6.6.5.cmml" xref="S4.E10.m1.6.6.5">ℒ</ci><apply id="S4.E10.m1.6.6.3.cmml" xref="S4.E10.m1.6.6.3"><plus id="S4.E10.m1.6.6.3.4.cmml" xref="S4.E10.m1.6.6.3.4"></plus><apply id="S4.E10.m1.4.4.1.1.cmml" xref="S4.E10.m1.4.4.1.1"><times id="S4.E10.m1.4.4.1.1.2.cmml" xref="S4.E10.m1.4.4.1.1.2"></times><ci id="S4.E10.m1.4.4.1.1.3.cmml" xref="S4.E10.m1.4.4.1.1.3">𝛼</ci><ci id="S4.E10.m1.4.4.1.1.4.cmml" xref="S4.E10.m1.4.4.1.1.4">ℋ</ci><interval closure="open" id="S4.E10.m1.4.4.1.1.1.2.cmml" xref="S4.E10.m1.4.4.1.1.1.1"><ci id="S4.E10.m1.1.1.cmml" xref="S4.E10.m1.1.1">𝑦</ci><apply id="S4.E10.m1.4.4.1.1.1.1.1.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1"><times id="S4.E10.m1.4.4.1.1.1.1.1.2.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.2"></times><ci id="S4.E10.m1.4.4.1.1.1.1.1.3.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.3">𝜎</ci><apply id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E10.m1.4.4.1.1.1.1.1.1.1.1.3">𝑠</ci></apply></apply></interval></apply><apply id="S4.E10.m1.6.6.3.3.cmml" xref="S4.E10.m1.6.6.3.3"><times id="S4.E10.m1.6.6.3.3.3.cmml" xref="S4.E10.m1.6.6.3.3.3"></times><ci id="S4.E10.m1.6.6.3.3.4.cmml" xref="S4.E10.m1.6.6.3.3.4">𝛽</ci><ci id="S4.E10.m1.6.6.3.3.5.cmml" xref="S4.E10.m1.6.6.3.3.5">ℋ</ci><interval closure="open" id="S4.E10.m1.6.6.3.3.2.3.cmml" xref="S4.E10.m1.6.6.3.3.2.2"><apply id="S4.E10.m1.5.5.2.2.1.1.1.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1"><times id="S4.E10.m1.5.5.2.2.1.1.1.2.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.2"></times><ci id="S4.E10.m1.5.5.2.2.1.1.1.3.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.3">𝜎</ci><interval closure="open" id="S4.E10.m1.5.5.2.2.1.1.1.1.2.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1"><apply id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.2">𝑧</ci><ci id="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E10.m1.5.5.2.2.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S4.E10.m1.2.2.cmml" xref="S4.E10.m1.2.2">𝑇</ci></interval></apply><apply id="S4.E10.m1.6.6.3.3.2.2.2.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2"><times id="S4.E10.m1.6.6.3.3.2.2.2.2.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.2"></times><ci id="S4.E10.m1.6.6.3.3.2.2.2.3.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.3">𝜎</ci><interval closure="open" id="S4.E10.m1.6.6.3.3.2.2.2.1.2.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1"><apply id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.1.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1">subscript</csymbol><ci id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.2.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.2">𝑧</ci><ci id="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.3.cmml" xref="S4.E10.m1.6.6.3.3.2.2.2.1.1.1.3">𝑠</ci></apply><ci id="S4.E10.m1.3.3.cmml" xref="S4.E10.m1.3.3">𝑇</ci></interval></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E10.m1.6c">\small\mathcal{L}=\alpha\mathcal{H}(y,\sigma(z_{s}))+\beta\mathcal{H}(\sigma(z_{t},T),\sigma(z_{s},T))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS4.p2.7">식 중 <a class="ltx_ref" href="#S4.E10" title="In IV-D Distillation-Assisted Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">10</span></a>, <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><mi id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><ci id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">\alpha</annotation></semantics></math> 및 <math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS4.p2.2.m2.1"><semantics id="S4.SS4.p2.2.m2.1a"><mi id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><ci id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">\beta</annotation></semantics></math>는 학생 모델로부터의 손실량과 증류 손실을 조정하기 위한 가중 계수이고, <math alttext="y" class="ltx_Math" display="inline" id="S4.SS4.p2.3.m3.1"><semantics id="S4.SS4.p2.3.m3.1a"><mi id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><ci id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">y</annotation></semantics></math>는 지상-진실 클래스 라벨이고, <math alttext="\mathcal{H}" class="ltx_Math" display="inline" id="S4.SS4.p2.4.m4.1"><semantics id="S4.SS4.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml">ℋ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><ci id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1">ℋ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">\mathcal{H}</annotation></semantics></math>는 교차 엔트로피 손실 함수이고, <math alttext="z_{s}" class="ltx_Math" display="inline" id="S4.SS4.p2.5.m5.1"><semantics id="S4.SS4.p2.5.m5.1a"><msub id="S4.SS4.p2.5.m5.1.1" xref="S4.SS4.p2.5.m5.1.1.cmml"><mi id="S4.SS4.p2.5.m5.1.1.2" xref="S4.SS4.p2.5.m5.1.1.2.cmml">z</mi><mi id="S4.SS4.p2.5.m5.1.1.3" xref="S4.SS4.p2.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.5.m5.1b"><apply id="S4.SS4.p2.5.m5.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.5.m5.1.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS4.p2.5.m5.1.1.2.cmml" xref="S4.SS4.p2.5.m5.1.1.2">𝑧</ci><ci id="S4.SS4.p2.5.m5.1.1.3.cmml" xref="S4.SS4.p2.5.m5.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.5.m5.1c">z_{s}</annotation></semantics></math>/<math alttext="z_{t}" class="ltx_Math" display="inline" id="S4.SS4.p2.6.m6.1"><semantics id="S4.SS4.p2.6.m6.1a"><msub id="S4.SS4.p2.6.m6.1.1" xref="S4.SS4.p2.6.m6.1.1.cmml"><mi id="S4.SS4.p2.6.m6.1.1.2" xref="S4.SS4.p2.6.m6.1.1.2.cmml">z</mi><mi id="S4.SS4.p2.6.m6.1.1.3" xref="S4.SS4.p2.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.6.m6.1b"><apply id="S4.SS4.p2.6.m6.1.1.cmml" xref="S4.SS4.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS4.p2.6.m6.1.1.1.cmml" xref="S4.SS4.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS4.p2.6.m6.1.1.2.cmml" xref="S4.SS4.p2.6.m6.1.1.2">𝑧</ci><ci id="S4.SS4.p2.6.m6.1.1.3.cmml" xref="S4.SS4.p2.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.6.m6.1c">z_{t}</annotation></semantics></math>는 학생/교사 모델에 의해 생성된 로짓이고, <math alttext="\sigma" class="ltx_Math" display="inline" id="S4.SS4.p2.7.m7.1"><semantics id="S4.SS4.p2.7.m7.1a"><mi id="S4.SS4.p2.7.m7.1.1" xref="S4.SS4.p2.7.m7.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.7.m7.1b"><ci id="S4.SS4.p2.7.m7.1.1.cmml" xref="S4.SS4.p2.7.m7.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.7.m7.1c">\sigma</annotation></semantics></math>는 소프트맥스 함수이고, T는 그 온도를 다음과 같이 정의한다:</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<table id="S4.E11" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E11.m1.1" class="ltx_Math" alttext="\small p_{i}=\frac{\exp{\frac{z_{i}}{T}}}{\sum_{j}\exp{\frac{z_{j}}{T}}}" display="block"><semantics id="S4.E11.m1.1a"><mrow id="S4.E11.m1.1.1" xref="S4.E11.m1.1.1.cmml"><msub id="S4.E11.m1.1.1.2" xref="S4.E11.m1.1.1.2.cmml"><mi mathsize="90%" id="S4.E11.m1.1.1.2.2" xref="S4.E11.m1.1.1.2.2.cmml">p</mi><mi mathsize="90%" id="S4.E11.m1.1.1.2.3" xref="S4.E11.m1.1.1.2.3.cmml">i</mi></msub><mo mathsize="90%" id="S4.E11.m1.1.1.1" xref="S4.E11.m1.1.1.1.cmml">=</mo><mfrac id="S4.E11.m1.1.1.3" xref="S4.E11.m1.1.1.3.cmml"><mrow id="S4.E11.m1.1.1.3.2" xref="S4.E11.m1.1.1.3.2.cmml"><mi mathsize="90%" id="S4.E11.m1.1.1.3.2.1" xref="S4.E11.m1.1.1.3.2.1.cmml">exp</mi><mo lspace="0.167em" id="S4.E11.m1.1.1.3.2a" xref="S4.E11.m1.1.1.3.2.cmml">⁡</mo><mfrac id="S4.E11.m1.1.1.3.2.2" xref="S4.E11.m1.1.1.3.2.2.cmml"><msub id="S4.E11.m1.1.1.3.2.2.2" xref="S4.E11.m1.1.1.3.2.2.2.cmml"><mi mathsize="90%" id="S4.E11.m1.1.1.3.2.2.2.2" xref="S4.E11.m1.1.1.3.2.2.2.2.cmml">z</mi><mi mathsize="90%" id="S4.E11.m1.1.1.3.2.2.2.3" xref="S4.E11.m1.1.1.3.2.2.2.3.cmml">i</mi></msub><mi mathsize="90%" id="S4.E11.m1.1.1.3.2.2.3" xref="S4.E11.m1.1.1.3.2.2.3.cmml">T</mi></mfrac></mrow><mrow id="S4.E11.m1.1.1.3.3" xref="S4.E11.m1.1.1.3.3.cmml"><msub id="S4.E11.m1.1.1.3.3.1" xref="S4.E11.m1.1.1.3.3.1.cmml"><mo maxsize="90%" minsize="90%" stretchy="true" id="S4.E11.m1.1.1.3.3.1.2" xref="S4.E11.m1.1.1.3.3.1.2.cmml">∑</mo><mi mathsize="90%" id="S4.E11.m1.1.1.3.3.1.3" xref="S4.E11.m1.1.1.3.3.1.3.cmml">j</mi></msub><mrow id="S4.E11.m1.1.1.3.3.2" xref="S4.E11.m1.1.1.3.3.2.cmml"><mi mathsize="90%" id="S4.E11.m1.1.1.3.3.2.1" xref="S4.E11.m1.1.1.3.3.2.1.cmml">exp</mi><mo lspace="0.167em" id="S4.E11.m1.1.1.3.3.2a" xref="S4.E11.m1.1.1.3.3.2.cmml">⁡</mo><mfrac id="S4.E11.m1.1.1.3.3.2.2" xref="S4.E11.m1.1.1.3.3.2.2.cmml"><msub id="S4.E11.m1.1.1.3.3.2.2.2" xref="S4.E11.m1.1.1.3.3.2.2.2.cmml"><mi mathsize="90%" id="S4.E11.m1.1.1.3.3.2.2.2.2" xref="S4.E11.m1.1.1.3.3.2.2.2.2.cmml">z</mi><mi mathsize="90%" id="S4.E11.m1.1.1.3.3.2.2.2.3" xref="S4.E11.m1.1.1.3.3.2.2.2.3.cmml">j</mi></msub><mi mathsize="90%" id="S4.E11.m1.1.1.3.3.2.2.3" xref="S4.E11.m1.1.1.3.3.2.2.3.cmml">T</mi></mfrac></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E11.m1.1b"><apply id="S4.E11.m1.1.1.cmml" xref="S4.E11.m1.1.1"><eq id="S4.E11.m1.1.1.1.cmml" xref="S4.E11.m1.1.1.1"></eq><apply id="S4.E11.m1.1.1.2.cmml" xref="S4.E11.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E11.m1.1.1.2.1.cmml" xref="S4.E11.m1.1.1.2">subscript</csymbol><ci id="S4.E11.m1.1.1.2.2.cmml" xref="S4.E11.m1.1.1.2.2">𝑝</ci><ci id="S4.E11.m1.1.1.2.3.cmml" xref="S4.E11.m1.1.1.2.3">𝑖</ci></apply><apply id="S4.E11.m1.1.1.3.cmml" xref="S4.E11.m1.1.1.3"><divide id="S4.E11.m1.1.1.3.1.cmml" xref="S4.E11.m1.1.1.3"></divide><apply id="S4.E11.m1.1.1.3.2.cmml" xref="S4.E11.m1.1.1.3.2"><exp id="S4.E11.m1.1.1.3.2.1.cmml" xref="S4.E11.m1.1.1.3.2.1"></exp><apply id="S4.E11.m1.1.1.3.2.2.cmml" xref="S4.E11.m1.1.1.3.2.2"><divide id="S4.E11.m1.1.1.3.2.2.1.cmml" xref="S4.E11.m1.1.1.3.2.2"></divide><apply id="S4.E11.m1.1.1.3.2.2.2.cmml" xref="S4.E11.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S4.E11.m1.1.1.3.2.2.2.1.cmml" xref="S4.E11.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S4.E11.m1.1.1.3.2.2.2.2.cmml" xref="S4.E11.m1.1.1.3.2.2.2.2">𝑧</ci><ci id="S4.E11.m1.1.1.3.2.2.2.3.cmml" xref="S4.E11.m1.1.1.3.2.2.2.3">𝑖</ci></apply><ci id="S4.E11.m1.1.1.3.2.2.3.cmml" xref="S4.E11.m1.1.1.3.2.2.3">𝑇</ci></apply></apply><apply id="S4.E11.m1.1.1.3.3.cmml" xref="S4.E11.m1.1.1.3.3"><apply id="S4.E11.m1.1.1.3.3.1.cmml" xref="S4.E11.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E11.m1.1.1.3.3.1.1.cmml" xref="S4.E11.m1.1.1.3.3.1">subscript</csymbol><sum id="S4.E11.m1.1.1.3.3.1.2.cmml" xref="S4.E11.m1.1.1.3.3.1.2"></sum><ci id="S4.E11.m1.1.1.3.3.1.3.cmml" xref="S4.E11.m1.1.1.3.3.1.3">𝑗</ci></apply><apply id="S4.E11.m1.1.1.3.3.2.cmml" xref="S4.E11.m1.1.1.3.3.2"><exp id="S4.E11.m1.1.1.3.3.2.1.cmml" xref="S4.E11.m1.1.1.3.3.2.1"></exp><apply id="S4.E11.m1.1.1.3.3.2.2.cmml" xref="S4.E11.m1.1.1.3.3.2.2"><divide id="S4.E11.m1.1.1.3.3.2.2.1.cmml" xref="S4.E11.m1.1.1.3.3.2.2"></divide><apply id="S4.E11.m1.1.1.3.3.2.2.2.cmml" xref="S4.E11.m1.1.1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.E11.m1.1.1.3.3.2.2.2.1.cmml" xref="S4.E11.m1.1.1.3.3.2.2.2">subscript</csymbol><ci id="S4.E11.m1.1.1.3.3.2.2.2.2.cmml" xref="S4.E11.m1.1.1.3.3.2.2.2.2">𝑧</ci><ci id="S4.E11.m1.1.1.3.3.2.2.2.3.cmml" xref="S4.E11.m1.1.1.3.3.2.2.2.3">𝑗</ci></apply><ci id="S4.E11.m1.1.1.3.3.2.2.3.cmml" xref="S4.E11.m1.1.1.3.3.2.2.3">𝑇</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E11.m1.1c">\small p_{i}=\frac{\exp{\frac{z_{i}}{T}}}{\sum_{j}\exp{\frac{z_{j}}{T}}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p class="ltx_p" id="S4.SS4.p4.1">이전의 지식 증류 방법은 다양한 지식 소스를 탐색하는 데 중점을 둔다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib95" title="">95</a>, <a class="ltx_ref" href="#bib.bib150" title="">150</a>, <a class="ltx_ref" href="#bib.bib192" title="">192</a>]</cite>는 지식의 원천으로 로짓(연성 확률)을 사용하는 반면, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib207" title="">207</a>, <a class="ltx_ref" href="#bib.bib269" title="">269</a>, <a class="ltx_ref" href="#bib.bib3" title="">3</a>]</cite>는 중간 계층의 지식을 활용하려고 한다. 교사 모델의 선택도 잘 연구되어 있는데, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib273" title="">273</a>, <a class="ltx_ref" href="#bib.bib235" title="">235</a>]</cite>는 여러 교사 모델을 사용하여 학생 모델을 공동으로 감독하는 반면, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib43" title="">43</a>, <a class="ltx_ref" href="#bib.bib277" title="">277</a>]</cite>는 추가 교사 모델 없이 자가 증류법을 적용한다.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS5.5.1.1" class="ltx_text">IV-E</span> </span><span id="S4.SS5.6.2" class="ltx_text ltx_font_italic">Extreme Quantization</span>
</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p1.1">양자화된 값을 1비트 표현으로 제한하여 메모리 요구량을 32<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><mo id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><times id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">\times</annotation></semantics></math>로 대폭 줄이는 이진화가 가장 극단적인 양자화 방법이다. 메모리 이점 외에도 이진(1-비트) 및 삼진(2-비트) 연산은 종종 비트 단위 연산으로 효율적으로 계산될 수 있고 FP32 및 INT8과 같은 더 높은 정밀도에서 상당한 가속을 달성할 수 있다. 예를 들어, NVIDIA V100 GPU에서 피크 이진 연산은 INT8보다 8배 더 높다. 그러나 순진한 이진화 방법은 상당한 정확도 저하를 초래할 것이다. 이와 같이 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib131" title="">131</a>, <a class="ltx_ref" href="#bib.bib135" title="">135</a>, <a class="ltx_ref" href="#bib.bib249" title="">249</a>, <a class="ltx_ref" href="#bib.bib262" title="">262</a>, <a class="ltx_ref" href="#bib.bib122" title="">122</a>, <a class="ltx_ref" href="#bib.bib198" title="">198</a>, <a class="ltx_ref" href="#bib.bib290" title="">290</a>, <a class="ltx_ref" href="#bib.bib251" title="">251</a>, <a class="ltx_ref" href="#bib.bib160" title="">160</a>, <a class="ltx_ref" href="#bib.bib288" title="">288</a>, <a class="ltx_ref" href="#bib.bib260" title="">260</a>, <a class="ltx_ref" href="#bib.bib92" title="">92</a>, <a class="ltx_ref" href="#bib.bib25" title="">25</a>, <a class="ltx_ref" href="#bib.bib78" title="">78</a>, <a class="ltx_ref" href="#bib.bib124" title="">124</a>, <a class="ltx_ref" href="#bib.bib52" title="">52</a>, <a class="ltx_ref" href="#bib.bib93" title="">93</a>, <a class="ltx_ref" href="#bib.bib141" title="">141</a>, <a class="ltx_ref" href="#bib.bib217" title="">217</a>, <a class="ltx_ref" href="#bib.bib120" title="">120</a>, <a class="ltx_ref" href="#bib.bib155" title="">155</a>, <a class="ltx_ref" href="#bib.bib129" title="">129</a>, <a class="ltx_ref" href="#bib.bib149" title="">149</a>, <a class="ltx_ref" href="#bib.bib83" title="">83</a>, <a class="ltx_ref" href="#bib.bib196" title="">196</a>, <a class="ltx_ref" href="#bib.bib18" title="">18</a>, <a class="ltx_ref" href="#bib.bib47" title="">47</a>, <a class="ltx_ref" href="#bib.bib77" title="">77</a>, <a class="ltx_ref" href="#bib.bib205" title="">205</a>]</cite>를 해결하기 위해 서로 다른 솔루션을 제안한 많은 작업이 있다.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p2.6">여기에서 중요한 작업은 가중치를 +1 또는 -1로 제한하는 BinaryConnect <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib42" title="">42</a>]</cite>이다. 이 접근법에서 가중치는 실제 값으로 유지되며 이진화 효과를 시뮬레이션하기 위해 전진 및 후진 패스 동안에만 이진화된다. 전진 패스 동안, 실수 값 가중치들은 부호 함수에 기초하여 +1 또는 -1로 변환된다. 그런 다음 STE를 사용하여 표준 훈련 방법을 사용하여 네트워크를 훈련하여 미분 불가능한 부호 함수를 통해 기울기를 전파할 수 있다. 이진화된 NN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib107" title="">107</a>]</cite> (BNN)는 가중치뿐만 아니라 활성화도 이진화하여 이 아이디어를 확장한다. 가중치 및 활성화를 공동 이진화하는 것은 비용이 많이 드는 부동 소수점 행렬 곱셈을 경량 XNOR 연산에 이어 비트 카운팅으로 대체할 수 있기 때문에 향상된 지연 시간의 추가적인 이점을 갖는다. 또 다른 흥미로운 작업은 가중치들에 스케일링 팩터를 통합하고 +1 또는 -1 대신에 +<math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS5.p2.1.m1.1"><semantics id="S4.SS5.p2.1.m1.1a"><mi id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><ci id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">\alpha</annotation></semantics></math> 또는 -<math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS5.p2.2.m2.1"><semantics id="S4.SS5.p2.2.m2.1a"><mi id="S4.SS5.p2.2.m2.1.1" xref="S4.SS5.p2.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><ci id="S4.SS5.p2.2.m2.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">\alpha</annotation></semantics></math>를 사용함으로써 더 높은 정확도를 달성하는 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib45" title="">45</a>]</cite>에서 제안된 BWN(Binary Weight Network) 및 XNOR-Net이다. 여기서, <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS5.p2.3.m3.1"><semantics id="S4.SS5.p2.3.m3.1a"><mi id="S4.SS5.p2.3.m3.1.1" xref="S4.SS5.p2.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.3.m3.1b"><ci id="S4.SS5.p2.3.m3.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.3.m3.1c">\alpha</annotation></semantics></math>는 실수값 가중치들과 결과 이진화된 가중치들 사이의 거리를 최소화하기 위해 선택된 스케일링 팩터이다. 다시 말해, 실수값 가중치 매트릭스 <math alttext="W" class="ltx_Math" display="inline" id="S4.SS5.p2.4.m4.1"><semantics id="S4.SS5.p2.4.m4.1a"><mi id="S4.SS5.p2.4.m4.1.1" xref="S4.SS5.p2.4.m4.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.4.m4.1b"><ci id="S4.SS5.p2.4.m4.1.1.cmml" xref="S4.SS5.p2.4.m4.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.4.m4.1c">W</annotation></semantics></math>는 <math alttext="W\approx\alpha B" class="ltx_Math" display="inline" id="S4.SS5.p2.5.m5.1"><semantics id="S4.SS5.p2.5.m5.1a"><mrow id="S4.SS5.p2.5.m5.1.1" xref="S4.SS5.p2.5.m5.1.1.cmml"><mi id="S4.SS5.p2.5.m5.1.1.2" xref="S4.SS5.p2.5.m5.1.1.2.cmml">W</mi><mo id="S4.SS5.p2.5.m5.1.1.1" xref="S4.SS5.p2.5.m5.1.1.1.cmml">≈</mo><mrow id="S4.SS5.p2.5.m5.1.1.3" xref="S4.SS5.p2.5.m5.1.1.3.cmml"><mi id="S4.SS5.p2.5.m5.1.1.3.2" xref="S4.SS5.p2.5.m5.1.1.3.2.cmml">α</mi><mo id="S4.SS5.p2.5.m5.1.1.3.1" lspace="0em" rspace="0em" xref="S4.SS5.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.SS5.p2.5.m5.1.1.3.3" xref="S4.SS5.p2.5.m5.1.1.3.3.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.5.m5.1b"><apply id="S4.SS5.p2.5.m5.1.1.cmml" xref="S4.SS5.p2.5.m5.1.1"><approx id="S4.SS5.p2.5.m5.1.1.1.cmml" xref="S4.SS5.p2.5.m5.1.1.1"></approx><ci id="S4.SS5.p2.5.m5.1.1.2.cmml" xref="S4.SS5.p2.5.m5.1.1.2">𝑊</ci><apply id="S4.SS5.p2.5.m5.1.1.3.cmml" xref="S4.SS5.p2.5.m5.1.1.3"><times id="S4.SS5.p2.5.m5.1.1.3.1.cmml" xref="S4.SS5.p2.5.m5.1.1.3.1"></times><ci id="S4.SS5.p2.5.m5.1.1.3.2.cmml" xref="S4.SS5.p2.5.m5.1.1.3.2">𝛼</ci><ci id="S4.SS5.p2.5.m5.1.1.3.3.cmml" xref="S4.SS5.p2.5.m5.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.5.m5.1c">W\approx\alpha B</annotation></semantics></math>로 공식화될 수 있으며, 여기서 <math alttext="B" class="ltx_Math" display="inline" id="S4.SS5.p2.6.m6.1"><semantics id="S4.SS5.p2.6.m6.1a"><mi id="S4.SS5.p2.6.m6.1.1" xref="S4.SS5.p2.6.m6.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.6.m6.1b"><ci id="S4.SS5.p2.6.m6.1.1.cmml" xref="S4.SS5.p2.6.m6.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.6.m6.1c">B</annotation></semantics></math>는 다음의 최적화 문제를 만족하는 이진 가중치 매트릭스이다:</p>
<table id="S4.E12" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E12.m1.3" class="ltx_Math" alttext="\small\alpha,B=\mathrm{argmin}\|W-\alpha B\|^{2}." display="block"><semantics id="S4.E12.m1.3a"><mrow id="S4.E12.m1.3.3.1" xref="S4.E12.m1.3.3.1.1.cmml"><mrow id="S4.E12.m1.3.3.1.1" xref="S4.E12.m1.3.3.1.1.cmml"><mrow id="S4.E12.m1.3.3.1.1.3.2" xref="S4.E12.m1.3.3.1.1.3.1.cmml"><mi mathsize="90%" id="S4.E12.m1.1.1" xref="S4.E12.m1.1.1.cmml">α</mi><mo mathsize="90%" id="S4.E12.m1.3.3.1.1.3.2.1" xref="S4.E12.m1.3.3.1.1.3.1.cmml">,</mo><mi mathsize="90%" id="S4.E12.m1.2.2" xref="S4.E12.m1.2.2.cmml">B</mi></mrow><mo mathsize="90%" id="S4.E12.m1.3.3.1.1.2" xref="S4.E12.m1.3.3.1.1.2.cmml">=</mo><mrow id="S4.E12.m1.3.3.1.1.1" xref="S4.E12.m1.3.3.1.1.1.cmml"><mi mathsize="90%" id="S4.E12.m1.3.3.1.1.1.3" xref="S4.E12.m1.3.3.1.1.1.3.cmml">argmin</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.3.3.1.1.1.2" xref="S4.E12.m1.3.3.1.1.1.2.cmml">​</mo><msup id="S4.E12.m1.3.3.1.1.1.1" xref="S4.E12.m1.3.3.1.1.1.1.cmml"><mrow id="S4.E12.m1.3.3.1.1.1.1.1.1" xref="S4.E12.m1.3.3.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.2" xref="S4.E12.m1.3.3.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E12.m1.3.3.1.1.1.1.1.1.1" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.1.2" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.2.cmml">W</mi><mo mathsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.1.1" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">α</mi><mo lspace="0em" rspace="0em" id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.1" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">B</mi></mrow></mrow><mo maxsize="90%" minsize="90%" id="S4.E12.m1.3.3.1.1.1.1.1.1.3" xref="S4.E12.m1.3.3.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn mathsize="90%" id="S4.E12.m1.3.3.1.1.1.1.3" xref="S4.E12.m1.3.3.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow><mo lspace="0em" mathsize="90%" id="S4.E12.m1.3.3.1.2" xref="S4.E12.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E12.m1.3b"><apply id="S4.E12.m1.3.3.1.1.cmml" xref="S4.E12.m1.3.3.1"><eq id="S4.E12.m1.3.3.1.1.2.cmml" xref="S4.E12.m1.3.3.1.1.2"></eq><list id="S4.E12.m1.3.3.1.1.3.1.cmml" xref="S4.E12.m1.3.3.1.1.3.2"><ci id="S4.E12.m1.1.1.cmml" xref="S4.E12.m1.1.1">𝛼</ci><ci id="S4.E12.m1.2.2.cmml" xref="S4.E12.m1.2.2">𝐵</ci></list><apply id="S4.E12.m1.3.3.1.1.1.cmml" xref="S4.E12.m1.3.3.1.1.1"><times id="S4.E12.m1.3.3.1.1.1.2.cmml" xref="S4.E12.m1.3.3.1.1.1.2"></times><ci id="S4.E12.m1.3.3.1.1.1.3.cmml" xref="S4.E12.m1.3.3.1.1.1.3">argmin</ci><apply id="S4.E12.m1.3.3.1.1.1.1.cmml" xref="S4.E12.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.E12.m1.3.3.1.1.1.1.2.cmml" xref="S4.E12.m1.3.3.1.1.1.1">superscript</csymbol><apply id="S4.E12.m1.3.3.1.1.1.1.1.2.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E12.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E12.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1"><minus id="S4.E12.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.1"></minus><ci id="S4.E12.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.2">𝑊</ci><apply id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3"><times id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.1"></times><ci id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.2">𝛼</ci><ci id="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E12.m1.3.3.1.1.1.1.1.1.1.3.3">𝐵</ci></apply></apply></apply><cn type="integer" id="S4.E12.m1.3.3.1.1.1.1.3.cmml" xref="S4.E12.m1.3.3.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E12.m1.3c">\small\alpha,B=\mathrm{argmin}\|W-\alpha B\|^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p3.1">또한, 많은 학습된 가중치들이 0에 가깝다는 관찰로부터 영감을 받아, 가중치/활성화를 삼진 값, 예를 들어 +1, 0 및 -1로 제한함으로써 네트워크를 삼진화하려는 시도들이 있었고, 이에 의해 양자화된 값들이 0<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib159" title="">159</a>, <a class="ltx_ref" href="#bib.bib145" title="">145</a>]</cite>가 되도록 명시적으로 허용하였다. 삼진화는 이진화처럼 값비싼 행렬 곱셈을 제거함으로써 추론 지연 시간을 획기적으로 줄인다. 나중에, Ternary-Binary Network (TBN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib244" title="">244</a>]</cite>는 이진 네트워크 가중치와 삼진 활성화를 결합하면 정확도와 계산 효율성 사이에서 최적의 트레이드오프를 달성할 수 있음을 보여준다.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p4.1">나이브 이진화 및 삼진화 방법은 일반적으로 이미지넷 분류와 같은 복잡한 작업에 대해 심각한 정확도 저하를 초래하기 때문에 극단적인 양자화에서 정확도 저하를 줄이기 위해 많은 솔루션이 제안되었다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib197" title="">197</a>]</cite>의 작업은 이러한 해들을 크게 세 갈래로 분류한다. 여기서는 각 브랜치에 대해 간략하게 논의하고 자세한 내용은 관심 있는 독자를 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib197" title="">197</a>]</cite>에 참조한다.</p>
</div>
<section id="S4.SS5.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Quantization Error Minimization</h5>

<div id="S4.SS5.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS5.SSS0.Px1.p1.1">솔루션의 첫 번째 분기는 양자화 오차, 즉 실제 값과 양자화된 값 사이의 간격 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib151" title="">151</a>, <a class="ltx_ref" href="#bib.bib103" title="">103</a>, <a class="ltx_ref" href="#bib.bib158" title="">158</a>, <a class="ltx_ref" href="#bib.bib164" title="">164</a>, <a class="ltx_ref" href="#bib.bib178" title="">178</a>, <a class="ltx_ref" href="#bib.bib34" title="">34</a>, <a class="ltx_ref" href="#bib.bib218" title="">218</a>, <a class="ltx_ref" href="#bib.bib19" title="">19</a>, <a class="ltx_ref" href="#bib.bib169" title="">169</a>, <a class="ltx_ref" href="#bib.bib62" title="">62</a>, <a class="ltx_ref" href="#bib.bib248" title="">248</a>]</cite>를 최소화하는 것을 목표로 한다. 실제 값 가중치/활성화를 나타내기 위해 단일 이진 행렬을 사용하는 대신, HORQ<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib151" title="">151</a>]</cite> 및 ABC-Net<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib158" title="">158</a>]</cite>는 양자화 오차를 줄이기 위해 다중 이진 행렬의 선형 조합, 즉 <math alttext="W\approx\alpha_{1}B_{1}+\cdots+\alpha_{M}B_{M}" class="ltx_Math" display="inline" id="S4.SS5.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS5.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS5.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.2.cmml">W</mi><mo id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.1.cmml">≈</mo><mrow id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mrow id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.cmml"><msub id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.cmml"><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.2.cmml">α</mi><mn id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.3.cmml">1</mn></msub><mo id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.1" lspace="0em" rspace="0em" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.1.cmml">​</mo><msub id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.cmml"><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.2.cmml">B</mi><mn id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.3.cmml">1</mn></msub></mrow><mo id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">+</mo><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.3" mathvariant="normal" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">⋯</mi><mo id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1a" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">+</mo><mrow id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.cmml"><msub id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.cmml"><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.2.cmml">α</mi><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.3.cmml">M</mi></msub><mo id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.1" lspace="0em" rspace="0em" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.1.cmml">​</mo><msub id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.cmml"><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.2" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.2.cmml">B</mi><mi id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.3" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.3.cmml">M</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1"><approx id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.1"></approx><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.2">𝑊</ci><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3"><plus id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.1"></plus><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2"><times id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.1"></times><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2">subscript</csymbol><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.2">𝛼</ci><cn id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.3.cmml" type="integer" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.2.3">1</cn></apply><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3">subscript</csymbol><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.2">𝐵</ci><cn id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.3.cmml" type="integer" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.2.3.3">1</cn></apply></apply><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.3">⋯</ci><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4"><times id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.1"></times><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2"><csymbol cd="ambiguous" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2">subscript</csymbol><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.2">𝛼</ci><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.2.3">𝑀</ci></apply><apply id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3"><csymbol cd="ambiguous" id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.1.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3">subscript</csymbol><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.2.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.2">𝐵</ci><ci id="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.3.cmml" xref="S4.SS5.SSS0.Px1.p1.1.m1.1.1.3.4.3.3">𝑀</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS0.Px1.p1.1.m1.1c">W\approx\alpha_{1}B_{1}+\cdots+\alpha_{M}B_{M}</annotation></semantics></math>를 사용한다. 활성화를 이진화하는 것이 후속 컨볼루션 블록에 대한 표현 능력을 감소시킨다는 사실에 영감을 받아 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib178" title="">178</a>]</cite> 및 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite>는 더 넓은 네트워크(즉, 더 많은 수의 필터를 갖는 네트워크)의 이진화가 정확도와 모델 크기 사이에서 좋은 트레이드오프를 달성할 수 있음을 보여준다.</p>
</div>
</section>
<section id="S4.SS5.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Improved Loss function</h5>

<div id="S4.SS5.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS5.SSS0.Px2.p1.1">작품의 또 다른 분야는 손실 함수 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib99" title="">99</a>, <a class="ltx_ref" href="#bib.bib98" title="">98</a>, <a class="ltx_ref" href="#bib.bib284" title="">284</a>, <a class="ltx_ref" href="#bib.bib48" title="">48</a>, <a class="ltx_ref" href="#bib.bib251" title="">251</a>]</cite>의 선택에 초점을 맞추고 있다. 여기에서 중요한 작업은 이진화/대체 가중치에 대한 손실을 직접 최소화하는 손실 인식 이진화 및 삼진화 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib99" title="">99</a>, <a class="ltx_ref" href="#bib.bib98" title="">98</a>]</cite>이다. 이는 가중치에만 근사하고 최종 손실을 고려하지 않는 다른 접근법과는 다르다. 완전 정밀 교사 모델로부터의 지식 증류는 이진화/대체화 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib33" title="">33</a>, <a class="ltx_ref" href="#bib.bib195" title="">195</a>, <a class="ltx_ref" href="#bib.bib260" title="">260</a>, <a class="ltx_ref" href="#bib.bib177" title="">177</a>]</cite> 이후의 정확도 저하를 복구하는 유망한 방법으로도 나타났다.</p>
</div>
</section>
<section id="S4.SS5.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Improved Training Method</h5>

<div id="S4.SS5.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS5.SSS0.Px3.p1.1">또 다른 흥미로운 작업 분야는 이진/2차 모델<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib44" title="">44</a>, <a class="ltx_ref" href="#bib.bib160" title="">160</a>, <a class="ltx_ref" href="#bib.bib73" title="">73</a>, <a class="ltx_ref" href="#bib.bib20" title="">20</a>, <a class="ltx_ref" href="#bib.bib164" title="">164</a>, <a class="ltx_ref" href="#bib.bib285" title="">285</a>, <a class="ltx_ref" href="#bib.bib288" title="">288</a>, <a class="ltx_ref" href="#bib.bib5" title="">5</a>]</cite>에 대한 더 나은 훈련 방법을 목표로 한다. 많은 노력들이 부호 함수를 통해 기울기를 역전파하는 데 있어 STE의 한계를 지적한다: STE는 [-1, 1]의 범위에 있는 가중치 및/또는 활성화에 대한 기울기만을 전파한다. 이를 해결하기 위해 BNN+<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib44" title="">44</a>]</cite>는 부호 함수의 도함수에 대한 연속 근사치를 도입하는 반면, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib261" title="">261</a>, <a class="ltx_ref" href="#bib.bib198" title="">198</a>, <a class="ltx_ref" href="#bib.bib272" title="">272</a>]</cite>는 부호 함수를 점진적으로 날카롭게 하고 부호 함수에 접근하는 매끄럽고 미분 가능한 함수로 대체한다. Bi-Real Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib164" title="">164</a>]</cite>는 연속적인 블록들에서 활성화들을 활성화들에 연결하는 아이덴티티 바로가기들을 소개하고, 이를 통해 32 비트 활성화들이 전파될 수 있다. 대부분의 연구는 추론 시간 지연을 줄이는 데 초점을 맞추고 있지만 DoReFa-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib285" title="">285</a>]</cite>는 훈련 속도를 가속화하기 위해 가중치와 활성화 외에도 기울기를 양자화한다.</p>
</div>
<div id="S4.SS5.SSS0.Px3.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS5.SSS0.Px3.p2.1">극한 양자화는 컴퓨터 비전 작업에서 많은 CNN 모델에 대한 모델 크기뿐만 아니라 추론/훈련 지연을 크게 줄이는 데 성공했다. 최근, 이 아이디어를 자연어 처리(Natural Language Processing; NLP) 작업들<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib278" title="">278</a>, <a class="ltx_ref" href="#bib.bib7" title="">7</a>, <a class="ltx_ref" href="#bib.bib121" title="">121</a>, <a class="ltx_ref" href="#bib.bib119" title="">119</a>]</cite>로 확장하려는 시도들이 있었다. 최첨단 NLP 모델(예: BERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib46" title="">46</a>]</cite>, RoBERTa<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib163" title="">163</a>]</cite>, GPT 계열<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib200" title="">200</a>, <a class="ltx_ref" href="#bib.bib201" title="">201</a>, <a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite>)의 엄청난 모델 크기와 추론 지연 시간을 고려할 때, 많은 양의 레이블이 지정되지 않은 데이터에 대해 사전 훈련된 극단적인 양자화는 NLP 추론 작업을 에지로 가져오기 위한 강력한 도구로 부상하고 있다.</p>
</div>
<div id="S4.SS5.SSS0.Px3.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS5.SSS0.Px3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.SSS0.Px3.p3.1.1">Summary(Extreme Quantization). </span> 극저비트 정밀 양자화는 매우 유망한 연구 라인이다. 그러나, 기존의 방법들은 매우 광범위한 튜닝 및 하이퍼파라미터 탐색이 수행되지 않는 한, 기준선에 비해 높은 정확도 저하를 초래하는 경우가 많다. 그러나 이러한 정확도 저하는 덜 중요한 응용 프로그램에서는 허용될 수 있다.</p>
</div>
</section>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS6.5.1.1" class="ltx_text">IV-F</span> </span><span id="S4.SS6.6.2" class="ltx_text ltx_font_italic">Vector Quantization</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS6.p1.1"><a class="ltx_ref" href="#S2" title="II General History of Quantization ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">II</span></a>에서 살펴본 바와 같이, 양자화는 기계 학습에서 발명된 것이 아니라 정보 이론, 특히 압축 도구로서의 디지털 신호 처리 분야에서 지난 세기 동안 널리 연구되어 왔다. 그러나 기계 학습을 위한 양자화 방법의 주요 차이점은 근본적으로 원래의 신호에 비해 최소한의 변화/오류로 신호를 압축하는 데 관심이 없다는 것이다. 대신에, 목표는 가능한 한 작은 손실을 초래하는 감소된-정밀 표현을 찾는 것이다. 따라서 양자화된 가중치/활성화가 양자화되지 않은 가중치/활성화와 멀리 떨어져 있으면 완전히 허용된다.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS6.p2.8">반면에, NN 양자화에 적용된 DSP의 고전적인 양자화 방법들, 특히 벡터 양자화 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib9" title="">9</a>]</cite>에는 많은 흥미로운 아이디어들이 있다. 특히 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib117" title="">117</a>, <a class="ltx_ref" href="#bib.bib74" title="">74</a>, <a class="ltx_ref" href="#bib.bib256" title="">256</a>, <a class="ltx_ref" href="#bib.bib189" title="">189</a>, <a class="ltx_ref" href="#bib.bib84" title="">84</a>, <a class="ltx_ref" href="#bib.bib1" title="">1</a>, <a class="ltx_ref" href="#bib.bib170" title="">170</a>, <a class="ltx_ref" href="#bib.bib180" title="">180</a>, <a class="ltx_ref" href="#bib.bib30" title="">30</a>]</cite>의 작업은 가중치를 서로 다른 그룹으로 군집화하고 추론 시 각 그룹의 중심을 양자화된 값으로 사용한다. 식에서 보는 바와 같이. <a class="ltx_ref" href="#S4.E13" title="In IV-F Vector Quantization ‣ IV Advanced Concepts: Quantization Below 8 bits ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">13</span></a>, <math alttext="i" class="ltx_Math" display="inline" id="S4.SS6.p2.1.m1.1"><semantics id="S4.SS6.p2.1.m1.1a"><mi id="S4.SS6.p2.1.m1.1.1" xref="S4.SS6.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.1.m1.1b"><ci id="S4.SS6.p2.1.m1.1.1.cmml" xref="S4.SS6.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.1.m1.1c">i</annotation></semantics></math>는 텐서 내의 가중치의 인덱스이고, <math alttext="c_{1},...,c_{k}" class="ltx_Math" display="inline" id="S4.SS6.p2.2.m2.3"><semantics id="S4.SS6.p2.2.m2.3a"><mrow id="S4.SS6.p2.2.m2.3.3.2" xref="S4.SS6.p2.2.m2.3.3.3.cmml"><msub id="S4.SS6.p2.2.m2.2.2.1.1" xref="S4.SS6.p2.2.m2.2.2.1.1.cmml"><mi id="S4.SS6.p2.2.m2.2.2.1.1.2" xref="S4.SS6.p2.2.m2.2.2.1.1.2.cmml">c</mi><mn id="S4.SS6.p2.2.m2.2.2.1.1.3" xref="S4.SS6.p2.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S4.SS6.p2.2.m2.3.3.2.3" xref="S4.SS6.p2.2.m2.3.3.3.cmml">,</mo><mi id="S4.SS6.p2.2.m2.1.1" mathvariant="normal" xref="S4.SS6.p2.2.m2.1.1.cmml">…</mi><mo id="S4.SS6.p2.2.m2.3.3.2.4" xref="S4.SS6.p2.2.m2.3.3.3.cmml">,</mo><msub id="S4.SS6.p2.2.m2.3.3.2.2" xref="S4.SS6.p2.2.m2.3.3.2.2.cmml"><mi id="S4.SS6.p2.2.m2.3.3.2.2.2" xref="S4.SS6.p2.2.m2.3.3.2.2.2.cmml">c</mi><mi id="S4.SS6.p2.2.m2.3.3.2.2.3" xref="S4.SS6.p2.2.m2.3.3.2.2.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.2.m2.3b"><list id="S4.SS6.p2.2.m2.3.3.3.cmml" xref="S4.SS6.p2.2.m2.3.3.2"><apply id="S4.SS6.p2.2.m2.2.2.1.1.cmml" xref="S4.SS6.p2.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS6.p2.2.m2.2.2.1.1.1.cmml" xref="S4.SS6.p2.2.m2.2.2.1.1">subscript</csymbol><ci id="S4.SS6.p2.2.m2.2.2.1.1.2.cmml" xref="S4.SS6.p2.2.m2.2.2.1.1.2">𝑐</ci><cn id="S4.SS6.p2.2.m2.2.2.1.1.3.cmml" type="integer" xref="S4.SS6.p2.2.m2.2.2.1.1.3">1</cn></apply><ci id="S4.SS6.p2.2.m2.1.1.cmml" xref="S4.SS6.p2.2.m2.1.1">…</ci><apply id="S4.SS6.p2.2.m2.3.3.2.2.cmml" xref="S4.SS6.p2.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS6.p2.2.m2.3.3.2.2.1.cmml" xref="S4.SS6.p2.2.m2.3.3.2.2">subscript</csymbol><ci id="S4.SS6.p2.2.m2.3.3.2.2.2.cmml" xref="S4.SS6.p2.2.m2.3.3.2.2.2">𝑐</ci><ci id="S4.SS6.p2.2.m2.3.3.2.2.3.cmml" xref="S4.SS6.p2.2.m2.3.3.2.2.3">𝑘</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.2.m2.3c">c_{1},...,c_{k}</annotation></semantics></math>는 클러스터링에 의해 발견된 <math alttext="k" class="ltx_Math" display="inline" id="S4.SS6.p2.3.m3.1"><semantics id="S4.SS6.p2.3.m3.1a"><mi id="S4.SS6.p2.3.m3.1.1" xref="S4.SS6.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.3.m3.1b"><ci id="S4.SS6.p2.3.m3.1.1.cmml" xref="S4.SS6.p2.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.3.m3.1c">k</annotation></semantics></math> centroids이고, <math alttext="c_{j}" class="ltx_Math" display="inline" id="S4.SS6.p2.4.m4.1"><semantics id="S4.SS6.p2.4.m4.1a"><msub id="S4.SS6.p2.4.m4.1.1" xref="S4.SS6.p2.4.m4.1.1.cmml"><mi id="S4.SS6.p2.4.m4.1.1.2" xref="S4.SS6.p2.4.m4.1.1.2.cmml">c</mi><mi id="S4.SS6.p2.4.m4.1.1.3" xref="S4.SS6.p2.4.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.4.m4.1b"><apply id="S4.SS6.p2.4.m4.1.1.cmml" xref="S4.SS6.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS6.p2.4.m4.1.1.1.cmml" xref="S4.SS6.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS6.p2.4.m4.1.1.2.cmml" xref="S4.SS6.p2.4.m4.1.1.2">𝑐</ci><ci id="S4.SS6.p2.4.m4.1.1.3.cmml" xref="S4.SS6.p2.4.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.4.m4.1c">c_{j}</annotation></semantics></math>는 <math alttext="w_{i}" class="ltx_Math" display="inline" id="S4.SS6.p2.5.m5.1"><semantics id="S4.SS6.p2.5.m5.1a"><msub id="S4.SS6.p2.5.m5.1.1" xref="S4.SS6.p2.5.m5.1.1.cmml"><mi id="S4.SS6.p2.5.m5.1.1.2" xref="S4.SS6.p2.5.m5.1.1.2.cmml">w</mi><mi id="S4.SS6.p2.5.m5.1.1.3" xref="S4.SS6.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.5.m5.1b"><apply id="S4.SS6.p2.5.m5.1.1.cmml" xref="S4.SS6.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS6.p2.5.m5.1.1.1.cmml" xref="S4.SS6.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS6.p2.5.m5.1.1.2.cmml" xref="S4.SS6.p2.5.m5.1.1.2">𝑤</ci><ci id="S4.SS6.p2.5.m5.1.1.3.cmml" xref="S4.SS6.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.5.m5.1c">w_{i}</annotation></semantics></math>에 대응하는 centroid이다. 클러스터링 후 가중치 <math alttext="w_{i}" class="ltx_Math" display="inline" id="S4.SS6.p2.6.m6.1"><semantics id="S4.SS6.p2.6.m6.1a"><msub id="S4.SS6.p2.6.m6.1.1" xref="S4.SS6.p2.6.m6.1.1.cmml"><mi id="S4.SS6.p2.6.m6.1.1.2" xref="S4.SS6.p2.6.m6.1.1.2.cmml">w</mi><mi id="S4.SS6.p2.6.m6.1.1.3" xref="S4.SS6.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.6.m6.1b"><apply id="S4.SS6.p2.6.m6.1.1.cmml" xref="S4.SS6.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS6.p2.6.m6.1.1.1.cmml" xref="S4.SS6.p2.6.m6.1.1">subscript</csymbol><ci id="S4.SS6.p2.6.m6.1.1.2.cmml" xref="S4.SS6.p2.6.m6.1.1.2">𝑤</ci><ci id="S4.SS6.p2.6.m6.1.1.3.cmml" xref="S4.SS6.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.6.m6.1c">w_{i}</annotation></semantics></math>는 코드북(look-up table)에서 <math alttext="c_{j}" class="ltx_Math" display="inline" id="S4.SS6.p2.8.m8.1"><semantics id="S4.SS6.p2.8.m8.1a"><msub id="S4.SS6.p2.8.m8.1.1" xref="S4.SS6.p2.8.m8.1.1.cmml"><mi id="S4.SS6.p2.8.m8.1.1.2" xref="S4.SS6.p2.8.m8.1.1.2.cmml">c</mi><mi id="S4.SS6.p2.8.m8.1.1.3" xref="S4.SS6.p2.8.m8.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.8.m8.1b"><apply id="S4.SS6.p2.8.m8.1.1.cmml" xref="S4.SS6.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS6.p2.8.m8.1.1.1.cmml" xref="S4.SS6.p2.8.m8.1.1">subscript</csymbol><ci id="S4.SS6.p2.8.m8.1.1.2.cmml" xref="S4.SS6.p2.8.m8.1.1.2">𝑐</ci><ci id="S4.SS6.p2.8.m8.1.1.3.cmml" xref="S4.SS6.p2.8.m8.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.8.m8.1c">c_{j}</annotation></semantics></math>와 관련된 클러스터 인덱스 <math alttext="j" class="ltx_Math" display="inline" id="S4.SS6.p2.7.m7.1"><semantics id="S4.SS6.p2.7.m7.1a"><mi id="S4.SS6.p2.7.m7.1.1" xref="S4.SS6.p2.7.m7.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.7.m7.1b"><ci id="S4.SS6.p2.7.m7.1.1.cmml" xref="S4.SS6.p2.7.m7.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.7.m7.1c">j</annotation></semantics></math>를 가질 것이다.</p>
<table id="S4.E13" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E13.m1.4" class="ltx_Math" alttext="\small\min_{c_{1},...,c_{k}}\sum_{i}\|w_{i}-c_{j}\|^{2}" display="block"><semantics id="S4.E13.m1.4a"><mrow id="S4.E13.m1.4.4" xref="S4.E13.m1.4.4.cmml"><munder id="S4.E13.m1.4.4.3" xref="S4.E13.m1.4.4.3.cmml"><mi mathsize="90%" id="S4.E13.m1.4.4.3.2" xref="S4.E13.m1.4.4.3.2.cmml">min</mi><mrow id="S4.E13.m1.3.3.3.3" xref="S4.E13.m1.3.3.3.4.cmml"><msub id="S4.E13.m1.2.2.2.2.1" xref="S4.E13.m1.2.2.2.2.1.cmml"><mi mathsize="90%" id="S4.E13.m1.2.2.2.2.1.2" xref="S4.E13.m1.2.2.2.2.1.2.cmml">c</mi><mn mathsize="90%" id="S4.E13.m1.2.2.2.2.1.3" xref="S4.E13.m1.2.2.2.2.1.3.cmml">1</mn></msub><mo mathsize="90%" id="S4.E13.m1.3.3.3.3.3" xref="S4.E13.m1.3.3.3.4.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="S4.E13.m1.1.1.1.1" xref="S4.E13.m1.1.1.1.1.cmml">…</mi><mo mathsize="90%" id="S4.E13.m1.3.3.3.3.4" xref="S4.E13.m1.3.3.3.4.cmml">,</mo><msub id="S4.E13.m1.3.3.3.3.2" xref="S4.E13.m1.3.3.3.3.2.cmml"><mi mathsize="90%" id="S4.E13.m1.3.3.3.3.2.2" xref="S4.E13.m1.3.3.3.3.2.2.cmml">c</mi><mi mathsize="90%" id="S4.E13.m1.3.3.3.3.2.3" xref="S4.E13.m1.3.3.3.3.2.3.cmml">k</mi></msub></mrow></munder><mo lspace="0em" rspace="0em" id="S4.E13.m1.4.4.2" xref="S4.E13.m1.4.4.2.cmml">​</mo><mrow id="S4.E13.m1.4.4.1" xref="S4.E13.m1.4.4.1.cmml"><munder id="S4.E13.m1.4.4.1.2" xref="S4.E13.m1.4.4.1.2.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" rspace="0em" stretchy="true" id="S4.E13.m1.4.4.1.2.2" xref="S4.E13.m1.4.4.1.2.2.cmml">∑</mo><mi mathsize="90%" id="S4.E13.m1.4.4.1.2.3" xref="S4.E13.m1.4.4.1.2.3.cmml">i</mi></munder><msup id="S4.E13.m1.4.4.1.1" xref="S4.E13.m1.4.4.1.1.cmml"><mrow id="S4.E13.m1.4.4.1.1.1.1" xref="S4.E13.m1.4.4.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.E13.m1.4.4.1.1.1.1.2" xref="S4.E13.m1.4.4.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E13.m1.4.4.1.1.1.1.1" xref="S4.E13.m1.4.4.1.1.1.1.1.cmml"><msub id="S4.E13.m1.4.4.1.1.1.1.1.2" xref="S4.E13.m1.4.4.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S4.E13.m1.4.4.1.1.1.1.1.2.2" xref="S4.E13.m1.4.4.1.1.1.1.1.2.2.cmml">w</mi><mi mathsize="90%" id="S4.E13.m1.4.4.1.1.1.1.1.2.3" xref="S4.E13.m1.4.4.1.1.1.1.1.2.3.cmml">i</mi></msub><mo mathsize="90%" id="S4.E13.m1.4.4.1.1.1.1.1.1" xref="S4.E13.m1.4.4.1.1.1.1.1.1.cmml">−</mo><msub id="S4.E13.m1.4.4.1.1.1.1.1.3" xref="S4.E13.m1.4.4.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S4.E13.m1.4.4.1.1.1.1.1.3.2" xref="S4.E13.m1.4.4.1.1.1.1.1.3.2.cmml">c</mi><mi mathsize="90%" id="S4.E13.m1.4.4.1.1.1.1.1.3.3" xref="S4.E13.m1.4.4.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo maxsize="90%" minsize="90%" id="S4.E13.m1.4.4.1.1.1.1.3" xref="S4.E13.m1.4.4.1.1.1.2.1.cmml">‖</mo></mrow><mn mathsize="90%" id="S4.E13.m1.4.4.1.1.3" xref="S4.E13.m1.4.4.1.1.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E13.m1.4b"><apply id="S4.E13.m1.4.4.cmml" xref="S4.E13.m1.4.4"><times id="S4.E13.m1.4.4.2.cmml" xref="S4.E13.m1.4.4.2"></times><apply id="S4.E13.m1.4.4.3.cmml" xref="S4.E13.m1.4.4.3"><csymbol cd="ambiguous" id="S4.E13.m1.4.4.3.1.cmml" xref="S4.E13.m1.4.4.3">subscript</csymbol><min id="S4.E13.m1.4.4.3.2.cmml" xref="S4.E13.m1.4.4.3.2"></min><list id="S4.E13.m1.3.3.3.4.cmml" xref="S4.E13.m1.3.3.3.3"><apply id="S4.E13.m1.2.2.2.2.1.cmml" xref="S4.E13.m1.2.2.2.2.1"><csymbol cd="ambiguous" id="S4.E13.m1.2.2.2.2.1.1.cmml" xref="S4.E13.m1.2.2.2.2.1">subscript</csymbol><ci id="S4.E13.m1.2.2.2.2.1.2.cmml" xref="S4.E13.m1.2.2.2.2.1.2">𝑐</ci><cn type="integer" id="S4.E13.m1.2.2.2.2.1.3.cmml" xref="S4.E13.m1.2.2.2.2.1.3">1</cn></apply><ci id="S4.E13.m1.1.1.1.1.cmml" xref="S4.E13.m1.1.1.1.1">…</ci><apply id="S4.E13.m1.3.3.3.3.2.cmml" xref="S4.E13.m1.3.3.3.3.2"><csymbol cd="ambiguous" id="S4.E13.m1.3.3.3.3.2.1.cmml" xref="S4.E13.m1.3.3.3.3.2">subscript</csymbol><ci id="S4.E13.m1.3.3.3.3.2.2.cmml" xref="S4.E13.m1.3.3.3.3.2.2">𝑐</ci><ci id="S4.E13.m1.3.3.3.3.2.3.cmml" xref="S4.E13.m1.3.3.3.3.2.3">𝑘</ci></apply></list></apply><apply id="S4.E13.m1.4.4.1.cmml" xref="S4.E13.m1.4.4.1"><apply id="S4.E13.m1.4.4.1.2.cmml" xref="S4.E13.m1.4.4.1.2"><csymbol cd="ambiguous" id="S4.E13.m1.4.4.1.2.1.cmml" xref="S4.E13.m1.4.4.1.2">subscript</csymbol><sum id="S4.E13.m1.4.4.1.2.2.cmml" xref="S4.E13.m1.4.4.1.2.2"></sum><ci id="S4.E13.m1.4.4.1.2.3.cmml" xref="S4.E13.m1.4.4.1.2.3">𝑖</ci></apply><apply id="S4.E13.m1.4.4.1.1.cmml" xref="S4.E13.m1.4.4.1.1"><csymbol cd="ambiguous" id="S4.E13.m1.4.4.1.1.2.cmml" xref="S4.E13.m1.4.4.1.1">superscript</csymbol><apply id="S4.E13.m1.4.4.1.1.1.2.cmml" xref="S4.E13.m1.4.4.1.1.1.1"><csymbol cd="latexml" id="S4.E13.m1.4.4.1.1.1.2.1.cmml" xref="S4.E13.m1.4.4.1.1.1.1.2">norm</csymbol><apply id="S4.E13.m1.4.4.1.1.1.1.1.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1"><minus id="S4.E13.m1.4.4.1.1.1.1.1.1.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.1"></minus><apply id="S4.E13.m1.4.4.1.1.1.1.1.2.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E13.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E13.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.2.2">𝑤</ci><ci id="S4.E13.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.E13.m1.4.4.1.1.1.1.1.3.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E13.m1.4.4.1.1.1.1.1.3.1.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E13.m1.4.4.1.1.1.1.1.3.2.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.3.2">𝑐</ci><ci id="S4.E13.m1.4.4.1.1.1.1.1.3.3.cmml" xref="S4.E13.m1.4.4.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><cn type="integer" id="S4.E13.m1.4.4.1.1.3.cmml" xref="S4.E13.m1.4.4.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E13.m1.4c">\small\min_{c_{1},...,c_{k}}\sum_{i}\|w_{i}-c_{j}\|^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS6.p2.9">k-평균 클러스터링을 사용하는 것은 상당한 정확도 저하 없이 모델 크기를 <math alttext="8\times" class="ltx_math_unparsed" display="inline" id="S4.SS6.p2.9.m1.1"><semantics id="S4.SS6.p2.9.m1.1a"><mrow id="S4.SS6.p2.9.m1.1b"><mn id="S4.SS6.p2.9.m1.1.1">8</mn><mo id="S4.SS6.p2.9.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS6.p2.9.m1.1c">8\times</annotation></semantics></math>까지 감소시키기에 충분하다는 것이 발견되었다. 그 외에도 가지치기 및 허프만 코딩과 함께 k-평균 기반 벡터 양자화를 공동으로 적용하면 모델 크기 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib84" title="">84</a>]</cite>를 더욱 줄일 수 있다.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS6.p3.1">Product quantization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib74" title="">74</a>, <a class="ltx_ref" href="#bib.bib256" title="">256</a>, <a class="ltx_ref" href="#bib.bib227" title="">227</a>]</cite>는 벡터 양자화의 확장으로 가중치 행렬을 부행렬로 나누고 각 부행렬에 벡터 양자화를 적용한다. 기본적인 곱 양자화 방법 외에도 클러스터링의 더 세밀한 사용은 정확도를 더욱 향상시킬 수 있다. 예를 들어, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib74" title="">74</a>]</cite>에서 k-평균 곱 양자화 후의 잔차들은 재귀적으로 더 양자화된다. 그리고 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib189" title="">189</a>]</cite>에서 저자들은 정보를 더 잘 보존하기 위해 더 중요한 양자화 범위에 더 많은 클러스터를 적용한다.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Quantization and Hardware Processors</span>
</h2>

<figure id="S5.F9" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2103.13630/assets/x11.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 9:</span></figcaption>
Throughput comparison of different commercial edge processors for NN inference at the edge.
</figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.1">우리는 양자화가 모델 크기를 감소시킬 뿐만 아니라 더 빠른 속도를 가능하게 하고 특히 낮은 정밀도 로직을 갖는 하드웨어의 경우 더 적은 전력을 필요로 한다고 말했다. 이와 같이, 양자화는 IoT 및 모바일 애플리케이션에서 에지 배포에 특히 중요했다. 에지 디바이스는 종종 컴퓨팅, 메모리 및 중요하게는 전력 예산을 포함하는 엄격한 리소스 제약을 갖는다. 이것은 종종 많은 심층 NN 모델에 충족하기에는 너무 비싸다. 또한, 많은 에지 프로세서는 특히 마이크로-컨트롤러에서 어떠한 지원 부동 소수점 연산도 갖지 않는다.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p" id="S5.p2.1">여기서는 양자화의 맥락에서 서로 다른 하드웨어 플랫폼에 대해 간략하게 논의한다. ARM Cortex-M은 32비트 RISC ARM 프로세서 코어 그룹으로서, 저가의 전력 효율적인 임베디드 디바이스를 위해 설계된다. 예를 들어, STM32 계열은 가장자리에서 NN 추론에도 사용되는 ARM Cortex-M 코어를 기반으로 하는 마이크로컨트롤러이다. 일부 ARM Cortex-M 코어에는 전용 부동 소수점 단위가 포함되어 있지 않기 때문에 배포 전에 먼저 모델을 양자화해야 합니다. CMSIS-NN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib136" title="">136</a>]</cite>는 ARM Cortex-M 코어에 NN 모델을 양자화하고 배포하는 데 도움이 되는 ARM의 라이브러리입니다. 구체적으로, 라이브러리는 고정 소수점 양자화 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib154" title="">154</a>, <a class="ltx_ref" href="#bib.bib113" title="">113</a>, <a class="ltx_ref" href="#bib.bib267" title="">267</a>]</cite>를 파워-오브-투 스케일링 팩터로 활용하여 비트 쉬프팅 연산으로 양자화 및 역양자화 과정을 효율적으로 수행할 수 있다. 전용 CNN 가속기를 이용한 에지 추론용 RISC-V SoC(System on Chip)인 GAP-8<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib64" title="">64</a>]</cite>는 정수 연산만을 지원하는 에지 프로세서의 또 다른 예이다. 프로그램 가능한 범용 프로세서는 유연성으로 인해 널리 채택되지만, 목적으로 구축된 ASIC 칩인 Google Edge TPU는 에지에서 추론을 실행하기 위한 또 다른 새로운 솔루션이다. 에지 TPU는 컴퓨팅 자원이 많은 구글 데이터센터에서 구동되는 클라우드 TPU와 달리 소형·저전력 기기용으로 설계돼 8비트 연산만 지원한다. NN 모델은 양자화 인식 훈련 또는 텐서플로우의 훈련 후 양자화를 사용하여 양자화되어야 한다.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p" id="S5.p3.1">그림 <a class="ltx_ref" href="#S5.F9" title="Figure 9 ‣ V Quantization and Hardware Processors ‣ A Survey of Quantization Methods for Efficient Neural Network Inference"><span class="ltx_text ltx_ref_tag">9</span></a>는 엣지에서 NN 추론을 위해 널리 사용되는 서로 다른 상용 엣지 프로세서의 처리량을 나타낸다. 지난 몇 년 동안 에지 프로세서의 컴퓨팅 성능이 크게 향상되었으며, 이는 이전에 서버에서만 사용 가능했던 값비싼 NN 모델의 배치 및 추론을 허용한다. 효율적인 저정밀 로직 및 전용 딥러닝 가속기와 결합된 양자화는 이러한 에지 프로세서의 진화를 위한 중요한 원동력 중 하나이다.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p class="ltx_p" id="S5.p4.1">양자화는 많은 에지 프로세서에서 필수 불가결한 기법이지만, 예를 들어 99번째 백분위수 지연과 같은 SLA(Service Level Agreement) 요구 사항을 충족하는 비-에지 프로세서에서도 현저한 개선을 가져올 수 있다. 좋은 예는 최근 NVIDIA 튜링 GPU, 특히 튜링 텐서 코어를 포함하는 T4 GPU에 의해 제공된다. 텐서 코어는 효율적인 저정밀 행렬 곱셈을 위해 설계된 특수 실행 단위입니다.</p>
</div>
</section>
<section id="S6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Future Directions for Research in Quantization</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1">여기서는 양자화의 향후 연구를 위한 몇 가지 높은 수준의 도전과 기회에 대해 간략하게 논의한다. 이것은 양자화 소프트웨어, 하드웨어 및 NN 아키텍처 공동 설계, 결합된 압축 방법 및 양자화된 훈련으로 분해된다.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Quantization Software:</span> 현재 방법을 사용 하면 정확도를 잃지 않고 다른 NN 모델을 양자화 하 고 INT8에 배포 하는 것이 간단 합니다. INT8 양자화된 모델(예: Nvidia의 TensorRT, TVM 등)을 배포하는 데 사용할 수 있는 여러 소프트웨어 패키지가 있으며 각각 좋은 설명서가 있습니다. 또한, 구현은 매우 최적이며 양자화를 통해 속도를 쉽게 관찰할 수 있다. 그러나, 보다 낮은 비트-정밀 양자화를 위한 소프트웨어는 널리 이용가능하지 않으며, 때때로 존재하지 않는다. 예를 들어, Nvidia의 TensorRT는 현재 sub-INT8 양자화를 지원하지 않는다. 더욱이, INT4 양자화에 대한 지원은 최근에야 TVM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib267" title="">267</a>]</cite>에 추가되었다. 최근 연구에서 INT4/INT8을 사용한 낮은 정밀도와 혼합 정밀도 양자화가 실제로 작동하는 것으로 나타났다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib267" title="">267</a>, <a class="ltx_ref" href="#bib.bib246" title="">246</a>, <a class="ltx_ref" href="#bib.bib108" title="">108</a>, <a class="ltx_ref" href="#bib.bib286" title="">286</a>, <a class="ltx_ref" href="#bib.bib246" title="">246</a>, <a class="ltx_ref" href="#bib.bib51" title="">51</a>, <a class="ltx_ref" href="#bib.bib239" title="">239</a>, <a class="ltx_ref" href="#bib.bib263" title="">263</a>, <a class="ltx_ref" href="#bib.bib199" title="">199</a>, <a class="ltx_ref" href="#bib.bib249" title="">249</a>, <a class="ltx_ref" href="#bib.bib102" title="">102</a>, <a class="ltx_ref" href="#bib.bib187" title="">187</a>, <a class="ltx_ref" href="#bib.bib82" title="">82</a>, <a class="ltx_ref" href="#bib.bib211" title="">211</a>]</cite> 따라서, 보다 낮은 정밀도 양자화를 위한 효율적인 소프트웨어 API를 개발하는 것은 중요한 영향을 미칠 것이다.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Hardware and NN Architecture Co-Design:</span> 위에서 논의한 바와 같이, 저-정밀 양자화의 고전적 작업과 기계 학습의 최근 작업 사이의 중요한 차이점은 NN 파라미터가 매우 상이한 양자화된 값을 가질 수 있지만 여전히 유사하게 잘 일반화될 수 있다는 사실이다. 예를 들어, 양자화 인식 훈련을 사용하면 단일 정밀도 매개변수로 원래 솔루션에서 멀리 떨어진 다른 솔루션으로 수렴할 수 있지만 여전히 좋은 정확도를 얻을 수 있다. 이러한 자유도를 이용할 수 있고 NN 아키텍처가 양자화되고 있는 동안에도 적응할 수 있다. 예를 들어, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib34" title="">34</a>]</cite>의 최근 연구는 NN 아키텍처의 폭을 변경하면 양자화 후 일반화 갭을 줄이거나 제거할 수 있음을 보여준다. 향후 작업의 한 줄은 모델이 양자화되고 있을 때 깊이 또는 개별 커널과 같은 다른 아키텍처 매개변수를 공동으로 적응시키는 것이다. 향후 작업의 또 다른 라인은 이러한 공동 설계를 하드웨어 아키텍처로 확장하는 것이다. 이것은 (다중-축적 요소들의 상이한 마이크로-아키텍처들과 같은) 많은 상이한 가능한 하드웨어 구성들을 탐색할 수 있고, 그리고 나서 이것을 NN 아키텍처 및 양자화 공동 설계와 결합할 수 있기 때문에, FPGA 배포에 특히 유용할 수 있다.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_bold" id="S6.p4.1.1">Coupled Compression Methods:</span> 위에서 논의한 바와 같이, 양자화는 NN의 효율적인 배치를 위한 방법들 중 하나일 뿐이다. 다른 방법에는 효율적인 NN 아키텍처 설계, 하드웨어와 NN 아키텍처의 공동 설계, 가지치기 및 지식 증류가 포함된다. 양자화는 이러한 다른 접근법들과 결합될 수 있다. 그러나 현재 이러한 방법의 최적 조합을 탐색하는 작업은 거의 없다. 예를 들어, 프루닝과 양자화는 모델의 오버헤드 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib87" title="">87</a>, <a class="ltx_ref" href="#bib.bib152" title="">152</a>]</cite>를 줄이기 위해 모델에 함께 적용될 수 있으며, 구조화/비구조화 프루닝과 양자화의 최상의 조합을 이해하는 것이 중요하다. 유사하게, 또 다른 미래의 방향은 이러한 방법과 위에서 설명한 다른 접근법 간의 결합을 연구하는 것이다.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p class="ltx_p" id="S6.p5.1"><span class="ltx_text ltx_font_bold" id="S6.p5.1.1">Quantized Training:</span> Perhaps the most important use of quantization has been accelerate NN training with half-precision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>, <a class="ltx_ref" href="#bib.bib79" title="">79</a>, <a class="ltx_ref" href="#bib.bib72" title="">72</a>, <a class="ltx_ref" href="#bib.bib175" title="">175</a>]</cite>. 이것은 트레이닝을 위해 훨씬 더 빠르고 더 전력 효율적인 감소된 정밀도 로직의 사용을 가능하게 했다. 그러나, 이것을 INT8 정밀 훈련으로 더 내려가는 것은 매우 어려웠다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib123" title="">123</a>, <a class="ltx_ref" href="#bib.bib173" title="">173</a>, <a class="ltx_ref" href="#bib.bib10" title="">10</a>, <a class="ltx_ref" href="#bib.bib137" title="">137</a>, <a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite> 이 영역에는 몇 가지 흥미로운 작업이 존재하지만, 제안된 방법은 하이퍼파라미터 튜닝이 많이 필요하거나 비교적 쉬운 학습 작업에서 몇 개의 NN 모델에서만 작동한다. 기본적인 문제는 INT8 정밀도로 훈련이 불안정해지고 발산할 수 있다는 것이다. 이러한 도전을 해결하는 것은 특히 에지에서의 트레이닝을 위해 여러 애플리케이션들에 높은 영향을 미칠 수 있다.</p>
</div>
</section>
<section id="S7" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Summary and Conclusions</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p" id="S7.p1.1">추상적인 수학적 계산이 디지털 컴퓨터에서의 계산에 적응하자마자, 그 계산들에서 수치들의 효율적인 표현, 조작, 및 통신의 문제가 발생하였다. 숫자 표현의 문제와 강하게 관련된 것은 양자화의 문제이다: 필요한 비트 수를 최소화하고 또한 수반되는 계산의 정확도를 최대화하기 위해 연속 실수 값 숫자 세트를 고정된 이산 숫자 세트에 분산해야 하는 방법은 무엇인가? 이러한 문제는 컴퓨터 과학만큼 오래되었지만 이러한 문제는 특히 효율적인 NN 모델의 설계와 관련이 있다. 이에 대한 몇 가지 이유가 있습니다. 첫째, NN은 계산 집약적이다. 따라서 수치의 효율적인 표현이 특히 중요하다. 둘째, 현재 대부분의 NN 모델은 지나치게 과대모수화되어 있다. 따라서 정확도에 영향을 미치지 않으면서 비트 정밀도를 줄일 수 있는 충분한 기회가 있습니다. 셋째, NN 모델의 계층 구조는 탐색할 수 있는 추가 차원을 제공한다. 따라서, NN 내의 상이한 층들은 손실 함수에 상이한 영향을 가지며, 이는 이러한 혼합-정밀 양자화에 대한 흥미로운 접근법들을 동기시킨다.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p" id="S7.p2.6">부동 소수점 표현에서 8/4비트 이하로 표현되는 저정밀 고정 정수 값으로 이동하는 것은 메모리 풋프린트와 레이턴시를 줄일 수 있는 잠재력을 보유한다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib157" title="">157</a>]</cite>는 TVM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib32" title="">32</a>]</cite> 양자화 라이브러리를 사용하여 ResNet50<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib88" title="">88</a>]</cite>, VGG-19<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib224" title="">224</a>]</cite>, inceptionV3<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib230" title="">230</a>]</cite>를 포함한 인기 있는 컴퓨터 비전 모델의 INT8 추론을 NVIDIA GTX 1080에서 각각 3.89<math alttext="\times" class="ltx_Math" display="inline" id="S7.p2.1.m1.1"><semantics id="S7.p2.1.m1.1a"><mo id="S7.p2.1.m1.1.1" xref="S7.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.1.m1.1b"><times id="S7.p2.1.m1.1.1.cmml" xref="S7.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.m1.1c">\times</annotation></semantics></math>, 3.32<math alttext="\times" class="ltx_Math" display="inline" id="S7.p2.2.m2.1"><semantics id="S7.p2.2.m2.1a"><mo id="S7.p2.2.m2.1.1" xref="S7.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.2.m2.1b"><times id="S7.p2.2.m2.1.1.cmml" xref="S7.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.2.m2.1c">\times</annotation></semantics></math>, 5.02<math alttext="\times" class="ltx_Math" display="inline" id="S7.p2.3.m3.1"><semantics id="S7.p2.3.m3.1a"><mo id="S7.p2.3.m3.1.1" xref="S7.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.3.m3.1b"><times id="S7.p2.3.m3.1.1.cmml" xref="S7.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.3.m3.1c">\times</annotation></semantics></math> speedup을 달성할 수 있음을 보여준다. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib213" title="">213</a>]</cite>는 ResNet50의 INT4 추론이 NVIDIA T4 및 RTX에 비해 50-60%의 추가 속도 향상을 가져올 수 있음을 보여주며 효율성을 최대화하기 위해 더 낮은 비트 정밀도를 사용하는 것이 중요함을 강조한다. 최근, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib267" title="">267</a>]</cite>는 Mix-precision 양자화를 활용하여 ResNet50에 대해 정확도 저하 없이 INT8 추론에 비해 23%의 속도 향상을 달성하였고, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib132" title="">132</a>]</cite>는 INT8 전용 추론을 BERT 모델에 확장하여 FP32보다 최대 4.0<math alttext="\times" class="ltx_Math" display="inline" id="S7.p2.4.m4.1"><semantics id="S7.p2.4.m4.1a"><mo id="S7.p2.4.m4.1.1" xref="S7.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.4.m4.1b"><times id="S7.p2.4.m4.1.1.cmml" xref="S7.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.4.m4.1c">\times</annotation></semantics></math> 빠른 추론을 가능하게 하였다. 앞서 언급한 연구들은 GPU에서의 가속도에 초점을 맞추고 있지만, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib114" title="">114</a>]</cite> 역시 다양한 컴퓨터 비전 모델의 INT8 양자화를 통해 Intel Cascade Lake CPU와 Raspberry Pi4(둘 다 non-GPU 아키텍처임)에서 각각 2.35<math alttext="\times" class="ltx_Math" display="inline" id="S7.p2.5.m5.1"><semantics id="S7.p2.5.m5.1a"><mo id="S7.p2.5.m5.1.1" xref="S7.p2.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.5.m5.1b"><times id="S7.p2.5.m5.1.1.cmml" xref="S7.p2.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.5.m5.1c">\times</annotation></semantics></math>와 1.40<math alttext="\times" class="ltx_Math" display="inline" id="S7.p2.6.m6.1"><semantics id="S7.p2.6.m6.1a"><mo id="S7.p2.6.m6.1.1" xref="S7.p2.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S7.p2.6.m6.1b"><times id="S7.p2.6.m6.1.1.cmml" xref="S7.p2.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.6.m6.1c">\times</annotation></semantics></math> latency speedup을 얻었다. 결과적으로, 우리의 참고문헌이 증명하는 바와 같이, NN 모델에서의 양자화의 문제는 매우 활발한 연구 영역이었다.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p class="ltx_p" id="S7.p3.1">이 작업에서 우리는 이러한 매우 다양한 노력에 몇 가지 개념적 구조를 가져오려고 노력했다. 우리는 균일, 비균일, 대칭, 비대칭, 정적 및 동적 양자화와 같은 양자화의 많은 응용 분야에 공통적인 주제에 대한 논의로 시작했다. 그런 다음 NN의 양자화에 더 고유한 양자화 문제를 고려했다. 여기에는 계층별, 그룹별, 채널별 및 부채널별 양자화가 포함됩니다. 또한 훈련과 양자화의 상호 관계를 고려하였으며, 훈련 후 양자화와 비교하여 양자화 인식 훈련의 장단점에 대해 논의하였다. 양자화와 트레이닝 사이의 관계에 대한 논의를 더 뉘앙스화하는 것은 데이터의 이용가능성의 문제이다. 이것의 극단적인 경우는 훈련에 사용되는 데이터가 프라이버시와 같은 다양한 합리적인 이유로 인해 더 이상 사용할 수 없는 경우이다. 이는 제로 샷 양자화의 문제를 야기한다.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p class="ltx_p" id="S7.p4.1">특히 에지 배치를 대상으로 하는 효율적인 NN에 대해 우려하기 때문에 이러한 환경에 고유한 문제를 고려했다. 이들은 8 비트보다 적은, 아마도 이진 값만큼 낮은 값으로 표현되는 파라미터들을 초래하는 양자화 기법들을 포함한다. 또한 부동 소수점 단위가 부족한 저전력 마이크로프로세서에 NN을 배치할 수 있는 정수 전용 양자화 문제를 고려하였다.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p class="ltx_p" id="S7.p5.1">이 설문조사와 그 조직을 통해, 우리는 신경망에 대한 양자화에 있어 현재 연구의 유용한 스냅샷을 제시하고 이 분야의 향후 연구에 대한 평가를 쉽게 할 수 있는 지능화된 조직을 제공하기를 희망한다.</p>
</div>
</section>
<section id="Sx1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p" id="Sx1.p1.1">UC버클리팀도 삼성(특히 조셉 하순), 인텔 법인, 인텔 VLAB팀, 구글 TRC팀, 구글 브레인(특히 데이비드 패터슨 교수, 에드 치 박사, 징 리 박사)의 공손한 지원을 인정한다. 아미르 골라미는 삼성 사잇의 자금 지원을 통해 지원받았다. 우리의 결론은 반드시 후원자의 입장이나 정책을 반영하는 것은 아니며 공식적인 지지를 추론해서는 안 된다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu
Timofte, Luca Benini, and Luc Van&nbsp;Gool.

</span>
<span class="ltx_bibblock">Soft-to-hard vector quantization for end-to-end learning compressible
representations.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1704.00648</span>, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Eirikur Agustsson and Lucas Theis.

</span>
<span class="ltx_bibblock">Universally quantized neural compression.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Sungsoo Ahn, Shell&nbsp;Xu Hu, Andreas Damianou, Neil&nbsp;D Lawrence, and Zhenwen Dai.

</span>
<span class="ltx_bibblock">Variational information distillation for knowledge transfer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 9163–9171, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Milad Alizadeh, Arash Behboodi, Mart van Baalen, Christos Louizos, Tijmen
Blankevoort, and Max Welling.

</span>
<span class="ltx_bibblock">Gradient l1 regularization for quantization robustness.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.07520</span>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Milad Alizadeh, Javier Fernández-Marqués, Nicholas&nbsp;D Lane, and Yarin
Gal.

</span>
<span class="ltx_bibblock">An empirical study of binary neural networks’ optimisation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Jimmy&nbsp;Lei Ba, Jamie&nbsp;Ryan Kiros, and Geoffrey&nbsp;E Hinton.

</span>
<span class="ltx_bibblock">Layer normalization.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1607.06450</span>, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Haoli Bai, Wei Zhang, Lu&nbsp;Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu,
Michael Lyu, and Irwin King.

</span>
<span class="ltx_bibblock">Binarybert: Pushing the limit of bert quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.15701</span>, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Yu&nbsp;Bai, Yu-Xiang Wang, and Edo Liberty.

</span>
<span class="ltx_bibblock">Proxquant: Quantized neural networks via proximal operators.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.00861</span>, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Dana&nbsp;Harry Ballard.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">An introduction to natural computation</span>.

</span>
<span class="ltx_bibblock">MIT press, 1999.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.

</span>
<span class="ltx_bibblock">Scalable methods for 8-bit training of neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry.

</span>
<span class="ltx_bibblock">Post-training 4-bit quantization of convolution networks for
rapid-deployment.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.05723</span>, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Chaim Baskin, Eli Schwartz, Evgenii Zheltonozhskii, Natan Liss, Raja Giryes,
Alex&nbsp;M Bronstein, and Avi Mendelson.

</span>
<span class="ltx_bibblock">Uniq: Uniform noise injection for non-uniform quantization of neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.10969</span>, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yoshua Bengio, Nicholas Léonard, and Aaron Courville.

</span>
<span class="ltx_bibblock">Estimating or propagating gradients through stochastic neurons for
conditional computation.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1308.3432</span>, 2013.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
William&nbsp;Ralph Bennett.

</span>
<span class="ltx_bibblock">Spectra of quantized signals.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">The Bell System Technical Journal</span>, 27(3):446–472, 1948.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak.

</span>
<span class="ltx_bibblock">Lsq+: Improving low-bit quantization through learnable offsets and
better initialization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span>, pages 696–697, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Davis Blalock, Jose Javier&nbsp;Gonzalez Ortiz, Jonathan Frankle, and John Guttag.

</span>
<span class="ltx_bibblock">What is the state of neural network pruning?

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.03033</span>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Tom&nbsp;B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.14165</span>, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos.

</span>
<span class="ltx_bibblock">High-capacity expert binary networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Adrian Bulat and Georgios Tzimiropoulos.

</span>
<span class="ltx_bibblock">Xnor-net++: Improved binary neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.13863</span>, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Adrian Bulat, Georgios Tzimiropoulos, Jean Kossaifi, and Maja Pantic.

</span>
<span class="ltx_bibblock">Improved training of binary networks for human pose estimation and
image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1904.05868</span>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Aydin Buluc and John&nbsp;R Gilbert.

</span>
<span class="ltx_bibblock">Challenges and advances in parallel sparse matrix-matrix
multiplication.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2008 37th International Conference on Parallel Processing</span>,
pages 503–510. IEEE, 2008.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.

</span>
<span class="ltx_bibblock">Once-for-all: Train one network and specialize it for efficient
deployment.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.09791</span>, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Han Cai, Ligeng Zhu, and Song Han.

</span>
<span class="ltx_bibblock">Proxylessnas: Direct neural architecture search on target task and
hardware.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.00332</span>, 2018.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael&nbsp;W Mahoney, and Kurt
Keutzer.

</span>
<span class="ltx_bibblock">Zeroq: A novel zero shot quantization framework.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 13169–13178, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos.

</span>
<span class="ltx_bibblock">Deep learning with low precision by half-wave gaussian quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 5918–5926, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Léopold Cambier, Anahita Bhiwandiwalla, Ting Gong, Mehran Nekuii, Oguz&nbsp;H
Elibol, and Hanlin Tang.

</span>
<span class="ltx_bibblock">Shifted and squeezed 8-bit floating point format for low-precision
training of deep neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.05674</span>, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Rishidev Chaudhuri and Ila Fiete.

</span>
<span class="ltx_bibblock">Computational principles of memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Nature neuroscience</span>, 19(3):394, 2016.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi,
Chunjing Xu, Chao Xu, and Qi&nbsp;Tian.

</span>
<span class="ltx_bibblock">Data-free learning of student networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 3514–3522, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Jianfei Chen, Yu&nbsp;Gai, Zhewei Yao, Michael&nbsp;W Mahoney, and Joseph&nbsp;E Gonzalez.

</span>
<span class="ltx_bibblock">A statistical framework for low-bitwidth training of deep neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.14298</span>, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Kuilin Chen and Chi-Guhn Lee.

</span>
<span class="ltx_bibblock">Incremental few-shot learning via vector quantization in deep
embedded space.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Shangyu Chen, Wenya Wang, and Sinno&nbsp;Jialin Pan.

</span>
<span class="ltx_bibblock">Metaquant: Learning to quantize by learning to penetrate
non-differentiable quantization.

</span>
<span class="ltx_bibblock">In H.&nbsp;Wallach, H.&nbsp;Larochelle, A.&nbsp;Beygelzimer, F.&nbsp;d'Alché-Buc, E.&nbsp;Fox, and R.&nbsp;Garnett, editors, <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Advances in Neural
Information Processing Systems</span>, volume&nbsp;32. Curran Associates, Inc., 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen
Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et&nbsp;al.

</span>
<span class="ltx_bibblock">TVM: An automated end-to-end optimizing compiler for deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.4.4" class="ltx_text ltx_font_italic">13th <math id="bib.bib32.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib32.1.1.m1.1a"><mo stretchy="false" id="bib.bib32.1.1.m1.1.1" xref="bib.bib32.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib32.1.1.m1.1b"><ci id="bib.bib32.1.1.m1.1.1.cmml" xref="bib.bib32.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib32.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib32.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib32.2.2.m2.1a"><mo stretchy="false" id="bib.bib32.2.2.m2.1.1" xref="bib.bib32.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib32.2.2.m2.1b"><ci id="bib.bib32.2.2.m2.1.1.cmml" xref="bib.bib32.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib32.2.2.m2.1c">\}</annotation></semantics></math> Symposium on Operating Systems Design and
Implementation (<math id="bib.bib32.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib32.3.3.m3.1a"><mo stretchy="false" id="bib.bib32.3.3.m3.1.1" xref="bib.bib32.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib32.3.3.m3.1b"><ci id="bib.bib32.3.3.m3.1.1.cmml" xref="bib.bib32.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib32.3.3.m3.1c">\{</annotation></semantics></math>OSDI<math id="bib.bib32.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib32.4.4.m4.1a"><mo stretchy="false" id="bib.bib32.4.4.m4.1.1" xref="bib.bib32.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib32.4.4.m4.1b"><ci id="bib.bib32.4.4.m4.1.1.cmml" xref="bib.bib32.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib32.4.4.m4.1c">\}</annotation></semantics></math> 18)</span>, pages 578–594, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Xiuyi Chen, Guangcan Liu, Jing Shi, Jiaming Xu, and Bo&nbsp;Xu.

</span>
<span class="ltx_bibblock">Distilled binary neural network for monaural speech separation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">2018 International Joint Conference on Neural Networks
(IJCNN)</span>, pages 1–8. IEEE, 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Ting-Wu Chin, Pierce I-Jen Chuang, Vikas Chandra, and Diana Marculescu.

</span>
<span class="ltx_bibblock">One weight bitwidth to rule them all.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Brian Chmiel, Liad Ben-Uri, Moran Shkolnik, Elad Hoffer, Ron Banner, and Daniel
Soudry.

</span>
<span class="ltx_bibblock">Neural gradients are near-lognormal: improved quantized and sparse
training.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang,
Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan.

</span>
<span class="ltx_bibblock">Pact: Parameterized clipping activation for quantized neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1805.06085</span>, 2018.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Yoojin Choi, Jihwan Choi, Mostafa El-Khamy, and Jungwon Lee.

</span>
<span class="ltx_bibblock">Data-free network quantization with adversarial knowledge
distillation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span>, pages 710–711, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee.

</span>
<span class="ltx_bibblock">Towards the limit of network quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1612.01543</span>, 2016.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee.

</span>
<span class="ltx_bibblock">Learning low precision deep neural networks through regularization.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1809.00095</span>, 2, 2018.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev.

</span>
<span class="ltx_bibblock">Low-bit quantization of neural networks for efficient inference.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">ICCV Workshops</span>, pages 3009–3018, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.

</span>
<span class="ltx_bibblock">Training deep neural networks with low precision multiplications.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.7024</span>, 2014.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.

</span>
<span class="ltx_bibblock">BinaryConnect: Training deep neural networks with binary weights
during propagations.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
3123–3131, 2015.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Elliot&nbsp;J Crowley, Gavin Gray, and Amos&nbsp;J Storkey.

</span>
<span class="ltx_bibblock">Moonshine: Distilling with cheap convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, pages 2893–2903, 2018.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Sajad Darabi, Mouloud Belbahri, Matthieu Courbariaux, and Vahid&nbsp;Partovi Nia.

</span>
<span class="ltx_bibblock">Bnn+: Improved binary network training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Lei Deng, Peng Jiao, Jing Pei, Zhenzhi Wu, and Guoqi Li.

</span>
<span class="ltx_bibblock">Gxnor-net: Training deep neural networks with ternary weights and
activations without full-precision memory under a unified discretization
framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Neural Networks</span>, 100:49–58, 2018.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
James Diffenderfer and Bhavya Kailkhura.

</span>
<span class="ltx_bibblock">Multi-prize lottery ticket hypothesis: Finding accurate binary neural
networks by pruning a randomly weighted network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Marculescu.

</span>
<span class="ltx_bibblock">Regularizing activation distribution for training binarized deep
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 11408–11417, 2019.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Xin Dong, Shangyu Chen, and Sinno&nbsp;Jialin Pan.

</span>
<span class="ltx_bibblock">Learning to prune deep neural networks via layer-wise optimal brain
surgeon.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1705.07565</span>, 2017.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael&nbsp;W. Mahoney, and
Kurt Keutzer.

</span>
<span class="ltx_bibblock">HAWQ-V2: Hessian aware trace-weighted quantization of neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Zhen Dong, Zhewei Yao, Amir Gholami, Michael&nbsp;W Mahoney, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">Hawq: Hessian aware quantization of neural networks with
mixed-precision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 293–302, 2019.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Yueqi Duan, Jiwen Lu, Ziwei Wang, Jianjiang Feng, and Jie Zhou.

</span>
<span class="ltx_bibblock">Learning deep binary descriptor with multi-quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 1183–1192, 2017.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
JG&nbsp;Dunn.

</span>
<span class="ltx_bibblock">The performance of a class of n dimensional quantizers for a gaussian
source.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Proc. Columbia Symp. Signal Transmission Processing</span>, pages
76–81, 1965.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Thomas Elsken, Jan&nbsp;Hendrik Metzen, Frank Hutter, et&nbsp;al.

</span>
<span class="ltx_bibblock">Neural architecture search: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">J. Mach. Learn. Res.</span>, 20(55):1–21, 2019.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
William&nbsp;H Equitz.

</span>
<span class="ltx_bibblock">A new vector quantization clustering algorithm.

</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">IEEE transactions on acoustics, speech, and signal processing</span>,
37(10):1568–1575, 1989.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Steven&nbsp;K Esser, Jeffrey&nbsp;L McKinstry, Deepika Bablani, Rathinakumar Appuswamy,
and Dharmendra&nbsp;S Modha.

</span>
<span class="ltx_bibblock">Learned step size quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1902.08153</span>, 2019.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel Roy, and Ali
Ramezani-Kebrya.

</span>
<span class="ltx_bibblock">Adaptive gradient quantization for data-parallel sgd.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
A&nbsp;Aldo Faisal, Luc&nbsp;PJ Selen, and Daniel&nbsp;M Wolpert.

</span>
<span class="ltx_bibblock">Noise in the nervous system.

</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">Nature reviews neuroscience</span>, 9(4):292–303, 2008.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval,
Hervé Jégou, and Armand Joulin.

</span>
<span class="ltx_bibblock">Training with quantization noise for extreme model compression.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">arXiv e-prints</span>, pages arXiv–2004, 2020.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis,
and Joseph Hassoun.

</span>
<span class="ltx_bibblock">Near-lossless post-training quantization of deep neural networks via
a piecewise linear approximation.

</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.00104</span>, 2020.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis,
and Joseph&nbsp;H Hassoun.

</span>
<span class="ltx_bibblock">Post-training piecewise linear quantization for deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 69–86.
Springer, 2020.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Julian Faraone, Nicholas Fraser, Michaela Blott, and Philip&nbsp;HW Leong.

</span>
<span class="ltx_bibblock">Syq: Learning symmetric quantization for efficient deep neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4300–4309, 2018.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Alexander Finkelstein, Uri Almog, and Mark Grobman.

</span>
<span class="ltx_bibblock">Fighting quantization bias with bias.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.03193</span>, 2019.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Eric Flamand, Davide Rossi, Francesco Conti, Igor Loi, Antonio Pullini, Florent
Rotenberg, and Luca Benini.

</span>
<span class="ltx_bibblock">Gap-8: A risc-v soc for ai at the edge of the iot.

</span>
<span class="ltx_bibblock">In <span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">2018 IEEE 29th International Conference on
Application-specific Systems, Architectures and Processors (ASAP)</span>, pages
1–4. IEEE, 2018.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Abram&nbsp;L Friesen and Pedro Domingos.

</span>
<span class="ltx_bibblock">Deep learning as a mixed convex-combinatorial optimization problem.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.11573</span>, 2017.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Trevor Gale, Erich Elsen, and Sara Hooker.

</span>
<span class="ltx_bibblock">The state of sparsity in deep neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1902.09574</span>, 2019.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
AE&nbsp;Gamal, L&nbsp;Hemachandra, Itzhak Shperling, and V&nbsp;Wei.

</span>
<span class="ltx_bibblock">Using simulated annealing to design good codes.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Theory</span>, 33(1):116–123, 1987.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Sahaj Garg, Anirudh Jain, Joe Lou, and Mitchell Nahmias.

</span>
<span class="ltx_bibblock">Confounding tradeoffs for neural network quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.06366</span>, 2021.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Sahaj Garg, Joe Lou, Anirudh Jain, and Mitchell Nahmias.

</span>
<span class="ltx_bibblock">Dynamic precision analog computing for neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.06365</span>, 2021.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter Jin,
Sicheng Zhao, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">SqueezeNext: Hardware-aware neural network design.

</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">Workshop paper in CVPR</span>, 2018.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Amir Gholami, Michael&nbsp;W Mahoney, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">An integrated approach to neural network design, training, and
inference.

</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">Univ. California, Berkeley, Berkeley, CA, USA, Tech. Rep</span>, 2020.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Boris Ginsburg, Sergei Nikolaev, Ahmad Kiswani, Hao Wu, Amir Gholaminejad,
Slawomir Kierat, Michael Houston, and Alex Fit-Florea.

</span>
<span class="ltx_bibblock">Tensor processing using low precision format, December&nbsp;28 2017.

</span>
<span class="ltx_bibblock">US Patent App. 15/624,577.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin,
Fengwei Yu, and Junjie Yan.

</span>
<span class="ltx_bibblock">Differentiable soft quantization: Bridging full-precision and low-bit
neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 4852–4861, 2019.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev.

</span>
<span class="ltx_bibblock">Compressing deep convolutional networks using vector quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6115</span>, 2014.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Ian&nbsp;J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1406.2661</span>, 2014.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Robert&nbsp;M. Gray and David&nbsp;L. Neuhoff.

</span>
<span class="ltx_bibblock">Quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">IEEE transactions on information theory</span>, 44(6):2325–2383,
1998.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Nianhui Guo, Joseph Bethge, Haojin Yang, Kai Zhong, Xuefei Ning, Christoph
Meinel, and Yu&nbsp;Wang.

</span>
<span class="ltx_bibblock">Boolnet: Minimizing the energy consumption of binary neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.06991</span>, 2021.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Yiwen Guo, Anbang Yao, Hao Zhao, and Yurong Chen.

</span>
<span class="ltx_bibblock">Network sketching: Exploiting binary structure in deep cnns.

</span>
<span class="ltx_bibblock">In <span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 5955–5963, 2017.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.

</span>
<span class="ltx_bibblock">Deep learning with limited numerical precision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
1737–1746. PMLR, 2015.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Philipp Gysel, Mohammad Motamedi, and Soheil Ghiasi.

</span>
<span class="ltx_bibblock">Hardware-oriented approximation of convolutional neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1604.03168</span>, 2016.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Philipp Gysel, Jon Pimentel, Mohammad Motamedi, and Soheil Ghiasi.

</span>
<span class="ltx_bibblock">Ristretto: A framework for empirical study of resource-efficient
inference in convolutional neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">IEEE transactions on neural networks and learning systems</span>,
29(11):5784–5789, 2018.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Hai&nbsp;Victor Habi, Roy&nbsp;H Jennings, and Arnon Netzer.

</span>
<span class="ltx_bibblock">Hmq: Hardware friendly mixed precision quantization block for cnns.

</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.09952</span>, 2020.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Enhua Wu, and Chang Xu.

</span>
<span class="ltx_bibblock">Training binary neural networks through learning with noisy
supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
4017–4026. PMLR, 2020.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Song Han, Huizi Mao, and William&nbsp;J Dally.

</span>
<span class="ltx_bibblock">Deep compression: Compressing deep neural networks with pruning,
trained quantization and huffman coding.

</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1510.00149</span>, 2015.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Matan Haroush, Itay Hubara, Elad Hoffer, and Daniel Soudry.

</span>
<span class="ltx_bibblock">The knowledge within: Methods for data-free model compression.

</span>
<span class="ltx_bibblock">In <span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 8494–8502, 2020.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Babak Hassibi and David&nbsp;G Stork.

</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">Second order derivatives for network pruning: Optimal brain
surgeon</span>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann, 1993.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Benjamin Hawks, Javier Duarte, Nicholas&nbsp;J Fraser, Alessandro Pappalardo, Nhan
Tran, and Yaman Umuroglu.

</span>
<span class="ltx_bibblock">Ps and qs: Quantization-aware pruning for efficient low latency
neural network inference.

</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.11289</span>, 2021.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Xiangyu He and Jian Cheng.

</span>
<span class="ltx_bibblock">Learning compression from limited unlabeled data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 752–769, 2018.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Xiangyu He, Qinghao Hu, Peisong Wang, and Jian Cheng.

</span>
<span class="ltx_bibblock">Generative zero-shot network quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.08430</span>, 2021.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Yihui He, Ji&nbsp;Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han.

</span>
<span class="ltx_bibblock">Amc: Automl for model compression and acceleration on mobile devices.

</span>
<span class="ltx_bibblock">In <span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 784–800, 2018.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Zhezhi He and Deliang Fan.

</span>
<span class="ltx_bibblock">Simultaneously optimizing weight and quantizer of ternary neural
network using truncated gaussian approximation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 11438–11446, 2019.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng,
and Roeland Nusselder.

</span>
<span class="ltx_bibblock">Latent weights do not exist: Rethinking binarized neural network
optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2019.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Dan Hendrycks and Kevin Gimpel.

</span>
<span class="ltx_bibblock">Gaussian error linear units (GELUs).

</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.08415</span>, 2016.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib95.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1503.02531</span>, 2015.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.

</span>
<span class="ltx_bibblock">Sparsity in deep learning: Pruning and growth for efficient inference
and training in neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.00554</span>, 2021.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Mark Horowitz.

</span>
<span class="ltx_bibblock">1.1 computing’s energy problem (and what we can do about it).

</span>
<span class="ltx_bibblock">In <span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">2014 IEEE International Solid-State Circuits Conference
Digest of Technical Papers (ISSCC)</span>, pages 10–14. IEEE, 2014.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Lu&nbsp;Hou and James&nbsp;T Kwok.

</span>
<span class="ltx_bibblock">Loss-aware weight quantization of deep networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.08635</span>, 2018.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Lu&nbsp;Hou, Quanming Yao, and James&nbsp;T Kwok.

</span>
<span class="ltx_bibblock">Loss-aware binarization of deep networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.01600</span>, 2016.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo&nbsp;Chen, Mingxing
Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et&nbsp;al.

</span>
<span class="ltx_bibblock">Searching for MobilenetV3.

</span>
<span class="ltx_bibblock">In <span id="bib.bib100.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 1314–1324, 2019.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Andrew&nbsp;G Howard, Menglong Zhu, Bo&nbsp;Chen, Dmitry Kalenichenko, Weijun Wang,
Tobias Weyand, Marco Andreetto, and Hartwig Adam.

</span>
<span class="ltx_bibblock">MobileNets: Efficient convolutional neural networks for mobile
vision applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1704.04861</span>, 2017.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Peng Hu, Xi&nbsp;Peng, Hongyuan Zhu, Mohamed M&nbsp;Sabry Aly, and Jie Lin.

</span>
<span class="ltx_bibblock">Opq: Compressing deep neural networks with one-shot
pruning-quantization.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Qinghao Hu, Peisong Wang, and Jian Cheng.

</span>
<span class="ltx_bibblock">From hashing to cnns: Training binary weight networks via hashing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume&nbsp;32, 2018.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Gao Huang, Zhuang Liu, Laurens Van Der&nbsp;Maaten, and Kilian&nbsp;Q Weinberger.

</span>
<span class="ltx_bibblock">Densely connected convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 4700–4708, 2017.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Qijing Huang, Dequan Wang, Zhen Dong, Yizhao Gao, Yaohui Cai, Tian Li, Bichen
Wu, Kurt Keutzer, and John Wawrzynek.

</span>
<span class="ltx_bibblock">Codenet: Efficient deployment of input-adaptive object detection on
embedded fpgas.

</span>
<span class="ltx_bibblock">In <span id="bib.bib105.1.1" class="ltx_text ltx_font_italic">The 2021 ACM/SIGDA International Symposium on
Field-Programmable Gate Arrays</span>, pages 206–216, 2021.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Zehao Huang and Naiyan Wang.

</span>
<span class="ltx_bibblock">Data-driven sparse structure selection for deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib106.1.1" class="ltx_text ltx_font_italic">Proceedings of the European conference on computer vision
(ECCV)</span>, pages 304–320, 2018.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio.

</span>
<span class="ltx_bibblock">Binarized neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib107.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
4107–4115, 2016.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry.

</span>
<span class="ltx_bibblock">Improving post training neural quantization: Layer-wise calibration
and integer programming.

</span>
<span class="ltx_bibblock"><span id="bib.bib108.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.10518</span>, 2020.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
David&nbsp;A Huffman.

</span>
<span class="ltx_bibblock">A method for the construction of minimum-redundancy codes.

</span>
<span class="ltx_bibblock"><span id="bib.bib109.1.1" class="ltx_text ltx_font_italic">Proceedings of the IRE</span>, 40(9):1098–1101, 1952.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Forrest&nbsp;N Iandola, Song Han, Matthew&nbsp;W Moskewicz, Khalid Ashraf, William&nbsp;J
Dally, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and&lt;
0.5 mb model size.

</span>
<span class="ltx_bibblock"><span id="bib.bib110.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1602.07360</span>, 2016.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Yani Ioannou, Duncan Robertson, Roberto Cipolla, and Antonio Criminisi.

</span>
<span class="ltx_bibblock">Deep roots: Improving cnn efficiency with hierarchical filter groups.

</span>
<span class="ltx_bibblock">In <span id="bib.bib111.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 1231–1240, 2017.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Sergey Ioffe and Christian Szegedy.

</span>
<span class="ltx_bibblock">Batch normalization: Accelerating deep network training by reducing
internal covariate shift.

</span>
<span class="ltx_bibblock">In <span id="bib.bib112.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
448–456. PMLR, 2015.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Benoit Jacob, Skirmantas Kligys, Bo&nbsp;Chen, Menglong Zhu, Matthew Tang, Andrew
Howard, Hartwig Adam, and Dmitry Kalenichenko.

</span>
<span class="ltx_bibblock">Quantization and training of neural networks for efficient
integer-arithmetic-only inference.

</span>
<span class="ltx_bibblock">In <span id="bib.bib113.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2018.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Animesh Jain, Shoubhik Bhattacharya, Masahiro Masuda, Vin Sharma, and Yida
Wang.

</span>
<span class="ltx_bibblock">Efficient execution of quantized deep learning models: A compiler
approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib114.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.10226</span>, 2020.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Shubham Jain, Swagath Venkataramani, Vijayalakshmi Srinivasan, Jungwook Choi,
Kailash Gopalakrishnan, and Leland Chang.

</span>
<span class="ltx_bibblock">Biscaled-dnn: Quantizing long-tailed datastructures with two scale
factors for deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib115.1.1" class="ltx_text ltx_font_italic">2019 56th ACM/IEEE Design Automation Conference (DAC)</span>, pages
1–6. IEEE, 2019.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Eric Jang, Shixiang Gu, and Ben Poole.

</span>
<span class="ltx_bibblock">Categorical reparameterization with gumbel-softmax.

</span>
<span class="ltx_bibblock"><span id="bib.bib116.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.01144</span>, 2016.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Herve Jegou, Matthijs Douze, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Product quantization for nearest neighbor search.

</span>
<span class="ltx_bibblock"><span id="bib.bib117.1.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>,
33(1):117–128, 2010.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Yongkweon Jeon, Baeseong Park, Se&nbsp;Jung Kwon, Byeongwook Kim, Jeongin Yun, and
Dongsoo Lee.

</span>
<span class="ltx_bibblock">Biqgemm: matrix multiplication with lookup table for
binary-coding-based quantized dnns.

</span>
<span class="ltx_bibblock"><span id="bib.bib118.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2005.09904</span>, 2020.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Tianchu Ji, Shraddhan Jain, Michael Ferdman, Peter Milder, H&nbsp;Andrew Schwartz,
and Niranjan Balasubramanian.

</span>
<span class="ltx_bibblock">On the distribution, sparsity, and inference-time quantization of
attention values in transformers.

</span>
<span class="ltx_bibblock"><span id="bib.bib119.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.01335</span>, 2021.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Kai Jia and Martin Rinard.

</span>
<span class="ltx_bibblock">Efficient exact verification of binarized neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib120.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, and Zhiliang Gan.

</span>
<span class="ltx_bibblock">Kdlsq-bert: A quantized bert combining knowledge distillation with
learned step size quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib121.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.05938</span>, 2021.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Qing Jin, Linjie Yang, and Zhenyu Liao.

</span>
<span class="ltx_bibblock">Adabits: Neural network quantization with adaptive bit-widths.

</span>
<span class="ltx_bibblock">In <span id="bib.bib122.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2146–2156, 2020.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
Jeff Johnson.

</span>
<span class="ltx_bibblock">Rethinking floating point for deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib123.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.01721</span>, 2018.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Felix Juefei-Xu, Vishnu Naresh&nbsp;Boddeti, and Marios Savvides.

</span>
<span class="ltx_bibblock">Local binary convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib124.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 19–28, 2017.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun
Kwak, Sung&nbsp;Ju Hwang, and Changkyu Choi.

</span>
<span class="ltx_bibblock">Learning to quantize deep networks by optimizing quantization
intervals with task loss.

</span>
<span class="ltx_bibblock">In <span id="bib.bib125.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 4350–4359, 2019.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Prad Kadambi, Karthikeyan&nbsp;Natesan Ramamurthy, and Visar Berisha.

</span>
<span class="ltx_bibblock">Comparing fisher information regularization with distillation for dnn
quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib126.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
PP&nbsp;Kanjilal, PK&nbsp;Dey, and DN&nbsp;Banerjee.

</span>
<span class="ltx_bibblock">Reduced-size neural networks through singular value decomposition and
subset selection.

</span>
<span class="ltx_bibblock"><span id="bib.bib127.1.1" class="ltx_text ltx_font_italic">Electronics Letters</span>, 29(17):1516–1518, 1993.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Mel&nbsp;Win Khaw, Luminita Stevens, and Michael Woodford.

</span>
<span class="ltx_bibblock">Discrete adjustment to a changing environment: Experimental evidence.

</span>
<span class="ltx_bibblock"><span id="bib.bib128.1.1" class="ltx_text ltx_font_italic">Journal of Monetary Economics</span>, 91:88–103, 2017.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Hyungjun Kim, Kyungsu Kim, Jinseok Kim, and Jae-Joon Kim.

</span>
<span class="ltx_bibblock">Binaryduo: Reducing gradient mismatch in binary activation network by
coupling binary activations.

</span>
<span class="ltx_bibblock"><span id="bib.bib129.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2020.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Jangho Kim, KiYoon Yoo, and Nojun Kwak.

</span>
<span class="ltx_bibblock">Position-based scaled gradient for model quantization and sparse
training.

</span>
<span class="ltx_bibblock"><span id="bib.bib130.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Minje Kim and Paris Smaragdis.

</span>
<span class="ltx_bibblock">Bitwise neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib131.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1601.06071</span>, 2016.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael&nbsp;W Mahoney, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">I-bert: Integer-only bert quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib132.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.01321</span>, 2021.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Raghuraman Krishnamoorthi.

</span>
<span class="ltx_bibblock">Quantizing deep convolutional networks for efficient inference: A
whitepaper.

</span>
<span class="ltx_bibblock"><span id="bib.bib133.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1806.08342</span>, 2018.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Andrey Kuzmin, Markus Nagel, Saurabh Pitre, Sandeep Pendyam, Tijmen
Blankevoort, and Max Welling.

</span>
<span class="ltx_bibblock">Taxonomy and evaluation of structured compression of convolutional
neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib134.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.09802</span>, 2019.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
Se&nbsp;Jung Kwon, Dongsoo Lee, Byeongwook Kim, Parichay Kapoor, Baeseong Park, and
Gu-Yeon Wei.

</span>
<span class="ltx_bibblock">Structured compression by weight encryption for unstructured pruning
and quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib135.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 1909–1918, 2020.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Liangzhen Lai, Naveen Suda, and Vikas Chandra.

</span>
<span class="ltx_bibblock">CMSIS-NN: Efficient neural network kernels for arm cortex-m cpus.

</span>
<span class="ltx_bibblock"><span id="bib.bib136.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1801.06601</span>, 2018.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
Hamed&nbsp;F Langroudi, Zachariah Carmichael, David Pastuch, and Dhireesha
Kudithipudi.

</span>
<span class="ltx_bibblock">Cheetah: Mixed low-precision hardware &amp; software co-design framework
for dnns on the edge.

</span>
<span class="ltx_bibblock"><span id="bib.bib137.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1908.02386</span>, 2019.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Kenneth&nbsp;W Latimer, Jacob&nbsp;L Yates, Miriam&nbsp;LR Meister, Alexander&nbsp;C Huk, and
Jonathan&nbsp;W Pillow.

</span>
<span class="ltx_bibblock">Single-trial spike trains in parietal cortex reveal discrete steps
during decision-making.

</span>
<span class="ltx_bibblock"><span id="bib.bib138.1.1" class="ltx_text ltx_font_italic">Science</span>, 349(6244):184–187, 2015.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Yann LeCun, John&nbsp;S Denker, and Sara&nbsp;A Solla.

</span>
<span class="ltx_bibblock">Optimal brain damage.

</span>
<span class="ltx_bibblock">In <span id="bib.bib139.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
598–605, 1990.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Difference target propagation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib140.1.1" class="ltx_text ltx_font_italic">Joint european conference on machine learning and knowledge
discovery in databases</span>, pages 498–515. Springer, 2015.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
Dongsoo Lee, Se&nbsp;Jung Kwon, Byeongwook Kim, Yongkweon Jeon, Baeseong Park, and
Jeongin Yun.

</span>
<span class="ltx_bibblock">Flexor: Trainable fractional quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib141.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Jun&nbsp;Haeng Lee, Sangwon Ha, Saerom Choi, Won-Jo Lee, and Seungwon Lee.

</span>
<span class="ltx_bibblock">Quantization for rapid deployment of deep neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib142.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.05488</span>, 2018.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip&nbsp;HS Torr.

</span>
<span class="ltx_bibblock">Snip: Single-shot network pruning based on connection sensitivity.

</span>
<span class="ltx_bibblock"><span id="bib.bib143.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.02340</span>, 2018.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
Cong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu, and Rong Jin.

</span>
<span class="ltx_bibblock">Extremely low bit neural network: Squeeze the last bit out with admm.

</span>
<span class="ltx_bibblock">In <span id="bib.bib144.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume&nbsp;32, 2018.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Fengfu Li, Bo&nbsp;Zhang, and Bin Liu.

</span>
<span class="ltx_bibblock">Ternary weight networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib145.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1605.04711</span>, 2016.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan, and Rui Fan.

</span>
<span class="ltx_bibblock">Fully quantized network for object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib146.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, 2019.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
Yuhang Li, Xin Dong, and Wei Wang.

</span>
<span class="ltx_bibblock">Additive powers-of-two quantization: An efficient non-uniform
discretization for neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib147.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.13144</span>, 2019.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
Yuhang Li, Ruihao Gong, Xu&nbsp;Tan, Yang Yang, Peng Hu, Qi&nbsp;Zhang, Fengwei Yu, Wei
Wang, and Shi Gu.

</span>
<span class="ltx_bibblock">Brecq: Pushing the limit of post-training quantization by block
reconstruction.

</span>
<span class="ltx_bibblock"><span id="bib.bib148.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
Yuhang Li, Ruihao Gong, Fengwei Yu, Xin Dong, and Xianglong Liu.

</span>
<span class="ltx_bibblock">Dms: Differentiable dimension search for binary neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib149.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2020.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia
Li.

</span>
<span class="ltx_bibblock">Learning from noisy labels with distillation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib150.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision</span>, pages 1910–1918, 2017.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and Wen Gao.

</span>
<span class="ltx_bibblock">Performance guaranteed network acceleration via high-order residual
quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib151.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 2584–2592, 2017.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
Tailin Liang, John Glossner, Lei Wang, and Shaobo Shi.

</span>
<span class="ltx_bibblock">Pruning and quantization for deep neural network acceleration: A
survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib152.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.09671</span>, 2021.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
Zhenyu Liao, Romain Couillet, and Michael&nbsp;W Mahoney.

</span>
<span class="ltx_bibblock">Sparse quantized spectral clustering.

</span>
<span class="ltx_bibblock"><span id="bib.bib153.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy.

</span>
<span class="ltx_bibblock">Fixed point quantization of deep convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib154.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
2849–2858. PMLR, 2016.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian Wu,
Feiyue Huang, and Chia-Wen Lin.

</span>
<span class="ltx_bibblock">Rotated binary neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib155.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
Shaohui Lin, Rongrong Ji, Yuchao Li, Yongjian Wu, Feiyue Huang, and Baochang
Zhang.

</span>
<span class="ltx_bibblock">Accelerating convolutional networks via global &amp; dynamic filter
pruning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib156.1.1" class="ltx_text ltx_font_italic">IJCAI</span>, pages 2425–2432, 2018.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
Wuwei Lin.

</span>
<span class="ltx_bibblock">Automating optimization of quantized deep learning models on cuda:
https://tvm.apache.org/2019/04/29/opt-cuda-quantized, 2019.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
Xiaofan Lin, Cong Zhao, and Wei Pan.

</span>
<span class="ltx_bibblock">Towards accurate binary convolutional neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib158.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.11294</span>, 2017.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Neural networks with few multiplications.

</span>
<span class="ltx_bibblock"><span id="bib.bib159.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1510.03009</span>, 2015.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu,
Rongrong Ji, and David Doermann.

</span>
<span class="ltx_bibblock">Circulant binary convolutional networks: Enhancing the performance of
1-bit dcnns with circulant back propagation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib160.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2691–2699, 2019.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
Hanxiao Liu, Karen Simonyan, and Yiming Yang.

</span>
<span class="ltx_bibblock">Darts: Differentiable architecture search.

</span>
<span class="ltx_bibblock"><span id="bib.bib161.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1806.09055</span>, 2018.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
Hongyang Liu, Sara Elkerdawy, Nilanjan Ray, and Mostafa Elhoushi.

</span>
<span class="ltx_bibblock">Layer importance estimation with imprinting for neural network
quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib162.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2408–2417, 2021.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">RoBERTa: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><span id="bib.bib163.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.11692</span>, 2019.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng.

</span>
<span class="ltx_bibblock">Bi-real net: Enhancing the performance of 1-bit cnns with improved
representational capability and advanced training algorithm.

</span>
<span class="ltx_bibblock">In <span id="bib.bib164.1.1" class="ltx_text ltx_font_italic">Proceedings of the European conference on computer vision
(ECCV)</span>, pages 722–737, 2018.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Zhi-Gang Liu and Matthew Mattina.

</span>
<span class="ltx_bibblock">Learning low-precision neural networks without straight-through
estimator (STE).

</span>
<span class="ltx_bibblock"><span id="bib.bib165.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1903.01061</span>, 2019.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.

</span>
<span class="ltx_bibblock">Thinet: A filter level pruning method for deep neural network
compression.

</span>
<span class="ltx_bibblock">In <span id="bib.bib166.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 5058–5066, 2017.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.

</span>
<span class="ltx_bibblock">Shufflenet V2: Practical guidelines for efficient cnn architecture
design.

</span>
<span class="ltx_bibblock">In <span id="bib.bib167.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 116–131, 2018.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
Franck Mamalet and Christophe Garcia.

</span>
<span class="ltx_bibblock">Simplifying convnets for fast learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib168.1.1" class="ltx_text ltx_font_italic">International Conference on Artificial Neural Networks</span>,
pages 58–65. Springer, 2012.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos.

</span>
<span class="ltx_bibblock">Training binary neural networks with real-to-binary convolutions.

</span>
<span class="ltx_bibblock"><span id="bib.bib169.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.11535</span>, 2020.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Julieta Martinez, Shobhit Zakhmi, Holger&nbsp;H Hoos, and James&nbsp;J Little.

</span>
<span class="ltx_bibblock">Lsq++: Lower running time and higher recall in multi-codebook
quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib170.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 491–506, 2018.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
Warren&nbsp;S McCulloch and Walter Pitts.

</span>
<span class="ltx_bibblock">A logical calculus of the ideas immanent in nervous activity.

</span>
<span class="ltx_bibblock"><span id="bib.bib171.1.1" class="ltx_text ltx_font_italic">The bulletin of mathematical biophysics</span>, 5(4):115–133, 1943.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
Jeffrey&nbsp;L McKinstry, Steven&nbsp;K Esser, Rathinakumar Appuswamy, Deepika Bablani,
John&nbsp;V Arthur, Izzet&nbsp;B Yildiz, and Dharmendra&nbsp;S Modha.

</span>
<span class="ltx_bibblock">Discovering low-precision networks close to full-precision networks
for efficient embedded inference.

</span>
<span class="ltx_bibblock"><span id="bib.bib172.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1809.04191</span>, 2018.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul.

</span>
<span class="ltx_bibblock">Mixed precision training with 8-bit floating point.

</span>
<span class="ltx_bibblock"><span id="bib.bib173.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.12334</span>, 2019.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman.

</span>
<span class="ltx_bibblock">Same, same but different: Recovering neural network quantization
error through weight factorization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib174.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
4486–4495. PMLR, 2019.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
Venkatesh, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mixed precision training.

</span>
<span class="ltx_bibblock"><span id="bib.bib175.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.03740</span>, 2017.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
Szymon Migacz.

</span>
<span class="ltx_bibblock">Nvidia 8-bit inference with tensorrt.

</span>
<span class="ltx_bibblock"><span id="bib.bib176.1.1" class="ltx_text ltx_font_italic">GPU Technology Conference</span>, 2017.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
Asit Mishra and Debbie Marr.

</span>
<span class="ltx_bibblock">Apprentice: Using knowledge distillation techniques to improve
low-precision network accuracy.

</span>
<span class="ltx_bibblock"><span id="bib.bib177.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.05852</span>, 2017.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Asit Mishra, Eriko Nurvitadhi, Jeffrey&nbsp;J Cook, and Debbie Marr.

</span>
<span class="ltx_bibblock">Wrpn: Wide reduced-precision networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib178.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1709.01134</span>, 2017.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
Daisuke Miyashita, Edward&nbsp;H Lee, and Boris Murmann.

</span>
<span class="ltx_bibblock">Convolutional neural networks using logarithmic data representation.

</span>
<span class="ltx_bibblock"><span id="bib.bib179.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1603.01025</span>, 2016.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
Lopamudra Mukherjee, Sathya&nbsp;N Ravi, Jiming Peng, and Vikas Singh.

</span>
<span class="ltx_bibblock">A biresolution spectral framework for product quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib180.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 3329–3338, 2018.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
Markus Nagel, Rana&nbsp;Ali Amjad, Mart Van&nbsp;Baalen, Christos Louizos, and Tijmen
Blankevoort.

</span>
<span class="ltx_bibblock">Up or down? adaptive rounding for post-training quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib181.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
7197–7206. PMLR, 2020.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
Markus Nagel, Mart&nbsp;van Baalen, Tijmen Blankevoort, and Max Welling.

</span>
<span class="ltx_bibblock">Data-free quantization through weight equalization and bias
correction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib182.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 1325–1334, 2019.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
Markus Nagel, Marios Fournarakis, Rana&nbsp;Ali Amjad, Yelysei Bondarenko, Mart van
Baalen, and Tijmen Blankevoort.

</span>
<span class="ltx_bibblock">A white paper on neural network quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib183.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.08295</span>, 2021.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
Maxim Naumov, Utku Diril, Jongsoo Park, Benjamin Ray, Jedrzej Jablonski, and
Andrew Tulloch.

</span>
<span class="ltx_bibblock">On periodic functions as regularizers for quantization of neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib184.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1811.09862</span>, 2018.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
Maxim Naumov, Dheevatsa Mudigere, Hao-Jun&nbsp;Michael Shi, Jianyu Huang, Narayanan
Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu,
Alisson&nbsp;G Azzolini, et&nbsp;al.

</span>
<span class="ltx_bibblock">Deep learning recommendation model for personalization and
recommendation systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib185.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.00091</span>, 2019.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
Renkun Ni, Hong-min Chu, Oscar Castañeda, Ping-yeh Chiang, Christoph
Studer, and Tom Goldstein.

</span>
<span class="ltx_bibblock">Wrapnet: Neural net inference with ultra-low-resolution arithmetic.

</span>
<span class="ltx_bibblock"><span id="bib.bib186.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.13242</span>, 2020.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
Lin Ning, Guoyang Chen, Weifeng Zhang, and Xipeng Shen.

</span>
<span class="ltx_bibblock">Simple augmentation goes a long way: {ADRL} for {dnn}
quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib187.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
BM&nbsp;Oliver, JR&nbsp;Pierce, and Claude&nbsp;E Shannon.

</span>
<span class="ltx_bibblock">The philosophy of pcm.

</span>
<span class="ltx_bibblock"><span id="bib.bib188.1.1" class="ltx_text ltx_font_italic">Proceedings of the IRE</span>, 36(11):1324–1331, 1948.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Eunhyeok Park, Junwhan Ahn, and Sungjoo Yoo.

</span>
<span class="ltx_bibblock">Weighted-entropy-based quantization for deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib189.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 5456–5464, 2017.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
Eunhyeok Park, Sungjoo Yoo, and Peter Vajda.

</span>
<span class="ltx_bibblock">Value-aware quantization for training and inference of neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib190.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 580–595, 2018.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
Sejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin.

</span>
<span class="ltx_bibblock">Lookahead: a far-sighted alternative of magnitude-based pruning.

</span>
<span class="ltx_bibblock"><span id="bib.bib191.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.04809</span>, 2020.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho.

</span>
<span class="ltx_bibblock">Relational knowledge distillation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib192.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 3967–3976, 2019.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
Peng Peng, Mingyu You, Weisheng Xu, and Jiaxin Li.

</span>
<span class="ltx_bibblock">Fully integer-based quantization for mobile convolutional neural
network inference.

</span>
<span class="ltx_bibblock"><span id="bib.bib193.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 432:194–205, 2021.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean.

</span>
<span class="ltx_bibblock">Efficient neural architecture search via parameters sharing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib194.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
4095–4104. PMLR, 2018.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
Antonio Polino, Razvan Pascanu, and Dan Alistarh.

</span>
<span class="ltx_bibblock">Model compression via distillation and quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib195.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.05668</span>, 2018.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
Haotong Qin, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao, Shuai Yi,
Xianglong Liu, and Hao Su.

</span>
<span class="ltx_bibblock">Bipointnet: Binary neural network for point clouds.

</span>
<span class="ltx_bibblock"><span id="bib.bib196.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu
Sebe.

</span>
<span class="ltx_bibblock">Binary neural networks: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib197.1.1" class="ltx_text ltx_font_italic">Pattern Recognition</span>, 105:107281, 2020.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu,
and Jingkuan Song.

</span>
<span class="ltx_bibblock">Forward and backward information retention for accurate binary neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib198.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2250–2259, 2020.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
Zhongnan Qu, Zimu Zhou, Yun Cheng, and Lothar Thiele.

</span>
<span class="ltx_bibblock">Adaptive loss-aware quantization for multi-bit networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib199.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, June 2020.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training, 2018.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib201.1.1" class="ltx_text ltx_font_italic">OpenAI blog</span>, 1(8):9, 2019.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
Prajit Ramachandran, Barret Zoph, and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Searching for activation functions.

</span>
<span class="ltx_bibblock"><span id="bib.bib202.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.05941</span>, 2017.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
Prajit Ramachandran, Barret Zoph, and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Swish: a self-gated activation function.

</span>
<span class="ltx_bibblock"><span id="bib.bib203.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.05941</span>, 7:1, 2017.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.

</span>
<span class="ltx_bibblock">Xnor-net: Imagenet classification using binary convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib204.1.1" class="ltx_text ltx_font_italic">European conference on computer vision</span>, pages 525–542.
Springer, 2016.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
Ryan Razani, Gregoire Morin, Eyyub Sari, and Vahid&nbsp;Partovi Nia.

</span>
<span class="ltx_bibblock">Adaptive binary-ternary quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib205.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 4613–4618, 2021.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
Bernhard Riemann.

</span>
<span class="ltx_bibblock"><span id="bib.bib206.1.1" class="ltx_text ltx_font_italic">Ueber die Darstellbarkeit einer Function durch eine
trigonometrische Reihe</span>, volume&nbsp;13.

</span>
<span class="ltx_bibblock">Dieterich, 1867.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
Adriana Romero, Nicolas Ballas, Samira&nbsp;Ebrahimi Kahou, Antoine Chassang, Carlo
Gatta, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Fitnets: Hints for thin deep nets.

</span>
<span class="ltx_bibblock"><span id="bib.bib207.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6550</span>, 2014.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
Kenneth Rose, Eitan Gurewitz, and Geoffrey Fox.

</span>
<span class="ltx_bibblock">A deterministic annealing approach to clustering.

</span>
<span class="ltx_bibblock"><span id="bib.bib208.1.1" class="ltx_text ltx_font_italic">Pattern Recognition Letters</span>, 11(9):589–594, 1990.

</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
Frank Rosenblatt.

</span>
<span class="ltx_bibblock"><span id="bib.bib209.1.1" class="ltx_text ltx_font_italic">The perceptron, a perceiving and recognizing automaton Project
Para</span>.

</span>
<span class="ltx_bibblock">Cornell Aeronautical Laboratory, 1957.

</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
Frank Rosenblatt.

</span>
<span class="ltx_bibblock">Principles of neurodynamics. perceptrons and the theory of brain
mechanisms.

</span>
<span class="ltx_bibblock">Technical report, Cornell Aeronautical Lab Inc Buffalo NY, 1961.

</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
Manuele Rusci, Marco Fariselli, Alessandro Capotondi, and Luca Benini.

</span>
<span class="ltx_bibblock">Leveraging automated mixed-low-precision quantization for tiny edge
microcontrollers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib211.1.1" class="ltx_text ltx_font_italic">IoT Streams for Data-Driven Predictive Maintenance and IoT,
Edge, and Mobile for Embedded Machine Learning</span>, pages 296–308. Springer,
2020.

</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
Tara&nbsp;N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana
Ramabhadran.

</span>
<span class="ltx_bibblock">Low-rank matrix factorization for deep neural network training with
high-dimensional output targets.

</span>
<span class="ltx_bibblock">In <span id="bib.bib212.1.1" class="ltx_text ltx_font_italic">2013 IEEE international conference on acoustics, speech and
signal processing</span>, pages 6655–6659. IEEE, 2013.

</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
Dave Salvator, Hao Wu, Milind Kulkarni, and Niall Emmart.

</span>
<span class="ltx_bibblock">Int4 precision for ai inference:
https://developer.nvidia.com/blog/int4-for-ai-inference/, 2019.

</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
Chen.

</span>
<span class="ltx_bibblock">MobilenetV2: Inverted residuals and linear bottlenecks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib214.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4510–4520, 2018.

</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
Claude&nbsp;E Shannon.

</span>
<span class="ltx_bibblock">A mathematical theory of communication.

</span>
<span class="ltx_bibblock"><span id="bib.bib215.1.1" class="ltx_text ltx_font_italic">The Bell system technical journal</span>, 27(3):379–423, 1948.

</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
Claude&nbsp;E Shannon.

</span>
<span class="ltx_bibblock">Coding theorems for a discrete source with a fidelity criterion.

</span>
<span class="ltx_bibblock"><span id="bib.bib216.1.1" class="ltx_text ltx_font_italic">IRE Nat. Conv. Rec</span>, 4(142-163):1, 1959.

</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
Alexander Shekhovtsov, Viktor Yanush, and Boris Flach.

</span>
<span class="ltx_bibblock">Path sample-analytic gradient estimators for stochastic binary
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib217.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
Mingzhu Shen, Xianglong Liu, Ruihao Gong, and Kai Han.

</span>
<span class="ltx_bibblock">Balanced binary neural networks with gated residual.

</span>
<span class="ltx_bibblock">In <span id="bib.bib218.1.1" class="ltx_text ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>, pages 4197–4201. IEEE, 2020.

</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
Michael&nbsp;W Mahoney, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">Q-BERT: Hessian based ultra low precision quantization of bert.

</span>
<span class="ltx_bibblock">In <span id="bib.bib219.1.1" class="ltx_text ltx_font_italic">AAAI</span>, pages 8815–8821, 2020.

</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
William&nbsp;Fleetwood Sheppard.

</span>
<span class="ltx_bibblock">On the calculation of the most probable values of
frequency-constants, for data arranged according to equidistant division of a
scale.

</span>
<span class="ltx_bibblock"><span id="bib.bib220.1.1" class="ltx_text ltx_font_italic">Proceedings of the London Mathematical Society</span>, 1(1):353–380,
1897.

</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
Sungho Shin, Kyuyeon Hwang, and Wonyong Sung.

</span>
<span class="ltx_bibblock">Fixed-point performance analysis of recurrent neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib221.1.1" class="ltx_text ltx_font_italic">2016 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</span>, pages 976–980. IEEE, 2016.

</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yuri Nahshan, Alex
Bronstein, and Uri Weiser.

</span>
<span class="ltx_bibblock">Robust quantization: One model to rule them all.

</span>
<span class="ltx_bibblock"><span id="bib.bib222.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
Gil Shomron, Freddy Gabbay, Samer Kurzum, and Uri Weiser.

</span>
<span class="ltx_bibblock">Post-training sparsity-aware quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib223.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.11010</span>, 2021.

</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
K.&nbsp;Simonyan and A.&nbsp;Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib224.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2015.

</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
S.&nbsp;M. Stigler.

</span>
<span class="ltx_bibblock"><span id="bib.bib225.1.1" class="ltx_text ltx_font_italic">The History of Statistics: The Measurement of Uncertainty before
1900</span>.

</span>
<span class="ltx_bibblock">Harvard University Press, Cambridge, 1986.

</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
Pierre Stock, Angela Fan, Benjamin Graham, Edouard Grave, Rémi Gribonval,
Herve Jegou, and Armand Joulin.

</span>
<span class="ltx_bibblock">Training with quantization noise for extreme model compression.

</span>
<span class="ltx_bibblock">In <span id="bib.bib226.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock">
Pierre Stock, Armand Joulin, Rémi Gribonval, Benjamin Graham, and Hervé
Jégou.

</span>
<span class="ltx_bibblock">And the bit goes down: Revisiting the quantization of neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib227.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1907.05686</span>, 2019.

</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock">
John&nbsp;Z Sun, Grace&nbsp;I Wang, Vivek&nbsp;K Goyal, and Lav&nbsp;R Varshney.

</span>
<span class="ltx_bibblock">A framework for bayesian optimality of psychophysical laws.

</span>
<span class="ltx_bibblock"><span id="bib.bib228.1.1" class="ltx_text ltx_font_italic">Journal of Mathematical Psychology</span>, 56(6):495–501, 2012.

</span>
</li>
<li id="bib.bib229" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock">
Wonyong Sung, Sungho Shin, and Kyuyeon Hwang.

</span>
<span class="ltx_bibblock">Resiliency of deep neural networks under quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib229.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.06488</span>, 2015.

</span>
</li>
<li id="bib.bib230" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock">
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
Wojna.

</span>
<span class="ltx_bibblock">Rethinking the Inception architecture for computer vision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib230.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 2818–2826, 2016.

</span>
</li>
<li id="bib.bib231" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock">
Shyam&nbsp;A Tailor, Javier Fernandez-Marques, and Nicholas&nbsp;D Lane.

</span>
<span class="ltx_bibblock">Degree-quant: Quantization-aware training for graph neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib231.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib232" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock">
Mingxing Tan, Bo&nbsp;Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
Howard, and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Mnasnet: Platform-aware neural architecture search for mobile.

</span>
<span class="ltx_bibblock">In <span id="bib.bib232.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2820–2828, 2019.

</span>
</li>
<li id="bib.bib233" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock">
Mingxing Tan and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">EfficientNet: Rethinking model scaling for convolutional neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib233.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.11946</span>, 2019.

</span>
</li>
<li id="bib.bib234" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock">
Wei Tang, Gang Hua, and Liang Wang.

</span>
<span class="ltx_bibblock">How to train a compact binary neural network with high accuracy?

</span>
<span class="ltx_bibblock">In <span id="bib.bib234.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, volume&nbsp;31, 2017.

</span>
</li>
<li id="bib.bib235" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[235]</span>
<span class="ltx_bibblock">
Antti Tarvainen and Harri Valpola.

</span>
<span class="ltx_bibblock">Mean teachers are better role models: Weight-averaged consistency
targets improve semi-supervised deep learning results.

</span>
<span class="ltx_bibblock"><span id="bib.bib235.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1703.01780</span>, 2017.

</span>
</li>
<li id="bib.bib236" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[236]</span>
<span class="ltx_bibblock">
James Tee and Desmond&nbsp;P Taylor.

</span>
<span class="ltx_bibblock">Is information in the brain represented in continuous or discrete
form?

</span>
<span class="ltx_bibblock"><span id="bib.bib236.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Molecular, Biological and Multi-Scale
Communications</span>, 6(3):199–209, 2020.

</span>
</li>
<li id="bib.bib237" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[237]</span>
<span class="ltx_bibblock">
L.N. Trefethen and D.&nbsp;Bau III.

</span>
<span class="ltx_bibblock"><span id="bib.bib237.1.1" class="ltx_text ltx_font_italic">Numerical Linear Algebra</span>.

</span>
<span class="ltx_bibblock">SIAM, Philadelphia, 1997.

</span>
</li>
<li id="bib.bib238" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[238]</span>
<span class="ltx_bibblock">
Frederick Tung and Greg Mori.

</span>
<span class="ltx_bibblock">Clip-q: Deep network compression learning by in-parallel
pruning-quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib238.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 7873–7882, 2018.

</span>
</li>
<li id="bib.bib239" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[239]</span>
<span class="ltx_bibblock">
Mart van Baalen, Christos Louizos, Markus Nagel, Rana&nbsp;Ali Amjad, Ying Wang,
Tijmen Blankevoort, and Max Welling.

</span>
<span class="ltx_bibblock">Bayesian bits: Unifying quantization and pruning.

</span>
<span class="ltx_bibblock"><span id="bib.bib239.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib240" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[240]</span>
<span class="ltx_bibblock">
Rufin VanRullen and Christof Koch.

</span>
<span class="ltx_bibblock">Is perception discrete or continuous?

</span>
<span class="ltx_bibblock"><span id="bib.bib240.1.1" class="ltx_text ltx_font_italic">Trends in cognitive sciences</span>, 7(5):207–213, 2003.

</span>
</li>
<li id="bib.bib241" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[241]</span>
<span class="ltx_bibblock">
Lav&nbsp;R Varshney, Per&nbsp;Jesper Sjöström, and Dmitri&nbsp;B Chklovskii.

</span>
<span class="ltx_bibblock">Optimal information storage in noisy synapses under resource
constraints.

</span>
<span class="ltx_bibblock"><span id="bib.bib241.1.1" class="ltx_text ltx_font_italic">Neuron</span>, 52(3):409–423, 2006.

</span>
</li>
<li id="bib.bib242" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[242]</span>
<span class="ltx_bibblock">
Lav&nbsp;R Varshney and Kush&nbsp;R Varshney.

</span>
<span class="ltx_bibblock">Decision making with quantized priors leads to discrimination.

</span>
<span class="ltx_bibblock"><span id="bib.bib242.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 105(2):241–255, 2016.

</span>
</li>
<li id="bib.bib243" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[243]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span id="bib.bib243.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
5998–6008, 2017.

</span>
</li>
<li id="bib.bib244" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[244]</span>
<span class="ltx_bibblock">
Diwen Wan, Fumin Shen, Li&nbsp;Liu, Fan Zhu, Jie Qin, Ling Shao, and Heng&nbsp;Tao Shen.

</span>
<span class="ltx_bibblock">Tbn: Convolutional neural network with ternary inputs and binary
weights.

</span>
<span class="ltx_bibblock">In <span id="bib.bib244.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 315–332, 2018.

</span>
</li>
<li id="bib.bib245" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[245]</span>
<span class="ltx_bibblock">
Dilin Wang, Meng Li, Chengyue Gong, and Vikas Chandra.

</span>
<span class="ltx_bibblock">Attentivenas: Improving neural architecture search via attentive
sampling.

</span>
<span class="ltx_bibblock"><span id="bib.bib245.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2011.09011</span>, 2020.

</span>
</li>
<li id="bib.bib246" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[246]</span>
<span class="ltx_bibblock">
Kuan Wang, Zhijian Liu, Yujun Lin, Ji&nbsp;Lin, and Song Han.

</span>
<span class="ltx_bibblock">HAQ: Hardware-aware automated quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib246.1.1" class="ltx_text ltx_font_italic">In Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, 2019.

</span>
</li>
<li id="bib.bib247" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[247]</span>
<span class="ltx_bibblock">
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash
Gopalakrishnan.

</span>
<span class="ltx_bibblock">Training deep neural networks with 8-bit floating point numbers.

</span>
<span class="ltx_bibblock"><span id="bib.bib247.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2018.

</span>
</li>
<li id="bib.bib248" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[248]</span>
<span class="ltx_bibblock">
Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang, Yang Liu, and Jian Cheng.

</span>
<span class="ltx_bibblock">Two-step quantization for low-bit neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib248.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on computer vision and
pattern recognition</span>, pages 4376–4384, 2018.

</span>
</li>
<li id="bib.bib249" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[249]</span>
<span class="ltx_bibblock">
Tianzhe Wang, Kuan Wang, Han Cai, Ji&nbsp;Lin, Zhijian Liu, Hanrui Wang, Yujun Lin,
and Song Han.

</span>
<span class="ltx_bibblock">Apq: Joint search for network architecture, pruning and quantization
policy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib249.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2078–2087, 2020.

</span>
</li>
<li id="bib.bib250" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[250]</span>
<span class="ltx_bibblock">
Ying Wang, Yadong Lu, and Tijmen Blankevoort.

</span>
<span class="ltx_bibblock">Differentiable joint pruning and quantization for hardware
efficiency.

</span>
<span class="ltx_bibblock">In <span id="bib.bib250.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 259–277.
Springer, 2020.

</span>
</li>
<li id="bib.bib251" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[251]</span>
<span class="ltx_bibblock">
Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi&nbsp;Tian.

</span>
<span class="ltx_bibblock">Learning channel-wise interactions for binary convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib251.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 568–577, 2019.

</span>
</li>
<li id="bib.bib252" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[252]</span>
<span class="ltx_bibblock">
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu,
Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">FBNet: Hardware-aware efficient convnet design via differentiable
neural architecture search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib252.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 10734–10742, 2019.

</span>
</li>
<li id="bib.bib253" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[253]</span>
<span class="ltx_bibblock">
Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah Golmant, Amir
Gholaminejad, Joseph Gonzalez, and Kurt Keutzer.

</span>
<span class="ltx_bibblock">Shift: A zero flop, zero parameter alternative to spatial
convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib253.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 9127–9135, 2018.

</span>
</li>
<li id="bib.bib254" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[254]</span>
<span class="ltx_bibblock">
Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt
Keutzer.

</span>
<span class="ltx_bibblock">Mixed precision quantization of convnets via differentiable neural
architecture search.

</span>
<span class="ltx_bibblock"><span id="bib.bib254.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.00090</span>, 2018.

</span>
</li>
<li id="bib.bib255" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[255]</span>
<span class="ltx_bibblock">
Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius.

</span>
<span class="ltx_bibblock">Integer quantization for deep learning inference: Principles and
empirical evaluation.

</span>
<span class="ltx_bibblock"><span id="bib.bib255.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.09602</span>, 2020.

</span>
</li>
<li id="bib.bib256" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[256]</span>
<span class="ltx_bibblock">
Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng.

</span>
<span class="ltx_bibblock">Quantized convolutional neural networks for mobile devices.

</span>
<span class="ltx_bibblock">In <span id="bib.bib256.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4820–4828, 2016.

</span>
</li>
<li id="bib.bib257" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[257]</span>
<span class="ltx_bibblock">
Xia Xiao, Zigeng Wang, and Sanguthevar Rajasekaran.

</span>
<span class="ltx_bibblock">Autoprune: Automatic network pruning by regularizing auxiliary
parameters.

</span>
<span class="ltx_bibblock">In <span id="bib.bib257.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
13681–13691, 2019.

</span>
</li>
<li id="bib.bib258" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[258]</span>
<span class="ltx_bibblock">
Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, and
Hongbin Zha.

</span>
<span class="ltx_bibblock">Alternating multi-bit quantization for recurrent neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib258.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.00150</span>, 2018.

</span>
</li>
<li id="bib.bib259" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[259]</span>
<span class="ltx_bibblock">
Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang,
and Mingkui Tan.

</span>
<span class="ltx_bibblock">Generative low-bitwidth data free quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib259.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 1–17.
Springer, 2020.

</span>
</li>
<li id="bib.bib260" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[260]</span>
<span class="ltx_bibblock">
Yinghao Xu, Xin Dong, Yudian Li, and Hao Su.

</span>
<span class="ltx_bibblock">A main/subsidiary network framework for simplifying binary neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib260.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 7154–7162, 2019.

</span>
</li>
<li id="bib.bib261" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[261]</span>
<span class="ltx_bibblock">
Zhe Xu and Ray&nbsp;CC Cheung.

</span>
<span class="ltx_bibblock">Accurate and compact convolutional neural networks with trained
binarization.

</span>
<span class="ltx_bibblock"><span id="bib.bib261.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.11366</span>, 2019.

</span>
</li>
<li id="bib.bib262" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[262]</span>
<span class="ltx_bibblock">
Haichuan Yang, Shupeng Gui, Yuhao Zhu, and Ji&nbsp;Liu.

</span>
<span class="ltx_bibblock">Automatic neural network compression by sparsity-quantization joint
learning: A constrained optimization-based approach.

</span>
<span class="ltx_bibblock">In <span id="bib.bib262.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 2178–2188, 2020.

</span>
</li>
<li id="bib.bib263" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[263]</span>
<span class="ltx_bibblock">
Huanrui Yang, Lin Duan, Yiran Chen, and Hai Li.

</span>
<span class="ltx_bibblock">Bsq: Exploring bit-level sparsity for mixed-precision neural network
quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib263.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2102.10462</span>, 2021.

</span>
</li>
<li id="bib.bib264" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[264]</span>
<span class="ltx_bibblock">
Jiwei Yang, Xu&nbsp;Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang
Huang, and Xian-sheng Hua.

</span>
<span class="ltx_bibblock">Quantization networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib264.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 7308–7316, 2019.

</span>
</li>
<li id="bib.bib265" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[265]</span>
<span class="ltx_bibblock">
Tien-Ju Yang, Andrew Howard, Bo&nbsp;Chen, Xiao Zhang, Alec Go, Mark Sandler,
Vivienne Sze, and Hartwig Adam.

</span>
<span class="ltx_bibblock">Netadapt: Platform-aware neural network adaptation for mobile
applications.

</span>
<span class="ltx_bibblock">In <span id="bib.bib265.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 285–300, 2018.

</span>
</li>
<li id="bib.bib266" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[266]</span>
<span class="ltx_bibblock">
Zhaohui Yang, Yunhe Wang, Kai Han, Chunjing Xu, Chao Xu, Dacheng Tao, and Chang
Xu.

</span>
<span class="ltx_bibblock">Searching for low-bit weights in quantized neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib266.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 2020.

</span>
</li>
<li id="bib.bib267" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[267]</span>
<span class="ltx_bibblock">
Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan,
Leyuan Wang, Qijing Huang, Yida Wang, Michael&nbsp;W Mahoney, et&nbsp;al.

</span>
<span class="ltx_bibblock">Hawqv3: Dyadic neural network quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib267.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2011.10680</span>, 2020.

</span>
</li>
<li id="bib.bib268" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[268]</span>
<span class="ltx_bibblock">
Jianming Ye, Shiliang Zhang, and Jingdong Wang.

</span>
<span class="ltx_bibblock">Distillation guided residual learning for binary convolutional neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib268.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.05223</span>, 2020.

</span>
</li>
<li id="bib.bib269" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[269]</span>
<span class="ltx_bibblock">
Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim.

</span>
<span class="ltx_bibblock">A gift from knowledge distillation: Fast optimization, network
minimization and transfer learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib269.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4133–4141, 2017.

</span>
</li>
<li id="bib.bib270" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[270]</span>
<span class="ltx_bibblock">
Hongxu Yin, Pavlo Molchanov, Jose&nbsp;M Alvarez, Zhizhong Li, Arun Mallya, Derek
Hoiem, Niraj&nbsp;K Jha, and Jan Kautz.

</span>
<span class="ltx_bibblock">Dreaming to distill: Data-free knowledge transfer via deepinversion.

</span>
<span class="ltx_bibblock">In <span id="bib.bib270.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 8715–8724, 2020.

</span>
</li>
<li id="bib.bib271" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[271]</span>
<span class="ltx_bibblock">
Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack
Xin.

</span>
<span class="ltx_bibblock">Understanding straight-through estimator in training activation
quantized neural nets.

</span>
<span class="ltx_bibblock"><span id="bib.bib271.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1903.05662</span>, 2019.

</span>
</li>
<li id="bib.bib272" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[272]</span>
<span class="ltx_bibblock">
Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack
Xin.

</span>
<span class="ltx_bibblock">Blended coarse gradient descent for full quantization of deep neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib272.1.1" class="ltx_text ltx_font_italic">Research in the Mathematical Sciences</span>, 6(1):14, 2019.

</span>
</li>
<li id="bib.bib273" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[273]</span>
<span class="ltx_bibblock">
Shan You, Chang Xu, Chao Xu, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Learning from multiple teacher networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib273.1.1" class="ltx_text ltx_font_italic">Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining</span>, pages 1285–1294, 2017.

</span>
</li>
<li id="bib.bib274" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[274]</span>
<span class="ltx_bibblock">
Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad&nbsp;I Morariu, Xintong Han,
Mingfei Gao, Ching-Yung Lin, and Larry&nbsp;S Davis.

</span>
<span class="ltx_bibblock">Nisp: Pruning networks using neuron importance score propagation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib274.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 9194–9203, 2018.

</span>
</li>
<li id="bib.bib275" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[275]</span>
<span class="ltx_bibblock">
Shixing Yu, Zhewei Yao, Amir Gholami, Zhen Dong, Michael&nbsp;W Mahoney, and Kurt
Keutzer.

</span>
<span class="ltx_bibblock">Hessian-aware pruning and optimal neural implant.

</span>
<span class="ltx_bibblock"><span id="bib.bib275.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.08940</span>, 2021.

</span>
</li>
<li id="bib.bib276" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[276]</span>
<span class="ltx_bibblock">
Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua.

</span>
<span class="ltx_bibblock">Lq-nets: Learned quantization for highly accurate and compact deep
neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib276.1.1" class="ltx_text ltx_font_italic">European conference on computer vision (ECCV)</span>, 2018.

</span>
</li>
<li id="bib.bib277" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[277]</span>
<span class="ltx_bibblock">
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng
Ma.

</span>
<span class="ltx_bibblock">Be your own teacher: Improve the performance of convolutional neural
networks via self distillation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib277.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 3713–3722, 2019.

</span>
</li>
<li id="bib.bib278" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[278]</span>
<span class="ltx_bibblock">
Wei Zhang, Lu&nbsp;Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu.

</span>
<span class="ltx_bibblock">Ternarybert: Distillation-aware ultra-low bit bert.

</span>
<span class="ltx_bibblock"><span id="bib.bib278.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2009.12812</span>, 2020.

</span>
</li>
<li id="bib.bib279" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[279]</span>
<span class="ltx_bibblock">
Chenglong Zhao, Bingbing Ni, Jian Zhang, Qiwei Zhao, Wenjun Zhang, and Qi&nbsp;Tian.

</span>
<span class="ltx_bibblock">Variational convolutional neural network pruning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib279.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 2780–2789, 2019.

</span>
</li>
<li id="bib.bib280" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[280]</span>
<span class="ltx_bibblock">
Qibin Zhao, Masashi Sugiyama, Longhao Yuan, and Andrzej Cichocki.

</span>
<span class="ltx_bibblock">Learning efficient tensor representations with ring-structured
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib280.1.1" class="ltx_text ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>, pages 8608–8612. IEEE, 2019.

</span>
</li>
<li id="bib.bib281" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[281]</span>
<span class="ltx_bibblock">
Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De&nbsp;Sa, and Zhiru Zhang.

</span>
<span class="ltx_bibblock">Improving neural network quantization without retraining using
outlier channel splitting.

</span>
<span class="ltx_bibblock"><span id="bib.bib281.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, 2019.

</span>
</li>
<li id="bib.bib282" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[282]</span>
<span class="ltx_bibblock">
Sijie Zhao, Tao Yue, and Xuemei Hu.

</span>
<span class="ltx_bibblock">Distribution-aware adaptive multi-bit quantization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib282.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 9281–9290, 2021.

</span>
</li>
<li id="bib.bib283" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[283]</span>
<span class="ltx_bibblock">
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen.

</span>
<span class="ltx_bibblock">Incremental network quantization: Towards lossless cnns with
low-precision weights.

</span>
<span class="ltx_bibblock"><span id="bib.bib283.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1702.03044</span>, 2017.

</span>
</li>
<li id="bib.bib284" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[284]</span>
<span class="ltx_bibblock">
Aojun Zhou, Anbang Yao, Kuan Wang, and Yurong Chen.

</span>
<span class="ltx_bibblock">Explicit loss-error-aware quantization for low-bit deep neural
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib284.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 9426–9435, 2018.

</span>
</li>
<li id="bib.bib285" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[285]</span>
<span class="ltx_bibblock">
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He&nbsp;Wen, and Yuheng Zou.

</span>
<span class="ltx_bibblock">Dorefa-net: Training low bitwidth convolutional neural networks with
low bitwidth gradients.

</span>
<span class="ltx_bibblock"><span id="bib.bib285.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.06160</span>, 2016.

</span>
</li>
<li id="bib.bib286" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[286]</span>
<span class="ltx_bibblock">
Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal
Frossard.

</span>
<span class="ltx_bibblock">Adaptive quantization for deep neural network.

</span>
<span class="ltx_bibblock"><span id="bib.bib286.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1712.01048</span>, 2017.

</span>
</li>
<li id="bib.bib287" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[287]</span>
<span class="ltx_bibblock">
Chenzhuo Zhu, Song Han, Huizi Mao, and William&nbsp;J Dally.

</span>
<span class="ltx_bibblock">Trained ternary quantization.

</span>
<span class="ltx_bibblock"><span id="bib.bib287.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1612.01064</span>, 2016.

</span>
</li>
<li id="bib.bib288" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[288]</span>
<span class="ltx_bibblock">
Shilin Zhu, Xin Dong, and Hao Su.

</span>
<span class="ltx_bibblock">Binary ensemble neural network: More bits per network or more
networks per bit?

</span>
<span class="ltx_bibblock">In <span id="bib.bib288.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 4923–4932, 2019.

</span>
</li>
<li id="bib.bib289" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[289]</span>
<span class="ltx_bibblock">
Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid.

</span>
<span class="ltx_bibblock">Towards effective low-bitwidth convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib289.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 7920–7928, 2018.

</span>
</li>
<li id="bib.bib290" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[290]</span>
<span class="ltx_bibblock">
Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid.

</span>
<span class="ltx_bibblock">Structured binary neural networks for accurate image classification
and semantic segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib290.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 413–422, 2019.

</span>
</li>
<li id="bib.bib291" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[291]</span>
<span class="ltx_bibblock">
Barret Zoph and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Neural architecture search with reinforcement learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib291.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.01578</span>, 2016.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2103.13629" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2103.13630" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2103.13630">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2103.13630" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2103.13631" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 17:04:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
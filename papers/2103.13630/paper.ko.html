<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '효율적인 신경망 추론을 위한 양자화 기법 연구\n' +
      '\n' +
      '아미르 골라미1, 세훈 김1, 진 동1, 제웨이 야오1, 마이클 W. 커트 커처 마호니\n' +
      '\n' +
      '버클리 캘리포니아 대학교\n' +
      '\n' +
      '{amirgh, sehoonkim, zhendong, zheweiy, mahoneymw, keutzer}@berkeley.edu\n' +
      '\n' +
      '1Equal contribution.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '추상적인 수학적 계산이 디지털 컴퓨터에서의 계산에 적응하자마자, 그 계산들에서 수치들의 효율적인 표현, 조작, 및 통신의 문제가 발생하였다. 숫자 표현의 문제와 강하게 관련된 것은 양자화의 문제이다: 필요한 비트 수를 최소화하고 또한 수반되는 계산의 정확도를 최대화하기 위해 연속 실수 값 숫자 세트를 고정된 이산 숫자 세트에 분산해야 하는 방법은 무엇인가? 양자화의 이러한 다년생 문제는 메모리 및/또는 계산 자원이 심각하게 제한될 때마다 특히 관련이 있으며, 컴퓨터 비전, 자연 언어 처리 및 관련 영역에서 신경망 모델의 놀라운 성능으로 인해 최근 몇 년 동안 전면에 등장했다. 부동 소수점 표현에서 4비트 이하로 표현된 저-정밀 고정 정수 값으로의 이동은 메모리 풋프린트 및 레이턴시를 16배만큼 감소시킬 가능성을 보유하며, 실제로, 4x 내지 8x의 감소는 종종 이러한 애플리케이션들에서 실제로 실현된다. 따라서, 양자화가 최근 신경망과 관련된 계산의 효율적인 구현에 있어서 중요하고 매우 활발한 연구의 하위 영역으로 부상한 것은 놀라운 일이 아니다. 본 논문에서는 심층신경망 계산에서 수치의 정량화 문제에 대한 기존의 방법들의 장단점을 고찰한다. 이 설문조사와 그 조직을 통해, 우리는 신경망에 대한 양자화에 있어 현재 연구의 유용한 스냅샷을 제시하고 이 분야의 향후 연구에 대한 평가를 쉽게 할 수 있는 지능화된 조직을 제공하기를 희망한다.\n' +
      '\n' +
      '## I Introduction\n' +
      '\n' +
      '지난 10년 동안, 우리는 종종 고도로 과대 매개변수화된 모델에 의해 달성되는 광범위한 문제에 대해 신경망(NN)의 정확도에서 상당한 개선을 관찰했다. 이러한 오버파라미터화된(따라서 매우 큰) NN 모델의 정확도가 상당히 증가했지만, 이러한 모델의 순전한 크기는 많은 리소스 제약 애플리케이션에 대해 이를 배포할 수 없음을 의미한다. 이는 자원이 제한된 환경에서 낮은 에너지 소비와 높은 정확도로 실시간 추론이 요구되는 퍼베이시브 딥러닝을 구현하기 위한 문제를 발생시킨다. 이 퍼베이시브 딥 러닝은 실시간 지능형 의료 모니터링, 자율 주행, 오디오 분석 및 음성 인식과 같은 광범위한 응용 프로그램에 상당한 영향을 미칠 것으로 예상된다.\n' +
      '\n' +
      '최적의 정확도로 효율적인 실시간 NN을 달성하려면 NN 모델의 설계, 훈련 및 배치를 다시 고려해야 한다[71]. NN 모델을 보다 효율적으로(레이턴시, 메모리 풋프린트 및 에너지 소비 등 측면에서) 만들면서 최적의 정확도/일반화 트레이드오프를 제공함으로써 이러한 문제를 해결하는 데 초점을 맞춘 많은 문헌이 있다. 이러한 노력은 크게 다음과 같이 범주화할 수 있다.\n' +
      '\n' +
      '효율적인 NN 모델 아키텍처를 설계하는 한 작업 라인은 그것의 마이크로-아키텍처[101, 111, 127, 167, 168, 212, 253, 280](예를 들어, 깊이-와이즈 컨볼루션 또는 저-순위 인수분해와 같은 커널 타입들) 뿐만 아니라 그것의 매크로-아키텍처[100, 101, 104, 110, 214, 233](예를 들어, 잔차, 또는 개시와 같은 모듈 타입들)의 관점에서 NN 모델 아키텍처를 최적화하는 데 초점을 맞추었다. 여기에서 고전적인 기술은 대부분 수동 검색을 사용하여 새로운 아키텍처 모듈을 찾았는데, 이는 확장성이 없다. 이와 같이, 새로운 작업 라인은 자동 기계 학습(AutoML) 및 신경망 구조 검색(NAS) 방법을 설계하는 것이다. 이들은 모델 크기, 깊이 및/또는 폭의 주어진 제약 하에서 올바른 NN 아키텍처를 자동화된 방식으로 찾는 것을 목표로 한다[161, 194, 232, 245, 252, 291]. NAS 방법에 대한 최근 조사는 관심 있는 독자를 [54]에 참조한다.\n' +
      '\n' +
      'NN 아키텍처와 하드웨어를 함께 공동 설계하는 또 다른 최근 작업 라인은 특정 타겟 하드웨어 플랫폼에 대해 NN 아키텍처를 적응(및 공동 설계)하는 것이었다. 이것의 중요성은 NN 컴포넌트의 오버헤드(레이턴시 및 에너지 측면에서)가 하드웨어 의존적이기 때문이다. 예를 들어, 전용 캐시 계층을 갖는 하드웨어는 이러한 캐시 계층이 없는 하드웨어보다 훨씬 효율적으로 대역폭 제한 연산을 실행할 수 있다. NN 아키텍처 설계와 유사하게, 아키텍처-하드웨어 공동 설계에서의 초기 접근법들은 수동적이었고, 여기서 전문가는 NN 아키텍처를 적응/변경할 것이고[70], 이어서 자동화된 AutoML 및/또는 NAS 기술들을 사용할 것이다[22, 23, 100, 252].\n' +
      '\n' +
      'NN들의 메모리 풋프린트 및 계산 비용을 감소시키기 위한 또 다른 접근법을 프루닝하는 것은 프루닝을 적용하는 것이다. 가지치기에서 _saliency_ (민감도)가 작은 뉴런이 제거되어 희박한 계산 그래프가 생성된다. 여기서, 현저성이 작은 뉴런은 제거가 모델 출력/손실 함수에 최소로 영향을 미치는 뉴런이다. 프루닝 방법은 크게 비구조적 프루닝[49, 86, 139, 143, 191, 257]과 구조적 프루닝[91, 106, 156, 166, 274, 275, 279]으로 분류할 수 있다. 구조화되지 않은 가지치기로 뉴런이 발생하는 곳마다 작은 돌출도로 뉴런을 제거한다. 이 접근법을 사용하면 모델의 일반화 성능에 거의 영향을 미치지 않으면서 대부분의 NN 매개변수를 제거하는 공격적인 가지치기를 수행할 수 있다. 그러나, 이러한 접근법은 가속하기 어려운 것으로 알려져 있고, 전형적으로 메모리-바운드인 희소 행렬 연산으로 이어진다[21, 66]. 반면에, 구조화된 프루닝으로, 파라미터들의 그룹(예를 들어, 전체 컨볼루션 필터들)이 제거된다. 이는 레이어 및 가중치 행렬의 입력 및 출력 형상을 변화시키는 효과를 가지며, 따라서 여전히 조밀한 행렬 연산을 허용한다. 그러나 공격적으로 구조화된 가지치기는 종종 상당한 정확도 저하를 초래한다. 첨단 성능을 유지하면서 높은 수준의 가지치기/희소성을 가진 훈련과 추론은 여전히 열린 문제로 남아 있다[16]. 우리는 가지치기/희소성의 관련 작업에 대한 철저한 조사를 위해 관심 있는 독자에게 [66, 96, 134]를 참조한다.\n' +
      '\n' +
      '지식 증류 모델 증류[3, 150, 177, 195, 207, 269, 270]는 큰 모델을 훈련시킨 후 이를 교사로 사용하여 보다 컴팩트한 모델을 훈련시키는 것을 포함한다. 학생 모델의 훈련 중에 "하드" 클래스 레이블을 사용하는 대신 모델 증류의 핵심 아이디어는 교사가 생성한 "소프트" 확률을 활용하는 것인데, 이러한 확률은 입력에 대한 더 많은 정보를 포함할 수 있기 때문이다. 증류에 대한 많은 작업에도 불구하고, 여기서 주요 과제는 증류만으로 높은 압축비를 달성하는 것이다. 지식증류법은 INT8과 낮은 정밀도로 \\(\\geq 4\\times\\) 압축으로 성능을 유지할 수 있는 양자화 및 프루닝에 비해 공격적 압축으로 무시할 수 없는 정확도 저하를 보이는 경향이 있다. 그러나, 지식 증류와 이전 방법(즉, 양자화 및 가지치기)의 조합은 큰 성공을 보여주었다[195].\n' +
      '\n' +
      '마지막으로 양자화는 NN 모델의 훈련과 추론 모두에서 훌륭하고 일관된 성공을 보인 접근법이다. 수치 표현과 양자화의 문제는 디지털 컴퓨팅만큼 오래되었지만 신경망은 개선의 독특한 기회를 제공한다. 양자화에 대한 이 조사는 대부분 추론에 초점을 맞추고 있지만, 우리는 양자화의 중요한 성공이 NN 훈련에서 이루어졌다는 것을 강조해야 한다[10, 35, 57, 130, 247]. 특히, 반 정밀도 및 혼합 정밀도 훈련의 획기적인 발전[41, 72, 79, 175]은 AI 가속기에서 훨씬 더 높은 처리량을 가능하게 한 주요 드라이버였다. 그러나, 상당한 튜닝 없이 반정밀도 이하로 가는 것은 매우 어렵다는 것이 입증되었고, 최근의 양자화 연구의 대부분은 추론에 초점을 맞추고 있다. 추론을 위한 이 양자화는 이 글의 초점이다.\n' +
      '\n' +
      '양자화 및 신경과학 NN 양자화와 밀접하게 관련되어(그리고 어떤 동기부여에 대해서는) 인간의 뇌가 연속적인 형태가 아닌 이산적/양자화된 형태로 정보를 저장함을 시사하는 신경과학에서의 작업이다[171, 236, 240]. 이 아이디어에 대한 인기 있는 근거는 연속적인 형태로 저장된 정보가 필연적으로 잡음(우리의 뇌를 포함한 물리적 환경에 항상 존재하며 열, 감각, 외부, 시냅스 잡음 등에 의해 유발될 수 있음)에 의해 손상될 것이라는 것이다[27, 58]. 그러나, 이산 신호 표현들은 그러한 저-레벨 노이즈에 더 견고할 수 있다. 이산 표현들[128, 138, 242]의 더 높은 일반화 파워 및 제한된 자원들[241] 하에서 그들의 더 높은 효율을 포함하는 다른 이유들도 제안되었다. 우리는 신경과학 문헌의 관련 작업에 대한 철저한 검토를 위해 독자에게 [228]을 참조한다.\n' +
      '\n' +
      '이 작업의 목표는 양자화에 사용되는 현재 방법과 개념을 소개하고 이 연구 라인에서 현재 도전과 기회를 논의하는 것이다. 그렇게 함으로써, 우리는 가장 관련된 일에 대해 논의하려고 노력했다. 짧은 조사의 페이지 한계에서 NN 양자화만큼 큰 분야의 모든 작업을 논의하는 것은 불가능하며, 우리가 일부 관련 논문을 놓쳤다는 것은 의심의 여지가 없다. 저희가 소홀히 했을 수 있는 논문의 독자와 저자 모두에게 미리 사과드립니다.\n' +
      '\n' +
      '이 조사의 구조 측면에서 우리는 먼저 섹션 II에서 양자화의 간략한 역사를 제공한 다음 섹션 III에서 양자화의 기본 개념을 소개할 것이다. 이러한 기본 개념은 대부분의 양자화 알고리즘과 공유되며, 기존의 방법을 이해하고 배치하는 데 필요하다. 그런 다음 섹션 IV에서 더 발전된 주제에 대해 논의합니다. 이들은 대부분 최근의 최첨단 방법들, 특히 저/혼합-정밀 양자화를 위한 방법들을 포함한다. 그런 다음 에지 프로세서에 특별한 초점을 맞춘 섹션 V의 하드웨어 가속기에서 양자화의 의미에 대해 논의한다. 마지막으로 섹션 VII에서 요약 및 결론을 제공한다.\n' +
      '\n' +
      '## II 양자화의 일반사\n' +
      '\n' +
      '그레이와 노이호프는 1998년까지 양자화의 역사에 대한 아주 좋은 조사를 썼다. 그 기사는 훌륭하고 전체적으로 읽을 가치가 있지만, 독자의 편의를 위해 여기에서 몇 가지 핵심 사항을 간략하게 요약할 것이다. 양자화는 큰(종종 연속) 집합의 입력 값에서 작은(종종 유한) 집합의 출력 값으로 매핑하는 방법으로서 오랜 역사를 가지고 있다. 반올림 및 절단이 대표적인 예입니다. 양자화는 미적분학의 기초와 관련이 있으며, 관련 방법은 1800년대 초(뿐만 아니라 훨씬 이전)에, 예를 들어 대규모(1800년대 초의 표준에 따라) 데이터 분석을 위한 최소 제곱 및 관련 기술에 대한 초기 작업에서 볼 수 있다[225]. 양자화에 대한 초기 작업은 1867년으로 거슬러 올라가는데, 여기서 이산화는 적분의 계산을 근사화하는 데 사용되었으며[206], 이후 1897년에 샤파드가 적분 결과에 대한 반올림 오차의 영향을 조사했다[220]. 최근 디지털 신호 처리에서 양자화는 디지털 형태로 신호를 표현하는 과정에서 일반적으로 반올림뿐만 아니라 수치 해석 및 수치 알고리즘 구현에 있어 중요해지고 있으며, 여기서 실수 값에 대한 계산은 유한 정밀 산술로 구현된다.\n' +
      '\n' +
      '1948년이 되어서야, 디지털 컴퓨터의 등장 무렵, 섀넌이 의사소통의 수학적 이론에 대한 그의 중요한 논문을 썼을 때, 양자화의 효과와 코딩 이론에서의 사용이 공식적으로 제시되었다[215]. 특히 섀넌은 그의 무손실 코딩 이론에서 관심 이벤트들이 비균일 확률을 가질 때, 동일한 비트 수를 사용하는 것은 낭비적이라고 주장했다. 그는 더 최적의 접근법은 사건의 확률에 따라 비트 수를 변화시키는 것이라고 주장했는데, 이 개념은 현재 _가변 속도 양자화_로 알려져 있다. 특히 허프만 코딩은 이 [109]에 의해 동기가 부여된다. 1959년 [216]의 후속 작업에서 섀넌은 벡터 양자화의 개념(섹션 IV-F에서도 간략하게 논의됨)뿐만 아니라 왜곡율 함수(코딩 후 신호 왜곡에 대한 하한을 제공함)를 도입했다. 이 개념은 실제 통신 응용을 위해 [53, 55, 67, 208]에서 확장되어 실용화되었다. 그 기간에서의 신호 처리에서의 양자화에 관한 다른 중요한 역사적 연구는 펄스 코드 변조(PCM) 개념(샘플링된 아날로그 신호들을 근사화/표현화/인코딩하기 위해 제안된 펄싱 방법)을 도입한 [188]과 고해상도 양자화의 고전적인 결과[14]를 포함한다. 우리는 이러한 문제에 대한 자세한 논의를 위해 관심 있는 독자에게 [76]을 참조한다.\n' +
      '\n' +
      '양화는 연속적인 수학적 양과 관련된 문제에 대해 수치 근사를 사용하는 알고리즘에서 약간 다른 방식으로 나타나며, 이 영역은 또한 오랜 역사를 가지고 있지만 디지털 컴퓨터의 등장으로 새로운 관심을 받았다. 수치 해석에서 중요한 개념은 _잘 놓인 문제_-의 개념이었다. 대략적으로, 문제는 잘 놓여있다: 해는 존재한다; 그 해는 유일하다; 그리고 그 해는 어떤 합리적인 토폴로지의 입력 데이터에 연속적으로 의존한다. 그러한 문제들은 때때로 잘 조절된 문제라고 불린다. 주어진 잘 조건화된 문제로 작업할 때에도 어떤 이상화된 의미에서 그 문제를 "정확하게" 해결하는 특정 알고리즘은 반올림 및 절단 오류의 특수성에 의해 도입된 "잡음"이 있는 경우 매우 저조한 성능을 발휘하는 것으로 나타났다. 이러한 반올림 오차는 유한한 수의 비트만으로 실수들을 표현하는 것과 관련이 있다. 예를 들어, IEEE 부동 소수점 표준에 의해 특정된 양자화; 그리고 반복 알고리즘의 유한한 수의 반복들만이 실제로 수행될 수 있기 때문에 절단 오차들이 발생한다. 후자는 "정확한 산술"에서도 중요한데, 왜냐하면 연속 수학의 대부분의 문제는 원칙적으로 유한한 기본 연산의 수열에 의해 해결될 수 없기 때문이다; 그러나 전자는 양자화와 관련이 있다. 이러한 문제는 알고리즘의 _숫자 안정성_이라는 개념으로 이어졌다. 숫자 알고리즘을 함수 \\(f\\)로 보자. 입력 데이터 \\(x\\)를 "true" 해 \\(y\\)에 매핑하려고 시도하지만 라운드오프 및 절단 오류로 인해 알고리즘의 출력은 실제로 다른 \\(y^{*}\\)이다. 이 경우 알고리즘의 _forward error_는 \\(\\Delta y=y^{*}-y\\)이고, 알고리즘의 _backward error_는 \\(f(x+\\Delta x)=y^{*}\\)와 같이 가장 작은 \\(\\Delta x\\)이다. 따라서 순방향 오류는 정확하거나 진정한 답과 알고리즘에 의해 출력된 것의 차이를 알려주고, 역방향 오류는 우리가 실행한 알고리즘이 실제로 정확히 푼 입력 데이터를 알려준다. 알고리즘에 대한 순방향 오차와 역방향 오차는 문제의 조건 수에 따라 관련이 있다. 우리는 이러한 문제에 대한 자세한 논의를 위해 관심 있는 독자에게 [237]을 참조한다.\n' +
      '\n' +
      '### _Quantization in Neural Nets_\n' +
      '\n' +
      '의심할 여지 없이 수천 개의 논문이 이러한 주제에 대해 작성되었으며, NN 양자화에 대한 최근 작업이 이전 작업과 어떻게 다른지 궁금할 수 있습니다. 확실히, 최근에 제안된 많은 "새로운 알고리즘"은 문헌에서 과거 작업과 강한 연결(그리고 어떤 경우에는 본질적으로 재발견)을 가지고 있다. 그러나, NN은 양자화의 문제에 독특한 도전과 기회를 가져온다. 첫째, 신경망의 추론과 훈련은 모두 계산 집약적이다. 따라서 수치의 효율적인 표현이 특히 중요하다. 둘째, 현재 대부분의 신경망 모델은 과도하게 매개변수화되어 있으므로 정확도에 영향을 미치지 않으면서 비트 정밀도를 줄일 수 있는 충분한 기회가 있다. 그러나, 한 가지 매우 중요한 차이점은 NN이 공격적인 양자화 및 극단적인 이산화에 매우 견고하다는 것이다. 여기서의 새로운 자유도는 관련된 매개변수의 수, 즉 과도하게 매개변수화된 모델로 작업하는 것과 관련이 있다. 이는 우리가 잘 정리된 문제를 해결하고 있는지, 순방향 오류에 관심이 있는지, 역방향 오류에 관심이 있는지 등에 직접적인 시사점을 준다. 양자화의 최근 발전을 추동하는 NN 응용들에서, 해결되고 있는 단일의 잘-포지셔닝되거나 잘-조건화된 문제는 없다. 대신에, 어떤 종류의 순방향 에러 메트릭(분류 품질, 복잡성 등에 기초함)에 관심이 있지만, 오버-파라미터화로 인해, 이 메트릭을 정확히 또는 근사적으로 최적화하는 매우 상이한 모델들이 많이 존재한다. 따라서, 양자화된 모델과 원래의 비양자화된 모델 사이의 높은 오차/거리를 가질 수 있지만, 여전히 매우 양호한 일반화 성능을 달성할 수 있다. 이 추가된 자유도는 신호를 너무 많이 변경하지 않는 압축 방법을 찾는 데 주로 초점을 맞춘 많은 고전적 연구 또는 "정확한" 대 "이산된" 계산 간의 차이에 대한 강력한 제어가 있는 수치적 방법에는 존재하지 않았다. 이 관찰은 NN 양자화를 위한 새로운 기술을 연구하는 주요 동인이었다. 마지막으로, 신경망 모델의 계층 구조는 탐색할 수 있는 추가 차원을 제공한다. 신경망의 다른 레이어는 손실 함수에 다른 영향을 미치며, 이는 양자화에 대한 혼합 정밀도 접근법의 동기를 부여한다.\n' +
      '\n' +
      '## III 양자화의 기본 개념\n' +
      '\n' +
      '이 절에서는 먼저 III-A절에서 공통 표기법과 문제 설정을 간략하게 소개하고, III-B-III-F절에서 기본 양자화 개념과 방법을 설명한다. 이후 섹션 III-G에서 다양한 미세 조정 방법에 대해 논의한 다음 섹션 III-H에서 확률적 양자화에 대해 논의한다.\n' +
      '\n' +
      '### _문제 설정 및 Notations_\n' +
      '\n' +
      'NN이 학습 가능한 매개변수 \\(\\{W_{1},W_{2},...,W_{L}\\}\\로 표시된 \\(L\\) 레이어를 가지고 있으며, \\(\\theta\\)는 이러한 모든 매개변수의 조합을 나타낸다. 일반성의 손실 없이, 우리는 명목상의 목표가 다음과 같은 경험적 위험 최소화 함수를 최적화하는 것인 지도 학습 문제에 초점을 맞춘다:\n' +
      '\n' +
      '\\[\\mathcal{L}(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N}l(x_{i},y_{i};\\theta), \\tag{1}\\]\n' +
      '\n' +
      '여기서, \\((x,y)\\)는 입력 데이터 및 대응하는 라벨이고, \\(l(x,y;\\theta)\\)는 손실 함수(예를 들어, 평균 제곱 오차 또는 교차 엔트로피 손실), 및 \\(N\\)는 총 데이터 포인트의 수이다. 또한 \\(i^{th}\\) 층의 입력 숨김 활성화를 \\(h_{i}\\)로 표시하고 해당 출력 숨김 활성화를 \\(a_{i}\\)로 표시한다. 우리는 학습된 모델 파라미터 \\(\\theta\\)가 부동 소수점 정밀도로 저장되어 있다고 가정한다. 양자화 과정에서 두 파라미터(\\(\\theta\\))와 중간 활성화 맵(\\(h_{i},~{}a_{i}\\))의 정밀도를 낮은 정밀도로 줄이는 것이 목표이며, 모델의 일반화 파워/정확도에 최소한의 영향을 미친다. 이를 위해서는 부동 소수점 값을 양자화된 값에 매핑하는 양자화 연산자를 정의해야 하는데, 다음에 설명한다.\n' +
      '\n' +
      '### _Uniform Quantization_\n' +
      '\n' +
      '우리는 먼저 유한한 값 집합에 대한 NN 가중치와 활성화를 양자화할 수 있는 함수를 정의해야 한다. 이거.\n' +
      '\n' +
      '도 1: 균일 양자화(왼쪽)와 비-균일 양자화(오른쪽)의 비교. 연속영역 \\(r\\)의 실수값은 양자화된 영역 \\(Q\\)에서 이산적이고 낮은 정밀도 값으로 매핑되며, 이 값은 주황색 글렛으로 표시된다. 양자화된 값들(양자화 레벨들) 사이의 거리들은 균일 양자화에 있어서 동일한 반면, 이들은 비-균일 양자화에 있어서 변할 수 있다는 것에 유의한다.\n' +
      '\n' +
      '함수는 부동 소수점에서 실수 값을 취하고, 이를 도 1에 예시된 바와 같이 더 낮은 정밀도 범위로 맵핑한다. 양자화 함수에 대한 인기 있는 선택은 다음과 같다:\n' +
      '\n' +
      '\\[Q(r)=\\text{Int}\\big{(}r/S\\big{)}-Z, \\tag{2}\\]\n' +
      '\n' +
      '여기서, \\(Q\\)는 양자화 연산자, \\(r\\)는 실수값 입력(활성화 또는 가중치), \\(S\\)는 실수값 스케일링 팩터, 및 \\(Z\\)는 정수 영점이다. 나아가, Int 함수는 반올림 연산(예를 들어, 가장 가까운 것으로 반올림 및 잘림)을 통해 실수 값을 정수 값으로 매핑한다. 본질적으로 이 함수는 실수 값 \\(r\\)에서 일부 정수 값으로 매핑됩니다. 이러한 양자화의 방법은 또한 _균일한 양자화_로 알려져 있는데, 그 결과 양자화된 값들(일명 양자화 레벨들)이 균일하게 이격되기 때문이다(도 1, 좌측). 양자화된 값들이 반드시 균일하게 이격되지 않는 _비균일 양자화_ 방법들이 또한 존재하며(도 1, 우측), 이러한 방법들은 섹션 III-F에서 더 상세히 논의될 것이다. 자주 _역양자화_라고 하는 연산을 통해 양자화된 값 \\(Q(r)\\)에서 실수 값 \\(r\\)을 복구할 수 있습니다.\n' +
      '\n' +
      '\\[\\tilde{r}=S(Q(r)+Z). \\tag{3}\\]\n' +
      '\n' +
      '복원된 실제 값 \\(\\tilde{r}\\)은 반올림 작업으로 인해 \\(r\\)과 정확히 일치하지 않습니다.\n' +
      '\n' +
      '### _대칭 및 비대칭 양자화_\n' +
      '\n' +
      '균일한 양자화에 있어서 하나의 중요한 인자는 Eq. 2에서 스케일링 인자 \\(S\\)의 선택이다. 이 스케일링 인자는 본질적으로 주어진 범위의 실수 값 \\(r\\)을 다수의 파티션으로 분할한다([133, 113]에서 논의된 바와 같이):\n' +
      '\n' +
      '\\[S=\\frac{\\beta-\\alpha}{2^{b}-1}, \\tag{4}\\]\n' +
      '\n' +
      '여기서, \\([\\alpha,\\beta]\\)는 실제 값을 클리핑하는 경계 범위인 클리핑 범위를 나타내고, \\(b\\)는 양자화 비트 폭을 나타낸다. 따라서 스케일링 팩터가 정의되기 위해서는 먼저 클리핑 범위 \\([\\alpha,\\beta]\\)가 결정되어야 한다. 클리핑 범위를 선택하는 프로세스를 종종 _보정_ 이라고 합니다. 간단한 선택은 클리핑 범위에 대한 신호의 최소/최대, 즉 \\(\\alpha=r_{min}\\) 및 \\(\\beta=r_{max}\\)를 사용하는 것이다. 이 접근법은 그림 2 (오른쪽)에 예시된 바와 같이 클리핑 범위가 원점, 즉 \\(-\\alpha\\neq\\beta\\에 대해 반드시 대칭인 것은 아니기 때문에 _비대칭 양자화_ 방식이다. 또한 \\(\\alpha=-\\beta\\)의 대칭 클리핑 범위를 선택하여 _대칭 양자화_ 방식을 사용할 수도 있습니다. 일반적인 선택은 신호의 최소/최대 값에 기초하여 이들을 선택하는 것이다: \\(-\\alpha=\\beta=\\max(|r_{max}|,|r_{min}|)\\). 비대칭 양자화는 종종 대칭 양자화에 비해 더 조밀한 클리핑 범위를 초래한다. 이는 목표 가중치 또는 활성화가 불균형할 때 특히 중요하며, 예를 들어 항상 음수가 아닌 값을 갖는 ReLU 이후의 활성화이다. 그러나 대칭 양자화를 사용하면 식에서 양자화 기능이 단순화된다. 2 by replace the zero point with \\(Z=0\\):\n' +
      '\n' +
      '\\[Q(r)=\\text{Int}\\left(\\frac{r}{S}\\right). \\tag{5}\\]\n' +
      '\n' +
      '여기서, 스케일링 팩터에 대한 두 가지 선택이 있다. "전체 범위"에서 대칭 양자화 S는 \\(\\frac{2max(|r|)}{2^{n}-1}\\)(바닥 라운딩 모드와 함께)로 선택되어 [-128,127]의 전체 INTS 범위를 사용한다. 그러나 "제한된 범위"에서 S는 [-127,127]의 범위만 사용하는 \\(\\frac{max(|r|)}{2^{n-1}-1}\\)로 선택된다. 예상대로 전체 범위 접근법이 더 정확합니다. 대칭 양자화는 제로 포인트를 제로 아웃하는 것이 추론 동안 계산 비용을 감소시킬 수 있기 때문에 가중치 양자화를 위해 실제로 널리 채택되고 있다[255]. 또한 구현이 더 간단해진다. 그러나, 비대칭 활성화에서 오프셋으로 인해 점유하는 교차 항이 정적 데이터 독립 항인 활성화를 위해\n' +
      '\n' +
      '도 2: 대칭 양자화와 비대칭 양자화의 일러스트레이션. 제한된 범위를 갖는 대칭 양자화는 실수 값을 [-127, 127]로 매핑하고, 전체 범위는 8비트 양자화를 위해 [-128, 127]로 매핑한다.\n' +
      '\n' +
      '바이어스에서 흡수될 수 있고(또는 누산기를 초기화하는 데 사용될 수 있음)(15).\n' +
      '\n' +
      '대칭 및 비대칭 양자화를 위해 신호의 최소/최대를 사용하는 것이 인기 있는 방법이다. 그러나 이 접근법은 활성화의 이상치 데이터에 취약하다. 이들은 불필요하게 범위를 증가시킬 수 있고, 결과적으로 양자화의 해상도를 감소시킬 수 있다. 이를 해결하기 위한 한 가지 접근법은 신호 [172]의 min/max 대신 백분위수를 사용하는 것이다. 즉, 가장 큰/가장 작은 값 대신 i번째 가장 큰/가장 작은 값을 \\(\\beta\\)/\\(\\alpha\\)로 사용한다. 또 다른 접근법은 \\(\\alpha\\) 및 \\(\\beta\\)를 선택하여 실제 값과 양자화된 값 사이의 KL 발산(즉, 정보 손실)을 최소화하는 것이다[176]. 다양한 모델에 대해 다양한 보정 방법이 평가되는 [255]에 관심 있는 독자를 참조한다.\n' +
      '\n' +
      '**요약(대칭 양자화 대 비대칭 양자화).** 대칭 양자화는 대칭 범위를 사용하여 클리핑을 분할합니다. 이것은 식 2에서 \\(Z=0\\)로 이어지기 때문에 구현이 더 쉽다는 장점이 있다. 그러나 범위가 대칭적이지 않고 치우칠 수 있는 경우에 대해서는 차선책이다. 그러한 경우들에 대해, 비대칭 양자화가 바람직하다.\n' +
      '\n' +
      '### _Range Calibration Algorithms: 정적 vs 동적 양자화_\n' +
      '\n' +
      '지금까지 우리는 \\([\\alpha,\\beta]\\)의 클리핑 범위를 결정하기 위한 다양한 교정 방법에 대해 논의했다. 양자화 방법의 또 다른 중요한 미분기는 클리핑 범위가 결정되는 경우입니다. 이 범위는 대부분의 경우 모수가 추론 중에 고정되므로 가중치에 대해 정적으로 계산할 수 있다. 그러나 활성화 맵은 식 1의 입력 샘플(\\(x\\))마다 다르다. 이와 같이 활성화를 양자화하는 방법에는 동적 양자화와 정적 양자화의 두 가지 방법이 있다.\n' +
      '\n' +
      '동적 양자화에서 이 범위는 런타임 동안 각 활성화 맵에 대해 _동적으로_ 계산됩니다. 이 방법은 매우 높은 오버헤드를 가질 수 있는 신호 통계(최소, 최대, 백분위수 등)의 실시간 계산을 요구한다. 그러나, 동적 양자화는 종종 신호 범위가 각각의 입력에 대해 정확하게 계산됨에 따라 더 높은 정확도를 초래한다.\n' +
      '\n' +
      '또 다른 양자화 접근법은 정적 양자화이며, 여기서 클리핑 범위는 추론 동안 미리 계산되고 _정적_이다. 이 접근법은 계산 오버헤드를 추가하지 않지만, 일반적으로 동적 양자화에 비해 낮은 정확도를 초래한다. 사전 계산을 위한 한 가지 인기 있는 방법은\n' +
      '\n' +
      '도 3: 상이한 양자화 입도의 예시. 계층별 양자화는 동일한 계층에 속하는 모든 필터에 동일한 클리핑 범위가 적용된다. 이는 좁은 분포를 갖는 채널들에 대한 불량 양자화 해상도를 초래할 수 있다(예를 들어, 도면의 필터 1). 다른 클리핑 범위를 다른 채널에 전용하는 채널별 양자화를 사용하여 더 나은 양자화 해상도를 달성할 수 있다.\n' +
      '\n' +
      '활성화들의 전형적인 범위를 계산하기 위한 교정 입력들의 시리즈[113, 267]. 원래의 미양자화된 가중치 분포와 대응하는 양자화된 값들 사이의 MSE(Mean Squared Error)를 최소화하는 것을 포함하여 최상의 범위를 찾기 위해 다수의 상이한 메트릭들이 제안되었다[40, 221, 229, 281]. MSE가 가장 일반적으로 사용되는 방법이지만 엔트로피 [189]와 같은 다른 메트릭을 사용하는 것도 고려할 수 있다. 또 다른 접근법은 NN 트레이닝 동안 이러한 클리핑 범위를 학습/부여하는 것이다[36, 276, 146, 277]. 여기에서 주목할 만한 작업은 훈련 중에 클리핑 범위와 NN의 가중치를 공동으로 최적화하는 LQNets[276], PACT[36], LSQ[56], LSQ+[15]이다.\n' +
      '\n' +
      '**요약(동적 대 정적 양자화).** 동적 양자화는 각 활성화의 클리핑 범위를 동적으로 계산하며 종종 가장 높은 정확도를 달성합니다. 그러나, 신호의 범위를 동적으로 계산하는 것은 매우 고가이며, 따라서, 실무자들은 클리핑 범위가 모든 입력들에 대해 고정된 정적 양자화를 가장 자주 사용한다.\n' +
      '\n' +
      '### _Quantization Granularity_\n' +
      '\n' +
      '대부분의 컴퓨터 비전 작업에서, 도 3에 예시된 바와 같이, 레이어에 대한 활성화 입력은 많은 상이한 컨볼루션 필터들과 컨벌루션된다. 이들 컨볼루션 필터들 각각은 상이한 범위의 값들을 가질 수 있다. 이와 같이 양자화 방법에 대한 하나의 미분기는 가중치에 대해 클리핑 범위 \\([\\alpha,\\beta]\\)를 계산하는 방법의 세분성이다. 우리는 그것들을 다음과 같이 분류했다.\n' +
      '\n' +
      'Layerwise Quantization 이 접근법에서, 클리핑 범위는 도 3의 세 번째 열에 도시된 바와 같이, 계층 [133]의 컨볼루션 필터들 내의 모든 가중치들을 고려함으로써 결정된다. 여기서, 하나는 그 계층 내의 전체 파라미터들의 통계(예를 들어, min, max, 백분위수 등)를 조사한 다음, 모든 컨볼루션 필터들에 대해 동일한 클리핑 범위를 사용한다. 이 접근법은 구현하기가 매우 간단하지만, 각 컨볼루션 필터의 범위가 많이 변할 수 있기 때문에 종종 차선 정확도를 초래한다. 예를 들어, 상대적으로 더 좁은 범위의 파라미터를 갖는 컨볼루션 커널은 더 넓은 범위를 갖는 동일한 레이어의 다른 커널로 인해 양자화 해상도를 잃을 수 있다.\n' +
      '\n' +
      'Groupwise QuantizationOne은 (활성화 또는 컨볼루션 커널 중 하나의) 클리핑 범위를 계산하기 위해 레이어 내부에 다수의 상이한 채널들을 그룹화할 수 있다. 이는 단일 컨볼루션/활성화에 걸친 파라미터의 분포가 많이 변하는 경우에 도움이 될 수 있다. 예를 들어, 이 접근법은 완전 연결 주의 계층으로 구성된 트랜스포머 [243] 모델을 양자화하는 데 Q-BERT [219]에서 유용한 것으로 밝혀졌다. 그러나 이 접근 방식은 필연적으로 다양한 스케일링 요인을 고려해야 하는 추가 비용이 수반된다.\n' +
      '\n' +
      'Channelwise QuantizationA 클리핑 범위의 인기 있는 선택은 도 3의 마지막 열에 도시된 바와 같이, 다른 채널들 [105, 113, 133, 222, 276, 285]과 무관하게, 각각의 컨볼루션 필터에 대해 고정된 값을 사용하는 것이다. 즉, 각각의 채널에는 전용 스케일링 팩터가 할당된다. 이것은 더 나은 양자화 해상도를 보장하고 종종 더 높은 정확도를 초래한다.\n' +
      '\n' +
      'Sub-channelwise Quantization 이전 접근법은 컨볼루션 또는 완전 연결 계층에서 임의의 파라미터 그룹에 대해 클리핑 범위가 결정되는 극단적인 것으로 간주될 수 있다. 그러나, 단일 컨볼루션 또는 풀-연결된 층을 프로세싱할 때 상이한 스케일링 팩터들이 고려될 필요가 있기 때문에, 이 접근법은 상당한 오버헤드를 추가할 수 있다. 따라서, 그룹별 양자화는 양자화 해상도와 계산 오버헤드 사이에 좋은 타협점을 설정할 수 있다.\n' +
      '\n' +
      '**요약(양자화 과립성).** 채널별 양자화는 현재 컨볼루션 커널을 양자화하는 데 사용되는 표준 방법입니다. 이는 실무자가 무시할 수 있는 오버헤드로 각각의 개별 커널에 대한 클리핑 범위를 조정할 수 있게 한다. 대조적으로, 서브-채널와이즈 양자화는 상당한 오버헤드를 초래할 수 있고, 현재 표준 선택이 아니다(또한, 이러한 설계 선택들과 연관된 트레이드오프들에 대해 관심 있는 판독기를 [68]에 참조한다).\n' +
      '\n' +
      '### _Non-Uniform Quantization_\n' +
      '\n' +
      '문헌의 일부 작업은 또한 비균일 양자화를 탐색하였다[25, 38, 62, 74, 79, 99, 118, 125, 153, 159, 179, 189, 190, 238, 248, 256, 264, 266, 276, 284], 여기서 양자화 단계들뿐만 아니라 양자화 레벨들이 비균일하게 이격되도록 허용된다. 불균일 양자화의 형식적 정의는 Eq. 도 6에서, \\(X_{i}\\)는 이산 양자화 레벨들을 나타내고 \\(\\Delta_{i}\\) 양자화 단계들(임계값들):\n' +
      '\n' +
      '\\[Q(r)=X_{i},\\ \\mathrm{if}\\ r\\in[\\Delta_{i},\\Delta_{i+1}). \\tag{6}\\]\n' +
      '\n' +
      '구체적으로 양자화 단계 \\(\\Delta_{i}\\)와 \\(\\Delta_{i+1}\\) 사이에 실수 \\(r\\)의 값이 떨어지면 양자화기 \\(Q\\)는 해당 양자화 레벨 \\(X_{i}\\)로 투영한다. \\(X_{i}\\)\'s도 \\(\\Delta_{i}\\)\'s도 균일하게 이격되어 있지 않다는 점에 유의한다.\n' +
      '\n' +
      '비균일 양자화는 고정된 비트-폭에 대해 더 높은 정확도를 달성할 수 있는데, 그 이유는 중요한 값 영역들에 더 집중하거나 적절한 동적 범위들을 발견함으로써 분포들을 더 잘 포착할 수 있기 때문이다. 예를 들어, 가중치의 종 모양의 분포를 위해 많은 비균일 양자화 방법이 설계되었고 활성화는 종종 긴 꼬리를 포함한다[12, 25, 61, 147, 179, 12]. 전형적인 규칙-기반 비균일 양자화는 로그 분포[179, 283]를 사용하는 것이며, 여기서 양자화 단계들 및 레벨들은 선형 대신에 지수적으로 증가한다. 또 다른 인기 있는 분기는 이진 코드 기반_양자화 [78, 107, 276, 118, 258]인데, 여기서 실수 벡터 \\(\\mathbf{r}\\in\\mathbb{R}^{n}\\)는 스케일링 계수 \\(\\alpha_{i}\\in\\mathbb{R}\\)와 이진 벡터 \\(\\mathbf{b}_{i}\\in\\{-1,+1\\}^{n}\\)를 사용하여 \\(\\mathbf{r}\\approx\\sum_{i=1}^{m}\\alpha_{i}\\mathbf{b}_{i}\\)를 나타내어 \\(m\\) 이진 벡터로 양자화된다. 기존 연구에서는 \\(\\mathbf{r}\\)와 \\(\\sum_{i=1}^{m}\\alpha_{i}\\mathbf{b}_{i}\\) 사이의 오차를 최소화하기 위한 closed-form 해법이 없기 때문에 휴리스틱 해법에 의존하고 있다. 양자화기를 더욱 개선하기 위해, 보다 최근의 작업[234, 258, 78]은 최적화 문제로서 불균일 양자화를 공식화한다. 식 에 나타낸 바와 같이. 도 7을 참조하면, 양자화기 \\(Q\\) 내의 양자화 단계들/레벨들은 원래의 텐서와 양자화된 대응물의 차이를 최소화하기 위해 조정된다.\n' +
      '\n' +
      '\\[\\min_{Q}\\|Q(r)-r\\|^{2} \\tag{7}\\]\n' +
      '\n' +
      '나아가, 양자화기 자체는 또한 모델 파라미터들과 공동으로 트레이닝될 수 있다. 이들 방법들은 학습가능한 양자화기들로 지칭되고, 양자화 단계들/레벨들은 일반적으로 반복 최적화[258, 276] 또는 경사 하강[125, 264, 158]으로 트레이닝된다.\n' +
      '\n' +
      '규칙 기반 및 최적화 기반 비균일 양자화에 더하여, 클러스터링은 또한 양자화로 인한 정보 손실을 경감시키는데 유익할 수 있다. 일부 작업[256, 74]은 양자화 단계 및 레벨을 결정하기 위해 상이한 텐서 상의 k-평균을 사용하는 반면, 다른 작업[38]은 성능 손실을 최소화하기 위해 가중치에 헤시안-가중 k-평균 클러스터링을 적용한다. 추가 논의는 섹션 IV-F에서 찾을 수 있다.\n' +
      '\n' +
      '**요약(Uniform vs Non-uniform Quantization).** 일반적으로 비균일 양자화를 사용하면 비트를 할당하고 매개 변수의 범위를 불균일하게 이산화하여 신호 정보를 더 잘 캡처할 수 있습니다. 그러나, 불균일한 양자화 방식들은 일반적으로 일반적인 계산 하드웨어, 예를 들어 GPU 및 CPU 상에서 효율적으로 배치되기 어렵다. 이와 같이, 균일 양자화는 그것의 단순성 및 하드웨어로의 효율적인 매핑으로 인해 현재 사실적인 방법이다.\n' +
      '\n' +
      '### _Fine-tuning Methods_\n' +
      '\n' +
      '양자화 후에 NN에서 파라미터들을 조정하는 것이 종종 필요하다. 이는 모델을 재트레이닝함으로써 수행될 수 있고, QAT(Quantization-Aware Training)라고 불리는 프로세스, 또는 PTQ(Post-Training Quantization)라고 종종 지칭되는 프로세스, 재트레이닝 없이 수행될 수 있다. 이 두 접근법 사이의 개략적인 비교가 그림 4에 예시되어 있으며, 아래에서 더 논의된다(이 주제에 대한 보다 상세한 논의를 위해 관심 있는 독자를 [183]에 참조한다).\n' +
      '\n' +
      '#### Iv-G1 양자화 인식 교육\n' +
      '\n' +
      '훈련된 모델이 주어지면, 양자화는 훈련된 모델 파라미터들에 섭동을 도입할 수 있고, 이는 부동 소수점 정밀도로 훈련되었을 때 그것이 수렴했던 지점으로부터 모델을 밀어낼 수 있다. 모델이 더 나은 손실을 갖는 포인트로 수렴할 수 있도록 양자화된 파라미터로 NN 모델을 재트레이닝함으로써 이를 해결할 수 있다. 한 가지 일반적인 접근법은 QAT(Quantization-Aware Training)를 사용하는 것인데, 여기서 통상적인 순방향 및 역방향 패스는 부동 소수점에서의 양자화된 모델에 대해 수행되지만, 모델 파라미터들은 각각의 구배 업데이트 후에 양자화된다(투영된 구배 하강과 유사하다). 특히 하는 것이 중요하다.\n' +
      '\n' +
      '그림 4: 양자화 인식 훈련(QAT, Left)과 훈련 후 양자화(PTQ, Right)의 비교. QAT에서는 미리 훈련된 모델을 양자화한 후 훈련 데이터를 사용하여 미세 조정하여 매개변수를 조정하고 정확도 저하를 복구한다. PTQ에서, 미리 트레이닝된 모델은 클리핑 범위들 및 스케일링 팩터들을 계산하기 위해 캘리브레이션 데이터(예를 들어, 트레이닝 데이터의 작은 서브세트)를 사용하여 캘리브레이션된다. 그 다음, 캘리브레이션 결과에 기초하여 모델이 양자화된다. 교정 프로세스는 종종 QAT에 대한 미세 조정 프로세스와 병행하여 수행된다는 점에 유의한다.\n' +
      '\n' +
      '가중치 갱신 후의 이 투영은 부동 소수점 정밀도로 수행된다. 부동점으로 백워드 패스를 수행하는 것은 중요한데, 양자화된 정밀도에서 그라디언트들을 누적하는 것은 특히 저-정밀도에서 높은 오차를 갖는 제로-그라디언트 또는 그라디언트들을 초래할 수 있기 때문이다[107, 159, 186, 204, 231, 42].\n' +
      '\n' +
      '역전파에서 중요한 미묘한 점은 미분 불가능한 양자화 연산자(Eq. 2)가 어떻게 되는가이다. 가 처리된 것을 특징으로 하는 반도체 소자의 제조 방법. 근사치 없이 이 연산자의 기울기는 Eq의 반올림 연산이기 때문에 거의 모든 곳에서 0이다. 도 2는 피스 와이즈 플랫 오퍼레이터이다. 이를 해결하기 위한 일반적인 접근법은 소위 STE(Straight Through Estimator)에 의해 이 연산자의 기울기를 근사화하는 것이다[13]. STE는 본질적으로 반올림 연산을 무시하고 그림 5와 같이 아이덴티티 함수로 근사화한다.\n' +
      '\n' +
      'STE의 대략적인 근사치에도 불구하고, 이진 양자화[8]와 같은 초저-정밀 양자화를 제외하고는 종종 실제로 잘 작동한다. [271]의 연구는 이러한 현상에 대한 이론적 정당성을 제공하며, STE의 거친 구배 근사치가 ( STE의 적절한 선택을 위해) 인구 구배와 상관관계가 있을 수 있음을 발견했다. 역사적 관점에서 STE의 원래 아이디어는 [209, 210]의 정액 작업으로 거슬러 올라갈 수 있으며, 여기서 ID 연산자는 이진 뉴런에서 기울기를 근사화하는 데 사용되었다.\n' +
      '\n' +
      'STE가 주류 접근법이지만[289, 226], 다른 접근법도 문헌에서 탐구되었다[144, 164, 25, 31, 59, 25]. 우리는 먼저 [13]도 STE의 대안으로 확률론적 뉴런 접근법을 제안한다는 것을 언급해야 한다(이것은 섹션 III-H에서 간략하게 논의된다). 조합 최적화[65], 표적 전파[140] 또는 Gumbel-softmax[116]를 사용하는 다른 접근법도 제안되었다. 다른 종류의 대체 방법은 양자화될 가중치를 강제하기 위해 정규화 연산자를 사용하려고 한다. 이것은 식 2에서 미분가능하지 않은 양자화 연산자를 사용할 필요성을 제거한다. 이들은 종종 _Non-STE_ 방법[144, 184, 39, 8, 39, 184, 283]으로 지칭된다. 이 분야의 최근 연구는 양자화 수식 Eq에서 반올림 연산을 제거하는 ProxQuant[8]을 포함한다. 2, 대신 소위 _W자형_, 비평활 정규화 함수를 사용하여 가중치를 양자화된 값에 적용합니다. 다른 주목할만한 연구는 불연속 포인트들의 도함수를 근사화하기 위해 펄스 트레이닝을 사용하는 것[45], 또는 양자화된 가중치들을 부동 포인트 및 양자화된 파라미터들의 아핀 조합으로 대체하는 것[165]을 포함한다. [181]의 최근 연구에서도 라운드 대 최근접 방법의 대안으로 적응적 라운딩 방법인 AdaRound를 제시하고 있다. 이 분야에서 흥미로운 작업에도 불구하고 이러한 방법은 종종 많은 조정을 필요로 하며 지금까지 STE 접근법이 가장 일반적으로 사용되는 방법이다.\n' +
      '\n' +
      '모델 매개 변수를 조정하는 것 외에도 일부 선행 연구에서는 QAT 동안 양자화 매개 변수를 학습하는 것이 효과적이라는 것을 발견했다. PACT[36]은 균일 양자화 하에서 활성화들의 클리핑 범위들을 학습하는 반면, QIT[125]는 또한 비-균일 양자화 설정으로의 확장으로서 양자화 단계들 및 레벨들을 학습한다. LSQ[56]은 QAT 동안 비-음성 활성화들(예를 들어, ReLU)에 대한 스케일링 팩터들을 학습하기 위해 새로운 구배 추정치를 도입하고, LSQ+[15]는 이 아이디어를 swish[202] 및 h-swish[100]과 같은 일반적인 활성화 함수들로 더 확장한다.\n' +
      '\n' +
      '도 5: STE(Straight Through Estimator)의 사용을 포함하는 양자화-인식 훈련 절차의 일러스트레이션.\n' +
      '\n' +
      '음의 값을 생성합니다.\n' +
      '\n' +
      '**요약(QAT).** QAT는 STE의 대략적인 근사치에도 불구하고 작동하는 것으로 나타났습니다. 그러나, QAT의 주요 단점은 NN 모델을 재훈련하는 데 드는 계산 비용이다. 이러한 재-트레이닝은 특히 저-비트 정밀도 양자화를 위해 정확도를 복구하기 위해 수백 에포크에 대해 수행될 필요가 있을 수 있다. 양자화된 모델이 장기간 배포될 것이고, 효율성과 정확성이 특히 중요하다면, 재훈련에 대한 이러한 투자는 가치가 있을 것이다. 그러나 일부 모델은 수명이 상대적으로 짧기 때문에 항상 그런 것은 아닙니다. 다음으로, 우리는 이 오버헤드를 갖지 않는 대안적인 접근법에 대해 논의한다.\n' +
      '\n' +
      '#### Iv-B2 훈련 후 양자화\n' +
      '\n' +
      '고가의 QAT 방법의 대안은 어떠한 미세 조정 없이, 양자화와 가중치의 조정을 수행하는 PTQ(Post-Training Quantization)이다[11, 24, 40, 60, 61, 68, 69, 89, 108, 142, 148, 174, 182, 223, 281]. 이와 같이, PTQ의 오버헤드는 매우 낮고 종종 무시할 수 있다. PTQ는 재학습을 위해 충분한 양의 학습 데이터가 필요한 QAT와 달리 데이터가 제한적이거나 레이블이 지정되지 않은 상황에서 적용할 수 있다는 추가적인 장점이 있다. 그러나, 이것은 특히 저-정밀 양자화의 경우, QAT에 비해 낮은 정확도의 비용으로 종종 온다.\n' +
      '\n' +
      '이러한 이유로, PTQ의 정확도 저하를 완화하기 위한 다중 접근법이 제안되었다. 예를 들어, [11, 63]은 그들의 양자화에 뒤따르는 가중치 값들의 평균 및 분산에서 고유한 바이어스를 관찰하고 바이어스 보정 방법들을 제안하며; 그리고 [174, 182]는 상이한 계층들 또는 채널들 사이에서 가중치 범위들(및 암시적으로 활성화 범위들)을 균등화하는 것이 양자화 에러들을 감소시킬 수 있음을 보여준다. ACIQ [11]은 PTQ에 대한 최적의 클리핑 범위와 채널별 비트폭 설정을 해석적으로 계산한다. ACIQ는 낮은 정확도 저하를 달성할 수 있지만 ACIQ에서 사용되는 채널별 활성화 양자화는 하드웨어에 효율적으로 배치되기 어렵다. 이를 해결하기 위해 OMSE 방법[40]은 활성화 상의 채널별 양자화를 제거하고 양자화된 텐서와 대응하는 부동 소수점 텐서 사이의 L2 거리를 최적화하여 PTQ를 수행하는 것을 제안한다. 또한, 이상치가 PTQ에 미치는 부정적인 영향을 완화하기 위해, 이상치 값이 포함된 채널을 중복 및 반으로 분할하는 이상치 채널 분할(OCS) 방법이 [281]에서 제안된다. 또 다른 주목할 만한 연구로는 AdaRound [181]을 들 수 있는데, 이는 양자화를 위한 순진한 라운드-최근접 방법이 반직관적으로 최적이 아닌 해로 귀결될 수 있음을 보여주며, 손실을 더 잘 줄이는 적응형 라운딩 방법을 제안한다. AdaRound는 양자화된 가중치들의 변화를 전체 정밀도 대응으로부터 \\(\\pm 1\\) 이내로 제한하지만, AdaQuant [108]은 양자화된 가중치들이 필요에 따라 변화하도록 하는 보다 일반적인 방법을 제안한다. PTQ 스킴들은 극단적으로 취할 수 있으며, 여기서 트레이닝 또는 테스트 데이터는 양자화 동안 활용되지 않는다(일명 제로-샷 시나리오들), 다음에 논의된다.\n' +
      '\n' +
      '**요약(PTQ).** PTQ에서 모든 가중치와 활성화 양자화 매개 변수는 NN 모델의 재훈련 없이 결정됩니다. 이와 같이 PTQ는 NN 모델을 양자화하는 매우 빠른 방법이다. 그러나, 이것은 종종 QAT에 비해 낮은 정확도의 비용으로 온다.\n' +
      '\n' +
      '#### Iv-B3 Zero-shot 양자화\n' +
      '\n' +
      '지금까지 논의된 바와 같이, 양자화 후 최소의 정확도 저하를 달성하기 위해, 우리는 훈련 데이터의 일부 전체에 대한 액세스가 필요하다. 먼저, 우리는 값을 클리핑하고 적절한 스케일링 팩터(일반적으로 문헌에서 보정이라고 함)를 결정할 수 있도록 활성화 범위를 알아야 한다. 둘째, 양자화된 모델은 모델 파라미터를 조정하고 정확도 저하를 복구하기 위해 미세 조정이 필요한 경우가 많다. 그러나, 많은 경우들에서, 원래의 트레이닝 데이터에 대한 액세스는 양자화 절차 동안 가능하지 않다. 이는 트레이닝 데이터세트가 너무 커서 배포할 수 없거나, 독점적(예를 들어, 구글의 JFT-300M)이거나, 보안 또는 프라이버시 우려(예를 들어, 의료 데이터)로 인해 민감하기 때문이다. 이 문제를 해결하기 위해 몇 가지 다른 방법이 제안되었으며, 이를 제로 샷 양자화(ZSQ)라고 한다. [182]에 의해 영감을 받아, 여기서는 먼저 두 개의 상이한 레벨의 제로-샷 양자화를 설명한다:\n' +
      '\n' +
      '* **레벨 1:** 데이터 없음 및 미세 조정 없음(ZSQ + PTQ).\n' +
      '* **레벨 2:** 데이터는 없지만 세부 조정(ZSQ + QAT)이 필요합니다.\n' +
      '\n' +
      '레벨 1은 어떠한 미세 조정도 없이 더 빠르고 더 쉬운 양자화를 허용한다. 파인튜닝은 일반적으로 시간이 많이 걸리고 종종 추가 하이퍼파라미터 검색이 필요하다. 그러나 수준 2는 특히 초저비트 정밀도 설정에서 양자화된 모델이 정확도 저하를 복구하는 데 도움이 되기 때문에 일반적으로 더 높은 정확도를 초래한다[85]. [182]의 작업은 가중치 범위를 균등화하고 바이어스 오류를 수정하여 주어진 NN 모델을 데이터나 미세 조정 없이 양자화에 더 적합하도록 하는 레벨 1 접근법을 사용한다. 그러나, 이 방법은 (piece-wise) 선형 활성화 함수의 스케일-등분 특성에 기초하기 때문에, GELU [94] 활성화를 갖는 BERT [46] 또는 스위시 활성화를 갖는 MobileNetV3 [100]과 같은 비선형 활성화를 갖는 NN에 대해 차선이 될 수 있다[203].\n' +
      '\n' +
      'ZSQ에서 인기 있는 연구 분야는 대상 사전 훈련 모델이 훈련된 실제 데이터와 유사한 합성 데이터를 생성하는 것이다. 그런 다음 합성 데이터는 양자화된 모델의 보정 및/또는 미세 조정을 위해 사용된다. 이 분야의 초기 작업[28]은 합성 데이터 생성을 위해 GAN(Generative Adversarial Networks) [75]를 이용한다. 사전 훈련된 모델을 판별기로 사용하여, 그 출력이 판별기에 의해 잘 분류될 수 있도록 생성기를 훈련시킨다. 그런 다음, 발전기로부터 수집된 합성 데이터 샘플들을 사용하여, 양자화된 모델은 완전-정밀 대응물로부터의 지식 증류로 미세조정될 수 있다(더 자세한 내용은 섹션 IV-D 참조). 그러나, 이 방법은 모델의 최종 출력만을 사용하여 생성되기 때문에 실제 데이터의 내부 통계(예를 들어, 중간 레이어 활성화의 분포)를 캡처하는 데 실패한다. 내부 통계를 고려하지 않은 합성 데이터는 실제 데이터 분포를 제대로 나타내지 않을 수 있다[85]. 이를 해결하기 위해, 다수의 후속 노력들은 보다 현실적인 합성 데이터를 생성하기 위해 배치 정규화(Batch Normalization; BatchNorm) [112]에 저장된 통계, 즉 채널별 평균 및 분산을 사용한다. 특히 [85]는 내부 통계의 KL 발산을 직접 최소화하여 데이터를 생성하며, 합성 데이터를 사용하여 양자화된 모델을 보정하고 미세 조정한다. 또한, ZeroQ[24]는 합성 데이터가 교정뿐만 아니라 감도 측정에 사용될 수 있음을 보여줌으로써, 훈련/검증 데이터에 대한 액세스 없이 혼합-정밀 사후 훈련 양자화를 가능하게 한다. ZeroQ는 또한 데이터를 생성할 때 출력 라벨에 의존하지 않기 때문에 ZSQ를 객체 검출 태스크로 확장한다. [85]와 [24] 모두 입력 이미지를 훈련 가능한 매개변수로 설정하고 내부 통계가 실제 데이터와 유사해질 때까지 직접 역전파를 수행한다. 한 걸음 더 나아가, 최근의 연구[37, 90, 259]는 실제 데이터 분포를 더 잘 포착하고 보다 현실적인 합성 데이터를 생성할 수 있는 생성 모델을 훈련하고 활용하는 것이 효과적임을 발견한다.\n' +
      '\n' +
      '**요약(ZSQ).** 제로 샷(일명 데이터 없음) 양자화는 훈련/검증 데이터에 대한 액세스 없이 전체 양자화를 수행합니다. 이는 데이터 세트에 액세스할 필요 없이 고객의 워크로드 배포를 가속화하려는 MLAaS(Machine Learning as a Service) 공급자에게 특히 중요합니다. 더욱이, 이것은 보안 또는 프라이버시 염려가 트레이닝 데이터에 대한 액세스를 제한할 수 있는 경우에 중요하다.\n' +
      '\n' +
      '### _Stochastic Quantization_\n' +
      '\n' +
      '추론 동안, 양자화 방식은 보통 결정적이다. 그러나, 이것이 유일한 가능성은 아니며, 몇몇 연구들은 감소된 정밀도 트레이닝뿐만 아니라 양자화 인식 트레이닝을 위한 확률적 양자화를 탐색하였다[13, 79]. 높은 레벨의 직관은 확률적 양자화가 결정론적 양자화와 비교하여 NN이 더 많이 탐색하도록 허용할 수 있다는 것이었다. 한 가지 인기 있는 지지 주장은 반올림 연산이 항상 동일한 가중치를 반환할 수 있기 때문에 작은 가중치 업데이트가 가중치 변경을 초래하지 않을 수 있다는 것이었다. 그러나 확률적 반올림을 활성화하면 NN이 _탈출_할 수 있는 기회를 제공하여 매개변수를 업데이트할 수 있습니다.\n' +
      '\n' +
      '보다 형식적으로, 확률적 양자화는 연관된 확률로 부동수를 위 또는 아래로 매핑한다.\n' +
      '\n' +
      '도 6: 완전-정밀 추론(Left), 시뮬레이트된 양자화를 갖는 추론(Middle), 및 정수-전용 양자화를 갖는 추론(Right) 간의 비교.\n' +
      '\n' +
      '를 포함하는 것을 특징으로 하는 가중치 업데이트 방법. 예를 들어 [29, 79]의 Int 연산자는 Eq. 2는 다음과 같이 정의된다.\n' +
      '\n' +
      '\\[\\text{Int}(x)=\\begin{cases}\\lfloor x\\rfloor&\\text{with probability }\\lceil x \\rceil-x,\\\\ \\lceil x\\rceil&\\text{with probability }x-\\lfloor x\\rfloor.\\end{cases} \\tag{8}\\]\n' +
      '\n' +
      '그러나, 이 정의는 이진 양자화에 사용될 수 없다. 따라서 [42]는 이를 다음과 같이 확장한다.\n' +
      '\n' +
      '\\[\\text{Binary}(x)=\\begin{cases}-1&\\text{with probability }1-\\sigma(x),\\\\ +1&\\text{with probability }\\sigma(x),\\end{cases} \\tag{9}\\]\n' +
      '\n' +
      '여기서 Binary은 실수값 \\(x\\)을 이진화하는 함수이고, \\(\\sigma(\\cdot)\\)는 시그모이드 함수이다.\n' +
      '\n' +
      '최근에는 QuantNoise[59]에 또 다른 확률적 양자화 방법이 소개되고 있다. 퀀트 노이즈는 각 순방향 패스 동안 가중치의 다른 랜덤 서브세트를 양자화하고 편향되지 않은 기울기로 모델을 훈련시킨다. 이는 많은 컴퓨터 비전 및 자연 언어 처리 모델에서 상당한 정확도 저하 없이 더 낮은 비트 정밀도 양자화를 가능하게 한다. 그러나 확률적 양자화 방법의 주요 과제는 모든 가중치 업데이트에 대해 난수를 생성하는 오버헤드이며, 따라서 실제로 아직 널리 채택되지 않았다.\n' +
      '\n' +
      '## IV 고급 개념: 8비트 이하의 양자화\n' +
      '\n' +
      '이 절에서는 sub-INT8 양자화에 주로 사용되는 양자화의 더 진보된 주제에 대해 논의할 것이다. 우리는 먼저 IV-A 절에서 정수 전용 양자화와 시뮬레이션된 양자화와 그 차이에 대해 논의할 것이다. 이후 IV-B절에서 혼합 정밀 양자화를 위한 다양한 방법에 대해 논의한 후 IV-C절에서 하드웨어 인식 양자화를 수행할 것이다. 그런 다음 증류를 사용하여 섹션 IV-D에서 양자화 정확도를 높일 수 있는 방법을 설명하고 섹션 IV-E에서 극도로 낮은 비트 정밀도 양자화에 대해 논의할 것이다. 마지막으로 IV-F 절에서 벡터 양자화를 위한 다양한 방법에 대해 간략하게 설명한다.\n' +
      '\n' +
      '### _Simulated and Integer-only Quantization_\n' +
      '\n' +
      '양자화된 NN 모델을 배포하는 두 가지 일반적인 접근법, 즉 _시뮬레이션된 양자화_(일명 가짜 양자화) 및 _정수 전용 양자화_(일명 고정 소수점 양자화)가 있다. 시뮬레이션된 양자화에서, 양자화된 모델 파라미터들은 낮은 정밀도로 저장되지만, 연산들(예를 들어, 행렬 곱셈들 및 컨볼루션들)은 부동 소수점 산술로 수행된다. 따라서, 양자화된 파라미터는 도 6(중간)에 개략적으로 도시된 바와 같이 부동 소수점 연산 이전에 역양자화될 필요가 있다. 따라서, 시뮬레이션된 양자화를 갖는 빠르고 효율적인 저-정밀 로직으로부터 완전히 이익을 얻을 수 없다. 그러나, 정수 전용 양자화는 도 6(오른쪽)에 예시된 바와 같이, 모든 연산들이 저-정밀 정수 산술[113, 132, 154, 193, 267]을 사용하여 수행된다. 이것은 임의의 파라미터들 또는 활성화들의 부동 소수점 역양자화 없이, 전체 추론이 효율적인 정수 산술로 수행될 수 있게 한다.\n' +
      '\n' +
      '일반적으로, 부동 소수점 산술로 완전-정밀도로 추론을 수행하는 것은 최종 양자화 정확도에 도움이 될 수 있지만, 이것은 저-정밀도 로직으로부터 이익을 얻을 수 없는 비용으로 온다. 저정밀 로직은 대기 시간, 전력 소비 및 면적 효율성 측면에서 전체 정밀 대응에 비해 여러 이점이 있다. 도 7(좌측)에 도시된 바와 같이, 많은\n' +
      '\n' +
      '그림 7: (왼쪽) 타이탄 RTX 및 A100 GPU 상의 상이한 비트-정밀 로직에 대한 피크 처리량 간의 비교. (오른쪽) 45nm 기술에 대한 상이한 정밀도에 대한 대응하는 에너지 비용 및 상대 면적 비용의 비교[97]. 보시다시피, 낮은 정밀도는 기하급수적으로 더 나은 에너지 효율과 더 높은 처리량을 제공한다.\n' +
      '\n' +
      'NVIDIA V100 및 타이탄 RTX를 포함한 하드웨어 프로세서는 추론 처리량과 대기 시간을 높일 수 있는 저정밀 산술의 빠른 처리를 지원한다. 더욱이, 45nm 기술에 대한 도 7(오른쪽)에 예시된 바와 같이[97], 저-정밀 로직은 에너지 및 면적 면에서 상당히 더 효율적이다. 예를 들어, INT8 덧셈을 수행하는 것은 FP32 덧셈[97]에 비해 \\(30\\times\\) 더 에너지 효율적이며 \\(116\\times\\) 더 면적 효율적이다.\n' +
      '\n' +
      '주목할 만한 정수 전용 양자화 작업으로는 이전 컨볼루션 계층에 배치 정규화를 융합하는 [154]와 배치 정규화가 있는 잔여 네트워크에 대한 정수 전용 계산 방법을 제안하는 [113]이 있다. 그러나 두 방법 모두 ReLU 활성화로 제한된다. [132]의 최근 연구는 GELU[94], Softmax 및 Layer Normalization[6]을 정수 산술로 근사화함으로써 이러한 한계를 해결하고 정수 전용 양자화를 Transformer[243] 아키텍처로 더욱 확장한다.\n' +
      '\n' +
      '_Dyadic 양자화_는 정수 전용 양자화의 또 다른 클래스이며, 여기서 모든 스케일링은 그들의 분자에 정수 값을 갖고 분모에 2의 거듭제곱을 갖는 유리수인 dyadic 수로 수행된다[267]. 이것은 단지 정수 덧셈, 곱셈, 비트 쉬프팅만을 필요로 하지만 정수 나눗셈을 필요로 하지 않는 계산 그래프를 초래한다. 중요한 것은, 이 접근법에서, 모든 추가들(예를 들어, 잔류 연결들)은 동일한 다이아딕 스케일을 갖도록 강제되고, 이는 더 높은 효율로 추가 로직을 더 단순하게 할 수 있다.\n' +
      '\n' +
      '**요약(시뮬레이션 대 정수 전용 양자화)** 일반적으로 정수 전용 및 다이애딕 양자화는 시뮬레이션/페이크 양자화에 비해 더 바람직합니다. 이는 정수만 산술을 위해 더 낮은 정밀도 로직을 사용하는 반면, 시뮬레이션된 양자화는 연산을 수행하기 위해 부동 소수점 로직을 사용하기 때문이다. 그러나, 이것이 가짜 양자화가 결코 유용하지 않다는 것을 의미하지는 않는다. 사실, 가짜 양자화 방법들은 추천 시스템들[185]에서와 같이 컴퓨트-바운드보다는 대역폭-바운드인 문제들에 유익할 수 있다. 이러한 작업의 병목 현상은 메모리 풋프린트와 메모리에서 매개 변수를 로드하는 비용입니다. 그러므로, 가짜 양자화를 수행하는 것은 이러한 경우에 수용될 수 있다.\n' +
      '\n' +
      '### _Mixed-Precision Quantization_\n' +
      '\n' +
      '낮은 정밀도 양자화를 사용하면 하드웨어 성능이 향상된다는 것을 쉽게 알 수 있다. 그러나, 모델을 초저정도로 균일하게 양자화하는 것은 상당한 정확도 저하를 야기할 수 있다. 이를 혼합-정밀 양자화로 해결하는 것이 가능하다[51, 82, 102, 162, 187, 199, 211, 239, 246, 249, 263, 282, 286].\n' +
      '\n' +
      '도 8: 혼합-정밀 양자화의 예시. 혼합-정밀 양자화의 목표는 민감하고 효율적인 계층을 더 높은 정밀도로 유지하는 것이며, 둔감하고 비효율적인 계층에만 저-정밀 양자화를 적용하는 것이다. 효율 메트릭은 하드웨어에 따라 달라지며 대기 시간 또는 에너지 소비일 수 있습니다.\n' +
      '\n' +
      '이 접근법에서, 각각의 계층은 도 8에 예시된 바와 같이 상이한 비트 정밀도로 양자화된다. 이 접근법의 한 가지 과제는 이 비트 설정을 선택하기 위한 탐색 공간이 계층들의 수에서 지수적이라는 것이다. 이 거대한 탐색 공간을 다루기 위해 상이한 접근법들이 제안되었다.\n' +
      '\n' +
      '각 레이어에 대해 이러한 혼합 정밀도를 선택하는 것은 본질적으로 탐색 문제이며, 이를 위해 많은 다른 방법들이 제안되었다. [246]의 최근 연구는 양자화 정책을 자동으로 결정하기 위해 강화 학습(reinforcement learning, RL) 기반 방법을 제안했고, 저자들은 RL 에이전트 피드백에서 하드웨어 가속기의 피드백을 취하기 위해 하드웨어 시뮬레이터를 사용했다. 논문 [254]는 혼합 정밀도 구성 탐색 문제를 신경망 구조 탐색(NAS: Neural Architecture Search) 문제로 정식화하였고, 탐색 공간을 효율적으로 탐색하기 위해 미분 가능 NAS(DNAS: Differentiable NAS) 방법을 사용하였다. 이러한 탐색 기반 방법[246, 254]의 한 가지 단점은 종종 큰 계산 자원을 필요로 하고, 그들의 성능은 전형적으로 하이퍼파라미터 및 심지어 초기화에 민감하다는 것이다.\n' +
      '\n' +
      '혼합-정밀 방법의 또 다른 클래스는 주기적 함수 정규화를 사용하여, 각각의 비트폭을 학습하면서 정확도에 관하여 상이한 층 및 그 변화하는 중요도를 자동으로 구별함으로써 혼합-정밀 모델을 트레이닝한다[184].\n' +
      '\n' +
      '이러한 탐색 및 정규화 기반 접근법과 달리 HAWQ [51]은 모델의 2차 민감도에 기반한 혼합 정밀도 설정을 자동으로 찾는 방법을 소개한다. 이론적으로 2차 연산자(즉, 헤시안)의 흔적이 최적 뇌 손상의 정액 작업에서 가지치기에 대한 결과와 유사하게 양자화에 대한 계층의 감도를 측정하는 데 사용될 수 있음을 보여주었다[50]. HAWQv2에서, 이 방법은 혼합-정밀 활성화 양자화[50]로 확장되었고, RL 기반 혼합-정밀 방법들[246]보다 100배 이상 빠른 것으로 나타났다. 최근에는 HAWQv3에서 정수 전용 하드웨어 인식 양자화를 도입하여 주어진 애플리케이션 특정 제약 조건(예: 모델 크기 또는 대기 시간)에 대한 최적의 비트 정밀도를 찾기 위해 고속 정수 선형 프로그래밍 방법을 제안했다[267]. 이 연구는 또한 혼합-정밀 양자화의 하드웨어 효율성에 대한 일반적인 질문을 T4 GPU에 직접 배치하여 INT8 양자화에 비해 혼합-정밀(INT4/INT8) 양자화로 최대 50% 속도를 보여준다.\n' +
      '\n' +
      '**요약(혼합 정밀도 양자화).** 혼합 정밀도 양자화는 서로 다른 NN 모델의 낮은 정밀도 양자화에 효과적이고 하드웨어 효율적인 방법임이 입증되었습니다. 이 접근법에서, NN의 계층들은 양자화에 민감한/민감한 것으로 그룹화되고, 상위/하위 비트가 각각의 계층에 대해 사용된다. 이와 같이, 정확도 저하를 최소화할 수 있고, 낮은 정밀도 양자화로 감소된 메모리 풋프린트와 더 빠른 속도로부터 여전히 이익을 얻을 수 있다. 최근 작업[267]은 또한 혼합 정밀도가 연산/계층에 걸쳐만 사용되기 때문에 이 접근법이 하드웨어 효율적이라는 것을 보여주었다.\n' +
      '\n' +
      '### _하드웨어 Aware Quantization_\n' +
      '\n' +
      '양자화의 목표 중 하나는 추론 지연 시간을 개선하는 것이다. 그러나, 어떤 계층/동작이 양자화된 후에 모든 하드웨어가 동일한 속도를 제공하는 것은 아니다. 사실, 양자화의 이점은 하드웨어 의존적이며, 온-칩 메모리, 대역폭, 및 캐시 계층 구조와 같은 많은 요인이 양자화 속도 증가에 영향을 미친다.\n' +
      '\n' +
      '하드웨어 인식 양자화를 통해 최적의 이익을 달성하기 위해 이 사실을 고려하는 것이 중요하다[87, 91, 246, 250, 254, 265, 267, 266]. 특히, 작업[246]은 강화 학습 에이전트를 사용하여, 상이한 비트폭을 갖는 상이한 레이어에 대한 레이턴시의 룩-업 테이블에 기초하여, 양자화를 위한 하드웨어 인식 혼합-정밀 설정을 결정한다. 그러나, 이 접근법은 시뮬레이션된 하드웨어 대기 시간을 사용한다. 이를 해결하기 위해 [267]의 최근 작업은 하드웨어에서 양자화된 연산을 직접 전개하고, 상이한 양자화 비트 정밀도에 대한 각 계층의 실제 전개 대기 시간을 측정한다.\n' +
      '\n' +
      '### _Distillation-Assisted Quantization_\n' +
      '\n' +
      '양자화에서 흥미로운 작업 라인은 모델 증류를 통합하여 양자화 정확도를 높이는 것이다[126, 177, 195, 267]. 모델 증류[3, 95, 150, 177, 195, 207, 268, 270, 289]는 보다 정확도가 높은 대형 모델을 교사로 사용하여 컴팩트한 학생 모델의 훈련을 돕는 방법이다. 학생 모델의 훈련 동안 지상 진리 수업 레이블만 사용하는 대신 모델 증류는 교사가 생성한 소프트 확률을 활용하도록 제안하며, 이는 입력의 더 많은 정보를 포함할 수 있다. 즉, 전체 손실 함수는 학생 손실 및 증류 손실 모두를 통합하며, 이는 전형적으로 다음과 같이 공식화된다:\n' +
      '\n' +
      '\\[\\mathcal{L}=\\alpha\\mathcal{H}(y,\\sigma(z_{s}))+\\beta\\mathcal{H}(\\sigma(z_{t},T),\\sigma(z_{s},T)) \\tag{10}\\]\n' +
      '\n' +
      'In Eq. 도 10에서, \\(\\alpha\\) 및 \\(\\beta\\)는 학생 모델로부터의 손실량과 증류 손실을 조정하기 위한 가중 계수이고, \\(y\\)는 지상-진실 클래스 라벨이고, \\(\\mathcal{H}\\)는 교차 엔트로피 손실 함수이고, \\(z_{s}\\)/\\(z_{t}\\)는 학생/교사 모델에 의해 생성된 로짓이고, \\(\\sigma\\)는 소프트맥스 함수이고, T는 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[p_{i}=\\frac{\\exp\\frac{z_{i}}{T}}{\\sum_{j}\\exp\\frac{z_{j}}{T}} \\tag{11}\\]\n' +
      '\n' +
      '이전의 지식 증류 방법들은 상이한 지식 소스들을 탐색하는 것에 초점을 맞춘다. [192, 95, 150] 지식 출처로 로짓(연성 확률)을 사용하는 반면 [269, 3, 207]은 중간 계층의 지식을 활용하려고 합니다. 교사 모델의 선택도 잘 연구되고 있는데, [235, 273]은 여러 교사 모델을 사용하여 학생 모델을 공동으로 감독하는 반면 [277, 43]은 추가 교사 모델 없이 자기 증류법을 적용한다.\n' +
      '\n' +
      '### _Extreme Quantization_\n' +
      '\n' +
      '양자화된 값을 1비트 표현으로 제한하여 메모리 요구량을 32\\(\\times\\) 감소시키는 이진화는 가장 극단적인 양자화 방법이다. 메모리 이점 외에도 이진(1-비트) 및 삼진(2-비트) 연산은 종종 비트 단위 연산으로 효율적으로 계산될 수 있고 FP32 및 INT8과 같은 더 높은 정밀도에서 상당한 가속을 달성할 수 있다. 예를 들어, NVIDIA V100 GPU에서 피크 이진 연산은 INT8보다 8배 더 높다. 그러나 순진한 이진화 방법은 상당한 정확도 저하를 초래할 것이다. 이와 같이, 이를 해결하기 위해 상이한 해결책을 제안한 큰 작업체가 있다[18, 25, 47, 52, 77, 78, 83, 92, 93, 120, 122, 124, 129, 131, 135, 141, 149, 155, 160, 196, 198, 205, 217, 249, 251, 260, 262, 288, 290].\n' +
      '\n' +
      '여기서 중요한 작업은 가중치를 +1 또는 -1로 제한하는 BinaryConnect[42]이다. 이 접근법에서 가중치는 실제 값으로 유지되며 이진화 효과를 시뮬레이션하기 위해 전진 및 후진 패스 동안에만 이진화된다. 전진 패스 동안, 실수 값 가중치들은 부호 함수에 기초하여 +1 또는 -1로 변환된다. 그런 다음 STE를 사용하여 표준 훈련 방법을 사용하여 네트워크를 훈련하여 미분 불가능한 부호 함수를 통해 기울기를 전파할 수 있다. 이진화된 NN[107](BNN)은 가중치뿐만 아니라 활성화도 이진화함으로써 이 개념을 확장한다. 가중치 및 활성화를 공동 이진화하는 것은 비용이 많이 드는 부동 소수점 행렬 곱셈을 경량 XNOR 연산에 이어 비트 카운팅으로 대체할 수 있기 때문에 향상된 지연 시간의 추가적인 이점을 갖는다. 또 다른 흥미로운 작업은 [45]에서 제안된 이진 가중치 네트워크(BWN)와 XNORNet으로, 가중치에 스케일링 팩터를 통합하고 +1 또는 -1 대신 +\\(\\alpha\\) 또는 -\\(\\alpha\\)를 사용하여 더 높은 정확도를 달성한다. 여기서, \\(\\alpha\\)는 실수값 가중치와 결과 이진화된 가중치 사이의 거리를 최소화하기 위해 선택된 스케일링 팩터이다. 즉, 실수 가중치 행렬 \\(W\\)은 \\(W\\approx\\alpha B\\)로 공식화될 수 있으며, 여기서 \\(B\\)는 다음과 같은 최적화 문제를 만족하는 이진 가중치 행렬이다:\n' +
      '\n' +
      '\\[\\alpha,B=\\operatorname*{argmin}\\lVert W-\\alpha B\\rVert^{2}. \\tag{12}\\]\n' +
      '\n' +
      '더욱이, 많은 학습된 가중치들이 0에 가깝다는 관찰로부터 영감을 받아, 삼진 값들, 예를 들어 +1, 0 및 -1로 가중치들/활성화들을 제한함으로써 네트워크를 삼진화하려는 시도들이 있어, 양자화된 값들이 0이 되도록 명시적으로 허용하였다[145, 159]. 삼진화는 이진화처럼 값비싼 행렬 곱셈을 제거함으로써 추론 지연 시간을 획기적으로 줄인다. 나중에, Ternary-Binary Network (TBN) [244]는 이진 네트워크 가중치들과 삼진 활성화들을 조합하는 것이 정확성과 계산 효율성 사이에서 최적의 트레이드오프를 달성할 수 있다는 것을 보여준다.\n' +
      '\n' +
      '나이브 이진화 및 삼진화 방법은 일반적으로 이미지넷 분류와 같은 복잡한 작업에 대해 심각한 정확도 저하를 초래하기 때문에 극단적인 양자화에서 정확도 저하를 줄이기 위해 많은 솔루션이 제안되었다. [197]의 작업은 이러한 해법을 크게 세 가지로 분류한다. 여기서는 각 지점에 대해 간략하게 논의하고, 관심 있는 독자들에게 자세한 내용은 [197]에 참조한다.\n' +
      '\n' +
      '양자화 오차 최소화 솔루션의 첫 번째 분기는 양자화 오차, 즉 실수 값들과 양자화된 값들 사이의 갭을 최소화하는 것을 목표로 한다[19, 34, 62, 103, 151, 158, 164, 169, 178, 248, 218]. HORQ [151]과 ABC-Net [158]은 실제 값 가중치/활성화를 표현하기 위해 단일 이진 행렬을 사용하는 대신, 양자화 오차를 줄이기 위해 \\(W\\approx\\alpha_{1}B_{1}+\\cdots+\\alpha_{M}B_{M}\\)와 같은 여러 이진 행렬의 선형 조합을 사용한다. 활성화들을 이진화하는 것이 후속 컨볼루션 블록에 대한 그들의 표현 능력을 감소시킨다는 사실에 영감을 받아, [178] 및 [34]는 더 넓은 네트워크들(즉, 더 많은 수의 필터들을 갖는 네트워크들)의 이진화가 정확도와 모델 크기 사이에서 양호한 트레이드오프를 달성할 수 있음을 보여준다.\n' +
      '\n' +
      '개선된 손실 함수 작업의 또 다른 분기는 손실 함수의 선택에 초점을 맞춘다[48, 98, 99, 251, 284]. 여기서 중요한 작업은 이진화/국제화 가중치에 대한 손실을 직접 최소화하는 손실 인식 이진화 및 삼원화[98, 99]이다. 이는 가중치에만 근사하고 최종 손실을 고려하지 않는 다른 접근법과는 다르다. 완전 정밀 교사 모델로부터의 지식 증류는 또한 이진화/ternarization 후에 정확도 저하를 복구할 수 있는 유망한 방법으로 나타났다[33, 177, 195, 260].\n' +
      '\n' +
      '개선된 훈련 방법 또 다른 흥미로운 분야는 이진/차원 모델에 대한 더 나은 훈련 방법을 목표로 한다[5, 20, 44, 73, 160, 164, 285, 288]. 많은 노력들이 부호 함수를 통해 기울기를 역전파하는 데 있어 STE의 한계를 지적한다: STE는 [-1, 1]의 범위에 있는 가중치 및/또는 활성화에 대한 기울기만을 전파한다. 이를 해결하기 위해, BNN+[44]는 부호 함수의 도함수에 대한 연속 근사치를 도입하는 반면, [198, 261, 272]는 부호 함수를 점진적으로 날카롭게 하고 부호 함수에 접근하는 매끄럽고 미분 가능한 함수로 대체한다. Bi-Real Net [164]는 연속적인 블록들에서 활성화들에 활성화들을 연결하는 아이덴티티 바로가기들을 소개하고, 이를 통해 32-비트 활성화들이 전파될 수 있다. 대부분의 연구는 추론 시간 지연을 줄이는 데 초점을 맞추고 있지만 DoReFa-Net[285]은 학습을 가속화하기 위해 가중치와 활성화 외에도 기울기를 양자화한다.\n' +
      '\n' +
      '극한 양자화는 컴퓨터 비전 작업에서 많은 CNN 모델에 대한 모델 크기뿐만 아니라 추론/훈련 지연을 크게 줄이는 데 성공했다. 최근, 이러한 아이디어를 자연어 처리(Natural Language Processing; NLP) 작업으로 확장하려는 시도가 있었다[7, 119, 121, 278]. 많은 양의 라벨링되지 않은 데이터에 대해 사전 트레이닝된 최신 NLP 모델들(예를 들어, BERT[46], RoBERTa[163], 및 GPT 패밀리[17, 200, 201])의 무시무시한 모델 크기 및 추론 대기 시간을 고려할 때, 극단적인 양자화는 NLP 추론 태스크들을 에지로 가져오기 위한 강력한 툴로서 부상하고 있다.\n' +
      '\n' +
      '**요약(극단 양자화).** 극단 저비트 정밀 양자화는 매우 유망한 연구 라인입니다. 그러나, 기존의 방법들은 매우 광범위한 튜닝 및 하이퍼파라미터 탐색이 수행되지 않는 한, 기준선에 비해 높은 정확도 저하를 초래하는 경우가 많다. 그러나 이러한 정확도 저하는 덜 중요한 응용 프로그램에서는 허용될 수 있다.\n' +
      '\n' +
      '### _Vector Quantization_\n' +
      '\n' +
      'II절에서 논의된 바와 같이, 양자화는 기계 학습에서 발명된 것이 아니라, 정보 이론, 특히 압축 도구로서 디지털 신호 처리 분야에서 지난 세기 동안 널리 연구되어 왔다. 그러나 기계 학습을 위한 양자화 방법의 주요 차이점은 근본적으로 원래의 신호에 비해 최소한의 변화/오류로 신호를 압축하는 데 관심이 없다는 것이다. 대신에, 목표는 가능한 한 작은 손실을 초래하는 감소된-정밀 표현을 찾는 것이다. 따라서 양자화된 가중치/활성화가 양자화되지 않은 가중치/활성화와 멀리 떨어져 있으면 완전히 허용된다.\n' +
      '\n' +
      '반면에, NN 양자화에 적용된 DSP에서의 고전적인 양자화 방법들, 특히 벡터 양자화[9]에는 많은 흥미로운 아이디어들이 있다. 특히 [1, 30, 74, 84, 117, 170, 189, 256, 180]의 작업은 가중치를 서로 다른 그룹으로 군집화하고 추론 시 각 그룹의 중심을 양자화된 값으로 사용한다. 식 에 나타낸 바와 같이. 도 13에서, \\(i\\)는 텐서에서의 가중치의 인덱스이고, \\(c_{1},...,c_{k}\\)는 클러스터링에 의해 발견된 \\(k\\) 중심이며, \\(c_{j}\\)는 \\(w_{i}\\)에 대응하는 중심이다. 클러스터링 후 가중치 \\(w_{i}\\)는 코드북(look-up table)에서 \\(c_{j}\\)과 관련된 클러스터 인덱스 \\(j\\)를 가질 것이다.\n' +
      '\n' +
      '\\[\\min_{c_{1},...,c_{k}}\\sum_{i}\\|w_{i}-c_{j}\\|^{2} \\tag{13}\\]\n' +
      '\n' +
      '그 결과 k-평균 군집화를 사용하면 큰 정확도 저하 없이 모델 크기를 최대 \\(8\\times\\)까지 줄일 수 있는 것으로 나타났다[74]. 그 외에도 가지치기 및 허프만 코딩과 함께 k-평균 기반 벡터 양자화를 공동으로 적용하면 모델 크기를 더욱 줄일 수 있다[84].\n' +
      '\n' +
      '제품 양자화[227, 74, 256]는 벡터 양자화의 확장으로서, 가중치 매트릭스는 서브 매트릭스들로 분할되고 벡터 양자화는 각각의 서브 매트릭스들에 적용된다. 기본적인 곱 양자화 방법 외에도 클러스터링의 더 세밀한 사용은 정확도를 더욱 향상시킬 수 있다. 예를 들어, [74]에서 k-평균 곱 양자화 후의 잔차들은 추가로 재귀적으로 양자화된다. 그리고 [189]에서 저자들은 정보를 더 잘 보존하기 위해 더 중요한 양자화 범위에 대해 더 많은 클러스터를 적용한다.\n' +
      '\n' +
      '## V 양자화 및 하드웨어 프로세서\n' +
      '\n' +
      '우리는 양자화가 모델 크기를 감소시킬 뿐만 아니라 더 빠른 속도를 가능하게 하고 특히 낮은 정밀도 로직을 갖는 하드웨어의 경우 더 적은 전력을 필요로 한다고 말했다. 이와 같이, 양자화는 IoT 및 모바일 애플리케이션에서 에지 배포에 특히 중요했다. 에지 디바이스는 종종 컴퓨팅, 메모리 및 중요하게는 전력 예산을 포함하는 엄격한 리소스 제약을 갖는다. 이것은 종종 많은 심층 NN 모델에 충족하기에는 너무 비싸다. 또한, 많은 에지 프로세서는 특히 마이크로-컨트롤러에서 어떠한 지원 부동 소수점 연산도 갖지 않는다.\n' +
      '\n' +
      '여기서는 양자화의 맥락에서 서로 다른 하드웨어 플랫폼에 대해 간략하게 논의한다. ARM Cortex-M은 32비트 RISC ARM 프로세서 코어 그룹으로서, 저가의 전력 효율적인 임베디드 디바이스를 위해 설계된다. 예를 들어, STM32 계열은 가장자리에서 NN 추론에도 사용되는 ARM Cortex-M 코어를 기반으로 하는 마이크로컨트롤러이다. 일부 ARM Cortex-M 코어에는 전용 부동 소수점 단위가 포함되어 있지 않기 때문에 배포 전에 먼저 모델을 양자화해야 합니다. CMSIS-NN [136]은 ARM Cortex-M 코어에 NN 모델을 양자화하고 배포하는 데 도움이 되는 ARM의 라이브러리입니다. 구체적으로, 라이브러리는 고정 소수점 양자화[113, 154, 267]를 두 개의 스케일링 팩터로 활용하여 비트 쉬프팅 연산으로 양자화 및 역양자화 과정이 효율적으로 수행될 수 있도록 한다. 전용 CNN 가속기와의 에지 추론을 위한 RISC-V SoC(System on Chip)인 GAP-8[64]는 정수 연산만을 지원하는 에지 프로세서의 또 다른 예이다. 프로그램 가능한 범용 프로세서는 유연성으로 인해 널리 채택되지만, 목적으로 구축된 ASIC 칩인 Google Edge TPU는 에지에서 추론을 실행하기 위한 또 다른 새로운 솔루션이다. 에지 TPU는 컴퓨팅 자원이 많은 구글 데이터센터에서 구동되는 클라우드 TPU와 달리 소형·저전력 기기용으로 설계돼 8비트 연산만 지원한다. NN 모델은 텐서플로우의 양자화 인식 훈련 또는 훈련 후 양자화를 사용하여 양자화되어야 한다.\n' +
      '\n' +
      '그림 9는 에지에서 NN 추론을 위해 널리 사용되는 서로 다른 상용 에지 프로세서의 처리량을 나타낸다. 지난 몇 년 동안 에지 프로세서의 컴퓨팅 성능이 크게 향상되었으며, 이는 이전에 서버에서만 사용 가능했던 값비싼 NN 모델의 배치 및 추론을 허용한다. 효율적인 저정밀 로직 및 전용 딥러닝 가속기와 결합된 양자화는 이러한 에지 프로세서의 진화를 위한 중요한 원동력 중 하나이다.\n' +
      '\n' +
      '양자화는 많은 에지 프로세서에서 필수 불가결한 기법이지만, 예를 들어 99번째 백분위수 지연과 같은 SLA(Service Level Agreement) 요구 사항을 충족하는 비-에지 프로세서에서도 현저한 개선을 가져올 수 있다. 좋은 예는 최근 NVIDIA 튜링 GPU, 특히 튜링 텐서 코어를 포함하는 T4 GPU에 의해 제공된다. 텐서 코어는 효율적인 저정밀 행렬 곱셈을 위해 설계된 특수 실행 단위입니다.\n' +
      '\n' +
      '## VI 양자화를 위한 연구 방향\n' +
      '\n' +
      '여기서는 양자화의 향후 연구를 위한 몇 가지 높은 수준의 도전과 기회에 대해 간략하게 논의한다. 이것은 양자화 소프트웨어, 하드웨어 및 NN 아키텍처 공동 설계, 결합된 압축 방법 및 양자화된 훈련으로 분해된다.\n' +
      '\n' +
      '**양자화 소프트웨어:** 현재 방법을 사용하면 다른 NN을 양자화하고 배포하는 것이 간단합니다.\n' +
      '\n' +
      '그림 9: 엣지에서 NN 추론을 위한 서로 다른 상용 엣지 프로세서의 처리량 비교.\n' +
      '\n' +
      '정확성을 잃지 않고 INT8에 대한 모델입니다. INT8 양자화된 모델(예: Nvidia의 TensorRT, TVM 등)을 배포하는 데 사용할 수 있는 여러 소프트웨어 패키지가 있으며, 각각 좋은 설명서가 있습니다. 또한, 구현은 매우 최적이며 양자화를 통해 속도를 쉽게 관찰할 수 있다. 그러나, 보다 낮은 비트-정밀 양자화를 위한 소프트웨어는 널리 이용가능하지 않으며, 때때로 존재하지 않는다. 예를 들어, Nvidia의 TensorRT는 현재 sub-INT8 양자화를 지원하지 않는다. 더욱이, INT4 양자화에 대한 지원은 최근에야 TVM에 추가되었다[267]. 최근의 연구는 INT4/INT8과의 낮은 정밀도 및 혼합-정밀 양자화가 실제로 작동한다는 것을 보여주었다[51, 82, 102, 108, 187, 199, 211, 239, 246, 249, 263, 267, 286]. 따라서, 보다 낮은 정밀도 양자화를 위한 효율적인 소프트웨어 API를 개발하는 것은 중요한 영향을 미칠 것이다.\n' +
      '\n' +
      '**하드웨어 및 NN 아키텍처 공동 설계:** 위에서 논의한 바와 같이, 저정밀 양자화의 고전적 작업과 기계 학습의 최근 작업의 중요한 차이점은 NN 매개변수가 매우 다른 양자화된 값을 가질 수 있지만 여전히 유사하게 잘 일반화될 수 있다는 사실이다. 예를 들어, 양자화 인식 훈련을 사용하면 단일 정밀도 매개변수로 원래 솔루션에서 멀리 떨어진 다른 솔루션으로 수렴할 수 있지만 여전히 좋은 정확도를 얻을 수 있다. 이러한 자유도를 이용할 수 있고 NN 아키텍처가 양자화되고 있는 동안에도 적응할 수 있다. 예를 들어, [34]의 최근 작업은 NN 아키텍처의 폭을 변경하는 것이 양자화 후에 일반화 갭을 감소/제거할 수 있음을 보여준다. 향후 작업의 한 줄은 모델이 양자화되고 있을 때 깊이 또는 개별 커널과 같은 다른 아키텍처 매개변수를 공동으로 적응시키는 것이다. 향후 작업의 또 다른 라인은 이러한 공동 설계를 하드웨어 아키텍처로 확장하는 것이다. 이것은 (다중-축적 요소들의 상이한 마이크로-아키텍처들과 같은) 많은 상이한 가능한 하드웨어 구성들을 탐색할 수 있고, 그리고 나서 이것을 NN 아키텍처 및 양자화 공동 설계와 결합할 수 있기 때문에, FPGA 배포에 특히 유용할 수 있다.\n' +
      '\n' +
      '**결합 압축 방법:** 위에서 설명한 대로 양자화는 NN의 효율적인 배포 방법 중 하나일 뿐입니다. 다른 방법에는 효율적인 NN 아키텍처 설계, 하드웨어와 NN 아키텍처의 공동 설계, 가지치기 및 지식 증류가 포함된다. 양자화는 이러한 다른 접근법들과 결합될 수 있다. 그러나 현재 이러한 방법의 최적 조합을 탐색하는 작업은 거의 없다. 예를 들어, 프루닝 및 양자화는 그 오버헤드를 감소시키기 위해 모델에 함께 적용될 수 있으며[87, 152], 구조화된/비구조화된 프루닝 및 양자화의 최상의 조합을 이해하는 것이 중요하다. 유사하게, 또 다른 미래의 방향은 이러한 방법과 위에서 설명한 다른 접근법 간의 결합을 연구하는 것이다.\n' +
      '\n' +
      '**양자화된 훈련:** 양자화의 가장 중요한 사용은 반정밀도 [41, 72, 79, 175]로 NN 훈련을 가속화하는 것일 수 있습니다. 이것은 트레이닝을 위해 훨씬 더 빠르고 더 전력 효율적인 감소된 정밀도 로직의 사용을 가능하게 했다. 그러나, 이것을 INT8 정밀 훈련으로 더 내려가는 것은 매우 어려웠다. 이 영역에는 몇 가지 흥미로운 작업이 존재하지만 [10, 123, 137, 173] 제안된 방법은 종종 많은 하이퍼파라미터 튜닝을 요구하거나 비교적 쉬운 학습 작업에서 몇 개의 NN 모델에 대해서만 작동한다. 기본적인 문제는 INT8 정밀도로 훈련이 불안정해지고 발산할 수 있다는 것이다. 이러한 도전을 해결하는 것은 특히 에지에서의 트레이닝을 위해 여러 애플리케이션들에 높은 영향을 미칠 수 있다.\n' +
      '\n' +
      '## VII 요약 및 결론\n' +
      '\n' +
      '추상적인 수학적 계산이 디지털 컴퓨터에서의 계산에 적응하자마자, 그 계산들에서 수치들의 효율적인 표현, 조작, 및 통신의 문제가 발생하였다. 숫자 표현의 문제와 강하게 관련된 것은 양자화의 문제이다: 필요한 비트 수를 최소화하고 또한 수반되는 계산의 정확도를 최대화하기 위해 연속 실수 값 숫자 세트를 고정된 이산 숫자 세트에 분산해야 하는 방법은 무엇인가? 이러한 문제는 컴퓨터 과학만큼 오래되었지만 이러한 문제는 특히 효율적인 NN 모델의 설계와 관련이 있다. 이에 대한 몇 가지 이유가 있습니다. 첫째, NN은 계산 집약적이다. 따라서 수치의 효율적인 표현이 특히 중요하다. 둘째, 현재 대부분의 NN 모델은 지나치게 과대모수화되어 있다. 따라서 정확도에 영향을 미치지 않으면서 비트 정밀도를 줄일 수 있는 충분한 기회가 있습니다. 셋째, NN 모델의 계층 구조는 탐색할 수 있는 추가 차원을 제공한다. 따라서, NN 내의 상이한 층들은 손실 함수에 상이한 영향을 가지며, 이는 이러한 혼합-정밀 양자화에 대한 흥미로운 접근법들을 동기시킨다.\n' +
      '\n' +
      '부동 소수점 표현에서 8/4 비트 이하로 표현되는 저-정밀 고정 정수 값으로 이동하는 것은 메모리 풋프린트 및 레이턴시를 감소시킬 수 있는 잠재력을 보유한다. [157] TVM [32] 양자화 라이브러리를 사용하여 ResNet50 [88], VGG-19 [224] 및 inceptionV3 [230]을 포함한 인기 있는 컴퓨터 비전 모델의 INT8 추론이 NVIDIA GTX 1080에서 각각 3.89\\(\\times\\), 3.32\\(\\times\\) 및 5.02\\(\\times\\) 속도 향상을 달성할 수 있음을 보여준다.\n' +
      '\n' +
      '[213]은 또한 ResNet50의 INT4 추론이 INT8 대응에 비해 NVIDIA T4 및 RTX에서 추가로 50-60%의 속도 향상을 가져올 수 있음을 보여주며, 효율성을 최대화하기 위해 더 낮은 비트 정밀도를 사용하는 것의 중요성을 강조한다. 최근, [267]은 Mix-precision 양자화를 활용하여 ResNet50의 경우 정확도 저하 없이 INT8 추론에 비해 23%의 속도 향상을 달성하였고, [132]는 INT8만을 BERT 모델에 확장하여 FP32보다 최대 4.0\\(\\times\\) 빠른 추론을 가능하게 하였다. 앞서 언급한 연구들은 GPU의 가속도에 초점을 맞추고 있지만, [114]는 다양한 컴퓨터 비전 모델의 INT8 양자화를 통해 Intel Cascade Lake CPU와 Raspberry Pi4(비GPU 아키텍처 모두)에서 각각 2.35\\(\\times\\)와 1.40\\(\\times\\) 지연 속도 향상을 얻었다. 결과적으로, 우리의 참고문헌이 증명하는 바와 같이, NN 모델에서의 양자화의 문제는 매우 활발한 연구 영역이었다.\n' +
      '\n' +
      '이 작업에서 우리는 이러한 매우 다양한 노력에 몇 가지 개념적 구조를 가져오려고 노력했다. 우리는 균일, 비균일, 대칭, 비대칭, 정적 및 동적 양자화와 같은 양자화의 많은 응용 분야에 공통적인 주제에 대한 논의로 시작했다. 그런 다음 NN의 양자화에 더 고유한 양자화 문제를 고려했다. 여기에는 계층별, 그룹별, 채널별 및 부채널별 양자화가 포함됩니다. 또한 훈련과 양자화의 상호 관계를 고려하였으며, 훈련 후 양자화와 비교하여 양자화 인식 훈련의 장단점에 대해 논의하였다. 양자화와 트레이닝 사이의 관계에 대한 논의를 더 뉘앙스화하는 것은 데이터의 이용가능성의 문제이다. 이것의 극단적인 경우는 훈련에 사용되는 데이터가 프라이버시와 같은 다양한 합리적인 이유로 인해 더 이상 사용할 수 없는 경우이다. 이는 제로 샷 양자화의 문제를 야기한다.\n' +
      '\n' +
      '특히 에지 배치를 대상으로 하는 효율적인 NN에 대해 우려하기 때문에 이러한 환경에 고유한 문제를 고려했다. 이들은 8 비트보다 적은, 아마도 이진 값만큼 낮은 값으로 표현되는 파라미터들을 초래하는 양자화 기법들을 포함한다. 또한 부동 소수점 단위가 부족한 저전력 마이크로프로세서에 NN을 배치할 수 있는 정수 전용 양자화 문제를 고려하였다.\n' +
      '\n' +
      '이 설문조사와 그 조직을 통해, 우리는 신경망에 대한 양자화에 있어 현재 연구의 유용한 스냅샷을 제시하고 이 분야의 향후 연구에 대한 평가를 쉽게 할 수 있는 지능화된 조직을 제공하기를 희망한다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      'UC버클리팀도 삼성(특히 조셉 하순), 인텔 법인, 인텔 VLAB팀, 구글 TRC팀, 구글 브레인(특히 데이비드 패터슨 교수, 에드 치 박사, 징 리 박사)의 공손한 지원을 인정한다. 아미르 골라미는 삼성 사잇의 자금 지원을 통해 지원받았다. 우리의 결론은 반드시 후원자의 입장이나 정책을 반영하는 것은 아니며 공식적인 지지를 추론해서는 안 된다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and Luc Van Gool. Soft-to-hard vector quantization for end-to-end learning compressible representations. _arXiv preprint arXiv:1704.00648_, 2017.\n' +
      '* [2] Eirikur Agustsson and Lucas Theis. Universally quantized neural compression. _Advances in neural information processing systems_, 2020.\n' +
      '* [3] Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Variational information distillation for knowledge transfer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9163-9171, 2019.\n' +
      '* [4] Milad Alizadeh, Arash Behboodi, Mart van Baalen, Christos Louizos, Tijmen Blankevoort, and Max Welling. Gradient ll regularization for quantization robustness. _arXiv preprint arXiv:2002.07520_, 2020.\n' +
      '* [5] Milad Alizadeh, Javier Fernandez-Marques, Nicholas D Lane, and Yarin Gal. An empirical study of binary neural networks\' optimisation. In _International Conference on Learning Representations_, 2018.\n' +
      '* [6] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.\n' +
      '* [7] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. _arXiv preprint arXiv:2012.15701_, 2020.\n' +
      '* [8] Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant: Quantized neural networks via proximal operators. _arXiv preprint arXiv:1810.00861_, 2018.\n' +
      '* [9] Dana Harry Ballard. _An introduction to natural computation_. MIT press, 1999.\n' +
      '\n' +
      '* [10] Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. _Advances in neural information processing systems_, 2018.\n' +
      '* [11] Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of convolution networks for rapid-deployment. _arXiv preprint arXiv:1810.05723_, 2018.\n' +
      '* [12] Chaim Baskin, Eli Schwartz, Evgenii Zheltonozhskii, Natan Liss, Raja Giryes, Alex M Bronstein, and Avi Mendelson. Uniq: Uniform noise injection for non-uniform quantization of neural networks. _arXiv preprint arXiv:1804.10969_, 2018.\n' +
      '* [13] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. _arXiv preprint arXiv:1308.3432_, 2013.\n' +
      '* [14] William Ralph Bennett. Spectra of quantized signals. _The Bell System Technical Journal_, 27(3):446-472, 1948.\n' +
      '* [15] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets and better initialization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, pages 696-697, 2020.\n' +
      '* [16] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of neural network pruning? _arXiv preprint arXiv:2003.03033_, 2020.\n' +
      '* [17] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020.\n' +
      '* [18] Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. High-capacity expert binary networks. _International Conference on Learning Representations_, 2021.\n' +
      '* [19] Adrian Bulat and Georgios Tzimiropoulos. Xnornet++: Improved binary neural networks. _arXiv preprint arXiv:1909.13863_, 2019.\n' +
      '* [20] Adrian Bulat, Georgios Tzimiropoulos, Jean Kossaifi, and Maja Pantic. Improved training of binary networks for human pose estimation and image recognition. _arXiv preprint arXiv:1904.05868_, 2019.\n' +
      '* [21] Aydin Buluc and John R Gilbert. Challenges and advances in parallel sparse matrix-matrix multiplication. In _2008 37th International Conference on Parallel Processing_, pages 503-510. IEEE, 2008.\n' +
      '* [22] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and specialize it for efficient deployment. _arXiv preprint arXiv:1908.09791_, 2019.\n' +
      '* [23] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. _arXiv preprint arXiv:1812.00332_, 2018.\n' +
      '* [24] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13169-13178, 2020.\n' +
      '* [25] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. Deep learning with low precision by half-wave gaussian quantization. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5918-5926, 2017.\n' +
      '* [26] Leopold Cambier, Anahita Bhiwandiwalla, Ting Gong, Mehran Nekuii, Oguz H Elibol, and Hanlin Tang. Shifted and squeezed 8-bit floating point format for low-precision training of deep neural networks. _arXiv preprint arXiv:2001.05674_, 2020.\n' +
      '* [27] Rishidev Chaudhuri and Ila Fiete. Computational principles of memory. _Nature neuroscience_, 19(3):394, 2016.\n' +
      '* [28] Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, and Qi Tian. Data-free learning of student networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3514-3522, 2019.\n' +
      '* [29] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical framework for low-bitwidth training of deep neural networks. _arXiv preprint arXiv:2010.14298_, 2020.\n' +
      '* [30] Kuilin Chen and Chi-Guhn Lee. Incremental few-shot learning via vector quantization in deep embedded space. In _International Conference on Learning Representations_, 2021.\n' +
      '* [31] Shangyu Chen, Wenya Wang, and Sinno Jialin Pan. Metaquant: Learning to quantize by learning to penetrate non-differentiable quantization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.\n' +
      '\n' +
      '* [32] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. TVM: An automated end-to-end optimizing compiler for deep learning. In _13th \\(\\{\\)USENIX\\(\\}\\) Symposium on Operating Systems Design and Implementation (\\(\\{\\)OSDI\\(\\}\\) 18)_, pages 578-594, 2018.\n' +
      '* [33] Xiuyi Chen, Guangcan Liu, Jing Shi, Jiaming Xu, and Bo Xu. Distilled binary neural network for monaural speech separation. In _2018 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2018.\n' +
      '* [34] Ting-Wu Chin, Pierce I-Jen Chuang, Vikas Chandra, and Diana Marculescu. One weight bitwidth to rule them all. _Proceedings of the European Conference on Computer Vision (ECCV)_, 2020.\n' +
      '* [35] Brian Chmiel, Liad Ben-Uri, Moran Shkolnik, Elad Hoffer, Ron Banner, and Daniel Soudry. Neural gradients are near-lognormal: improved quantized and sparse training. In _International Conference on Learning Representations_, 2021.\n' +
      '* [36] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. _arXiv preprint arXiv:1805.06085_, 2018.\n' +
      '* [37] Yoojin Choi, Jihwan Choi, Mostafa El-Khamy, and Jungwon Lee. Data-free network quantization with adversarial knowledge distillation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, pages 710-711, 2020.\n' +
      '* [38] Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Towards the limit of network quantization. _arXiv preprint arXiv:1612.01543_, 2016.\n' +
      '* [39] Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Learning low precision deep neural networks through regularization. _arXiv preprint arXiv:1809.00095_, 2, 2018.\n' +
      '* [40] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. Low-bit quantization of neural networks for efficient inference. In _ICCV Workshops_, pages 3009-3018, 2019.\n' +
      '* [41] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Training deep neural networks with low precision multiplications. _arXiv preprint arXiv:1412.7024_, 2014.\n' +
      '* [42] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. BinaryConnect: Training deep neural networks with binary weights during propagations. In _Advances in neural information processing systems_, pages 3123-3131, 2015.\n' +
      '* [43] Elliot J Crowley, Gavin Gray, and Amos J Storkey. Moonshine: Distilling with cheap convolutions. In _NeurIPS_, pages 2893-2903, 2018.\n' +
      '* [44] Sajad Darabi, Mouloud Belbahri, Matthieu Courbariaux, and Vahid Partovi Nia. Bnn+: Improved binary network training. 2018.\n' +
      '* [45] Lei Deng, Peng Jiao, Jing Pei, Zhenzhi Wu, and Guoqi Li. Gxnor-net: Training deep neural networks with ternary weights and activations without full-precision memory under a unified discretization framework. _Neural Networks_, 100:49-58, 2018.\n' +
      '* [46] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* [47] James Diffenderfer and Bhavya Kailkhura. Multi-prize lottery ticket hypothesis: Finding accurate binary neural networks by pruning a randomly weighted network. In _International Conference on Learning Representations_, 2021.\n' +
      '* [48] Ruizhou Ding, Ting-Wu Chin, Zeye Liu, and Diana Marculescu. Regularizing activation distribution for training binarized deep networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11408-11417, 2019.\n' +
      '* [49] Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. _arXiv preprint arXiv:1705.07565_, 2017.\n' +
      '* [50] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. HAWQ-V2: Hessian aware trace-weighted quantization of neural networks. _Advances in neural information processing systems_, 2020.\n' +
      '* [51] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 293-302, 2019.\n' +
      '* [52] Yueqi Duan, Jiwen Lu, Ziwei Wang, Jianjiang Feng, and Jie Zhou. Learning deep binary descriptor with multi-quantization. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1183-1192, 2017.\n' +
      '* [53] JG Dunn. The performance of a class of n dimensional quantizers for a gaussian source. In _Proc. Columbia Symp. Signal Transmission Processing_, pages 76-81, 1965.\n' +
      '* [54] Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, et al. Neural architecture search: A survey. _J. Mach. Learn. Res._, 20(55):1-21, 2019.\n' +
      '* [55] William H Equitz. A new vector quantization clustering algorithm. _IEEE transactions on acoustics, speech, and signal processing_, 37(10):1568-1575, 1989.\n' +
      '* [56] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmenta S Modha. Learned step size quantization. _arXiv preprint arXiv:1902.08153_, 2019.\n' +
      '* [57] Fartash Faghri, Iman Tabrizian, Ilia Markov, Dan Alistarh, Daniel Roy, and Ali Ramezani-Kebrya. Adaptive gradient quantization for data-parallel sgd. _Advances in neural information processing systems_, 2020.\n' +
      '* [58] A Aldo Faisal, Luc PJ Selen, and Daniel M Wolpert. Noise in the nervous system. _Nature reviews neuroscience_, 9(4):292-303, 2008.\n' +
      '* [59] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme model compression. _arXiv e-prints_, pages arXiv-2004, 2020.\n' +
      '* [60] Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, and Joseph Hassoun. Near-lossless post-training quantization of deep neural networks via a piecewise linear approximation. _arXiv preprint arXiv:2002.00104_, 2020.\n' +
      '* [61] Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, and Joseph H Hassoun. Post-training piecewise linear quantization for deep neural networks. In _European Conference on Computer Vision_, pages 69-86. Springer, 2020.\n' +
      '* [62] Julian Faraone, Nicholas Fraser, Michaela Blott, and Philip HW Leong. Syq: Learning symmetric quantization for efficient deep neural networks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 4300-4309, 2018.\n' +
      '* [63] Alexander Finkelstein, Uri Almog, and Mark Grobman. Fighting quantization bias with bias. _arXiv preprint arXiv:1906.03193_, 2019.\n' +
      '* [64] Eric Flamand, Davide Rossi, Francesco Conti, Igor Loi, Antonio Pullini, Florent Rotenberg, and Luca Benini. Gap-8: A risc-v soc for ai at the edge of the iot. In _2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)_, pages 1-4. IEEE, 2018.\n' +
      '* [65] Abram L Friesen and Pedro Domingos. Deep learning as a mixed convex-combinatorial optimization problem. _arXiv preprint arXiv:1710.11573_, 2017.\n' +
      '* [66] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. _arXiv preprint arXiv:1902.09574_, 2019.\n' +
      '* [67] AE Gamal, L Hemachandra, Itzhak Shperling, and V Wei. Using simulated annealing to design good codes. _IEEE Transactions on Information Theory_, 33(1):116-123, 1987.\n' +
      '* [68] Sahaj Garg, Anirudh Jain, Joe Lou, and Mitchell Nahmias. Confounding tradeoffs for neural network quantization. _arXiv preprint arXiv:2102.06366_, 2021.\n' +
      '* [69] Sahaj Garg, Joe Lou, Anirudh Jain, and Mitchell Nahmias. Dynamic precision analog computing for neural networks. _arXiv preprint arXiv:2102.06365_, 2021.\n' +
      '* [70] Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng Tai, Xiangyu Yue, Peter Jin, Sicheng Zhao, and Kurt Keutzer. SqueezeNext: Hardware-aware neural network design. _Workshop paper in CVPR_, 2018.\n' +
      '* [71] Amir Gholami, Michael W Mahoney, and Kurt Keutzer. An integrated approach to neural network design, training, and inference. _Univ. California, Berkeley, Berkeley, CA, USA, Tech. Rep_, 2020.\n' +
      '* [72] Boris Ginsburg, Sergei Nikolaev, Ahmad Kiswani, Hao Wu, Amir Gholaminejad, Slawomir Kierat, Michael Houston, and Alex Fit-Florea. Tensor processing using low precision format, December 28 2017. US Patent App. 15/624,577.\n' +
      '* [73] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4852-4861, 2019.\n' +
      '* [74] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. _arXiv preprint arXiv:1412.6115_, 2014.\n' +
      '* [75] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _arXiv preprint _arXiv:1406.2661_, 2014.\n' +
      '* [76] Robert M. Gray and David L. Neuhoff. Quantization. _IEEE transactions on information theory_, 44(6):2325-2383, 1998.\n' +
      '* [77] Nianhui Guo, Joseph Bethge, Haojin Yang, Kai Zhong, Xuefei Ning, Christoph Meinel, and Yu Wang. Boolnet: Minimizing the energy consumption of binary neural networks. _arXiv preprint arXiv:2106.06991_, 2021.\n' +
      '* [78] Yiwen Guo, Anbang Yao, Hao Zhao, and Yurong Chen. Network sketching: Exploiting binary structure in deep cnns. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5955-5963, 2017.\n' +
      '* [79] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In _International conference on machine learning_, pages 1737-1746. PMLR, 2015.\n' +
      '* [80] Philipp Gysel, Mohammad Motamedi, and Soheil Ghiasi. Hardware-oriented approximation of convolutional neural networks. _arXiv preprint arXiv:1604.03168_, 2016.\n' +
      '* [81] Philipp Gysel, Jon Pimentel, Mohammad Motamedi, and Soheil Ghiasi. Ristretto: A framework for empirical study of resource-efficient inference in convolutional neural networks. _IEEE transactions on neural networks and learning systems_, 29(11):5784-5789, 2018.\n' +
      '* [82] Hai Victor Habi, Roy H Jennings, and Arnon Netzer. Hmq: Hardware friendly mixed precision quantization block for cnns. _arXiv preprint arXiv:2007.09952_, 2020.\n' +
      '* [83] Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Enhua Wu, and Chang Xu. Training binary neural networks through learning with noisy supervision. In _International Conference on Machine Learning_, pages 4017-4026. PMLR, 2020.\n' +
      '* [84] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. _arXiv preprint arXiv:1510.00149_, 2015.\n' +
      '* [85] Matan Haroush, Itay Hubara, Elad Hoffer, and Daniel Soudry. The knowledge within: Methods for data-free model compression. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8494-8502, 2020.\n' +
      '* [86] Babak Hassibi and David G Stork. _Second order derivatives for network pruning: Optimal brain surgeon_. Morgan Kaufmann, 1993.\n' +
      '* [87] Benjamin Hawks, Javier Duarte, Nicholas J Fraser, Alessandro Pappalardo, Nhan Tran, and Yaman Umuroglu. Ps and qs: Quantization-aware pruning for efficient low latency neural network inference. _arXiv preprint arXiv:2102.11289_, 2021.\n' +
      '* [88] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.\n' +
      '* [89] Xiangyu He and Jian Cheng. Learning compression from limited unlabeled data. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 752-769, 2018.\n' +
      '* [90] Xiangyu He, Qinghao Hu, Peisong Wang, and Jian Cheng. Generative zero-shot network quantization. _arXiv preprint arXiv:2101.08430_, 2021.\n' +
      '* [91] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. Amc: Automl for model compression and acceleration on mobile devices. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 784-800, 2018.\n' +
      '* [92] Zhezhi He and Deliang Fan. Simultaneously optimizing weight and quantizer of ternary neural network using truncated gaussian approximation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11438-11446, 2019.\n' +
      '* [93] Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng, and Roeland Nusselder. Latent weights do not exist: Rethinking binarized neural network optimization. _Advances in neural information processing systems_, 2019.\n' +
      '* [94] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). _arXiv preprint arXiv:1606.08415_, 2016.\n' +
      '* [95] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.\n' +
      '* [96] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. _arXiv preprint arXiv:2102.00554_, 2021.\n' +
      '* [97] Mark Horowitz. 1.1 computing\'s energy problem (and what we can do about it). In _2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)_, pages 10-14. IEEE, 2014.\n' +
      '* [98] Lu Hou and James T Kwok. Loss-aware weight quantization of deep networks. _arXiv preprint arXiv:1802.08635_, 2018.\n' +
      '* [99] Lu Hou, Quanming Yao, and James T Kwok. Loss-aware binarization of deep networks. _arXiv preprint arXiv:1611.01600_, 2016.\n' +
      '* [100] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for MobilenetV3. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 1314-1324, 2019.\n' +
      '* [101] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. MobileNets: Efficient convolutional neural networks for mobile vision applications. _arXiv preprint arXiv:1704.04861_, 2017.\n' +
      '* [102] Peng Hu, Xi Peng, Hongyuan Zhu, Mohamed M Sabry Aly, and Jie Lin. Opq: Compressing deep neural networks with one-shot pruning-quantization. 2021.\n' +
      '* [103] Qinghao Hu, Peisong Wang, and Jian Cheng. From hashing to cnns: Training binary weight networks via hashing. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.\n' +
      '* [104] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.\n' +
      '* [105] Qijing Huang, Dequan Wang, Zhen Dong, Yizhao Gao, Yaohui Cai, Tian Li, Bichen Wu, Kurt Keutzer, and John Wawrzynek. Codenet: Efficient deployment of input-adaptive object detection on embedded fpgas. In _The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays_, pages 206-216, 2021.\n' +
      '* [106] Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks. In _Proceedings of the European conference on computer vision (ECCV)_, pages 304-320, 2018.\n' +
      '* [107] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In _Advances in neural information processing systems_, pages 4107-4115, 2016.\n' +
      '* [108] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Improving post training neural quantization: Layer-wise calibration and integer programming. _arXiv preprint arXiv:2006.10518_, 2020.\n' +
      '* [109] David A Huffman. A method for the construction of minimum-redundancy codes. _Proceedings of the IRE_, 40(9):1098-1101, 1952.\n' +
      '* [110] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. SqueezeNet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. _arXiv preprint arXiv:1602.07360_, 2016.\n' +
      '* [111] Yani Ioannou, Duncan Robertson, Roberto Cipolla, and Antonio Criminisi. Deep roots: Improving cnn efficiency with hierarchical filter groups. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1231-1240, 2017.\n' +
      '* [112] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning_, pages 448-456. PMLR, 2015.\n' +
      '* [113] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.\n' +
      '* [114] Animesh Jain, Shoubhik Bhattacharya, Masahiro Masuda, Vin Sharma, and Yida Wang. Efficient execution of quantized deep learning models: A compiler approach. _arXiv preprint arXiv:2006.10226_, 2020.\n' +
      '* [115] Shubham Jain, Swagath Venkataramani, Vijayalakshmi Srinivasan, Jungwook Choi, Kailash Gopalakrishnan, and Leland Chang. Biscaled-dnn: Quantizing long-tailed datastructures with two scale factors for deep neural networks. In _2019 56th ACM/IEEE Design Automation Conference (DAC)_, pages 1-6. IEEE, 2019.\n' +
      '* [116] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. _arXiv preprint arXiv:1611.01144_, 2016.\n' +
      '* [117] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. _IEEE transactions on pattern analysis and machine intelligence_, 33(1):117-128, 2010.\n' +
      '* [118] Yongkweon Jeon, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Jeongin Yun, and Dongsoo Lee. Biqgemm: matrix multiplication with lookup table for binary-coding-based quantized dnns. _arXivpreprint arXiv:2005.09904_, 2020.\n' +
      '* [119] Tianchu Ji, Shraddhan Jain, Michael Ferdman, Peter Milder, H Andrew Schwartz, and Niranjan Balasubramanian. On the distribution, sparsity, and inference-time quantization of attention values in transformers. _arXiv preprint arXiv:2106.01335_, 2021.\n' +
      '* [120] Kai Jia and Martin Rinard. Efficient exact verification of binarized neural networks. _Advances in neural information processing systems_, 2020.\n' +
      '* [121] Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, and Zhiliang Gan. Kdlsq-bert: A quantized bert combining knowledge distillation with learned step size quantization. _arXiv preprint arXiv:2101.05938_, 2021.\n' +
      '* [122] Qing Jin, Linjie Yang, and Zhenyu Liao. Adabits: Neural network quantization with adaptive bit-widths. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2146-2156, 2020.\n' +
      '* [123] Jeff Johnson. Rethinking floating point for deep learning. _arXiv preprint arXiv:1811.01721_, 2018.\n' +
      '* [124] Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios Savvides. Local binary convolutional neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 19-28, 2017.\n' +
      '* [125] Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. Learning to quantize deep networks by optimizing quantization intervals with task loss. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4350-4359, 2019.\n' +
      '* [126] Prad Kadambi, Karthikeyan Natesan Ramamurthy, and Visar Berisha. Comparing fisher information regularization with distillation for dnn quantization. _Advances in neural information processing systems_, 2020.\n' +
      '* [127] PP Kanjilal, PK Dey, and DN Banerjee. Reduced-size neural networks through singular value decomposition and subset selection. _Electronics Letters_, 29(17):1516-1518, 1993.\n' +
      '* [128] Mel Win Khaw, Luminita Stevens, and Michael Woodford. Discrete adjustment to a changing environment: Experimental evidence. _Journal of Monetary Economics_, 91:88-103, 2017.\n' +
      '* [129] Hyungjun Kim, Kyungsu Kim, Jinseok Kim, and Jae-Joon Kim. Binaryduo: Reducing gradient mismatch in binary activation network by coupling binary activations. _International Conference on Learning Representations_, 2020.\n' +
      '* [130] Jangho Kim, KiYoon Yoo, and Nojun Kwak. Position-based scaled gradient for model quantization and sparse training. _Advances in neural information processing systems_, 2020.\n' +
      '* [131] Minje Kim and Paris Smaragdis. Bitwise neural networks. _arXiv preprint arXiv:1601.06071_, 2016.\n' +
      '* [132] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. _arXiv preprint arXiv:2101.01321_, 2021.\n' +
      '* [133] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper. _arXiv preprint arXiv:1806.08342_, 2018.\n' +
      '* [134] Andrey Kuzmin, Markus Nagel, Saurabh Pitre, Sandeep Pendyam, Tijmen Blankevoort, and Max Welling. Taxonomy and evaluation of structured compression of convolutional neural networks. _arXiv preprint arXiv:1912.09802_, 2019.\n' +
      '* [135] Se Jung Kwon, Dongsoo Lee, Byeongwook Kim, Parichay Kapoor, Baeseong Park, and Gu-Yeon Wei. Structured compression by weight encryption for unstructured pruning and quantization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1909-1918, 2020.\n' +
      '* [136] Liangzhen Lai, Naveen Suda, and Vikas Chandra. CMSIS-NN: Efficient neural network kernels for arm cortex-m cpus. _arXiv preprint arXiv:1801.06601_, 2018.\n' +
      '* [137] Hamed F Langroudi, Zachariah Carmichael, David Pastuch, and Dhireesha Kudithipudi. Cheetah: Mixed low-precision hardware & software co-design framework for dnns on the edge. _arXiv preprint arXiv:1908.02386_, 2019.\n' +
      '* [138] Kenneth W Latimer, Jacob L Yates, Miriam LR Meister, Alexander C Huk, and Jonathan W Pillow. Single-trial spike trains in parietal cortex reveal discrete steps during decision-making. _Science_, 349(6244):184-187, 2015.\n' +
      '* [139] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In _Advances in neural information processing systems_, pages 598-605, 1990.\n' +
      '* [140] Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio. Difference target propagation. In _Joint european conference on machine learning and knowledge discovery in databases_, pages 498-503.\n' +
      '\n' +
      '515. 스프링어, 2015.\n' +
      '* [141] Dongsoo Lee, Se Jung Kwon, Byeongwook Kim, Yongkweon Jeon, Baescong Park, and Jeongin Yun. Flexor: Trainable fractional quantization. _Advances in neural information processing systems_, 2020.\n' +
      '* [142] Jun Haeng Lee, Sangwon Ha, Saerom Choi, Won-Jo Lee, and Seungwon Lee. Quantization for rapid deployment of deep neural networks. _arXiv preprint arXiv:1810.05488_, 2018.\n' +
      '* [143] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. _arXiv preprint arXiv:1810.02340_, 2018.\n' +
      '* [144] Cong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu, and Rong Jin. Extremely low bit neural network: Squeeze the last bit out with admm. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.\n' +
      '* [145] Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. _arXiv preprint arXiv:1605.04711_, 2016.\n' +
      '* [146] Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan, and Rui Fan. Fully quantized network for object detection. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019.\n' +
      '* [147] Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks. _arXiv preprint arXiv:1909.13144_, 2019.\n' +
      '* [148] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. _International Conference on Learning Representations_, 2021.\n' +
      '* [149] Yuhang Li, Ruihao Gong, Fengwei Yu, Xin Dong, and Xianglong Liu. Dms: Differentiable dimension search for binary neural networks. _International Conference on Learning Representations_, 2020.\n' +
      '* [150] Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from noisy labels with distillation. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 1910-1918, 2017.\n' +
      '* [151] Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and Wen Gao. Performance guaranteed network acceleration via high-order residual quantization. In _Proceedings of the IEEE international conference on computer vision_, pages 2584-2592, 2017.\n' +
      '* [152] Tailin Liang, John Glossner, Lei Wang, and Shaobo Shi. Pruning and quantization for deep neural network acceleration: A survey. _arXiv preprint arXiv:2101.09671_, 2021.\n' +
      '* [153] Zhenyu Liao, Romain Couillet, and Michael W Mahoney. Sparse quantized spectral clustering. _International Conference on Learning Representations_, 2021.\n' +
      '* [154] Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy. Fixed point quantization of deep convolutional networks. In _International conference on machine learning_, pages 2849-2858. PMLR, 2016.\n' +
      '* [155] Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian Wu, Feiyue Huang, and Chia-Wen Lin. Rotated binary neural network. _Advances in neural information processing systems_, 2020.\n' +
      '* [156] Shaohui Lin, Rongrong Ji, Yuchao Li, Yongjian Wu, Feiyue Huang, and Baochang Zhang. Accelerating convolutional networks via global & dynamic filter pruning. In _IJCAI_, pages 2425-2432, 2018.\n' +
      '* [157] Wuwei Lin. Automating optimization of quantized deep learning models on cuda: [https://tvm.apache.org/2019/04/29/opt-cuda-quantized](https://tvm.apache.org/2019/04/29/opt-cuda-quantized), 2019.\n' +
      '* [158] Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. _arXiv preprint arXiv:1711.11294_, 2017.\n' +
      '* [159] Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. Neural networks with few multiplications. _arXiv preprint arXiv:1510.03009_, 2015.\n' +
      '* [160] Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong Ji, and David Doermann. Circulant binary convolutional networks: Enhancing the performance of 1-bit dcnns with circulant back propagation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2691-2699, 2019.\n' +
      '* [161] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. _arXiv preprint arXiv:1806.09055_, 2018.\n' +
      '* [162] Hongyang Liu, Sara Elkerdawy, Nilanjan Ray, and Mostafa Elhoushi. Layer importance estimation with imprinting for neural network quantization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2408-2417, 2021.\n' +
      '* [163] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.\n' +
      '* [164] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In _Proceedings of the European conference on computer vision (ECCV)_, pages 722-737, 2018.\n' +
      '* [165] Zhi-Gang Liu and Matthew Mattina. Learning low-precision neural networks without straight-through estimator (STE). _arXiv preprint arXiv:1903.01061_, 2019.\n' +
      '* [166] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In _Proceedings of the IEEE international conference on computer vision_, pages 5058-5066, 2017.\n' +
      '* [167] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet V2: Practical guidelines for efficient cnn architecture design. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 116-131, 2018.\n' +
      '* [168] Franck Mamalet and Christophe Garcia. Simplifying convnets for fast learning. In _International Conference on Artificial Neural Networks_, pages 58-65. Springer, 2012.\n' +
      '* [169] Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural networks with real-to-binary convolutions. _arXiv preprint arXiv:2003.11535_, 2020.\n' +
      '* [170] Julieta Martinez, Shobhit Zakhmi, Holger H Hoos, and James J Little. Lsq++: Lower running time and higher recall in multi-codebook quantization. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 491-506, 2018.\n' +
      '* [171] Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. _The bulletin of mathematical biophysics_, 5(4):115-133, 1943.\n' +
      '* [172] Jeffrey L McKinstry, Steven K Esser, Rathinakumar Appuswamy, Deepika Bablani, John V Arthur, Izzet B Yildiz, and Dharmendra S Modha. Discovering low-precision networks close to full-precision networks for efficient embedded inference. _arXiv preprint arXiv:1809.04191_, 2018.\n' +
      '* [173] Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul. Mixed precision training with 8-bit floating point. _arXiv preprint arXiv:1905.12334_, 2019.\n' +
      '* [174] Eldad Meller, Alexander Finkelstein, Uri Almog, and Mark Grobman. Same, same but different: Recovering neural network quantization error through weight factorization. In _International Conference on Machine Learning_, pages 4486-4495. PMLR, 2019.\n' +
      '* [175] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. _arXiv preprint arXiv:1710.03740_, 2017.\n' +
      '* [176] Szymon Migacz. Nvidia 8-bit inference with tensortr. _GPU Technology Conference_, 2017.\n' +
      '* [177] Asit Mishra and Debbie Marr. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. _arXiv preprint arXiv:1711.05852_, 2017.\n' +
      '* [178] Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook, and Debbie Marr. Wrpn: Wide reduced-precision networks. _arXiv preprint arXiv:1709.01134_, 2017.\n' +
      '* [179] Daisuke Miyashita, Edward H Lee, and Boris Murmann. Convolutional neural networks using logarithmic data representation. _arXiv preprint arXiv:1603.01025_, 2016.\n' +
      '* [180] Lopamudra Mukherjee, Sathya N Ravi, Jiming Peng, and Vikas Singh. A biresolution spectral framework for product quantization. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 3329-3338, 2018.\n' +
      '* [181] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In _International Conference on Machine Learning_, pages 7197-7206. PMLR, 2020.\n' +
      '* [182] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1325-1334, 2019.\n' +
      '* [183] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelyei Bondarenko, Mart van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization. _arXiv preprint arXiv:2106.08295_, 2021.\n' +
      '* [184] Maxim Naumov, Utku Diril, Jongsoo Park, Benjamin Ray, Jedrzejablonski, and Andrew Tulloch. On periodic functions as regularizers for quantization of neural networks. _arXiv preprint arXiv:1811.09862_, 2018.\n' +
      '* [185] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization and recommendation systems. _arXiv preprint arXiv:1906.00091_, 2019.\n' +
      '* [186] Renkun Ni, Hong-min Chu, Oscar Castaneda, Ping-yeh Chiang, Christoph Studer, and Tom Goldstein. Wrapnet: Neural net inference with ultra-low-resolution arithmetic. _arXiv preprint arXiv:2007.13242_, 2020.\n' +
      '* [187] Lin Ning, Guoyang Chen, Weifeng Zhang, and Xipeng Shen. Simple augmentation goes a long way: {ADRL} for {dnn} quantization. In _International Conference on Learning Representations_, 2021.\n' +
      '* [188] BM Oliver, JR Pierce, and Claude E Shannon. The philosophy of pcm. _Proceedings of the IRE_, 36(11):1324-1331, 1948.\n' +
      '* [189] Eunhyeok Park, Junwhan Ahn, and Sungjoo Yoo. Weighted-entropy-based quantization for deep neural networks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5456-5464, 2017.\n' +
      '* [190] Eunhyeok Park, Sungjoo Yoo, and Peter Vajda. Value-aware quantization for training and inference of neural networks. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 580-595, 2018.\n' +
      '* [191] Sejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin. Lookahead: a far-sighted alternative of magnitude-based pruning. _arXiv preprint arXiv:2002.04809_, 2020.\n' +
      '* [192] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3967-3976, 2019.\n' +
      '* [193] Peng Peng, Mingyu You, Weisheng Xu, and Jiaxin Li. Fully integer-based quantization for mobile convolutional neural network inference. _Neurocomputing_, 432:194-205, 2021.\n' +
      '* [194] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In _International Conference on Machine Learning_, pages 4095-4104. PMLR, 2018.\n' +
      '* [195] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization. _arXiv preprint arXiv:1802.05668_, 2018.\n' +
      '* [196] Haotong Qin, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao, Shuai Yi, Xianglong Liu, and Hao Su. Bipointnet: Binary neural network for point clouds. _International Conference on Learning Representations_, 2021.\n' +
      '* [197] Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural networks: A survey. _Pattern Recognition_, 105:107281, 2020.\n' +
      '* [198] Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan Song. Forward and backward information retention for accurate binary neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2250-2259, 2020.\n' +
      '* [199] Zhongnan Qu, Zimu Zhou, Yun Cheng, and Lothar Thiele. Adaptive loss-aware quantization for multi-bit networks. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2020.\n' +
      '* [200] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018.\n' +
      '* [201] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n' +
      '* [202] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. _arXiv preprint arXiv:1710.05941_, 2017.\n' +
      '* [203] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Swish: a self-gated activation function. _arXiv preprint arXiv:1710.05941_, 7:1, 2017.\n' +
      '* [204] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In _European conference on computer vision_, pages 525-542. Springer, 2016.\n' +
      '* [205] Ryan Razani, Gregoire Morin, Eyyub Sari, and Vahid Partovi Nia. Adaptive binary-ternary quantization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4613-4618, 2021.\n' +
      '* [206] Bernhard Riemann. _Ueber die Darstellbarkeit einer Function durch eine trigonometrische Reihe_, volume 13. Dieterich, 1867.\n' +
      '\n' +
      '* [207] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. _arXiv preprint arXiv:1412.6550_, 2014.\n' +
      '* [208] Kenneth Rose, Eitan Gurewitz, and Geoffrey Fox. A deterministic annealing approach to clustering. _Pattern Recognition Letters_, 11(9):589-594, 1990.\n' +
      '* [209] Frank Rosenblatt. _The perceptron, a perceiving and recognizing automaton Project Para_. Cornell Aeronautical Laboratory, 1957.\n' +
      '* [210] Frank Rosenblatt. Principles of neurodynamics. perceptrons and the theory of brain mechanisms. Technical report, Cornell Aeronautical Lab Inc Buffalo NY, 1961.\n' +
      '* [211] Manuele Rusci, Marco Fariselli, Alessandro Capotondi, and Luca Benini. Leveraging automated mixed-low-precision quantization for tiny edge microcontrollers. In _IoT Streams for Data-Driven Predictive Maintenance and IoT, Edge, and Mobile for Embedded Machine Learning_, pages 296-308. Springer, 2020.\n' +
      '* [212] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In _2013 IEEE international conference on acoustics, speech and signal processing_, pages 6655-6659. IEEE, 2013.\n' +
      '* [213] Dave Salvator, Hao Wu, Milind Kulkarni, and Niall Emmart. Int4 precision for ai inference: [https://developer.nvidia.com/blog/int4-for-ai-inference/](https://developer.nvidia.com/blog/int4-for-ai-inference/), 2019.\n' +
      '* [214] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobilenetV2: Inverted residuals and linear bottlenecks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 4510-4520, 2018.\n' +
      '* [215] Claude E Shannon. A mathematical theory of communication. _The Bell system technical journal_, 27(3):379-423, 1948.\n' +
      '* [216] Claude E Shannon. Coding theorems for a discrete source with a fidelity criterion. _IRE Nat. Conv. Rec_, 4(142-163):1, 1959.\n' +
      '* [217] Alexander Shekhovtsov, Viktor Yanush, and Boris Flach. Path sample-analytic gradient estimators for stochastic binary networks. _Advances in neural information processing systems_, 2020.\n' +
      '* [218] Mingzhu Shen, Xianglong Liu, Ruihao Gong, and Kai Han. Balanced binary neural networks with gated residual. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 4197-4201. IEEE, 2020.\n' +
      '* [219] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-BERT: Hessian based ultra low precision quantization of bert. In _AAAI_, pages 8815-8821, 2020.\n' +
      '* [220] William Fleetwood Sheppard. On the calculation of the most probable values of frequency-constants, for data arranged according to equidistant division of a scale. _Proceedings of the London Mathematical Society_, 1(1):353-380, 1897.\n' +
      '* [221] Sungho Shin, Kyuyeon Hwang, and Wonyong Sung. Fixed-point performance analysis of recurrent neural networks. In _2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 976-980. IEEE, 2016.\n' +
      '* [222] Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yuri Nahshan, Alex Bronstein, and Uri Weiser. Robust quantization: One model to rule them all. _Advances in neural information processing systems_, 2020.\n' +
      '* [223] Gil Shomron, Freddy Gabbay, Samer Kurzum, and Uri Weiser. Post-training sparsity-aware quantization. _arXiv preprint arXiv:2105.11010_, 2021.\n' +
      '* [224] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In _International Conference on Learning Representations_, 2015.\n' +
      '* [225] S. M. Stigler. _The History of Statistics: The Measurement of Uncertainty before 1900_. Harvard University Press, Cambridge, 1986.\n' +
      '* [226] Pierre Stock, Angela Fan, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme model compression. In _International Conference on Learning Representations_, 2021.\n' +
      '* [227] Pierre Stock, Armand Joulin, Remi Gribonval, Benjamin Graham, and Herve Jegou. And the bit goes down: Revisiting the quantization of neural networks. _arXiv preprint arXiv:1907.05686_, 2019.\n' +
      '* [228] John Z Sun, Grace I Wang, Vivek K Goyal, and Lav R Varshney. A framework for bayesian optimality of psychophysical laws. _Journal of Mathematical Psychology_, 56(6):495-501, 2012.\n' +
      '* [229] Wonyong Sung, Sungho Shin, and Kyuyeon Hwang. Resiliency of deep neural networks under quantization. _arXiv preprint arXiv:1511.06488_, 2015.\n' +
      '* [230] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception architecture for computer vision. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2818-2826, 2016.\n' +
      '* [231] Shyam A Tailor, Javier Fernandez-Marques, and Nicholas D Lane. Degree-quant: Quantization-aware training for graph neural networks. _International Conference on Learning Representations_, 2021.\n' +
      '* [232] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2820-2828, 2019.\n' +
      '* [233] Mingxing Tan and Quoc V Le. EfficientNet: Rethinking model scaling for convolutional neural networks. _arXiv preprint arXiv:1905.11946_, 2019.\n' +
      '* [234] Wei Tang, Gang Hua, and Liang Wang. How to train a compact binary neural network with high accuracy? In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31, 2017.\n' +
      '* [235] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. _arXiv preprint arXiv:1703.01780_, 2017.\n' +
      '* [236] James Tee and Desmond P Taylor. Is information in the brain represented in continuous or discrete form? _IEEE Transactions on Molecular, Biological and Multi-Scale Communications_, 6(3):199-209, 2020.\n' +
      '* [237] L.N. Trefethen and D. Bau III. _Numerical Linear Algebra_. SIAM, Philadelphia, 1997.\n' +
      '* [238] Frederick Tung and Greg Mori. Clip-q: Deep network compression learning by in-parallel pruning-quantization. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 7873-7882, 2018.\n' +
      '* [239] Mart van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, and Max Welling. Bayesian bits: Unifying quantization and pruning. _Advances in neural information processing systems_, 2020.\n' +
      '* [240] Rufin VanRullen and Christof Koch. Is perception discrete or continuous? _Trends in cognitive sciences_, 7(5):207-213, 2003.\n' +
      '* [241] Lav R Varshney, Per Jesper Sjostrom, and Dmitri B Chklovskii. Optimal information storage in noisy synapses under resource constraints. _Neuron_, 52(3):409-423, 2006.\n' +
      '* [242] Lav R Varshney and Kush R Varshney. Decision making with quantized priors leads to discrimination. _Proceedings of the IEEE_, 105(2):241-255, 2016.\n' +
      '* [243] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems_, pages 5998-6008, 2017.\n' +
      '* [244] Diwen Wan, Fumin Shen, Li Liu, Fan Zhu, Jie Qin, Ling Shao, and Heng Tao Shen. Tbn: Convolutional neural network with ternary inputs and binary weights. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 315-332, 2018.\n' +
      '* [245] Dilin Wang, Meng Li, Chengyue Gong, and Vikas Chandra. Attentivenas: Improving neural architecture search via attentive sampling. _arXiv preprint arXiv:2011.09011_, 2020.\n' +
      '* [246] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: Hardware-aware automated quantization. _In Proceedings of the IEEE conference on computer vision and pattern recognition_, 2019.\n' +
      '* [247] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. _Advances in neural information processing systems_, 2018.\n' +
      '* [248] Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang, Yang Liu, and Jian Cheng. Two-step quantization for low-bit neural networks. In _Proceedings of the IEEE Conference on computer vision and pattern recognition_, pages 4376-4384, 2018.\n' +
      '* [249] Tianzhe Wang, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Hanrui Wang, Yujun Lin, and Song Han. Apq: Joint search for network architecture, pruning and quantization policy. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2078-2087, 2020.\n' +
      '* [250] Ying Wang, Yadong Lu, and Tijmen Blankevoort. Differentiable joint pruning and quantization for hardware efficiency. In _European Conference on Computer Vision_, pages 259-277. Springer, 2020.\n' +
      '* [251] Ziwei Wang, Jiwen Lu, Chenxin Tao, Jie Zhou, and Qi Tian. Learning channel-wise interactionsfor binary convolutional neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 568-577, 2019.\n' +
      '* [252] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. FBNet: Hardware-aware efficient convnet design via differentiable neural architecture search. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 10734-10742, 2019.\n' +
      '* [253] Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah Golmant, Amir Gholaminejad, Joseph Gonzalez, and Kurt Keutzer. Shift: A zero flop, zero parameter alternative to spatial convolutions. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 9127-9135, 2018.\n' +
      '* [254] Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt Keutzer. Mixed precision quantization of convnets via differentiable neural architecture search. _arXiv preprint arXiv:1812.00090_, 2018.\n' +
      '* [255] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for deep learning inference: Principles and empirical evaluation. _arXiv preprint arXiv:2004.09602_, 2020.\n' +
      '* [256] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized convolutional neural networks for mobile devices. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 4820-4828, 2016.\n' +
      '* [257] Xia Xiao, Zigeng Wang, and Sangthevar Rajasekaran. Autoprune: Automatic network pruning by regularizing auxiliary parameters. In _Advances in Neural Information Processing Systems_, pages 13681-13691, 2019.\n' +
      '* [258] Chen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. _arXiv preprint arXiv:1802.00150_, 2018.\n' +
      '* [259] Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun Liang, and Mingkui Tan. Generative low-bitwidth data free quantization. In _European Conference on Computer Vision_, pages 1-17. Springer, 2020.\n' +
      '* [260] Yinghao Xu, Xin Dong, Yudian Li, and Hao Su. A main/subsidiary network framework for simplifying binary neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7154-7162, 2019.\n' +
      '* [261] Zhe Xu and Ray CC Cheung. Accurate and compact convolutional neural networks with trained binarization. _arXiv preprint arXiv:1909.11366_, 2019.\n' +
      '* [262] Haichuan Yang, Shupeng Gui, Yuhao Zhu, and Ji Liu. Automatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2178-2188, 2020.\n' +
      '* [263] Huanrui Yang, Lin Duan, Yiran Chen, and Hai Li. Bsq: Exploring bit-level sparsity for mixed-precision neural network quantization. _arXiv preprint arXiv:2102.10462_, 2021.\n' +
      '* [264] Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, and Xian-sheng Hua. Quantization networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7308-7316, 2019.\n' +
      '* [265] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam. Netadapt: Platform-aware neural network adaptation for mobile applications. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 285-300, 2018.\n' +
      '* [266] Zhaohui Yang, Yunhe Wang, Kai Han, Chunjing Xu, Chao Xu, Dacheng Tao, and Chang Xu. Searching for low-bit weights in quantized neural networks. _Advances in neural information processing systems_, 2020.\n' +
      '* [267] Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael W Mahoney, et al. Hawqv3: Dyadic neural network quantization. _arXiv preprint arXiv:2011.10680_, 2020.\n' +
      '* [268] Jianming Ye, Shiliang Zhang, and Jingdong Wang. Distillation guided residual learning for binary convolutional neural networks. _arXiv preprint arXiv:2007.05223_, 2020.\n' +
      '* [269] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 4133-4141, 2017.\n' +
      '\n' +
      '* [270] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8715-8724, 2020.\n' +
      '* [271] Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Understanding straight-through estimator in training activation quantized neural nets. _arXiv preprint arXiv:1903.05662_, 2019.\n' +
      '* [272] Penghang Yin, Shuai Zhang, Jiancheng Lyu, Stanley Osher, Yingyong Qi, and Jack Xin. Blended coarse gradient descent for full quantization of deep neural networks. _Research in the Mathematical Sciences_, 6(1):14, 2019.\n' +
      '* [273] Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from multiple teacher networks. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1285-1294, 2017.\n' +
      '* [274] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han, Mingfei Gao, Ching-Yung Lin, and Larry S Davis. Nisp: Pruning networks using neuron importance score propagation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 9194-9203, 2018.\n' +
      '* [275] Shixing Yu, Zhewei Yao, Amir Gholami, Zhen Dong, Michael W Mahoney, and Kurt Keutzer. Hessian-aware pruning and optimal neural implant. _arXiv preprint arXiv:2101.08940_, 2021.\n' +
      '* [276] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In _European conference on computer vision (ECCV)_, 2018.\n' +
      '* [277] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3713-3722, 2019.\n' +
      '* [278] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit bert. _arXiv preprint arXiv:2009.12812_, 2020.\n' +
      '* [279] Chenglong Zhao, Bingbing Ni, Jian Zhang, Qiwei Zhao, Wenjun Zhang, and Qi Tian. Variational convolutional neural network pruning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 2780-2789, 2019.\n' +
      '* [280] Qibin Zhao, Masashi Sugiyama, Longhao Yuan, and Andrzej Cichocki. Learning efficient tensor representations with ring-structured networks. In _ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 8608-8612. IEEE, 2019.\n' +
      '* [281] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. _Proceedings of Machine Learning Research_, 2019.\n' +
      '* [282] Sijie Zhao, Tao Yue, and Xuemei Hu. Distribution-aware adaptive multi-bit quantization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9281-9290, 2021.\n' +
      '* [283] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless cnns with low-precision weights. _arXiv preprint arXiv:1702.03044_, 2017.\n' +
      '* [284] Aojun Zhou, Anbang Yao, Kuan Wang, and Yurong Chen. Explicit loss-error-aware quantization for low-bit deep neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 9426-9435, 2018.\n' +
      '* [285] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. _arXiv preprint arXiv:1606.06160_, 2016.\n' +
      '* [286] Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal Frossard. Adaptive quantization for deep neural network. _arXiv preprint arXiv:1712.01048_, 2017.\n' +
      '* [287] Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. _arXiv preprint arXiv:1612.01064_, 2016.\n' +
      '* [288] Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network or more networks per bit? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4923-4932, 2019.\n' +
      '* [289] Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Towards effective low-bitwidth convolutional neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 7920-7928, 2018.\n' +
      '* [290] Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Structured binary neural networks for accurate image classification and semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 413-422, 2019.\n' +
      '* [291] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. _arXiv preprint arXiv:1611.01578_, 2016.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
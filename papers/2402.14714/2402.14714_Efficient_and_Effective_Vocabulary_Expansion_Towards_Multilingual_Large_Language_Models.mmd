# Efficient and Effective Vocabulary Expansion

Towards Multilingual Large Language Models

Seungduk Kim1  Seungtaek Choi1  Myeongho Jeong

Yanolja, South Korea

{seungduk.kim, seungtaek.choi, myeongho.jeong}@yaonolja.com

Equal Contribution.

Footnote 1: [https://huggingface.co/yanolja/EEVE-Korean-10.88-v1.0](https://huggingface.co/yanolja/EEVE-Korean-10.88-v1.0)

###### Abstract

This report introduces EEVE-Korean-v1.0, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model EEVE-Korean-10.88-v1.0 ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face's leaderboard. We open-source our models on Huggingface to empower the open research community in various languages.

## 1 Introduction

Recent advancements in the field of large language models (LLMs), such as GPT-4 [2], Gemini [14], and Claude [1], have demonstrated remarkable capabilities in processing and understanding multiple languages. On the other hand, though notable models in open source community, such as LLaMA [15, 16], MPT [15], Falcon [11], Mistral [18], Mistral [18], SOLAR [19], and Phi-1.5 [10] have set benchmarks in English tasks, these developments have predominantly favored English, leading to a performance gap in non-English languages.

Such disparity can be found not only in their language proficiency but also in computational efficiency, where non-English languages like Korean require significantly more tokens than English even for equivalent semantic content (Figure 1). And, of course, this negatively affects the user experiences, such as longer response times, shorter context lengths, and higher API costs [23]. Expanding the tokenizer vocabulary, which introduces some frequently used yet long words as additional tokens, is thus indispensable for non-English users, but vocabulary expansion is a very challenging task because new embeddings require trillions of training tokens [18].

To this end, this technical report presents a novel approach for efficient and effective vocabulary expansion, namely EEVE, which can better train the embeddings of newly added tokens. For ease of adaptation, we utilize subword-based embedding initialization and design seven training stages with parameter freezing, which elaborately adjust the order and amount of parameters to be trained. We meticulously transfer the advanced capabilities of foundational models from English to Korean by initially focusing on the training of only input embeddings and progressively expanding to encompass the full parameters in the final stage.

Using EEVE, we officially release a family of Korean LLMs, EEVE-Korean-10.88-v1.01 and EEVE-Korean-2.88-v1.02, which are built on recent English-centric LLMs, specifically SOLAR-10.7B [15] and Phi-2 [10], with further Korean-centric pre-training. We evaluate our models on lm-evaluation-harness3[1] for both English and Korean language tasks, such

[MISSING_PAGE_FAIL:2]

will be further discussed.

### Multi-stage Training

Here we describe the nuanced approach of our seven-stage training methodology for efficient vocabulary expansion, emphasizing the meticulous process of integrating new tokens derived from languages beyond the initial English-centric training scope.

**Stage 1 (new input embeddings):** Initially, our focus is narrow yet critical: to learn the input embeddings of the newly added tokens while freezing all other model parameters. This stage is foundational, allowing the model to adjust its recognition and processing of these tokens from the beginning. The pre-initialized embeddings serve as a starting point, guiding the model to better utilize these new tokens in its existing framework. Our principal hypothesis here is that if the input and output token sequences in causal language modeling can be differentiated, by utilizing both the old and new tokenizers at the same time, the model can more efficiently and effectively learn new vocabulary embeddings, as it could leverage its established knowledge in the embedding spaces from old tokens. However, employing distinct tokenizers for input and output sequences at once poses implementation challenges, such as the difficulty of applying teacher forcing due to mismatched input/output sequences. Here, the subword-based embedding initialization (Sec 2.2) provides a proxy for using the old tokenizer for output sequences, such that the model is tasked to generate the subword token (old) given the whole word token (new). In other words, the model could learn to align their representations for generating the new token with that for generating its first subword token, by optimizing only the input embeddings without any modification of input/output token sequences as described in Figure 2. However, at this stage, the model is not yet able to distinguish between tokens sharing the same hidden state.

**Stage 2 (new output embeddings):** Our goal is to enhance the model's proficiency in accurately generating new tokens across various contexts by solely adjusting the output embeddings (lm_head). The decision to freeze all other parameters stems from the model's current unstable state. Allowing both input and output embeddings to be trained simultaneously would complicate achieving convergence, thus hindering the model's progress toward optimal performance. By freezing most of the parameters, we achieve more stable convergence. Moreover, this approach significantly reduces the training time, as it eliminates the necessity for back-propagation through the other layers.

**Stage 3 (new input and output embeddings):** At this stage, the input embeddings (embed_tokens) still remain optimized based on the initial embeddings of the output embeddings. This stage allows

Figure 1: Training stages with parameter freezing. The fire and snowflake emojis indicate the trainable and frozen parameters respectively.

[MISSING_PAGE_FAIL:4]

kens to represent them, but our new tokenizer can do so with almost half, using only 1.6B tokens. This difference becomes even more pronounced in the case of Phi-2 and EEVE-Korean-2.88 models, where they require 5.6B tokens and 1.6B tokens respectively. Considering that transformers have a quadratic-increasing computation complexity with respect to token length, this can be interpreted in two significant ways. First, it allows for processing sequences more than 4 times longer on the same GPU. Or second, it means our model can be trained nearly 4 times more computationally efficiently on the same dataset. This difference becomes even more pronounced in the case of the Phi-2 and EEVE-Korean-2.8B tokenizers.

For _fine-tuning_ of EEVE-Korean models, we employed the Direct Preference Optimization (DPO; Rafailov et al.2023) based on LLaM-Factory implementation. To further enhance the models' capabilities of following Korean instructions, we translated the publicly available instruction datasets, specifically Orca5(Mukherjee et al., 2023; Lian et al., 2023) and UltraFeedback6(Cui et al., 2023) into Korean. In the process of translating these datasets into Korean, ensuring the integrity of programming code formats and correcting translation errors, such as instances where both the source and target languages were inadvertently translated into Korean, was crucial for maintaining the quality and effectiveness of our fine-tuned models. We named the fine-tuned models as EEVE-Korean-Instruct.

Footnote 5: [https://huggingface.co/datasets/Open-Orca/slimOrca-bedup](https://huggingface.co/datasets/Open-Orca/slimOrca-bedup)

Footnote 6: [https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)

### Training

As foundational architectures, we opt for SOLAR-10.7B (Kim et al., 2023) and Phi-2 (Li et al., 2023), because both have shown outstanding performances among similar sizes of LLMs. This choice of foundational architectures aligns with our strategic training objectives, leveraging their proven strengths to ensure our new models achieve similar levels of language understanding and reasoning capabilities in Korean.

For the training of the model variants, we utilized two distinct codebases: Axolot7 for the initial pre-training phase and LLaMA-Factory8(hiyouga, 2023) for subsequent fine-tuning. These codebases provided a strong and reliable base for our training process.

Footnote 7: [https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)

Footnote 8: [https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

Specifically, we train our models with a setup of 8 x NVIDIA H100 GPUs with 80GB memory each, utilizing 64 CPU cores. For EEVE-Korean-10.8B-v1.0, under bf16 precision, the training process is configured with a sequence of length 4096, gradient accumulation steps set to 4, and a micro-batch size of 8, whereas EEVE-Korean-2.8B-v1.0 adopts a sequence length of 2048, gradient accumulation of 16, and a micro-batch size of 16. We employ the AdamW (Loshchilov and Hutter, 2018) optimizer, paired with a cosine learning rate scheduler that includes a warmup phase of 10 steps. The learning rate for the 10.8B variant is set to 4e-5, while we used 2e-4 for the small model. We continued training at each stage until the loss converged, observing the loss converged before reaching 400 global steps, which signifies the efficiency of our training strategy. Though our training strategy involves 7 different stages, it is noteworthy that, for our 2.8B variant, the overall pre-training can be done in less than two days as optimizing only the output embeddings doesn't incur much computation.

## 4 Evaluations

We evaluate our models on both Korean and English LLM benchmarks, to highlight the advantages of our vocabulary expansion method, which could efficiently leverage the strong multilingual capabilities of base foundational models. Desirably, we expect a model to show improved performance in Korean tasks and comparable performance in English tasks.

### Benchmarks

For Korean tasks, we adopt the KoBEST benchmark (Jang et al., 2022), whose tasks are designed

\begin{table}
\begin{tabular}{l|l|l} \hline \hline Model & Total (tokens) & Average (tokens) \\ \hline SOLAR-10.7B & 3.1B & 964 \\ EEVE-Korean-10.8B-v1.0 & 1.6B & 500 \\ \hline Phi-2 & 5.6B & 1748 \\ EEVE-Korean-2.8B-v1.0 & 1.6B & 484 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of tokenizers for our 6.7GB pre-training corpus of a total 3.2M documents.

to evaluate the various aspects of language understanding and reasoning. Specifically, this benchmark provides a Korean-translated version of language understanding tasks: boolean question answering [1], commonsense causal reasoning (COPA; Roemmele et al.2011, context-sensitive word understanding (WiC; Pilehvar and Camacho-Collados2019), commonsense reasoning [11], and sentiment negation recognition (SentiNeg). For English tasks, we employ the following original tasks of KoBEST, BoolQ, COPA, and HelaSwag, which can better highlight the alignment between the English and Korean capabilities of LLMs. To ensure consistent comparisons, we employ an open-source LLM evaluation framework, lm-evaluation-harness9[7].

Footnote 9: [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)

### Results

We now present evaluation results for both our EEVE-Korean and EEVE-Korean-Instruct variants with other top-performing models in Table 3. EEVE-Korean-10.8B-v1.0 outperforms other pre-trained models of similar sizes in the average performance. It is noteworthy that, EEVE-Korean is the only case where the performance in Korean is improved without compromising the performance in English. For example, though OPEN-SOLAR-KO-10.7B, which is built on the same base model as ours, performs slightly better than our EEVE-Korean-Instruct-10.8B-v1.0, it fails to preserve the English capabilities, showing lower performance in English tasks than its base model, SOLAR-10.7B-v1.0. We observe similar trends even for our smaller model, EEVE-Korean-2.8B-v1.0 in comparison with the phi-2-ko-v0.1 model, sharing Phi-2 as its base model. This demonstrates the effectiveness of our training strategy, especially considering that we used even fewer training tokens than our competitors.

Notably, but not surprisingly, preference tuning on English datasets even makes the models underperform in Korean tasks. For example, LLaMA-2-chat variants, which are the preference-tuned version of LLaMA-2 checkpoints, show improved performances in English tasks (Llama-2-7b 0.7774 \(\rightarrow\) LLama-2-7b-chat 0.7976 in English BoolQ), while underperforming in Korean tasks (Llama-2-7b 0.5242 \(\rightarrow\) LLama-2-7b-chat 0.5157 in Korean BoolQ), which highlights the importance of Korean-specific training for LLMs. On the other hand, we observe that preference tuning our models on Korean instruction datasets doesn't hurt the model performance in English tasks, rather even improving it. We posit that it is because the embedding spaces are already well-aligned between Korean and English tokens, thus fine-tuning on a specific language doesn't incur a significant change in model parameters.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Types} & \multicolumn{4}{c}{English} & \multicolumn{4}{c}{Korean} & \multirow{2}{*}{**Avg.**} \\ \cline{3-3} \cline{5-10}  & & & \multicolumn{1}{c}{} & & & & & & & & \\ \cline{3-10}  & & & \multicolumn{1}{c}{} & & & & & & & & \\ \hline meta-llama/Llama-2-7b-hf & PT & 0.7774 & 0.8700 & 0.5714 & 0.5242 & 0.5700 & 0.4420 & 0.4610 & 0.4881 & 0.5880 \\ meta-llama/Llama-2-13b-hf & PT & 0.8055 & 0.9100 & 0.6006 & 0.5214 & 0.6010 & 0.4380 & 0.5038 & 0.4881 & 0.6086 \\ mistralA/Mistral-7B-v0.1 & PT & 0.8379 & 0.9200 & 0.6129 & 0.6282 & 0.5880 & 0.4300 & 0.5365 & 0.4881 & 0.6302 \\ meta-llama/Llama-2-7b-chat-hf & FT & 0.7976 & 0.8700 & 0.5779 & 0.5157 & 0.5530 & 0.4160 & 0.4987 & 0.4881 & 0.5896 \\ meta-llama/Llama-2-13b-chat-hf & FT & 0.8165 & 0.8800 & 0.6072 & 0.5057 & 0.5760 & 0.4040 & 0.4685 & 0.4881 & 0.5933 \\ \hline upstage/SOLAR-10.7B-v1.0 (base) & PT & 0.8257 & 0.8700 & 0.6393 & 0.5057 & 0.5750 & 0.4320 & 0.6146 & 0.4881 & 0.6188 \\ upstage/SOLAR-10.7B-Instruct-v1.0 & FT & **0.8853** & **0.9400** & **0.6866** & 0.8184 & 0.6370 & 0.4560 & 0.5668 & 0.4921 & 0.6853 \\ boomi/OPEN-SOLAR-KO-10.7B\({}^{*}\) & PT & 0.8187 & 0.8800 & 0.5570 & 0.8355 & **0.8010** & **0.5040** & 0.6952 & 0.4897 & 0.6976 \\ yanoj1/EEVE-Korean-10.8B-v1.0\({}^{*}\) & PT & 0.8492 & 0.9000 & 0.6203 & 0.8568 & 0.7530 & 0.4900 & 0.6675 & **0.4992** & 0.7045 \\ yanoj1/EEVE-Korean-10.8B-v1.0\({}^{*}\) & FT & 0.8810 & 0.9300 & 0.6502 & **0.8860** & 0.7610 & 0.4700 & **0.9521** & 0.4937 & **0.7530** \\ \hline microsoft/Phi-2 (base) & PT & **0.8336** & **0.9000** & **0.5583** & 0.5021 & 0.4770 & 0.3280 & 0.5063 & 0.4881 & 0.5742 \\ dacehum-ml/pli-2.ko-v0.1\({}^{*}\) & PT & 0.6141 & 0.5800 & 0.3257 & 0.5164 & **0.6100** & **0.3860** & 0.4484 & 0.4881 & 0.4961 \\ yanoj1/EEVE-Korean-2.8B-v1.0\({}^{*}\) & PT & 0.7404 & 0.8900 & 0.5247 & 0.5299 & 0.5820 & 0.3800 & 0.5164 & 0.4881 & 0.5814 \\ yanoj1/EEVE-Korean-Instruct-2.8B-v1.0\({}^{*}\) & FT & 0.8248 & 0.8700 & 0.5392 & **0.7066** & 0.5640 & 0.3660 & **0.5290** & **0.5230** & **0.6153** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Main evaluation results based on lm-evaluation-harness. The dataset names are abbreviated for brevity: BQ for BoolQ, CP for COPA, HS for HellaSwag, and SN for SentiNeg. Korean tasks are from [7]. Accuracy (acc) is used as the evaluation metric for all tasks. In the ‘Types’ column, the models are categorized into two groups: pre-trained (PT) and fine-tuned (FT). We denote the models trained in Korean datasets with \({}^{*}\). For ease of reproduction, we adopt their official names at HuggingFace.

## 5 Conclusion & Future Work

The report introduces EEVE-Korean-v1.0, a Korean adaptation of large language models that utilizes an Efficient and Effective Vocabulary Expansion (EEVE) method to enhance Korean text processing capabilities significantly. The method, based on parameter freezing and subword initialization, enables the EEVE-Korean-10.8B-v1.0 model to excel in Korean language tasks while maintaining strong English capabilities. Achieved with a corpus of just 2 billion tokens, this approach represents a notable advancement in language model training efficiency and effectiveness. By making these models available to the research community, the project aims to contribute to the development of more inclusive and efficient language processing technologies.

Expanding our vision, future efforts will explore the application of our vocabulary expansion methodology to additional languages, assessing its generalizability and effectiveness. We aim to not only extend the EEVE-Korean model's linguistic range but also to delve deeper into evaluating its reasoning and generative capabilities through diverse tasks, including complex mathematical reasoning tests like GSM8K Cobbe et al. (2021), and human evaluations in interactive settings like chatbots Zheng et al. (2023). Moreover, efforts to enhance pre-training data quality, and to analyze performance in code-switching scenarios Zhang et al. (2023) will underpin our commitment to refining the model's robustness and versatility. These initiatives are designed to broaden the model's applicability and efficacy, pushing the boundaries of what is achievable with advanced language models.

## References

* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open language models. _arXiv preprint arXiv:2311.16867_.
* Anthropic (2023) Anthropic. 2023. Model card and evaluations for claude models. _Anthropic technical Report_.
* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 2924-2936.
* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_.
* Cui et al. (2023) Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback. _arXiv preprint arXiv:2310.01377_.
* Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muenninghoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation.
* Hewitt (2021) John Hewitt. 2021. Initializing new word embeddings for pretrained language models.
* hiyouga (2023) hiyouga. 2023. Llama factory. [https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory).
* Jang et al. (2022) Myeongjun Jang, Dohyung Kim, Deuk Sin Kwon, and Eric Davis. 2022. Kobest: Korean balanced evaluation of significant tasks. In _Proceedings of the 29th International Conference on Computational Linguistics_, pages 3697-3708.
* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. _arXiv preprint arXiv:2310.06825_.
* Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mistral of experts. _arXiv preprint arXiv:2401.04088_.
* Kim et al. (2023) Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. 2023. Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. _arXiv preprint arXiv:2312.15166_.
* Kim et al. (2021) Ildoo Kim, Gunsoo Han, Jiyeon Ham, and Woonhyuk Baek. 2021. Kogpt: Kakaobrain korean(hangul) generative pretrained transformer.
* Ko et al. (2023) Hyunwoong Ko, Kichang Yang, Minho Ryu, Taekyoon Choi, Seungmu Yang, Sungho Park, et al. 2023. A technical report for polyglot-ko: Open-source large-scale korean language models. _arXiv preprint arXiv:2306.02254_.
* Kog et al. (2021)L. Junbum (2024)Solar-ko-10.7b. External Links: Link Cited by: SS1.
* H. Li, T. Lan, Z. Fu, D. Cai, L. Liu, N. Collier, T. Watanabe, and Y. Su (2023)Repetition in repetition out: towards understanding neural text degeneration from the data perspective. In Thirty-seventh Conference on Neural Information Processing Systems, Cited by: SS1.
* Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee (2023)Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463. Cited by: SS1.
* W. Lian, G. Wang, B. Goodson, E. Pentland, A. Cook, C. Vong, T. Yun, and N. Hoos (2023)Slimorca dedup: a dedupicated subset of slimorca. Cited by: SS1.
* I. Loshchilov and F. Hutter (2018)Decoupled weight decay regularization. In International Conference on Learning Representations, Cited by: SS1.
* S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah (2023)Orca: progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707. Cited by: SS1.
* O. Al (2023)Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Cited by: SS1.
* C. Park, H. Lee, H. Park, H. Kim, S. Kim, S. Cho, S. Kim, and S. Lee (2023)Open ko-llm leaderboard. Cited by: SS1.
* A. Petrov, E. La Malfa, P. H. Torr, and A. Bibi (2023)Language model tokenizers introduce unfairness between languages. arXiv preprint arXiv:2305.15425. Cited by: SS1.
* M. Taher Pilehvar and J. Camacho-Collados (2019)Wic: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1267-1273. Cited by: SS1.
* R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn (2023)Direct preference optimization: your language model is secretly a reward model. arXiv preprint arXiv:2305.18290. Cited by: SS1.
* M. Roemmele, C. A. Bejan, and A. S. Gordon (2011)Choice of plausible alternatives: an evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, Cited by: SS1.
* G. Team, R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. (2023)Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Cited by: SS1.
* M. NLP Team et al. (2023)Introducing mpt-30b: raising the bar for open-source foundation models. Cited by: SS1.
* H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. (2023)LIMA: open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Cited by: SS1.
* H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. (2023)LIMA 2: open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Cited by: SS1.
* C. Welch, R. Mihalcea, and J. K. Kummerfeld (2020)Improving low compute language modeling with in-domain embedding initialisation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8625-8634. Cited by: SS1.
* R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi (2019)Hellaswag: can a machine really finish your sentence?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791-4800. Cited by: SS1.
* R. Zhang, S. Cahyawijaya, J. Christian Blaise Cruz, and A. Fikri Aji (2023)Multilingual large language models are not (yet) code-switchers. arXiv preprint arXiv:2305.14235. Cited by: SS1.
* J. Zhao, Z. Zhang, Q. Zhang, T. Gui, and X. Huang (2024)LIMA beyond english: an empirical study on language capability transfer. arXiv preprint arXiv:2401.01055. Cited by: SS1.
* L. Zheng, W. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, et al. (2023)Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685. Cited by: SS1.
<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models</title>
<!--Generated on Thu Feb 22 17:11:50 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.14714v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2402.14714v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2402.14714v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2402.14714v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S1" title="1 Introduction ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S2" title="2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Efficient and Effective Vocabulary Expansion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S2.SS1" title="2.1 Preliminary 1: Tokenizer Training ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Preliminary 1: Tokenizer Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S2.SS2" title="2.2 Preliminary 2: Subword-based Embeddings Initialization ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Preliminary 2: Subword-based Embeddings Initialization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S2.SS3" title="2.3 Multi-stage Training ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Multi-stage Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S3" title="3 Implementation Details ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S3.SS1" title="3.1 Datasets ‣ 3 Implementation Details ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S3.SS2" title="3.2 Training ‣ 3 Implementation Details ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S4" title="4 Evaluations ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S4.SS1" title="4.1 Benchmarks ‣ 4 Evaluations ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S4.SS2" title="4.2 Results ‣ 4 Evaluations ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S5" title="5 Conclusion &amp; Future Work ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion &amp; Future Work</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
<li>failed: kotex</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2402.14714v1 [cs.CL] 22 Feb 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Efficient and Effective Vocabulary Expansion 
<br class="ltx_break">Towards Multilingual Large Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
 Seungduk Kim
 Seungtaek Choi<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
 <span class="ltx_text ltx_font_bold" id="id1.1.id1">Myeongho Jeong</span>
<br class="ltx_break">Yanolja, South Korea 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">{seungduk.kim, seungtaek.choi, myeongho.jeong}@yanolja.com</span>
<br class="ltx_break">
</span><span class="ltx_author_notes"> Equal Contribution.</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id3.id1">This report introduces <span class="ltx_text ltx_font_typewriter" id="id3.id1.1">EEVE-Korean-v1.0</span>, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization.
In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens.
Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model <span class="ltx_text ltx_font_typewriter" id="id3.id1.2">EEVE-Korean-10.8B-v1.0</span> ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face’s leaderboard.
We open-source our models on Huggingface to empower the open research community in various languages.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Efficient and Effective Vocabulary Expansion 
<br class="ltx_break">Towards Multilingual Large Language Models</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break">
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1" style="width:433.6pt;"><span class="ltx_text" id="p2.1.1.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.1.1.1.1.1.1">
 Seungduk Kim<span class="ltx_note ltx_role_thanks" id="p2.1.1.1.1.1.1.1.1.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span> Equal Contribution.</span></span></span>
 Seungtaek Choi<span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex2.1.1.1">1</span></span></span></span></span>
 Myeongho Jeong</span></span></span>
<span class="ltx_tr" id="p2.1.1.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.2.2.1">Yanolja, South Korea</span></span>
<span class="ltx_tr" id="p2.1.1.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.1.1.1.3.3.1.1">{seungduk.kim, seungtaek.choi, myeongho.jeong}@yanolja.com</span></span></span>
</span>
</span></span> </span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent advancements in the field of large language models (LLMs), such as GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib21" title="">2023</a>)</cite>, Gemini&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Team et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib27" title="">2023a</a>)</cite>, and Claude&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Anthropic (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib2" title="">2023</a>)</cite>, have demonstrated remarkable capabilities in processing and understanding multiple languages.
On the other hand, though notable models in open source community, such as LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib29" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib30" title="">b</a>)</cite>, MPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Team et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib28" title="">2023b</a>)</cite>, Falcon&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Almazrouei et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib1" title="">2023</a>)</cite>, Mistral&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib10" title="">2023</a>)</cite>, Mixtral&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib11" title="">2024</a>)</cite>, SOLAR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kim et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib12" title="">2023</a>)</cite>, and Phi-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib17" title="">2023b</a>)</cite> have set benchmarks in English tasks, these developments have predominantly favored English, leading to a performance gap in non-English languages.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Such disparity can be found not only in their language proficiency but also in computational efficiency, where non-English languages like Korean require significantly more tokens than English even for equivalent semantic content (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S2.T1" title="Table 1 ‣ 2.1 Preliminary 1: Tokenizer Training ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>).
And, of course, this negatively affects the user experiences, such as longer response times, shorter context lengths, and higher API costs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Petrov et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib23" title="">2023</a>)</cite>.
Expanding the tokenizer vocabulary, which introduces some frequently used yet long words as additional tokens, is thus indispensable for non-English users, but vocabulary expansion is a very challenging task because new embeddings require trillions of training tokens&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib34" title="">2024</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To this end, this technical report presents a novel approach for <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">e</span>fficient and <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">e</span>ffective <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">v</span>ocabulary <span class="ltx_text ltx_font_bold" id="S1.p3.1.4">e</span>xpansion, namely EEVE, which can better train the embeddings of newly added tokens.
For ease of adaptation, we utilize subword-based embedding initialization and design seven training stages with parameter freezing, which elaborately adjust the order and amount of parameters to be trained.
We meticulously transfer the advanced capabilities of foundational models from English to Korean by initially focusing on the training of only input embeddings and progressively expanding to encompass the full parameters in the final stage.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Using <span class="ltx_text ltx_font_typewriter" id="S1.p4.1.1">EEVE</span>, we officially release a family of Korean LLMs, <span class="ltx_text ltx_font_typewriter" id="S1.p4.1.2">EEVE-Korean-10.8B-v1.0<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote1.1.1.1">1</span></span><a class="ltx_ref ltx_url" href="https://huggingface.co/yanolja/EEVE-Korean-10.8B-v1.0" title="">https://huggingface.co/yanolja/EEVE-Korean-10.8B-v1.0</a></span></span></span></span> and <span class="ltx_text ltx_font_typewriter" id="S1.p4.1.3">EEVE-Korean-2.8B-v1.0<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote2.1.1.1">2</span></span><a class="ltx_ref ltx_url" href="https://huggingface.co/yanolja/EEVE-Korean-2.8B-v1.0" title="">https://huggingface.co/yanolja/EEVE-Korean-2.8B-v1.0</a></span></span></span></span>, which are built on recent English-centric LLMs, specifically SOLAR-10.7B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kim et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib12" title="">2023</a>)</cite> and Phi-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib17" title="">2023b</a>)</cite>, with further Korean-centric pre-training.
We evaluate our models on <span class="ltx_text ltx_font_typewriter" id="S1.p4.1.4">lm-evaluation-harness<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote3.1.1.1">3</span></span><a class="ltx_ref ltx_url" href="https://github.com/EleutherAI/lm-evaluation-harness" title="">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib6" title="">2023</a>)</cite> for both English and Korean language tasks, such as boolean question answering (BoolQ;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Clark et&nbsp;al. <a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib3" title="">2019</a></cite>), commonsense causal reasoning (COPA;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Roemmele et&nbsp;al. <a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib26" title="">2011</a></cite>, context-sensitive word understanding (WiC;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Pilehvar and Camacho-Collados <a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib24" title="">2019</a></cite>), commonsense reasoning (HellaSwag;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Zellers et&nbsp;al. <a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib32" title="">2019</a></cite>), and sentiment negation recognition (SentiNeg).
From the evaluation, we observe that our models outperform the recent open Korean pre-trained LLMs like OPEN-SOLAR-KO-10.7B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">L. Junbum (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib15" title="">2024</a>)</cite>, Polyglot-Ko&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Ko et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib14" title="">2023</a>)</cite>, and KoGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kim et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib13" title="">2021</a>)</cite>, while preserving the strong English capability of the base English-centric LLMs in terms of benchmark performance, being ranked as the leading Korean pre-trained model in Open Ko-LLM Leaderboard&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Park et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib22" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Efficient and Effective Vocabulary Expansion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">To address the challenge of efficiently extending English-centric Language Models (LLMs) to include non-English languages, we introduce a novel methodology for vocabulary expansion. This method combines parameter freezing with subword-based embedding initialization to effectively incorporate and adapt to new linguistic tokens from languages beyond its initial training scope, thereby enhancing its applicability across various linguistic contexts. Our approach outlines a structured seven-stage training process, as illustrated in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S2.F1" title="Figure 1 ‣ 2.1 Preliminary 1: Tokenizer Training ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, meticulously designed to effectively integrate new tokens into the model’s vocabulary. During pre-training, our objective is causal language modeling.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Our core assumption is that foundational models, having been extensively trained in English texts, possess a substantial level of understanding and reasoning capabilities. Transferring these capabilities from English to another language, such as Korean, could be more efficient than developing performance from standalone Korean pre-training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Preliminary 1: Tokenizer Training</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S2.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.18" style="width:398.9pt;height:142.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.1pt,7.5pt) scale(0.90437232719092,0.90437232719092) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.18.18">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.18.18.19.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.18.18.19.1.1" style="width:411.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.18.18.19.1.1.1">English (<span class="ltx_text ltx_font_italic" id="S2.T1.18.18.19.1.1.1.1">8 tokens</span>)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.18.18.20.2">
<td class="ltx_td ltx_align_justify" id="S2.T1.18.18.20.2.1" style="width:411.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.18.18.20.2.1.1">“<span class="ltx_text ltx_font_italic" id="S2.T1.18.18.20.2.1.1.1">Hello, the weather is nice today.</span>”</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.18.18.21.3">
<td class="ltx_td ltx_align_justify" id="S2.T1.18.18.21.3.1" style="width:411.9pt;"><span class="ltx_text ltx_font_typewriter ltx_align_top" id="S2.T1.18.18.21.3.1.1">[‘_Hello’, ‘,’, ‘_the’, ‘_weather’, ‘_is’, ‘_nice’, ‘_today’, ‘.’]</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.18.18.22.4">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.18.18.22.4.1" style="width:411.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.18.18.22.4.1.1">Korean (<span class="ltx_text ltx_font_italic" id="S2.T1.18.18.22.4.1.1.1">26 tokens</span>)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.18.18.23.5">
<td class="ltx_td ltx_align_justify" id="S2.T1.18.18.23.5.1" style="width:411.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.18.18.23.5.1.1">“<span class="ltx_text ltx_font_italic" id="S2.T1.18.18.23.5.1.1.1">안녕하세요, 오늘은 날씨가 좋네요.</span>”</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.18.18.18">
<td class="ltx_td ltx_align_justify" id="S2.T1.18.18.18.18" style="width:411.9pt;"><span class="ltx_text ltx_font_typewriter ltx_align_top" id="S2.T1.18.18.18.18.18.18.18">[‘_’, ‘안’, ‘<math alttext="<" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.1.m1.1b"><lt id="S2.T1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.1.1.1.m1.1d">&lt;</annotation></semantics></math>0xEB<math alttext=">" class="ltx_Math" display="inline" id="S2.T1.2.2.2.2.2.2.2.m2.1"><semantics id="S2.T1.2.2.2.2.2.2.2.m2.1a"><mo id="S2.T1.2.2.2.2.2.2.2.m2.1.1" mathvariant="normal" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.2.2.2.m2.1b"><gt id="S2.T1.2.2.2.2.2.2.2.m2.1.1.cmml" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.2.2.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.2.2.2.2.m2.1d">&gt;</annotation></semantics></math>’, ‘<math alttext="<" class="ltx_Math" display="inline" id="S2.T1.3.3.3.3.3.3.3.m3.1"><semantics id="S2.T1.3.3.3.3.3.3.3.m3.1a"><mo id="S2.T1.3.3.3.3.3.3.3.m3.1.1" mathvariant="normal" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.3.3.3.3.m3.1b"><lt id="S2.T1.3.3.3.3.3.3.3.m3.1.1.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.3.3.3.3.m3.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.3.3.3.3.3.m3.1d">&lt;</annotation></semantics></math>0x85<math alttext=">" class="ltx_Math" display="inline" id="S2.T1.4.4.4.4.4.4.4.m4.1"><semantics id="S2.T1.4.4.4.4.4.4.4.m4.1a"><mo id="S2.T1.4.4.4.4.4.4.4.m4.1.1" mathvariant="normal" xref="S2.T1.4.4.4.4.4.4.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.4.4.4.4.m4.1b"><gt id="S2.T1.4.4.4.4.4.4.4.m4.1.1.cmml" xref="S2.T1.4.4.4.4.4.4.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.4.4.4.4.m4.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.4.4.4.4.4.4.m4.1d">&gt;</annotation></semantics></math>’, ‘<math alttext="<" class="ltx_Math" display="inline" id="S2.T1.5.5.5.5.5.5.5.m5.1"><semantics id="S2.T1.5.5.5.5.5.5.5.m5.1a"><mo id="S2.T1.5.5.5.5.5.5.5.m5.1.1" mathvariant="normal" xref="S2.T1.5.5.5.5.5.5.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.5.5.5.5.m5.1b"><lt id="S2.T1.5.5.5.5.5.5.5.m5.1.1.cmml" xref="S2.T1.5.5.5.5.5.5.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.5.5.5.5.m5.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.5.5.5.5.5.5.m5.1d">&lt;</annotation></semantics></math>0x95<math alttext=">" class="ltx_Math" display="inline" id="S2.T1.6.6.6.6.6.6.6.m6.1"><semantics id="S2.T1.6.6.6.6.6.6.6.m6.1a"><mo id="S2.T1.6.6.6.6.6.6.6.m6.1.1" mathvariant="normal" xref="S2.T1.6.6.6.6.6.6.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.6.6.6.6.6.m6.1b"><gt id="S2.T1.6.6.6.6.6.6.6.m6.1.1.cmml" xref="S2.T1.6.6.6.6.6.6.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.6.6.6.6.6.m6.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.6.6.6.6.6.6.6.m6.1d">&gt;</annotation></semantics></math>’, ‘하’, ‘세’, ‘요’, ‘,’, ‘_’, ‘오’, ‘<math alttext="<" class="ltx_Math" display="inline" id="S2.T1.7.7.7.7.7.7.7.m7.1"><semantics id="S2.T1.7.7.7.7.7.7.7.m7.1a"><mo id="S2.T1.7.7.7.7.7.7.7.m7.1.1" mathvariant="normal" xref="S2.T1.7.7.7.7.7.7.7.m7.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.7.7.7.7.7.m7.1b"><lt id="S2.T1.7.7.7.7.7.7.7.m7.1.1.cmml" xref="S2.T1.7.7.7.7.7.7.7.m7.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.7.7.7.7.7.m7.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.7.7.7.7.7.7.7.m7.1d">&lt;</annotation></semantics></math>0xEB<math alttext=">" class="ltx_Math" display="inline" id="S2.T1.8.8.8.8.8.8.8.m8.1"><semantics id="S2.T1.8.8.8.8.8.8.8.m8.1a"><mo id="S2.T1.8.8.8.8.8.8.8.m8.1.1" mathvariant="normal" xref="S2.T1.8.8.8.8.8.8.8.m8.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.8.8.8.8.8.m8.1b"><gt id="S2.T1.8.8.8.8.8.8.8.m8.1.1.cmml" xref="S2.T1.8.8.8.8.8.8.8.m8.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.8.8.8.8.8.m8.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.8.8.8.8.8.8.8.m8.1d">&gt;</annotation></semantics></math>’, ‘<math alttext="<" class="ltx_Math" display="inline" id="S2.T1.9.9.9.9.9.9.9.m9.1"><semantics id="S2.T1.9.9.9.9.9.9.9.m9.1a"><mo id="S2.T1.9.9.9.9.9.9.9.m9.1.1" mathvariant="normal" xref="S2.T1.9.9.9.9.9.9.9.m9.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.9.9.9.9.9.m9.1b"><lt id="S2.T1.9.9.9.9.9.9.9.m9.1.1.cmml" xref="S2.T1.9.9.9.9.9.9.9.m9.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.9.9.9.9.9.m9.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.9.9.9.9.9.9.9.m9.1d">&lt;</annotation></semantics></math>0x8A<math alttext=">" class="ltx_Math" display="inline" id="S2.T1.10.10.10.10.10.10.10.m10.1"><semantics id="S2.T1.10.10.10.10.10.10.10.m10.1a"><mo id="S2.T1.10.10.10.10.10.10.10.m10.1.1" mathvariant="normal" xref="S2.T1.10.10.10.10.10.10.10.m10.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.10.10.10.10.10.10.10.m10.1b"><gt id="S2.T1.10.10.10.10.10.10.10.m10.1.1.cmml" xref="S2.T1.10.10.10.10.10.10.10.m10.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.10.10.10.10.10.10.m10.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.10.10.10.10.10.10.10.m10.1d">&gt;</annotation></semantics></math>’, ‘<math alttext="<" class="ltx_Math" display="inline" id="S2.T1.11.11.11.11.11.11.11.m11.1"><semantics id="S2.T1.11.11.11.11.11.11.11.m11.1a"><mo id="S2.T1.11.11.11.11.11.11.11.m11.1.1" mathvariant="normal" xref="S2.T1.11.11.11.11.11.11.11.m11.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.11.11.11.11.11.11.11.m11.1b"><lt id="S2.T1.11.11.11.11.11.11.11.m11.1.1.cmml" xref="S2.T1.11.11.11.11.11.11.11.m11.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.11.11.11.11.11.11.m11.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.11.11.11.11.11.11.11.m11.1d">&lt;</annotation></semantics></math>0x98<math alttext=">" class="ltx_Math" display="inline" id="S2.T1.12.12.12.12.12.12.12.m12.1"><semantics id="S2.T1.12.12.12.12.12.12.12.m12.1a"><mo id="S2.T1.12.12.12.12.12.12.12.m12.1.1" mathvariant="normal" xref="S2.T1.12.12.12.12.12.12.12.m12.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.12.12.12.12.12.12.12.m12.1b"><gt id="S2.T1.12.12.12.12.12.12.12.m12.1.1.cmml" xref="S2.T1.12.12.12.12.12.12.12.m12.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.12.12.12.12.12.12.m12.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.12.12.12.12.12.12.12.m12.1d">&gt;</annotation></semantics></math>’, ‘은’, ‘_’, ‘날’, ‘<math alttext="<" class="ltx_Math" display="inline" id="S2.T1.13.13.13.13.13.13.13.m13.1"><semantics id="S2.T1.13.13.13.13.13.13.13.m13.1a"><mo id="S2.T1.13.13.13.13.13.13.13.m13.1.1" mathvariant="normal" xref="S2.T1.13.13.13.13.13.13.13.m13.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.13.13.13.13.13.13.13.m13.1b"><lt id="S2.T1.13.13.13.13.13.13.13.m13.1.1.cmml" xref="S2.T1.13.13.13.13.13.13.13.m13.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.13.13.13.13.13.13.13.m13.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.13.13.13.13.13.13.13.m13.1d">&lt;</annotation></semantics></math>0xEC<math alttext=">" class="ltx_Math" display="inline" id="S2.T1.14.14.14.14.14.14.14.m14.1"><semantics id="S2.T1.14.14.14.14.14.14.14.m14.1a"><mo id="S2.T1.14.14.14.14.14.14.14.m14.1.1" mathvariant="normal" xref="S2.T1.14.14.14.14.14.14.14.m14.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.14.14.14.14.14.14.14.m14.1b"><gt id="S2.T1.14.14.14.14.14.14.14.m14.1.1.cmml" xref="S2.T1.14.14.14.14.14.14.14.m14.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.14.14.14.14.14.14.14.m14.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.14.14.14.14.14.14.14.m14.1d">&gt;</annotation></semantics></math>’, ‘<math alttext="<" class="ltx_Math" display="inline" id="S2.T1.15.15.15.15.15.15.15.m15.1"><semantics id="S2.T1.15.15.15.15.15.15.15.m15.1a"><mo id="S2.T1.15.15.15.15.15.15.15.m15.1.1" mathvariant="normal" xref="S2.T1.15.15.15.15.15.15.15.m15.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.15.15.15.15.15.15.15.m15.1b"><lt id="S2.T1.15.15.15.15.15.15.15.m15.1.1.cmml" xref="S2.T1.15.15.15.15.15.15.15.m15.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.15.15.15.15.15.15.15.m15.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.15.15.15.15.15.15.15.m15.1d">&lt;</annotation></semantics></math>0x94<math alttext=">" class="ltx_Math" display="inline" id="S2.T1.16.16.16.16.16.16.16.m16.1"><semantics id="S2.T1.16.16.16.16.16.16.16.m16.1a"><mo id="S2.T1.16.16.16.16.16.16.16.m16.1.1" mathvariant="normal" xref="S2.T1.16.16.16.16.16.16.16.m16.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.16.16.16.16.16.16.16.m16.1b"><gt id="S2.T1.16.16.16.16.16.16.16.m16.1.1.cmml" xref="S2.T1.16.16.16.16.16.16.16.m16.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.16.16.16.16.16.16.16.m16.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.16.16.16.16.16.16.16.m16.1d">&gt;</annotation></semantics></math>’, ‘<math alttext="<" class="ltx_Math" display="inline" id="S2.T1.17.17.17.17.17.17.17.m17.1"><semantics id="S2.T1.17.17.17.17.17.17.17.m17.1a"><mo id="S2.T1.17.17.17.17.17.17.17.m17.1.1" mathvariant="normal" xref="S2.T1.17.17.17.17.17.17.17.m17.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.17.17.17.17.17.17.17.m17.1b"><lt id="S2.T1.17.17.17.17.17.17.17.m17.1.1.cmml" xref="S2.T1.17.17.17.17.17.17.17.m17.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.17.17.17.17.17.17.17.m17.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.17.17.17.17.17.17.17.m17.1d">&lt;</annotation></semantics></math>0xA8<math alttext=">" class="ltx_Math" display="inline" id="S2.T1.18.18.18.18.18.18.18.m18.1"><semantics id="S2.T1.18.18.18.18.18.18.18.m18.1a"><mo id="S2.T1.18.18.18.18.18.18.18.m18.1.1" mathvariant="normal" xref="S2.T1.18.18.18.18.18.18.18.m18.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.18.18.18.18.18.18.18.m18.1b"><gt id="S2.T1.18.18.18.18.18.18.18.m18.1.1.cmml" xref="S2.T1.18.18.18.18.18.18.18.m18.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.18.18.18.18.18.18.18.m18.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S2.T1.18.18.18.18.18.18.18.m18.1d">&gt;</annotation></semantics></math>’, ‘가’, ‘_’, ‘좋’, ‘네’, ‘요’, ‘.’]</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.18.18.24.6">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.18.18.24.6.1" style="width:411.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.18.18.24.6.1.1">Expanded Tokenizer (<span class="ltx_text ltx_font_italic" id="S2.T1.18.18.24.6.1.1.1">9 tokens</span>)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.18.18.25.7">
<td class="ltx_td ltx_align_justify ltx_border_b" id="S2.T1.18.18.25.7.1" style="width:411.9pt;"><span class="ltx_text ltx_font_typewriter ltx_align_top" id="S2.T1.18.18.25.7.1.1">[‘_안’, ‘녕’, ‘하세요’, ‘,’, ‘_오늘은’, ‘_날씨가’, ‘_좋’, ‘네요’, ‘.’]</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>A comparison of token consumption between English and Korean. We used the tokenizers of SOLAR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kim et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib12" title="">2023</a>)</cite> and our <span class="ltx_text ltx_font_typewriter" id="S2.T1.20.1">EEVE-Korean-10.8B-v1.0</span>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">We trained a new tokenizer on our Korean corpus. Since our goal is to maximize the leverage of the base model’s performance, we maintained the base model’s vocabulary and added 8,960 tokens from the corpus that appeared at least 6,000 times, prioritizing those with the highest frequency. Ultimately, the tokenizer’s vocabulary expanded to 40,960 tokens for <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.1.1">EEVE-Korean-10.8B-v1.0</span>. This process entailed several rounds of tokenizer training and a manual curation of tokens, based on an analysis of token frequency, ensuring a comprehensive and relevant vocabulary for our model.
As shown in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S2.T1" title="Table 1 ‣ 2.1 Preliminary 1: Tokenizer Training ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, the overall token consumption for Korean texts is significantly improved, almost three-fold, contributing to the reduction of computational costs during the entire training process.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4" id="S2.F0.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="142" id="S2.F0.sf1.g1" src="extracted/5424172/figure_latex/figures/figure-stages-0.png" width="117">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Stage 0</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4" id="S2.F0.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="142" id="S2.F0.sf2.g1" src="extracted/5424172/figure_latex/figures/figure-stages-1.png" width="134">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Stage 1</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4" id="S2.F0.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="142" id="S2.F0.sf3.g1" src="extracted/5424172/figure_latex/figures/figure-stages-2.png" width="134">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Stage 2</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4" id="S2.F0.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="142" id="S2.F0.sf4.g1" src="extracted/5424172/figure_latex/figures/figure-stages-3.png" width="134">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Stage 3</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4" id="S2.F0.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="142" id="S2.F0.sf5.g1" src="extracted/5424172/figure_latex/figures/figure-stages-4.png" width="134">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>Stage 4</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4" id="S2.F0.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="142" id="S2.F0.sf6.g1" src="extracted/5424172/figure_latex/figures/figure-stages-5.png" width="134">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>Stage 5</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4" id="S2.F0.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="142" id="S2.F0.sf7.g1" src="extracted/5424172/figure_latex/figures/figure-stages-6.png" width="134">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(g) </span>Stage 6</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure class="ltx_figure ltx_flex_size_4" id="S2.F0.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="142" id="S2.F0.sf8.g1" src="extracted/5424172/figure_latex/figures/figure-stages-7.png" width="134">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(h) </span>Stage 7</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Training stages with parameter freezing. The fire and snowflake emojis indicate the trainable and frozen parameters respectively.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Preliminary 2: Subword-based Embeddings Initialization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The integration process starts before actual training, introducing new input and output embeddings, called <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p1.1.1">embed_tokens</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p1.1.2">lm_head</span>, to the model’s parameters. This preliminary step is crucial for preparing for the sophisticated learning process that follows.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">For the input embeddings of the newly added tokens, we adopt the approach of using the average embeddings of the subword tokens that make up these new tokens as in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Hewitt (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib7" title="">2021</a>); Welch et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib31" title="">2020</a>)</cite>. This method utilizes the semantic richness of the model’s existing subword embeddings to offer a meaningful starting point for the new tokens’ representations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Conversely, the output embeddings for the newly added tokens are initialized with the embeddings of the first subword token that comprises the new token. This strategy aims to align the new tokens’ output representations closely with the semantic characteristics of their constituent subwords, enabling a smoother integration into the model’s predictive framework. The significance of such initialization will be further discussed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Multi-stage Training</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Here we describe the nuanced approach of our seven-stage training methodology for efficient vocabulary expansion, emphasizing the meticulous process of integrating new tokens derived from languages beyond the initial English-centric training scope.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="498" id="S2.F2.g1" src="extracted/5424172/figure_latex/figures/figure-subword.png" width="449">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An illustrative example of showing how our subword-based embedding initialization enables harmonize the old and new tokens at Stage 1. In Stage 1, the output embeddings of newly added tokens are initialized with the output embeddings of their first subword tokens that make up these new tokens, such that the last hidden representation for predicting “하세요” yields the same logits for the newly added token “하세요” with its first subword token “하”. Even if we give the new token “하세요” as a gold token, the gradients are eventually computed based on its subword token “하”, so the model takes the input embeddings of “하세요” to predict its subword “하”.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.1">Stage 1 (new input embeddings):</span> Initially, our focus is narrow yet critical: to learn the input embeddings of the newly added tokens while freezing all other model parameters. This stage is foundational, allowing the model to adjust its recognition and processing of these tokens from the beginning. The pre-initialized embeddings serve as a starting point, guiding the model to better utilize these new tokens in its existing framework. Our principal hypothesis here is that if the input and output token sequences in causal language modeling can be differentiated, by utilizing both the old and new tokenizers at the same time, the model can more efficiently and effectively learn new vocabulary embeddings, as it could leverage its established knowledge in the embedding spaces from old tokens. However, employing distinct tokenizers for input and output sequences at once poses implementation challenges, such as the difficulty of applying teacher forcing due to mismatched input/output sequences. Here, the subword-based embedding initialization (Sec&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S2.SS2" title="2.2 Preliminary 2: Subword-based Embeddings Initialization ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_tag">2.2</span></a>) provides a proxy for using the old tokenizer for output sequences, such that the model is tasked to generate the subword token (old) given the whole word token (new).
In other words, the model could learn to align their representations for generating the new token with that for generating its first subword token, by optimizing only the input embeddings without any modification of input/output token sequences as described in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S2.F2" title="Figure 2 ‣ 2.3 Multi-stage Training ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>.
However, at this stage, the model is not yet able to distinguish between tokens sharing the same hidden state.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.1">Stage 2 (new output embeddings):</span>
Our goal is to enhance the model’s proficiency in accurately generating new tokens across various contexts by solely adjusting the output embeddings (<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p3.1.2">lm_head</span>). The decision to freeze all other parameters stems from the model’s current unstable state. Allowing both input and output embeddings to be trained simultaneously would complicate achieving convergence, thus hindering the model’s progress toward optimal performance. By freezing most of the parameters, we achieve more stable convergence. Moreover, this approach significantly reduces the training time, as it eliminates the necessity for backpropagation through the other layers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p4.1.1">Stage 3 (new input and output embeddings):</span>
At this stage, the input embeddings (<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p4.1.2">embed_tokens</span>) still remain optimized based on the initial embeddings of the output embeddings.
This stage allows the updates of both input and output embeddings of the newly added tokens simultaneously.
By aligning between input and output embeddings, the model learns to use the new tokens in both understanding and prediction.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p5.1.1">Stage 4 (all output embeddings):</span>
As all the original parameters of the base model were frozen until this stage, we assumed the logits between old and new tokenizers were differently scaled, or less optimized to be used as a whole vocabulary. To this end, we begin to allow the update of the old parameters, specifically the output embeddings of old tokens here, making the model better generate the new tokens. In our preliminary experiments, we found this stage is critical for improving the model’s generative capabilities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p6">
<p class="ltx_p" id="S2.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p6.1.1">Stage 5 (new input and all output embeddings):</span> At this stage, the training extends to fine-tuning all output embeddings across the model’s vocabulary while continuing to refine the input embeddings for the newly added tokens. The goal is to ensure that the model can accurately predict any token within its expanded vocabulary. This phase emphasizes the integration of new tokens within the broader context of the model’s linguistic understanding, ensuring that they are both well-represented as inputs and accurately generated as outputs. This dual focus aids in harmonizing the model’s overall performance, ensuring that the expanded vocabulary is seamlessly woven into its language generation processes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p7">
<p class="ltx_p" id="S2.SS3.p7.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p7.1.1">Stage 6 (all layers):</span> Contrary to being the final phase, this stage represents an advanced step in the vocabulary expansion process, where all model parameters are subject to optimization, including both newly introduced and pre-existing ones. The focus here is on integrating the enhancements made to the embedding layers within the model’s overall parameters. Techniques such as QLoRA are utilized not just for efficiency but to ensure the preservation of the model’s strong capabilities as much as possible, while allowing effective integration of the expanded vocabulary.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p8">
<p class="ltx_p" id="S2.SS3.p8.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p8.1.1">Stage 7 (internal layers):</span> Following the extensive integration and optimization efforts, this stage serves as a “cool down” phase, focusing on updating the model’s internal layers, which includes all the layers except the input and output embedding layers.
The objective is to ensure that the enhancements made during the vocabulary expansion are deeply embedded within the model’s core processing capabilities.
This phase prepares the model for robust performance, ensuring that it not only recognizes and generates the new tokens but does so with a nuanced understanding of their use in varied linguistic contexts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Implementation Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">For <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">pre-training</span>, we curated publicly available Korean corpora from diverse sources, such as Korean web content, English vocabulary, and parallel corpus in Korean AI Hub<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aihub.or.kr/" title="">https://aihub.or.kr/</a></span></span></span>, etc.
To construct a high-quality pre-training corpus, we applied a set of preprocessing rules: 1) perplexity-based filtering, 2) n-gram repetition&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib16" title="">2023a</a>)</cite>-based filtering, and 3) stopword-based filtering.
For the efficient training of newly added Korean tokens, we intentionally filtered out documents that do not contain many of these tokens.
Subsequently, we acquired a pre-training corpus totaling 3.2M documents (or, 6.7GB).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:398.9pt;height:237.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(104.3pt,-62.1pt) scale(2.09503048839611,2.09503048839611) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.1.1.2.1">
<tbody><tr class="ltx_tr" id="S3.T2.1.1.1.1.2.1.1">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.2.1.1.1">Total</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.1.1.2.1.2">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.2.1.2.1">(tokens)</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1.1.1.3.1">
<tbody><tr class="ltx_tr" id="S3.T2.1.1.1.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.3.1.1.1">Average</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.1.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1.3.1.2.1">(tokens)</td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1.1">SOLAR-10.7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1.2">3.1B</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.2.1.3">964</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.3.2.1">EEVE-Korean-10.8B-v1.0</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.3.2.2">1.6B</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.3.2.3">500</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.4.3.1">Phi-2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.4.3.2">5.6B</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.4.3.3">1748</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T2.1.1.5.4.1">EEVE-Korean-2.8B-v1.0</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T2.1.1.5.4.2">1.6B</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.1.1.5.4.3">484</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of tokenizers for our 6.7GB pre-training corpus of a total 3.2M documents.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">As can be seen in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S3.T2" title="Table 2 ‣ 3.1 Datasets ‣ 3 Implementation Details ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>, for the entire corpus, the SOLAR tokenizer needed to use 3.1B tokens to represent them, but our new tokenizer can do so with almost half, using only 1.6B tokens. This difference becomes even more pronounced in the case of Phi-2 and <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.1">EEVE-Korean-2.8B</span> models, where they require 5.6B tokens and 1.6B tokens respectively.
Considering that transformers have a quadratic-increasing computation complexity with respect to token length, this can be interpreted in two significant ways. First, it allows for processing sequences more than 4 times longer on the same GPU. Or second, it means our model can be trained nearly 4 times more computationally efficiently on the same dataset. This difference becomes even more pronounced in the case of the Phi-2 and <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.2">EEVE-Korean-2.8B</span> tokenizers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">For <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.1">fine-tuning</span> of <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p3.1.2">EEVE-Korean</span> models, we employed the Direct Preference Optimization (DPO;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Rafailov et&nbsp;al. <a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib25" title="">2023</a></cite>) based on LLaMA-Factory implementation.
To further enhance the models’ capabilities of following Korean instructions, we translated the publicly available instruction datasets, specifically Orca<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup" title="">https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup</a></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Mukherjee et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib20" title="">2023</a>); Lian et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib18" title="">2023</a>)</cite> and UltraFeedback<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned" title="">https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned</a></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Cui et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib5" title="">2023</a>)</cite> into Korean.
In the process of translating these datasets into Korean, ensuring the integrity of programming code formats and correcting translation errors, such as instances where both the source and target languages were inadvertently translated into Korean, was crucial for maintaining the quality and effectiveness of our fine-tuned models.
We named the fine-tuned models as <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p3.1.3">EEVE-Korean-Instruct</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">As foundational architectures, we opt for SOLAR-10.7B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kim et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib12" title="">2023</a>)</cite> and Phi-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib17" title="">2023b</a>)</cite>, because both have shown outstanding performances among similar sizes of LLMs. This choice of foundational architectures aligns with our strategic training objectives, leveraging their proven strengths to ensure our new models achieve similar levels of language understanding and reasoning capabilities in Korean.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">For the training of the model variants, we utilized two distinct codebases: Axolotl<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenAccess-AI-Collective/axolotl" title="">https://github.com/OpenAccess-AI-Collective/axolotl</a></span></span></span> for the initial pre-training phase and LLaMA-Factory<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hiyouga/LLaMA-Factory" title="">https://github.com/hiyouga/LLaMA-Factory</a></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">hiyouga (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib8" title="">2023</a>)</cite> for subsequent fine-tuning. These codebases provided a strong and reliable base for our training process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Specifically, we train our models with a setup of 8 x NVIDIA H100 GPUs with 80GB memory each, utilizing 64 CPU cores.
For <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.1">EEVE-Korean-10.8B-v1.0</span>, under bf16 precision, the training process is configured with a sequence of length 4096, gradient accumulation steps set to 4, and a micro-batch size of 8, whereas <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.2">EEVE-Korean-2.8B-v1.0</span> adopts a sequence length of 2048, gradient accumulation of 16, and a micro-batch size of 16. We employ the AdamW&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib19" title="">2018</a>)</cite> optimizer, paired with a cosine learning rate scheduler that includes a warmup phase of 10 steps. The learning rate for the 10.8B variant is set to 4e-5, while we used 2e-4 for the small model. We continued training at each stage until the loss converged, observing the loss converged before reaching 400 global steps, which signifies the efficiency of our training strategy.
Though our training strategy involves 7 different stages, it is noteworthy that, for our 2.8B variant, the overall pre-training can be done in less than two days as optimizing only the output embeddings doesn’t incur much computation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.6" style="width:424.9pt;height:282.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.5pt,14.9pt) scale(0.904308479588229,0.904308479588229) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.6.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.6.6.7.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.6.6.7.1.1" rowspan="2"><span class="ltx_text" id="S4.T3.6.6.7.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.6.6.7.1.2" rowspan="2"><span class="ltx_text" id="S4.T3.6.6.7.1.2.1">Types</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T3.6.6.7.1.3">English</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T3.6.6.7.1.4">Korean</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.6.6.7.1.5" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.7.1.5.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.8.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.8.2.1"><span class="ltx_text" id="S4.T3.6.6.8.2.1.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.6.6.8.2.1.1.1">
<span class="ltx_tr" id="S4.T3.6.6.8.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.1.1.1.1.1">BQ</span></span>
<span class="ltx_tr" id="S4.T3.6.6.8.2.1.1.1.2">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.1.1.1.2.1">(0)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.8.2.2"><span class="ltx_text" id="S4.T3.6.6.8.2.2.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.6.6.8.2.2.1.1">
<span class="ltx_tr" id="S4.T3.6.6.8.2.2.1.1.1">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.2.1.1.1.1">CP</span></span>
<span class="ltx_tr" id="S4.T3.6.6.8.2.2.1.1.2">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.2.1.1.2.1">(0)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.8.2.3"><span class="ltx_text" id="S4.T3.6.6.8.2.3.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.6.6.8.2.3.1.1">
<span class="ltx_tr" id="S4.T3.6.6.8.2.3.1.1.1">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.3.1.1.1.1">HS</span></span>
<span class="ltx_tr" id="S4.T3.6.6.8.2.3.1.1.2">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.3.1.1.2.1">(0)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.8.2.4"><span class="ltx_text" id="S4.T3.6.6.8.2.4.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.6.6.8.2.4.1.1">
<span class="ltx_tr" id="S4.T3.6.6.8.2.4.1.1.1">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.4.1.1.1.1">BQ</span></span>
<span class="ltx_tr" id="S4.T3.6.6.8.2.4.1.1.2">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.4.1.1.2.1">(0)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.8.2.5"><span class="ltx_text" id="S4.T3.6.6.8.2.5.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.6.6.8.2.5.1.1">
<span class="ltx_tr" id="S4.T3.6.6.8.2.5.1.1.1">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.5.1.1.1.1">CP</span></span>
<span class="ltx_tr" id="S4.T3.6.6.8.2.5.1.1.2">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.5.1.1.2.1">(0)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.8.2.6"><span class="ltx_text" id="S4.T3.6.6.8.2.6.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.6.6.8.2.6.1.1">
<span class="ltx_tr" id="S4.T3.6.6.8.2.6.1.1.1">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.6.1.1.1.1">HS</span></span>
<span class="ltx_tr" id="S4.T3.6.6.8.2.6.1.1.2">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.6.1.1.2.1">(0)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.8.2.7"><span class="ltx_text" id="S4.T3.6.6.8.2.7.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.6.6.8.2.7.1.1">
<span class="ltx_tr" id="S4.T3.6.6.8.2.7.1.1.1">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.7.1.1.1.1">SN</span></span>
<span class="ltx_tr" id="S4.T3.6.6.8.2.7.1.1.2">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.7.1.1.2.1">(0)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.8.2.8"><span class="ltx_text" id="S4.T3.6.6.8.2.8.1">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.6.6.8.2.8.1.1">
<span class="ltx_tr" id="S4.T3.6.6.8.2.8.1.1.1">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.8.1.1.1.1">WIC</span></span>
<span class="ltx_tr" id="S4.T3.6.6.8.2.8.1.1.2">
<span class="ltx_td ltx_align_center" id="S4.T3.6.6.8.2.8.1.1.2.1">(0)</span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.9.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.6.6.9.3.1">meta-llama/Llama-2-7b-hf</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.9.3.2">PT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.9.3.3">0.7774</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.9.3.4">0.8700</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.9.3.5">0.5714</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.9.3.6">0.5242</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.9.3.7">0.5700</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.9.3.8">0.4420</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.9.3.9">0.4610</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.9.3.10">0.4881</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.9.3.11">0.5880</td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.10.4">
<td class="ltx_td ltx_align_left" id="S4.T3.6.6.10.4.1">meta-llama/Llama-2-13b-hf</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.10.4.2">PT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.10.4.3">0.8055</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.10.4.4">0.9100</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.10.4.5">0.6006</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.10.4.6">0.5214</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.10.4.7">0.6010</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.10.4.8">0.4380</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.10.4.9">0.5038</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.10.4.10">0.4881</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.10.4.11">0.6086</td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.11.5">
<td class="ltx_td ltx_align_left" id="S4.T3.6.6.11.5.1">mistralai/Mistral-7B-v0.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.11.5.2">PT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.11.5.3">0.8379</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.11.5.4">0.9200</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.11.5.5">0.6129</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.11.5.6">0.6282</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.11.5.7">0.5880</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.11.5.8">0.4300</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.11.5.9">0.5365</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.11.5.10">0.4881</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.11.5.11">0.6302</td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.12.6">
<td class="ltx_td ltx_align_left" id="S4.T3.6.6.12.6.1">meta-llama/Llama-2-7b-chat-hf</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.12.6.2">FT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.12.6.3">0.7976</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.12.6.4">0.8700</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.12.6.5">0.5779</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.12.6.6">0.5157</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.12.6.7">0.5530</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.12.6.8">0.4160</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.12.6.9">0.4987</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.12.6.10">0.4881</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.12.6.11">0.5896</td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.13.7">
<td class="ltx_td ltx_align_left" id="S4.T3.6.6.13.7.1">meta-llama/Llama-2-13b-chat-hf</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.13.7.2">FT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.13.7.3">0.8165</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.13.7.4">0.8800</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.13.7.5">0.6072</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.13.7.6">0.5057</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.13.7.7">0.5760</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.13.7.8">0.4040</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.13.7.9">0.4685</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.13.7.10">0.4881</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.13.7.11">0.5933</td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.14.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.6.6.14.8.1">upstage/SOLAR-10.7B-v1.0 (base)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.14.8.2">PT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.14.8.3">0.8257</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.14.8.4">0.8700</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.14.8.5">0.6393</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.14.8.6">0.5057</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.14.8.7">0.5750</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.14.8.8">0.4320</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.14.8.9">0.6146</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.14.8.10">0.4881</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.14.8.11">0.6188</td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.15.9">
<td class="ltx_td ltx_align_left" id="S4.T3.6.6.15.9.1">upstage/SOLAR-10.7B-Instruct-v1.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.15.9.2">FT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.15.9.3"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.15.9.3.1">0.8853</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.15.9.4"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.15.9.4.1">0.9400</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.15.9.5"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.15.9.5.1">0.6866</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.15.9.6">0.8184</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.15.9.7">0.6370</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.15.9.8">0.4560</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.15.9.9">0.5668</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.15.9.10">0.4921</td>
<td class="ltx_td ltx_align_center" id="S4.T3.6.6.15.9.11">0.6853</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.1.1">beomi/OPEN-SOLAR-KO-10.7B<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><msup id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.1.m1.1.1a" xref="S4.T3.1.1.1.1.m1.1.1.cmml"></mi><mo id="S4.T3.1.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"><times id="S4.T3.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.2">PT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.3">0.8187</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.4">0.8800</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.5">0.5570</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.6">0.8355</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.7.1">0.8010</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.8.1">0.5040</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.9">0.6952</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.10">0.4897</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.11">0.6976</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2">
<td class="ltx_td ltx_align_left" id="S4.T3.2.2.2.1">yanolja/EEVE-Korean-10.8B-v1.0<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.m1.1"><semantics id="S4.T3.2.2.2.1.m1.1a"><msup id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml"><mi id="S4.T3.2.2.2.1.m1.1.1a" xref="S4.T3.2.2.2.1.m1.1.1.cmml"></mi><mo id="S4.T3.2.2.2.1.m1.1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><apply id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1"><times id="S4.T3.2.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.2">PT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.3">0.8492</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.4">0.9000</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.5">0.6203</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.6">0.8568</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.7">0.7530</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.8">0.4900</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.9">0.6675</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.10"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.10.1">0.4992</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2.11">0.7045</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.3">
<td class="ltx_td ltx_align_left" id="S4.T3.3.3.3.1">yanolja/EEVE-Korean-Instruct-10.8B-v1.0<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T3.3.3.3.1.m1.1"><semantics id="S4.T3.3.3.3.1.m1.1a"><msup id="S4.T3.3.3.3.1.m1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.cmml"><mi id="S4.T3.3.3.3.1.m1.1.1a" xref="S4.T3.3.3.3.1.m1.1.1.cmml"></mi><mo id="S4.T3.3.3.3.1.m1.1.1.1" xref="S4.T3.3.3.3.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.m1.1b"><apply id="S4.T3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1"><times id="S4.T3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.2">FT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.3">0.8810</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.4">0.9300</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.5">0.6502</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.6"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.6.1">0.8860</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.7">0.7610</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.8">0.4700</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.9"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.9.1">0.9521</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.10">0.4937</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.11"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.11.1">0.7530</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.16.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.6.6.16.10.1">microsoft/Phi-2 (base)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.16.10.2">PT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.16.10.3"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.16.10.3.1">0.8336</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.16.10.4"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.16.10.4.1">0.9000</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.16.10.5"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.16.10.5.1">0.5583</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.16.10.6">0.5021</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.16.10.7">0.4770</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.16.10.8">0.3280</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.16.10.9">0.5063</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.16.10.10">0.4881</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.16.10.11">0.5742</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.4">
<td class="ltx_td ltx_align_left" id="S4.T3.4.4.4.1">daekeun-ml/phi-2-ko-v0.1<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T3.4.4.4.1.m1.1"><semantics id="S4.T3.4.4.4.1.m1.1a"><msup id="S4.T3.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.1.m1.1.1.cmml"><mi id="S4.T3.4.4.4.1.m1.1.1a" xref="S4.T3.4.4.4.1.m1.1.1.cmml"></mi><mo id="S4.T3.4.4.4.1.m1.1.1.1" xref="S4.T3.4.4.4.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b"><apply id="S4.T3.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1"><times id="S4.T3.4.4.4.1.m1.1.1.1.cmml" xref="S4.T3.4.4.4.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.2">PT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.3">0.6141</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.4">0.5800</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.5">0.3257</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.6">0.5164</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.7"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.4.7.1">0.6100</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.8"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.4.8.1">0.3860</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.9">0.4484</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.10">0.4881</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.11">0.4961</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.5.5">
<td class="ltx_td ltx_align_left" id="S4.T3.5.5.5.1">yanolja/EEVE-Korean-2.8B-v1.0<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T3.5.5.5.1.m1.1"><semantics id="S4.T3.5.5.5.1.m1.1a"><msup id="S4.T3.5.5.5.1.m1.1.1" xref="S4.T3.5.5.5.1.m1.1.1.cmml"><mi id="S4.T3.5.5.5.1.m1.1.1a" xref="S4.T3.5.5.5.1.m1.1.1.cmml"></mi><mo id="S4.T3.5.5.5.1.m1.1.1.1" xref="S4.T3.5.5.5.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.m1.1b"><apply id="S4.T3.5.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1"><times id="S4.T3.5.5.5.1.m1.1.1.1.cmml" xref="S4.T3.5.5.5.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.2">PT</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.3">0.7404</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.4">0.8900</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.5">0.5247</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.6">0.5299</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.7">0.5820</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.8">0.3800</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.9">0.5164</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.10">0.4881</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5.11">0.5814</td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.6.6.6.1">yanolja/EEVE-Korean-Instruct-2.8B-v1.0<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T3.6.6.6.1.m1.1"><semantics id="S4.T3.6.6.6.1.m1.1a"><msup id="S4.T3.6.6.6.1.m1.1.1" xref="S4.T3.6.6.6.1.m1.1.1.cmml"><mi id="S4.T3.6.6.6.1.m1.1.1a" xref="S4.T3.6.6.6.1.m1.1.1.cmml"></mi><mo id="S4.T3.6.6.6.1.m1.1.1.1" xref="S4.T3.6.6.6.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.1.m1.1b"><apply id="S4.T3.6.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.6.1.m1.1.1"><times id="S4.T3.6.6.6.1.m1.1.1.1.cmml" xref="S4.T3.6.6.6.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.6.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.2">FT</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.3">0.8248</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.4">0.8700</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.5">0.5392</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.6"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.6.6.1">0.7066</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.7">0.5640</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.8">0.3660</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.9"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.6.9.1">0.5290</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.10"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.6.10.1">0.5230</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.6.6.6.11"><span class="ltx_text ltx_font_bold" id="S4.T3.6.6.6.11.1">0.6153</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Main evaluation results based on <span class="ltx_text ltx_font_typewriter" id="S4.T3.11.1">lm-evaluation-harness</span>. The dataset names are abbreviated for brevity: BQ for BoolQ, CP for COPA, HS for HellaSwag, and SN for SentiNeg. Korean tasks are from&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib9" title="">2022</a>)</cite>. Accuracy (<span class="ltx_text ltx_font_typewriter" id="S4.T3.12.2">acc</span>) is used as the evaluation metric for all tasks. In the ‘Types’ column, the models are categorized into two groups: pre-trained (PT) and fine-tuned (FT). We denote the models trained in Korean datasets with <math alttext="{}^{*}" class="ltx_Math" display="inline" id="S4.T3.8.m1.1"><semantics id="S4.T3.8.m1.1b"><msup id="S4.T3.8.m1.1.1" xref="S4.T3.8.m1.1.1.cmml"><mi id="S4.T3.8.m1.1.1b" xref="S4.T3.8.m1.1.1.cmml"></mi><mo id="S4.T3.8.m1.1.1.1" xref="S4.T3.8.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S4.T3.8.m1.1c"><apply id="S4.T3.8.m1.1.1.cmml" xref="S4.T3.8.m1.1.1"><times id="S4.T3.8.m1.1.1.1.cmml" xref="S4.T3.8.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.m1.1d">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.m1.1e">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>. For ease of reproduction, we adopt their official names at HuggingFace.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate our models on both Korean and English LLM benchmarks, to highlight the advantages of our vocabulary expansion method, which could efficiently leverage the strong multilingual capabilities of base foundational models. Desirably, we expect a model to show improved performance in Korean tasks and comparable performance in English tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Benchmarks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">For Korean tasks, we adopt the KoBEST benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib9" title="">2022</a>)</cite>, whose tasks are designed to evaluate the various aspects of language understanding and reasoning. Specifically, this benchmark provides a Korean-translated version of language understanding tasks: boolean question answering (BoolQ;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Clark et&nbsp;al. <a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib3" title="">2019</a></cite>), commonsense causal reasoning (COPA;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Roemmele et&nbsp;al. <a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib26" title="">2011</a></cite>, context-sensitive word understanding (WiC;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Pilehvar and Camacho-Collados <a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib24" title="">2019</a></cite>), commonsense reasoning (HellaSwag;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Zellers et&nbsp;al. <a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib32" title="">2019</a></cite>), and sentiment negation recognition (SentiNeg).
For English tasks, we employ the following original tasks of KoBEST, BoolQ, COPA, and HellaSwag, which can better highlight the alignment between the English and Korean capabilities of LLMs.
To ensure consistent comparisons, we employ an open-source LLM evaluation framework, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.1.1">lm-evaluation-harness<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote9.1.1.1">9</span></span><a class="ltx_ref ltx_url" href="https://github.com/EleutherAI/lm-evaluation-harness" title="">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib6" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We now present evaluation results for both our <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.1">EEVE-Korean</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.2">EEVE-Korean-Instruct</span> variants with other top-performing models in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#S4.T3" title="Table 3 ‣ 4 Evaluations ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>.
<span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.3">EEVE-Korean-10.8B-v1.0</span> outperforms other pre-trained models of similar sizes in the average performance.
It is noteworthy that, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.4">EEVE-Korean</span> is the only case where the performance in Korean is improved without compromising the performance in English.
For example, though OPEN-SOLAR-KO-10.7B, which is built on the same base model as ours, performs slightly better than our <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.5">EEVE-Korean-Instruct-10.8B-v1.0</span>, it fails to preserve the English capabilities, showing lower performance in English tasks than its base model, SOLAR-10.7B-v1.0.
We observe similar trends even for our smaller model, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.6">EEVE-Korean-2.8B-v1.0</span> in comparison with the phi-2-ko-v0.1 model, sharing Phi-2 as its base model.
This demonstrates the effectiveness of our training strategy, especially considering that we used even fewer training tokens than our competitors.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.2">Notably, but not surprisingly, preference tuning on English datasets even makes the models underperform in Korean tasks. For example, LLaMA-2-chat variants, which are the preference-tuned version of LLaMA-2 checkpoints, show improved performances in English tasks (Llama-2-7b 0.7774 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mo id="S4.SS2.p2.1.m1.1.1" stretchy="false" xref="S4.SS2.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">→</annotation></semantics></math> Llama-2-7b-chat 0.7976 in English BoolQ), while underperforming in Korean tasks (Llama-2-7b 0.5242 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2.1"><semantics id="S4.SS2.p2.2.m2.1a"><mo id="S4.SS2.p2.2.m2.1.1" stretchy="false" xref="S4.SS2.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.2.m2.1d">→</annotation></semantics></math> Llama-2-7b-chat 0.5157 in Korean BoolQ), which highlights the importance of Korean-specific training for LLMs.
On the other hand, we observe that preference tuning our models on Korean instruction datasets doesn’t hurt the model performance in English tasks, rather even improving it. We posit that it is because the embedding spaces are already well-aligned between Korean and English tokens, thus fine-tuning on a specific language doesn’t incur a significant change in model parameters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion &amp; Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The report introduces <span class="ltx_text ltx_font_typewriter" id="S5.p1.1.1">EEVE-Korean-v1.0</span>, a Korean adaptation of large language models that utilizes an Efficient and Effective Vocabulary Expansion (EEVE) method to enhance Korean text processing capabilities significantly. The method, based on parameter freezing and subword initialization, enables the <span class="ltx_text ltx_font_typewriter" id="S5.p1.1.2">EEVE-Korean-10.8B-v1.0</span> model to excel in Korean language tasks while maintaining strong English capabilities. Achieved with a corpus of just 2 billion tokens, this approach represents a notable advancement in language model training efficiency and effectiveness. By making these models available to the research community, the project aims to contribute to the development of more inclusive and efficient language processing technologies.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Expanding our vision, future efforts will explore the application of our vocabulary expansion methodology to additional languages, assessing its generalizability and effectiveness. We aim to not only extend the <span class="ltx_text ltx_font_typewriter" id="S5.p2.1.1">EEVE-Korean</span> model’s linguistic range but also to delve deeper into evaluating its reasoning and generative capabilities through diverse tasks, including complex mathematical reasoning tests like GSM8K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Cobbe et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib4" title="">2021</a>)</cite>, and human evaluations in interactive settings like chatbots&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zheng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib35" title="">2023</a>)</cite>. Moreover, efforts to enhance pre-training data quality, and to analyze performance in code-switching scenarios&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.14714v1#bib.bib33" title="">2023</a>)</cite> will underpin our commitment to refining the model’s robustness and versatility. These initiatives are designed to broaden the model’s applicability and efficacy, pushing the boundaries of what is achievable with advanced language models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almazrouei et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">The falcon series of open language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2311.16867</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.anthropic.com/news/claude-2" title="">Model card and evaluations for claude models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Anthropic technical Report</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 2924–2936.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et&nbsp;al. 2021.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2110.14168</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023.

</span>
<span class="ltx_bibblock">Ultrafeedback: Boosting language models with high-quality feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2310.01377</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le&nbsp;Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.5281/zenodo.10256836" title="">A framework for few-shot language model evaluation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hewitt (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
John Hewitt. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https:/nlp.stanford.edu/~johnhew//vocab-expansion.html" title="">Initializing new word embeddings for pretrained language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">hiyouga (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
hiyouga. 2023.

</span>
<span class="ltx_bibblock">Llama factory.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hiyouga/LLaMA-Factory" title="">https://github.com/hiyouga/LLaMA-Factory</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Myeongjun Jang, Dohyung Kim, Deuk&nbsp;Sin Kwon, and Eric Davis. 2022.

</span>
<span class="ltx_bibblock">Kobest: Korean balanced evaluation of significant tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 29th International Conference on Computational Linguistics</em>, pages 3697–3708.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2310.06825</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Emma&nbsp;Bou Hanna, Florian Bressand, et&nbsp;al. 2024.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2401.04088</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2312.15166</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ildoo Kim, Gunsoo Han, Jiyeon Ham, and Woonhyuk Baek. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/kakaobrain/kogpt" title="">Kogpt: Kakaobrain korean(hangul) generative pretrained transformer</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ko et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hyunwoong Ko, Kichang Yang, Minho Ryu, Taekyoon Choi, Seungmu Yang, Sungho Park, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">A technical report for polyglot-ko: Open-source large-scale korean language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2306.02254</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">L. Junbum (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L. Junbum. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://huggingface.co/beomi/SOLAR-KO-10.7B" title="">Solar-ko-10.7b</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Huayang Li, Tian Lan, Zihao Fu, Deng Cai, Lemao Liu, Nigel Collier, Taro Watanabe, and Yixuan Su. 2023a.

</span>
<span class="ltx_bibblock">Repetition in repetition out: Towards understanding neural text degeneration from the data perspective.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del&nbsp;Giorno, Suriya Gunasekar, and Yin&nbsp;Tat Lee. 2023b.

</span>
<span class="ltx_bibblock">Textbooks are all you need ii: phi-1.5 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2309.05463</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, "Teknium", and Nathan Hoos. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup/" title="">Slimorca dedup: A deduplicated subset of slimorca</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2018.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mukherjee et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023.

</span>
<span class="ltx_bibblock">Orca: Progressive learning from complex explanation traces of gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2306.02707</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chanjun Park, Hwalsuk Lee, Hyunbyung Park, Hyeonwoo Kim, Sanghoon Kim, Seonghwan Cho, Sunghun Kim, and Sukyung Lee. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard" title="">Open ko-llm leaderboard</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aleksandar Petrov, Emanuele La&nbsp;Malfa, Philip&nbsp;HS Torr, and Adel Bibi. 2023.

</span>
<span class="ltx_bibblock">Language model tokenizers introduce unfairness between languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2305.15425</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pilehvar and Camacho-Collados (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mohammad&nbsp;Taher Pilehvar and Jose Camacho-Collados. 2019.

</span>
<span class="ltx_bibblock">Wic: the word-in-context dataset for evaluating context-sensitive meaning representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 1267–1273.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher&nbsp;D Manning, and Chelsea Finn. 2023.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2305.18290</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roemmele et&nbsp;al. (2011)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Melissa Roemmele, Cosmin&nbsp;Adrian Bejan, and Andrew&nbsp;S Gordon. 2011.

</span>
<span class="ltx_bibblock">Choice of plausible alternatives: An evaluation of commonsense causal reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">2011 AAAI Spring Symposium Series</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M Dai, Anja Hauth, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2312.11805</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
MosaicML&nbsp;NLP Team et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Introducing mpt-30b: Raising the bar for open-source foundation models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welch et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Charles Welch, Rada Mihalcea, and Jonathan&nbsp;K Kummerfeld. 2020.

</span>
<span class="ltx_bibblock">Improving low compute language modeling with in-domain embedding initialisation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 8625–8634.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4791–4800.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ruochen Zhang, Samuel Cahyawijaya, Jan Christian&nbsp;Blaise Cruz, and Alham&nbsp;Fikri Aji. 2023.

</span>
<span class="ltx_bibblock">Multilingual large language models are not (yet) code-switchers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2305.14235</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jun Zhao, Zhihao Zhang, Qi&nbsp;Zhang, Tao Gui, and Xuanjing Huang. 2024.

</span>
<span class="ltx_bibblock">Llama beyond english: An empirical study on language capability transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2401.01055</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi&nbsp;Lin, Zhuohan Li, Dacheng Li, Eric Xing, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2306.05685</em>.

</span>
</li>
</ul>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
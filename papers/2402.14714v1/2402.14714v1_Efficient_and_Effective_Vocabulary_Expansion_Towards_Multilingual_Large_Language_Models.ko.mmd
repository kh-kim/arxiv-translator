# 효율적이고 효과적인 어휘 확장

다국어 대용량 언어 모델에 관한 연구

승덕김1승택최1명호

야놀자코리아

{승덕.김, 승택.최, 명호.정}@야오놀자.com

Equal Contribution.

각주 1: [https://huggingface.co/yanolja/EEVE-Korean-10.88-v1.0](https://huggingface.co/yanolja/EEVE-Korean-10.88-v1.0)

###### Abstract

이 보고서는 영어와 한국어 텍스트 이해에 걸쳐 놀라운 능력을 나타내는 대형 언어 모델의 한국어 각색인 EEVE-Korean-v1.0을 소개한다. SOLAR-10.7B, Phi-2와 같이 영어가 아닌 텍스트가 영어가 아닌 토큰사이저로 비효율적으로 처리되는 최근의 매우 유능하지만 영어가 중심이 되는 LLM을 기반으로 하여, 본 논문에서는 파라미터 동결 및 서브워드 초기화를 포함하는 효율적이고 효과적인 어휘 확장(EEVE) 방법을 제시한다. 새로운 임베딩이 수조 개의 트레이닝 토큰을 필요로 한다는 이전의 노력과 대조적으로, 우리는 우리의 방법이 단지 20억 토큰 내에서 비영어 능력을 크게 향상시킬 수 있다는 것을 보여준다. Open Ko-LLM 리더보드에서 대부분의 명령어 조정 LLM을 통과하면 2024년 1월 현재 우리의 모델 EEVE-Korean-10.88-v1.0은 오픈 소스 커뮤니티에서 한국 사전 훈련의 선두 모델로 랭크된다. 우리는 다양한 언어로 열린 연구 커뮤니티에 힘을 실어주기 위해 Huggingface에 모델을 오픈 소스합니다.

## 1 Introduction

GPT-4[2], 제미니[14], 클로드[1]와 같은 대형 언어 모델(LLM) 분야의 최근 발전은 여러 언어를 처리하고 이해하는 데 놀라운 능력을 보여주었다. 반면에 오픈 소스 커뮤니티에서 주목할 만한 모델인 LLaMA[15, 16], MPT[15], Falcon[11], Mistral[18], Mistral[18], SOLAR[19], Phi-1.5[10]은 영어 작업에서 벤치마크를 설정했지만 이러한 개발은 주로 영어를 선호하여 비영어 언어의 성능 격차를 초래했다.

이러한 불일치는 언어 능력뿐만 아니라 계산 효율에서도 발견될 수 있는데, 한국어 같은 비영어 언어는 동등한 의미 내용이라도 영어보다 훨씬 더 많은 토큰을 필요로 한다(그림 1). 그리고, 물론, 이것은 더 긴 응답 시간들, 더 짧은 컨텍스트 길이들, 및 더 높은 API 비용들과 같은 사용자 경험들에 부정적인 영향을 미친다[23]. 일부 자주 사용되지만 긴 단어를 추가 토큰으로 소개하는 토큰화기 어휘를 확장하는 것은 따라서 비영어권 사용자에게 필수적이지만, 새로운 임베딩은 수조 개의 트레이닝 토큰을 필요로 하기 때문에 어휘 확장은 매우 어려운 작업이다[18].

이를 위해 이 기술 보고서는 새롭고 효과적인 어휘 확장을 위한 새로운 접근법, 즉 새로 추가된 토큰의 임베딩을 더 잘 훈련시킬 수 있는 EEVE를 제시한다. 적응의 용이성을 위해 서브워드 기반 임베딩 초기화를 활용하고, 훈련될 파라미터들의 순서와 양을 정교하게 조정하는 파라미터 동결을 갖는 7개의 훈련 단계를 설계한다. 우리는 처음에 입력 임베딩만 훈련하는 데 집중하고 최종 단계에서 전체 매개변수를 포함하도록 점진적으로 확장함으로써 기초 모델의 고급 기능을 영어에서 한국어로 꼼꼼하게 전달한다.

EEVE를 사용하여 최근 영어 중심 LLM, 특히 SOLAR-10.7B [15] 및 Phi-2 [10]에 구축된 한국어 LLM 제품군인 EEVE-Korean-10.88-v1.01 및 EEVE-Korean-2.88-v1.02를 공식 출시하고 한국어 중심 사전 교육을 추가한다. 우리는 영어와 한국어 작업에 대한 lm-평가-harness3[1]에 대한 모델을 평가한다.

[MISSING_PAGE_FAIL:2]

더 논의될 것이다.

### Multi-stage Training

여기서는 효율적인 어휘 확장을 위한 7단계 훈련 방법론의 미묘한 접근법을 설명하며, 초기 영어 중심 훈련 범위를 넘어 언어에서 파생된 새로운 토큰을 통합하는 세심한 과정을 강조한다.

**1단계(새 입력 임베딩):** 처음에 초점은 좁지만 중요합니다. 다른 모든 모델 매개 변수를 동결하면서 새로 추가된 토큰의 입력 임베딩을 학습합니다. 이 단계는 기반이 되어 모델이 처음부터 이러한 토큰에 대한 인식과 처리를 조정할 수 있다. 사전 초기화된 임베딩은 모델이 기존 프레임워크에서 이러한 새로운 토큰을 더 잘 활용하도록 안내하는 출발점 역할을 한다. 우리의 주요 가설은 인과적 언어 모델링에서 입력 및 출력 토큰 시퀀스를 구별할 수 있다면 구형 토큰과 새로운 토큰화기를 동시에 활용함으로써 모델이 구형 토큰의 임베딩 공간에서 확립된 지식을 활용할 수 있기 때문에 새로운 어휘 임베딩을 보다 효율적이고 효과적으로 학습할 수 있다는 것이다. 그러나 입력 및 출력 시퀀스에 고유한 토큰라이저를 한 번에 사용하는 것은 일치하지 않는 입력/출력 시퀀스로 인한 교사 강제 적용의 어려움과 같은 구현 문제를 제기한다. 여기서, 서브워드 기반 임베딩 초기화(Sec 2.2)는 출력 시퀀스들에 대해 구 토큰나이저를 사용하기 위한 프록시를 제공하여, 모델이 전체 워드 토큰(new)이 주어진 서브워드 토큰(old)을 생성하도록 태스크된다. 즉, 모델은 그림 2에 설명된 바와 같이 입력/출력 토큰 시퀀스의 수정 없이 입력 임베딩만을 최적화함으로써 새로운 토큰을 생성하기 위한 표현과 첫 번째 서브워드 토큰을 생성하기 위한 표현을 정렬하는 것을 학습할 수 있다. 그러나 이 단계에서 모델은 아직 동일한 은닉 상태를 공유하는 토큰을 구별할 수 없다.

**2단계(새 출력 임베딩):** 출력 임베딩(lm_head)만 조정하여 다양한 컨텍스트에서 새 토큰을 정확하게 생성하는 모델의 숙련도를 향상시키는 것이 목표입니다. 다른 모든 매개변수를 동결하기로 한 결정은 모델의 현재 불안정한 상태에서 비롯된다. 입력 임베딩과 출력 임베딩을 동시에 학습하도록 하는 것은 수렴을 달성하는 것을 복잡하게 할 것이고, 따라서 최적의 성능을 향한 모델의 진행을 방해할 것이다. 대부분의 매개변수를 동결함으로써 보다 안정적인 수렴을 달성한다. 더욱이, 이 접근법은 다른 층들을 통한 역전파의 필요성을 제거하기 때문에 트레이닝 시간을 상당히 감소시킨다.

**3 단계(새 입력 및 출력 임베딩):** 이 단계에서 입력 임베딩(embed_tokens)은 여전히 출력 임베딩의 초기 임베딩을 기반으로 최적화 상태로 유지됩니다. 이 단계를 통해

그림 1: 매개변수 동결이 있는 훈련 단계. 화재 및 눈송이 이모지는 각각 훈련 가능한 매개변수와 동결 매개변수를 나타낸다.

[MISSING_PAGE_FAIL:4]

kens는 그것을 나타내지만, 우리의 새로운 토큰화기는 1.6B 토큰만을 사용하여 거의 절반으로 그렇게 할 수 있습니다. 이러한 차이는 Phi-2와 EEVE-Korean-2.88 모델의 경우에 더욱 두드러지게 나타나며, 여기서 각각 5.6B 토큰과 1.6B 토큰이 필요하다. 변압기가 토큰 길이와 관련하여 2차 증가하는 계산 복잡도를 갖는다는 점을 고려할 때, 이는 두 가지 중요한 방식으로 해석될 수 있다. 첫째, 동일한 GPU에서 4배 이상 더 긴 시퀀스를 처리할 수 있습니다. 또는 두 번째, 이는 우리의 모델이 동일한 데이터 세트에서 거의 4배 더 계산적으로 효율적으로 훈련될 수 있음을 의미한다. 이러한 차이는 Phi-2 및 EEVE-Korean-2.8B 토큰라이저의 경우 더욱 두드러진다.

EEVE-Korean 모델의 _fine-tuning_을 위해 LLaM-Factory 구현을 기반으로 한 Direct Preference Optimization (DPO; Rafailov et al.2023)을 사용하였다. 본 연구에서는 한국어의 명령어 수행 능력을 향상시키기 위해 공개 명령어 데이터셋인 Orca5(Mukherjee et al., 2023; Lian et al., 2023)와 UltraFeedback6(Cui et al., 2023)을 한국어로 번역하였다. 이러한 데이터 세트를 한국어로 번역하는 과정에서 소스 언어와 타겟 언어가 모두 실수로 한국어로 번역되는 경우와 같이 프로그래밍 코드 형식의 무결성을 보장하고 번역 오류를 수정하는 것이 미세 조정 모델의 품질과 효율성을 유지하는 데 중요했다. 미세 조정된 모델을 EEVE-Korean-Instruct로 명명했습니다.

각주 5: [https://huggingface.co/datasets/Open-Orca/slimOrca-bedup](https://huggingface.co/datasets/Open-Orca/slimOrca-bedup)

각주 6: [https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned)

### Training

기본 아키텍처로서 SOLAR-10.7B (Kim et al., 2023)와 Phi-2 (Li et al., 2023)를 선택하였는데, 이는 둘 다 유사한 크기의 LLMs 사이에서 뛰어난 성능을 보였기 때문이다. 이러한 기초 아키텍처의 선택은 우리의 전략적 훈련 목표와 일치하며, 우리의 새로운 모델이 한국어에서 유사한 수준의 언어 이해 및 추론 능력을 달성할 수 있도록 입증된 장점을 활용한다.

모델 변형의 학습을 위해 초기 사전 학습 단계에서 Axolot7과 후속 미세 조정을 위해 LLaMA-Factory8(hiyouga, 2023)의 두 가지 별개의 코드베이스를 활용했다. 이러한 코드베이스는 우리의 훈련 과정에 강력하고 신뢰할 수 있는 기반을 제공했다.

각주 7: [https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)

각주 8: [https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

특히, 64개의 CPU 코어를 사용하여 각각 80GB 메모리가 있는 8 x NVIDIA H100 GPU의 설정으로 모델을 훈련한다. EEVE-Korean-10.8B-v1.0의 경우, bf16 정밀도에서 훈련 과정은 길이 4096, 기울기 누적 단계 4, 마이크로 배치 크기 8로 구성된 반면, EEVE-Korean-2.8B-v1.0은 시퀀스 길이 2048, 기울기 누적 단계 16, 마이크로 배치 크기 16으로 구성된다. AdamW(Loshchilov and Hutter, 2018) 최적화기는 워밍업 단계 10단계를 포함하는 코사인 학습률 스케줄러와 쌍을 이룬다. 10.8B 변형에 대한 학습률은 4e-5로 설정된 반면, 작은 모델에 대해서는 2e-4를 사용했다. 우리는 손실이 수렴될 때까지 각 단계에서 훈련을 계속하여 400개의 글로벌 단계에 도달하기 전에 손실이 수렴되는 것을 관찰했으며 이는 훈련 전략의 효율성을 나타낸다. 우리의 훈련 전략은 7개의 다른 단계를 포함하지만, 2.8B 변형의 경우 출력 임베딩만을 최적화해도 많은 계산이 발생하지 않기 때문에 전체 사전 훈련이 이틀 이내에 수행될 수 있다는 점은 주목할 만하다.

## 4 Evaluations

한국어 및 영어 LLM 벤치마크에 대한 모델을 평가하여 기본 기본 모델의 강력한 다국어 기능을 효율적으로 활용할 수 있는 어휘 확장 방법의 이점을 강조한다. 바람직하게는 한국어 과제에서는 향상된 성능을 보이고 영어 과제에서는 비슷한 성능을 보일 수 있는 모델을 기대한다.

### Benchmarks

한국 태스크의 경우 태스크가 설계된 KoBEST 벤치마크(Jang et al., 2022)를 채택한다.

\begin{table}
\begin{tabular}{l|l|l} \hline \hline Model & Total (tokens) & Average (tokens) \\ \hline SOLAR-10.7B & 3.1B & 964 \\ EEVE-Korean-10.8B-v1.0 & 1.6B & 500 \\ \hline Phi-2 & 5.6B & 1748 \\ EEVE-Korean-2.8B-v1.0 & 1.6B & 484 \\ \hline \hline \end{tabular}
\end{table}
표 2: 총 3.2M 문서의 6.7GB 사전 훈련 코퍼스에 대한 토큰라이저의 비교.

언어 이해와 추론의 다양한 측면을 평가합니다. 구체적으로, 이 벤치마크는 한국어 번역 버전의 언어 이해 태스크: 부울 질문 응답[1], 상식 인과 추론(COPA; Roemmele et al.2011), 문맥 민감 단어 이해(WiC; Pilehvar and Camacho-Collados2019), 상식 추론[11], 및 감정 부정 인식(SentiNeg)을 제공한다. 영어 작업의 경우 KoBEST, BoolQ, COPA 및 HelaSwag의 다음과 같은 원래 작업을 사용하며, 이는 LLM의 영어와 한국어 능력 간의 정렬을 더 잘 강조할 수 있다. 일관된 비교를 보장하기 위해 오픈 소스 LLM 평가 프레임워크인 lm-평가-harness9[7]를 사용한다.

각주 9: [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)

### Results

이제 표 3의 다른 최고 성능 모델을 사용하여 EEVE-Korean 및 EEVE-Korean-Instruct 변형에 대한 평가 결과를 제시한다. EEVE-Korean-10.8B-v1.0은 평균 성능에서 유사한 크기의 다른 사전 훈련된 모델보다 우수하다. 주목할 점은 EEVE-Korean이 영어의 성능을 손상시키지 않으면서 한국어의 성능을 향상시킨 유일한 경우라는 점이다. 예를 들어, 동일한 기본 모델에 구축된 OPEN-SOLAR-KO-10.7B는 우리의 EEVE-Korean-Instruct-10.8B-v1.0보다 약간 더 나은 성능을 보이지만, 영어 능력을 보존하지 못하여 기본 모델인 SOLAR-10.7B-v1.0보다 낮은 성능을 보인다. 우리는 Phi-2를 기본 모델로 공유하는 phi-2-ko-v0.1 모델과 비교하여 작은 모델인 EEVE-Korean-2.8B-v1.0에서도 유사한 경향을 관찰한다. 이는 특히 경쟁사보다 훨씬 적은 수의 훈련 토큰을 사용했다는 점을 고려할 때 훈련 전략의 효과를 보여줍니다.

특히, 놀랍지 않게, 영어 데이터 세트에 대한 선호도 튜닝은 심지어 모델이 한국어 작업에서 저조하게 성능을 발휘하게 한다. 예를 들어, LLaMA-2 체크포인트의 선호도 조정 버전인 LLaMA-2-chat 변형은 영어 작업(Llama-2-7b 0.7774 \(\rightarrow\) LLama-2-7b-chat 0.7976 영어 BoolQ)에서는 향상된 성능을 보이는 반면 한국어 작업(Llama-2-7b 0.5242 \(\rightarrow\) LLama-2-7b-chat 0.5157 한국어 BoolQ)에서는 낮은 성능을 보여 LLMs에 대한 한국어 특화 교육의 중요성을 강조한다. 반면에, 한국어 명령어 데이터셋에 대한 선호 튜닝은 영어 태스크에서 모델 성능에 부정적인 영향을 미치지 않고 오히려 개선되는 것을 관찰한다. 우리는 임베딩 공간이 이미 한국어 토큰과 영어 토큰 사이에 잘 정렬되어 있기 때문에 특정 언어에 대한 미세 조정이 모델 매개변수에 큰 변화를 일으키지 않기 때문이라고 가정한다.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Types} & \multicolumn{4}{c}{English} & \multicolumn{4}{c}{Korean} & \multirow{2}{*}{**Avg.**} \\ \cline{3-3} \cline{5-10}  & & & \multicolumn{1}{c}{} & & & & & & & & \\ \cline{3-10}  & & & \multicolumn{1}{c}{} & & & & & & & & \\ \hline meta-llama/Llama-2-7b-hf & PT & 0.7774 & 0.8700 & 0.5714 & 0.5242 & 0.5700 & 0.4420 & 0.4610 & 0.4881 & 0.5880 \\ meta-llama/Llama-2-13b-hf & PT & 0.8055 & 0.9100 & 0.6006 & 0.5214 & 0.6010 & 0.4380 & 0.5038 & 0.4881 & 0.6086 \\ mistralA/Mistral-7B-v0.1 & PT & 0.8379 & 0.9200 & 0.6129 & 0.6282 & 0.5880 & 0.4300 & 0.5365 & 0.4881 & 0.6302 \\ meta-llama/Llama-2-7b-chat-hf & FT & 0.7976 & 0.8700 & 0.5779 & 0.5157 & 0.5530 & 0.4160 & 0.4987 & 0.4881 & 0.5896 \\ meta-llama/Llama-2-13b-chat-hf & FT & 0.8165 & 0.8800 & 0.6072 & 0.5057 & 0.5760 & 0.4040 & 0.4685 & 0.4881 & 0.5933 \\ \hline upstage/SOLAR-10.7B-v1.0 (base) & PT & 0.8257 & 0.8700 & 0.6393 & 0.5057 & 0.5750 & 0.4320 & 0.6146 & 0.4881 & 0.6188 \\ upstage/SOLAR-10.7B-Instruct-v1.0 & FT & **0.8853** & **0.9400** & **0.6866** & 0.8184 & 0.6370 & 0.4560 & 0.5668 & 0.4921 & 0.6853 \\ boomi/OPEN-SOLAR-KO-10.7B\({}^{*}\) & PT & 0.8187 & 0.8800 & 0.5570 & 0.8355 & **0.8010** & **0.5040** & 0.6952 & 0.4897 & 0.6976 \\ yanoj1/EEVE-Korean-10.8B-v1.0\({}^{*}\) & PT & 0.8492 & 0.9000 & 0.6203 & 0.8568 & 0.7530 & 0.4900 & 0.6675 & **0.4992** & 0.7045 \\ yanoj1/EEVE-Korean-10.8B-v1.0\({}^{*}\) & FT & 0.8810 & 0.9300 & 0.6502 & **0.8860** & 0.7610 & 0.4700 & **0.9521** & 0.4937 & **0.7530** \\ \hline microsoft/Phi-2 (base) & PT & **0.8336** & **0.9000** & **0.5583** & 0.5021 & 0.4770 & 0.3280 & 0.5063 & 0.4881 & 0.5742 \\ dacehum-ml/pli-2.ko-v0.1\({}^{*}\) & PT & 0.6141 & 0.5800 & 0.3257 & 0.5164 & **0.6100** & **0.3860** & 0.4484 & 0.4881 & 0.4961 \\ yanoj1/EEVE-Korean-2.8B-v1.0\({}^{*}\) & PT & 0.7404 & 0.8900 & 0.5247 & 0.5299 & 0.5820 & 0.3800 & 0.5164 & 0.4881 & 0.5814 \\ yanoj1/EEVE-Korean-Instruct-2.8B-v1.0\({}^{*}\) & FT & 0.8248 & 0.8700 & 0.5392 & **0.7066** & 0.5640 & 0.3660 & **0.5290** & **0.5230** & **0.6153** \\ \hline \hline \end{tabular}
\end{table}
표 3: lm-평가-하르니스 기준의 주요 평가 결과. 데이터 세트 이름은 간결함을 위해 약칭됩니다. BoolQ의 경우 BQ, COPA의 경우 CP, HellaSwag의 경우 HS, SentiNeg의 경우 SN입니다. 한국의 업무는 [7]부터입니다. 정확도(acc)는 모든 작업에 대한 평가 메트릭으로 사용된다. '유형' 열에서 모델은 사전 훈련(PT)과 미세 조정(FT)의 두 그룹으로 분류된다. 우리는 \({}^{*}\)을 사용하여 한국어 데이터셋에서 학습된 모델을 나타낸다. 번식을 쉽게 하기 위해 HuggingFace에서 공식 이름을 사용합니다.

## 5 결론 & 미래 작업

이 보고서는 한국어 텍스트 처리 능력을 크게 향상시키기 위해 효과적이고 효과적인 어휘 확장(EEVE) 방법을 활용하는 대규모 언어 모델의 한국어 적응인 EEVE-Korean-v1.0을 소개한다. 파라미터 동결 및 서브워드 초기화에 기반한 이 방법은 EEVE-Korean-10.8B-v1.0 모델이 강력한 영어 능력을 유지하면서 한국어 작업에 탁월할 수 있도록 한다. 단 20억 토큰의 코퍼스로 달성된 이 접근법은 언어 모델 훈련 효율성과 효율성에서 주목할만한 발전을 나타낸다. 이 프로젝트는 이러한 모델을 연구 커뮤니티에서 사용할 수 있도록 함으로써 보다 포괄적이고 효율적인 언어 처리 기술의 개발에 기여하는 것을 목표로 한다.

비전을 확장하면 향후 노력은 어휘 확장 방법론을 추가 언어에 적용하여 일반화 가능성과 효율성을 평가하는 것을 탐구할 것이다. GSM8K Cobbe 등(2021)과 같은 복잡한 수학적 추론 테스트, 챗봇 Zheng 등(2023)과 같은 대화형 환경에서의 인간 평가 등 다양한 작업을 통해 EEVE-Korean 모델의 언어 범위를 확장할 뿐만 아니라 추론 및 생성 능력을 더 깊이 평가하는 것을 목표로 한다. 또한, 사전 훈련 데이터 품질을 향상시키고 코드 전환 시나리오에서 성능을 분석하기 위한 노력 Zhang 등(2023)은 모델의 견고성과 범용성을 개선하려는 우리의 약속을 뒷받침할 것이다. 이러한 이니셔티브는 모델의 적용 가능성과 효능을 넓히기 위해 고안되었으며 고급 언어 모델로 달성할 수 있는 것의 경계를 확장한다.

## References

* Almazrouei 등(2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open language models. _ arXiv preprint arXiv:2311.16867_.
* Anthropic(2023) Anthropic. 2023. 모델 카드 및 클로드 모델에 대한 평가. _ 인류 기술 보고서_.
* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: 자연스러운 예/아니오 질문의 놀라운 난이도를 탐구합니다. 전산 언어학 협회의 2019 북미 회의 회보: 인간 언어 기술 1권(장문 및 단문)_ 2924-2936쪽.
* Cobbe 등(2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. _ arXiv preprint arXiv:2110.14168_.
* Cui et al. (2023) Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: 고품질 피드백으로 언어 모델을 부스팅합니다. _ arXiv preprint arXiv:2310.01377_.
* Gao 등(2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muenninghoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation.
* Hewitt (2021) John Hewitt. 2021. 사전 훈련된 언어 모델에 대한 새로운 단어 임베딩을 초기화하는 단계.
* hiyouga (2023) hiyouga. 2023. Llama factory. [https://github.com/hiyouga/LLaMA-Factory] (https://github.com/hiyouga/LLaMA-Factory).
*Jang et al.(2022) 장명준, 김도형, 득신권, 에릭 데이비스. 2022. Kobest: Korean balanced evaluation of significant tasks. 《제29회 컴퓨터 언어학 국제 회의》 3697-3708쪽입니다.
* Jiang 등(2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. _ arXiv preprint arXiv:2310.06825_.
* Jiang 등(2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mistral of experts. _ arXiv preprint arXiv:2401.04088_.
* Kim 등(2023) Kim 다현, 박찬준, 김상훈, 원성 이원호, 송원호, 김윤수, 김현우, 김윤기, 김현주, 이현주, 김지후, 등 2023. Solar 10.7 b: 단순하면서도 효과적인 depth up-scaling을 갖는 대형 언어 모델 스케일링_ arXiv preprint arXiv:2312.15166_.
*김 외(2021) 일두 김, 한건수, 함지연, 백운혁. 2021. Kogpt: Kakaobrain korean(한글) 생성 사전 훈련 변압기.
* Ko et al.(2023) Ko 현웅, Kichang Yang, Minho Ryu, Taekyoon Choi, Seungmu Yang, Sunggho Park, et al. 2023. A technical report for polyglot-ko: Open-source large-scale Korean language models. _ arXiv preprint arXiv:2306.02254_.
* Kog 등(2021)L. 준범(2024)Solar-ko-10.7b. 외부 링크: 연결된 링크: SS1입니다.
*H. Li, T. 란지 부동채 유남 콜리어 와타나베, Y. Su (2023)Repetition in repetition out: to understand neural text degeneration from data perspective. SS1에 의해 인용된, 신경 정보 처리 시스템에 관한 제37차 회의에서.
* Y. 이성훈 부벡 엘단 델 조르노 구나세카르와 Y. T. Lee (2023) 교과서는 ii: phi-1.5 기술 보고서만 있으면 됩니다. arXiv preprint arXiv:2309.05463. 인용: SS1.
* W. Lian, G. Wang, B. Goodson, E. Pentland, A. Cook, C. Vong, T. 윤남 후스(2023) 슬리모르카 디듀업: 슬리모르카의 디듀싱된 부분집합. 로 인용: SS1.
* I. Loshchilov and F. Hutter (2018)Decoupled weight decay regularization. International Conference on Learning Representations, 인용: SS1.
* S. 무케르지, A. 미트라, G. 자와하르, S. Agarwal, H. Palangi, and A. Awadallah (2023)Orca: progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707. Cited by: SS1.
*O. Al (2023)Gpt-4 기술 보고서. arXiv preprint arXiv:2303.08774. 인용: SS1.
* C. Park, H. Lee, H. Park, H. Kim, S. 김상훈 조성훈 김수현 Lee (2023)Open ko-llm leaderboard. 로 인용: SS1.
* A. Petrov, E. La Malfa, P. H. Torr 및 A. Bibi(2023)Language model tokenizers introduce unfairness between language. arXiv preprint arXiv:2305.15425. 인용: SS1.
* M. Taher Pilehvar and J. Camacho-Collados (2019)Wic: the word-in-context dataset for evaluating context-sensitive meaning representation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers), pp. 1267-1273. 인용: SS1.
* R. 라파일로프, A. 샤르마, E. 미첼, S. Ermon, C. D. Manning, C. Finn(2023)Direct preference optimization: 당신의 언어 모델은 비밀리에 보상 모델이다. arXiv preprint arXiv:2305.18290. 인용: SS1.
* M. Roemmele, C. A. Bejan, and A. S. Gordon (2011) Choice of the plausable alternatives: a evaluation of 상식적인 인과 추론. 2011년 AAAI 봄 심포지엄 시리즈에서 SS1이 인용했다.
* R. G. Team 아닐세 보르헤오 우재락 Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. (2023)Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. 인용: SS1.
* M. NLP Team 등(2023)Introducing mpt-30b: raising the bar for open-source foundation models. 로 인용: SS1.
* H. Touvron, T. 라브릴, G. 이자카드, X. 마틴 라쇼 라크루아 B. 로지에르 Goyal, E. Hambro, F. Azhar, et al. (2023)LIMA: open and efficient foundation language models. arXiv preprint arXiv:2302.13971. 인용: SS1.
* H. Touvron, L. 마틴 스톤, P. 알버트, A. 알마하이리, Y. 바배이 바슐리코프 P. Bhargava, S. Bhosale, et al. (2023)LIMA 2: open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. 인용: SS1.
* C. Welch, R. Mihalcea, J. K. Kummerfeld (2020)Improving low compute language modeling with in-domain embedding initialisation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8625-8634. Cited by: SS1.
* R. 젤러스 A. 홀츠만 Bisk, A. Farhadi, Y. 최(2019)헬라스왁: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791-4800. 인용: SS1.
* R. 장성 Cahyawijaya, J. Christian Blaise Cruz, A. Fikri Aji (2023) 다국어 대형 언어 모델은 (아직) 코드 스위쳐가 아니다. arXiv preprint arXiv:2305.14235. 인용: SS1.
* J. Zhao, Z. 장규 장태 Gui, X. 영어를 넘어선 황(2024)LIMA: 언어 능력 전이에 대한 실증적 연구. arXiv preprint arXiv:2401.01055. 인용: SS1.
* L. 정우 장영 성성 장주 우영 장주 임종호 Li, D. Li, E. Xing, et al. (2023)Judging llm-as-a-judge with mt-bench and chatbot areana. arXiv preprint arXiv:2306.05685. 인용: SS1.

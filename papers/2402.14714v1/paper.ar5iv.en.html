<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.14714] Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models</title><meta property="og:description" content="This report introduces EEVE-Korean-v1.0, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-cen…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.14714">

<!--Generated on Tue Mar  5 14:08:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Efficient and Effective Vocabulary Expansion 
<br class="ltx_break">Towards Multilingual Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
 Seungduk Kim
 Seungtaek Choi<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
 <span id="id1.1.id1" class="ltx_text ltx_font_bold">Myeongho Jeong</span> 
<br class="ltx_break">Yanolja, South Korea 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">{seungduk.kim, seungtaek.choi, myeongho.jeong}@yanolja.com</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes"> Equal Contribution.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">This report introduces <span id="id3.id1.1" class="ltx_text ltx_font_typewriter">EEVE-Korean-v1.0</span>, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization.
In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens.
Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model <span id="id3.id1.2" class="ltx_text ltx_font_typewriter">EEVE-Korean-10.8B-v1.0</span> ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face’s leaderboard.
We open-source our models on Huggingface to empower the open research community in various languages.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Efficient and Effective Vocabulary Expansion 
<br class="ltx_break">Towards Multilingual Large Language Models</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">

<span id="p1.1.2.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.1.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">
 Seungduk Kim<span id="p1.1.2.1.1.1.1.1.1.1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span> Equal Contribution.</span></span></span>
 Seungtaek Choi<span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex2.1.1.1" class="ltx_text ltx_font_medium">1</span></span></span></span></span>
 Myeongho Jeong</span></span></span>
<span id="p1.1.2.1.1.2.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.2.1" class="ltx_td ltx_align_center">Yanolja, South Korea</span></span>
<span id="p1.1.2.1.1.3.3" class="ltx_tr">
<span id="p1.1.2.1.1.3.3.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.3.3.1.1" class="ltx_text ltx_font_typewriter">{seungduk.kim, seungtaek.choi, myeongho.jeong}@yanolja.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent advancements in the field of large language models (LLMs), such as GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>, Gemini&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Team et&nbsp;al. (<a href="#bib.bib27" title="" class="ltx_ref">2023a</a>)</cite>, and Claude&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Anthropic (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>, have demonstrated remarkable capabilities in processing and understanding multiple languages.
On the other hand, though notable models in open source community, such as LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a href="#bib.bib29" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib30" title="" class="ltx_ref">b</a>)</cite>, MPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Team et&nbsp;al. (<a href="#bib.bib28" title="" class="ltx_ref">2023b</a>)</cite>, Falcon&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Almazrouei et&nbsp;al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>, Mistral&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>, Mixtral&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib11" title="" class="ltx_ref">2024</a>)</cite>, SOLAR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kim et&nbsp;al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>, and Phi-1.5&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2023b</a>)</cite> have set benchmarks in English tasks, these developments have predominantly favored English, leading to a performance gap in non-English languages.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Such disparity can be found not only in their language proficiency but also in computational efficiency, where non-English languages like Korean require significantly more tokens than English even for equivalent semantic content (Figure&nbsp;<a href="#S2.T1" title="Table 1 ‣ 2.1 Preliminary 1: Tokenizer Training ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
And, of course, this negatively affects the user experiences, such as longer response times, shorter context lengths, and higher API costs&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Petrov et&nbsp;al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>.
Expanding the tokenizer vocabulary, which introduces some frequently used yet long words as additional tokens, is thus indispensable for non-English users, but vocabulary expansion is a very challenging task because new embeddings require trillions of training tokens&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhao et&nbsp;al. (<a href="#bib.bib34" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To this end, this technical report presents a novel approach for <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">e</span>fficient and <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">e</span>ffective <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">v</span>ocabulary <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">e</span>xpansion, namely EEVE, which can better train the embeddings of newly added tokens.
For ease of adaptation, we utilize subword-based embedding initialization and design seven training stages with parameter freezing, which elaborately adjust the order and amount of parameters to be trained.
We meticulously transfer the advanced capabilities of foundational models from English to Korean by initially focusing on the training of only input embeddings and progressively expanding to encompass the full parameters in the final stage.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Using <span id="S1.p4.1.1" class="ltx_text ltx_font_typewriter">EEVE</span>, we officially release a family of Korean LLMs, <span id="S1.p4.1.2" class="ltx_text ltx_font_typewriter">EEVE-Korean-10.8B-v1.0<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_serif">1</span></span><a target="_blank" href="https://huggingface.co/yanolja/EEVE-Korean-10.8B-v1.0" title="" class="ltx_ref ltx_url">https://huggingface.co/yanolja/EEVE-Korean-10.8B-v1.0</a></span></span></span></span> and <span id="S1.p4.1.3" class="ltx_text ltx_font_typewriter">EEVE-Korean-2.8B-v1.0<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_serif">2</span></span><a target="_blank" href="https://huggingface.co/yanolja/EEVE-Korean-2.8B-v1.0" title="" class="ltx_ref ltx_url">https://huggingface.co/yanolja/EEVE-Korean-2.8B-v1.0</a></span></span></span></span>, which are built on recent English-centric LLMs, specifically SOLAR-10.7B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kim et&nbsp;al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> and Phi-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2023b</a>)</cite>, with further Korean-centric pre-training.
We evaluate our models on <span id="S1.p4.1.4" class="ltx_text ltx_font_typewriter">lm-evaluation-harness<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text ltx_font_serif">3</span></span><a target="_blank" href="https://github.com/EleutherAI/lm-evaluation-harness" title="" class="ltx_ref ltx_url">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> for both English and Korean language tasks, such as boolean question answering (BoolQ;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Clark et&nbsp;al. <a href="#bib.bib3" title="" class="ltx_ref">2019</a></cite>), commonsense causal reasoning (COPA;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Roemmele et&nbsp;al. <a href="#bib.bib26" title="" class="ltx_ref">2011</a></cite>, context-sensitive word understanding (WiC;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Pilehvar and Camacho-Collados <a href="#bib.bib24" title="" class="ltx_ref">2019</a></cite>), commonsense reasoning (HellaSwag;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Zellers et&nbsp;al. <a href="#bib.bib32" title="" class="ltx_ref">2019</a></cite>), and sentiment negation recognition (SentiNeg).
From the evaluation, we observe that our models outperform the recent open Korean pre-trained LLMs like OPEN-SOLAR-KO-10.7B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">L. Junbum (<a href="#bib.bib15" title="" class="ltx_ref">2024</a>)</cite>, Polyglot-Ko&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Ko et&nbsp;al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>, and KoGPT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kim et&nbsp;al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>, while preserving the strong English capability of the base English-centric LLMs in terms of benchmark performance, being ranked as the leading Korean pre-trained model in Open Ko-LLM Leaderboard&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Park et&nbsp;al. (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Efficient and Effective Vocabulary Expansion</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">To address the challenge of efficiently extending English-centric Language Models (LLMs) to include non-English languages, we introduce a novel methodology for vocabulary expansion. This method combines parameter freezing with subword-based embedding initialization to effectively incorporate and adapt to new linguistic tokens from languages beyond its initial training scope, thereby enhancing its applicability across various linguistic contexts. Our approach outlines a structured seven-stage training process, as illustrated in Figure&nbsp;<a href="#S2.F1" title="Figure 1 ‣ 2.1 Preliminary 1: Tokenizer Training ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, meticulously designed to effectively integrate new tokens into the model’s vocabulary. During pre-training, our objective is causal language modeling.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Our core assumption is that foundational models, having been extensively trained in English texts, possess a substantial level of understanding and reasoning capabilities. Transferring these capabilities from English to another language, such as Korean, could be more efficient than developing performance from standalone Korean pre-training.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Preliminary 1: Tokenizer Training</h3>

<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.18" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:398.9pt;height:209pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.8pt,8.3pt) scale(0.926441220853871,0.926441220853871) ;">
<table id="S2.T1.18.18" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.18.18.19.1" class="ltx_tr">
<td id="S2.T1.18.18.19.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">&nbsp;</span></td>
</tr>
<tr id="S2.T1.18.18.20.2" class="ltx_tr">
<td id="S2.T1.18.18.20.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.18.18.20.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.18.18.20.2.1.1.1" class="ltx_p" style="width:411.9pt;"><span id="S2.T1.18.18.20.2.1.1.1.1" class="ltx_text ltx_font_bold">English (<span id="S2.T1.18.18.20.2.1.1.1.1.1" class="ltx_text ltx_font_italic">8 tokens</span>)</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.18.18.21.3" class="ltx_tr">
<td id="S2.T1.18.18.21.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.18.18.21.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.18.18.21.3.1.1.1" class="ltx_p" style="width:411.9pt;">“<span id="S2.T1.18.18.21.3.1.1.1.1" class="ltx_text ltx_font_italic">Hello, the weather is nice today.</span>”</span>
</span>
</td>
</tr>
<tr id="S2.T1.18.18.22.4" class="ltx_tr">
<td id="S2.T1.18.18.22.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.18.18.22.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.18.18.22.4.1.1.1" class="ltx_p" style="width:411.9pt;"><span id="S2.T1.18.18.22.4.1.1.1.1" class="ltx_text ltx_font_typewriter">[‘_Hello’, ‘,’, ‘_the’, ‘_weather’, ‘_is’, ‘_nice’, ‘_today’, ‘.’]</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.18.18.23.5" class="ltx_tr">
<td id="S2.T1.18.18.23.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">&nbsp;</span></td>
</tr>
<tr id="S2.T1.18.18.24.6" class="ltx_tr">
<td id="S2.T1.18.18.24.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.18.18.24.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.18.18.24.6.1.1.1" class="ltx_p" style="width:411.9pt;"><span id="S2.T1.18.18.24.6.1.1.1.1" class="ltx_text ltx_font_bold">Korean (<span id="S2.T1.18.18.24.6.1.1.1.1.1" class="ltx_text ltx_font_italic">26 tokens</span>)</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.18.18.25.7" class="ltx_tr">
<td id="S2.T1.18.18.25.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.18.18.25.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.18.18.25.7.1.1.1" class="ltx_p" style="width:411.9pt;">“<span id="S2.T1.18.18.25.7.1.1.1.1" class="ltx_text ltx_font_italic">안녕하세요, 오늘은 날씨가 좋네요.</span>”</span>
</span>
</td>
</tr>
<tr id="S2.T1.18.18.18" class="ltx_tr">
<td id="S2.T1.18.18.18.18" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.18.18.18.18.18" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.18.18.18.18.18.18" class="ltx_p" style="width:411.9pt;"><span id="S2.T1.18.18.18.18.18.18.18" class="ltx_text ltx_font_typewriter">[‘_’, ‘안’, ‘<math id="S2.T1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S2.T1.1.1.1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.1.m1.1b"><lt id="S2.T1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.1.m1.1c">&lt;</annotation></semantics></math>0xEB<math id="S2.T1.2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S2.T1.2.2.2.2.2.2.2.m2.1a"><mo id="S2.T1.2.2.2.2.2.2.2.m2.1.1" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.2.2.2.m2.1b"><gt id="S2.T1.2.2.2.2.2.2.2.m2.1.1.cmml" xref="S2.T1.2.2.2.2.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.2.2.2.m2.1c">&gt;</annotation></semantics></math>’, ‘<math id="S2.T1.3.3.3.3.3.3.3.m3.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S2.T1.3.3.3.3.3.3.3.m3.1a"><mo id="S2.T1.3.3.3.3.3.3.3.m3.1.1" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.3.3.3.3.m3.1b"><lt id="S2.T1.3.3.3.3.3.3.3.m3.1.1.cmml" xref="S2.T1.3.3.3.3.3.3.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.3.3.3.3.m3.1c">&lt;</annotation></semantics></math>0x85<math id="S2.T1.4.4.4.4.4.4.4.m4.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S2.T1.4.4.4.4.4.4.4.m4.1a"><mo id="S2.T1.4.4.4.4.4.4.4.m4.1.1" xref="S2.T1.4.4.4.4.4.4.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.4.4.4.4.m4.1b"><gt id="S2.T1.4.4.4.4.4.4.4.m4.1.1.cmml" xref="S2.T1.4.4.4.4.4.4.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.4.4.4.4.m4.1c">&gt;</annotation></semantics></math>’, ‘<math id="S2.T1.5.5.5.5.5.5.5.m5.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S2.T1.5.5.5.5.5.5.5.m5.1a"><mo id="S2.T1.5.5.5.5.5.5.5.m5.1.1" xref="S2.T1.5.5.5.5.5.5.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.5.5.5.5.m5.1b"><lt id="S2.T1.5.5.5.5.5.5.5.m5.1.1.cmml" xref="S2.T1.5.5.5.5.5.5.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.5.5.5.5.m5.1c">&lt;</annotation></semantics></math>0x95<math id="S2.T1.6.6.6.6.6.6.6.m6.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S2.T1.6.6.6.6.6.6.6.m6.1a"><mo id="S2.T1.6.6.6.6.6.6.6.m6.1.1" xref="S2.T1.6.6.6.6.6.6.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.6.6.6.6.6.m6.1b"><gt id="S2.T1.6.6.6.6.6.6.6.m6.1.1.cmml" xref="S2.T1.6.6.6.6.6.6.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.6.6.6.6.6.m6.1c">&gt;</annotation></semantics></math>’, ‘하’, ‘세’, ‘요’, ‘,’, ‘_’, ‘오’, ‘<math id="S2.T1.7.7.7.7.7.7.7.m7.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S2.T1.7.7.7.7.7.7.7.m7.1a"><mo id="S2.T1.7.7.7.7.7.7.7.m7.1.1" xref="S2.T1.7.7.7.7.7.7.7.m7.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.7.7.7.7.7.m7.1b"><lt id="S2.T1.7.7.7.7.7.7.7.m7.1.1.cmml" xref="S2.T1.7.7.7.7.7.7.7.m7.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.7.7.7.7.7.m7.1c">&lt;</annotation></semantics></math>0xEB<math id="S2.T1.8.8.8.8.8.8.8.m8.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S2.T1.8.8.8.8.8.8.8.m8.1a"><mo id="S2.T1.8.8.8.8.8.8.8.m8.1.1" xref="S2.T1.8.8.8.8.8.8.8.m8.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.8.8.8.8.8.m8.1b"><gt id="S2.T1.8.8.8.8.8.8.8.m8.1.1.cmml" xref="S2.T1.8.8.8.8.8.8.8.m8.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.8.8.8.8.8.m8.1c">&gt;</annotation></semantics></math>’, ‘<math id="S2.T1.9.9.9.9.9.9.9.m9.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S2.T1.9.9.9.9.9.9.9.m9.1a"><mo id="S2.T1.9.9.9.9.9.9.9.m9.1.1" xref="S2.T1.9.9.9.9.9.9.9.m9.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.9.9.9.9.9.m9.1b"><lt id="S2.T1.9.9.9.9.9.9.9.m9.1.1.cmml" xref="S2.T1.9.9.9.9.9.9.9.m9.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.9.9.9.9.9.m9.1c">&lt;</annotation></semantics></math>0x8A<math id="S2.T1.10.10.10.10.10.10.10.m10.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S2.T1.10.10.10.10.10.10.10.m10.1a"><mo id="S2.T1.10.10.10.10.10.10.10.m10.1.1" xref="S2.T1.10.10.10.10.10.10.10.m10.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.10.10.10.10.10.10.10.m10.1b"><gt id="S2.T1.10.10.10.10.10.10.10.m10.1.1.cmml" xref="S2.T1.10.10.10.10.10.10.10.m10.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.10.10.10.10.10.10.m10.1c">&gt;</annotation></semantics></math>’, ‘<math id="S2.T1.11.11.11.11.11.11.11.m11.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S2.T1.11.11.11.11.11.11.11.m11.1a"><mo id="S2.T1.11.11.11.11.11.11.11.m11.1.1" xref="S2.T1.11.11.11.11.11.11.11.m11.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.11.11.11.11.11.11.11.m11.1b"><lt id="S2.T1.11.11.11.11.11.11.11.m11.1.1.cmml" xref="S2.T1.11.11.11.11.11.11.11.m11.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.11.11.11.11.11.11.m11.1c">&lt;</annotation></semantics></math>0x98<math id="S2.T1.12.12.12.12.12.12.12.m12.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S2.T1.12.12.12.12.12.12.12.m12.1a"><mo id="S2.T1.12.12.12.12.12.12.12.m12.1.1" xref="S2.T1.12.12.12.12.12.12.12.m12.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.12.12.12.12.12.12.12.m12.1b"><gt id="S2.T1.12.12.12.12.12.12.12.m12.1.1.cmml" xref="S2.T1.12.12.12.12.12.12.12.m12.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.12.12.12.12.12.12.m12.1c">&gt;</annotation></semantics></math>’, ‘은’, ‘_’, ‘날’, ‘<math id="S2.T1.13.13.13.13.13.13.13.m13.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S2.T1.13.13.13.13.13.13.13.m13.1a"><mo id="S2.T1.13.13.13.13.13.13.13.m13.1.1" xref="S2.T1.13.13.13.13.13.13.13.m13.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.13.13.13.13.13.13.13.m13.1b"><lt id="S2.T1.13.13.13.13.13.13.13.m13.1.1.cmml" xref="S2.T1.13.13.13.13.13.13.13.m13.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.13.13.13.13.13.13.13.m13.1c">&lt;</annotation></semantics></math>0xEC<math id="S2.T1.14.14.14.14.14.14.14.m14.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S2.T1.14.14.14.14.14.14.14.m14.1a"><mo id="S2.T1.14.14.14.14.14.14.14.m14.1.1" xref="S2.T1.14.14.14.14.14.14.14.m14.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.14.14.14.14.14.14.14.m14.1b"><gt id="S2.T1.14.14.14.14.14.14.14.m14.1.1.cmml" xref="S2.T1.14.14.14.14.14.14.14.m14.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.14.14.14.14.14.14.14.m14.1c">&gt;</annotation></semantics></math>’, ‘<math id="S2.T1.15.15.15.15.15.15.15.m15.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S2.T1.15.15.15.15.15.15.15.m15.1a"><mo id="S2.T1.15.15.15.15.15.15.15.m15.1.1" xref="S2.T1.15.15.15.15.15.15.15.m15.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.15.15.15.15.15.15.15.m15.1b"><lt id="S2.T1.15.15.15.15.15.15.15.m15.1.1.cmml" xref="S2.T1.15.15.15.15.15.15.15.m15.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.15.15.15.15.15.15.15.m15.1c">&lt;</annotation></semantics></math>0x94<math id="S2.T1.16.16.16.16.16.16.16.m16.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S2.T1.16.16.16.16.16.16.16.m16.1a"><mo id="S2.T1.16.16.16.16.16.16.16.m16.1.1" xref="S2.T1.16.16.16.16.16.16.16.m16.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.16.16.16.16.16.16.16.m16.1b"><gt id="S2.T1.16.16.16.16.16.16.16.m16.1.1.cmml" xref="S2.T1.16.16.16.16.16.16.16.m16.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.16.16.16.16.16.16.16.m16.1c">&gt;</annotation></semantics></math>’, ‘<math id="S2.T1.17.17.17.17.17.17.17.m17.1" class="ltx_Math" alttext="<" display="inline"><semantics id="S2.T1.17.17.17.17.17.17.17.m17.1a"><mo id="S2.T1.17.17.17.17.17.17.17.m17.1.1" xref="S2.T1.17.17.17.17.17.17.17.m17.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.17.17.17.17.17.17.17.m17.1b"><lt id="S2.T1.17.17.17.17.17.17.17.m17.1.1.cmml" xref="S2.T1.17.17.17.17.17.17.17.m17.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.17.17.17.17.17.17.17.m17.1c">&lt;</annotation></semantics></math>0xA8<math id="S2.T1.18.18.18.18.18.18.18.m18.1" class="ltx_Math" alttext=">" display="inline"><semantics id="S2.T1.18.18.18.18.18.18.18.m18.1a"><mo id="S2.T1.18.18.18.18.18.18.18.m18.1.1" xref="S2.T1.18.18.18.18.18.18.18.m18.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.18.18.18.18.18.18.18.m18.1b"><gt id="S2.T1.18.18.18.18.18.18.18.m18.1.1.cmml" xref="S2.T1.18.18.18.18.18.18.18.m18.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.18.18.18.18.18.18.18.m18.1c">&gt;</annotation></semantics></math>’, ‘가’, ‘_’, ‘좋’, ‘네’, ‘요’, ‘.’]</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.18.18.26.8" class="ltx_tr">
<td id="S2.T1.18.18.26.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">&nbsp;</span></td>
</tr>
<tr id="S2.T1.18.18.27.9" class="ltx_tr">
<td id="S2.T1.18.18.27.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.18.18.27.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.18.18.27.9.1.1.1" class="ltx_p" style="width:411.9pt;"><span id="S2.T1.18.18.27.9.1.1.1.1" class="ltx_text ltx_font_bold">Expanded Tokenizer (<span id="S2.T1.18.18.27.9.1.1.1.1.1" class="ltx_text ltx_font_italic">9 tokens</span>)</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.18.18.28.10" class="ltx_tr">
<td id="S2.T1.18.18.28.10.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S2.T1.18.18.28.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.18.18.28.10.1.1.1" class="ltx_p" style="width:411.9pt;"><span id="S2.T1.18.18.28.10.1.1.1.1" class="ltx_text ltx_font_typewriter">[‘_안’, ‘녕’, ‘하세요’, ‘,’, ‘_오늘은’, ‘_날씨가’, ‘_좋’, ‘네요’, ‘.’]</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.18.18.29.11" class="ltx_tr">
<td id="S2.T1.18.18.29.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">&nbsp;</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>A comparison of token consumption between English and Korean. We used the tokenizers of SOLAR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kim et&nbsp;al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> and our <span id="S2.T1.20.1" class="ltx_text ltx_font_typewriter">EEVE-Korean-10.8B-v1.0</span>.</figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We trained a new tokenizer on our Korean corpus. Since our goal is to maximize the leverage of the base model’s performance, we maintained the base model’s vocabulary and added 8,960 tokens from the corpus that appeared at least 6,000 times, prioritizing those with the highest frequency. Ultimately, the tokenizer’s vocabulary expanded to 40,960 tokens for <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">EEVE-Korean-10.8B-v1.0</span>. This process entailed several rounds of tokenizer training and a manual curation of tokens, based on an analysis of token frequency, ensuring a comprehensive and relevant vocabulary for our model.
As shown in Table&nbsp;<a href="#S2.T1" title="Table 1 ‣ 2.1 Preliminary 1: Tokenizer Training ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the overall token consumption for Korean texts is significantly improved, almost three-fold, contributing to the reduction of computational costs during the entire training process.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F1.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2402.14714/assets/figure_latex/figures/figure-stages-0.png" id="S2.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="117" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Stage 0</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F1.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2402.14714/assets/figure_latex/figures/figure-stages-1.png" id="S2.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="134" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Stage 1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F1.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2402.14714/assets/figure_latex/figures/figure-stages-2.png" id="S2.F1.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="134" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Stage 2</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F1.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/2402.14714/assets/figure_latex/figures/figure-stages-3.png" id="S2.F1.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="134" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Stage 3</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F1.sf5" class="ltx_figure ltx_figure_panel"><img src="/html/2402.14714/assets/figure_latex/figures/figure-stages-4.png" id="S2.F1.sf5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="134" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>Stage 4</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F1.sf6" class="ltx_figure ltx_figure_panel"><img src="/html/2402.14714/assets/figure_latex/figures/figure-stages-5.png" id="S2.F1.sf6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="134" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>Stage 5</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F1.sf7" class="ltx_figure ltx_figure_panel"><img src="/html/2402.14714/assets/figure_latex/figures/figure-stages-6.png" id="S2.F1.sf7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="134" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(g) </span>Stage 6</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S2.F1.sf8" class="ltx_figure ltx_figure_panel"><img src="/html/2402.14714/assets/figure_latex/figures/figure-stages-7.png" id="S2.F1.sf8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="134" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(h) </span>Stage 7</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Training stages with parameter freezing. The fire and snowflake emojis indicate the trainable and frozen parameters respectively.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Preliminary 2: Subword-based Embeddings Initialization</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The integration process starts before actual training, introducing new input and output embeddings, called <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">embed_tokens</span> and <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_typewriter">lm_head</span>, to the model’s parameters. This preliminary step is crucial for preparing for the sophisticated learning process that follows.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">For the input embeddings of the newly added tokens, we adopt the approach of using the average embeddings of the subword tokens that make up these new tokens as in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Hewitt (<a href="#bib.bib7" title="" class="ltx_ref">2021</a>); Welch et&nbsp;al. (<a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite>. This method utilizes the semantic richness of the model’s existing subword embeddings to offer a meaningful starting point for the new tokens’ representations.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Conversely, the output embeddings for the newly added tokens are initialized with the embeddings of the first subword token that comprises the new token. This strategy aims to align the new tokens’ output representations closely with the semantic characteristics of their constituent subwords, enabling a smoother integration into the model’s predictive framework. The significance of such initialization will be further discussed.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Multi-stage Training</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Here we describe the nuanced approach of our seven-stage training methodology for efficient vocabulary expansion, emphasizing the meticulous process of integrating new tokens derived from languages beyond the initial English-centric training scope.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2402.14714/assets/figure_latex/figures/figure-subword.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="449" height="498" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An illustrative example of showing how our subword-based embedding initialization enables harmonize the old and new tokens at Stage 1. In Stage 1, the output embeddings of newly added tokens are initialized with the output embeddings of their first subword tokens that make up these new tokens, such that the last hidden representation for predicting “하세요” yields the same logits for the newly added token “하세요” with its first subword token “하”. Even if we give the new token “하세요” as a gold token, the gradients are eventually computed based on its subword token “하”, so the model takes the input embeddings of “하세요” to predict its subword “하”.</figcaption>
</figure>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">Stage 1 (new input embeddings):</span> Initially, our focus is narrow yet critical: to learn the input embeddings of the newly added tokens while freezing all other model parameters. This stage is foundational, allowing the model to adjust its recognition and processing of these tokens from the beginning. The pre-initialized embeddings serve as a starting point, guiding the model to better utilize these new tokens in its existing framework. Our principal hypothesis here is that if the input and output token sequences in causal language modeling can be differentiated, by utilizing both the old and new tokenizers at the same time, the model can more efficiently and effectively learn new vocabulary embeddings, as it could leverage its established knowledge in the embedding spaces from old tokens. However, employing distinct tokenizers for input and output sequences at once poses implementation challenges, such as the difficulty of applying teacher forcing due to mismatched input/output sequences. Here, the subword-based embedding initialization (Sec&nbsp;<a href="#S2.SS2" title="2.2 Preliminary 2: Subword-based Embeddings Initialization ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>) provides a proxy for using the old tokenizer for output sequences, such that the model is tasked to generate the subword token (old) given the whole word token (new).
In other words, the model could learn to align their representations for generating the new token with that for generating its first subword token, by optimizing only the input embeddings without any modification of input/output token sequences as described in Figure&nbsp;<a href="#S2.F2" title="Figure 2 ‣ 2.3 Multi-stage Training ‣ 2 Efficient and Effective Vocabulary Expansion ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
However, at this stage, the model is not yet able to distinguish between tokens sharing the same hidden state.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">Stage 2 (new output embeddings):</span>
Our goal is to enhance the model’s proficiency in accurately generating new tokens across various contexts by solely adjusting the output embeddings (<span id="S2.SS3.p3.1.2" class="ltx_text ltx_font_typewriter">lm_head</span>). The decision to freeze all other parameters stems from the model’s current unstable state. Allowing both input and output embeddings to be trained simultaneously would complicate achieving convergence, thus hindering the model’s progress toward optimal performance. By freezing most of the parameters, we achieve more stable convergence. Moreover, this approach significantly reduces the training time, as it eliminates the necessity for backpropagation through the other layers.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_bold">Stage 3 (new input and output embeddings):</span>
At this stage, the input embeddings (<span id="S2.SS3.p4.1.2" class="ltx_text ltx_font_typewriter">embed_tokens</span>) still remain optimized based on the initial embeddings of the output embeddings.
This stage allows the updates of both input and output embeddings of the newly added tokens simultaneously.
By aligning between input and output embeddings, the model learns to use the new tokens in both understanding and prediction.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS3.p5.1" class="ltx_p"><span id="S2.SS3.p5.1.1" class="ltx_text ltx_font_bold">Stage 4 (all output embeddings):</span>
As all the original parameters of the base model were frozen until this stage, we assumed the logits between old and new tokenizers were differently scaled, or less optimized to be used as a whole vocabulary. To this end, we begin to allow the update of the old parameters, specifically the output embeddings of old tokens here, making the model better generate the new tokens. In our preliminary experiments, we found this stage is critical for improving the model’s generative capabilities.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para ltx_noindent">
<p id="S2.SS3.p6.1" class="ltx_p"><span id="S2.SS3.p6.1.1" class="ltx_text ltx_font_bold">Stage 5 (new input and all output embeddings):</span> At this stage, the training extends to fine-tuning all output embeddings across the model’s vocabulary while continuing to refine the input embeddings for the newly added tokens. The goal is to ensure that the model can accurately predict any token within its expanded vocabulary. This phase emphasizes the integration of new tokens within the broader context of the model’s linguistic understanding, ensuring that they are both well-represented as inputs and accurately generated as outputs. This dual focus aids in harmonizing the model’s overall performance, ensuring that the expanded vocabulary is seamlessly woven into its language generation processes.</p>
</div>
<div id="S2.SS3.p7" class="ltx_para ltx_noindent">
<p id="S2.SS3.p7.1" class="ltx_p"><span id="S2.SS3.p7.1.1" class="ltx_text ltx_font_bold">Stage 6 (all layers):</span> Contrary to being the final phase, this stage represents an advanced step in the vocabulary expansion process, where all model parameters are subject to optimization, including both newly introduced and pre-existing ones. The focus here is on integrating the enhancements made to the embedding layers within the model’s overall parameters. Techniques such as QLoRA are utilized not just for efficiency but to ensure the preservation of the model’s strong capabilities as much as possible, while allowing effective integration of the expanded vocabulary.</p>
</div>
<div id="S2.SS3.p8" class="ltx_para ltx_noindent">
<p id="S2.SS3.p8.1" class="ltx_p"><span id="S2.SS3.p8.1.1" class="ltx_text ltx_font_bold">Stage 7 (internal layers):</span> Following the extensive integration and optimization efforts, this stage serves as a “cool down” phase, focusing on updating the model’s internal layers, which includes all the layers except the input and output embedding layers.
The objective is to ensure that the enhancements made during the vocabulary expansion are deeply embedded within the model’s core processing capabilities.
This phase prepares the model for robust performance, ensuring that it not only recognizes and generates the new tokens but does so with a nuanced understanding of their use in varied linguistic contexts.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Implementation Details</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">For <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">pre-training</span>, we curated publicly available Korean corpora from diverse sources, such as Korean web content, English vocabulary, and parallel corpus in Korean AI Hub<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://aihub.or.kr/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aihub.or.kr/</a></span></span></span>, etc.
To construct a high-quality pre-training corpus, we applied a set of preprocessing rules: 1) perplexity-based filtering, 2) n-gram repetition&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2023a</a>)</cite>-based filtering, and 3) stopword-based filtering.
For the efficient training of newly added Korean tokens, we intentionally filtered out documents that do not contain many of these tokens.
Subsequently, we acquired a pre-training corpus totaling 3.2M documents (or, 6.7GB).</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:398.9pt;height:189.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(80.3pt,-38.2pt) scale(1.67324249457693,1.67324249457693) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt">Model</th>
<th id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<table id="S3.T2.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T2.1.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Total</td>
</tr>
<tr id="S3.T2.1.1.1.1.2.1.2" class="ltx_tr">
<td id="S3.T2.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(tokens)</td>
</tr>
</tbody></table>
</th>
<th id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S3.T2.1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T2.1.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Average</td>
</tr>
<tr id="S3.T2.1.1.1.1.3.1.2" class="ltx_tr">
<td id="S3.T2.1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(tokens)</td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.2.1" class="ltx_tr">
<td id="S3.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SOLAR-10.7B</td>
<td id="S3.T2.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3.1B</td>
<td id="S3.T2.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">964</td>
</tr>
<tr id="S3.T2.1.1.3.2" class="ltx_tr">
<td id="S3.T2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_r">EEVE-Korean-10.8B-v1.0</td>
<td id="S3.T2.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r">1.6B</td>
<td id="S3.T2.1.1.3.2.3" class="ltx_td ltx_align_left">500</td>
</tr>
<tr id="S3.T2.1.1.4.3" class="ltx_tr">
<td id="S3.T2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Phi-2</td>
<td id="S3.T2.1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">5.6B</td>
<td id="S3.T2.1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_t">1748</td>
</tr>
<tr id="S3.T2.1.1.5.4" class="ltx_tr">
<td id="S3.T2.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">EEVE-Korean-2.8B-v1.0</td>
<td id="S3.T2.1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">1.6B</td>
<td id="S3.T2.1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_bb">484</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of tokenizers for our 6.7GB pre-training corpus of a total 3.2M documents.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">As can be seen in Table&nbsp;<a href="#S3.T2" title="Table 2 ‣ 3.1 Datasets ‣ 3 Implementation Details ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, for the entire corpus, the SOLAR tokenizer needed to use 3.1B tokens to represent them, but our new tokenizer can do so with almost half, using only 1.6B tokens. This difference becomes even more pronounced in the case of Phi-2 and <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">EEVE-Korean-2.8B</span> models, where they require 5.6B tokens and 1.6B tokens respectively.
Considering that transformers have a quadratic-increasing computation complexity with respect to token length, this can be interpreted in two significant ways. First, it allows for processing sequences more than 4 times longer on the same GPU. Or second, it means our model can be trained nearly 4 times more computationally efficiently on the same dataset. This difference becomes even more pronounced in the case of the Phi-2 and <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">EEVE-Korean-2.8B</span> tokenizers.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">For <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">fine-tuning</span> of <span id="S3.SS1.p3.1.2" class="ltx_text ltx_font_typewriter">EEVE-Korean</span> models, we employed the Direct Preference Optimization (DPO;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Rafailov et&nbsp;al. <a href="#bib.bib25" title="" class="ltx_ref">2023</a></cite>) based on LLaMA-Factory implementation.
To further enhance the models’ capabilities of following Korean instructions, we translated the publicly available instruction datasets, specifically Orca<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup</a></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Mukherjee et&nbsp;al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>); Lian et&nbsp;al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite> and UltraFeedback<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned</a></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Cui et&nbsp;al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> into Korean.
In the process of translating these datasets into Korean, ensuring the integrity of programming code formats and correcting translation errors, such as instances where both the source and target languages were inadvertently translated into Korean, was crucial for maintaining the quality and effectiveness of our fine-tuned models.
We named the fine-tuned models as <span id="S3.SS1.p3.1.3" class="ltx_text ltx_font_typewriter">EEVE-Korean-Instruct</span>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As foundational architectures, we opt for SOLAR-10.7B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kim et&nbsp;al. (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> and Phi-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2023b</a>)</cite>, because both have shown outstanding performances among similar sizes of LLMs. This choice of foundational architectures aligns with our strategic training objectives, leveraging their proven strengths to ensure our new models achieve similar levels of language understanding and reasoning capabilities in Korean.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">For the training of the model variants, we utilized two distinct codebases: Axolotl<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/OpenAccess-AI-Collective/axolotl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenAccess-AI-Collective/axolotl</a></span></span></span> for the initial pre-training phase and LLaMA-Factory<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://github.com/hiyouga/LLaMA-Factory" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/hiyouga/LLaMA-Factory</a></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">hiyouga (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> for subsequent fine-tuning. These codebases provided a strong and reliable base for our training process.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Specifically, we train our models with a setup of 8 x NVIDIA H100 GPUs with 80GB memory each, utilizing 64 CPU cores.
For <span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_typewriter">EEVE-Korean-10.8B-v1.0</span>, under bf16 precision, the training process is configured with a sequence of length 4096, gradient accumulation steps set to 4, and a micro-batch size of 8, whereas <span id="S3.SS2.p3.1.2" class="ltx_text ltx_font_typewriter">EEVE-Korean-2.8B-v1.0</span> adopts a sequence length of 2048, gradient accumulation of 16, and a micro-batch size of 16. We employ the AdamW&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite> optimizer, paired with a cosine learning rate scheduler that includes a warmup phase of 10 steps. The learning rate for the 10.8B variant is set to 4e-5, while we used 2e-4 for the small model. We continued training at each stage until the loss converged, observing the loss converged before reaching 400 global steps, which signifies the efficiency of our training strategy.
Though our training strategy involves 7 different stages, it is noteworthy that, for our 2.8B variant, the overall pre-training can be done in less than two days as optimizing only the output embeddings doesn’t incur much computation.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluations</h2>

<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:424.9pt;height:220.5pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-88.5pt,45.8pt) scale(0.705992702245592,0.705992702245592) ;">
<table id="S4.T3.6.6" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.6.6.7.1" class="ltx_tr">
<td id="S4.T3.6.6.7.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S4.T3.6.6.7.1.1.1" class="ltx_text">Model</span></td>
<td id="S4.T3.6.6.7.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.6.6.7.1.2.1" class="ltx_text">Types</span></td>
<td id="S4.T3.6.6.7.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">English</td>
<td id="S4.T3.6.6.7.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="5">Korean</td>
<td id="S4.T3.6.6.7.1.5" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.6.6.7.1.5.1" class="ltx_text ltx_font_bold">Avg.</span></td>
</tr>
<tr id="S4.T3.6.6.8.2" class="ltx_tr">
<td id="S4.T3.6.6.8.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.6.8.2.1.1" class="ltx_text">
<span id="S4.T3.6.6.8.2.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.6.8.2.1.1.1.1" class="ltx_tr">
<span id="S4.T3.6.6.8.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">BQ</span></span>
<span id="S4.T3.6.6.8.2.1.1.1.2" class="ltx_tr">
<span id="S4.T3.6.6.8.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(0)</span></span>
</span></span></td>
<td id="S4.T3.6.6.8.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.6.8.2.2.1" class="ltx_text">
<span id="S4.T3.6.6.8.2.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.6.8.2.2.1.1.1" class="ltx_tr">
<span id="S4.T3.6.6.8.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">CP</span></span>
<span id="S4.T3.6.6.8.2.2.1.1.2" class="ltx_tr">
<span id="S4.T3.6.6.8.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(0)</span></span>
</span></span></td>
<td id="S4.T3.6.6.8.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.6.8.2.3.1" class="ltx_text">
<span id="S4.T3.6.6.8.2.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.6.8.2.3.1.1.1" class="ltx_tr">
<span id="S4.T3.6.6.8.2.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">HS</span></span>
<span id="S4.T3.6.6.8.2.3.1.1.2" class="ltx_tr">
<span id="S4.T3.6.6.8.2.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(0)</span></span>
</span></span></td>
<td id="S4.T3.6.6.8.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.6.8.2.4.1" class="ltx_text">
<span id="S4.T3.6.6.8.2.4.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.6.8.2.4.1.1.1" class="ltx_tr">
<span id="S4.T3.6.6.8.2.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">BQ</span></span>
<span id="S4.T3.6.6.8.2.4.1.1.2" class="ltx_tr">
<span id="S4.T3.6.6.8.2.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(0)</span></span>
</span></span></td>
<td id="S4.T3.6.6.8.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.6.8.2.5.1" class="ltx_text">
<span id="S4.T3.6.6.8.2.5.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.6.8.2.5.1.1.1" class="ltx_tr">
<span id="S4.T3.6.6.8.2.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">CP</span></span>
<span id="S4.T3.6.6.8.2.5.1.1.2" class="ltx_tr">
<span id="S4.T3.6.6.8.2.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(0)</span></span>
</span></span></td>
<td id="S4.T3.6.6.8.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.6.8.2.6.1" class="ltx_text">
<span id="S4.T3.6.6.8.2.6.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.6.8.2.6.1.1.1" class="ltx_tr">
<span id="S4.T3.6.6.8.2.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">HS</span></span>
<span id="S4.T3.6.6.8.2.6.1.1.2" class="ltx_tr">
<span id="S4.T3.6.6.8.2.6.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(0)</span></span>
</span></span></td>
<td id="S4.T3.6.6.8.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.6.8.2.7.1" class="ltx_text">
<span id="S4.T3.6.6.8.2.7.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.6.8.2.7.1.1.1" class="ltx_tr">
<span id="S4.T3.6.6.8.2.7.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">SN</span></span>
<span id="S4.T3.6.6.8.2.7.1.1.2" class="ltx_tr">
<span id="S4.T3.6.6.8.2.7.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(0)</span></span>
</span></span></td>
<td id="S4.T3.6.6.8.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.6.8.2.8.1" class="ltx_text">
<span id="S4.T3.6.6.8.2.8.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.6.6.8.2.8.1.1.1" class="ltx_tr">
<span id="S4.T3.6.6.8.2.8.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">WIC</span></span>
<span id="S4.T3.6.6.8.2.8.1.1.2" class="ltx_tr">
<span id="S4.T3.6.6.8.2.8.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(0)</span></span>
</span></span></td>
</tr>
<tr id="S4.T3.6.6.9.3" class="ltx_tr">
<td id="S4.T3.6.6.9.3.1" class="ltx_td ltx_align_left ltx_border_t">meta-llama/Llama-2-7b-hf</td>
<td id="S4.T3.6.6.9.3.2" class="ltx_td ltx_align_center ltx_border_t">PT</td>
<td id="S4.T3.6.6.9.3.3" class="ltx_td ltx_align_center ltx_border_t">0.7774</td>
<td id="S4.T3.6.6.9.3.4" class="ltx_td ltx_align_center ltx_border_t">0.8700</td>
<td id="S4.T3.6.6.9.3.5" class="ltx_td ltx_align_center ltx_border_t">0.5714</td>
<td id="S4.T3.6.6.9.3.6" class="ltx_td ltx_align_center ltx_border_t">0.5242</td>
<td id="S4.T3.6.6.9.3.7" class="ltx_td ltx_align_center ltx_border_t">0.5700</td>
<td id="S4.T3.6.6.9.3.8" class="ltx_td ltx_align_center ltx_border_t">0.4420</td>
<td id="S4.T3.6.6.9.3.9" class="ltx_td ltx_align_center ltx_border_t">0.4610</td>
<td id="S4.T3.6.6.9.3.10" class="ltx_td ltx_align_center ltx_border_t">0.4881</td>
<td id="S4.T3.6.6.9.3.11" class="ltx_td ltx_align_center ltx_border_t">0.5880</td>
</tr>
<tr id="S4.T3.6.6.10.4" class="ltx_tr">
<td id="S4.T3.6.6.10.4.1" class="ltx_td ltx_align_left">meta-llama/Llama-2-13b-hf</td>
<td id="S4.T3.6.6.10.4.2" class="ltx_td ltx_align_center">PT</td>
<td id="S4.T3.6.6.10.4.3" class="ltx_td ltx_align_center">0.8055</td>
<td id="S4.T3.6.6.10.4.4" class="ltx_td ltx_align_center">0.9100</td>
<td id="S4.T3.6.6.10.4.5" class="ltx_td ltx_align_center">0.6006</td>
<td id="S4.T3.6.6.10.4.6" class="ltx_td ltx_align_center">0.5214</td>
<td id="S4.T3.6.6.10.4.7" class="ltx_td ltx_align_center">0.6010</td>
<td id="S4.T3.6.6.10.4.8" class="ltx_td ltx_align_center">0.4380</td>
<td id="S4.T3.6.6.10.4.9" class="ltx_td ltx_align_center">0.5038</td>
<td id="S4.T3.6.6.10.4.10" class="ltx_td ltx_align_center">0.4881</td>
<td id="S4.T3.6.6.10.4.11" class="ltx_td ltx_align_center">0.6086</td>
</tr>
<tr id="S4.T3.6.6.11.5" class="ltx_tr">
<td id="S4.T3.6.6.11.5.1" class="ltx_td ltx_align_left">mistralai/Mistral-7B-v0.1</td>
<td id="S4.T3.6.6.11.5.2" class="ltx_td ltx_align_center">PT</td>
<td id="S4.T3.6.6.11.5.3" class="ltx_td ltx_align_center">0.8379</td>
<td id="S4.T3.6.6.11.5.4" class="ltx_td ltx_align_center">0.9200</td>
<td id="S4.T3.6.6.11.5.5" class="ltx_td ltx_align_center">0.6129</td>
<td id="S4.T3.6.6.11.5.6" class="ltx_td ltx_align_center">0.6282</td>
<td id="S4.T3.6.6.11.5.7" class="ltx_td ltx_align_center">0.5880</td>
<td id="S4.T3.6.6.11.5.8" class="ltx_td ltx_align_center">0.4300</td>
<td id="S4.T3.6.6.11.5.9" class="ltx_td ltx_align_center">0.5365</td>
<td id="S4.T3.6.6.11.5.10" class="ltx_td ltx_align_center">0.4881</td>
<td id="S4.T3.6.6.11.5.11" class="ltx_td ltx_align_center">0.6302</td>
</tr>
<tr id="S4.T3.6.6.12.6" class="ltx_tr">
<td id="S4.T3.6.6.12.6.1" class="ltx_td ltx_align_left">meta-llama/Llama-2-7b-chat-hf</td>
<td id="S4.T3.6.6.12.6.2" class="ltx_td ltx_align_center">FT</td>
<td id="S4.T3.6.6.12.6.3" class="ltx_td ltx_align_center">0.7976</td>
<td id="S4.T3.6.6.12.6.4" class="ltx_td ltx_align_center">0.8700</td>
<td id="S4.T3.6.6.12.6.5" class="ltx_td ltx_align_center">0.5779</td>
<td id="S4.T3.6.6.12.6.6" class="ltx_td ltx_align_center">0.5157</td>
<td id="S4.T3.6.6.12.6.7" class="ltx_td ltx_align_center">0.5530</td>
<td id="S4.T3.6.6.12.6.8" class="ltx_td ltx_align_center">0.4160</td>
<td id="S4.T3.6.6.12.6.9" class="ltx_td ltx_align_center">0.4987</td>
<td id="S4.T3.6.6.12.6.10" class="ltx_td ltx_align_center">0.4881</td>
<td id="S4.T3.6.6.12.6.11" class="ltx_td ltx_align_center">0.5896</td>
</tr>
<tr id="S4.T3.6.6.13.7" class="ltx_tr">
<td id="S4.T3.6.6.13.7.1" class="ltx_td ltx_align_left">meta-llama/Llama-2-13b-chat-hf</td>
<td id="S4.T3.6.6.13.7.2" class="ltx_td ltx_align_center">FT</td>
<td id="S4.T3.6.6.13.7.3" class="ltx_td ltx_align_center">0.8165</td>
<td id="S4.T3.6.6.13.7.4" class="ltx_td ltx_align_center">0.8800</td>
<td id="S4.T3.6.6.13.7.5" class="ltx_td ltx_align_center">0.6072</td>
<td id="S4.T3.6.6.13.7.6" class="ltx_td ltx_align_center">0.5057</td>
<td id="S4.T3.6.6.13.7.7" class="ltx_td ltx_align_center">0.5760</td>
<td id="S4.T3.6.6.13.7.8" class="ltx_td ltx_align_center">0.4040</td>
<td id="S4.T3.6.6.13.7.9" class="ltx_td ltx_align_center">0.4685</td>
<td id="S4.T3.6.6.13.7.10" class="ltx_td ltx_align_center">0.4881</td>
<td id="S4.T3.6.6.13.7.11" class="ltx_td ltx_align_center">0.5933</td>
</tr>
<tr id="S4.T3.6.6.14.8" class="ltx_tr">
<td id="S4.T3.6.6.14.8.1" class="ltx_td ltx_align_left ltx_border_t">upstage/SOLAR-10.7B-v1.0 (base)</td>
<td id="S4.T3.6.6.14.8.2" class="ltx_td ltx_align_center ltx_border_t">PT</td>
<td id="S4.T3.6.6.14.8.3" class="ltx_td ltx_align_center ltx_border_t">0.8257</td>
<td id="S4.T3.6.6.14.8.4" class="ltx_td ltx_align_center ltx_border_t">0.8700</td>
<td id="S4.T3.6.6.14.8.5" class="ltx_td ltx_align_center ltx_border_t">0.6393</td>
<td id="S4.T3.6.6.14.8.6" class="ltx_td ltx_align_center ltx_border_t">0.5057</td>
<td id="S4.T3.6.6.14.8.7" class="ltx_td ltx_align_center ltx_border_t">0.5750</td>
<td id="S4.T3.6.6.14.8.8" class="ltx_td ltx_align_center ltx_border_t">0.4320</td>
<td id="S4.T3.6.6.14.8.9" class="ltx_td ltx_align_center ltx_border_t">0.6146</td>
<td id="S4.T3.6.6.14.8.10" class="ltx_td ltx_align_center ltx_border_t">0.4881</td>
<td id="S4.T3.6.6.14.8.11" class="ltx_td ltx_align_center ltx_border_t">0.6188</td>
</tr>
<tr id="S4.T3.6.6.15.9" class="ltx_tr">
<td id="S4.T3.6.6.15.9.1" class="ltx_td ltx_align_left">upstage/SOLAR-10.7B-Instruct-v1.0</td>
<td id="S4.T3.6.6.15.9.2" class="ltx_td ltx_align_center">FT</td>
<td id="S4.T3.6.6.15.9.3" class="ltx_td ltx_align_center"><span id="S4.T3.6.6.15.9.3.1" class="ltx_text ltx_font_bold">0.8853</span></td>
<td id="S4.T3.6.6.15.9.4" class="ltx_td ltx_align_center"><span id="S4.T3.6.6.15.9.4.1" class="ltx_text ltx_font_bold">0.9400</span></td>
<td id="S4.T3.6.6.15.9.5" class="ltx_td ltx_align_center"><span id="S4.T3.6.6.15.9.5.1" class="ltx_text ltx_font_bold">0.6866</span></td>
<td id="S4.T3.6.6.15.9.6" class="ltx_td ltx_align_center">0.8184</td>
<td id="S4.T3.6.6.15.9.7" class="ltx_td ltx_align_center">0.6370</td>
<td id="S4.T3.6.6.15.9.8" class="ltx_td ltx_align_center">0.4560</td>
<td id="S4.T3.6.6.15.9.9" class="ltx_td ltx_align_center">0.5668</td>
<td id="S4.T3.6.6.15.9.10" class="ltx_td ltx_align_center">0.4921</td>
<td id="S4.T3.6.6.15.9.11" class="ltx_td ltx_align_center">0.6853</td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left">beomi/OPEN-SOLAR-KO-10.7B<sup id="S4.T3.1.1.1.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center">PT</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center">0.8187</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center">0.8800</td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center">0.5570</td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_align_center">0.8355</td>
<td id="S4.T3.1.1.1.7" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.7.1" class="ltx_text ltx_font_bold">0.8010</span></td>
<td id="S4.T3.1.1.1.8" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.1.8.1" class="ltx_text ltx_font_bold">0.5040</span></td>
<td id="S4.T3.1.1.1.9" class="ltx_td ltx_align_center">0.6952</td>
<td id="S4.T3.1.1.1.10" class="ltx_td ltx_align_center">0.4897</td>
<td id="S4.T3.1.1.1.11" class="ltx_td ltx_align_center">0.6976</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2.1" class="ltx_td ltx_align_left">yanolja/EEVE-Korean-10.8B-v1.0<sup id="S4.T3.2.2.2.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center">PT</td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center">0.8492</td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center">0.9000</td>
<td id="S4.T3.2.2.2.5" class="ltx_td ltx_align_center">0.6203</td>
<td id="S4.T3.2.2.2.6" class="ltx_td ltx_align_center">0.8568</td>
<td id="S4.T3.2.2.2.7" class="ltx_td ltx_align_center">0.7530</td>
<td id="S4.T3.2.2.2.8" class="ltx_td ltx_align_center">0.4900</td>
<td id="S4.T3.2.2.2.9" class="ltx_td ltx_align_center">0.6675</td>
<td id="S4.T3.2.2.2.10" class="ltx_td ltx_align_center"><span id="S4.T3.2.2.2.10.1" class="ltx_text ltx_font_bold">0.4992</span></td>
<td id="S4.T3.2.2.2.11" class="ltx_td ltx_align_center">0.7045</td>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.3.1" class="ltx_td ltx_align_left">yanolja/EEVE-Korean-Instruct-10.8B-v1.0<sup id="S4.T3.3.3.3.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.3.3.3.2" class="ltx_td ltx_align_center">FT</td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center">0.8810</td>
<td id="S4.T3.3.3.3.4" class="ltx_td ltx_align_center">0.9300</td>
<td id="S4.T3.3.3.3.5" class="ltx_td ltx_align_center">0.6502</td>
<td id="S4.T3.3.3.3.6" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.3.6.1" class="ltx_text ltx_font_bold">0.8860</span></td>
<td id="S4.T3.3.3.3.7" class="ltx_td ltx_align_center">0.7610</td>
<td id="S4.T3.3.3.3.8" class="ltx_td ltx_align_center">0.4700</td>
<td id="S4.T3.3.3.3.9" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.3.9.1" class="ltx_text ltx_font_bold">0.9521</span></td>
<td id="S4.T3.3.3.3.10" class="ltx_td ltx_align_center">0.4937</td>
<td id="S4.T3.3.3.3.11" class="ltx_td ltx_align_center"><span id="S4.T3.3.3.3.11.1" class="ltx_text ltx_font_bold">0.7530</span></td>
</tr>
<tr id="S4.T3.6.6.16.10" class="ltx_tr">
<td id="S4.T3.6.6.16.10.1" class="ltx_td ltx_align_left ltx_border_t">microsoft/Phi-2 (base)</td>
<td id="S4.T3.6.6.16.10.2" class="ltx_td ltx_align_center ltx_border_t">PT</td>
<td id="S4.T3.6.6.16.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.6.16.10.3.1" class="ltx_text ltx_font_bold">0.8336</span></td>
<td id="S4.T3.6.6.16.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.6.16.10.4.1" class="ltx_text ltx_font_bold">0.9000</span></td>
<td id="S4.T3.6.6.16.10.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.6.16.10.5.1" class="ltx_text ltx_font_bold">0.5583</span></td>
<td id="S4.T3.6.6.16.10.6" class="ltx_td ltx_align_center ltx_border_t">0.5021</td>
<td id="S4.T3.6.6.16.10.7" class="ltx_td ltx_align_center ltx_border_t">0.4770</td>
<td id="S4.T3.6.6.16.10.8" class="ltx_td ltx_align_center ltx_border_t">0.3280</td>
<td id="S4.T3.6.6.16.10.9" class="ltx_td ltx_align_center ltx_border_t">0.5063</td>
<td id="S4.T3.6.6.16.10.10" class="ltx_td ltx_align_center ltx_border_t">0.4881</td>
<td id="S4.T3.6.6.16.10.11" class="ltx_td ltx_align_center ltx_border_t">0.5742</td>
</tr>
<tr id="S4.T3.4.4.4" class="ltx_tr">
<td id="S4.T3.4.4.4.1" class="ltx_td ltx_align_left">daekeun-ml/phi-2-ko-v0.1<sup id="S4.T3.4.4.4.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.4.4.4.2" class="ltx_td ltx_align_center">PT</td>
<td id="S4.T3.4.4.4.3" class="ltx_td ltx_align_center">0.6141</td>
<td id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center">0.5800</td>
<td id="S4.T3.4.4.4.5" class="ltx_td ltx_align_center">0.3257</td>
<td id="S4.T3.4.4.4.6" class="ltx_td ltx_align_center">0.5164</td>
<td id="S4.T3.4.4.4.7" class="ltx_td ltx_align_center"><span id="S4.T3.4.4.4.7.1" class="ltx_text ltx_font_bold">0.6100</span></td>
<td id="S4.T3.4.4.4.8" class="ltx_td ltx_align_center"><span id="S4.T3.4.4.4.8.1" class="ltx_text ltx_font_bold">0.3860</span></td>
<td id="S4.T3.4.4.4.9" class="ltx_td ltx_align_center">0.4484</td>
<td id="S4.T3.4.4.4.10" class="ltx_td ltx_align_center">0.4881</td>
<td id="S4.T3.4.4.4.11" class="ltx_td ltx_align_center">0.4961</td>
</tr>
<tr id="S4.T3.5.5.5" class="ltx_tr">
<td id="S4.T3.5.5.5.1" class="ltx_td ltx_align_left">yanolja/EEVE-Korean-2.8B-v1.0<sup id="S4.T3.5.5.5.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.5.5.5.2" class="ltx_td ltx_align_center">PT</td>
<td id="S4.T3.5.5.5.3" class="ltx_td ltx_align_center">0.7404</td>
<td id="S4.T3.5.5.5.4" class="ltx_td ltx_align_center">0.8900</td>
<td id="S4.T3.5.5.5.5" class="ltx_td ltx_align_center">0.5247</td>
<td id="S4.T3.5.5.5.6" class="ltx_td ltx_align_center">0.5299</td>
<td id="S4.T3.5.5.5.7" class="ltx_td ltx_align_center">0.5820</td>
<td id="S4.T3.5.5.5.8" class="ltx_td ltx_align_center">0.3800</td>
<td id="S4.T3.5.5.5.9" class="ltx_td ltx_align_center">0.5164</td>
<td id="S4.T3.5.5.5.10" class="ltx_td ltx_align_center">0.4881</td>
<td id="S4.T3.5.5.5.11" class="ltx_td ltx_align_center">0.5814</td>
</tr>
<tr id="S4.T3.6.6.6" class="ltx_tr">
<td id="S4.T3.6.6.6.1" class="ltx_td ltx_align_left ltx_border_bb">yanolja/EEVE-Korean-Instruct-2.8B-v1.0<sup id="S4.T3.6.6.6.1.1" class="ltx_sup">∗</sup>
</td>
<td id="S4.T3.6.6.6.2" class="ltx_td ltx_align_center ltx_border_bb">FT</td>
<td id="S4.T3.6.6.6.3" class="ltx_td ltx_align_center ltx_border_bb">0.8248</td>
<td id="S4.T3.6.6.6.4" class="ltx_td ltx_align_center ltx_border_bb">0.8700</td>
<td id="S4.T3.6.6.6.5" class="ltx_td ltx_align_center ltx_border_bb">0.5392</td>
<td id="S4.T3.6.6.6.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.6.6.6.6.1" class="ltx_text ltx_font_bold">0.7066</span></td>
<td id="S4.T3.6.6.6.7" class="ltx_td ltx_align_center ltx_border_bb">0.5640</td>
<td id="S4.T3.6.6.6.8" class="ltx_td ltx_align_center ltx_border_bb">0.3660</td>
<td id="S4.T3.6.6.6.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.6.6.6.9.1" class="ltx_text ltx_font_bold">0.5290</span></td>
<td id="S4.T3.6.6.6.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.6.6.6.10.1" class="ltx_text ltx_font_bold">0.5230</span></td>
<td id="S4.T3.6.6.6.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.6.6.6.11.1" class="ltx_text ltx_font_bold">0.6153</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Main evaluation results based on <span id="S4.T3.12.1" class="ltx_text ltx_font_typewriter">lm-evaluation-harness</span>. The dataset names are abbreviated for brevity: BQ for BoolQ, CP for COPA, HS for HellaSwag, and SN for SentiNeg. Korean tasks are from&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jang et&nbsp;al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>. Accuracy (<span id="S4.T3.13.2" class="ltx_text ltx_font_typewriter">acc</span>) is used as the evaluation metric for all tasks. In the ‘Types’ column, the models are categorized into two groups: pre-trained (PT) and fine-tuned (FT). We denote the models trained in Korean datasets with <sup id="S4.T3.14.3" class="ltx_sup">∗</sup>. For ease of reproduction, we adopt their official names at HuggingFace.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We evaluate our models on both Korean and English LLM benchmarks, to highlight the advantages of our vocabulary expansion method, which could efficiently leverage the strong multilingual capabilities of base foundational models. Desirably, we expect a model to show improved performance in Korean tasks and comparable performance in English tasks.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Benchmarks</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For Korean tasks, we adopt the KoBEST benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jang et&nbsp;al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>, whose tasks are designed to evaluate the various aspects of language understanding and reasoning. Specifically, this benchmark provides a Korean-translated version of language understanding tasks: boolean question answering (BoolQ;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Clark et&nbsp;al. <a href="#bib.bib3" title="" class="ltx_ref">2019</a></cite>), commonsense causal reasoning (COPA;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Roemmele et&nbsp;al. <a href="#bib.bib26" title="" class="ltx_ref">2011</a></cite>, context-sensitive word understanding (WiC;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Pilehvar and Camacho-Collados <a href="#bib.bib24" title="" class="ltx_ref">2019</a></cite>), commonsense reasoning (HellaSwag;&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Zellers et&nbsp;al. <a href="#bib.bib32" title="" class="ltx_ref">2019</a></cite>), and sentiment negation recognition (SentiNeg).
For English tasks, we employ the following original tasks of KoBEST, BoolQ, COPA, and HellaSwag, which can better highlight the alignment between the English and Korean capabilities of LLMs.
To ensure consistent comparisons, we employ an open-source LLM evaluation framework, <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">lm-evaluation-harness<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note"><span id="footnote9.1.1.1" class="ltx_text ltx_font_serif">9</span></span><a target="_blank" href="https://github.com/EleutherAI/lm-evaluation-harness" title="" class="ltx_ref ltx_url">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We now present evaluation results for both our <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">EEVE-Korean</span> and <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_typewriter">EEVE-Korean-Instruct</span> variants with other top-performing models in Table&nbsp;<a href="#S4.T3" title="Table 3 ‣ 4 Evaluations ‣ Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
<span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_typewriter">EEVE-Korean-10.8B-v1.0</span> outperforms other pre-trained models of similar sizes in the average performance.
It is noteworthy that, <span id="S4.SS2.p1.1.4" class="ltx_text ltx_font_typewriter">EEVE-Korean</span> is the only case where the performance in Korean is improved without compromising the performance in English.
For example, though OPEN-SOLAR-KO-10.7B, which is built on the same base model as ours, performs slightly better than our <span id="S4.SS2.p1.1.5" class="ltx_text ltx_font_typewriter">EEVE-Korean-Instruct-10.8B-v1.0</span>, it fails to preserve the English capabilities, showing lower performance in English tasks than its base model, SOLAR-10.7B-v1.0.
We observe similar trends even for our smaller model, <span id="S4.SS2.p1.1.6" class="ltx_text ltx_font_typewriter">EEVE-Korean-2.8B-v1.0</span> in comparison with the phi-2-ko-v0.1 model, sharing Phi-2 as its base model.
This demonstrates the effectiveness of our training strategy, especially considering that we used even fewer training tokens than our competitors.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">Notably, but not surprisingly, preference tuning on English datasets even makes the models underperform in Korean tasks. For example, LLaMA-2-chat variants, which are the preference-tuned version of LLaMA-2 checkpoints, show improved performances in English tasks (Llama-2-7b 0.7774 <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mo stretchy="false" id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\rightarrow</annotation></semantics></math> Llama-2-7b-chat 0.7976 in English BoolQ), while underperforming in Korean tasks (Llama-2-7b 0.5242 <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mo stretchy="false" id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">\rightarrow</annotation></semantics></math> Llama-2-7b-chat 0.5157 in Korean BoolQ), which highlights the importance of Korean-specific training for LLMs.
On the other hand, we observe that preference tuning our models on Korean instruction datasets doesn’t hurt the model performance in English tasks, rather even improving it. We posit that it is because the embedding spaces are already well-aligned between Korean and English tokens, thus fine-tuning on a specific language doesn’t incur a significant change in model parameters.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion &amp; Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The report introduces <span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">EEVE-Korean-v1.0</span>, a Korean adaptation of large language models that utilizes an Efficient and Effective Vocabulary Expansion (EEVE) method to enhance Korean text processing capabilities significantly. The method, based on parameter freezing and subword initialization, enables the <span id="S5.p1.1.2" class="ltx_text ltx_font_typewriter">EEVE-Korean-10.8B-v1.0</span> model to excel in Korean language tasks while maintaining strong English capabilities. Achieved with a corpus of just 2 billion tokens, this approach represents a notable advancement in language model training efficiency and effectiveness. By making these models available to the research community, the project aims to contribute to the development of more inclusive and efficient language processing technologies.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Expanding our vision, future efforts will explore the application of our vocabulary expansion methodology to additional languages, assessing its generalizability and effectiveness. We aim to not only extend the <span id="S5.p2.1.1" class="ltx_text ltx_font_typewriter">EEVE-Korean</span> model’s linguistic range but also to delve deeper into evaluating its reasoning and generative capabilities through diverse tasks, including complex mathematical reasoning tests like GSM8K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Cobbe et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>, and human evaluations in interactive settings like chatbots&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zheng et&nbsp;al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>. Moreover, efforts to enhance pre-training data quality, and to analyze performance in code-switching scenarios&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite> will underpin our commitment to refining the model’s robustness and versatility. These initiatives are designed to broaden the model’s applicability and efficacy, pushing the boundaries of what is achievable with advanced language models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almazrouei et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">The falcon series of open language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.16867</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)</span>
<span class="ltx_bibblock">
Anthropic. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.anthropic.com/news/claude-2" title="" class="ltx_ref ltx_href">Model card and evaluations for claude models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Anthropic technical Report</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 2924–2936.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et&nbsp;al. 2021.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14168</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023.

</span>
<span class="ltx_bibblock">Ultrafeedback: Boosting language models with high-quality feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01377</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le&nbsp;Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5281/zenodo.10256836" title="" class="ltx_ref ltx_href">A framework for few-shot language model evaluation</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hewitt (2021)</span>
<span class="ltx_bibblock">
John Hewitt. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/nlp.stanford.edu/~johnhew//vocab-expansion.html" title="" class="ltx_ref ltx_href">Initializing new word embeddings for pretrained language models</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">hiyouga (2023)</span>
<span class="ltx_bibblock">
hiyouga. 2023.

</span>
<span class="ltx_bibblock">Llama factory.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/hiyouga/LLaMA-Factory" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/hiyouga/LLaMA-Factory</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Myeongjun Jang, Dohyung Kim, Deuk&nbsp;Sin Kwon, and Eric Davis. 2022.

</span>
<span class="ltx_bibblock">Kobest: Korean balanced evaluation of significant tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International Conference on Computational Linguistics</em>, pages 3697–3708.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06825</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Emma&nbsp;Bou Hanna, Florian Bressand, et&nbsp;al. 2024.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.04088</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15166</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Ildoo Kim, Gunsoo Han, Jiyeon Ham, and Woonhyuk Baek. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/kakaobrain/kogpt" title="" class="ltx_ref ltx_href">Kogpt: Kakaobrain korean(hangul) generative pretrained transformer</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ko et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hyunwoong Ko, Kichang Yang, Minho Ryu, Taekyoon Choi, Seungmu Yang, Sungho Park, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">A technical report for polyglot-ko: Open-source large-scale korean language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.02254</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">L. Junbum (2024)</span>
<span class="ltx_bibblock">
L. Junbum. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://huggingface.co/beomi/SOLAR-KO-10.7B" title="" class="ltx_ref ltx_href">Solar-ko-10.7b</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Huayang Li, Tian Lan, Zihao Fu, Deng Cai, Lemao Liu, Nigel Collier, Taro Watanabe, and Yixuan Su. 2023a.

</span>
<span class="ltx_bibblock">Repetition in repetition out: Towards understanding neural text degeneration from the data perspective.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del&nbsp;Giorno, Suriya Gunasekar, and Yin&nbsp;Tat Lee. 2023b.

</span>
<span class="ltx_bibblock">Textbooks are all you need ii: phi-1.5 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.05463</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, "Teknium", and Nathan Hoos. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup/" title="" class="ltx_ref ltx_href">Slimorca dedup: A deduplicated subset of slimorca</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2018)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2018.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mukherjee et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023.

</span>
<span class="ltx_bibblock">Orca: Progressive learning from complex explanation traces of gpt-4.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.02707</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Chanjun Park, Hwalsuk Lee, Hyunbyung Park, Hyeonwoo Kim, Sanghoon Kim, Seonghwan Cho, Sunghun Kim, and Sukyung Lee. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard" title="" class="ltx_ref ltx_href">Open ko-llm leaderboard</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrov et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Aleksandar Petrov, Emanuele La&nbsp;Malfa, Philip&nbsp;HS Torr, and Adel Bibi. 2023.

</span>
<span class="ltx_bibblock">Language model tokenizers introduce unfairness between languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15425</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pilehvar and Camacho-Collados (2019)</span>
<span class="ltx_bibblock">
Mohammad&nbsp;Taher Pilehvar and Jose Camacho-Collados. 2019.

</span>
<span class="ltx_bibblock">Wic: the word-in-context dataset for evaluating context-sensitive meaning representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 1267–1273.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher&nbsp;D Manning, and Chelsea Finn. 2023.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18290</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roemmele et&nbsp;al. (2011)</span>
<span class="ltx_bibblock">
Melissa Roemmele, Cosmin&nbsp;Adrian Bejan, and Andrew&nbsp;S Gordon. 2011.

</span>
<span class="ltx_bibblock">Choice of plausible alternatives: An evaluation of commonsense causal reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">2011 AAAI Spring Symposium Series</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M Dai, Anja Hauth, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
MosaicML&nbsp;NLP Team et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Introducing mpt-30b: Raising the bar for open-source foundation models.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welch et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Charles Welch, Rada Mihalcea, and Jonathan&nbsp;K Kummerfeld. 2020.

</span>
<span class="ltx_bibblock">Improving low compute language modeling with in-domain embedding initialisation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 8625–8634.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4791–4800.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ruochen Zhang, Samuel Cahyawijaya, Jan Christian&nbsp;Blaise Cruz, and Alham&nbsp;Fikri Aji. 2023.

</span>
<span class="ltx_bibblock">Multilingual large language models are not (yet) code-switchers.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14235</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Jun Zhao, Zhihao Zhang, Qi&nbsp;Zhang, Tao Gui, and Xuanjing Huang. 2024.

</span>
<span class="ltx_bibblock">Llama beyond english: An empirical study on language capability transfer.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.01055</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi&nbsp;Lin, Zhuohan Li, Dacheng Li, Eric Xing, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.05685</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.14713" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.14714" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2402.14714">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.14714" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.14715" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 14:08:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
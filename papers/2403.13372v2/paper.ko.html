<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LlamaFactory: 100+ 언어 모델의 통합 효율적인 미세 조정\n' +
      '\n' +
      ' 야오웨이 정1, 리총 장1, 준하오 장1, 옌한 예1, 쯔옌 루오1, 용창 마2\n' +
      '\n' +
      '중국 북항대학교 컴퓨터공학부\n' +
      '\n' +
      '중국 북경대학 소프트웨어.마이크로전자학부\n' +
      '\n' +
      'hiyouoga@buaa.edu.cn, zhangrc@act.buaa.edu.cn, {zhang.jh,yeyanhan, akanya}@buaa.edu.cn, codingma@pku.edu.cn\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)을 다운스트림 작업에 적용하기 위해서는 효율적인 미세 조정이 필수적이다. 그러나 이러한 방법을 다른 모델에 구현하려면 사소하지 않은 노력이 필요하다. 우리는 최첨단 효율적인 교육 방법을 통합하는 통합된 프레임워크인 라마팩토리를 제시한다. 내장된 웹 UI 라마보드를 통해 코딩할 필요 없이 100+ LLM의 미세 조정을 유연하게 커스터마이징할 수 있다. 우리는 언어 모델링 및 텍스트 생성 태스크에 대한 프레임워크의 효율성과 유효성을 경험적으로 검증한다. [https://github.com/hiyouga/LLMaMa-Factory](https://github.com/hiyouga/LLMaMa-Factory)에서 출시되었으며 이미 13,000개 이상의 별과 1,600개의 포크를 받았습니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대형 언어 모델들(LLMs)(Zhao et al., 2023)은 놀라운 추론 능력들을 제시하고 광범위한 애플리케이션들, 이를테면 질문 응답(Jiang et al., 2023), 기계 번역(Wang et al., 2023; Jiao et al., 2023), 및 정보 추출(Jiao et al., 2023)에 권한을 부여한다. 그 후, 상당한 수의 LLM이 개발되고 오픈 소스 커뮤니티를 통해 액세스할 수 있다. 예를 들어, Hugging Face의 오픈 LLM 리더보드(Beeching et al., 2023)는 5,000개 이상의 모델을 자랑하여 LLM의 힘을 활용하고자 하는 개인에게 편의를 제공한다.\n' +
      '\n' +
      '리소스가 제한된 매우 많은 수의 매개변수를 미세 조정하는 것은 LLM을 다운스트림 작업에 적용하는 주요 과제가 된다. 인기 있는 해결책은 효율적인 미세 조정(Houlsby et al., 2019; Hu et al., 2022; Ben Zaken et al., 2022; Dettmers et al., 2023; Zhao et al., 2024)이며, 이는 다양한 태스크들에 적응할 때 LLM들의 트레이닝 비용을 감소시킨다. 그러나, 커뮤니티는 LLM들을 효율적으로 미세 조정하기 위한 다양한 방법들에 기여하며, 이러한 방법들을 다른 LLM들에 적응하고 통합하고 사용자 맞춤화를 위한 친숙한 인터페이스를 제공하는 체계적인 프레임워크가 부족하다.\n' +
      '\n' +
      '위의 문제를 해결하기 위해 LLM의 미세 조정을 민주화하는 프레임워크인 LlamaFactory를 개발한다. 확장 가능한 모듈을 통해 다양한 효율적인 미세 조정 방법을 통합하여 최소 리소스와 높은 처리량으로 수백 개의 LLM을 미세 조정할 수 있습니다. 또한, 생성 사전 훈련(Radford et al., 2018), 감독 미세 조정(supervised fine-tuning, SFT)(Wei et al., 2022), 인간 피드백으로부터의 강화 학습(RLHF)(Ouyang et al., 2022), 및 직접 선호도 최적화(direct preference optimization, DPO)(Rafailov et al., 2023)를 포함하는 일반적으로 사용되는 훈련 접근법을 간소화한다. 사용자는 명령줄 또는 웹 인터페이스를 활용하여 최소 또는 전혀 코딩 노력 없이 LLM을 사용자 지정하고 미세 조정할 수 있습니다.\n' +
      '\n' +
      'LlamaFactory는 _모델 로더_, _데이터 작업자_ 및 _트레이너_의 세 가지 주요 모듈로 구성됩니다. 특정 모델 및 데이터 세트에 대한 이러한 모듈의 종속성을 최소화하여 프레임워크가 수백 개의 모델 및 데이터 세트로 유연하게 확장할 수 있도록 한다. 구체적으로, 먼저 모델 로더가 정확한 레이어를 식별하여 미리 훈련된 모델에 어댑터를 정확하게 부착할 수 있는 모델 레지스트리를 구축한다. 그런 다음 데이터 작업자가 해당 열을 정렬하여 데이터 세트를 수집할 수 있는 데이터 설명 사양을 개발합니다. 또한, 트레이너가 기본 방법을 대체하여 활성화할 수 있도록 하는 효율적인 미세 조정 방법의 플러그 앤 플레이 구현을 제공한다. 우리의 설계는 이러한 모듈들이 상이한 트레이닝 접근법들에 걸쳐 재사용될 수 있게 하여, 새로운 방법들의 통합 비용을 상당히 감소시킨다.\n' +
      '\n' +
      'LamaFactory는 PyTorch(Paszke et al., 2019)로 구현되며, 트랜스포머(Wolf et al., 2020), PEFT(Mangrulkar et al., 2022) 및 TRL(von Werra et al., 2020)과 같은 오픈 소스 라이브러리로부터 상당한 이점을 얻는다. 이를 기반으로 더 높은 수준의 추상화를 제공하는 즉흥적인 프레임워크를 제공합니다. 또한 Gradio(Abid et al., 2019)를 사용하여 LlamaBoard를 구축하여 코딩 노력 없이 LLM을 미세 조정할 수 있다.\n' +
      '\n' +
      'LamaFactory는 Apache-2.0 라이선스에 따라 오픈 소스됩니다. GitHub1에서는 이미 13,000개 이상의 별과 1,600개의 포크를 얻었고, Hugging Face Hub2에서는 수백 개의 오픈 소스 모델이 LlamaFactory에 구축되었다. 예를 들어, 잘 알려진 GemSUra-7B(Nguyen et al., 2024)는 LlamaFactory를 기반으로 구축되며, 이는 먼저 Gemma(Mesnard et al., 2024)의 교차 언어 능력을 드러낸다. 또한, Wang et al. (2023), Yu et al. (2023), Bhardwaj et al. (2024)와 같은 LLMs에서 새로운 방법을 탐구하기 위해 수십 개의 연구 연구가 우리의 프레임워크를 활용한다.\n' +
      '\n' +
      '각주 1: [https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)\n' +
      '\n' +
      '각주 2: [https://huggingface.co/models?other=llama-factory](https://huggingface.co/models?other=llama-factory)\n' +
      '\n' +
      '## 2 효율적인 미세 조정 기술\n' +
      '\n' +
      '효율적인 LLM 미세 조정 기술은 최적화에 초점을 맞춘 기술과 계산에 초점을 맞춘 기술의 두 가지 주요 범주로 나눌 수 있다. 효율적인 최적화 기법의 주요 목적은 비용을 최소로 유지하면서 LLM의 매개변수를 조정하는 것이다. 반면에, 효율적인 연산 방법은 LLMs에서 필요한 연산을 위한 시간이나 공간을 줄이기 위해 노력한다. 라마 팩토리에 포함된 방법은 표 1에 나열되어 있다. 우리는 이러한 효율적인 미세 조정 기술을 제시하고 다음 섹션에서 프레임워크에 통합함으로써 달성된 실질적인 효율성 향상을 보여줄 것이다.\n' +
      '\n' +
      '### Efficient Optimization\n' +
      '\n' +
      '먼저, LlamaFactory에서 사용되는 효율적인 최적화 기법에 대한 개요를 제공한다. 동결-튜닝 방법(Houlsby et al., 2019)은 디코더 층들의 작은 서브세트에서 나머지 파라미터들을 미세-튜닝하면서 대부분의 파라미터들을 동결시키는 것을 포함한다. 그라디언트 저순위 투영(Gradient low-rank projection, GaLore)(Zhao et al., 2024)이라고 불리는 다른 방법은 그라디언트를 저차원 공간으로 투영하여, 메모리 효율적인 방식으로 풀-파라미터 학습을 용이하게 한다. 반대로, Low-rank adaptation (LoRA)(Hu et al., 2022) 방법은 미리 훈련된 모든 가중치들을 동결시키고, 한 쌍의 훈련 가능한 Low-rank 행렬들을 지정된 레이어에 도입한다. 양자화와 결합될 때, 이 접근법은 QLoRA(Dettmers et al., 2023)로 지칭되며, 이는 추가적으로 메모리 사용량을 감소시킨다. 가중치 분해 저순위 적응(DoRA, Liu et al., 2024) 방법은 LLM의 미세 조정을 향상시키기 위해 방향성 성분에만 LoRA를 적용하여 미리 훈련된 가중치를 크기와 방향 성분으로 분해한다. LoRA+(Hayou et al., 2024)는 LoRA의 부-최적성을 극복하기 위해 제안된다.\n' +
      '\n' +
      '### Efficient Computation\n' +
      '\n' +
      'LlamaFactory에서는 다양한 효율적인 계산 기법을 통합한다. 일반적으로 활용되는 기술은 혼합 정밀 훈련(Micikevicius et al., 2018) 및 활성화 체크포인팅(Chen et al., 2016)을 포함한다. 어텐션 계층의 입력-출력(IO) 비용을 조사하여 인사이트를 도출하고, 플래시 어텐션(Dao et al., 2022)은 어텐션 계산을 향상시키기 위해 하드웨어 친화적인 접근법을 도입한다. S\\({}^{2}\\) 주의집중(Chen et al., 2024)은 블록-희소 주의집중에서 컨텍스트를 확장하는 문제를 해결하여, 미세 조정 롱-컨텍스트 LLMs에서 메모리 사용을 감소시킨다. 다양한 양자화 전략들(Dettmers et al., 2022; Frantar et al., 2023; Lin et al., 2023; Egiazarian et al., 2024)은 가중치들에 대한 더 낮은-정밀 표현들을 이용함으로써 큰 언어 모델들(LLM들)에서 메모리 요건들을 감소시킨다. 그럼에도 불구하고 양자화된 모델의 미세 조정은 LoRA와 같은 어댑터 기반 기술로 제한된다(Hu et al., 2022). Unsloth(Han and Han, 2023)는 LoRA의 역방향 전파를 구현하기 위해 Triton(Tillet et al., 2019)을 통합하며, 이는 경사 하강 동안 부동 소수점 연산(FLOPs)을 감소시키고 신속한 LoRA 트레이닝으로 이어진다.\n' +
      '\n' +
      'LamaFactory는 이러한 기술을 LLM 미세 조정의 효율성을 크게 향상시키는 응집 구조에 효과적으로 결합합니다. 이는 혼합 정밀 트레이닝 동안 파라미터당 18 바이트(Micikevicius et al., 2018) 또는 bfloat16 트레이닝에서 파라미터당 8 바이트(Le Scao et al., 2022)에서 파라미터당 0.6 바이트만으로 메모리 풋프린트를 감소시키는 결과를 초래한다. 우리의 프레임워크에 대한 추가 세부 사항은 후속 섹션에서 제공될 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline  & Freeze-tuning & GaLore & LoRA & DoRA \\\\ \\hline Mixed precision & ✓ & ✓ & ✓ & ✓ \\\\ Checkpointing & ✓ & ✓ & ✓ & ✓ \\\\ Flash attention & ✓ & ✓ & ✓ & ✓ \\\\ S\\({}^{2}\\) attention & ✓ & ✓ & ✓ & ✓ \\\\ Quantization & ✗ & ✗ & ✓ & ✓ \\\\ Unsloth & ✗ & ✗ & ✓ & ✗ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 라마 팩토리에 등장하는 효율적인 미세 조정 기술. 서로 호환되는 기술은 ✓로 표시되고 호환되지 않는 기술은 ✗로 표시된다.\n' +
      '\n' +
      '## 3 LlamaFactory Framework\n' +
      '\n' +
      'LamaFactory는 모델 로더, 데이터 워커 및 트레이너의 세 가지 주요 모듈로 구성된다. 모델 로더는 100개 이상의 LLM을 지원하는 미세 조정을 위한 다양한 아키텍처를 준비한다. 데이터 작업자는 50개 이상의 데이터 세트를 지원하는 잘 설계된 파이프라인을 통해 다양한 작업의 데이터를 처리합니다. 트레이너는 이러한 모델을 다른 작업 및 데이터 세트에 적응시키기 위해 효율적인 미세 조정 방법을 통합하며, 이는 네 가지 훈련 접근법을 제공한다. LamaBoard는 위의 모듈들에 대한 친숙한 시각적 인터페이스를 제공하여, 사용자가 코드 없는 방식으로 개별 LLM 미세 조정 프로세스를 구성하고 시작하고 훈련 상태를 즉석에서 모니터링할 수 있게 한다. 우리는 그림 1에서 라마팩토리의 전체 아키텍처를 설명한다.\n' +
      '\n' +
      '### Model Loader\n' +
      '\n' +
      '이 섹션에서는 처음에 모델 로더의 네 가지 구성 요소인 모델 초기화, 모델 패치, 모델 양자화 및 어댑터 부착을 제시한 다음 미세 조정 시 매개변수 부동 소수점 정밀도를 처리하여 광범위한 장치에 적응하는 접근법에 대해 설명한다.\n' +
      '\n' +
      '모델 초기화 모델 로딩 및 파라미터 초기화를 위해 Transformers의 AutoModel API(Wolf et al., 2020)를 사용한다. 프레임워크가 서로 다른 모델 아키텍처와 호환되도록 하기 위해 각 계층의 유형을 저장하는 모델 레지스트리를 설정하여 효율적인 미세 조정 기술의 활용을 보다 쉽게 한다. 토큰나이저의 어휘 크기가 임베딩 레이어의 용량을 초과하는 경우 레이어의 크기를 조정하고 노이즈 평균 초기화를 통해 새로운 파라미터를 초기화한다. RoPE 스케일링에 대한 스케일링 팩터를 결정하기 위해(Chen 등, 2023), 우리는 이를 모델의 컨텍스트 길이에 대한 최대 입력 시퀀스 길이의 비로 계산한다.\n' +
      '\n' +
      '모델 패칭은 플래시 어텐션과 S\\({}^{2}\\) 어텐션을 가능하게 하기 위해 원숭이 패치를 사용하여 모델의 정방향 계산을 대체한다. 그럼에도 불구하고 Transformers 4.34.0 이후 플래시 어텐션이 지원되었기 때문에 API를 사용하여 플래시 어텐션을 사용하도록 한다. 동적 모듈의 과도한 분할을 방지하기 위해 DeepSpeed ZeRO-3 (Rasley et al., 2020)에서 최적화 할 때 전문가 혼합 블록(MoE)을 리프 모듈로 설정한다.\n' +
      '\n' +
      'Model QuantizationDynamically quantizing models to 8 bits or 4 bits with LLM.int8 (Dettmers et al., 2022) can be performed through the bitsand-bytes library (Dettmers, 2021). 4-비트 양자화를 위해, 우리는 QLoRA로서 이중 양자화 및 4-비트 노멀 플로트를 이용한다(Dettmers et al., 2023). 또한 GPTQ(Frantar et al., 2023), AWQ(Lin et al., 2023) 및 AQLM(Egiazarian et al., 2024)을 포함하는 PTQ(post-training quantization) 방법에 의해 양자화된 모델들의 미세 조정을 지원한다. 양자화된 가중치를 직접 미세 조정할 수는 없으므로 양자화된 모델은 어댑터 기반 방법과만 호환됩니다.\n' +
      '\n' +
      '어댑터 첨부 어댑터를 첨부하기 위해 모델 레지스트리를 사용하여 적절한 계층을 자동으로 식별합니다. 어댑터는 메모리를 절약하기 위해 기본적으로 레이어의 서브세트에 부착되지만, 모든 선형 레이어에 부착하면 더 나은 성능을 얻을 수 있다(Dettmers et al., 2023). PEFT(Mangrulkar et al., 2022) 라이브러리는 LoRA(Hu et al., 2022), rsloRA(Kalajdzievski, 2023), DoRA(Liu et al., 2024)와 같은 어댑터를 부착하는 매우 편리한 방법을 제공한다. LoRA를 가속하기 위해 역방향 계산을 Unsloth(Han and Han, 2023)로 대체한다. 인간 피드백(RLHF)으로부터 강화 학습을 수행하기 위해, 우리는 각 토큰의 표현을 스칼라에 매핑하는 선형 계층인 모델에 값 헤드를 추가한다.\n' +
      '\n' +
      '정밀 적응을 기반으로 사전 훈련된 모델의 부동 소수점 정밀도를 처리한다.\n' +
      '\n' +
      '그림 1: LlamaFactory의 아키텍처.\n' +
      '\n' +
      '장치의 기능. NVIDIA GPU의 경우 계산 능력이 8.0 이상인 경우 bfloat16 정밀도를 사용한다. 그렇지 않으면, float16이 채택된다. 어센드 NPU 및 AMD GPU에는 float16을 사용하고 비 CUDA 디바이스에는 float32를 사용한다. 플로트16 정밀도로 bfloat16 모델을 로드하면 오버플로 문제가 발생할 수 있습니다. 혼합 정밀 훈련에서는 훈련 가능한 모든 파라미터를 float32로 설정하지만, bfloat16 훈련에서는 훈련 가능한 파라미터를 bfloat16으로 유지한다.\n' +
      '\n' +
      '### Data Worker\n' +
      '\n' +
      '데이터 처리 파이프라인은 데이터 로딩, 데이터 정렬, 데이터 병합 및 데이터 전처리를 포함한다. 다양한 태스크의 데이터 세트를 통일된 형식으로 표준화하여 다양한 형식의 데이터 세트에 대한 모델을 미세 조정할 수 있습니다.\n' +
      '\n' +
      '데이터 세트 로드(Lhoest 등, 2021) 라이브러리를 사용하여 데이터를 로드합니다. 이를 통해 사용자는 휴징 페이스 허브에서 원격 데이터 세트를 로드하거나 스크립트를 통해 또는 파일을 통해 로컬 데이터 세트를 읽을 수 있습니다. 데이터 세트 라이브러리는 데이터 처리 중 메모리 오버헤드를 크게 줄이고 화살표(Apache, 2016)를 사용하여 샘플 쿼리를 가속화합니다. 기본적으로 전체 데이터 세트는 로컬 디스크에 다운로드됩니다. 그러나 데이터 세트가 너무 커서 저장할 수 없는 경우, 프레임워크는 다운로드하지 않고 데이터 세트 스트리밍을 반복하여 제공합니다.\n' +
      '\n' +
      '데이터 세트 정렬 데이터 세트 형식을 통일하기 위해 데이터 세트의 구조를 특성화하기 위해 데이터 설명 사양을 설계한다. 예를 들어, 알파카 데이터세트는 명령어, 입력 및 출력의 세 개의 열을 갖는다(Taori et al., 2023). 데이터 기술 사양에 따라 다양한 작업과 호환되는 표준 구조로 데이터 세트를 변환합니다. 데이터 세트 구조의 일부 예는 표 2에 나와 있다.\n' +
      '\n' +
      '데이터 세트 병합 통합 데이터 세트 구조는 여러 데이터 세트를 병합하는 데 효율적인 접근 방식을 제공합니다. 비스트리밍 모드의 데이터 세트는 훈련 중에 데이터 세트가 섞이기 전에 단순히 연결한다. 그러나 스트리밍 모드에서는 데이터 집합을 단순히 연결하면 데이터 셔플링이 방해됩니다. 따라서 우리는 서로 다른 데이터 세트에서 데이터를 교대로 읽을 수 있는 방법을 제공한다.\n' +
      '\n' +
      '데이터 세트 전처리 LlamaFactory는 채팅 완료에 주로 사용되는 텍스트 생성 모델을 미세 조정하기 위해 설계된다. 채팅 템플릿은 이러한 모델의 명령어 수행 능력과 매우 관련이 있기 때문에 이러한 모델에서 중요한 구성 요소이다. 따라서 모델 유형에 따라 자동으로 선택할 수 있는 수십 개의 채팅 템플릿을 제공합니다. 토나이저를 이용하여 채팅 템플릿을 적용한 후 문장을 인코딩한다. 기본적으로, 우리는 완성들에 대한 손실만을 계산하는 반면, 프롬프트들은 무시된다(Taori et al., 2023). 선택적으로, 우리는 생성 사전 트레이닝을 수행할 때 자동으로 인에이블되는 트레이닝 시간을 감소시키기 위해 시퀀스 패킹(Krell 등, 2021)을 활용할 수 있다. 부록 C는 템플릿 디자인의 세부 사항을 보여줍니다.\n' +
      '\n' +
      '### Trainer\n' +
      '\n' +
      '효율적인 TrainingWe는 LoRA+(Hayou et al., 2024) 및 GaLore(Zhao et al., 2024)를 포함한 최첨단의 효율적인 미세 조정 방법들을 디폴트 컴포넌트를 교체함으로써 트레이너에 통합한다. 이러한 훈련 접근 방식은 트레이너와 독립적이어서 다양한 작업에 쉽게 적용할 수 있다. 사전 훈련과 SFT는 Transformers(Wolf et al., 2020)의 트레이너를 활용하였고, RLHF와 DPO는 TRL(von Werra et al., 2020)의 트레이너를 채택하였다. 맞춤형 데이터 콜레이터는 다양한 훈련 접근 방식의 트레이너를 차별화하기 위해 활용된다. 선호도 데이터에 대한 트레이너의 입력 형식을 맞추기 위해 첫 번째 \\(n\\) 샘플이 선택되고 마지막 \\(n\\) 샘플이 거부된 예가 있는 배치에서 \\(2n\\) 샘플을 빌드한다.\n' +
      '\n' +
      '소비자 장치에 대한 모델 공유 RLHF 허용 RLHF 훈련은 LLM 미세 조정에 유용한 특성이다. 그러나 RLHF 훈련은 4개의 다른 모델이 필요하기 때문에 어렵다. 이 문제를 해결하기 위해, 우리는 모델 공유 RLHF를 제안하여, 하나 이상의 사전 훈련된 모델 없이 전체 RLHF 훈련을 가능하게 한다. 구체적으로, 우리는 먼저 보상 모델링을 위한 목적 함수와 어댑터 및 값 헤드를 훈련하여 모델이 보상 점수를 계산할 수 있도록 한다. 그런 다음 다른 어댑터와 값 헤드를 초기화하고 PPO 알고리즘으로 훈련한다(Ouyang et al., 2022). 어댑터와 값 헤드가 동적으로 전환됨\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Plain text & [[’text’: ”...’], [’text’: ”...’]] \\\\ Alpaca-like data & [[’instruction’: ”...’, "input’: ”...’, "output’: ”...’]] \\\\ ShareGPT-like data & [’conversations’: [’from’: ’human’, ”value’: "...’], [’from’: “gpt’, “value’: ”...’]]] \\\\ Preference data & [[’instruction’: ”...’, ”input’: ”...’, “output’: [’...’, ”...’]]] \\\\ \\hline Standardized data & [’prompt’: [’[’role’: ”...’, ‘content’: ”...’]], \\\\  & "response’: [’role’: ”...’, ‘content’: ”...’]]. \\\\  & "system’: [’...’, ‘tools’: ”...’] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: LlamaFactory의 데이터 세트 구조.\n' +
      '\n' +
      '트레이닝 동안 PEFT(Mangrulkar et al., 2022)의 set_adapter 및 disable_adapter API를 통해, 미리 트레이닝된 모델이 정책, 값, 참조 및 보상 모델로서의 역할을 동시에 수행할 수 있게 한다. 우리가 아는 한, 이것은 소비자 장치에 대한 RLHF 훈련을 지원하는 첫 번째 방법이다.\n' +
      '\n' +
      'Distributed Training We can combine the 위 트레이너들과 DeepSpeed (Rasley et al., 2020) for distributed training. DeepSpeed ZeRO 최적화기를 활용하면 분할 또는 오프로딩을 통해 메모리 소비를 더욱 줄일 수 있습니다.\n' +
      '\n' +
      '### Utilities\n' +
      '\n' +
      '추론 시간 동안 가속 추론은 데이터 작업자의 채팅 템플릿을 재사용하여 모델 입력을 구축합니다. 우리는 트랜스포머(Wolf et al., 2020)와 vLLM(Kwon et al., 2023)을 사용하여 모델 출력을 샘플링하는 지원을 제공하며, 둘 다 스트림 디코딩을 지원한다. 또한, 비동기 LLM 엔진과 vLLM의 페이징된 주의를 활용하여 높은 처리량의 동시 추론 서비스를 제공하여 미세 조정된 LLM을 다양한 응용 프로그램에 쉽게 배치할 수 있는 OpenAI 스타일 API를 구현한다.\n' +
      '\n' +
      '종합 평가 우리는 MMLU(Hendrycks et al., 2021), CMMLU(Li et al., 2023), 및 C-Eval(Huang et al., 2023)과 같은 객관식 태스크를 포함하는 LLMs를 평가하기 위한 여러 메트릭을 포함할 뿐만 아니라 BLEU-4(Papineni et al., 2002) 및 ROUGE(Lin, 2004)와 같은 텍스트 유사성 스코어를 계산한다.\n' +
      '\n' +
      '### LlamaBoard: LlamaFactory를 위한 통합 인터페이스\n' +
      '\n' +
      'LamaBoard는 Gradio(Abid et al., 2019)에 기초한 통일된 사용자 인터페이스로서, 사용자가 어떠한 코드도 작성하지 않고 LLMs의 미세 조정을 맞춤화할 수 있게 한다. 간소화 된 모델 미세 조정 및 추론 서비스를 제공 하 여 사용자가 실제에서 100 + LLM 및 50 + 데이터 집합을 쉽게 활용할 수 있습니다. LlamaBoard에는 다음과 같은 주목할 만한 기능이 있습니다. **쉬운 구성** LlamaBoard를 사용하면 사용자가 웹 인터페이스와의 상호 작용을 통해 미세 조정 인수를 사용자 지정할 수 있습니다. 대부분의 사용자에게 권장되는 많은 인수에 기본값을 제공하여 구성 프로세스를 단순화합니다. 또한 사용자는 웹 UI에서 데이터 세트를 미리 보고 사용자 지정 형식을 확인할 수 있습니다.\n' +
      '\n' +
      '**모니터블 교육** 교육 프로세스 중에 교육 로그 및 손실 곡선이 실시간으로 시각화되고 업데이트되어 사용자가 교육 진행 상황을 모니터링할 수 있습니다. 이 기능은 미세 조정 프로세스를 분석하는 데 유용한 통찰력을 제공합니다.\n' +
      '\n' +
      '**유연한 평가**LlamaBoard는 모델을 자동으로 평가하기 위해 데이터 세트에서 텍스트 유사성 점수를 계산하거나 모델과 채팅하여 인간 평가를 수행하는 것을 지원합니다.\n' +
      '\n' +
      '**다국어 지원** 라마보드는 로컬리제이션 파일을 제공하여 인터페이스를 렌더링하기 위한 새 언어의 통합을 용이하게 합니다. 현재 우리는 영어, 러시아어, 중국어의 세 가지 언어를 지원하며, 이는 LLM을 미세 조정하기 위해 더 넓은 범위의 사용자가 LamaBoard를 활용할 수 있도록 한다.\n' +
      '\n' +
      '## 4 Empirical Study\n' +
      '\n' +
      '우리는 두 가지 관점에서 LlamaFactory를 체계적으로 평가한다: 1) 메모리 사용량, 처리량 및 복잡도 측면에서 훈련 효율성. 2) 다운스트림 태스크에 대한 적응의 효과.\n' +
      '\n' +
      '### Training Efficiency\n' +
      '\n' +
      '실험 설정 PubMed(Canese and Weis, 2013) 데이터 세트를 활용했으며, 이는 3,600만 건 이상의 생체 의학 문헌 기록으로 구성된다. 우리는 문헌의 초록에서 약 400,000개의 토큰을 추출하여 훈련 사례를 구성합니다. 우리는 Gemma-2B (Mesnard et al., 2024), Llama2-7B 및 Llama2-13B (Touvron et al., 2023) 모델을 다양한 효율적인 미세 조정 방법을 사용하여 생성 사전 훈련 목표를 사용하여 미세 조정한다. Full-tuning, freeze-tuning, GaLore, LoRA 및 4-bit QLoRA의 결과를 비교한다. 미세 조정 후, 다양한 방법의 효율성을 평가하기 위해 훈련 예제에 대한 복잡도를 계산한다. 또한 사전 훈련된 모델의 복잡성을 기준선으로 통합한다. 더 많은 실험 세부 사항은 부록 D.1에서 찾을 수 있다.\n' +
      '\n' +
      '결과 트레이닝 효율 결과는 표 3에 제시되며, 여기서 메모리는 트레이닝 동안 소비되는 피크 메모리를 지칭하고, 처리량은 초당 트레이닝된 토큰의 수로 계산되며, PPL은 트레이닝 예들에 대한 모델의 복잡도를 나타낸다. 전체 조정 라마2-13B는 메모리 오버플로로 이어지기 때문에 결과는 기록되지 않는다. QLoRA는 사전 훈련된 가중치들이 더 낮은 정밀도로 표현되기 때문에 일관되게 가장 낮은 메모리 풋프린트를 갖는다는 것을 관찰한다. LoRA는 Unsloth에 의해 LoRA 계층에서 최적화를 활용하는 더 높은 처리량을 나타낸다. GaLore는 대형 모델에서는 낮은 PPL을 달성하는 반면 LoRA는 소형 모델에서는 이점이 있다.\n' +
      '\n' +
      '### Downstream 태스크의 미세 조정\n' +
      '\n' +
      '실험 설정 다양한 효율적인 미세 조정 방법의 효과를 평가하기 위해 다운스트림 작업에 대한 미세 조정 후 다양한 모델의 성능을 비교한다. CNN/DM (Nallapati et al., 2016), XSum (Narayan et al., 2018) 및 AdGen (Shao et al., 2019)을 포함한 세 가지 대표적인 텍스트 생성 태스크로부터 각각 2,000개의 예제와 1,000개의 예를 사용하여 훈련 집합과 테스트 집합을 구성한다. 서로 다른 미세 조정 방법을 사용하여 시퀀스 대 시퀀스 태스크에 따라 여러 개의 명령어 조정 모델을 선택하고 미세 조정한다. 본 논문에서는 Full-tuning (FT), GaLore, LoRA 및 4-bit QLoRA의 결과를 비교한다. 미세 조정 후 각 태스크의 테스트 세트에 대한 ROUGE 점수(Lin, 2004)를 계산한다. 우리는 또한 원래 지시 조정 모델의 점수를 기준선으로 통합한다. 더 많은 실험 세부 사항은 부록 D.2에서 확인할 수 있다.\n' +
      '\n' +
      '결과 다운스트림 작업에 대한 평가 결과는 표 4에 나와 있다. 우리는 각 LLM 및 각 데이터 세트에 대해 ROUGE-1, ROUGE-2 및 ROUGE-L에 대한 평균 점수를 보고한다. Gemma-7B의 일부 결과는 GaLore 방법이 이 모델에 적용되지 않기 때문에 표에 포함되지 않는다. 결과에서 흥미로운 발견은 CNN/DM 및 AdGen 데이터 세트에서 Llama2-7B 및 ChatGLM3-6B 모델을 제외하고 LoRA 및 QLoRA가 대부분의 경우에 최상의 성능을 달성한다는 것이다. 이 현상은 LLM 모델을 특정 작업에 적용하는 데 있어 이러한 효율적인 미세 조정 방법의 효율성을 강조한다. 또한, 미스트랄-7B 모델이 영어 데이터 세트에서 더 나은 성능을 보이는 반면 Qwen1.5-7B 모델은 중국 데이터 세트에서 더 높은 점수를 달성한다는 것을 관찰한다. 이러한 결과는 미세 조정된 모델의 성능이 특정 언어에 대한 고유한 기능과도 관련이 있음을 시사한다.\n' +
      '\n' +
      '## 5 결론 및 향후 작업\n' +
      '\n' +
      '본 논문에서는 LLM의 효율적인 미세 조정을 위한 통합된 프레임워크인 LlamaFactory를 보여준다. 모듈러 설계를 통해 모델, 데이터 세트 및 훈련 방법 간의 종속성을 최소화하고 다양한 범위의 효율적인 미세 조정 기술로 100 LLM 이상을 미세 조정하는 통합 접근법을 제공한다. 또한 유연한 웹 UI 라마보드를 제공하여 코딩 노력 없이 LLM의 맞춤형 미세 조정 및 평가가 가능합니다. 우리는 언어 모델링 및 텍스트 생성 태스크에 대한 프레임워크의 효율성과 유효성을 경험적으로 검증한다.\n' +
      '\n' +
      '앞으로 우리는 라마팩토리를 최신 모델과 효율적인 미세 조정 기술과 일관되게 동기화할 것이다. 우리는 또한 오픈 소스 커뮤니티의 기여를 환영합니다. 향후 버전에서는 보다 발전된 병렬 훈련 전략과 LLM의 다중 모드 효율적인 미세 조정을 탐색할 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c c c c c c c} \\hline \\hline  & \\multicolumn{4}{c|}{CNN/DM} & \\multicolumn{4}{c}{XSum} & \\multicolumn{4}{c}{AdGen} \\\\ Model & Baseline & FT & GaLore & LoRA & QLoRA & Baseline & FT & GaLore & LoRA & Baseline & FT & GaLore & LoRA & QLoRA \\\\ \\hline Llama2-7B & 12.94 & 22.87 & 22.40 & 22.70 & 22.61 & 13.89 & 27.69 & 27.64 & 28.80 & 28.05 & 0.61 & 20.51 & 19.61 & 20.29 & 20.45 \\\\ Mistral-7B & 14.39 & 22.03 & 22.99 & **24.27** & 23.28 & 15.87 & 23.57 & 28.00 & 30.41 & **28.44** & 7.82 & 20.14 & 20.90 & 20.29 & 20.56 \\\\ Gemma-7B & 15.97 & 22.07 & / & 22.41 & 22.44 & 15.31 & 25.13 & / & 28.67 & 29.02 & 11.57 & 19.99 & / & 20.62 & 19.81 \\\\ Qwen1.5-7B & 15.40 & 22.46 & 21.76 & 22.71 & 22.52 & 19.27 & 26.68 & 26.64 & 27.27 & 27.60 & 14.49 & 20.42 & 21.08 & 20.13 & **21.34** \\\\ Yi-6B & 16.85 & 22.40 & 22.68 & 22.98 & 22.97 & 18.24 & 27.09 & 28.25 & 28.71 & 29.21 & 13.34 & 19.68 & 20.06 & 20.97 & 20.31 \\\\ ChatGLM3-6B & 18.51 & 22.00 & 22.16 & 21.68 & 21.70 & 16.14 & 26.25 & 26.34 & 26.50 & 26.78 & 14.53 & 19.91 & 20.57 & 20.47 & 20.49 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: LlamaFactory에서 서로 다른 미세 조정 방법을 사용하여 특정 작업에 대한 성능(ROUGE 측면에서) 비교. 각 모델의 최상의 결과는 밑줄이 그어져 있으며 각 작업의 최상의 결과는 **굵게** 표시됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|r r r|r r|r r|r r} \\hline \\hline  & \\multicolumn{4}{c|}{Gamma-2B} & \\multicolumn{4}{c|}{Llama2-7B} & \\multicolumn{4}{c}{Llama2-13B} \\\\ \\hline Method & Memory & Throughput & PPL & Memory & Throughput & PPL & Memory & Throughput & PPL \\\\  & (GB) & (Tokens/s) & & & (GB) & (Tokens/s) & & (GB) & (Tokens/s) & \\\\ \\hline Baseline & / & & / & 11.83 & / & / & 7.53 & / & / & 6.66 \\\\ Full-tuning & 17.06 & 3090.42 & 10.34 & 38.72 & 1334.72 & 5.56 & / & / & / & / \\\\ Freeze-tuning & 8.10 & 5608.49 & 11.33 & 15.69 & 2904.98 & 6.59 & 29.02 & 1841.46 & 6.56 \\\\ GaLore & 10.16 & 2483.05 & 10.38 & 15.43 & 1583.77 & 5.88 & 28.91 & 956.39 & **5.72** \\\\ LoRA & 7.91 & **3521.05** & **10.19** & 16.32 & **1954.07** & **5.81** & 30.09 & **1468.19** & 5.75 \\\\ QLoRA & **5.21** & 3158.59 & 10.46 & **7.52** & 1579.16 & 5.91 & **12.61** & 973.53 & 5.81 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: LlamaFactory에서 서로 다른 미세 조정 방법을 사용한 훈련 효율성 비교. 각 모델의 GaLore, LoRA 및 QLoRA 중 가장 좋은 결과는 **볼드** 입니다.\n' +
      '\n' +
      '## 6 Broader Impact and Responsible Use\n' +
      '\n' +
      '라마 공장은 자체 모델을 미세 조정할 가능성을 탐구하기 위해 LLM에 관심이 있는 많은 개인을 끌어들였다. 이것은 오픈 소스 커뮤니티의 성장에 크게 기여합니다. 그것은 점점 더 주목을 받고 있으며 LLM에 대한 효율적인 미세 조정 프레임워크의 대표로 어썸 트랜스포머3에 소개되고 있다. 우리는 실무자들이 사회에 이익을 가져다주는 우리의 프레임워크를 기반으로 LLM을 구축하기를 기대한다. LLM을 미세 조정하기 위해 LamaFactory를 사용할 때 모델 라이선스를 준수해야 하므로 잠재적인 오용을 방지할 수 있습니다.\n' +
      '\n' +
      '각주 3: [https://github.com/huggingface/transformers/blob/v4.39.0/awesome-transformers.md](https://github.com/huggingface/transformers/blob/v4.39.0/awesome-transformers.md)\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* A. Abid, A. Abdalla, A. Abid, D. Khan, A. Alfozan, and J. Zou(2019)Grado: Hassle-free sharing and testing of ml models in the wild. arXiv preprint arXiv:1906.02569. 인용: SS1.\n' +
      '* L. AI (2023)Lit-gpt. 로 인용: SS1.\n' +
      '* E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. 코조카루 데바, E. 고핀, D. 헤슬로, J. Launay, Q. Malartic, et al. (2023)The falcon series of open language models. arXiv preprint arXiv:2311.16867. 인용: SS1.\n' +
      '* Y. 아난드 Nussbaum, B. Duderstadt, B. Schmidt, and A. Mulyar (2023)GPT4All: training a assistant-style chatbot with large scale data distillation from GPT-3.5-turbo. 로 인용: SS1.\n' +
      '* A. (2016)Arrow. 로 인용: SS1.\n' +
      '* J. Bai, S. 배영 주종근 최광 당, 엑스 등영 팬우 계영 Han, F. Huang, et al. (2023)Qwen technical report. arXiv preprint arXiv:2309.16609. 인용: SS1.\n' +
      '* E. Beeching, C. Fourrier, N. 하빕 한남 람버트 오라자니 산세비에로 Tunstall, T. 울프(2023) 오픈 LLM 리더보드. 로 인용: SS1.\n' +
      '* E. B. Zaken, Y. Goldberg, S. Ravfogel (2022)BitFit: 간단한 파라미터-효율적인 미세-튜닝 for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Dublin, Ireland, pp. 1-9. 인용: SS1.\n' +
      '* R. Bhardwaj, D. D. Anh, and S. Poria (2024) 언어 모델은 홈런 심슨입니다! 과제 산술을 통한 미세 조정된 언어 모델의 안전 재정렬. arXiv preprint arXiv:2402.11746. 인용: SS1.\n' +
      '*X. 비덕천 천동대 동규 두진 Fu, et al.(2024)DeepSeek LLM: scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954. 인용: SS1.\n' +
      '* K. Canese와 S. Weis (2013)PubMed: 서지 데이터베이스. NCBI 핸드북2(1). 로 인용: SS1.\n' +
      '*D. Chen, Y 황석호 이영 이영 류현판 서동장 Zhang, K. 한(2024)오리온-14b: 오픈소스 다국어 대형 언어 모델. arXiv preprint arXiv:2401.12246. 인용: SS1.\n' +
      '* S. 천성호 왕락 천윤 Tian (2023) 위치 보간을 통한 대용량 언어 모델의 컨텍스트 윈도우 확장. arXiv preprint arXiv:2306.15595. 인용: SS1.\n' +
      '*T. Chen, B. Xu, C. Zhang, C. Guestrin (2016)Training deep net with sublinear memory cost. arXiv preprint arXiv:1604.06174. 인용: SS1.\n' +
      '* Y. 천성호 천현탕 이지 유승 Han, and J. Jia (2024)LongLoRA: long-context large language models의 효율적인 미세 조정. International Conference on Learning Representations, 인용: SS1.\n' +
      '* Y. 추이진 양, 엑스. Yao (2023)Efficient and effective text encoding for chinese llama and alpaca. arXiv preprint arXiv:2304.08177. 인용: SS1.\n' +
      '* D. Dai, C. Deng, C. Zhao, R. 서현가오 정재욱 유영 Wu, et al. (2024)DeepSeek-MoE: towards ultimate expert specialized in mixture-of-experts language models. arXiv preprint arXiv:2401.06066. 인용: SS1.\n' +
      '*T. 다오두복 Ermon, A. Rudra, and C. Re (2022)Flashattention: fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems35, pp. 16344-16359. Cited by: SS1.\n' +
      '*T. M. Dettmers 루이스 벨카다, L. Zettlemoyer (2022)GPT3.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems35, pp. 30318-30332. Cited by: SS1.\n' +
      '*T. M. Dettmers 루이스 Shleifer, L. Zettlemoyer (2022)8-bit optimizers via block-wise quantization. International Conference on Learning Representations, 인용: SS1.\n' +
      '\n' +
      '팀 데트머스 아르티도로 파그노니 아리 홀츠만 루크 제틀모이어 2023. QLoRA: Efficient finetuning of quantized llms. _ Neural Information Processing Systems_, 36:10088-10115에서의 진보.\n' +
      '* Diao et al. (2023) Shizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang. 2023. LMFlow: 대형 기초 모델의 미세 조정 및 추론을 위한 확장 가능한 툴킷. _ arXiv preprint arXiv:2306.12420_.\n' +
      '* Du et al.(2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zilin Yang, and Jie Tang. 2022. GLM: General language model pretraining with autoregressive blank infilling. [계산 언어학 협회 제60차 연례 회의(제1권: 장문)]에서 아일랜드 더블린 320-335쪽 계산 언어학 협회\n' +
      '* Egiazarian 등(2024) Vage Egiazarian, Andrei Panferov, Denis Kuzmedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. 2024. 가산 양자화를 통한 대형 언어 모델의 극단적인 압축. _ arXiv preprint arXiv:2401.06118_.\n' +
      '* Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ: 생성 미리 훈련된 변압기에 대한 정확한 훈련 후 양자화. <학습 표현에 관한 국제 회의>에서.\n' +
      '* Groeneveld 등(2024) Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. OLMo: Accelerating the science of language models. _ arXiv preprint arXiv:2402.00838_.\n' +
      '* 코드 인텔리전스의 증가입니다. _ arXiv preprint arXiv:2401.14196_.\n' +
      '* Han and Han (2023) Daniel Han and Michael Han. 2023년입니다.\n' +
      '* Hayou et al.(2024) Soufiane Hayou, Nikhil Ghosh, and Bin Yu. 2024. LoRA+: 대형 모델의 효율적인 저순위 적응. _ arXiv preprint arXiv:2402.12354_.\n' +
      '* Hendrycks 등(2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. 대규모 멀티태스킹 언어 이해도 측정. <학습 표현에 관한 국제 회의>에서.\n' +
      '* Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. 파라미터-efficient transfer learning for nlp. 기계 학습에 관한 국제 회의 2790-2799 페이지. PMLR.\n' +
      '* Hu et al.(2022) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. LoRA: Low-rank adaptation of large language models. <학습 표현에 관한 국제 회의>에서.\n' +
      '* Huang et al. (2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chancheng Lv, Yikai Zhang, Yao Fu, et al. 2023. C-Eval: A multi-level multi-disc discipline chinese evaluation suite for foundation models. _ Neural Information Processing Systems_, 36에서의 진보.\n' +
      '* 장 등(2023a) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023a. 미스트랄 7b. _ arXiv preprint arXiv:2310.06825_.\n' +
      '* Jiang et al.(2023b) Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yaliang Li, and Ji-Rong Wen. 2023b. 추론LM: 지식 그래프에 대한 질의 응답을 위해 사전 훈련된 언어 모델에서 구조적 서브그래프 추론을 가능하게 한다. "2023 자연 언어 처리 실증 방법에 관한 회의 회보"에서 싱가포르 3721-3735쪽입니다. 계산 언어학 협회\n' +
      '* Jiao et al.(2023a) Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023a. ParroT: 인간 번역 및 피드백으로 튜닝된 대형 언어 모델을 사용하여 채팅 중에 번역. 계산 언어학 협회의 _Findings: EMNLP 2023_, 페이지 15009-15020, Singapore. 계산 언어학 협회\n' +
      '* Jiao et al.(2023b) Yizhu Jiao, Ming Zhong, Sha Li, Ruining Zhao, Siru Ouyang, Heng Ji, and Jiawei Han. 2023b. 지시 및 추출: 주문형 정보 추출을 위한 지시 튜닝. "2023 자연 언어 처리 실증 방법에 관한 회의 회보"에서 싱가포르 10030-10051 페이지입니다. 계산 언어학 협회\n' +
      '* Kalajdziievski (2023) Damjan Kalajdziievski. 2023. LoRA와의 미세 조정을 위한 순위 안정화 스케일링 팩터. _ arXiv preprint arXiv:2312.03732_.\n' +
      '*김 외(2023) 김다현, 박찬준, 김상훈, 원성 이원호, 송원호, 김윤수, 김현우, 김윤기, 김현주, 이현주, 김지후, 등 2023. SOLAR 10.7B: 단순하면서도 효과적인 깊이 업-스케일링을 갖는 대형 언어 모델 스케일링_ arXiv preprint arXiv:2312.15166_.\n' +
      '* Krell 등(2021) Mario Michael Krell, Matej Kosec, Sergio P Perez, and Andrew Fitzgibbon. 2021. 교차 오염 없이 효율적인 시퀀스 패킹: 성능에 영향을 주지 않고 대규모 언어 모델을 가속화합니다. _ arXiv preprint arXiv:2107.02027_.\n' +
      '* Kwon et al. (2023) 우석권, 주환리, 시위안장, 영성, 리아민정, 코디하오유, 조셉곤잘레스, 하오장, 이온스토이카. 2023. Efficient\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '제프 래슬리, 사미암 라즈반다리, 올라툰지 루와세, 위슝 허 2020. DeepSpeed: 시스템 최적화는 1000억 개 이상의 파라미터를 갖는 딥 러닝 모델을 트레이닝할 수 있게 한다. 《제26회 ACM SIGKDD 국제 학술대회 지식 발견 및 데이터 마이닝》의 3505-3506 페이지.\n' +
      '* Shao et al.(2019) Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. 2019. 계획 기반 계층적 변분 모델을 사용한 길고 다양한 텍스트 생성. <프로시빙스 of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_>, 3257-3268 페이지, 홍콩, 중국. 계산 언어학 협회\n' +
      '* Shao et al.(2024) Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, YWu, and Daa Guo. 2024. DeepSeeKMath: 개방형 언어 모델에서 수학적 추론의 한계를 밀어내고 있습니다. _ arXiv preprint arXiv:2402.03300_.\n' +
      '* Taori 등(2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsumori B. Hashimoto. 2023. Stanford alpaca: instruction-Following lama model.\n' +
      '* 팀 (2023) InternLM 팀. 2023. InternLM: 점진적으로 향상된 기능을 가진 다국어 언어 모델.\n' +
      '* Tillet 등(2019) Philippe Tillet, Hsiang-Tsung Kung, and David Cox. 2019. Triton: a intermediate language and compiler for tiled neural network computation. [기계 학습 및 프로그래밍 언어에 관한 제3차 ACM SIGPLAN 국제 워크샵의 진행]에서 10-19페이지입니다.\n' +
      '* Touvron 등(2023a) Hugo Touvron, Thibaut Lavril, Gautier Izzard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. LLAMA: 개방적이고 효율적인 기초 언어 모델입니다. _ arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron 등(2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023b. 라마 2: 오픈 파운데이션과 미세 조정된 채팅 모델입니다. _ arXiv preprint arXiv:2307.09288_.\n' +
      '* Tunstall 등(2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, et al. 2023. Zephyr: Direct distillation of LM alignment. _ arXiv preprint arXiv:2310.16944_.\n' +
      '* Werra 등(2020) Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. 2020. TRL: 트랜스포머 강화 학습.\n' +
      '* Wang et al.(2023a) Chenglong Wang, Hang Zhou, Yimin Hu, Yifu Huo, Bei Li, Tongran Liu, Tong Xiao, and Jingbo Zhu. 2023a. Esrl: 시퀀스 생성을 위한 효율적인 샘플링 기반 강화 학습. _ arXiv preprint arXiv:2308.02223_.\n' +
      '* Wang et al.(2023b) Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023b. OpenChat: 고품질 데이터가 혼합된 오픈 소스 언어 모델을 발전시킵니다. _ arXiv preprint arXiv:2309.11235_.\n' +
      '* Wang et al.(2023c) Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023c. 대형 언어 모델을 사용한 문서 수준의 기계 번역입니다. 싱가포르의 16646-16661 페이지의 _2023 자연 언어 처리 실증 방법에 관한 회의 회보_에서. 계산 언어학 협회\n' +
      '* Wang 등(2023d) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 2023d. 낙타는 어디까지 갈 수 있나요? 오픈 리소스에서 명령어 튜닝 상태를 탐색합니다. _ Neural Information Processing Systems_, 36에서의 진보.\n' +
      '* Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2022. 파인튜닝 언어 모델은 제로샷 학습자입니다. <학습 표현에 관한 국제 회의>에서.\n' +
      '* Wei et al.(2023) Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lu, Rui Hu, et al. 2023. Skywork: A more open bilingual foundation model. _ arXiv preprint arXiv:2310.19341_.\n' +
      '* Wolf 등(2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. 트랜스포머: 최첨단의 자연어 처리. "2020년 자연 언어 처리 실증 방법에 관한 회의: 시스템 시연"에서 38-45페이지, 온라인. 계산 언어학 협회\n' +
      '* Wu 등 (2023) Shaohua Wu, Xudong Zhao, Shenling Wang, Jiangang Luo, Lingjun Li, Xi Chen, Bing Zhao, Wei Wang, Tong Yu, Rongguo Zhang, et al. 2023. YUAN 2.0: A large language model with localized filtering-based attention _ arXiv preprint arXiv:2311.15786_.\n' +
      '* Yang et al.(2023) Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. 2023. 바이촨 2: Open large-scale language models. _ arXiv preprint arXiv:2309.10305_.\n' +
      '* Yao et al.(2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing reasoning and acting in language models. <학습 표현에 관한 국제 회의>에서.\n' +
      '* Young et al.(2024) Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi: Open foundation models by 01.ai. _arXiv preprint arXiv:2403.04652_.\n' +
      '* Yu et al. (2023) Hao Yu, Zachary Yang, Kellin Pelrine, Jean Francois Godbout, and Reihaneh Rabbany. 2023. Open, closed, small language models for text classification? _ arXiv preprint arXiv:2308.10092_.\n' +
      '*Zhang et al. (2022) Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. 2022. LLaMA-Adapter: 제로-init 주의력을 갖는 언어 모델의 효율적인 미세 조정. <학습 표현에 관한 국제 회의>에서.\n' +
      '* Zhao et al.(2024) Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. 2024. GaLore: Gradient low-rank projection에 의한 Memory-efficient llm training. _ arXiv preprint arXiv:2403.03507_.\n' +
      '* Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. _ arXiv preprint arXiv:2303.18223_.\n' +
      '* Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging LLM-as-a-jaduge with mt-bench and chatbot arenna. _ Neural Information Processing Systems_, 36에서의 진보.\n' +
      '\n' +
      '## 부록 A 관련 작업\n' +
      '\n' +
      '이 섹션에서는 LLM을 미세 조정하기 위한 기존 프레임워크를 열거하고 특히 효율적인 미세 조정을 위한 프레임워크를 강조한다.\n' +
      '\n' +
      'LLaMA-Adapter (Zhang et al., 2022)는 제로-이니트 주의력을 사용하여 수업 후속 능력에 대해 Llama 모델 (Touvron et al., 2023a)을 효율적으로 미세 조정한다. FastChat(Zheng et al., 2023)은 채팅 완료 목적을 위한 LLMs을 훈련하고 평가하는 데 초점을 맞춘 프레임워크이다. LitGPT(AI, 2023)는 생성 모델의 구현을 제공하며, 다양한 훈련 방법을 지원한다. Open-Instruct (Wang et al., 2023d)는 효율적인 미세 조정을 지원하는 미세 조정 프레임워크이다. 거대 AI(Li 등, 2023b)는 분산 훈련을 위해 고급 병렬 전략을 취한다. LMFlow(Diao et al., 2023)는 풀-튜닝 및 어댑터 튜닝을 모두 지원하는 디코더-전용 모델들을 지원하는 확장가능하고 효율적인 미세-튜닝 프레임워크이다. GPT4All(Anand et al., 2023)은 LLM들이 소비자 디바이스들 상에서 실행될 수 있게 하는 한편, 또한 미세-조정 능력들을 제공한다.\n' +
      '\n' +
      '위에서 제시한 프레임워크와 달리 라마 팩토리는 보다 광범위한 효율적인 미세 조정 방법을 지원한다. 우리는 표 5의 기존 작업과 프레임워크를 비교한다.\n' +
      '\n' +
      '## 부록 B 지원 모델 LlamaFactory\n' +
      '\n' +
      '우리는 LlamaFactory에서 지원하는 인기 있는 사전 훈련 모델을 선택하고 밀도 모델부터 희박한 전문가 혼합 모델(MoE) 모델에 이르기까지 표 6에 나열한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l} \\hline \\hline Model & Variant & Organization \\\\ \\hline Llama (Touvron et al., 2023a) & 78/13B3/68B & Meta AI \\\\ Llama 2 (Touvron et al., 2023b) & 78/13B/70B & Meta AI \\\\ Bakuchan (Yang et al., 2023) & 78/13B & Bakuchan Inc \\\\ Bakuchan (Zung et al., 2023) & 78/13B & Bakuchan Inc \\\\ BLOOM (Le Scano et al., 2022) & 560M/3B/7.1B & BigScience \\\\ BLOMOM (Le Scano et al., 2022) & 560M/3B/7.1B & BigScience \\\\ ChaMiZ (Du et al., 2022) & 68B & THUIMM \\\\ ChatGLM3 (Du et al., 2022) & 68 & THUIM \\\\ ChineseLAM(Li et al., 2023) & 38/7B/13B & HFL \\\\ DeepSetL-ILM (Bi et al., 2024) & 78/67B & DeepSeek \\\\ DeepSetL-Math (Diao et al., 2024) & 78 & DeepSeek \\\\ DeepSetL-Soft (Dui et al., 2024) & 168 & DeepSeek \\\\ DeepSet-Coder (Gu et al., 2024) & 6.7B/7B/33B & DeepSeek \\\\ Falcon (Llamaertov et al., 2023) & 78/40B/180B & TII \\\\ Gemma (Measard et al., 2024) & 28/7B & Google \\\\ InternalM (Team, 2023) & 78/20B & Shanghai AI Lab \\\\ InternalM (Team, 2023) & 1.8B/7B/20B & Shanghai AI Lab \\\\ Mineral (Zhang et al., 2023a) & 78 & Mineral AI \\\\ Mixtal (Jiang et al., 2023a) & 8/7B & MistAI AI \\\\ MXM (Groeneveld et al., 2024) & 18/7B & Allen AI \\\\ OpenCharM (Wang et al., 2023b) & 78 & OpenCharM \\\\ Open (Chen et al., 2024a) & 14B & Oriosciar \\\\ Phi-15 (Li et al., 2023a) & 1.3B & Microsoft \\\\ Phi-2 (Li et al., 2023c) & 2.7B & Microsoft \\\\ Open (Biai et al., 2023) & 1.8B/7B/14B/7B & Albashus Cloud \\\\ OpenJ (Biai et al., 2023) & 1.8B/7B/14B/7B & Albashus Cloud \\\\ SOLAR (Kim et al., 2023) & 10.7B & Upstage AI \\\\ Skywork (Wei et al., 2023) & 13B & Skywork \\\\ SunCoder (Zenlowlowk et al., 2024) & 38/7B/13B & DeGoc \\\\ Viciani (Zheng et al., 2023) & 78/13B & LMSYS \\\\ Yi (Young et al., 2024) & 68/9B/43B & 0.1A \\\\ Yuan2 (Wu et al., 2023) & 2B/5B/18/10B & BIT \\\\ Zephyr (Tunstull et al., 2023) & 7B & Hugging Face H4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 지원되는 모델 목록입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c} \\hline \\hline Methods & LlamaFactor & FastChat & LitGPT & LMFlow & Open \\\\ \\hline LoRa & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ QLoRa & ✓ & ✓ & ✓ & & ✓ \\\\ LoRa\\(\\star\\) & ✓ & ✓ & & & ✓ \\\\ DoRa\\(\\star\\) & ✓ & & & & \\\\ Galore & ✓ & & & & \\\\ \\hline STF & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ RLHF & ✓ & & & ✓ & \\\\ DPO & ✓ & & & & ✓ \\\\ \\hline Flash attention & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ Unskoth & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ DeepSpeed & ✓ & ✓ & ✓ & ✓ & ✓ \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '<표 5>: LlamaFactor의 특징과 기존 LLM 미세조정의 비교\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
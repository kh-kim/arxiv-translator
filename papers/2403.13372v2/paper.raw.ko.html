<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</title>
<!--Generated on Thu Mar 21 08:35:35 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2403.13372v2/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2403.13372v2">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2403.13372v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2403.13372v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S1" title="1 Introduction ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S2" title="2 Efficient Fine-Tuning Techniques ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Efficient Fine-Tuning Techniques</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S2.SS1" title="2.1 Efficient Optimization ‣ 2 Efficient Fine-Tuning Techniques ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Efficient Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S2.SS2" title="2.2 Efficient Computation ‣ 2 Efficient Fine-Tuning Techniques ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Efficient Computation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3" title="3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">LlamaFactory</span> Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1" title="3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Model Loader</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1.SSS0.Px1" title="Model Initialization ‣ 3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Model Initialization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1.SSS0.Px2" title="Model Patching ‣ 3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Model Patching</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1.SSS0.Px3" title="Model Quantization ‣ 3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Model Quantization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1.SSS0.Px4" title="Adapter Attaching ‣ 3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Adapter Attaching</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1.SSS0.Px5" title="Precision Adaptation ‣ 3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Precision Adaptation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS2" title="3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Worker</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS2.SSS0.Px1" title="Dataset Loading ‣ 3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Dataset Loading</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS2.SSS0.Px2" title="Dataset Aligning ‣ 3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Dataset Aligning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS2.SSS0.Px3" title="Dataset Merging ‣ 3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Dataset Merging</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS2.SSS0.Px4" title="Dataset Pre-processing ‣ 3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Dataset Pre-processing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS3" title="3.3 Trainer ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Trainer</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS3.SSS0.Px1" title="Efficient Training ‣ 3.3 Trainer ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Efficient Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS3.SSS0.Px2" title="Model-Sharing RLHF ‣ 3.3 Trainer ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Model-Sharing RLHF</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS3.SSS0.Px3" title="Distributed Training ‣ 3.3 Trainer ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Distributed Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS4" title="3.4 Utilities ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Utilities</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS4.SSS0.Px1" title="Accelerated Inference ‣ 3.4 Utilities ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Accelerated Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS4.SSS0.Px2" title="Comprehensive Evaluation ‣ 3.4 Utilities ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Comprehensive Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS5" title="3.5 LlamaBoard: A Unified Interface for LlamaFactory ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span><span class="ltx_text ltx_font_smallcaps">LlamaBoard</span>: A Unified Interface for <span class="ltx_text ltx_font_smallcaps">LlamaFactory</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4" title="4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">4</span> </span>Empirical Study</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS1" title="4.1 Training Efficiency ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">4.1</span> </span>Training Efficiency</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS1.SSS0.Px1" title="Experimental Setup ‣ 4.1 Training Efficiency ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS1.SSS0.Px2" title="Results ‣ 4.1 Training Efficiency ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS2" title="4.2 Fine-Tuning on Downstream Tasks ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">4.2</span> </span>Fine-Tuning on Downstream Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS2.SSS0.Px1" title="Experimental Setup ‣ 4.2 Fine-Tuning on Downstream Tasks ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS2.SSS0.Px2" title="Results ‣ 4.2 Fine-Tuning on Downstream Tasks ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S5" title="5 Conclusion and Future Work ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">5</span> </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S6" title="6 Broader Impact and Responsible Use ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">6</span> </span>Broader Impact and Responsible Use</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A1" title="Appendix A Related Work ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">A</span> </span><span class="ltx_text ltx_font_bold">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A2" title="Appendix B Supported Models in LlamaFactory ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">B</span> </span><span class="ltx_text ltx_font_bold">Supported Models in </span><span class="ltx_text ltx_font_smallcaps">LlamaFactory</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A3" title="Appendix C Details of Designing Chat Template ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">C</span> </span><span class="ltx_text ltx_font_bold">Details of Designing Chat Template</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A4" title="Appendix D Experiment Details ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">D</span> </span><span class="ltx_text ltx_font_bold">Experiment Details</span></span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A4.SS1" title="D.1 Training Efficiency ‣ Appendix D Experiment Details ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">D.1</span> </span>Training Efficiency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A4.SS2" title="D.2 Fine-Tuning on Downstream Tasks ‣ Appendix D Experiment Details ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">D.2</span> </span>Fine-Tuning on Downstream Tasks</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2403.13372v2 [cs.CL] 21 Mar 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_smallcaps" id="id1.id1">LlamaFactory</span>: Unified Efficient Fine-Tuning of 100+ Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yaowei Zheng<sup class="ltx_sup" id="id2.1.id1">1</sup>, Richong Zhang<sup class="ltx_sup" id="id3.2.id2">1</sup>, Junhao Zhang<sup class="ltx_sup" id="id4.3.id3">1</sup>, Yanhan Ye<sup class="ltx_sup" id="id5.4.id4">1</sup>, Zheyan Luo<sup class="ltx_sup" id="id6.5.id5">1</sup>, Yongqiang Ma<sup class="ltx_sup" id="id7.6.id6">2</sup>
<br class="ltx_break"><sup class="ltx_sup" id="id8.7.id7">1</sup>School of Computer Science and Engineering, Beihang University, China 
<br class="ltx_break"><sup class="ltx_sup" id="id9.8.id8">2</sup>School of Software and Microelectronics, Peking University, China 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id10.9.id9">hiyouga@buaa.edu.cn</span>, <span class="ltx_text ltx_font_typewriter" id="id11.10.id10">zhangrc@act.buaa.edu.cn</span>, <span class="ltx_text ltx_font_typewriter" id="id12.11.id11">{zhang.jh,yeyanhan,akamya}@buaa.edu.cn</span>, <span class="ltx_text ltx_font_typewriter" id="id13.12.id12">codingma@pku.edu.cn</span>
<br class="ltx_break">Demonstration video: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://youtu.be/W29FgeZEpus" title="">https://youtu.be/W29FgeZEpus</a>
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id14.id1">대규모 언어 모델(LLM)을 다운스트림 작업에 적용하기 위해서는 효율적인 미세 조정이 필수적이다. 그러나 이러한 방법을 다른 모델에 구현하려면 사소하지 않은 노력이 필요하다. 우리는 최첨단 효율적인 훈련 방법을 통합한 통합 프레임워크인 <span class="ltx_text ltx_font_smallcaps" id="id14.id1.1">LlamaFactory</span>을 제시한다. 사용자가 기본 제공 웹 UI <span class="ltx_text ltx_font_smallcaps" id="id14.id1.2">LlamaBoard</span>을 통해 코딩할 필요 없이 100+ LLM의 미세 조정을 유연하게 사용자 지정할 수 있습니다. 우리는 언어 모델링 및 텍스트 생성 태스크에 대한 프레임워크의 효율성과 유효성을 경험적으로 검증한다. 그것은 <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hiyouga/LLaMA-Factory" title="">https://github.com/hiyouga/LLaMA-Factory</a>에서 출시되었으며 이미 13,000개 이상의 별과 1,600개의 포크를 받았습니다.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_smallcaps" id="p1.1.1">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="p1.1.2">: Unified Efficient Fine-Tuning of 100+ Language Models</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break">
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1.1" style="width:433.6pt;"><span class="ltx_tabular ltx_align_top" id="p2.1.1.1.1.1.1.1"><span class="ltx_td ltx_align_center" id="p2.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><span class="ltx_sup" id="p2.1.1.1.1.1.1.1.1.1.1.1.1">1</sup>, Yanhao Zhang<sup class="ltx_sup class="ltx_sup" id="p2.1.1.1.1.1.1.1.1.1.1.4">1</sup>, Zheyan Luo<sup class="ltx_sup" id="p2.1.1.1.1.1.1.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">대형 언어 모델(LLMs) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Zhao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib87" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>는 놀라운 추론 능력을 제시하고, 질문 응답 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Jiang et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib36" title=""><span class="ltx_text ltx_font_bold">2023b</span></a>)</cite>, 기계 번역 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wang et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib75" title=""><span class="ltx_text ltx_font_bold">2023c</span></a>; <span class="ltx_text ltx_font_bold">Jiao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib37" title=""><span class="ltx_text ltx_font_bold">2023a</span></a>)</cite>, 정보 추출 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Jiao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib38" title=""><span class="ltx_text ltx_font_bold">2023b</span></a>)</cite>와 같이 광범위한 응용 분야에 힘을 실어준다. 그 후, 상당한 수의 LLM이 개발되고 오픈 소스 커뮤니티를 통해 액세스할 수 있다. 예를 들어, Hugging Face의 오픈 LLM 리더보드 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Beeching et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib7" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>는 5,000개 이상의 모델을 자랑하여 LLM의 힘을 활용하려는 개인에게 편의를 제공한다.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">리소스가 제한된 매우 많은 수의 매개변수를 미세 조정하는 것은 LLM을 다운스트림 작업에 적용하는 주요 과제가 된다. 대중적인 솔루션은 다양한 작업에 적응할 때 LLM의 훈련 비용을 줄이는 효율적인 미세 조정 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Houlsby et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib32" title=""><span class="ltx_text ltx_font_bold">2019</span></a>; <span class="ltx_text ltx_font_bold">Hu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib33" title=""><span class="ltx_text ltx_font_bold">2022</span></a>; <span class="ltx_text ltx_font_bold">Ben Zaken et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib8" title=""><span class="ltx_text ltx_font_bold">2022</span></a>; <span class="ltx_text ltx_font_bold">Dettmers et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib22" title=""><span class="ltx_text ltx_font_bold">2023</span></a>; <span class="ltx_text ltx_font_bold">Zhao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib86" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>이다. 그러나, 커뮤니티는 LLM들을 효율적으로 미세 조정하기 위한 다양한 방법들에 기여하며, 이러한 방법들을 다른 LLM들에 적응하고 통합하고 사용자 맞춤화를 위한 친숙한 인터페이스를 제공하는 체계적인 프레임워크가 부족하다.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">위의 문제를 해결하기 위해 LLM의 미세 조정을 민주화하는 프레임워크인 <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.1">LlamaFactory</span>을 개발한다. 확장 가능한 모듈을 통해 다양한 효율적인 미세 조정 방법을 통합하여 최소 리소스와 높은 처리량으로 수백 개의 LLM을 미세 조정할 수 있습니다. 또한, 생성적 사전 훈련 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Radford et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib61" title=""><span class="ltx_text ltx_font_bold">2018</span></a>)</cite>, 감독 미세 조정(supervised fine-tuning; SFT) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wei et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib77" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>, 인간 피드백으로부터의 강화 학습(RLHF) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_bold">Ouyang et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib58" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>, 직접 선호 최적화(DPO) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_bold">Rafailov et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib62" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite> 등 일반적으로 사용되는 훈련 접근법을 간소화한다. 사용자는 명령줄 또는 웹 인터페이스를 활용하여 최소 또는 전혀 코딩 노력 없이 LLM을 사용자 지정하고 미세 조정할 수 있습니다.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">LlamaFactory</span>은 <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p4.1.2">Model Loader</span>, <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p4.1.3">Data Worker</span> 및 <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p4.1.4">Trainer</span>의 세 가지 주요 모듈로 구성됩니다. 특정 모델 및 데이터 세트에 대한 이러한 모듈의 종속성을 최소화하여 프레임워크가 수백 개의 모델 및 데이터 세트로 유연하게 확장할 수 있도록 한다. 구체적으로, 먼저 모델 로더가 정확한 레이어를 식별하여 미리 훈련된 모델에 어댑터를 정확하게 부착할 수 있는 모델 레지스트리를 구축한다. 그런 다음 데이터 작업자가 해당 열을 정렬하여 데이터 세트를 수집할 수 있는 데이터 설명 사양을 개발합니다. 또한, 트레이너가 기본 방법을 대체하여 활성화할 수 있도록 하는 효율적인 미세 조정 방법의 플러그 앤 플레이 구현을 제공한다. 우리의 설계는 이러한 모듈들이 상이한 트레이닝 접근법들에 걸쳐 재사용될 수 있게 하여, 새로운 방법들의 통합 비용을 상당히 감소시킨다.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.1">LlamaFactory</span>은 PyTorch <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Paszke et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib60" title=""><span class="ltx_text ltx_font_bold">2019</span></a>)</cite>로 구현되며, Transformers <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wolf et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib79" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite>, PEFT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Mangrulkar et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib52" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>, TRL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">von Werra et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib72" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite>와 같은 오픈 소스 라이브러리로부터 상당한 이점을 얻을 수 있다. 이를 기반으로 더 높은 수준의 추상화를 제공하는 즉흥적인 프레임워크를 제공합니다. 또한 Gradio <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Abid et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib1" title=""><span class="ltx_text ltx_font_bold">2019</span></a>)</cite>를 사용하여 <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.2">LlamaBoard</span>을 빌드하여 코딩 노력이 필요 없는 미세 조정 LLM을 가능하게 합니다.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.1">LlamaFactory</span> is open-sourced under the Apache-2.0 license. 이미 GitHub<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/hiyouga/LLaMA-Factory</span></span></span>에서 13,000개 이상의 별과 1,600개의 포크를 획득했으며 수백 개의 오픈 소스 모델이 <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.2">LlamaFactory</span> on the Hugging Face Hub<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/models?other=llama-factory</span></span></span>에 구축되었습니다. 예를 들어, 잘 알려진 GemSUra-7B <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Nguyen et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib57" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>는 <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.3">LlamaFactory</span>을 기반으로 구축되며, 이는 먼저 Gemma <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Mesnard et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib53" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>의 교차 언어 능력을 드러낸다. 또한, 수십 개의 연구 연구는 <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_bold">Wang et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib73" title=""><span class="ltx_text ltx_font_bold">2023a</span></a>); <span class="ltx_text ltx_font_bold">Yu et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib84" title=""><span class="ltx_text ltx_font_bold">2023</span></a>); <span class="ltx_text ltx_font_bold">Bhardwaj et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib9" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>와 같은 LLMs에서 새로운 방법을 탐구하기 위해 프레임워크를 활용한다.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Efficient Fine-Tuning Techniques</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">효율적인 LLM 미세 조정 기술은 최적화에 초점을 맞춘 기술과 계산에 초점을 맞춘 기술의 두 가지 주요 범주로 나눌 수 있다. 효율적인 최적화 기법의 주요 목적은 비용을 최소로 유지하면서 LLM의 매개변수를 조정하는 것이다. 반면에, 효율적인 연산 방법은 LLMs에서 필요한 연산을 위한 시간이나 공간을 줄이기 위해 노력한다. <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.1">LlamaFactory</span>에 포함된 메서드는 Table <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S2.T1" title="Table 1 ‣ 2 Efficient Fine-Tuning Techniques ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>에 나열되어 있다. 이러한 효율적인 미세 조정 기술을 제시하고 다음 섹션에서 프레임워크에 통합함으로써 달성된 실질적인 효율성 개선을 보여준다.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span>Efficient fine-tuning techniques featured in <span class="ltx_text ltx_font_smallcaps" id="S2.T1.3.1">LlamaFactory</span>. 서로 호환되는 기술은 ✓로 표시되고 호환되지 않는 기술은 ✗로 표시된다.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.1" style="width:433.6pt;height:255.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(109.8pt,-64.6pt) scale(2.02533799915901,2.02533799915901) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S2.T1.1.1.2.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.2">Freeze-tuning</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.3">GaLore</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.4">LoRA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.5">DoRA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.1.3.1.1">Mixed precision</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.1.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.1.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.1.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.1.5">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.4.2.1">Checkpointing</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2.2">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2.5">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.5.3.1">Flash attention</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3.2">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3.5">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.1.1">S<math alttext="{}^{2}" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><msup id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml"><mi id="S2.T1.1.1.1.1.m1.1.1a" xref="S2.T1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S2.T1.1.1.1.1.m1.1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1"><cn id="S2.T1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S2.T1.1.1.1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math> attention</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.2">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.5">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.6.4.1">Quantization</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.4.2">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.4.3">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.4.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.4.5">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T1.1.1.7.5.1">Unsloth</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.5.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.5.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.5.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.5.5">✗</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Efficient Optimization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">먼저 <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p1.1.1">LlamaFactory</span>에서 활용되는 효율적인 최적화 기법에 대한 개요를 제공한다. 동결-튜닝 방법 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Houlsby et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib32" title=""><span class="ltx_text ltx_font_bold">2019</span></a>)</cite>는 디코더 층들의 작은 서브세트에서 나머지 파라미터들을 미세-튜닝하면서 대부분의 파라미터들을 동결시키는 것을 포함한다. Gradient low-rank projection (GaLore) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Zhao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib86" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>라고 불리는 다른 방법은 Gradient를 더 낮은 차원의 공간으로 투영하여, 메모리 효율적인 방식으로 풀-파라미터 학습을 용이하게 한다. 반대로, Low-rank adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib33" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite> 방법은 미리 훈련된 모든 가중치들을 동결시키고, 훈련 가능한 한 쌍의 Low-rank 행렬들을 지정된 레이어에 도입한다. 양자화와 결합될 때, 이 접근법은 QLoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib22" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>로 지칭되며, 이는 추가적으로 메모리 사용량을 감소시킨다. 가중치 분해 저순위 적응(DoRA) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Liu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib50" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite> 방법은 LLM의 미세 조정을 향상시키기 위해 방향성 성분에만 LoRA를 적용하여 미리 훈련된 가중치를 크기와 방향 성분으로 분해한다. LoRA의 부최적성을 극복하기 위해 LoRA+ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hayou et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib30" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>를 제안한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Efficient Computation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p1.1.1">LlamaFactory</span>에서는 다양한 효율적인 계산 기술을 통합합니다. 일반적으로 사용되는 기술은 혼합 정밀 훈련 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Micikevicius et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib54" title=""><span class="ltx_text ltx_font_bold">2018</span></a>)</cite> 및 활성화 체크포인팅 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Chen et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib14" title=""><span class="ltx_text ltx_font_bold">2016</span></a>)</cite>를 포함한다. 어텐션 계층의 입력-출력(IO) 비용을 조사하여 통찰력을 도출한 플래시 어텐션 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib18" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>는 어텐션 계산을 향상시키기 위한 하드웨어 친화적인 접근법을 도입한다. S<math alttext="{}^{2}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><msup id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1a" xref="S2.SS2.p1.1.m1.1.1.cmml"></mi><mn id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><cn id="S2.SS2.p1.1.m1.1.1.1.cmml" type="integer" xref="S2.SS2.p1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math> attention <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Chen et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib15" title=""><span class="ltx_text ltx_font_bold">2024b</span></a>)</cite>는 블록 간 주의 집중에서 컨텍스트를 확장하는 문제를 해결하여 미세 조정 긴 컨텍스트 LLMs에서 메모리 사용량을 줄입니다. 다양한 양자화 전략 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib20" title=""><span class="ltx_text ltx_font_bold">2022a</span></a>; <span class="ltx_text ltx_font_bold">Frantar et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib26" title=""><span class="ltx_text ltx_font_bold">2023</span></a>; <span class="ltx_text ltx_font_bold">Lin et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib49" title=""><span class="ltx_text ltx_font_bold">2023</span></a>; <span class="ltx_text ltx_font_bold">Egiazarian et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib25" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>는 가중치에 대한 낮은 정밀도 표현을 활용함으로써 대형 언어 모델(LLM)에서 메모리 요구량을 감소시킨다. 그럼에도 불구하고 양자화된 모델의 미세 조정은 LoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib33" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>와 같은 어댑터 기반 기술로 제한된다. Unsloth <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Han and Han</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib29" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>는 LoRA의 역방향 전파를 구현하기 위해 Triton <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Tillet et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib68" title=""><span class="ltx_text ltx_font_bold">2019</span></a>)</cite>를 통합하며, 이는 경사 하강 동안 부동 소수점 연산(FLOPs)을 감소시키고 신속한 LoRA 훈련으로 이어진다.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p2.1.1">LlamaFactory</span>은 이러한 기술을 효과적으로 결합하여 LLM 미세 조정의 효율성을 크게 향상시키는 응집 구조로 결합합니다. 이는 혼합 정밀 트레이닝 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Micikevicius et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib54" title=""><span class="ltx_text ltx_font_bold">2018</span></a>)</cite> 또는 bfloat16 트레이닝 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Le Scao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib43" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>에서 파라미터당 8바이트에서 파라미터당 0.6바이트로 메모리 풋프린트를 감소시키는 결과를 초래한다. 우리의 프레임워크에 대한 추가 세부 사항은 후속 섹션에서 제공될 것이다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">LlamaFactory</span> Framework</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">LlamaFactory</span>은 Model Loader, Data Worker, Trainer의 세 가지 주요 모듈로 구성되어 있습니다. 모델 로더는 100개 이상의 LLM을 지원하는 미세 조정을 위한 다양한 아키텍처를 준비한다. 데이터 작업자는 50개 이상의 데이터 세트를 지원하는 잘 설계된 파이프라인을 통해 다양한 작업의 데이터를 처리합니다. 트레이너는 이러한 모델을 다른 작업 및 데이터 세트에 적응시키기 위해 효율적인 미세 조정 방법을 통합하며, 이는 네 가지 훈련 접근법을 제공한다. <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">LlamaBoard</span>은 위의 모듈에 대한 친근한 시각적 인터페이스를 제공하여 사용자가 코드리스 방식으로 개별 LLM 미세 조정 프로세스를 구성하고 시작하고 즉시 훈련 상태를 모니터링할 수 있도록 합니다. <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.F1" title="Figure 1 ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>에서 <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.3">LlamaFactory</span>의 전체 아키텍처를 설명한다.</p>
</div>
<figure class="ltx_figure" id="S3.F1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.F1.1" style="width:429.3pt;height:512.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(68.5pt,-81.9pt) scale(1.46918076443966,1.46918076443966) ;"><svg class="ltx_picture" height="482.98" id="S3.F1.1.pic1" overflow="visible" version="1.1" width="395.08"><g transform="translate(0,482.98) matrix(1 0 0 -1 0 0) translate(0.69,0) translate(0,8.57)"><g color="#000000" fill="#FAFAFA" stroke="#717171" stroke-width="1.0pt"><path d="M 0 385.83 M 0 391.36 L 0 451.16 C 0 454.22 2.48 456.69 5.53 456.69 L 388.17 456.69 C 391.22 456.69 393.7 454.22 393.7 451.16 L 393.7 391.36 C 393.7 388.3 391.22 385.83 388.17 385.83 L 5.53 385.83 C 2.48 385.83 0 388.3 0 391.36 Z M 393.7 456.69"></path></g><g fill="#000000" stroke="#000000"><g color="#000000" stroke-width="1.0pt"><g fill="#717171"><path d="M 127.95 442.91 M 127.95 448.45 L 127.95 468.87 C 127.95 471.93 130.43 474.41 133.49 474.41 L 260.21 474.41 C 263.27 474.41 265.75 471.93 265.75 468.87 L 265.75 448.45 C 265.75 445.39 263.27 442.91 260.21 442.91 L 133.49 442.91 C 130.43 442.91 127.95 445.39 127.95 448.45 Z M 265.75 474.41" style="stroke:none"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 159.55 453.86)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="74.6"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.F1.1.pic1.1.1.1.1.1.1.1">LlamaBoard</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#FAFAFA" stroke="#717171"><path d="M 7.87 399.61 M 7.87 405.14 L 7.87 425.57 C 7.87 428.62 10.35 431.1 13.41 431.1 L 183.44 431.1 C 186.5 431.1 188.98 428.62 188.98 425.57 L 188.98 405.14 C 188.98 402.08 186.5 399.61 183.44 399.61 L 13.41 399.61 C 10.35 399.61 7.87 402.08 7.87 405.14 Z M 188.98 431.1"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 27.07 411.89)"><foreignObject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="143.1"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.2.2.2.2.1.1.1">Argument Configurator</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#FAFAFA" stroke="#717171"><path d="M 204.72 399.61 M 204.72 405.14 L 204.72 425.57 C 204.72 428.62 207.2 431.1 210.26 431.1 L 380.29 431.1 C 383.35 431.1 385.83 428.62 385.83 425.57 L 385.83 405.14 C 385.83 402.08 383.35 399.61 380.29 399.61 L 210.26 399.61 C 207.2 399.61 204.72 402.08 204.72 405.14 Z M 385.83 431.1"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 221.4 411.97)"><foreignObject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="147.75"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.3.3.3.3.1.1.1">Training Status Monitor</span></foreignObject></g></g><g color="#000000" fill="#F4F4F4" stroke="#0B0B0B" stroke-width="1.0pt"><path d="M 0 200.79 M 0 206.32 L 0 344.86 C 0 347.92 2.48 350.39 5.53 350.39 L 388.17 350.39 C 391.22 350.39 393.7 347.92 393.7 344.86 L 393.7 206.32 C 393.7 203.27 391.22 200.79 388.17 200.79 L 5.53 200.79 C 2.48 200.79 0 203.27 0 206.32 Z M 393.7 350.39"></path></g><g color="#000000" stroke-width="1.0pt"><g fill="#0B0B0B"><path d="M 127.95 334.65 M 127.95 340.18 L 127.95 360.61 C 127.95 363.66 130.43 366.14 133.49 366.14 L 260.21 366.14 C 263.27 366.14 265.75 363.66 265.75 360.61 L 265.75 340.18 C 265.75 337.12 263.27 334.65 260.21 334.65 L 133.49 334.65 C 130.43 334.65 127.95 337.12 127.95 340.18 Z M 265.75 366.14" style="stroke:none"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 174.71 345.67)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="44.28"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.F1.1.pic1.4.4.4.4.1.1.1">Trainer</span></foreignObject></g></g><g color="#000000" fill="#DCDCDC" stroke="#0B0B0B" stroke-width="1.0pt"><path d="M 7.87 214.57 M 7.87 220.1 L 7.87 319.27 C 7.87 322.33 10.35 324.8 13.41 324.8 L 183.44 324.8 C 186.5 324.8 188.98 322.33 188.98 319.27 L 188.98 220.1 C 188.98 217.04 186.5 214.57 183.44 214.57 L 13.41 214.57 C 10.35 214.57 7.87 217.04 7.87 220.1 Z M 188.98 324.8"></path></g><g color="#000000" fill="#DCDCDC" stroke="#0B0B0B" stroke-width="1.0pt"><path d="M 204.72 214.57 M 204.72 220.1 L 204.72 319.27 C 204.72 322.33 207.2 324.8 210.26 324.8 L 380.29 324.8 C 383.35 324.8 385.83 322.33 385.83 319.27 L 385.83 220.1 C 385.83 217.04 383.35 214.57 380.29 214.57 L 210.26 214.57 C 207.2 214.57 204.72 217.04 204.72 220.1 Z M 385.83 324.8"></path></g><g color="#000000" fill="#000000" fill-opacity="1.000000" stroke="#000000" stroke-width="1.0pt" transform="matrix(1.0 0.0 0.0 1.0 58.45 305.67)"><foreignObject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="79.95"><span class="ltx_text ltx_font_sansserif ltx_font_bold" id="S3.F1.1.pic1.5.5.5.5.1.1">Optimization</span></foreignObject></g><g color="#000000" fill="#000000" fill-opacity="1.000000" stroke="#000000" stroke-width="1.0pt" transform="matrix(1.0 0.0 0.0 1.0 260.24 305.6)"><foreignObject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="70.07"><span class="ltx_text ltx_font_sansserif ltx_font_bold" id="S3.F1.1.pic1.6.6.6.6.1.1">Approaches</span></foreignObject></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 17.72 261.81 M 17.72 267.35 L 17.72 287.77 C 17.72 290.83 20.19 293.31 23.25 293.31 L 86.98 293.31 C 90.04 293.31 92.52 290.83 92.52 287.77 L 92.52 267.35 C 92.52 264.29 90.04 261.81 86.98 261.81 L 23.25 261.81 C 20.19 261.81 17.72 264.29 17.72 267.35 Z M 92.52 293.31"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 31.67 273.41)"><foreignObject height="10.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="46.89"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.7.7.7.7.1.1.1">LoRA+</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 104.33 261.81 M 104.33 267.35 L 104.33 287.77 C 104.33 290.83 106.81 293.31 109.87 293.31 L 173.6 293.31 C 176.66 293.31 179.13 290.83 179.13 287.77 L 179.13 267.35 C 179.13 264.29 176.66 261.81 173.6 261.81 L 109.87 261.81 C 106.81 261.81 104.33 264.29 104.33 267.35 Z M 179.13 293.31"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 119.28 272.83)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="44.91"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.8.8.8.8.1.1.1">GaLore</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 17.72 222.44 M 17.72 227.98 L 17.72 248.4 C 17.72 251.46 20.19 253.94 23.25 253.94 L 86.98 253.94 C 90.04 253.94 92.52 251.46 92.52 248.4 L 92.52 227.98 C 92.52 224.92 90.04 222.44 86.98 222.44 L 23.25 222.44 C 20.19 222.44 17.72 224.92 17.72 227.98 Z M 92.52 253.94"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 32.82 233.38)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="44.59"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.9.9.9.9.1.1.1">Offload</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 104.33 222.44 M 104.33 227.98 L 104.33 248.4 C 104.33 251.46 106.81 253.94 109.87 253.94 L 173.6 253.94 C 176.66 253.94 179.13 251.46 179.13 248.4 L 179.13 227.98 C 179.13 224.92 176.66 222.44 173.6 222.44 L 109.87 222.44 C 106.81 222.44 104.33 224.92 104.33 227.98 Z M 179.13 253.94"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 114.52 233.46)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="54.43"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.10.10.10.10.1.1.1">Partition</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 214.57 261.81 M 214.57 267.35 L 214.57 287.77 C 214.57 290.83 217.04 293.31 220.1 293.31 L 283.83 293.31 C 286.89 293.31 289.37 290.83 289.37 287.77 L 289.37 267.35 C 289.37 264.29 286.89 261.81 283.83 261.81 L 220.1 261.81 C 217.04 261.81 214.57 264.29 214.57 267.35 Z M 289.37 293.31"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 224.54 272.83)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="54.85"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.11.11.11.11.1.1.1">Pre-train</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 301.18 261.81 M 301.18 267.35 L 301.18 287.77 C 301.18 290.83 303.66 293.31 306.72 293.31 L 370.45 293.31 C 373.51 293.31 375.98 290.83 375.98 287.77 L 375.98 267.35 C 375.98 264.29 373.51 261.81 370.45 261.81 L 306.72 261.81 C 303.66 261.81 301.18 264.29 301.18 267.35 Z M 375.98 293.31"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 325.23 272.83)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.71"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.12.12.12.12.1.1.1">SFT</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 214.57 222.44 M 214.57 227.98 L 214.57 248.4 C 214.57 251.46 217.04 253.94 220.1 253.94 L 283.83 253.94 C 286.89 253.94 289.37 251.46 289.37 248.4 L 289.37 227.98 C 289.37 224.92 286.89 222.44 283.83 222.44 L 220.1 222.44 C 217.04 222.44 214.57 224.92 214.57 227.98 Z M 289.37 253.94"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 232.85 233.46)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="38.24"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.13.13.13.13.1.1.1">RLHF</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 301.18 222.44 M 301.18 227.98 L 301.18 248.4 C 301.18 251.46 303.66 253.94 306.72 253.94 L 370.45 253.94 C 373.51 253.94 375.98 251.46 375.98 248.4 L 375.98 227.98 C 375.98 224.92 373.51 222.44 370.45 222.44 L 306.72 222.44 C 303.66 222.44 301.18 224.92 301.18 227.98 Z M 375.98 253.94"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 323.21 233.46)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="30.75"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.14.14.14.14.1.1.1">DPO</span></foreignObject></g></g><g color="#000000" fill="#F9F9F9" stroke="#3C3C3C" stroke-width="1.0pt"><path d="M 0 51.18 M 0 56.72 L 0 159.82 C 0 162.88 2.48 165.35 5.53 165.35 L 183.44 165.35 C 186.5 165.35 188.98 162.88 188.98 159.82 L 188.98 56.72 C 188.98 53.66 186.5 51.18 183.44 51.18 L 5.53 51.18 C 2.48 51.18 0 53.66 0 56.72 Z M 188.98 165.35"></path></g><g color="#000000" fill="#F9F9F9" stroke="#3D3D3D" stroke-width="1.0pt"><path d="M 204.72 51.18 M 204.72 56.72 L 204.72 159.82 C 204.72 162.88 207.2 165.35 210.26 165.35 L 388.17 165.35 C 391.22 165.35 393.7 162.88 393.7 159.82 L 393.7 56.72 C 393.7 53.66 391.22 51.18 388.17 51.18 L 210.26 51.18 C 207.2 51.18 204.72 53.66 204.72 56.72 Z M 393.7 165.35"></path></g><g color="#000000" stroke-width="1.0pt"><g fill="#3C3C3C"><path d="M 25.59 149.61 M 25.59 155.14 L 25.59 175.57 C 25.59 178.62 28.07 181.1 31.13 181.1 L 157.85 181.1 C 160.91 181.1 163.39 178.62 163.39 175.57 L 163.39 155.14 C 163.39 152.08 160.91 149.61 157.85 149.61 L 31.13 149.61 C 28.07 149.61 25.59 152.08 25.59 155.14 Z M 163.39 181.1" style="stroke:none"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 52.48 160.55)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="83.64"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.F1.1.pic1.15.15.15.15.1.1.1">Model Loader</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#3D3D3D"><path d="M 230.32 149.61 M 230.32 155.14 L 230.32 175.57 C 230.32 178.62 232.79 181.1 235.85 181.1 L 362.58 181.1 C 365.63 181.1 368.11 178.62 368.11 175.57 L 368.11 155.14 C 368.11 152.08 365.63 149.61 362.58 149.61 L 235.85 149.61 C 232.79 149.61 230.32 152.08 230.32 155.14 Z M 368.11 181.1" style="stroke:none"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 260.07 160.55)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="78.68"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.F1.1.pic1.16.16.16.16.1.1.1">Data Worker</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3C3C3C"><path d="M 7.87 104.33 M 7.87 109.87 L 7.87 130.29 C 7.87 133.35 10.35 135.83 13.41 135.83 L 96.83 135.83 C 99.88 135.83 102.36 133.35 102.36 130.29 L 102.36 109.87 C 102.36 106.81 99.88 104.33 96.83 104.33 L 13.41 104.33 C 10.35 104.33 7.87 106.81 7.87 109.87 Z M 102.36 135.83"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 16.49 115.27)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="77.26"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.17.17.17.17.1.1.1">Initialization</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3C3C3C"><path d="M 110.24 104.33 M 110.24 109.87 L 110.24 130.29 C 110.24 133.35 112.71 135.83 115.77 135.83 L 175.57 135.83 C 178.62 135.83 181.1 133.35 181.1 130.29 L 181.1 109.87 C 181.1 106.81 178.62 104.33 175.57 104.33 L 115.77 104.33 C 112.71 104.33 110.24 106.81 110.24 109.87 Z M 181.1 135.83"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 122.47 115.27)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="46.39"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.18.18.18.18.1.1.1">Patches</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3C3C3C"><path d="M 7.87 62.99 M 7.87 68.53 L 7.87 88.95 C 7.87 92.01 10.35 94.49 13.41 94.49 L 96.83 94.49 C 99.88 94.49 102.36 92.01 102.36 88.95 L 102.36 68.53 C 102.36 65.47 99.88 62.99 96.83 62.99 L 13.41 62.99 C 10.35 62.99 7.87 65.47 7.87 68.53 Z M 102.36 94.49"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 15.72 75.36)"><foreignObject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="79.18"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.19.19.19.19.1.1.1">Quantization</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3C3C3C"><path d="M 110.24 62.99 M 110.24 68.53 L 110.24 88.95 C 110.24 92.01 112.71 94.49 115.77 94.49 L 175.57 94.49 C 178.62 94.49 181.1 92.01 181.1 88.95 L 181.1 68.53 C 181.1 65.47 178.62 62.99 175.57 62.99 L 115.77 62.99 C 112.71 62.99 110.24 65.47 110.24 68.53 Z M 181.1 94.49"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 118.13 75.28)"><foreignObject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="55.08"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.20.20.20.20.1.1.1">Adapters</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3D3D3D"><path d="M 291.34 104.33 M 291.34 109.87 L 291.34 130.29 C 291.34 133.35 293.82 135.83 296.87 135.83 L 380.29 135.83 C 383.35 135.83 385.83 133.35 385.83 130.29 L 385.83 109.87 C 385.83 106.81 383.35 104.33 380.29 104.33 L 296.87 104.33 C 293.82 104.33 291.34 106.81 291.34 109.87 Z M 385.83 135.83"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 313.02 116.62)"><foreignObject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="51.12"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.21.21.21.21.1.1.1">Aligning</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3D3D3D"><path d="M 212.6 104.33 M 212.6 109.87 L 212.6 130.29 C 212.6 133.35 215.08 135.83 218.13 135.83 L 277.93 135.83 C 280.99 135.83 283.46 133.35 283.46 130.29 L 283.46 109.87 C 283.46 106.81 280.99 104.33 277.93 104.33 L 218.13 104.33 C 215.08 104.33 212.6 106.81 212.6 109.87 Z M 283.46 135.83"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 223.72 116.62)"><foreignObject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="48.62"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.22.22.22.22.1.1.1">Loading</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3D3D3D"><path d="M 291.34 62.99 M 291.34 68.53 L 291.34 88.95 C 291.34 92.01 293.82 94.49 296.87 94.49 L 380.29 94.49 C 383.35 94.49 385.83 92.01 385.83 88.95 L 385.83 68.53 C 385.83 65.47 383.35 62.99 380.29 62.99 L 296.87 62.99 C 293.82 62.99 291.34 65.47 291.34 68.53 Z M 385.83 94.49"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 306.28 75.36)"><foreignObject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="64.23"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.23.23.23.23.1.1.1">Preprocess</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3D3D3D"><path d="M 212.6 62.99 M 212.6 68.53 L 212.6 88.95 C 212.6 92.01 215.08 94.49 218.13 94.49 L 277.93 94.49 C 280.99 94.49 283.46 92.01 283.46 88.95 L 283.46 68.53 C 283.46 65.47 280.99 62.99 277.93 62.99 L 218.13 62.99 C 215.08 62.99 212.6 65.47 212.6 68.53 Z M 283.46 94.49"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 223.22 75.36)"><foreignObject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="49.62"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.24.24.24.24.1.1.1">Merging</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F9F9F9" stroke="#3C3C3C"><path d="M 0 -7.87 M 0 -2.34 L 0 18.09 C 0 21.14 2.48 23.62 5.53 23.62 L 183.44 23.62 C 186.5 23.62 188.98 21.14 188.98 18.09 L 188.98 -2.34 C 188.98 -5.4 186.5 -7.87 183.44 -7.87 L 5.53 -7.87 C 2.48 -7.87 0 -5.4 0 -2.34 Z M 188.98 23.62"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 16.48 3.65)"><foreignObject height="10.76" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="155.63"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.25.25.25.25.1.1.1">100+ Pre-Trained Models</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F9F9F9" stroke="#3D3D3D"><path d="M 204.72 -7.87 M 204.72 -2.34 L 204.72 18.09 C 204.72 21.14 207.2 23.62 210.26 23.62 L 388.17 23.62 C 391.22 23.62 393.7 21.14 393.7 18.09 L 393.7 -2.34 C 393.7 -5.4 391.22 -7.87 388.17 -7.87 L 210.26 -7.87 C 207.2 -7.87 204.72 -5.4 204.72 -2.34 Z M 393.7 23.62"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 241.96 3.72)"><foreignObject height="10.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="114.5"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.26.26.26.26.1.1.1">50+ NLP Datasets</span></foreignObject></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 98.43 399.61 L 98.43 357.75" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 -1.0 1.0 0.0 98.43 357.75)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 295.28 350.39 L 295.28 392.25" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 1.0 -1.0 0.0 295.28 392.25)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 94.49 181.1 L 94.49 207.21" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 1.0 -1.0 0.0 94.49 207.21)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 299.21 181.1 L 299.21 207.21" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 1.0 -1.0 0.0 299.21 207.21)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 94.49 23.62 L 94.49 43.83" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 1.0 -1.0 0.0 94.49 43.83)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 299.21 23.62 L 299.21 43.83" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 1.0 -1.0 0.0 299.21 43.83)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1:</span>The architecture of <span class="ltx_text ltx_font_smallcaps" id="S3.F1.3.1">LlamaFactory</span>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Loader</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">이 섹션에서는 처음에 모델 로더의 네 가지 구성 요소인 모델 초기화, 모델 패치, 모델 양자화 및 어댑터 부착을 제시한 다음 미세 조정 시 매개변수 부동 소수점 정밀도를 처리하여 광범위한 장치에 적응하는 접근법에 대해 설명한다.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Model Initialization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">모델 로딩 및 파라미터 초기화를 위해 <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px1.p1.1.1">AutoModel</span> Transformers <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wolf et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib79" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite> API를 사용한다. 프레임워크가 서로 다른 모델 아키텍처와 호환되도록 하기 위해 각 계층의 유형을 저장하는 모델 레지스트리를 설정하여 효율적인 미세 조정 기술의 활용을 보다 쉽게 한다. 토큰나이저의 어휘 크기가 임베딩 레이어의 용량을 초과하는 경우 레이어의 크기를 조정하고 노이즈 평균 초기화를 통해 새로운 파라미터를 초기화한다. RoPE 스케일링 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Chen et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib13" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>에 대한 스케일링 계수를 결정하기 위해 모델의 컨텍스트 길이에 대한 최대 입력 시퀀스 길이의 비율로 계산한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Model Patching</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">플래시 어텐션과 S<math alttext="{}^{2}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><msup id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1a" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"></mi><mn id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1"><cn id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" type="integer" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math> 어텐션을 가능하게 하기 위해 원숭이 패치를 사용하여 모델의 순방향 계산을 대체한다. 그럼에도 불구하고 Transformers 4.34.0 이후 플래시 어텐션이 지원되었기 때문에 API를 사용하여 플래시 어텐션을 사용하도록 한다. 동적 모듈의 과도한 분할을 방지하기 위해 DeepSpeed ZeRO-3 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Rasley et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib63" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite> 하에서 최적화 할 때 전문가 혼합 블록(MoE)을 리프 모듈로 설정한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Model Quantization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">LLM.int8 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib20" title=""><span class="ltx_text ltx_font_bold">2022a</span></a>)</cite>로 8비트 또는 4비트로 모델을 동적으로 양자화하는 것은 비트 및 바이트 라이브러리 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib19" title=""><span class="ltx_text ltx_font_bold">2021</span></a>)</cite>를 통해 수행될 수 있다. 4비트 양자화를 위해 이중 양자화와 4비트 노멀 플로트를 QLoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib22" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>로 활용한다. 또한 GPTQ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Frantar et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib26" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>, AWQ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Lin et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib49" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>, AQLM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Egiazarian et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib25" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>를 포함한 PTQ(post-training quantization) 방법으로 양자화된 모델의 미세 조정을 지원한다. 양자화된 가중치를 직접 미세 조정할 수는 없으므로 양자화된 모델은 어댑터 기반 방법과만 호환됩니다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Adapter Attaching</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">어댑터를 부착하기 위해 모델 레지스트리를 사용하여 적절한 계층을 자동으로 식별합니다. 어댑터는 메모리를 절약하기 위해 기본적으로 레이어의 서브세트에 부착되지만, 모든 선형 레이어에 부착하면 더 나은 성능 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib22" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>를 얻을 수 있다. PEFT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Mangrulkar et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib52" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite> 라이브러리는 LoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib33" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>, rsLoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Kalajdzievski</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib39" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>, DoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Liu et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib50" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>와 같은 어댑터를 부착하는 매우 편리한 방법을 제공한다. LoRA를 가속하기 위해 역방향 연산을 Unsloth <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Han and Han</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib29" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>로 대체한다. 인간 피드백(RLHF)으로부터 강화 학습을 수행하기 위해, 우리는 각 토큰의 표현을 스칼라에 매핑하는 선형 계층인 모델에 값 헤드를 추가한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Precision Adaptation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px5.p1.1">디바이스의 기능을 기반으로 사전 훈련된 모델의 부동 소수점 정밀도를 처리합니다. NVIDIA GPU의 경우 계산 능력이 8.0 이상인 경우 bfloat16 정밀도를 사용한다. 그렇지 않으면, float16이 채택된다. 어센드 NPU 및 AMD GPU에는 float16을 사용하고 비 CUDA 디바이스에는 float32를 사용한다. 플로트16 정밀도로 bfloat16 모델을 로드하면 오버플로 문제가 발생할 수 있습니다. 혼합 정밀 훈련에서는 훈련 가능한 모든 파라미터를 float32로 설정하지만, bfloat16 훈련에서는 훈련 가능한 파라미터를 bfloat16으로 유지한다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Worker</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">데이터 처리 파이프라인은 데이터 로딩, 데이터 정렬, 데이터 병합 및 데이터 전처리를 포함한다. 다양한 태스크의 데이터 세트를 통일된 형식으로 표준화하여 다양한 형식의 데이터 세트에 대한 모델을 미세 조정할 수 있습니다.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Dataset Loading</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Lhoest et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib44" title=""><span class="ltx_text ltx_font_bold">2021</span></a>)</cite> 라이브러리를 사용하여 데이터를 로드합니다. 이를 통해 사용자는 휴징 페이스 허브에서 원격 데이터 세트를 로드하거나 스크립트를 통해 또는 파일을 통해 로컬 데이터 세트를 읽을 수 있습니다. 데이터 세트 라이브러리는 데이터 처리 중 메모리 오버헤드를 크게 줄이고 화살표 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Apache</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib5" title=""><span class="ltx_text ltx_font_bold">2016</span></a>)</cite>를 사용하여 샘플 쿼리를 가속화합니다. 기본적으로 전체 데이터 세트는 로컬 디스크에 다운로드됩니다. 그러나 데이터 세트가 너무 커서 저장할 수 없는 경우, 프레임워크는 다운로드하지 않고 데이터 세트 스트리밍을 반복하여 제공합니다.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span> <span class="ltx_text ltx_font_smallcaps" id="S3.T2.2.1">LlamaFactory</span>의 데이터셋 구조.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.3" style="width:429.3pt;height:104.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.9pt,10.7pt) scale(0.83018324188649,0.83018324188649) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T2.3.1.1.1.1">Plain text</th>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S3.T2.3.1.1.1.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.1.1.2.1">[{"text": "…"}, {"text": "…"}]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.2.2.1">Alpaca-like data</th>
<td class="ltx_td ltx_align_justify" id="S3.T2.3.1.2.2.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.2.2.2.1">[{"instruction": "…", "input": "…", "output": "…"}]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.3.3.1">ShareGPT-like data</th>
<td class="ltx_td ltx_align_justify" id="S3.T2.3.1.3.3.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.3.3.2.1">[{"conversations": [{"from": "human", "value": "…"}, {"from": "gpt", "value": "…"}]}]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.4.4.1">Preference data</th>
<td class="ltx_td ltx_align_justify" id="S3.T2.3.1.4.4.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.4.4.2.1">[{"instruction": "…", "input": "…", "output": ["…", "…"]}]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.1.5.5.1">Standardized data</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.3.1.5.5.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.5.5.2.1">{"prompt": [{"role": "…", "content": "…"}],</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.6.6">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T2.3.1.6.6.1"></th>
<td class="ltx_td ltx_align_justify" id="S3.T2.3.1.6.6.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.6.6.2.1">"response": [{"role": "…", "content": "…"}],</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.7.7">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb" id="S3.T2.3.1.7.7.1"></th>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S3.T2.3.1.7.7.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.7.7.2.1">"system": "…", "tools": "…"}</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Dataset Aligning</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">데이터 세트 형식을 통일하기 위해 데이터 세트의 구조를 특성화하기 위해 데이터 기술 사양을 설계한다. 예를 들어, alpaca 데이터 세트는 명령어, 입력 및 출력 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Taori et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib66" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>의 세 개의 열을 갖는다. 데이터 기술 사양에 따라 다양한 작업과 호환되는 표준 구조로 데이터 세트를 변환합니다. 데이터 세트 구조의 일부 예는 표 <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.T2" title="Table 2 ‣ Dataset Loading ‣ 3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>에 나와 있다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Dataset Merging</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">통합된 데이터 집합 구조는 여러 데이터 집합을 병합하기 위한 효율적인 접근 방식을 제공한다. 비스트리밍 모드의 데이터 세트는 훈련 중에 데이터 세트가 섞이기 전에 단순히 연결한다. 그러나 스트리밍 모드에서는 데이터 집합을 단순히 연결하면 데이터 셔플링이 방해됩니다. 따라서 우리는 서로 다른 데이터 세트에서 데이터를 교대로 읽을 수 있는 방법을 제공한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Dataset Pre-processing</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px4.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px4.p1.1.1">LlamaFactory</span> is designed for fine-tuning the text generative models, mainly used in chat completion. 채팅 템플릿은 이러한 모델의 명령어 수행 능력과 매우 관련이 있기 때문에 이러한 모델에서 중요한 구성 요소이다. 따라서 모델 유형에 따라 자동으로 선택할 수 있는 수십 개의 채팅 템플릿을 제공합니다. 토나이저를 이용하여 채팅 템플릿을 적용한 후 문장을 인코딩한다. 기본적으로 완료에 대한 손실만 계산하는 반면 프롬프트는 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Taori et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib66" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>를 무시합니다. 선택적으로, 우리는 시퀀스 패킹 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Krell et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib41" title=""><span class="ltx_text ltx_font_bold">2021</span></a>)</cite>를 활용하여 생성 사전 트레이닝을 수행할 때 자동으로 인에이블되는 트레이닝 시간을 줄일 수 있다. 부록 <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A3" title="Appendix C Details of Designing Chat Template ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">C</span></a>는 템플릿 설계의 세부 정보를 보여 줍니다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Trainer</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Efficient Training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.3">LoRA+ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hayou et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib30" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>와 GaLore <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Zhao et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib86" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>를 포함한 최첨단 효율적인 미세 조정 방법을 기본 구성 요소를 교체하여 훈련자에게 통합한다. 이러한 훈련 접근 방식은 트레이너와 독립적이어서 다양한 작업에 쉽게 적용할 수 있다. 사전 훈련과 SFT는 Transformers <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wolf et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib79" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite>의 트레이너를 활용하였고, RLHF와 DPO는 TRL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">von Werra et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib72" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite>의 트레이너를 채택하였다. 맞춤형 데이터 콜레이터는 다양한 훈련 접근 방식의 트레이너를 차별화하기 위해 활용된다. 선호도 데이터에 대한 트레이너의 입력 포맷을 맞추기 위해, 첫 번째 <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.2.m2.1d">italic_n</annotation></semantics></math> 샘플이 선택되고 마지막 <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.3.m3.1d">italic_n</annotation></semantics></math> 샘플이 거부된 예시인 배치에서 <math alttext="2n" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml">2</mn><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1"><times id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1"></times><cn id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2">2</cn><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">2n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">2 italic_n</annotation></semantics></math> 샘플을 구축한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Model-Sharing RLHF</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">소비자 디바이스에서 RLHF 트레이닝을 허용하는 것은 LLM 미세 조정에 유용한 속성이다. 그러나 RLHF 훈련은 4개의 다른 모델이 필요하기 때문에 어렵다. 이 문제를 해결하기 위해, 우리는 모델 공유 RLHF를 제안하여, 하나 이상의 사전 훈련된 모델 없이 전체 RLHF 훈련을 가능하게 한다. 구체적으로, 우리는 먼저 보상 모델링을 위한 목적 함수와 어댑터 및 값 헤드를 훈련하여 모델이 보상 점수를 계산할 수 있도록 한다. 그런 다음 다른 어댑터와 값 헤드를 초기화하고 PPO 알고리즘 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Ouyang et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib58" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>로 훈련한다. 어댑터와 값 헤드는 <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px2.p1.1.1">set_adapter</span> 및 <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px2.p1.1.2">disable_adapter</span> PEFT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Mangrulkar et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib52" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>의 API를 통해 동적으로 전환되어 사전 훈련된 모델이 정책, 값, 참조 및 보상 모델 역할을 동시에 수행할 수 있습니다. 우리가 아는 한, 이것은 소비자 장치에 대한 RLHF 훈련을 지원하는 첫 번째 방법이다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Distributed Training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">우리는 분산 훈련을 위해 위의 트레이너들을 DeepSpeed <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Rasley et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib63" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite>와 결합할 수 있다. DeepSpeed ZeRO 최적화기를 활용하면 분할 또는 오프로딩을 통해 메모리 소비를 더욱 줄일 수 있습니다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Utilities</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Accelerated Inference</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">추론 시간 동안 데이터 워커의 채팅 템플릿을 재사용하여 모델 입력을 구축한다. 스트림 디코딩을 지원하는 트랜스포머 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wolf et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib79" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite> 및 vLLM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Kwon et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib42" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>를 사용하여 모델 출력을 샘플링하는 지원을 제공합니다. 또한, 비동기 LLM 엔진과 vLLM의 페이징된 주의를 활용하여 높은 처리량의 동시 추론 서비스를 제공하여 미세 조정된 LLM을 다양한 응용 프로그램에 쉽게 배치할 수 있는 OpenAI 스타일 API를 구현한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Comprehensive Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.1">MLU <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hendrycks et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib31" title=""><span class="ltx_text ltx_font_bold">2021</span></a>)</cite>, CMMLU <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Li et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib45" title=""><span class="ltx_text ltx_font_bold">2023a</span></a>)</cite>, C-Eval <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Huang et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib34" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>, BLEU-4 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Papineni et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib59" title=""><span class="ltx_text ltx_font_bold">2002</span></a>)</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Lin</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib48" title=""><span class="ltx_text ltx_font_bold">2004</span></a>)</cite>와 같은 텍스트 유사성 점수를 계산하는 등 LLMs를 평가하기 위한 여러 메트릭을 포함한다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.1.1">LlamaBoard</span>: A Unified Interface for <span class="ltx_text ltx_font_smallcaps" id="S3.SS5.2.2">LlamaFactory</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p1.1.1">LlamaBoard</span>은 Gradio <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Abid et al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib1" title=""><span class="ltx_text ltx_font_bold">2019</span></a>)</cite>를 기반으로 하는 통합 사용자 인터페이스로 사용자가 코드를 작성하지 않고도 LLM의 미세 조정을 사용자 지정할 수 있습니다. 간소화 된 모델 미세 조정 및 추론 서비스를 제공 하 여 사용자가 실제에서 100 + LLM 및 50 + 데이터 집합을 쉽게 활용할 수 있습니다. <span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p1.1.2">LlamaBoard</span> has following notable features:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S3.SS5.p2.1.1">Easy Configuration</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p2.1.2"></span><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p2.1.3">LlamaBoard</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p2.1.4">를 사용하면 사용자가 웹 인터페이스와의 상호 작용을 통해 미세 조정 인수를 사용자 지정할 수 있습니다. 대부분의 사용자에게 권장되는 많은 인수에 기본값을 제공하여 구성 프로세스를 단순화합니다. 또한 사용자는 웹 UI에서 데이터 세트를 미리 보고 사용자 지정 형식을 확인할 수 있습니다. </span></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S3.SS5.p3.1.1">Monitorable Training</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p3.1.2">훈련 과정에서 훈련 로그와 손실 곡선이 실시간으로 시각화되고 업데이트되어 사용자가 훈련 진행 상황을 모니터링할 수 있다. 이 기능은 미세 조정 프로세스를 분석하는 데 유용한 통찰력을 제공합니다. </span></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p4">
<p class="ltx_p" id="S3.SS5.p4.1"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S3.SS5.p4.1.1">Flexible Evaluation</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p4.1.2"></span><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p4.1.3">LlamaBoard</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p4.1.4">Dataets에서 텍스트 유사성 점수를 계산하여 모델을 자동으로 평가하거나 모델과 채팅하여 인간 평가를 수행하도록 지원합니다. </span></p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p5">
<p class="ltx_p" id="S3.SS5.p5.1"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S3.SS5.p5.1.1">Multilingual Support</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p5.1.2"></span><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p5.1.3">LlamaBoard</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p5.1.4">localization files provides the integration of new languages for rendering the interface. 현재 우리는 영어, 러시아어 및 중국어의 세 가지 언어를 지원하며, 이는 더 넓은 범위의 사용자가 LLM을 미세 조정하기 위해 </span><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p5.1.5">LlamaBoard</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p5.1.6">를 활용할 수 있도록 한다. </span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_medium" id="S4.1.1.1">4</span> </span>Empirical Study</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering ltx_font_bold"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="S4.T3.5.1.1">표 3</span>:</span> <span class="ltx_text ltx_font_medium ltx_font_smallcaps" id="S4.T3.6.2">LlamaFactory</span>에서 서로 다른 미세 조정 방법을 사용하여 훈련 효율성을 비교합니다. 각 모델의 GaLore, LoRA, QLoRA 중 가장 좋은 결과는 굵은 글씨로 되어 있다.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.7" style="width:862.9pt;height:359.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(237.3pt,-99.0pt) scale(2.22175270402722,2.22175270402722) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.7.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.7.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt" colspan="3" id="S4.T3.7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.2.1">Gemma-2B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt" colspan="3" id="S4.T3.7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.3.1">Llama2-7B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="3" id="S4.T3.7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.4.1">Llama2-13B</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.7.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.7.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.1.1">Method</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.2.1">Memory</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.3.1">Throughput</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.7.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.4.1">PPL</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.5.1">Memory</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.6.1">Throughput</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.7.1.2.1.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.7.1">PPL</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.8.1">Memory</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.9.1">Throughput</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.10.1">PPL</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.3.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.3.2.1"></th>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.2.1">(GB)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.3.1">(Tokens/s)</span></td>
<td class="ltx_td ltx_border_r" id="S4.T3.7.1.3.2.4"></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.5.1">(GB)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.6.1">(Tokens/s)</span></td>
<td class="ltx_td ltx_border_r" id="S4.T3.7.1.3.2.7"></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.8.1">(GB)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.9.1">(Tokens/s)</span></td>
<td class="ltx_td" id="S4.T3.7.1.3.2.10"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.7.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.1.1">Baseline</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.2.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.3.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.7.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.4.1">11.83</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.5.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.6.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.7.1.4.3.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.7.1">7.53</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.8.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.9.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.10.1">6.66</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.1.1">Full-tuning</span></th>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.2.1">17.06</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.3.1">3090.42</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.4.1">10.34</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.5.1">38.72</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.6.1">1334.72</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.5.4.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.7.1">5.56</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.8.1">/</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.9.1">/</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.10.1">/</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.1.1">Freeze-tuning</span></th>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.2.1">8.10</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.3.1">5608.49</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.6.5.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.4.1">11.33</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.5.1">15.69</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.6.1">2904.98</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.6.5.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.7.1">6.59</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.8.1">29.02</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.9.1">1841.46</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.10.1">6.56</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.1.1">GaLore</span></th>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.2.1">10.16</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.3.1">2483.05</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.4.1">10.38</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.5.1">15.43</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.6.1">1583.77</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.7.6.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.7.1">5.88</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.8.1">28.91</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.9.1">956.39</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.10.1">5.72</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.8.7.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.1.1">LoRA</span></th>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.2.1">7.91</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.3.1">3521.05</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.8.7.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.4.1">10.19</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.5.1">16.32</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.6.1">1954.07</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.8.7.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.7.1">5.81</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.8.1">30.09</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.9.1">1468.19</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.10.1">5.75</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.7.1.9.8.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.1.1">QLoRA</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.2.1">5.21</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.3.1">3158.59</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T3.7.1.9.8.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.4.1">10.46</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.5.1">7.52</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.6.1">1579.16</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T3.7.1.9.8.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.7.1">5.91</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.8.1">12.61</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.9.1">973.53</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.10.1">5.81</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering ltx_font_bold"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="S4.T4.7.1.1">표 4</span>:</span> <span class="ltx_text ltx_font_medium ltx_font_smallcaps" id="S4.T4.8.2">LlamaFactory</span>에서 서로 다른 미세 조정 방법을 사용하여 특정 작업에 대한 성능(ROUGE 측면에서) 비교. 각 모델의 최상의 결과는 <span class="ltx_text ltx_framed_underline" id="S4.T4.9.3">underlined</span>이며 각 작업의 최상의 결과는 굵게 표시됩니다.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.10" style="width:862.9pt;height:239.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(171.9pt,-47.7pt) scale(1.66218396737816,1.66218396737816) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.10.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.10.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.10.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt" colspan="5" id="S4.T4.10.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.1.1.2.1">CNN / DM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt" colspan="5" id="S4.T4.10.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.1.1.3.1">XSum</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="5" id="S4.T4.10.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.1.1.4.1">AdGen</span></th>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T4.10.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.2.1">Baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.3.1">FT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.4.1">GaLore</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.5.1">LoRA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T4.10.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.6.1">QLoRA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.7.1">Baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.8.1">FT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.9.1">GaLore</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.10"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.10.1">LoRA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T4.10.1.2.2.11"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.11.1">QLoRA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.12.1">Baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.13.1">FT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.14.1">GaLore</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.15"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.15.1">LoRA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.16.1">QLoRA</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.10.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.10.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.1.1">LLaMA2-7B</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.2.1">12.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.3"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.3.1.3.1">22.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.4.1">22.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.5"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.5.1">22.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.1.3.1.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.6.1">22.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.7.1">13.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.8.1">27.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.9.1">27.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.10"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.3.1.10.1">28.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.1.3.1.11"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.11.1">28.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.12.1">0.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.13"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.3.1.13.1">20.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.14.1">19.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.15"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.15.1">20.29</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.16.1">20.45</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.10.1.4.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.1.1">Mistral-7B</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.2.1">14.39</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.3.1">22.03</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.4.1">22.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.5"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.4.2.5.1">23.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.4.2.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.6.1">23.28</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.7.1">15.87</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.8.1">23.57</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.9.1">28.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.10"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.10.1">30.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.4.2.11"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.4.2.11.1">30.44</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.12.1">7.82</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.13.1">20.14</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.14.1">20.90</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.15"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.4.2.15.1">20.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.16.1">20.56</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.10.1.5.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.1.1">Gemma-7B</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.2.1">15.97</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.3.1">22.07</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.4.1">/</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.5"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.5.1">22.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.5.3.6"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.5.3.6.1">22.44</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.7.1">15.31</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.8.1">25.13</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.9.1">/</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.10"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.10.1">28.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.5.3.11"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.5.3.11.1">29.02</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.12.1">11.57</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.13.1">19.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.14.1">/</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.15"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.5.3.15.1">20.62</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.16.1">19.81</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.10.1.6.4.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.1.1">Qwen1.5-7B</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.2.1">15.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.3.1">22.46</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.4.1">21.76</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.5"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.6.4.5.1">22.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.6.4.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.6.1">22.52</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.7.1">19.27</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.8.1">26.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.9.1">26.64</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.10"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.6.4.10.1">27.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.6.4.11"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.11.1">27.60</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.12.1">14.49</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.13.1">20.42</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.14.1">21.08</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.15"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.15.1">21.31</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.16"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.6.4.16.1">21.34</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.10.1.7.5.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.1.1">Yi-6B</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.2.1">16.85</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.3.1">22.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.4.1">22.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.5"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.7.5.5.1">22.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.7.5.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.6.1">22.97</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.7.1">18.24</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.8.1">27.09</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.9.1">28.25</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.10"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.10.1">28.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.7.5.11"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.7.5.11.1">29.21</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.12.1">13.34</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.13.1">19.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.14.1">20.06</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.15"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.7.5.15.1">20.97</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.16.1">20.31</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T4.10.1.8.6.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.1.1">ChatGLM3-6B</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.2.1">18.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.3.1">22.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.4"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.8.6.4.1">22.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.5"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.5.1">21.68</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.10.1.8.6.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.6.1">21.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.7.1">16.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.8.1">26.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.9.1">26.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.10"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.10.1">26.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.10.1.8.6.11"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.8.6.11.1">26.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.12.1">14.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.13.1">19.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.14"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.8.6.14.1">20.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.15"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.15.1">20.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.16.1">20.49</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">We systematically evaluate </span><span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.2">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="S4.p1.1.3"> from two perspectives: 1) training efficiency in terms of memory usage, throughput and perplexity. 2) 다운스트림 태스크에 대한 적응의 효과. </span></p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_medium" id="S4.SS1.1.1.1">4.1</span> </span>Training Efficiency</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Experimental Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.1">PubMed</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_bold">Canese and Weis</span> <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.2.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib11" title=""><span class="ltx_text ltx_font_bold">2013</span></a><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.3.2.2.1">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.4">데이터세트는 3,600만 개 이상의 생체 의학 문헌 레코드를 포함한다. 우리는 문헌의 초록에서 약 400,000개의 토큰을 추출하여 훈련 사례를 구성합니다. Gemma-2B</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.5.1">(</span><span class="ltx_text ltx_font_bold">Mesnard et al.</span><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.6.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib53" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.7.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.8">, Llama2-7B and Llama2-13B</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.9.1">(</span><span class="ltx_text ltx_font_bold">Touvron et al.</span><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.10.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib70" title=""><span class="ltx_text ltx_font_bold">2023b</span></a><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.11.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.12"> models using the generative pre-training objective with various efficient fine-tuning methods. Full-tuning, freeze-tuning, GaLore, LoRA 및 4-bit QLoRA의 결과를 비교한다. 미세 조정 후, 다양한 방법의 효율성을 평가하기 위해 훈련 예제에 대한 복잡도를 계산한다. 또한 사전 훈련된 모델의 복잡성을 기준선으로 통합한다. 더 많은 실험 세부 사항은 Appendix </span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#A4.SS1" title="D.1 Training Efficiency ‣ Appendix D Experiment Details ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">D.1</span></a><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.13">에서 찾을 수 있다. </span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px2.p1.1.1">트레이닝 효율성 결과는 표</span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#S4.T3" title="Table 3 ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px2.p1.1.2"에서 제시되며, 여기서 메모리는 트레이닝 동안 소비되는 피크 메모리를 지칭하고, 처리량은 초당 트레이닝된 토큰의 수로 계산되며, PPL은 트레이닝 예들에 대한 모델의 복잡도를 나타낸다. 전체 조정 라마2-13B는 메모리 오버플로로 이어지기 때문에 결과는 기록되지 않는다. QLoRA는 사전 훈련된 가중치들이 더 낮은 정밀도로 표현되기 때문에 일관되게 가장 낮은 메모리 풋프린트를 갖는다는 것을 관찰한다. LoRA는 Unsloth에 의해 LoRA 계층에서 최적화를 활용하는 더 높은 처리량을 나타낸다. GaLore는 대형 모델에서는 낮은 PPL을 달성하는 반면 LoRA는 소형 모델에서는 이점이 있다. </span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_medium" id="S4.SS2.1.1.1">4.2</span> </span>Fine-Tuning on Downstream Tasks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Experimental Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.1">서로 다른 효율적인 미세 조정 방법의 유효성을 평가하기 위해 다운스트림 태스크에서 미세 조정 후 다양한 모델의 성능을 비교한다. CNN/DM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.2.1">(</span><span class="ltx_text ltx_font_bold">Nallapati et al.</span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib55" title=""><span class="ltx_text ltx_font_bold">2016</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.5">, XSum </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.6.1">(</span><span class="ltx_text ltx_font_bold">Narayan et al.</span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.7.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib56" title=""><span class="ltx_text ltx_font_bold">2018</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.8.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.9"> 및 AdGen </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.10.1">(</span><span class="ltx_text ltx_font_bold">Shao et al.</span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.11.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib64" title=""><span class="ltx_text ltx_font_bold">2019</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.12.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.13" 등의 세 가지 대표적인 텍스트 생성 작업에서 2,000개의 예제와 1,000개의 예를 사용하여 훈련 집합과 테스트 집합을 구성한다. 서로 다른 미세 조정 방법을 사용하여 시퀀스 대 시퀀스 태스크에 따라 여러 개의 명령어 조정 모델을 선택하고 미세 조정한다. 본 논문에서는 Full-tuning (FT), GaLore, LoRA 및 4-bit QLoRA의 결과를 비교한다. 미세 조정 후 각 태스크의 테스트 세트에서 ROUGE 점수 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.14.1">(</span><span class="ltx_text ltx_font_bold">Lin</span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.15.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib48" title=""><span class="ltx_text ltx_font_bold">2004</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.16.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.17">를 계산한다. 우리는 또한 원래 지시 조정 모델의 점수를 기준선으로 통합한다. 더 많은 실험 세부 사항은 Appendix </span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#A4.SS2" title="D.2 Fine-Tuning on Downstream Tasks ‣ Appendix D Experiment Details ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">D.2</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.18">에서 찾을 수 있다. </span></p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px2.p1.1.1">다운스트림 작업에 대한 평가 결과는 Table </span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#S4.T4" title="Table 4 ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px2.p1.1.2">에 나와 있습니다. 각 LLM 및 각 데이터 세트에 대해 ROUGE-1, ROUGE-2 및 ROUGE-L에 대한 평균 점수를 보고한다. Gemma-7B의 일부 결과는 GaLore 방법이 이 모델에 적용되지 않기 때문에 표에 포함되지 않는다. 결과에서 흥미로운 발견은 CNN/DM 및 AdGen 데이터 세트에서 Llama2-7B 및 ChatGLM3-6B 모델을 제외하고 LoRA 및 QLoRA가 대부분의 경우에 최상의 성능을 달성한다는 것이다. 이 현상은 LLM 모델을 특정 작업에 적용하는 데 있어 이러한 효율적인 미세 조정 방법의 효율성을 강조한다. 또한, 미스트랄-7B 모델이 영어 데이터 세트에서 더 나은 성능을 보이는 반면 Qwen1.5-7B 모델은 중국 데이터 세트에서 더 높은 점수를 달성한다는 것을 관찰한다. 이러한 결과는 미세 조정된 모델의 성능이 특정 언어에 대한 고유한 기능과도 관련이 있음을 시사한다. </span></p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_medium" id="S5.1.1.1">5</span> </span>Conclusion and Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">본 논문에서는 LLM의 효율적인 미세 조정을 위한 통합 프레임워크인 </span><span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.2">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="S5.p1.1.3"을 시연한다. 모듈러 설계를 통해 모델, 데이터 세트 및 훈련 방법 간의 종속성을 최소화하고 다양한 범위의 효율적인 미세 조정 기술로 100 LLM 이상을 미세 조정하는 통합 접근법을 제공한다. 또한 유연한 웹 UI </span><span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.4">LlamaBoard</span><span class="ltx_text ltx_font_bold" id="S5.p1.1.5"를 제공하여 코딩 노력 없이 LLM의 맞춤형 미세 조정 및 평가를 가능하게 합니다. 우리는 언어 모델링 및 텍스트 생성 태스크에 대한 프레임워크의 효율성과 유효성을 경험적으로 검증한다. </span></p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">향후에는 </span><span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.2">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="S5.p2.1.3">synchronous with the state-of-the-art models and efficient fine-tuning techniques. 우리는 또한 오픈 소스 커뮤니티의 기여를 환영합니다. 향후 버전에서는 보다 발전된 병렬 훈련 전략과 LLM의 다중 모드 효율적인 미세 조정을 탐색할 것이다. </span></p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_bold ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_medium" id="S6.1.1.1">6</span> </span>Broader Impact and Responsible Use</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="S6.p1.1.2">LLMs에 관심 있는 많은 개체를 끌어들여 자체 모델을 미세 조정 하는 가능성을 탐색 했습니다. 이것은 오픈 소스 커뮤니티의 성장에 크게 기여합니다. 그것은 점점 더 주목을 받고 있으며 LLMs에 대한 효율적인 미세 조정 프레임워크의 대표로 Awesome Transformers</span><span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/transformers/blob/v4.39.0/awesome-transformers.md" title="">https://github.com/huggingface/transformers/blob/v4.39.0/awesome-transformers.md</a></span></span></span><span class="ltx_text ltx_font_bold" id="S6.p1.1.3">에 소개되고 있다. 우리는 실무자들이 사회에 이익을 가져다주는 우리의 프레임워크를 기반으로 LLM을 구축하기를 기대한다. 모델 라이선스에 대한 부착은 LLM을 미세 조정하기 위해 </span><span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.4">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="S6.p1.1.5">를 사용할 때 필수이므로 잠재적인 오용을 방지할 수 있습니다. </span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_font_bold ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib1.5.5.1">Abid et&nbsp;al. (2019)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib1.7.1">
Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/1906.02569" title="">Gradio: Hassle-free sharing and testing of ml models in the wild</a><span class="ltx_text ltx_font_bold" id="bib.bib1.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib1.9.1">arXiv preprint arXiv:1906.02569</em><span class="ltx_text ltx_font_bold" id="bib.bib1.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib2.4.4.1">AI (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib2.6.1">
Lightning AI. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/Lightning-AI/lit-gpt" title="">Lit-GPT</a><span class="ltx_text ltx_font_bold" id="bib.bib2.7.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib3.5.5.1">Almazrouei et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib3.7.1">
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2311.16867" title="">The falcon series of open language models</a><span class="ltx_text ltx_font_bold" id="bib.bib3.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib3.9.1">arXiv preprint arXiv:2311.16867</em><span class="ltx_text ltx_font_bold" id="bib.bib3.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib4.5.5.1">Anand et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib4.7.1">
Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/nomic-ai/gpt4all" title="">GPT4All: Training an assistant-style chatbot with large scale data distillation from GPT-3.5-turbo</a><span class="ltx_text ltx_font_bold" id="bib.bib4.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib5.4.4.1">Apache (2016)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib5.6.1">
Apache. 2016.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/apache/arrow" title="">Arrow</a><span class="ltx_text ltx_font_bold" id="bib.bib5.7.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib6.5.5.1">Bai et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib6.7.1">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2309.16609" title="">Qwen technical report</a><span class="ltx_text ltx_font_bold" id="bib.bib6.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib6.9.1">arXiv preprint arXiv:2309.16609</em><span class="ltx_text ltx_font_bold" id="bib.bib6.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib7.5.5.1">Beeching et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib7.7.1">
Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" title="">Open LLM leaderboard</a><span class="ltx_text ltx_font_bold" id="bib.bib7.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib8.5.5.1">Ben&nbsp;Zaken et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib8.7.1">
Elad Ben&nbsp;Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2022.acl-short.1" title="">BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</a><span class="ltx_text ltx_font_bold" id="bib.bib8.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib8.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib8.10.2">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em><span class="ltx_text ltx_font_bold" id="bib.bib8.11.3">, pages 1–9, Dublin, Ireland. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib9.5.5.1">Bhardwaj et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib9.7.1">
Rishabh Bhardwaj, Do&nbsp;Duc Anh, and Soujanya Poria. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.11746" title="">Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic</a><span class="ltx_text ltx_font_bold" id="bib.bib9.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib9.9.1">arXiv preprint arXiv:2402.11746</em><span class="ltx_text ltx_font_bold" id="bib.bib9.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib10.5.5.1">Bi et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib10.7.1">
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2401.02954" title="">DeepSeek LLM: Scaling open-source language models with longtermism</a><span class="ltx_text ltx_font_bold" id="bib.bib10.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib10.9.1">arXiv preprint arXiv:2401.02954</em><span class="ltx_text ltx_font_bold" id="bib.bib10.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib11.4.4.1">Canese and Weis (2013)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib11.6.1">
Kathi Canese and Sarah Weis. 2013.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://www.be-md.ncbi.nlm.nih.gov/books/NBK153385/pdf/Bookshelf_NBK153385.pdf" title="">PubMed: the bibliographic database</a><span class="ltx_text ltx_font_bold" id="bib.bib11.7.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib11.8.1">The NCBI handbook</em><span class="ltx_text ltx_font_bold" id="bib.bib11.9.2">, 2(1).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib12.5.5.1">Chen et&nbsp;al. (2024a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib12.7.1">
Du&nbsp;Chen, Yi&nbsp;Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, and Kun Han. 2024a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2401.12246" title="">Orion-14b: Open-source multilingual large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib12.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib12.9.1">arXiv preprint arXiv:2401.12246</em><span class="ltx_text ltx_font_bold" id="bib.bib12.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib13.5.5.1">Chen et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib13.7.1">
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2306.15595" title="">Extending context window of large language models via positional interpolation</a><span class="ltx_text ltx_font_bold" id="bib.bib13.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib13.9.1">arXiv preprint arXiv:2306.15595</em><span class="ltx_text ltx_font_bold" id="bib.bib13.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib14.5.5.1">Chen et&nbsp;al. (2016)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib14.7.1">
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/1604.06174" title="">Training deep nets with sublinear memory cost</a><span class="ltx_text ltx_font_bold" id="bib.bib14.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib14.9.1">arXiv preprint arXiv:1604.06174</em><span class="ltx_text ltx_font_bold" id="bib.bib14.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib15.5.5.1">Chen et&nbsp;al. (2024b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib15.7.1">
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2024b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=6PmJoRfdaK" title="">LongLoRA: Efficient fine-tuning of long-context large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib15.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib15.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib15.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib15.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib16.5.5.1">Cui et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib16.7.1">
Yiming Cui, Ziqing Yang, and Xin Yao. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2304.08177" title="">Efficient and effective text encoding for chinese llama and alpaca</a><span class="ltx_text ltx_font_bold" id="bib.bib16.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib16.9.1">arXiv preprint arXiv:2304.08177</em><span class="ltx_text ltx_font_bold" id="bib.bib16.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib17.5.5.1">Dai et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib17.7.1">
Damai Dai, Chengqi Deng, Chenggang Zhao, RX&nbsp;Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y&nbsp;Wu, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2401.06066" title="">DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models</a><span class="ltx_text ltx_font_bold" id="bib.bib17.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib17.9.1">arXiv preprint arXiv:2401.06066</em><span class="ltx_text ltx_font_bold" id="bib.bib17.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib18.5.5.1">Dao et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib18.7.1">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html" title="">Flashattention: Fast and memory-efficient exact attention with io-awareness</a><span class="ltx_text ltx_font_bold" id="bib.bib18.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib18.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib18.10.2">, 35:16344–16359.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib19.4.4.1">Dettmers (2021)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib19.6.1">
Tim Dettmers. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/TimDettmers/bitsandbytes" title="">Bitsandbytes</a><span class="ltx_text ltx_font_bold" id="bib.bib19.7.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib20.5.5.1">Dettmers et&nbsp;al. (2022a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib20.7.1">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/c3ba4962c05c49636d4c6206a97e9c8a-Abstract-Conference.html" title="">GPT3.int8(): 8-bit matrix multiplication for transformers at scale</a><span class="ltx_text ltx_font_bold" id="bib.bib20.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib20.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib20.10.2">, 35:30318–30332.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib21.5.5.1">Dettmers et&nbsp;al. (2022b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib21.7.1">
</span><span class="ltx_text ltx_font_bold" id="bib.bib21.8.2">Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2022b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=shpkpVXzo3h" title="">8-bit optimizers via block-wise quantization</a><span class="ltx_text ltx_font_bold" id="bib.bib21.9.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib21.10.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib21.11.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib21.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib22.5.5.1">Dettmers et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib22.7.1">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html" title="">QLoRA: Efficient finetuning of quantized llms</a><span class="ltx_text ltx_font_bold" id="bib.bib22.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib22.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib22.10.2">, 36:10088–10115.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib23.5.5.1">Diao et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib23.7.1">
Shizhe Diao, Rui Pan, Hanze Dong, Ka&nbsp;Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2306.12420" title="">LMFlow: An extensible toolkit for finetuning and inference of large foundation models</a><span class="ltx_text ltx_font_bold" id="bib.bib23.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib23.9.1">arXiv preprint arXiv:2306.12420</em><span class="ltx_text ltx_font_bold" id="bib.bib23.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib24.5.5.1">Du et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib24.7.1">
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2022.acl-long.26" title="">GLM: General language model pretraining with autoregressive blank infilling</a><span class="ltx_text ltx_font_bold" id="bib.bib24.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib24.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib24.10.2">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em><span class="ltx_text ltx_font_bold" id="bib.bib24.11.3">, pages 320–335, Dublin, Ireland. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib25.5.5.1">Egiazarian et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib25.7.1">
Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2401.06118" title="">Extreme compression of large language models via additive quantization</a><span class="ltx_text ltx_font_bold" id="bib.bib25.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib25.9.1">arXiv preprint arXiv:2401.06118</em><span class="ltx_text ltx_font_bold" id="bib.bib25.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib26.5.5.1">Frantar et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib26.7.1">
</span><span class="ltx_text ltx_font_bold" id="bib.bib26.8.2">Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=tcbBPnfwxS" title="">GPTQ: Accurate post-training quantization for generative pre-trained transformers</a><span class="ltx_text ltx_font_bold" id="bib.bib26.9.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib26.10.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib26.11.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib26.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib27.5.5.1">Groeneveld et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib27.7.1">
Dirk Groeneveld, Iz&nbsp;Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya&nbsp;Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.00838" title="">OLMo: Accelerating the science of language models</a><span class="ltx_text ltx_font_bold" id="bib.bib27.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib27.9.1">arXiv preprint arXiv:2402.00838</em><span class="ltx_text ltx_font_bold" id="bib.bib27.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib28.5.5.1">Guo et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib28.7.1">
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y&nbsp;Wu, YK&nbsp;Li, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2401.14196" title="">DeepSeek-Coder: When the large language model meets programming – the rise of code intelligence</a><span class="ltx_text ltx_font_bold" id="bib.bib28.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib28.9.1">arXiv preprint arXiv:2401.14196</em><span class="ltx_text ltx_font_bold" id="bib.bib28.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib29.4.4.1">Han and Han (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib29.6.1">
Daniel Han and Michael Han. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/unslothai/unsloth" title="">unsloth</a><span class="ltx_text ltx_font_bold" id="bib.bib29.7.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib30.5.5.1">Hayou et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib30.7.1">
Soufiane Hayou, Nikhil Ghosh, and Bin Yu. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.12354" title="">LoRA+: Efficient low rank adaptation of large models</a><span class="ltx_text ltx_font_bold" id="bib.bib30.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib30.9.1">arXiv preprint arXiv:2402.12354</em><span class="ltx_text ltx_font_bold" id="bib.bib30.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib31.5.5.1">Hendrycks et&nbsp;al. (2021)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib31.7.1">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=d7KBjmI3GmQ" title="">Measuring massive multitask language understanding</a><span class="ltx_text ltx_font_bold" id="bib.bib31.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib31.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib31.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib31.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib32.5.5.1">Houlsby et&nbsp;al. (2019)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib32.7.1">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De&nbsp;Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.mlr.press/v97/houlsby19a.html" title="">Parameter-efficient transfer learning for nlp</a><span class="ltx_text ltx_font_bold" id="bib.bib32.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib32.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib32.10.2">International conference on machine learning</em><span class="ltx_text ltx_font_bold" id="bib.bib32.11.3">, pages 2790–2799. PMLR.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib33.5.5.1">Hu et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib33.7.1">
Edward&nbsp;J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu&nbsp;Wang, Weizhu Chen, et&nbsp;al. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">LoRA: Low-rank adaptation of large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib33.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib33.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib33.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib33.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib34.5.5.1">Huang et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib34.7.1">
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/c6ec1844bec96d6d32ae95ae694e23d8-Abstract-Datasets_and_Benchmarks.html" title="">C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models</a><span class="ltx_text ltx_font_bold" id="bib.bib34.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib34.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib34.10.2">, 36.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib35.5.5.1">Jiang et&nbsp;al. (2023a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib35.7.1">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al. 2023a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2310.06825" title="">Mistral 7b</a><span class="ltx_text ltx_font_bold" id="bib.bib35.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib35.9.1">arXiv preprint arXiv:2310.06825</em><span class="ltx_text ltx_font_bold" id="bib.bib35.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib36.5.5.1">Jiang et&nbsp;al. (2023b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib36.7.1">
Jinhao Jiang, Kun Zhou, Wayne&nbsp;Xin Zhao, Yaliang Li, and Ji-Rong Wen. 2023b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2023.emnlp-main.228" title="">ReasoningLM: Enabling structural subgraph reasoning in pre-trained language models for question answering over knowledge graph</a><span class="ltx_text ltx_font_bold" id="bib.bib36.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib36.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib36.10.2">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text ltx_font_bold" id="bib.bib36.11.3">, pages 3721–3735, Singapore. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib37.5.5.1">Jiao et&nbsp;al. (2023a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib37.7.1">
Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2023.findings-emnlp.1001" title="">ParroT: Translating during chat using large language models tuned with human translation and feedback</a><span class="ltx_text ltx_font_bold" id="bib.bib37.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib37.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib37.10.2">Findings of the Association for Computational Linguistics: EMNLP 2023</em><span class="ltx_text ltx_font_bold" id="bib.bib37.11.3">, pages 15009–15020, Singapore. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib38.5.5.1">Jiao et&nbsp;al. (2023b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib38.7.1">
Yizhu Jiao, Ming Zhong, Sha Li, Ruining Zhao, Siru Ouyang, Heng Ji, and Jiawei Han. 2023b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2023.emnlp-main.620" title="">Instruct and extract: Instruction tuning for on-demand information extraction</a><span class="ltx_text ltx_font_bold" id="bib.bib38.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib38.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib38.10.2">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text ltx_font_bold" id="bib.bib38.11.3">, pages 10030–10051, Singapore. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib39.4.4.1">Kalajdzievski (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib39.6.1">
Damjan Kalajdzievski. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2312.03732" title="">A rank stabilization scaling factor for fine-tuning with LoRA</a><span class="ltx_text ltx_font_bold" id="bib.bib39.7.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib39.8.1">arXiv preprint arXiv:2312.03732</em><span class="ltx_text ltx_font_bold" id="bib.bib39.9.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib40.5.5.1">Kim et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib40.7.1">
Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2312.15166" title="">SOLAR 10.7B: Scaling large language models with simple yet effective depth up-scaling</a><span class="ltx_text ltx_font_bold" id="bib.bib40.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib40.9.1">arXiv preprint arXiv:2312.15166</em><span class="ltx_text ltx_font_bold" id="bib.bib40.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib41.5.5.1">Krell et&nbsp;al. (2021)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib41.7.1">
Mario&nbsp;Michael Krell, Matej Kosec, Sergio&nbsp;P Perez, and Andrew Fitzgibbon. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2107.02027" title="">Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance</a><span class="ltx_text ltx_font_bold" id="bib.bib41.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib41.9.1">arXiv preprint arXiv:2107.02027</em><span class="ltx_text ltx_font_bold" id="bib.bib41.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib42.5.5.1">Kwon et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib42.7.1">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.1145/3600006.3613165" title="">Efficient memory management for large language model serving with PagedAttention</a><span class="ltx_text ltx_font_bold" id="bib.bib42.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib42.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib42.10.2">Proceedings of the 29th Symposium on Operating Systems Principles</em><span class="ltx_text ltx_font_bold" id="bib.bib42.11.3">, pages 611–626.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib43.5.5.1">Le&nbsp;Scao et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib43.7.1">
Teven Le&nbsp;Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni, François Yvon, Matthias Gallé, et&nbsp;al. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2211.05100" title="">BLOOM: A 176b-parameter open-access multilingual language model</a><span class="ltx_text ltx_font_bold" id="bib.bib43.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib43.9.1">arXiv preprint arXiv:2211.05100</em><span class="ltx_text ltx_font_bold" id="bib.bib43.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib44.5.5.1">Lhoest et&nbsp;al. (2021)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib44.7.1">
Quentin Lhoest, Albert&nbsp;Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et&nbsp;al. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2021.emnlp-demo.21" title="">Datasets: A community library for natural language processing</a><span class="ltx_text ltx_font_bold" id="bib.bib44.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib44.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib44.10.2">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em><span class="ltx_text ltx_font_bold" id="bib.bib44.11.3">, pages 175–184.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib45.5.5.1">Li et&nbsp;al. (2023a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib45.7.1">
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2306.09212" title="">CMMLU: Measuring massive multitask language understanding in chinese</a><span class="ltx_text ltx_font_bold" id="bib.bib45.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib45.9.1">arXiv preprint arXiv:2306.09212</em><span class="ltx_text ltx_font_bold" id="bib.bib45.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib46.5.5.1">Li et&nbsp;al. (2023b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib46.7.1">
Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen Huang, Yuliang Liu, Boxiang Wang, and Yang You. 2023b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.1145/3605573.3605613" title="">Colossal-AI: A unified deep learning system for large-scale parallel training</a><span class="ltx_text ltx_font_bold" id="bib.bib46.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib46.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib46.10.2">Proceedings of the 52nd International Conference on Parallel Processing</em><span class="ltx_text ltx_font_bold" id="bib.bib46.11.3">, pages 766–775.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib47.5.5.1">Li et&nbsp;al. (2023c)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib47.7.1">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del&nbsp;Giorno, Suriya Gunasekar, and Yin&nbsp;Tat Lee. 2023c.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2309.05463" title="">Textbooks are all you need ii: phi-1.5 technical report</a><span class="ltx_text ltx_font_bold" id="bib.bib47.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib47.9.1">arXiv preprint arXiv:2309.05463</em><span class="ltx_text ltx_font_bold" id="bib.bib47.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib48.4.4.1">Lin (2004)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib48.6.1">
Chin-Yew Lin. 2004.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://aclanthology.org/W04-1013/" title="">ROUGE: A package for automatic evaluation of summaries</a><span class="ltx_text ltx_font_bold" id="bib.bib48.7.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib48.8.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib48.9.2">Text Summarization Branches Out</em><span class="ltx_text ltx_font_bold" id="bib.bib48.10.3">, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib49.5.5.1">Lin et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib49.7.1">
Ji&nbsp;Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2306.00978" title="">AWQ: Activation-aware weight quantization for llm compression and acceleration</a><span class="ltx_text ltx_font_bold" id="bib.bib49.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib49.9.1">arXiv preprint arXiv:2306.00978</em><span class="ltx_text ltx_font_bold" id="bib.bib49.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib50.5.5.1">Liu et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib50.7.1">
Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang&nbsp;Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.09353" title="">DoRA: Weight-decomposed low-rank adaptation</a><span class="ltx_text ltx_font_bold" id="bib.bib50.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib50.9.1">arXiv preprint arXiv:2402.09353</em><span class="ltx_text ltx_font_bold" id="bib.bib50.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib51.5.5.1">Lozhkov et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib51.7.1">
Anton Lozhkov, Raymond Li, Loubna&nbsp;Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao&nbsp;Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.19173" title="">StarCoder 2 and The Stack v2: The next generation</a><span class="ltx_text ltx_font_bold" id="bib.bib51.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib51.9.1">arXiv preprint arXiv:2402.19173</em><span class="ltx_text ltx_font_bold" id="bib.bib51.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib52.5.5.1">Mangrulkar et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib52.7.1">
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/huggingface/peft" title="">PEFT: State-of-the-art parameter-efficient fine-tuning methods</a><span class="ltx_text ltx_font_bold" id="bib.bib52.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib53.5.5.1">Mesnard et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib53.7.1">
Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir&nbsp;Sanjay Kale, Juliette Love, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2403.08295" title="">Gemma: Open models based on gemini research and technology</a><span class="ltx_text ltx_font_bold" id="bib.bib53.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib53.9.1">arXiv preprint arXiv:2403.08295</em><span class="ltx_text ltx_font_bold" id="bib.bib53.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib54.5.5.1">Micikevicius et&nbsp;al. (2018)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib54.7.1">
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et&nbsp;al. 2018.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=r1gs9JgRZ" title="">Mixed precision training</a><span class="ltx_text ltx_font_bold" id="bib.bib54.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib54.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib54.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib54.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib55.5.5.1">Nallapati et&nbsp;al. (2016)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib55.7.1">
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/K16-1028" title="">Abstractive text summarization using sequence-to-sequence rnns and beyond</a><span class="ltx_text ltx_font_bold" id="bib.bib55.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib55.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib55.10.2">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</em><span class="ltx_text ltx_font_bold" id="bib.bib55.11.3">, pages 280–290.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib56.5.5.1">Narayan et&nbsp;al. (2018)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib56.7.1">
Shashi Narayan, Shay&nbsp;B. Cohen, and Mirella Lapata. 2018.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/D18-1206" title="">Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</a><span class="ltx_text ltx_font_bold" id="bib.bib56.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib56.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib56.10.2">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text ltx_font_bold" id="bib.bib56.11.3">, pages 1797–1807, Brussels, Belgium. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib57.5.5.1">Nguyen et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib57.7.1">
Duc&nbsp;Q. Nguyen, Sang&nbsp;T. Truong, Toan D.&nbsp;V. Nguyen, Dong&nbsp;D. Le, Nhi&nbsp;N. Truong, Tho Quan, and Sanmi Koyejo. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://huggingface.co/ura-hcmut/GemSUra-7B" title="">Crossing linguistic horizons: Finetuning and comprehensive evaluation of vietnamese large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib57.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib58.5.5.1">Ouyang et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib58.7.1">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et&nbsp;al. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html" title="">Training language models to follow instructions with human feedback</a><span class="ltx_text ltx_font_bold" id="bib.bib58.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib58.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib58.10.2">, 35:27730–27744.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib59.5.5.1">Papineni et&nbsp;al. (2002)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib59.7.1">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://aclanthology.org/P02-1040.pdf" title="">BLEU: a method for automatic evaluation of machine translation</a><span class="ltx_text ltx_font_bold" id="bib.bib59.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib59.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib59.10.2">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em><span class="ltx_text ltx_font_bold" id="bib.bib59.11.3">, pages 311–318.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib60.5.5.1">Paszke et&nbsp;al. (2019)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib60.7.1">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et&nbsp;al. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html" title="">PyTorch: An imperative style, high-performance deep learning library</a><span class="ltx_text ltx_font_bold" id="bib.bib60.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib60.9.1">Advances in neural information processing systems</em><span class="ltx_text ltx_font_bold" id="bib.bib60.10.2">, 32.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib61.5.5.1">Radford et&nbsp;al. (2018)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib61.7.1">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et&nbsp;al. 2018.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" title="">Improving language understanding by generative pre-training</a><span class="ltx_text ltx_font_bold" id="bib.bib61.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib61.9.1">OpenAI blog</em><span class="ltx_text ltx_font_bold" id="bib.bib61.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib62.5.5.1">Rafailov et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib62.7.1">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher&nbsp;D Manning, and Chelsea Finn. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=HPuSIXJaa9" title="">Direct preference optimization: Your language model is secretly a reward model</a><span class="ltx_text ltx_font_bold" id="bib.bib62.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib62.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib62.10.2">, 37.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib63.5.5.1">Rasley et&nbsp;al. (2020)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib63.7.1">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.1145/3394486.3406703" title="">DeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters</a><span class="ltx_text ltx_font_bold" id="bib.bib63.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib63.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib63.10.2">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em><span class="ltx_text ltx_font_bold" id="bib.bib63.11.3">, pages 3505–3506.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib64.5.5.1">Shao et&nbsp;al. (2019)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib64.7.1">
Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/D19-1321" title="">Long and diverse text generation with planning-based hierarchical variational model</a><span class="ltx_text ltx_font_bold" id="bib.bib64.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib64.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib64.10.2">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em><span class="ltx_text ltx_font_bold" id="bib.bib64.11.3">, pages 3257–3268, Hong Kong, China. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib65.5.5.1">Shao et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib65.7.1">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK&nbsp;Li, Y&nbsp;Wu, and Daya Guo. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.03300" title="">DeepSeekMath: Pushing the limits of mathematical reasoning in open language models</a><span class="ltx_text ltx_font_bold" id="bib.bib65.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib65.9.1">arXiv preprint arXiv:2402.03300</em><span class="ltx_text ltx_font_bold" id="bib.bib65.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib66.5.5.1">Taori et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib66.7.1">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/tatsu-lab/stanford_alpaca" title="">Stanford alpaca: An instruction-following llama model</a><span class="ltx_text ltx_font_bold" id="bib.bib66.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib67.4.4.1">Team (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib67.6.1">
InternLM Team. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf" title="">InternLM: A multilingual language model with progressively enhanced capabilities</a><span class="ltx_text ltx_font_bold" id="bib.bib67.7.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib68.5.5.1">Tillet et&nbsp;al. (2019)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib68.7.1">
Philippe Tillet, Hsiang-Tsung Kung, and David Cox. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.1145/3315508.3329973" title="">Triton: an intermediate language and compiler for tiled neural network computations</a><span class="ltx_text ltx_font_bold" id="bib.bib68.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib68.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib68.10.2">Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</em><span class="ltx_text ltx_font_bold" id="bib.bib68.11.3">, pages 10–19.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib69.5.5.1">Touvron et&nbsp;al. (2023a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib69.7.1">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2302.13971" title="">LLaMA: Open and efficient foundation language models</a><span class="ltx_text ltx_font_bold" id="bib.bib69.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib69.9.1">arXiv preprint arXiv:2302.13971</em><span class="ltx_text ltx_font_bold" id="bib.bib69.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib70.5.5.1">Touvron et&nbsp;al. (2023b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib70.7.1">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al. 2023b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a><span class="ltx_text ltx_font_bold" id="bib.bib70.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib70.9.1">arXiv preprint arXiv:2307.09288</em><span class="ltx_text ltx_font_bold" id="bib.bib70.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib71.5.5.1">Tunstall et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib71.7.1">
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2310.16944" title="">Zephyr: Direct distillation of LM alignment</a><span class="ltx_text ltx_font_bold" id="bib.bib71.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib71.9.1">arXiv preprint arXiv:2310.16944</em><span class="ltx_text ltx_font_bold" id="bib.bib71.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib72.5.5.1">von Werra et&nbsp;al. (2020)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib72.7.1">
</span><span class="ltx_text ltx_font_bold" id="bib.bib72.8.2">Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/huggingface/trl" title="">TRL: Transformer reinforcement learning</a><span class="ltx_text ltx_font_bold" id="bib.bib72.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib73.5.5.1">Wang et&nbsp;al. (2023a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib73.7.1">
Chenglong Wang, Hang Zhou, Yimin Hu, Yifu Huo, Bei Li, Tongran Liu, Tong Xiao, and Jingbo Zhu. 2023a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2308.02223" title="">Esrl: Efficient sampling-based reinforcement learning for sequence generation</a><span class="ltx_text ltx_font_bold" id="bib.bib73.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib73.9.1">arXiv preprint arXiv:2308.02223</em><span class="ltx_text ltx_font_bold" id="bib.bib73.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib74.5.5.1">Wang et&nbsp;al. (2023b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib74.7.1">
Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2309.11235" title="">OpenChat: Advancing open-source language models with mixed-quality data</a><span class="ltx_text ltx_font_bold" id="bib.bib74.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib74.9.1">arXiv preprint arXiv:2309.11235</em><span class="ltx_text ltx_font_bold" id="bib.bib74.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib75.5.5.1">Wang et&nbsp;al. (2023c)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib75.7.1">
Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023c.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2023.emnlp-main.1036" title="">Document-level machine translation with large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib75.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib75.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib75.10.2">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text ltx_font_bold" id="bib.bib75.11.3">, pages 16646–16661, Singapore. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib76.5.5.1">Wang et&nbsp;al. (2023d)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib76.7.1">
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah&nbsp;A Smith, Iz&nbsp;Beltagy, et&nbsp;al. 2023d.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=w4zZNC4ZaV" title="">How far can camels go? exploring the state of instruction tuning on open resources</a><span class="ltx_text ltx_font_bold" id="bib.bib76.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib76.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib76.10.2">, 36.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib77.5.5.1">Wei et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib77.7.1">
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams&nbsp;Wei Yu, Brian Lester, Nan Du, Andrew&nbsp;M Dai, and Quoc&nbsp;V Le. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=gEZrGCozdqR" title="">Finetuned language models are zero-shot learners</a><span class="ltx_text ltx_font_bold" id="bib.bib77.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib77.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib77.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib77.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib78.5.5.1">Wei et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib78.7.1">
Tianwen Wei, Liang Zhao, Lichang Zhang, Bo&nbsp;Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2310.19341" title="">Skywork: A more open bilingual foundation model</a><span class="ltx_text ltx_font_bold" id="bib.bib78.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib78.9.1">arXiv preprint arXiv:2310.19341</em><span class="ltx_text ltx_font_bold" id="bib.bib78.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib79.5.5.1">Wolf et&nbsp;al. (2020)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib79.7.1">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le&nbsp;Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2020.emnlp-demos.6" title="">Transformers: State-of-the-art natural language processing</a><span class="ltx_text ltx_font_bold" id="bib.bib79.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib79.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib79.10.2">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em><span class="ltx_text ltx_font_bold" id="bib.bib79.11.3">, pages 38–45, Online. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib80.5.5.1">Wu et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib80.7.1">
Shaohua Wu, Xudong Zhao, Shenling Wang, Jiangang Luo, Lingjun Li, Xi&nbsp;Chen, Bing Zhao, Wei Wang, Tong Yu, Rongguo Zhang, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2311.15786" title="">YUAN 2.0: A large language model with localized filtering-based attention</a><span class="ltx_text ltx_font_bold" id="bib.bib80.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib80.9.1">arXiv preprint arXiv:2311.15786</em><span class="ltx_text ltx_font_bold" id="bib.bib80.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib81.5.5.1">Yang et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib81.7.1">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce&nbsp;Bian, Chao Yin, Chenxu Lv, Da&nbsp;Pan, Dian Wang, Dong Yan, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2309.10305" title="">Baichuan 2: Open large-scale language models</a><span class="ltx_text ltx_font_bold" id="bib.bib81.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib81.9.1">arXiv preprint arXiv:2309.10305</em><span class="ltx_text ltx_font_bold" id="bib.bib81.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib82.5.5.1">Yao et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib82.7.1">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik&nbsp;R Narasimhan, and Yuan Cao. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=WE_vluYUL-X" title="">ReAct: Synergizing reasoning and acting in language models</a><span class="ltx_text ltx_font_bold" id="bib.bib82.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib82.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib82.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib82.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib83.5.5.1">Young et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib83.7.1">
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge&nbsp;Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2403.04652" title="">Yi: Open foundation models by 01.ai</a><span class="ltx_text ltx_font_bold" id="bib.bib83.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib83.9.1">arXiv preprint arXiv:2403.04652</em><span class="ltx_text ltx_font_bold" id="bib.bib83.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib84.5.5.1">Yu et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib84.7.1">
Hao Yu, Zachary Yang, Kellin Pelrine, Jean&nbsp;Francois Godbout, and Reihaneh Rabbany. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2308.10092" title="">Open, closed, or small language models for text classification?</a><span class="ltx_text ltx_font_bold" id="bib.bib84.8.1">
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib84.9.1">arXiv preprint arXiv:2308.10092</em><span class="ltx_text ltx_font_bold" id="bib.bib84.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib85.5.5.1">Zhang et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib85.7.1">
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu&nbsp;Qiao. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=d4UiXAHN2W" title="">LLaMA-Adapter: Efficient fine-tuning of language models with zero-init attention</a><span class="ltx_text ltx_font_bold" id="bib.bib85.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib85.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib85.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib85.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib86.5.5.1">Zhao et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib86.7.1">
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2403.03507" title="">GaLore: Memory-efficient llm training by gradient low-rank projection</a><span class="ltx_text ltx_font_bold" id="bib.bib86.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib86.9.1">arXiv preprint arXiv:2403.03507</em><span class="ltx_text ltx_font_bold" id="bib.bib86.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib87.5.5.1">Zhao et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib87.7.1">
Wayne&nbsp;Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2303.18223" title="">A survey of large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib87.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib87.9.1">arXiv preprint arXiv:2303.18223</em><span class="ltx_text ltx_font_bold" id="bib.bib87.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib88.5.5.1">Zheng et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib88.7.1">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi&nbsp;Lin, Zhuohan Li, Dacheng Li, Eric Xing, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html" title="">Judging LLM-as-a-judge with mt-bench and chatbot arena</a><span class="ltx_text ltx_font_bold" id="bib.bib88.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib88.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib88.10.2">, 36.
</span>
</span>
</li>
</ul>
</section>
<figure class="ltx_table" id="A0.T5">
<figcaption class="ltx_caption ltx_centering ltx_font_bold"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="A0.T5.5.1.1">표 5</span>:</span> <span class="ltx_text ltx_font_medium ltx_font_smallcaps" id="A0.T5.6.2">LlamaFactory</span> with existing LLM fine-tuning frameworks.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A0.T5.7" style="width:429.3pt;height:308.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(64.2pt,-46.1pt) scale(1.4269209217438,1.4269209217438) ;">
<table class="ltx_tabular ltx_align_middle" id="A0.T5.7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A0.T5.7.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.1.1.1.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.2"><span class="ltx_text ltx_font_smallcaps" id="A0.T5.7.1.1.1.2.1">LlamaFactory</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.1.1.3.1">FastChat</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.1.1.4.1">LitGPT</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.1.1.5.1">LMFlow</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.6"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.1.1.6.1">Open Instruct</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.2.2.1.1">LoRA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.2"><span class="ltx_text" id="A0.T5.7.1.2.2.2.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.3"><span class="ltx_text" id="A0.T5.7.1.2.2.3.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.4"><span class="ltx_text" id="A0.T5.7.1.2.2.4.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.5"><span class="ltx_text" id="A0.T5.7.1.2.2.5.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.6"><span class="ltx_text" id="A0.T5.7.1.2.2.6.1">✓</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.3.3">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.3.3.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.3.3.1.1">QLoRA</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.3.3.2"><span class="ltx_text" id="A0.T5.7.1.3.3.2.1">✓</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.3.3.3"><span class="ltx_text" id="A0.T5.7.1.3.3.3.1">✓</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.3.3.4"><span class="ltx_text" id="A0.T5.7.1.3.3.4.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.3.3.5"></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.3.3.6"><span class="ltx_text" id="A0.T5.7.1.3.3.6.1">✓</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.4.4">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.4.4.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.4.4.1.1">LoRA+</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.4.4.2"><span class="ltx_text" id="A0.T5.7.1.4.4.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.4.4.3"></td>
<td class="ltx_td" id="A0.T5.7.1.4.4.4"></td>
<td class="ltx_td" id="A0.T5.7.1.4.4.5"></td>
<td class="ltx_td" id="A0.T5.7.1.4.4.6"></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.5.5">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.5.5.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.5.5.1.1">DoRA</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.5.5.2"><span class="ltx_text" id="A0.T5.7.1.5.5.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.5.5.3"></td>
<td class="ltx_td" id="A0.T5.7.1.5.5.4"></td>
<td class="ltx_td" id="A0.T5.7.1.5.5.5"></td>
<td class="ltx_td" id="A0.T5.7.1.5.5.6"></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.6.6">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.6.6.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.6.6.1.1">GaLore</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.6.6.2"><span class="ltx_text" id="A0.T5.7.1.6.6.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.6.6.3"></td>
<td class="ltx_td" id="A0.T5.7.1.6.6.4"></td>
<td class="ltx_td" id="A0.T5.7.1.6.6.5"></td>
<td class="ltx_td" id="A0.T5.7.1.6.6.6"></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.7.7.1.1">SFT</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.2"><span class="ltx_text" id="A0.T5.7.1.7.7.2.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.3"><span class="ltx_text" id="A0.T5.7.1.7.7.3.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.4"><span class="ltx_text" id="A0.T5.7.1.7.7.4.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.5"><span class="ltx_text" id="A0.T5.7.1.7.7.5.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.6"><span class="ltx_text" id="A0.T5.7.1.7.7.6.1">✓</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.8.8">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.8.8.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.8.8.1.1">RLHF</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.8.8.2"><span class="ltx_text" id="A0.T5.7.1.8.8.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.8.8.3"></td>
<td class="ltx_td" id="A0.T5.7.1.8.8.4"></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.8.8.5"><span class="ltx_text" id="A0.T5.7.1.8.8.5.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.8.8.6"></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.9.9">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.9.9.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.9.9.1.1">DPO</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.9.9.2"><span class="ltx_text" id="A0.T5.7.1.9.9.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.9.9.3"></td>
<td class="ltx_td" id="A0.T5.7.1.9.9.4"></td>
<td class="ltx_td" id="A0.T5.7.1.9.9.5"></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.9.9.6"><span class="ltx_text" id="A0.T5.7.1.9.9.6.1">✓</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.10.10.1.1">Flash attention</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.2"><span class="ltx_text" id="A0.T5.7.1.10.10.2.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.3"><span class="ltx_text" id="A0.T5.7.1.10.10.3.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.4"><span class="ltx_text" id="A0.T5.7.1.10.10.4.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.5"><span class="ltx_text" id="A0.T5.7.1.10.10.5.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.6"><span class="ltx_text" id="A0.T5.7.1.10.10.6.1">✓</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.11.11">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.11.11.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.11.11.1.1">Unsloth</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.11.11.2"><span class="ltx_text" id="A0.T5.7.1.11.11.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.11.11.3"></td>
<td class="ltx_td" id="A0.T5.7.1.11.11.4"></td>
<td class="ltx_td" id="A0.T5.7.1.11.11.5"></td>
<td class="ltx_td" id="A0.T5.7.1.11.11.6"></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.12.12.1.1">DeepSpeed</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.2"><span class="ltx_text" id="A0.T5.7.1.12.12.2.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.3"><span class="ltx_text" id="A0.T5.7.1.12.12.3.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.4"><span class="ltx_text" id="A0.T5.7.1.12.12.4.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.5"><span class="ltx_text" id="A0.T5.7.1.12.12.5.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.6"><span class="ltx_text" id="A0.T5.7.1.12.12.6.1">✓</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_font_bold ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_medium" id="A1.1.1.1">Appendix A</span> </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.p1.1.1">In this section, we enumerate existing framework for fine-tuning LLMs, especially highlight those for efficient fine-tuning </span></p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.p2.1.1">LLaMA-Adapter</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.2.1">(</span><span class="ltx_text ltx_font_bold">Zhang et al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib85" title=""><span class="ltx_text ltx_font_bold">2022</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.4.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.5"> efficiently fine-tunes the Llama model</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.6.1">(</span><span class="ltx_text ltx_font_bold">Touvron et al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.7.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib69" title=""><span class="ltx_text ltx_font_bold">2023a</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.8.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.9"> for instruction-following abilities using zero-init attention. FastChat</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.10.1">(</span><span class="ltx_text ltx_font_bold">Zheng et al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.11.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib88" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.12.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.13">은 채팅 완료 목적을 위한 LLMs를 훈련하고 평가하는 데 초점을 맞춘 프레임워크이다. LitGPT</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.14.1">(</span><span class="ltx_text ltx_font_bold">AI</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.15.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib2" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.16.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.17">은 생성 모델의 구현을 제공하며 다양한 훈련 방법을 지원한다. Open-Instruct </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.18.1">(</span><span class="ltx_text ltx_font_bold">Wang et al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.19.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib76" title=""><span class="ltx_text ltx_font_bold">2023d</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.20.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.21">은 효율적인 미세 조정을 지원하는 미세 조정 프레임워크이다. Colossal AI</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.22.1">(</span><span class="ltx_text ltx_font_bold">Li et al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.23.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib46" title=""><span class="ltx_text ltx_font_bold">2023b</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.24.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.25">는 분산 훈련을 위한 고급 병렬 처리 전략을 취한다. LMFlow</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.26.1">(</span><span class="ltx_text ltx_font_bold">Diao et al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.27.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib23" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.28.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.29">는 풀 튜닝과 어댑터 튜닝을 모두 지원하는 디코더 전용 모델을 지원하는 확장 가능하고 효율적인 미세 튜닝 프레임워크입니다. GPT4All</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.30.1">(</span><span class="ltx_text ltx_font_bold">Anand et al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.31.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib4" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.32.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.33">은 LLMs가 소비자 디바이스에서 실행될 수 있도록 하는 동시에 미세 조정 기능을 제공합니다. </span></p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1"><span class="ltx_text ltx_font_bold" id="A1.p3.1.1">위에서 제시된 프레임워크와 대조적으로, </span><span class="ltx_text ltx_font_smallcaps" id="A1.p3.1.2">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="A1.p3.1.3">은 더 넓은 범위의 효율적인 미세 조정 방법을 지원합니다. 우리는 Table </span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#A0.T5" title="Table 5 ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text ltx_font_bold" id="A1.p3.1.4">의 기존 작업과 프레임워크를 비교한다. </span></p>
</div>
<figure class="ltx_table" id="A1.T6">
<figcaption class="ltx_caption ltx_centering ltx_font_bold"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="A1.T6.3.1.1">Table 6</span>:</span>List of supported models.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T6.4" style="width:420.6pt;height:567.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.6pt,22.4pt) scale(0.92690966982746,0.92690966982746) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T6.4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T6.4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T6.4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T6.4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.1.1.2.1">Variant</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T6.4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.1.1.3.1">Organization</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.4.1.2.1.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.1.1">Llama </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.1.2.1">(</span><span class="ltx_text ltx_font_bold">Touvron et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib69" title=""><span class="ltx_text ltx_font_bold">2023a</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.4.1.2.1.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.2.1">7B/13B/33B/65B</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.4.1.2.1.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.3.1">Meta AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.3.2">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.3.2.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.1.1">Llama 2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.1.2.1">(</span><span class="ltx_text ltx_font_bold">Touvron et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib70" title=""><span class="ltx_text ltx_font_bold">2023b</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.3.2.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.2.1">7B/13B/70B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.3.2.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.3.1">Meta AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.4.3">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.4.3.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.4.3.1.1">Baichuan </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_bold">Yang et&nbsp;al.</span> <span class="ltx_text ltx_font_bold" id="A1.T6.4.1.4.3.1.2.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib81" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.4.3.1.3.2.2.1">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.4.3.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.4.3.2.1">7B/13B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.4.3.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.4.3.3.1">Baichuan Inc</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.5.4">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.5.4.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.1.1">Baichuan2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.1.2.1">(</span><span class="ltx_text ltx_font_bold">Yang et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib81" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.5.4.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.2.1">7B/13B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.5.4.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.3.1">Baichuan Inc</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.6.5">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.6.5.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.1.1">BLOOM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.1.2.1">(</span><span class="ltx_text ltx_font_bold">Le&nbsp;Scao et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib43" title=""><span class="ltx_text ltx_font_bold">2022</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.6.5.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.2.1">560M/3B/7.1B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.6.5.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.3.1">BigScience</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.7.6">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.7.6.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.1.1">BLOOMZ </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.1.2.1">(</span><span class="ltx_text ltx_font_bold">Le&nbsp;Scao et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib43" title=""><span class="ltx_text ltx_font_bold">2022</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.7.6.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.2.1">560M/3B/7.1B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.7.6.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.3.1">BigScience</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.8.7">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.8.7.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.1.1">ChatGLM2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.1.2.1">(</span><span class="ltx_text ltx_font_bold">Du et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib24" title=""><span class="ltx_text ltx_font_bold">2022</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.8.7.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.2.1">6B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.8.7.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.3.1">THUDM</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.9.8">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.9.8.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.1.1">ChatGLM3 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.1.2.1">(</span><span class="ltx_text ltx_font_bold">Du et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib24" title=""><span class="ltx_text ltx_font_bold">2022</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.9.8.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.2.1">6B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.9.8.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.3.1">THUDM</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.10.9">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.10.9.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.1.1">ChineseLLaMA2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.1.2.1">(</span><span class="ltx_text ltx_font_bold">Cui et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib16" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.10.9.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.2.1">3B/7B/13B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.10.9.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.3.1">HFL</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.11.10">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.11.10.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.1.1">DeepSeek-LLM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.1.2.1">(</span><span class="ltx_text ltx_font_bold">Bi et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib10" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.11.10.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.2.1">7B/67B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.11.10.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.3.1">DeepSeek</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.12.11">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.12.11.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.1.1">DeepSeek-Math </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.1.2.1">(</span><span class="ltx_text ltx_font_bold">Shao et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib65" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.12.11.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.2.1">7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.12.11.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.3.1">DeepSeek</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.13.12">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.13.12.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.1.1">DeepSeek-MoE </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.1.2.1">(</span><span class="ltx_text ltx_font_bold">Dai et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib17" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.13.12.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.2.1">16B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.13.12.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.3.1">DeepSeek</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.14.13">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.14.13.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.1.1">DeepSeek-Coder </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.1.2.1">(</span><span class="ltx_text ltx_font_bold">Guo et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib28" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.14.13.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.2.1">6.7B/7B/33B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.14.13.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.3.1">DeepSeek</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.15.14">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.15.14.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.1.1">Falcon </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.1.2.1">(</span><span class="ltx_text ltx_font_bold">Almazrouei et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib3" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.15.14.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.2.1">7B/40B/180B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.15.14.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.3.1">TII</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.16.15">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.16.15.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.1.1">Gemma </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.1.2.1">(</span><span class="ltx_text ltx_font_bold">Mesnard et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib53" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.16.15.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.2.1">2B/7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.16.15.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.3.1">Google</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.17.16">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.17.16.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.1.1">InternLM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.1.2.1">(</span><span class="ltx_text ltx_font_bold">Team</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib67" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.17.16.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.2.1">7B/20B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.17.16.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.3.1">Shanghai AI Lab</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.18.17">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.18.17.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.1.1">InternLM2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.1.2.1">(</span><span class="ltx_text ltx_font_bold">Team</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib67" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.18.17.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.2.1">1.8B/7B/20B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.18.17.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.3.1">Shanghai AI Lab</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.19.18">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.19.18.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.1.1">Mistral </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.1.2.1">(</span><span class="ltx_text ltx_font_bold">Jiang et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib35" title=""><span class="ltx_text ltx_font_bold">2023a</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.19.18.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.2.1">7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.19.18.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.3.1">Mistral AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.20.19">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.20.19.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.1.1">Mixtral </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.1.2.1">(</span><span class="ltx_text ltx_font_bold">Jiang et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib35" title=""><span class="ltx_text ltx_font_bold">2023a</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.20.19.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.2.1">8x7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.20.19.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.3.1">Mistral AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.21.20">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.21.20.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.1.1">OLMo </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.1.2.1">(</span><span class="ltx_text ltx_font_bold">Groeneveld et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib27" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.21.20.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.2.1">1B/7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.21.20.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.3.1">Allen AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.22.21">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.22.21.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.1.1">OpenChat </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.1.2.1">(</span><span class="ltx_text ltx_font_bold">Wang et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib74" title=""><span class="ltx_text ltx_font_bold">2023b</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.22.21.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.2.1">7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.22.21.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.3.1">OpenChat</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.23.22">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.23.22.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.1.1">Orion </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.1.2.1">(</span><span class="ltx_text ltx_font_bold">Chen et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib12" title=""><span class="ltx_text ltx_font_bold">2024a</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.23.22.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.2.1">14B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.23.22.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.3.1">OrionStar</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.24.23">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.24.23.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.1.1">Phi-1.5 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.1.2.1">(</span><span class="ltx_text ltx_font_bold">Li et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib47" title=""><span class="ltx_text ltx_font_bold">2023c</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.24.23.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.2.1">1.3B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.24.23.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.3.1">Microsoft</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.25.24">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.25.24.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.1.1">Phi-2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.1.2.1">(</span><span class="ltx_text ltx_font_bold">Li et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib47" title=""><span class="ltx_text ltx_font_bold">2023c</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.25.24.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.2.1">2.7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.25.24.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.3.1">Microsoft</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.26.25">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.26.25.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.1.1">Qwen </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.1.2.1">(</span><span class="ltx_text ltx_font_bold">Bai et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib6" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.26.25.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.2.1">1.8B/7B/14B/72B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.26.25.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.3.1">Alibaba Cloud</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.27.26">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.27.26.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.1.1">Qwen1.5 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.1.2.1">(</span><span class="ltx_text ltx_font_bold">Bai et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib6" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.27.26.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.2.1">1.8B/7B/14B/72B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.27.26.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.3.1">Alibaba Cloud</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.28.27">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.28.27.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.1.1">SOLAR </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.1.2.1">(</span><span class="ltx_text ltx_font_bold">Kim et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib40" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.28.27.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.2.1">10.7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.28.27.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.3.1">Upstage AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.29.28">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.29.28.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.1.1">Skywork </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.1.2.1">(</span><span class="ltx_text ltx_font_bold">Wei et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib78" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.29.28.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.2.1">13B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.29.28.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.3.1">Skywork</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.30.29">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.30.29.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.1.1">StarCoder2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.1.2.1">(</span><span class="ltx_text ltx_font_bold">Lozhkov et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib51" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.30.29.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.2.1">3B/7B/15B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.30.29.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.3.1">BigCode</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.31.30">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.31.30.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.1.1">Vicuna1.5 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.1.2.1">(</span><span class="ltx_text ltx_font_bold">Zheng et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib88" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.31.30.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.2.1">7B/13B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.31.30.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.3.1">LMSYS</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.32.31">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.32.31.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.1.1">Yi </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.1.2.1">(</span><span class="ltx_text ltx_font_bold">Young et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib83" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.32.31.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.2.1">6B/9B/34B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.32.31.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.3.1">01.AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.33.32">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.33.32.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.1.1">Yuan2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.1.2.1">(</span><span class="ltx_text ltx_font_bold">Wu et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib80" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.33.32.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.2.1">2B/51B/102B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.33.32.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.3.1">IEIT</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.34.33">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.4.1.34.33.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.1.1">Zephyr </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.1.2.1">(</span><span class="ltx_text ltx_font_bold">Tunstall et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib71" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.4.1.34.33.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.2.1">7B</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.4.1.34.33.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.3.1">Hugging Face H4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_font_bold ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_medium" id="A2.1.1.1">Appendix B</span> </span>Supported Models in <span class="ltx_text ltx_font_medium ltx_font_smallcaps" id="A2.2.2">LlamaFactory</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.p1.1.1">우리는 </span><span class="ltx_text ltx_font_smallcaps" id="A2.p1.1.2">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="A2.p1.1.3">테이블에 나열합니다. </span></p>
</div>
<figure class="ltx_table" id="A2.T7">
<figcaption class="ltx_caption ltx_centering ltx_font_bold"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="A2.T7.5.1.1">표 7</span>:</span>Prompt of function calling in <span class="ltx_text ltx_font_medium ltx_font_smallcaps" id="A2.T7.6.2">LlamaFactory</span>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T7.7" style="width:429.3pt;height:186.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.8pt,1.2pt) scale(0.987304193006671,0.987304193006671) ;">
<table class="ltx_tabular ltx_align_middle" id="A2.T7.7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T7.7.1.1.1">
<td class="ltx_td ltx_align_justify ltx_border_tt" id="A2.T7.7.1.1.1.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.1.1.1.1.1">You have access to the following tools:</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.2.2">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.2.2.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.2.2.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.2.2.1.1.1">&gt; Tool Name: tool1</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.3.3">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.3.3.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.3.3.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.3.3.1.1.1">Tool Description: the usage of tool1</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.4.4">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.4.4.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.4.4.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.4.4.1.1.1">Tool Args:</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.5.5">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.5.5.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.5.5.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.5.5.1.1.1">- arg1 (dtype): the usage of arg1</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.6.6">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.6.6.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.6.6.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.6.6.1.1.1">Use the following format if using a tool:</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.7.7">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.7.7.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.7.7.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.7.7.1.1.1">```</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.8.8">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.8.8.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.8.8.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.8.8.1.1.1">Action: tool name (one of [tool1]).</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.9.9">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.9.9.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.9.9.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.9.9.1.1.1">Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. ```{"input": "hello world", "num_beams": 5}```).</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.10.10">
<td class="ltx_td ltx_align_justify ltx_border_bb" id="A2.T7.7.1.10.10.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.10.10.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.10.10.1.1.1">```</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_font_bold ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_medium" id="A3.1.1.1">Appendix C</span> </span>Details of Designing Chat Template</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1"><span class="ltx_text ltx_font_bold" id="A3.p1.1.1">Transformers 4.34.0이 채팅 템플릿의 기능을 도입하지만, 여러 모델(예: ChatGLM 및 Qwen)은 특정 동작을 나타내고 특수 토큰을 직접 인코딩할 수 없는 토큰라이저를 가지고 있습니다. 따라서 텍스트 입력을 삽입 ID로 강력하게 변환하기 위해 </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.2">Formatter</span><span class="ltx_text ltx_font_bold" id="A3.p1.1.3"> 클래스를 설계한다. 구체적으로, </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.4">EmptyFormatter</span><span class="ltx_text ltx_font_bold" id="A3.p1.1.5">, </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.6">StringFormatter</span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.8">FunctionFormatter</span><span class="ltx_text ltx_font_bold" id="A3.p1.1.9"> 및 </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.10">ToolFormatter</span><span class="ltx_text ltx_font 또한, </span><span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.12">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="A3.p1.1.13">fine-tuning models to obtain function calling capabilities. ReAct 프롬프트 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_bold">Yao et al.</span> <span class="ltx_text ltx_font_bold" id="A3.p1.1.14.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib82" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A3.p1.1.15.2.2.1">)</span></cite><span class="ltx_text ltx_font_bold" id="A3.p1.1.16">은 도구를 사용하는 일반적인 선택이지만 중첩된 도구 매개 변수에 대해서는 충분하지 않습니다. 도구 프롬프트를 최적화하여 Table </span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#A2.T7" title="Table 7 ‣ Appendix B Supported Models in LlamaFactory ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">7</span></a><span class="ltx_text ltx_font_bold" id="A3.p1.1.17">에 제공하였다. </span></p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_font_bold ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_medium" id="A4.1.1.1">Appendix D</span> </span>Experiment Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_medium" id="A4.SS1.1.1.1">D.1</span> </span>Training Efficiency</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p" id="A4.SS1.p1.7"><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.1">Learning rate of </span><math alttext="10^{-5}" class="ltx_Math" display="inline" id="A4.SS1.p1.1.m1.1"><semantics id="A4.SS1.p1.1.m1.1a"><msup id="A4.SS1.p1.1.m1.1.1" xref="A4.SS1.p1.1.m1.1.1.cmml"><mn id="A4.SS1.p1.1.m1.1.1.2" xref="A4.SS1.p1.1.m1.1.1.2.cmml">10</mn><mrow id="A4.SS1.p1.1.m1.1.1.3" xref="A4.SS1.p1.1.m1.1.1.3.cmml"><mo id="A4.SS1.p1.1.m1.1.1.3a" xref="A4.SS1.p1.1.m1.1.1.3.cmml">−</mo><mn id="A4.SS1.p1.1.m1.1.1.3.2" xref="A4.SS1.p1.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.1.m1.1b"><apply id="A4.SS1.p1.1.m1.1.1.cmml" xref="A4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A4.SS1.p1.1.m1.1.1.1.cmml" xref="A4.SS1.p1.1.m1.1.1">superscript</csymbol><cn id="A4.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="A4.SS1.p1.1.m1.1.1.2">10</cn><apply id="A4.SS1.p1.1.m1.1.1.3.cmml" xref="A4.SS1.p1.1.m1.1.1.3"><minus id="A4.SS1.p1.1.m1.1.1.3.1.cmml" xref="A4.SS1.p1.1.m1.1.1.3"></minus><cn id="A4.SS1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A4.SS1.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.1.m1.1c">10^{-5}</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.1.m1.1d">10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.2">, token batch size of </span><math alttext="512" class="ltx_Math" display="inline" id="A4.SS1.p1.2.m2.1"><semantics id="A4.SS1.p1.2.m2.1a"><mn id="A4.SS1.p1.2.m2.1.1" xref="A4.SS1.p1.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.2.m2.1b"><cn id="A4.SS1.p1.2.m2.1.1.cmml" type="integer" xref="A4.SS1.p1.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.2.m2.1c">512</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.2.m2.1d">512</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.3"> to fine-tune these models using the 8-bit AdamW optimizer</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.4.1">(</span><span class="ltx_text ltx_font_bold">Dettmers et al.</span><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.5.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib21" title=""><span class="ltx_text ltx_font_bold">2022b</span></a><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.6.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.7"> in bfloat16 precision with activation checkpointing to reduce memory footprint. 동결 튜닝에서는 마지막 </span><math alttext="3" class="ltx_Math" display="inline" id="A4.SS1.p1.3.m3.1"><semantics id="A4.SS1.p1.3.m3.1a"><mn id="A4.SS1.p1.3.m3.1.1" xref="A4.SS1.p1.3.m3.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.3.m3.1b"><cn id="A4.SS1.p1.3.m3.1.1.cmml" type="integer" xref="A4.SS1.p1.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.3.m3.1c">3</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.3.m3.1d">3</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.8">모델의 디코더 계층만 미세 조정합니다. GaLore의 경우, 랭크 및 스케일을 각각 </span><math alttext="128" class="ltx_Math" display="inline" id="A4.SS1.p1.4.m4.1"><semantics id="A4.SS1.p1.4.m4.1a"><mn id="A4.SS1.p1.4.m4.1.1" xref="A4.SS1.p1.4.m4.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.4.m4.1b"><cn id="A4.SS1.p1.4.m4.1.1.cmml" type="integer" xref="A4.SS1.p1.4.m4.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.4.m4.1c">128</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.4.m4.1d">128</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.9"> 및 </span><math alttext="2.0" class="ltx_Math" display="inline" id="A4.SS1.p1.5.m5.1"><semantics id="A4.SS1.p1.5.m5.1a"><mn id="A4.SS1.p1.5.m5.1.1" xref="A4.SS1.p1.5.m5.1.1.cmml">2.0</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.5.m5.1b"><cn id="A4.SS1.p1.5.m5.1.1.cmml" type="float" xref="A4.SS1.p1.5.m5.1.1">2.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.5.m5.1c">2.0</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.5.m5.1d">2.0</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.10""으로 설정하였다. LoRA 및 QLoRA의 경우, 모든 선형 레이어에 어댑터를 부착하고 랭크 및 알파를 각각 </span><math alttext="128" class="ltx_Math" display="inline" id="A4.SS1.p1.6.m6.1"><semantics id="A4.SS1.p1.6.m6.1a"><mn id="A4.SS1.p1.6.m6.1.1" xref="A4.SS1.p1.6.m6.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.6.m6.1b"><cn id="A4.SS1.p1.6.m6.1.1.cmml" type="integer" xref="A4.SS1.p1.6.m6.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.6.m6.1c">128</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.6.m6.1d">128</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.11"> 및 </span><math alttext="256" class="ltx_Math" display="inline" id="A4.SS1.p1.7.m7.1"><semantics id="A4.SS1.p1.7.m7.1a"><mn id="A4.SS1.p1.7.m7.1.1" xref="A4.SS1.p1.7.m7.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.7.m7.1b"><cn id="A4.SS1.p1.7.m7.1.1.cmml" type="integer" xref="A4.SS1.p1.7.m7.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.7.m7.1c">256</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.7.m7.1d">256</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.12""로 설정한다. 모든 실험은 단일 NVIDIA A100 40GB GPU에서 수행된다. 우리는 LoRA 및 QLoRA 실험을 위한 모든 실험과 Unsloth에서 플래시 주의를 가능하게 한다. </span></p>
</div>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_medium" id="A4.SS2.1.1.1">D.2</span> </span>Fine-Tuning on Downstream Tasks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.SS2.p1">
<p class="ltx_p" id="A4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="A4.SS2.p1.1.1">서로 다른 미세 조정 방법의 유효성을 평가하는 데 사용되는 데이터 세트에 대해 간략하게 설명합니다. </span></p>
</div>
<div class="ltx_para" id="A4.SS2.p2">
<ul class="ltx_itemize" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.1">CNN/DM</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.2.1">(</span><span class="ltx_text ltx_font_bold">Nallapati et al.</span><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib55" title=""><span class="ltx_text ltx_font_bold">2016</span></a><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.5">이 데이터 세트는 요약 생성 작업에 사용됩니다. 영국 기사는 CNN과 데일리 메일에서 수집된다. 우리는 300k 이상의 샘플로 구성된 버전 1.0.0을 활용한다. </span></p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i2.p1">
<p class="ltx_p" id="A4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.1">XSum</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.2.1">(</span><span class="ltx_text ltx_font_bold">Narayan et al.</span><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib56" title=""><span class="ltx_text ltx_font_bold">2018</span></a><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.5"> This dataset is used for the abstractive summary generation task, consists of English documents and one-sentence summaries, totaling over 200k samples. </span></p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i3.p1">
<p class="ltx_p" id="A4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.1">AdGen</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.2.1">(</span><span class="ltx_text ltx_font_bold">Shao et al.</span><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib64" title=""><span class="ltx_text ltx_font_bold">2019</span></a><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.5"> This dataset is a Chinese dataset requires models to generate advertising text based based on given keywords, totalling over 100k samples. </span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A4.SS2.p3">
<p class="ltx_p" id="A4.SS2.p3.7"><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.1">이 실험에서 우리는 </span><math alttext="10^{-5}" class="ltx_Math" display="inline" id="A4.SS2.p3.1.m1.1"><semantics id="A4.SS2.p3.1.m1.1a"><msup id="A4.SS2.p3.1.m1.1.1" xref="A4.SS2.p3.1.m1.1.1.cmml"><mn id="A4.SS2.p3.1.m1.1.1.2" xref="A4.SS2.p3.1.m1.1.1.2.cmml">10</mn><mrow id="A4.SS2.p3.1.m1.1.1.3" xref="A4.SS2.p3.1.m1.1.1.3.cmml"><mo id="A4.SS2.p3.1.m1.1.1.3a" xref="A4.SS2.p3.1.m1.1.1.3.cmml">−</mo><mn id="A4.SS2.p3.1.m1.1.1.3.2" xref="A4.SS2.p3.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.1.m1.1b"><apply id="A4.SS2.p3.1.m1.1.1.cmml" xref="A4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A4.SS2.p3.1.m1.1.1.1.cmml" xref="A4.SS2.p3.1.m1.1.1">superscript</csymbol><cn id="A4.SS2.p3.1.m1.1.1.2.cmml" type="integer" xref="A4.SS2.p3.1.m1.1.1.2">10</cn><apply id="A4.SS2.p3.1.m1.1.1.3.cmml" xref="A4.SS2.p3.1.m1.1.1.3"><minus id="A4.SS2.p3.1.m1.1.1.3.1.cmml" xref="A4.SS2.p3.1.m1.1.1.3"></minus><cn id="A4.SS2.p3.1.m1.1.1.3.2.cmml" type="integer" xref="A4.SS2.p3.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.1.m1.1c">10^{-5}</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.1.m1.1d">10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.2">의 학습률을 채택한다. 배치 크기를 </span><math alttext="4" class="ltx_Math" display="inline" id="A4.SS2.p3.2.m2.1"><semantics id="A4.SS2.p3.2.m2.1a"><mn id="A4.SS2.p3.2.m2.1.1" xref="A4.SS2.p3.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.2.m2.1b"><cn id="A4.SS2.p3.2.m2.1.1.cmml" type="integer" xref="A4.SS2.p3.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.2.m2.1c">4</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.2.m2.1d">4</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.3">로 설정하고 최대 입력 길이는 </span><math alttext="2048" class="ltx_Math" display="inline" id="A4.SS2.p3.3.m3.1"><semantics id="A4.SS2.p3.3.m3.1a"><mn id="A4.SS2.p3.3.m3.1.1" xref="A4.SS2.p3.3.m3.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.3.m3.1b"><cn id="A4.SS2.p3.3.m3.1.1.cmml" type="integer" xref="A4.SS2.p3.3.m3.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.3.m3.1c">2048</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.3.m3.1d">2048</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.4">이다. 8비트 AdamW 최적화기</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.5.1">(</span><span class="ltx_text ltx_font_bold">Dettmers et al.</span><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.6.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib21" title=""><span class="ltx_text ltx_font_bold">2022b</span></a><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.7.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.8"> in bfloat16 precision with activation checkpointing to reduce memory footprint. GaLore의 경우, 랭크 및 스케일을 각각 </span><math alttext="128" class="ltx_Math" display="inline" id="A4.SS2.p3.4.m4.1"><semantics id="A4.SS2.p3.4.m4.1a"><mn id="A4.SS2.p3.4.m4.1.1" xref="A4.SS2.p3.4.m4.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.4.m4.1b"><cn id="A4.SS2.p3.4.m4.1.1.cmml" type="integer" xref="A4.SS2.p3.4.m4.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.4.m4.1c">128</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.4.m4.1d">128</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.9"> 및 </span><math alttext="2.0" class="ltx_Math" display="inline" id="A4.SS2.p3.5.m5.1"><semantics id="A4.SS2.p3.5.m5.1a"><mn id="A4.SS2.p3.5.m5.1.1" xref="A4.SS2.p3.5.m5.1.1.cmml">2.0</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.5.m5.1b"><cn id="A4.SS2.p3.5.m5.1.1.cmml" type="float" xref="A4.SS2.p3.5.m5.1.1">2.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.5.m5.1c">2.0</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.5.m5.1d">2.0</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.10""으로 설정하였다. LoRA 및 QLoRA의 경우, 모든 선형 레이어에 어댑터를 부착하고 랭크 및 알파를 각각 </span><math alttext="128" class="ltx_Math" display="inline" id="A4.SS2.p3.6.m6.1"><semantics id="A4.SS2.p3.6.m6.1a"><mn id="A4.SS2.p3.6.m6.1.1" xref="A4.SS2.p3.6.m6.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.6.m6.1b"><cn id="A4.SS2.p3.6.m6.1.1.cmml" type="integer" xref="A4.SS2.p3.6.m6.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.6.m6.1c">128</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.6.m6.1d">128</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.11"> 및 </span><math alttext="256" class="ltx_Math" display="inline" id="A4.SS2.p3.7.m7.1"><semantics id="A4.SS2.p3.7.m7.1a"><mn id="A4.SS2.p3.7.m7.1.1" xref="A4.SS2.p3.7.m7.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.7.m7.1b"><cn id="A4.SS2.p3.7.m7.1.1.cmml" type="integer" xref="A4.SS2.p3.7.m7.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.7.m7.1c">256</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.7.m7.1d">256</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.12""로 설정한다. 모든 실험 결과는 플래시 어텐션이 활성화된 NVIDIA A100 40GB GPU에서 수행된다. </span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</title>
<!--Generated on Thu Mar 21 08:35:35 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.13372v2/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2403.13372v2">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2403.13372v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2403.13372v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S1" title="1 Introduction ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S2" title="2 Efficient Fine-Tuning Techniques ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Efficient Fine-Tuning Techniques</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S2.SS1" title="2.1 Efficient Optimization ‣ 2 Efficient Fine-Tuning Techniques ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Efficient Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S2.SS2" title="2.2 Efficient Computation ‣ 2 Efficient Fine-Tuning Techniques ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Efficient Computation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3" title="3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">LlamaFactory</span> Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1" title="3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Model Loader</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1.SSS0.Px1" title="Model Initialization ‣ 3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Model Initialization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1.SSS0.Px2" title="Model Patching ‣ 3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Model Patching</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1.SSS0.Px3" title="Model Quantization ‣ 3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Model Quantization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1.SSS0.Px4" title="Adapter Attaching ‣ 3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Adapter Attaching</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS1.SSS0.Px5" title="Precision Adaptation ‣ 3.1 Model Loader ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Precision Adaptation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS2" title="3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Worker</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS2.SSS0.Px1" title="Dataset Loading ‣ 3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Dataset Loading</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS2.SSS0.Px2" title="Dataset Aligning ‣ 3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Dataset Aligning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS2.SSS0.Px3" title="Dataset Merging ‣ 3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Dataset Merging</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS2.SSS0.Px4" title="Dataset Pre-processing ‣ 3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Dataset Pre-processing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS3" title="3.3 Trainer ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Trainer</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS3.SSS0.Px1" title="Efficient Training ‣ 3.3 Trainer ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Efficient Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS3.SSS0.Px2" title="Model-Sharing RLHF ‣ 3.3 Trainer ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Model-Sharing RLHF</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS3.SSS0.Px3" title="Distributed Training ‣ 3.3 Trainer ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Distributed Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS4" title="3.4 Utilities ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Utilities</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS4.SSS0.Px1" title="Accelerated Inference ‣ 3.4 Utilities ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Accelerated Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS4.SSS0.Px2" title="Comprehensive Evaluation ‣ 3.4 Utilities ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Comprehensive Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.SS5" title="3.5 LlamaBoard: A Unified Interface for LlamaFactory ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span><span class="ltx_text ltx_font_smallcaps">LlamaBoard</span>: A Unified Interface for <span class="ltx_text ltx_font_smallcaps">LlamaFactory</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4" title="4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">4</span> </span>Empirical Study</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS1" title="4.1 Training Efficiency ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">4.1</span> </span>Training Efficiency</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS1.SSS0.Px1" title="Experimental Setup ‣ 4.1 Training Efficiency ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS1.SSS0.Px2" title="Results ‣ 4.1 Training Efficiency ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS2" title="4.2 Fine-Tuning on Downstream Tasks ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">4.2</span> </span>Fine-Tuning on Downstream Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS2.SSS0.Px1" title="Experimental Setup ‣ 4.2 Fine-Tuning on Downstream Tasks ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S4.SS2.SSS0.Px2" title="Results ‣ 4.2 Fine-Tuning on Downstream Tasks ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S5" title="5 Conclusion and Future Work ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">5</span> </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S6" title="6 Broader Impact and Responsible Use ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">6</span> </span>Broader Impact and Responsible Use</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A1" title="Appendix A Related Work ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">A</span> </span><span class="ltx_text ltx_font_bold">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A2" title="Appendix B Supported Models in LlamaFactory ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">B</span> </span><span class="ltx_text ltx_font_bold">Supported Models in </span><span class="ltx_text ltx_font_smallcaps">LlamaFactory</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A3" title="Appendix C Details of Designing Chat Template ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">C</span> </span><span class="ltx_text ltx_font_bold">Details of Designing Chat Template</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A4" title="Appendix D Experiment Details ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">D</span> </span><span class="ltx_text ltx_font_bold">Experiment Details</span></span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A4.SS1" title="D.1 Training Efficiency ‣ Appendix D Experiment Details ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">D.1</span> </span>Training Efficiency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A4.SS2" title="D.2 Fine-Tuning on Downstream Tasks ‣ Appendix D Experiment Details ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text ltx_font_medium">D.2</span> </span>Fine-Tuning on Downstream Tasks</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2403.13372v2 [cs.CL] 21 Mar 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_smallcaps" id="id1.id1">LlamaFactory</span>: Unified Efficient Fine-Tuning of 100+ Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yaowei Zheng<sup class="ltx_sup" id="id2.1.id1">1</sup>, Richong Zhang<sup class="ltx_sup" id="id3.2.id2">1</sup>, Junhao Zhang<sup class="ltx_sup" id="id4.3.id3">1</sup>, Yanhan Ye<sup class="ltx_sup" id="id5.4.id4">1</sup>, Zheyan Luo<sup class="ltx_sup" id="id6.5.id5">1</sup>, Yongqiang Ma<sup class="ltx_sup" id="id7.6.id6">2</sup>
<br class="ltx_break"><sup class="ltx_sup" id="id8.7.id7">1</sup>School of Computer Science and Engineering, Beihang University, China 
<br class="ltx_break"><sup class="ltx_sup" id="id9.8.id8">2</sup>School of Software and Microelectronics, Peking University, China 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id10.9.id9">hiyouga@buaa.edu.cn</span>, <span class="ltx_text ltx_font_typewriter" id="id11.10.id10">zhangrc@act.buaa.edu.cn</span>, <span class="ltx_text ltx_font_typewriter" id="id12.11.id11">{zhang.jh,yeyanhan,akamya}@buaa.edu.cn</span>, <span class="ltx_text ltx_font_typewriter" id="id13.12.id12">codingma@pku.edu.cn</span>
<br class="ltx_break">Demonstration video: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://youtu.be/W29FgeZEpus" title="">https://youtu.be/W29FgeZEpus</a>
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id14.id1">Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present <span class="ltx_text ltx_font_smallcaps" id="id14.id1.1">LlamaFactory</span>, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI <span class="ltx_text ltx_font_smallcaps" id="id14.id1.2">LlamaBoard</span>. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hiyouga/LLaMA-Factory" title="">https://github.com/hiyouga/LLaMA-Factory</a> and already received over 13,000 stars and 1,600 forks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_smallcaps" id="p1.1.1">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="p1.1.2">: Unified Efficient Fine-Tuning of 100+ Language Models</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break">
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1" style="width:433.6pt;"><span class="ltx_text" id="p2.1.1.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.1.1.1.1.1.1">
Yaowei Zheng<sup class="ltx_sup" id="p2.1.1.1.1.1.1.1.1.1">1</sup>, Richong Zhang<sup class="ltx_sup" id="p2.1.1.1.1.1.1.1.1.2">1</sup>, Junhao Zhang<sup class="ltx_sup" id="p2.1.1.1.1.1.1.1.1.3">1</sup>, Yanhan Ye<sup class="ltx_sup" id="p2.1.1.1.1.1.1.1.1.4">1</sup>, Zheyan Luo<sup class="ltx_sup" id="p2.1.1.1.1.1.1.1.1.5">1</sup>, Yongqiang Ma<sup class="ltx_sup" id="p2.1.1.1.1.1.1.1.1.6">2</sup></span></span></span>
<span class="ltx_tr" id="p2.1.1.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.2.2.1"><sup class="ltx_sup" id="p2.1.1.1.1.2.2.1.1">1</sup>School of Computer Science and Engineering, Beihang University, China</span></span>
<span class="ltx_tr" id="p2.1.1.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.3.3.1"><sup class="ltx_sup" id="p2.1.1.1.1.3.3.1.1">2</sup>School of Software and Microelectronics, Peking University, China</span></span>
<span class="ltx_tr" id="p2.1.1.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.4.4.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.1.1.1.4.4.1.1">hiyouga@buaa.edu.cn</span>, <span class="ltx_text ltx_font_typewriter" id="p2.1.1.1.1.4.4.1.2">zhangrc@act.buaa.edu.cn</span>, <span class="ltx_text ltx_font_typewriter" id="p2.1.1.1.1.4.4.1.3">{zhang.jh,yeyanhan,akamya}@buaa.edu.cn</span>, <span class="ltx_text ltx_font_typewriter" id="p2.1.1.1.1.4.4.1.4">codingma@pku.edu.cn</span></span></span>
<span class="ltx_tr" id="p2.1.1.1.1.5.5">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.5.5.1">Demonstration video: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://youtu.be/W29FgeZEpus" title="">https://youtu.be/W29FgeZEpus</a></span></span>
</span>
</span></span> </span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Zhao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib87" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite> present remarkable reasoning capabilities and empower a wide range of applications, such as question answering <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Jiang et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib36" title=""><span class="ltx_text ltx_font_bold">2023b</span></a>)</cite>, machine translation <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wang et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib75" title=""><span class="ltx_text ltx_font_bold">2023c</span></a>; <span class="ltx_text ltx_font_bold">Jiao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib37" title=""><span class="ltx_text ltx_font_bold">2023a</span></a>)</cite>, and information extraction <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Jiao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib38" title=""><span class="ltx_text ltx_font_bold">2023b</span></a>)</cite>. Subsequently, a substantial number of LLMs are developed and accessible through open-source communities. For example, Hugging Face’s open LLM leaderboard <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Beeching et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib7" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite> boasts over 5,000 models, offering convenience for individuals seeking to leverage the power of LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Fine-tuning extremely large number of parameters with limited resources becomes the main challenge of adapting LLM to downstream tasks. A popular solution is efficient fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Houlsby et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib32" title=""><span class="ltx_text ltx_font_bold">2019</span></a>; <span class="ltx_text ltx_font_bold">Hu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib33" title=""><span class="ltx_text ltx_font_bold">2022</span></a>; <span class="ltx_text ltx_font_bold">Ben&nbsp;Zaken et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib8" title=""><span class="ltx_text ltx_font_bold">2022</span></a>; <span class="ltx_text ltx_font_bold">Dettmers et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib22" title=""><span class="ltx_text ltx_font_bold">2023</span></a>; <span class="ltx_text ltx_font_bold">Zhao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib86" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>, which reduces the training cost of LLMs when adapting to various tasks. However, the community contributes various methods for efficient fine-tuning LLMs, lacking a systematic framework that adapts and unifies these methods to different LLMs and provides a friendly interface for user customization.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address the above problems, we develop <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.1">LlamaFactory</span>, a framework that democratizes the fine-tuning of LLMs. It unifies a variety of efficient fine-tuning methods through scalable modules, enabling the fine-tuning of hundreds of LLMs with minimal resources and high throughput. In addition, it streamlines commonly used training approaches, including generative pre-training <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Radford et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib61" title=""><span class="ltx_text ltx_font_bold">2018</span></a>)</cite>, supervised fine-tuning (SFT) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wei et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib77" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>, reinforcement learning from human feedback (RLHF) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_bold">Ouyang et&nbsp;al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib58" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>, and direct preference optimization (DPO) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_bold">Rafailov et&nbsp;al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib62" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>. Users can leverage command-line or web interfaces to customize and fine-tune their LLMs with minimal or no coding effort.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.1">LlamaFactory</span> consists of three main modules: <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p4.1.2">Model Loader</span>, <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p4.1.3">Data Worker</span> and <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p4.1.4">Trainer</span>. We minimize the dependencies of these modules on specific models and datasets, allowing the framework to flexibly scale to hundreds of models and datasets. Concretely, we first establish a model registry where the Model Loader can precisely attach adapters to the pre-trained models by identifying exact layers. Then we develop a data description specification that allows the Data Worker to gather datasets by aligning corresponding columns. Furthermore, we provide plug-and-play implementations of efficient fine-tuning methods that enable the Trainer to active by replacing default ones. Our design allows these modules to be reused across different training approaches, significantly reducing integration cost of new methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.1">LlamaFactory</span> is implemented with PyTorch <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Paszke et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib60" title=""><span class="ltx_text ltx_font_bold">2019</span></a>)</cite> and significantly benefits from open-source libraries, such as Transformers <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wolf et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib79" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite>, PEFT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Mangrulkar et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib52" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>, and TRL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">von Werra et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib72" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite>. On the basis, we provide an out-of-the-box framework with a higher level of abstraction. Additionally, we build <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.2">LlamaBoard</span> with Gradio <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Abid et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib1" title=""><span class="ltx_text ltx_font_bold">2019</span></a>)</cite>, enabling fine-tuning LLMs with no coding efforts required.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.1">LlamaFactory</span> is open-sourced under the Apache-2.0 license. It has already garnered over 13,000 stars and 1,600 forks on the GitHub<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/hiyouga/LLaMA-Factory</span></span></span>, and hundreds of open-source models have been built upon <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.2">LlamaFactory</span> on the Hugging Face Hub<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/models?other=llama-factory</span></span></span>. For example, the well-known GemSUra-7B <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Nguyen et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib57" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite> is built based on <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.3">LlamaFactory</span>, which first reveals the cross-lingual abilities of Gemma <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Mesnard et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib53" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>. Furthermore, dozens of research studies utilize our framework to explore new methods in LLMs, such as <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text ltx_font_bold">Wang et&nbsp;al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib73" title=""><span class="ltx_text ltx_font_bold">2023a</span></a>); <span class="ltx_text ltx_font_bold">Yu et&nbsp;al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib84" title=""><span class="ltx_text ltx_font_bold">2023</span></a>); <span class="ltx_text ltx_font_bold">Bhardwaj et&nbsp;al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib9" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>.

</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Efficient Fine-Tuning Techniques</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Efficient LLM fine-tuning techniques can be divided into two main categories: those focused on optimization and those aimed at computation. The primary objective of efficient optimization techniques is to adjust the parameters of LLMs while keeping costs to a minimum. On the other hand, efficient computation methods seek to decrease the time or space for the required computation in LLMs. The methods included in <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.1">LlamaFactory</span> are listed in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S2.T1" title="Table 1 ‣ 2 Efficient Fine-Tuning Techniques ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>. We will present these efficient fine-tuning techniques and show the substantial efficiency improvement achieved by incorporating them into our framework in the following sections.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Efficient fine-tuning techniques featured in <span class="ltx_text ltx_font_smallcaps" id="S2.T1.3.1">LlamaFactory</span>. Techniques that are compatible with each other are marked with ✓, while those that are not compatible are marked with ✗.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.1" style="width:433.6pt;height:255.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(109.8pt,-64.6pt) scale(2.02533799915901,2.02533799915901) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S2.T1.1.1.2.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.2">Freeze-tuning</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.3">GaLore</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.4">LoRA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.2.1.5">DoRA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.1.3.1.1">Mixed precision</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.1.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.1.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.1.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.3.1.5">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.4.2.1">Checkpointing</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2.2">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2.5">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.5.3.1">Flash attention</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3.2">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3.5">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.1.1">S<math alttext="{}^{2}" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><msup id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml"><mi id="S2.T1.1.1.1.1.m1.1.1a" xref="S2.T1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S2.T1.1.1.1.1.m1.1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1"><cn id="S2.T1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S2.T1.1.1.1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math> attention</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.2">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.3">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.5">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.1.6.4.1">Quantization</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.4.2">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.4.3">✗</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.4.4">✓</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.4.5">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T1.1.1.7.5.1">Unsloth</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.5.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.5.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.5.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.7.5.5">✗</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Efficient Optimization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Firstly, we provide an overview of the efficient optimization techniques utilized in <span class="ltx_text ltx_font_smallcaps" id="S2.SS1.p1.1.1">LlamaFactory</span>. The freeze-tuning method <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Houlsby et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib32" title=""><span class="ltx_text ltx_font_bold">2019</span></a>)</cite> involves freezing a majority of parameters while fine-tuning the remaining parameters in a small subset of decoder layers. Another method called gradient low-rank projection (GaLore) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Zhao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib86" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite> projects gradients into a lower-dimensional space, facilitating full-parameter learning in a memory-efficient manner. On the contrary, the low-rank adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib33" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite> method freezes all pre-trained weights and introduces a pair of trainable low-rank matrices to the designated layer. When combined with quantization, this approach is referred to as QLoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib22" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>, which additionally reduces the memory usage. The weight-decomposed low-rank adaptation (DoRA) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Liu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib50" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite> method breaks down pre-trained weights into magnitude and direction components, applying LoRA solely on the directional components to enhance the fine-tuning of LLMs. LoRA+ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hayou et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib30" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite> is proposed to overcome the sub-optimality of LoRA.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Efficient Computation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In <span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p1.1.1">LlamaFactory</span>, we integrate a range of efficient computational techniques. Commonly utilized techniques encompass mixed precision training <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Micikevicius et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib54" title=""><span class="ltx_text ltx_font_bold">2018</span></a>)</cite> and activation checkpointing <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Chen et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib14" title=""><span class="ltx_text ltx_font_bold">2016</span></a>)</cite>. Drawing insights from the examination of the input-output (IO) expenses of the attention layer, flash attention <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib18" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite> introduces a hardware-friendly approach to enhance attention computation. S<math alttext="{}^{2}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><msup id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1a" xref="S2.SS2.p1.1.m1.1.1.cmml"></mi><mn id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><cn id="S2.SS2.p1.1.m1.1.1.1.cmml" type="integer" xref="S2.SS2.p1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math> attention <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Chen et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib15" title=""><span class="ltx_text ltx_font_bold">2024b</span></a>)</cite> tackles the challenge of extending context in block-sparse attention, thereby diminishing memory usage in fine-tuning long-context LLMs. Various quantization strategies <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib20" title=""><span class="ltx_text ltx_font_bold">2022a</span></a>; <span class="ltx_text ltx_font_bold">Frantar et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib26" title=""><span class="ltx_text ltx_font_bold">2023</span></a>; <span class="ltx_text ltx_font_bold">Lin et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib49" title=""><span class="ltx_text ltx_font_bold">2023</span></a>; <span class="ltx_text ltx_font_bold">Egiazarian et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib25" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite> decrease memory requirements in large language models (LLMs) by utilizing lower-precision representations for weights. Nevertheless, the fine-tuning of quantized models is restricted to the adapter-based techniques like LoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib33" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>. Unsloth <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Han and Han</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib29" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite> incorporates Triton <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Tillet et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib68" title=""><span class="ltx_text ltx_font_bold">2019</span></a>)</cite> for implementing the backward propagation of LoRA, which reduces floating-point operations (FLOPs) during gradient descent and leads to expedited LoRA training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><span class="ltx_text ltx_font_smallcaps" id="S2.SS2.p2.1.1">LlamaFactory</span> effectively combines these techniques into a cohesive structure that greatly enhances the efficiency of LLM fine-tuning. This results in a reduction of the memory footprint from 18 bytes per parameter during mixed precision training <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Micikevicius et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib54" title=""><span class="ltx_text ltx_font_bold">2018</span></a>)</cite> or 8 bytes per parameter in bfloat16 training <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Le&nbsp;Scao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib43" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite> to only 0.6 bytes per parameter. Further elaboration on our framework will be provided in the subsequent section.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">LlamaFactory</span> Framework</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">LlamaFactory</span> consists of three main modules: Model Loader, Data Worker, and Trainer. The Model Loader prepares various architectures for fine-tuning, which supports over 100 LLMs. The Data Worker processes data from different tasks through a well-designed pipeline, which support more than 50 datasets. The Trainer unifies efficient fine-tuning methods to adapt these models on different tasks and datasets, which offers four training approaches. <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">LlamaBoard</span> provides a friendly visual interface for the above modules, enabling users to configure and launch individual LLM fine-tuning process in a codeless way and monitor the training status on the fly. We illustrate the overall architecture of <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.3">LlamaFactory</span> in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.F1" title="Figure 1 ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.F1.1" style="width:429.3pt;height:512.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(68.5pt,-81.9pt) scale(1.46918076443966,1.46918076443966) ;"><svg class="ltx_picture" height="482.98" id="S3.F1.1.pic1" overflow="visible" version="1.1" width="395.08"><g transform="translate(0,482.98) matrix(1 0 0 -1 0 0) translate(0.69,0) translate(0,8.57)"><g color="#000000" fill="#FAFAFA" stroke="#717171" stroke-width="1.0pt"><path d="M 0 385.83 M 0 391.36 L 0 451.16 C 0 454.22 2.48 456.69 5.53 456.69 L 388.17 456.69 C 391.22 456.69 393.7 454.22 393.7 451.16 L 393.7 391.36 C 393.7 388.3 391.22 385.83 388.17 385.83 L 5.53 385.83 C 2.48 385.83 0 388.3 0 391.36 Z M 393.7 456.69"></path></g><g fill="#000000" stroke="#000000"><g color="#000000" stroke-width="1.0pt"><g fill="#717171"><path d="M 127.95 442.91 M 127.95 448.45 L 127.95 468.87 C 127.95 471.93 130.43 474.41 133.49 474.41 L 260.21 474.41 C 263.27 474.41 265.75 471.93 265.75 468.87 L 265.75 448.45 C 265.75 445.39 263.27 442.91 260.21 442.91 L 133.49 442.91 C 130.43 442.91 127.95 445.39 127.95 448.45 Z M 265.75 474.41" style="stroke:none"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 159.55 453.86)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="74.6"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.F1.1.pic1.1.1.1.1.1.1.1">LlamaBoard</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#FAFAFA" stroke="#717171"><path d="M 7.87 399.61 M 7.87 405.14 L 7.87 425.57 C 7.87 428.62 10.35 431.1 13.41 431.1 L 183.44 431.1 C 186.5 431.1 188.98 428.62 188.98 425.57 L 188.98 405.14 C 188.98 402.08 186.5 399.61 183.44 399.61 L 13.41 399.61 C 10.35 399.61 7.87 402.08 7.87 405.14 Z M 188.98 431.1"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 27.07 411.89)"><foreignObject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="143.1"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.2.2.2.2.1.1.1">Argument Configurator</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#FAFAFA" stroke="#717171"><path d="M 204.72 399.61 M 204.72 405.14 L 204.72 425.57 C 204.72 428.62 207.2 431.1 210.26 431.1 L 380.29 431.1 C 383.35 431.1 385.83 428.62 385.83 425.57 L 385.83 405.14 C 385.83 402.08 383.35 399.61 380.29 399.61 L 210.26 399.61 C 207.2 399.61 204.72 402.08 204.72 405.14 Z M 385.83 431.1"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 221.4 411.97)"><foreignObject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="147.75"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.3.3.3.3.1.1.1">Training Status Monitor</span></foreignObject></g></g><g color="#000000" fill="#F4F4F4" stroke="#0B0B0B" stroke-width="1.0pt"><path d="M 0 200.79 M 0 206.32 L 0 344.86 C 0 347.92 2.48 350.39 5.53 350.39 L 388.17 350.39 C 391.22 350.39 393.7 347.92 393.7 344.86 L 393.7 206.32 C 393.7 203.27 391.22 200.79 388.17 200.79 L 5.53 200.79 C 2.48 200.79 0 203.27 0 206.32 Z M 393.7 350.39"></path></g><g color="#000000" stroke-width="1.0pt"><g fill="#0B0B0B"><path d="M 127.95 334.65 M 127.95 340.18 L 127.95 360.61 C 127.95 363.66 130.43 366.14 133.49 366.14 L 260.21 366.14 C 263.27 366.14 265.75 363.66 265.75 360.61 L 265.75 340.18 C 265.75 337.12 263.27 334.65 260.21 334.65 L 133.49 334.65 C 130.43 334.65 127.95 337.12 127.95 340.18 Z M 265.75 366.14" style="stroke:none"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 174.71 345.67)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="44.28"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.F1.1.pic1.4.4.4.4.1.1.1">Trainer</span></foreignObject></g></g><g color="#000000" fill="#DCDCDC" stroke="#0B0B0B" stroke-width="1.0pt"><path d="M 7.87 214.57 M 7.87 220.1 L 7.87 319.27 C 7.87 322.33 10.35 324.8 13.41 324.8 L 183.44 324.8 C 186.5 324.8 188.98 322.33 188.98 319.27 L 188.98 220.1 C 188.98 217.04 186.5 214.57 183.44 214.57 L 13.41 214.57 C 10.35 214.57 7.87 217.04 7.87 220.1 Z M 188.98 324.8"></path></g><g color="#000000" fill="#DCDCDC" stroke="#0B0B0B" stroke-width="1.0pt"><path d="M 204.72 214.57 M 204.72 220.1 L 204.72 319.27 C 204.72 322.33 207.2 324.8 210.26 324.8 L 380.29 324.8 C 383.35 324.8 385.83 322.33 385.83 319.27 L 385.83 220.1 C 385.83 217.04 383.35 214.57 380.29 214.57 L 210.26 214.57 C 207.2 214.57 204.72 217.04 204.72 220.1 Z M 385.83 324.8"></path></g><g color="#000000" fill="#000000" fill-opacity="1.000000" stroke="#000000" stroke-width="1.0pt" transform="matrix(1.0 0.0 0.0 1.0 58.45 305.67)"><foreignObject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="79.95"><span class="ltx_text ltx_font_sansserif ltx_font_bold" id="S3.F1.1.pic1.5.5.5.5.1.1">Optimization</span></foreignObject></g><g color="#000000" fill="#000000" fill-opacity="1.000000" stroke="#000000" stroke-width="1.0pt" transform="matrix(1.0 0.0 0.0 1.0 260.24 305.6)"><foreignObject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="70.07"><span class="ltx_text ltx_font_sansserif ltx_font_bold" id="S3.F1.1.pic1.6.6.6.6.1.1">Approaches</span></foreignObject></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 17.72 261.81 M 17.72 267.35 L 17.72 287.77 C 17.72 290.83 20.19 293.31 23.25 293.31 L 86.98 293.31 C 90.04 293.31 92.52 290.83 92.52 287.77 L 92.52 267.35 C 92.52 264.29 90.04 261.81 86.98 261.81 L 23.25 261.81 C 20.19 261.81 17.72 264.29 17.72 267.35 Z M 92.52 293.31"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 31.67 273.41)"><foreignObject height="10.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="46.89"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.7.7.7.7.1.1.1">LoRA+</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 104.33 261.81 M 104.33 267.35 L 104.33 287.77 C 104.33 290.83 106.81 293.31 109.87 293.31 L 173.6 293.31 C 176.66 293.31 179.13 290.83 179.13 287.77 L 179.13 267.35 C 179.13 264.29 176.66 261.81 173.6 261.81 L 109.87 261.81 C 106.81 261.81 104.33 264.29 104.33 267.35 Z M 179.13 293.31"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 119.28 272.83)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="44.91"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.8.8.8.8.1.1.1">GaLore</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 17.72 222.44 M 17.72 227.98 L 17.72 248.4 C 17.72 251.46 20.19 253.94 23.25 253.94 L 86.98 253.94 C 90.04 253.94 92.52 251.46 92.52 248.4 L 92.52 227.98 C 92.52 224.92 90.04 222.44 86.98 222.44 L 23.25 222.44 C 20.19 222.44 17.72 224.92 17.72 227.98 Z M 92.52 253.94"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 32.82 233.38)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="44.59"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.9.9.9.9.1.1.1">Offload</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 104.33 222.44 M 104.33 227.98 L 104.33 248.4 C 104.33 251.46 106.81 253.94 109.87 253.94 L 173.6 253.94 C 176.66 253.94 179.13 251.46 179.13 248.4 L 179.13 227.98 C 179.13 224.92 176.66 222.44 173.6 222.44 L 109.87 222.44 C 106.81 222.44 104.33 224.92 104.33 227.98 Z M 179.13 253.94"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 114.52 233.46)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="54.43"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.10.10.10.10.1.1.1">Partition</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 214.57 261.81 M 214.57 267.35 L 214.57 287.77 C 214.57 290.83 217.04 293.31 220.1 293.31 L 283.83 293.31 C 286.89 293.31 289.37 290.83 289.37 287.77 L 289.37 267.35 C 289.37 264.29 286.89 261.81 283.83 261.81 L 220.1 261.81 C 217.04 261.81 214.57 264.29 214.57 267.35 Z M 289.37 293.31"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 224.54 272.83)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="54.85"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.11.11.11.11.1.1.1">Pre-train</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 301.18 261.81 M 301.18 267.35 L 301.18 287.77 C 301.18 290.83 303.66 293.31 306.72 293.31 L 370.45 293.31 C 373.51 293.31 375.98 290.83 375.98 287.77 L 375.98 267.35 C 375.98 264.29 373.51 261.81 370.45 261.81 L 306.72 261.81 C 303.66 261.81 301.18 264.29 301.18 267.35 Z M 375.98 293.31"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 325.23 272.83)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.71"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.12.12.12.12.1.1.1">SFT</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 214.57 222.44 M 214.57 227.98 L 214.57 248.4 C 214.57 251.46 217.04 253.94 220.1 253.94 L 283.83 253.94 C 286.89 253.94 289.37 251.46 289.37 248.4 L 289.37 227.98 C 289.37 224.92 286.89 222.44 283.83 222.44 L 220.1 222.44 C 217.04 222.44 214.57 224.92 214.57 227.98 Z M 289.37 253.94"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 232.85 233.46)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="38.24"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.13.13.13.13.1.1.1">RLHF</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#E8E8E8" stroke="#0B0B0B"><path d="M 301.18 222.44 M 301.18 227.98 L 301.18 248.4 C 301.18 251.46 303.66 253.94 306.72 253.94 L 370.45 253.94 C 373.51 253.94 375.98 251.46 375.98 248.4 L 375.98 227.98 C 375.98 224.92 373.51 222.44 370.45 222.44 L 306.72 222.44 C 303.66 222.44 301.18 224.92 301.18 227.98 Z M 375.98 253.94"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 323.21 233.46)"><foreignObject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="30.75"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.14.14.14.14.1.1.1">DPO</span></foreignObject></g></g><g color="#000000" fill="#F9F9F9" stroke="#3C3C3C" stroke-width="1.0pt"><path d="M 0 51.18 M 0 56.72 L 0 159.82 C 0 162.88 2.48 165.35 5.53 165.35 L 183.44 165.35 C 186.5 165.35 188.98 162.88 188.98 159.82 L 188.98 56.72 C 188.98 53.66 186.5 51.18 183.44 51.18 L 5.53 51.18 C 2.48 51.18 0 53.66 0 56.72 Z M 188.98 165.35"></path></g><g color="#000000" fill="#F9F9F9" stroke="#3D3D3D" stroke-width="1.0pt"><path d="M 204.72 51.18 M 204.72 56.72 L 204.72 159.82 C 204.72 162.88 207.2 165.35 210.26 165.35 L 388.17 165.35 C 391.22 165.35 393.7 162.88 393.7 159.82 L 393.7 56.72 C 393.7 53.66 391.22 51.18 388.17 51.18 L 210.26 51.18 C 207.2 51.18 204.72 53.66 204.72 56.72 Z M 393.7 165.35"></path></g><g color="#000000" stroke-width="1.0pt"><g fill="#3C3C3C"><path d="M 25.59 149.61 M 25.59 155.14 L 25.59 175.57 C 25.59 178.62 28.07 181.1 31.13 181.1 L 157.85 181.1 C 160.91 181.1 163.39 178.62 163.39 175.57 L 163.39 155.14 C 163.39 152.08 160.91 149.61 157.85 149.61 L 31.13 149.61 C 28.07 149.61 25.59 152.08 25.59 155.14 Z M 163.39 181.1" style="stroke:none"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 52.48 160.55)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="83.64"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.F1.1.pic1.15.15.15.15.1.1.1">Model Loader</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#3D3D3D"><path d="M 230.32 149.61 M 230.32 155.14 L 230.32 175.57 C 230.32 178.62 232.79 181.1 235.85 181.1 L 362.58 181.1 C 365.63 181.1 368.11 178.62 368.11 175.57 L 368.11 155.14 C 368.11 152.08 365.63 149.61 362.58 149.61 L 235.85 149.61 C 232.79 149.61 230.32 152.08 230.32 155.14 Z M 368.11 181.1" style="stroke:none"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 260.07 160.55)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="78.68"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.F1.1.pic1.16.16.16.16.1.1.1">Data Worker</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3C3C3C"><path d="M 7.87 104.33 M 7.87 109.87 L 7.87 130.29 C 7.87 133.35 10.35 135.83 13.41 135.83 L 96.83 135.83 C 99.88 135.83 102.36 133.35 102.36 130.29 L 102.36 109.87 C 102.36 106.81 99.88 104.33 96.83 104.33 L 13.41 104.33 C 10.35 104.33 7.87 106.81 7.87 109.87 Z M 102.36 135.83"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 16.49 115.27)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="77.26"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.17.17.17.17.1.1.1">Initialization</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3C3C3C"><path d="M 110.24 104.33 M 110.24 109.87 L 110.24 130.29 C 110.24 133.35 112.71 135.83 115.77 135.83 L 175.57 135.83 C 178.62 135.83 181.1 133.35 181.1 130.29 L 181.1 109.87 C 181.1 106.81 178.62 104.33 175.57 104.33 L 115.77 104.33 C 112.71 104.33 110.24 106.81 110.24 109.87 Z M 181.1 135.83"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 122.47 115.27)"><foreignObject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="46.39"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.18.18.18.18.1.1.1">Patches</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3C3C3C"><path d="M 7.87 62.99 M 7.87 68.53 L 7.87 88.95 C 7.87 92.01 10.35 94.49 13.41 94.49 L 96.83 94.49 C 99.88 94.49 102.36 92.01 102.36 88.95 L 102.36 68.53 C 102.36 65.47 99.88 62.99 96.83 62.99 L 13.41 62.99 C 10.35 62.99 7.87 65.47 7.87 68.53 Z M 102.36 94.49"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 15.72 75.36)"><foreignObject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="79.18"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.19.19.19.19.1.1.1">Quantization</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3C3C3C"><path d="M 110.24 62.99 M 110.24 68.53 L 110.24 88.95 C 110.24 92.01 112.71 94.49 115.77 94.49 L 175.57 94.49 C 178.62 94.49 181.1 92.01 181.1 88.95 L 181.1 68.53 C 181.1 65.47 178.62 62.99 175.57 62.99 L 115.77 62.99 C 112.71 62.99 110.24 65.47 110.24 68.53 Z M 181.1 94.49"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 118.13 75.28)"><foreignObject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="55.08"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.20.20.20.20.1.1.1">Adapters</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3D3D3D"><path d="M 291.34 104.33 M 291.34 109.87 L 291.34 130.29 C 291.34 133.35 293.82 135.83 296.87 135.83 L 380.29 135.83 C 383.35 135.83 385.83 133.35 385.83 130.29 L 385.83 109.87 C 385.83 106.81 383.35 104.33 380.29 104.33 L 296.87 104.33 C 293.82 104.33 291.34 106.81 291.34 109.87 Z M 385.83 135.83"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 313.02 116.62)"><foreignObject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="51.12"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.21.21.21.21.1.1.1">Aligning</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3D3D3D"><path d="M 212.6 104.33 M 212.6 109.87 L 212.6 130.29 C 212.6 133.35 215.08 135.83 218.13 135.83 L 277.93 135.83 C 280.99 135.83 283.46 133.35 283.46 130.29 L 283.46 109.87 C 283.46 106.81 280.99 104.33 277.93 104.33 L 218.13 104.33 C 215.08 104.33 212.6 106.81 212.6 109.87 Z M 283.46 135.83"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 223.72 116.62)"><foreignObject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="48.62"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.22.22.22.22.1.1.1">Loading</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3D3D3D"><path d="M 291.34 62.99 M 291.34 68.53 L 291.34 88.95 C 291.34 92.01 293.82 94.49 296.87 94.49 L 380.29 94.49 C 383.35 94.49 385.83 92.01 385.83 88.95 L 385.83 68.53 C 385.83 65.47 383.35 62.99 380.29 62.99 L 296.87 62.99 C 293.82 62.99 291.34 65.47 291.34 68.53 Z M 385.83 94.49"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 306.28 75.36)"><foreignObject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="64.23"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.23.23.23.23.1.1.1">Preprocess</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F3F3F3" stroke="#3D3D3D"><path d="M 212.6 62.99 M 212.6 68.53 L 212.6 88.95 C 212.6 92.01 215.08 94.49 218.13 94.49 L 277.93 94.49 C 280.99 94.49 283.46 92.01 283.46 88.95 L 283.46 68.53 C 283.46 65.47 280.99 62.99 277.93 62.99 L 218.13 62.99 C 215.08 62.99 212.6 65.47 212.6 68.53 Z M 283.46 94.49"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 223.22 75.36)"><foreignObject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="49.62"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.24.24.24.24.1.1.1">Merging</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F9F9F9" stroke="#3C3C3C"><path d="M 0 -7.87 M 0 -2.34 L 0 18.09 C 0 21.14 2.48 23.62 5.53 23.62 L 183.44 23.62 C 186.5 23.62 188.98 21.14 188.98 18.09 L 188.98 -2.34 C 188.98 -5.4 186.5 -7.87 183.44 -7.87 L 5.53 -7.87 C 2.48 -7.87 0 -5.4 0 -2.34 Z M 188.98 23.62"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 16.48 3.65)"><foreignObject height="10.76" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="155.63"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.25.25.25.25.1.1.1">100+ Pre-Trained Models</span></foreignObject></g></g><g color="#000000" stroke-width="1.0pt"><g fill="#F9F9F9" stroke="#3D3D3D"><path d="M 204.72 -7.87 M 204.72 -2.34 L 204.72 18.09 C 204.72 21.14 207.2 23.62 210.26 23.62 L 388.17 23.62 C 391.22 23.62 393.7 21.14 393.7 18.09 L 393.7 -2.34 C 393.7 -5.4 391.22 -7.87 388.17 -7.87 L 210.26 -7.87 C 207.2 -7.87 204.72 -5.4 204.72 -2.34 Z M 393.7 23.62"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 241.96 3.72)"><foreignObject height="10.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="114.5"><span class="ltx_text ltx_font_sansserif" id="S3.F1.1.pic1.26.26.26.26.1.1.1">50+ NLP Datasets</span></foreignObject></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 98.43 399.61 L 98.43 357.75" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 -1.0 1.0 0.0 98.43 357.75)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 295.28 350.39 L 295.28 392.25" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 1.0 -1.0 0.0 295.28 392.25)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 94.49 181.1 L 94.49 207.21" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 1.0 -1.0 0.0 94.49 207.21)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 299.21 181.1 L 299.21 207.21" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 1.0 -1.0 0.0 299.21 207.21)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 94.49 23.62 L 94.49 43.83" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 1.0 -1.0 0.0 94.49 43.83)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g><g color="#000000" stroke="#000000" stroke-width="1.0pt"><g stroke-width="2.25pt"><path d="M 299.21 23.62 L 299.21 43.83" style="fill:none"></path></g><g fill="#000000" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter" stroke-width="1.18121pt" transform="matrix(0.0 1.0 -1.0 0.0 299.21 43.83)"><path d="M 5.25 0 C 4.61 0.23 1.77 1.52 0 2.92 L 0 -2.92 C 1.77 -1.52 4.61 -0.23 5.25 0 Z"></path></g></g></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The architecture of <span class="ltx_text ltx_font_smallcaps" id="S3.F1.3.1">LlamaFactory</span>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Loader</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">This section initially presents the four components in Model Loader: Model Initialization, Model Patching, Model Quantization, and Adapter Attaching, followed by a description of our approach of adapting to a wide range of devices by handling the parameter float-point precision when fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Model Initialization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">We employ the <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px1.p1.1.1">AutoModel</span> API of Transformers <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wolf et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib79" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite> for loading models and initializing parameters. In order to make our framework compatible with different model architectures, we establish a model registry to store the type of each layer, thereby facilitating the utilization of efficient fine-tuning techniques more straightforward. In cases where the vocabulary size of the tokenizer exceeds the capacity of the embedding layer, we resize the layer and initialize new parameters with noisy mean initialization. To determine the scaling factor for RoPE scaling <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Chen et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib13" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>, we compute it as the ratio of the maximum input sequence length to the context length of the model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Model Patching</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">To enable flash attention and S<math alttext="{}^{2}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><msup id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1a" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"></mi><mn id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1"><cn id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" type="integer" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math> attention, we employ a monkey patch to replace the forward computation of models. Nevertheless, since flash attention has been supported since Transformers 4.34.0, we use the API to enable flash attention. To prevent excessive partitioning of the dynamic modules, we set the mixture-of-experts (MoE) blocks as leaf modules when we optimize it under DeepSpeed ZeRO-3 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Rasley et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib63" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Model Quantization</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">Dynamically quantizing models to 8 bits or 4 bits with LLM.int8 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib20" title=""><span class="ltx_text ltx_font_bold">2022a</span></a>)</cite> can be performed through the bitsandbytes library <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib19" title=""><span class="ltx_text ltx_font_bold">2021</span></a>)</cite>. For 4-bit quantization, we utilize the double quantization and 4-bit normal float as QLoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib22" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>. We also support fine-tuning the models quantized by the post-training quantization (PTQ) methods, including GPTQ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Frantar et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib26" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>, AWQ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Lin et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib49" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>, and AQLM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Egiazarian et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib25" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>. Note that we cannot directly fine-tune the quantized weights; thus, the quantized models are only compatible with adapter-based methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Adapter Attaching</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">We automatically identify the appropriate layers using the model registry to attach adapters. Adapters are attached to a subset of layers by default to save memory, but attaching them to all linear layers may yield better performance <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Dettmers et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib22" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>. The PEFT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Mangrulkar et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib52" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite> library offers an extremely convenient way to attach adapters such as LoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib33" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>, rsLoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Kalajdzievski</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib39" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>, and DoRA <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Liu et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib50" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite>. We replace the backward computation with the one of Unsloth <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Han and Han</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib29" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite> to accelerate LoRA. To perform reinforcement learning from human feedback (RLHF), we add a value head upon the model, a linear layer that maps the representation of each token to a scalar.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px5">
<h4 class="ltx_title ltx_title_paragraph">Precision Adaptation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px5.p1.1">We handle the floating-point precision of pre-trained models based on the capabilities of devices. For NVIDIA GPUs, we use bfloat16 precision if the computation capability is 8.0 or higher. Otherwise, float16 is adopted. We use float16 for Ascend NPUs and AMD GPUs and float32 for non-CUDA devices. Note that loading bfloat16 models with float16 precision may lead to overflow issues. In mixed precision training, we set all trainable parameters to float32. Nevertheless, we retain the trainable parameters as bfloat16 in bfloat16 training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Worker</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We develop a data processing pipeline, including dataset loading, dataset aligning, dataset merging and dataset pre-processing. It standardizes datasets of different tasks into a unified format, enabling us to fine-tune models on datasets in various formats.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Dataset Loading</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">We utilize the datasets <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Lhoest et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib44" title=""><span class="ltx_text ltx_font_bold">2021</span></a>)</cite> library to load the data, which allows the users to load remote datasets from the Hugging Face Hub or read local datasets via scripts or through files. The datasets library significantly reduces memory overhead during data processing and accelerates sample querying using Arrow <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Apache</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib5" title=""><span class="ltx_text ltx_font_bold">2016</span></a>)</cite>. By default, the whole dataset is downloaded to local disk. However, if a dataset is too large to be stored, our framework provides dataset streaming to iterate over it without downloading.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Dataset structures in <span class="ltx_text ltx_font_smallcaps" id="S3.T2.2.1">LlamaFactory</span>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.3" style="width:429.3pt;height:104.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.9pt,10.7pt) scale(0.83018324188649,0.83018324188649) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T2.3.1.1.1.1">Plain text</th>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S3.T2.3.1.1.1.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.1.1.2.1">[{"text": "…"}, {"text": "…"}]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.2.2.1">Alpaca-like data</th>
<td class="ltx_td ltx_align_justify" id="S3.T2.3.1.2.2.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.2.2.2.1">[{"instruction": "…", "input": "…", "output": "…"}]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.3.3.1">ShareGPT-like data</th>
<td class="ltx_td ltx_align_justify" id="S3.T2.3.1.3.3.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.3.3.2.1">[{"conversations": [{"from": "human", "value": "…"}, {"from": "gpt", "value": "…"}]}]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.4.4.1">Preference data</th>
<td class="ltx_td ltx_align_justify" id="S3.T2.3.1.4.4.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.4.4.2.1">[{"instruction": "…", "input": "…", "output": ["…", "…"]}]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.1.5.5.1">Standardized data</th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.3.1.5.5.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.5.5.2.1">{"prompt": [{"role": "…", "content": "…"}],</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.6.6">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T2.3.1.6.6.1"></th>
<td class="ltx_td ltx_align_justify" id="S3.T2.3.1.6.6.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.6.6.2.1">"response": [{"role": "…", "content": "…"}],</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.7.7">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb" id="S3.T2.3.1.7.7.1"></th>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S3.T2.3.1.7.7.2" style="width:213.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.3.1.7.7.2.1">"system": "…", "tools": "…"}</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Dataset Aligning</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">To unify the dataset format, we design a data description specification to characterize the structure of datasets. For example, the alpaca dataset has three columns: instruction, input and output <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Taori et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib66" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>. We convert the dataset into a standard structure that is compatible with various tasks according to the data description specification. Some examples of dataset structures are shown in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#S3.T2" title="Table 2 ‣ Dataset Loading ‣ 3.2 Data Worker ‣ 3 LlamaFactory Framework ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Dataset Merging</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">The unified dataset structure provides an efficient approach for merging multiple datasets. For the datasets in non-streaming mode, we simply concatenate them before the datasets are shuffled during training. However, in streaming mode, simply concatenating the datasets impedes data shuffling. Therefore, we offer methods to alternately read the data from different datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Dataset Pre-processing</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px4.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.SS2.SSS0.Px4.p1.1.1">LlamaFactory</span> is designed for fine-tuning the text generative models, which is primarily used in chat completion. Chat template is a crucial component in these models, because it is highly related to the instruction-following abilities of these models. Therefore, we provide dozens of chat templates that can be automatically chosen according to the model type. We encode the sentence after applying the chat template using the tokenizer. By default, we only compute loss on the completions, while the prompts are disregarded <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Taori et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib66" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>. Optionally, we can utilize sequence packing <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Krell et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib41" title=""><span class="ltx_text ltx_font_bold">2021</span></a>)</cite> to reduce the training time, which is automatically enabled when performing generative pre-training. Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#A3" title="Appendix C Details of Designing Chat Template ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">C</span></a> shows details of our template design.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Trainer</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Efficient Training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.3">We integrate state-of-the-art efficient fine-tuning methods, including LoRA+ <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hayou et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib30" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite> and GaLore <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Zhao et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib86" title=""><span class="ltx_text ltx_font_bold">2024</span></a>)</cite> to the trainer by replacing the default component. These training approaches are independent of the trainer, making them easily applicable to various tasks. We utilize the trainer of Transformers <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wolf et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib79" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite> for pre-training and SFT, while adopting the trainers of TRL <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">von Werra et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib72" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite> for RLHF and DPO. The tailored data collators are leveraged to differentiate trainers of various training approaches. To match the input format of the trainer for preference data, we build <math alttext="2n" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml">2</mn><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1"><times id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.1"></times><cn id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.2">2</cn><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">2n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">2 italic_n</annotation></semantics></math> samples in a batch where the first <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.2.m2.1d">italic_n</annotation></semantics></math> samples are chosen examples and the last <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.3.m3.1d">italic_n</annotation></semantics></math> samples are rejected examples.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Model-Sharing RLHF</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">Allowing RLHF training on consumer devices is a useful property for LLM fine-tuning. However, it is difficult because RLHF training requires four different models. To address this problem, we propose model-sharing RLHF, enabling entire RLHF training with no more than one pre-trained model. Concretely, we first train an adapter and a value head with the objective function for reward modeling, allowing the model to compute reward scores. Then we initialize another adapter and value head and train them with the PPO algorithm <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Ouyang et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib58" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite>. The adapters and value heads are dynamically switched through the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px2.p1.1.1">set_adapter</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px2.p1.1.2">disable_adapter</span> APIs of PEFT <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Mangrulkar et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib52" title=""><span class="ltx_text ltx_font_bold">2022</span></a>)</cite> during training, allowing the pre-trained model to serve as policy, value, reference, and reward models simultaneously. To the best of our knowledge, this is the first method that supports RLHF training on consumer devices.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Distributed Training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">We can combine the above trainers with DeepSpeed <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Rasley et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib63" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite> for distributed training. Leveraging the DeepSpeed ZeRO optimizer, the memory consumption can be further reduced via partitioning or offloading.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Utilities</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Accelerated Inference</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">During inference time, we reuses the chat template from the data worker to build the model inputs. We offer support for sampling the model outputs using Transformers <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Wolf et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib79" title=""><span class="ltx_text ltx_font_bold">2020</span></a>)</cite> and vLLM <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Kwon et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib42" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>, both of which support stream decoding. Additionally, we implement an OpenAI-style API that utilizes the asynchronous LLM engine and paged attention of vLLM to provide high-throughput concurrent inference services, facilitating the deployment of fine-tuned LLMs into various applications.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Comprehensive Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.1">We include several metrics for evaluating LLMs, including multiple-choice tasks such as MMLU <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Hendrycks et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib31" title=""><span class="ltx_text ltx_font_bold">2021</span></a>)</cite>, CMMLU <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Li et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib45" title=""><span class="ltx_text ltx_font_bold">2023a</span></a>)</cite>, and C-Eval <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Huang et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib34" title=""><span class="ltx_text ltx_font_bold">2023</span></a>)</cite>, as well as calculating text similarity scores like BLEU-4 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Papineni et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib59" title=""><span class="ltx_text ltx_font_bold">2002</span></a>)</cite> and ROUGE <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Lin</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib48" title=""><span class="ltx_text ltx_font_bold">2004</span></a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.1.1">LlamaBoard</span>: A Unified Interface for <span class="ltx_text ltx_font_smallcaps" id="S3.SS5.2.2">LlamaFactory</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p1.1.1">LlamaBoard</span> is a unified user interface based on Gradio <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text ltx_font_bold">Abid et&nbsp;al.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib1" title=""><span class="ltx_text ltx_font_bold">2019</span></a>)</cite> that allows users to customize the fine-tuning of LLMs without writing any code. It offers a streamlined model fine-tuning and inference service, enabling users to easily leveraging 100+ LLMs and 50+ datasets in their practice. <span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p1.1.2">LlamaBoard</span> has the following notable features:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S3.SS5.p2.1.1">Easy Configuration</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p2.1.2"> </span><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p2.1.3">LlamaBoard</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p2.1.4"> allows users to customize fine-tuning arguments through interaction with the web interface. We provide default values for many arguments that are recommended for most users, simplifying the configuration process. Moreover, users can preview datasets on the web UI to check their customized format.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S3.SS5.p3.1.1">Monitorable Training</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p3.1.2"> During the training process, the training logs and loss curves are visualized and updated in real time, allowing users to monitor the training progress. This feature provides valuable insights to analyze the fine-tuning process.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p4">
<p class="ltx_p" id="S3.SS5.p4.1"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S3.SS5.p4.1.1">Flexible Evaluation</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p4.1.2"> </span><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p4.1.3">LlamaBoard</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p4.1.4"> supports calculating the text similarity scores on the datasets to automatically evaluate the model or performing human evaluation by chatting with the model.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p5">
<p class="ltx_p" id="S3.SS5.p5.1"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S3.SS5.p5.1.1">Multilingual Support</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p5.1.2"> </span><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p5.1.3">LlamaBoard</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p5.1.4"> provides localization files, facilitating the integration of new languages for rendering the interface. Currently we support three languages: English, Russian and Chinese, which allows a broader range of users to utilize </span><span class="ltx_text ltx_font_smallcaps" id="S3.SS5.p5.1.5">LlamaBoard</span><span class="ltx_text ltx_font_bold" id="S3.SS5.p5.1.6"> for fine-tuning LLMs.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_medium" id="S4.1.1.1">4</span> </span>Empirical Study</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering ltx_font_bold"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="S4.T3.5.1.1">Table 3</span>: </span>Comparison of the training efficiency using different fine-tuning methods in <span class="ltx_text ltx_font_medium ltx_font_smallcaps" id="S4.T3.6.2">LlamaFactory</span>. The best result among GaLore, LoRA and QLoRA of each model is in bold.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.7" style="width:862.9pt;height:359.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(237.3pt,-99.0pt) scale(2.22175270402722,2.22175270402722) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.7.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.7.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt" colspan="3" id="S4.T3.7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.2.1">Gemma-2B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt" colspan="3" id="S4.T3.7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.3.1">Llama2-7B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="3" id="S4.T3.7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.4.1">Llama2-13B</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.7.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.7.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.1.1">Method</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.2.1">Memory</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.3.1">Throughput</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.7.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.4.1">PPL</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.5.1">Memory</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.6.1">Throughput</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.7.1.2.1.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.7.1">PPL</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.8.1">Memory</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.9.1">Throughput</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.2.1.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.2.1.10.1">PPL</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.3.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.3.2.1"></th>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.2.1">(GB)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.3.1">(Tokens/s)</span></td>
<td class="ltx_td ltx_border_r" id="S4.T3.7.1.3.2.4"></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.5.1">(GB)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.6.1">(Tokens/s)</span></td>
<td class="ltx_td ltx_border_r" id="S4.T3.7.1.3.2.7"></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.8.1">(GB)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.3.2.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.3.2.9.1">(Tokens/s)</span></td>
<td class="ltx_td" id="S4.T3.7.1.3.2.10"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.7.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.1.1">Baseline</span></th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.2.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.3.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.7.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.4.1">11.83</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.5.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.6.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.7.1.4.3.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.7.1">7.53</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.8.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.9.1">/</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T3.7.1.4.3.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.4.3.10.1">6.66</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.1.1">Full-tuning</span></th>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.2.1">17.06</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.3.1">3090.42</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.4.1">10.34</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.5.1">38.72</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.6.1">1334.72</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.5.4.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.7.1">5.56</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.8.1">/</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.9.1">/</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.5.4.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.5.4.10.1">/</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.1.1">Freeze-tuning</span></th>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.2.1">8.10</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.3.1">5608.49</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.6.5.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.4.1">11.33</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.5.1">15.69</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.6.1">2904.98</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.6.5.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.7.1">6.59</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.8.1">29.02</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.9.1">1841.46</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.6.5.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.6.5.10.1">6.56</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.1.1">GaLore</span></th>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.2.1">10.16</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.3.1">2483.05</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.4.1">10.38</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.5.1">15.43</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.6.1">1583.77</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.7.6.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.7.1">5.88</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.8.1">28.91</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.9.1">956.39</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.7.6.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.7.6.10.1">5.72</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.8.7.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.1.1">LoRA</span></th>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.2.1">7.91</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.3.1">3521.05</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.8.7.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.4.1">10.19</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.5.1">16.32</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.6.1">1954.07</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.7.1.8.7.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.7.1">5.81</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.8.1">30.09</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.9.1">1468.19</span></td>
<td class="ltx_td ltx_align_right" id="S4.T3.7.1.8.7.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.8.7.10.1">5.75</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.7.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.7.1.9.8.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.1.1">QLoRA</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.2"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.2.1">5.21</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.3"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.3.1">3158.59</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T3.7.1.9.8.4"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.4.1">10.46</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.5"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.5.1">7.52</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.6"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.6.1">1579.16</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T3.7.1.9.8.7"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.7.1">5.91</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.8"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.8.1">12.61</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.9"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.9.1">973.53</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T3.7.1.9.8.10"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.9.8.10.1">5.81</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering ltx_font_bold"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="S4.T4.7.1.1">Table 4</span>: </span>Comparison of the performance (in terms of ROUGE) on specific tasks using different fine-tuning methods in <span class="ltx_text ltx_font_medium ltx_font_smallcaps" id="S4.T4.8.2">LlamaFactory</span>. The best result of each model is <span class="ltx_text ltx_framed_underline" id="S4.T4.9.3">underlined</span>, and the best result of each task is in bold.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.10" style="width:862.9pt;height:239.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(171.9pt,-47.7pt) scale(1.66218396737816,1.66218396737816) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.10.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.10.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.10.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt" colspan="5" id="S4.T4.10.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.1.1.2.1">CNN / DM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt" colspan="5" id="S4.T4.10.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.1.1.3.1">XSum</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_tt" colspan="5" id="S4.T4.10.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.1.1.4.1">AdGen</span></th>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T4.10.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.2.1">Baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.3.1">FT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.4.1">GaLore</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.5.1">LoRA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T4.10.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.6.1">QLoRA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.7.1">Baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.8.1">FT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.9.1">GaLore</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.10"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.10.1">LoRA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T4.10.1.2.2.11"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.11.1">QLoRA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.12.1">Baseline</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.13.1">FT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.14.1">GaLore</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.15"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.15.1">LoRA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.10.1.2.2.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.2.2.16.1">QLoRA</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.10.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.10.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.1.1">LLaMA2-7B</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.2.1">12.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.3"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.3.1.3.1">22.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.4.1">22.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.5"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.5.1">22.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.1.3.1.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.6.1">22.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.7.1">13.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.8.1">27.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.9.1">27.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.10"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.3.1.10.1">28.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.1.3.1.11"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.11.1">28.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.12.1">0.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.13"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.3.1.13.1">20.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.14.1">19.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.15"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.15.1">20.29</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.1.3.1.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.3.1.16.1">20.45</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.10.1.4.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.1.1">Mistral-7B</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.2.1">14.39</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.3.1">22.03</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.4.1">22.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.5"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.4.2.5.1">23.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.4.2.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.6.1">23.28</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.7.1">15.87</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.8.1">23.57</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.9.1">28.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.10"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.10.1">30.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.4.2.11"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.4.2.11.1">30.44</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.12.1">7.82</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.13.1">20.14</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.14.1">20.90</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.15"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.4.2.15.1">20.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.4.2.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.4.2.16.1">20.56</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.10.1.5.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.1.1">Gemma-7B</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.2.1">15.97</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.3.1">22.07</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.4.1">/</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.5"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.5.1">22.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.5.3.6"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.5.3.6.1">22.44</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.7.1">15.31</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.8.1">25.13</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.9.1">/</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.10"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.10.1">28.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.5.3.11"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.5.3.11.1">29.02</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.12.1">11.57</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.13.1">19.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.14.1">/</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.15"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.5.3.15.1">20.62</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.5.3.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.5.3.16.1">19.81</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.10.1.6.4.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.1.1">Qwen1.5-7B</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.2.1">15.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.3.1">22.46</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.4.1">21.76</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.5"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.6.4.5.1">22.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.6.4.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.6.1">22.52</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.7.1">19.27</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.8.1">26.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.9.1">26.64</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.10"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.6.4.10.1">27.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.6.4.11"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.11.1">27.60</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.12.1">14.49</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.13.1">20.42</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.14.1">21.08</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.15"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.6.4.15.1">21.31</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.6.4.16"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.6.4.16.1">21.34</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.10.1.7.5.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.1.1">Yi-6B</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.2.1">16.85</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.3.1">22.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.4"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.4.1">22.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.5"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.7.5.5.1">22.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.7.5.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.6.1">22.97</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.7.1">18.24</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.8.1">27.09</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.9.1">28.25</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.10"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.10.1">28.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.1.7.5.11"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.7.5.11.1">29.21</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.12.1">13.34</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.13.1">19.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.14"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.14.1">20.06</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.15"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.7.5.15.1">20.97</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.1.7.5.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.7.5.16.1">20.31</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T4.10.1.8.6.1"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.1.1">ChatGLM3-6B</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.2"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.2.1">18.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.3"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.3.1">22.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.4"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.8.6.4.1">22.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.5"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.5.1">21.68</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.10.1.8.6.6"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.6.1">21.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.7"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.7.1">16.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.8"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.8.1">26.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.9"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.9.1">26.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.10"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.10.1">26.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.10.1.8.6.11"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.8.6.11.1">26.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.12"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.12.1">14.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.13"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.13.1">19.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.14"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S4.T4.10.1.8.6.14.1">20.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.15"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.15.1">20.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.10.1.8.6.16"><span class="ltx_text ltx_font_bold" id="S4.T4.10.1.8.6.16.1">20.49</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">We systematically evaluate </span><span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.2">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="S4.p1.1.3"> from two perspectives: 1) the training efficiency in terms of memory usage, throughput and perplexity. 2) the effectiveness of adaptation to downstream tasks.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_medium" id="S4.SS1.1.1.1">4.1</span> </span>Training Efficiency</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Experimental Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.1">We utilize the PubMed </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_bold">Canese and Weis</span> <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.2.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib11" title=""><span class="ltx_text ltx_font_bold">2013</span></a><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.3.2.2.1">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.4"> dataset, which comprises over 36 million records of biomedical literature. We extract around 400,000 tokens from the abstract of the literature to construct the training examples. We fine-tune the Gemma-2B </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.5.1">(</span><span class="ltx_text ltx_font_bold">Mesnard et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.6.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib53" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.7.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.8">, Llama2-7B and Llama2-13B </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.9.1">(</span><span class="ltx_text ltx_font_bold">Touvron et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.10.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib70" title=""><span class="ltx_text ltx_font_bold">2023b</span></a><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.11.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.12"> models using the generative pre-training objective with various efficient fine-tuning methods. We compare the results of full-tuning, freeze-tuning, GaLore, LoRA and 4-bit QLoRA. After fine-tuning, we calculate the perplexity on the training examples to evaluate the efficiency of different methods. We also incorporate the perplexities of the pre-trained models as baselines. More experimental details can be found in Appendix&nbsp;</span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#A4.SS1" title="D.1 Training Efficiency ‣ Appendix D Experiment Details ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">D.1</span></a><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.13">.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px2.p1.1.1">The training efficiency results are presented in Table&nbsp;</span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#S4.T3" title="Table 3 ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px2.p1.1.2">, where memory refers to the peak memory consumed during training, throughput is calculated as the number of tokens trained per second, and PPL represents the perplexity of the model on the training examples. Since full-tuning Llama2-13B lead to a memory overflow, the results are not recorded. We observe that QLoRA consistently has the lowest memory footprint because the pre-trained weights are represented in lower precision. LoRA exhibits higher throughput leveraging the optimization in LoRA layers by Unsloth. GaLore achieves lower PPL on large models while LoRA advantages on smaller ones.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_medium" id="S4.SS2.1.1.1">4.2</span> </span>Fine-Tuning on Downstream Tasks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Experimental Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.1">To evaluate the effectiveness of different efficient fine-tuning methods, we compare the performance of various models after fine-tuning on downstream tasks. We construct the training set and test set using 2,000 examples and 1,000 examples from three representative text generation tasks, including CNN/DM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.2.1">(</span><span class="ltx_text ltx_font_bold">Nallapati et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib55" title=""><span class="ltx_text ltx_font_bold">2016</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.5">, XSum </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.6.1">(</span><span class="ltx_text ltx_font_bold">Narayan et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.7.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib56" title=""><span class="ltx_text ltx_font_bold">2018</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.8.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.9"> and AdGen </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.10.1">(</span><span class="ltx_text ltx_font_bold">Shao et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.11.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib64" title=""><span class="ltx_text ltx_font_bold">2019</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.12.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.13">, respectively. We select several instruction-tuned models and fine-tune them following the sequence-to-sequence task using different fine-tuning methods. We compare the results of full-tuning (FT), GaLore, LoRA and 4-bit QLoRA. After fine-tuning, we calculate the ROUGE score </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.14.1">(</span><span class="ltx_text ltx_font_bold">Lin</span><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.15.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib48" title=""><span class="ltx_text ltx_font_bold">2004</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.16.3">)</span></cite><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.17"> on the test set of each task. We also incorporate the scores of the original instruction-tuned models as baselines. More experimental details can be found in Appendix&nbsp;</span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#A4.SS2" title="D.2 Fine-Tuning on Downstream Tasks ‣ Appendix D Experiment Details ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">D.2</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px1.p1.1.18">.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px2.p1.1.1">The evaluation results on downstream tasks are shown in Table&nbsp;</span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#S4.T4" title="Table 4 ‣ 4 Empirical Study ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px2.p1.1.2">. We report the averaged scores over ROUGE-1, ROUGE-2 and ROUGE-L for each LLM and each dataset. Some results of Gemma-7B are not included in the table because the GaLore method is not applicable to this model. An interesting finding from the results is that LoRA and QLoRA achieve the best performance in most cases, except for the Llama2-7B and ChatGLM3-6B models on the CNN/DM and AdGen datasets. This phenomenon highlights the effectiveness of these efficient fine-tuning methods in adapting LLM models to specific tasks. Additionally, we observe that the Mistral-7B model performs better on English datasets while the Qwen1.5-7B model achieves higher scores on Chinese dataset. These results suggest that the performance of the fine-tuned models is also associated with their inherent capabilities on specific languages.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_medium" id="S5.1.1.1">5</span> </span>Conclusion and Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">In this paper, we demonstrate </span><span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.2">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="S5.p1.1.3">, a unified framework for the efficient fine-tuning of LLMs. Through a modular design, we minimize dependencies between the models, datasets and training methods and provide an integrated approach to fine-tune over 100 LLMs with a diverse range of efficient fine-tuning techniques. Additionally, we offer a flexible web UI </span><span class="ltx_text ltx_font_smallcaps" id="S5.p1.1.4">LlamaBoard</span><span class="ltx_text ltx_font_bold" id="S5.p1.1.5">, enabling customized fine-tuning and evaluation of LLMs without coding efforts. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">In the future, we will consistently keep </span><span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.2">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="S5.p2.1.3"> synchronous with the state-of-the-art models and efficient fine-tuning techniques. We also welcome contributions from the open-source community. In future versions, we will explore more advanced parallel training strategies and multimodal efficient fine-tuning of LLMs.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_bold ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text ltx_font_medium" id="S6.1.1.1">6</span> </span>Broader Impact and Responsible Use</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="S6.p1.1.2"> has attracted a large number of individuals interested in LLMs to explore the possibility of fine-tuning their own models. This contributes significantly to the growth of the open-source community. It is gaining increasing attention and is being featured in Awesome Transformers</span><span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/transformers/blob/v4.39.0/awesome-transformers.md" title="">https://github.com/huggingface/transformers/blob/v4.39.0/awesome-transformers.md</a></span></span></span><span class="ltx_text ltx_font_bold" id="S6.p1.1.3"> as a representative of efficient fine-tuning frameworks for LLMs. We anticipate that practitioners build their LLMs upon our framework that bring benefits to society. Adherence to the model license is mandatory when using </span><span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.4">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="S6.p1.1.5"> for fine-tuning LLMs, thus preventing from any potential misuse.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_font_bold ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib1.5.5.1">Abid et&nbsp;al. (2019)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib1.7.1">
Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/1906.02569" title="">Gradio: Hassle-free sharing and testing of ml models in the wild</a><span class="ltx_text ltx_font_bold" id="bib.bib1.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib1.9.1">arXiv preprint arXiv:1906.02569</em><span class="ltx_text ltx_font_bold" id="bib.bib1.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib2.4.4.1">AI (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib2.6.1">
Lightning AI. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/Lightning-AI/lit-gpt" title="">Lit-GPT</a><span class="ltx_text ltx_font_bold" id="bib.bib2.7.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib3.5.5.1">Almazrouei et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib3.7.1">
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2311.16867" title="">The falcon series of open language models</a><span class="ltx_text ltx_font_bold" id="bib.bib3.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib3.9.1">arXiv preprint arXiv:2311.16867</em><span class="ltx_text ltx_font_bold" id="bib.bib3.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib4.5.5.1">Anand et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib4.7.1">
Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/nomic-ai/gpt4all" title="">GPT4All: Training an assistant-style chatbot with large scale data distillation from GPT-3.5-turbo</a><span class="ltx_text ltx_font_bold" id="bib.bib4.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib5.4.4.1">Apache (2016)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib5.6.1">
Apache. 2016.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/apache/arrow" title="">Arrow</a><span class="ltx_text ltx_font_bold" id="bib.bib5.7.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib6.5.5.1">Bai et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib6.7.1">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2309.16609" title="">Qwen technical report</a><span class="ltx_text ltx_font_bold" id="bib.bib6.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib6.9.1">arXiv preprint arXiv:2309.16609</em><span class="ltx_text ltx_font_bold" id="bib.bib6.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib7.5.5.1">Beeching et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib7.7.1">
Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" title="">Open LLM leaderboard</a><span class="ltx_text ltx_font_bold" id="bib.bib7.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib8.5.5.1">Ben&nbsp;Zaken et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib8.7.1">
Elad Ben&nbsp;Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2022.acl-short.1" title="">BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</a><span class="ltx_text ltx_font_bold" id="bib.bib8.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib8.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib8.10.2">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em><span class="ltx_text ltx_font_bold" id="bib.bib8.11.3">, pages 1–9, Dublin, Ireland. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib9.5.5.1">Bhardwaj et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib9.7.1">
Rishabh Bhardwaj, Do&nbsp;Duc Anh, and Soujanya Poria. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.11746" title="">Language models are homer simpson! safety re-alignment of fine-tuned language models through task arithmetic</a><span class="ltx_text ltx_font_bold" id="bib.bib9.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib9.9.1">arXiv preprint arXiv:2402.11746</em><span class="ltx_text ltx_font_bold" id="bib.bib9.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib10.5.5.1">Bi et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib10.7.1">
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2401.02954" title="">DeepSeek LLM: Scaling open-source language models with longtermism</a><span class="ltx_text ltx_font_bold" id="bib.bib10.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib10.9.1">arXiv preprint arXiv:2401.02954</em><span class="ltx_text ltx_font_bold" id="bib.bib10.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib11.4.4.1">Canese and Weis (2013)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib11.6.1">
Kathi Canese and Sarah Weis. 2013.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://www.be-md.ncbi.nlm.nih.gov/books/NBK153385/pdf/Bookshelf_NBK153385.pdf" title="">PubMed: the bibliographic database</a><span class="ltx_text ltx_font_bold" id="bib.bib11.7.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib11.8.1">The NCBI handbook</em><span class="ltx_text ltx_font_bold" id="bib.bib11.9.2">, 2(1).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib12.5.5.1">Chen et&nbsp;al. (2024a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib12.7.1">
Du&nbsp;Chen, Yi&nbsp;Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, and Kun Han. 2024a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2401.12246" title="">Orion-14b: Open-source multilingual large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib12.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib12.9.1">arXiv preprint arXiv:2401.12246</em><span class="ltx_text ltx_font_bold" id="bib.bib12.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib13.5.5.1">Chen et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib13.7.1">
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2306.15595" title="">Extending context window of large language models via positional interpolation</a><span class="ltx_text ltx_font_bold" id="bib.bib13.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib13.9.1">arXiv preprint arXiv:2306.15595</em><span class="ltx_text ltx_font_bold" id="bib.bib13.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib14.5.5.1">Chen et&nbsp;al. (2016)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib14.7.1">
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/1604.06174" title="">Training deep nets with sublinear memory cost</a><span class="ltx_text ltx_font_bold" id="bib.bib14.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib14.9.1">arXiv preprint arXiv:1604.06174</em><span class="ltx_text ltx_font_bold" id="bib.bib14.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib15.5.5.1">Chen et&nbsp;al. (2024b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib15.7.1">
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2024b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=6PmJoRfdaK" title="">LongLoRA: Efficient fine-tuning of long-context large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib15.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib15.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib15.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib15.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib16.5.5.1">Cui et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib16.7.1">
Yiming Cui, Ziqing Yang, and Xin Yao. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2304.08177" title="">Efficient and effective text encoding for chinese llama and alpaca</a><span class="ltx_text ltx_font_bold" id="bib.bib16.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib16.9.1">arXiv preprint arXiv:2304.08177</em><span class="ltx_text ltx_font_bold" id="bib.bib16.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib17.5.5.1">Dai et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib17.7.1">
Damai Dai, Chengqi Deng, Chenggang Zhao, RX&nbsp;Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y&nbsp;Wu, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2401.06066" title="">DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models</a><span class="ltx_text ltx_font_bold" id="bib.bib17.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib17.9.1">arXiv preprint arXiv:2401.06066</em><span class="ltx_text ltx_font_bold" id="bib.bib17.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib18.5.5.1">Dao et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib18.7.1">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html" title="">Flashattention: Fast and memory-efficient exact attention with io-awareness</a><span class="ltx_text ltx_font_bold" id="bib.bib18.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib18.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib18.10.2">, 35:16344–16359.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib19.4.4.1">Dettmers (2021)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib19.6.1">
Tim Dettmers. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/TimDettmers/bitsandbytes" title="">Bitsandbytes</a><span class="ltx_text ltx_font_bold" id="bib.bib19.7.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib20.5.5.1">Dettmers et&nbsp;al. (2022a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib20.7.1">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/c3ba4962c05c49636d4c6206a97e9c8a-Abstract-Conference.html" title="">GPT3.int8(): 8-bit matrix multiplication for transformers at scale</a><span class="ltx_text ltx_font_bold" id="bib.bib20.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib20.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib20.10.2">, 35:30318–30332.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib21.5.5.1">Dettmers et&nbsp;al. (2022b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib21.7.1">
</span><span class="ltx_text ltx_font_bold" id="bib.bib21.8.2">Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2022b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=shpkpVXzo3h" title="">8-bit optimizers via block-wise quantization</a><span class="ltx_text ltx_font_bold" id="bib.bib21.9.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib21.10.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib21.11.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib21.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib22.5.5.1">Dettmers et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib22.7.1">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html" title="">QLoRA: Efficient finetuning of quantized llms</a><span class="ltx_text ltx_font_bold" id="bib.bib22.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib22.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib22.10.2">, 36:10088–10115.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib23.5.5.1">Diao et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib23.7.1">
Shizhe Diao, Rui Pan, Hanze Dong, Ka&nbsp;Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2306.12420" title="">LMFlow: An extensible toolkit for finetuning and inference of large foundation models</a><span class="ltx_text ltx_font_bold" id="bib.bib23.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib23.9.1">arXiv preprint arXiv:2306.12420</em><span class="ltx_text ltx_font_bold" id="bib.bib23.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib24.5.5.1">Du et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib24.7.1">
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2022.acl-long.26" title="">GLM: General language model pretraining with autoregressive blank infilling</a><span class="ltx_text ltx_font_bold" id="bib.bib24.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib24.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib24.10.2">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em><span class="ltx_text ltx_font_bold" id="bib.bib24.11.3">, pages 320–335, Dublin, Ireland. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib25.5.5.1">Egiazarian et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib25.7.1">
Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2401.06118" title="">Extreme compression of large language models via additive quantization</a><span class="ltx_text ltx_font_bold" id="bib.bib25.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib25.9.1">arXiv preprint arXiv:2401.06118</em><span class="ltx_text ltx_font_bold" id="bib.bib25.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib26.5.5.1">Frantar et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib26.7.1">
</span><span class="ltx_text ltx_font_bold" id="bib.bib26.8.2">Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=tcbBPnfwxS" title="">GPTQ: Accurate post-training quantization for generative pre-trained transformers</a><span class="ltx_text ltx_font_bold" id="bib.bib26.9.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib26.10.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib26.11.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib26.12.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib27.5.5.1">Groeneveld et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib27.7.1">
Dirk Groeneveld, Iz&nbsp;Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya&nbsp;Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.00838" title="">OLMo: Accelerating the science of language models</a><span class="ltx_text ltx_font_bold" id="bib.bib27.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib27.9.1">arXiv preprint arXiv:2402.00838</em><span class="ltx_text ltx_font_bold" id="bib.bib27.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib28.5.5.1">Guo et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib28.7.1">
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y&nbsp;Wu, YK&nbsp;Li, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2401.14196" title="">DeepSeek-Coder: When the large language model meets programming – the rise of code intelligence</a><span class="ltx_text ltx_font_bold" id="bib.bib28.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib28.9.1">arXiv preprint arXiv:2401.14196</em><span class="ltx_text ltx_font_bold" id="bib.bib28.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib29.4.4.1">Han and Han (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib29.6.1">
Daniel Han and Michael Han. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/unslothai/unsloth" title="">unsloth</a><span class="ltx_text ltx_font_bold" id="bib.bib29.7.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib30.5.5.1">Hayou et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib30.7.1">
Soufiane Hayou, Nikhil Ghosh, and Bin Yu. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.12354" title="">LoRA+: Efficient low rank adaptation of large models</a><span class="ltx_text ltx_font_bold" id="bib.bib30.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib30.9.1">arXiv preprint arXiv:2402.12354</em><span class="ltx_text ltx_font_bold" id="bib.bib30.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib31.5.5.1">Hendrycks et&nbsp;al. (2021)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib31.7.1">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=d7KBjmI3GmQ" title="">Measuring massive multitask language understanding</a><span class="ltx_text ltx_font_bold" id="bib.bib31.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib31.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib31.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib31.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib32.5.5.1">Houlsby et&nbsp;al. (2019)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib32.7.1">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De&nbsp;Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.mlr.press/v97/houlsby19a.html" title="">Parameter-efficient transfer learning for nlp</a><span class="ltx_text ltx_font_bold" id="bib.bib32.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib32.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib32.10.2">International conference on machine learning</em><span class="ltx_text ltx_font_bold" id="bib.bib32.11.3">, pages 2790–2799. PMLR.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib33.5.5.1">Hu et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib33.7.1">
Edward&nbsp;J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu&nbsp;Wang, Weizhu Chen, et&nbsp;al. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">LoRA: Low-rank adaptation of large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib33.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib33.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib33.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib33.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib34.5.5.1">Huang et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib34.7.1">
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/c6ec1844bec96d6d32ae95ae694e23d8-Abstract-Datasets_and_Benchmarks.html" title="">C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models</a><span class="ltx_text ltx_font_bold" id="bib.bib34.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib34.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib34.10.2">, 36.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib35.5.5.1">Jiang et&nbsp;al. (2023a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib35.7.1">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al. 2023a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2310.06825" title="">Mistral 7b</a><span class="ltx_text ltx_font_bold" id="bib.bib35.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib35.9.1">arXiv preprint arXiv:2310.06825</em><span class="ltx_text ltx_font_bold" id="bib.bib35.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib36.5.5.1">Jiang et&nbsp;al. (2023b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib36.7.1">
Jinhao Jiang, Kun Zhou, Wayne&nbsp;Xin Zhao, Yaliang Li, and Ji-Rong Wen. 2023b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2023.emnlp-main.228" title="">ReasoningLM: Enabling structural subgraph reasoning in pre-trained language models for question answering over knowledge graph</a><span class="ltx_text ltx_font_bold" id="bib.bib36.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib36.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib36.10.2">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text ltx_font_bold" id="bib.bib36.11.3">, pages 3721–3735, Singapore. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib37.5.5.1">Jiao et&nbsp;al. (2023a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib37.7.1">
Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2023.findings-emnlp.1001" title="">ParroT: Translating during chat using large language models tuned with human translation and feedback</a><span class="ltx_text ltx_font_bold" id="bib.bib37.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib37.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib37.10.2">Findings of the Association for Computational Linguistics: EMNLP 2023</em><span class="ltx_text ltx_font_bold" id="bib.bib37.11.3">, pages 15009–15020, Singapore. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib38.5.5.1">Jiao et&nbsp;al. (2023b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib38.7.1">
Yizhu Jiao, Ming Zhong, Sha Li, Ruining Zhao, Siru Ouyang, Heng Ji, and Jiawei Han. 2023b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2023.emnlp-main.620" title="">Instruct and extract: Instruction tuning for on-demand information extraction</a><span class="ltx_text ltx_font_bold" id="bib.bib38.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib38.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib38.10.2">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text ltx_font_bold" id="bib.bib38.11.3">, pages 10030–10051, Singapore. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib39.4.4.1">Kalajdzievski (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib39.6.1">
Damjan Kalajdzievski. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2312.03732" title="">A rank stabilization scaling factor for fine-tuning with LoRA</a><span class="ltx_text ltx_font_bold" id="bib.bib39.7.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib39.8.1">arXiv preprint arXiv:2312.03732</em><span class="ltx_text ltx_font_bold" id="bib.bib39.9.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib40.5.5.1">Kim et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib40.7.1">
Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2312.15166" title="">SOLAR 10.7B: Scaling large language models with simple yet effective depth up-scaling</a><span class="ltx_text ltx_font_bold" id="bib.bib40.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib40.9.1">arXiv preprint arXiv:2312.15166</em><span class="ltx_text ltx_font_bold" id="bib.bib40.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib41.5.5.1">Krell et&nbsp;al. (2021)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib41.7.1">
Mario&nbsp;Michael Krell, Matej Kosec, Sergio&nbsp;P Perez, and Andrew Fitzgibbon. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2107.02027" title="">Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance</a><span class="ltx_text ltx_font_bold" id="bib.bib41.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib41.9.1">arXiv preprint arXiv:2107.02027</em><span class="ltx_text ltx_font_bold" id="bib.bib41.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib42.5.5.1">Kwon et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib42.7.1">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.1145/3600006.3613165" title="">Efficient memory management for large language model serving with PagedAttention</a><span class="ltx_text ltx_font_bold" id="bib.bib42.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib42.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib42.10.2">Proceedings of the 29th Symposium on Operating Systems Principles</em><span class="ltx_text ltx_font_bold" id="bib.bib42.11.3">, pages 611–626.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib43.5.5.1">Le&nbsp;Scao et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib43.7.1">
Teven Le&nbsp;Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni, François Yvon, Matthias Gallé, et&nbsp;al. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2211.05100" title="">BLOOM: A 176b-parameter open-access multilingual language model</a><span class="ltx_text ltx_font_bold" id="bib.bib43.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib43.9.1">arXiv preprint arXiv:2211.05100</em><span class="ltx_text ltx_font_bold" id="bib.bib43.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib44.5.5.1">Lhoest et&nbsp;al. (2021)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib44.7.1">
Quentin Lhoest, Albert&nbsp;Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et&nbsp;al. 2021.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2021.emnlp-demo.21" title="">Datasets: A community library for natural language processing</a><span class="ltx_text ltx_font_bold" id="bib.bib44.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib44.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib44.10.2">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em><span class="ltx_text ltx_font_bold" id="bib.bib44.11.3">, pages 175–184.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib45.5.5.1">Li et&nbsp;al. (2023a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib45.7.1">
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2306.09212" title="">CMMLU: Measuring massive multitask language understanding in chinese</a><span class="ltx_text ltx_font_bold" id="bib.bib45.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib45.9.1">arXiv preprint arXiv:2306.09212</em><span class="ltx_text ltx_font_bold" id="bib.bib45.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib46.5.5.1">Li et&nbsp;al. (2023b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib46.7.1">
Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen Huang, Yuliang Liu, Boxiang Wang, and Yang You. 2023b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.1145/3605573.3605613" title="">Colossal-AI: A unified deep learning system for large-scale parallel training</a><span class="ltx_text ltx_font_bold" id="bib.bib46.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib46.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib46.10.2">Proceedings of the 52nd International Conference on Parallel Processing</em><span class="ltx_text ltx_font_bold" id="bib.bib46.11.3">, pages 766–775.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib47.5.5.1">Li et&nbsp;al. (2023c)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib47.7.1">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del&nbsp;Giorno, Suriya Gunasekar, and Yin&nbsp;Tat Lee. 2023c.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2309.05463" title="">Textbooks are all you need ii: phi-1.5 technical report</a><span class="ltx_text ltx_font_bold" id="bib.bib47.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib47.9.1">arXiv preprint arXiv:2309.05463</em><span class="ltx_text ltx_font_bold" id="bib.bib47.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib48.4.4.1">Lin (2004)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib48.6.1">
Chin-Yew Lin. 2004.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://aclanthology.org/W04-1013/" title="">ROUGE: A package for automatic evaluation of summaries</a><span class="ltx_text ltx_font_bold" id="bib.bib48.7.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib48.8.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib48.9.2">Text Summarization Branches Out</em><span class="ltx_text ltx_font_bold" id="bib.bib48.10.3">, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib49.5.5.1">Lin et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib49.7.1">
Ji&nbsp;Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2306.00978" title="">AWQ: Activation-aware weight quantization for llm compression and acceleration</a><span class="ltx_text ltx_font_bold" id="bib.bib49.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib49.9.1">arXiv preprint arXiv:2306.00978</em><span class="ltx_text ltx_font_bold" id="bib.bib49.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib50.5.5.1">Liu et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib50.7.1">
Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang&nbsp;Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.09353" title="">DoRA: Weight-decomposed low-rank adaptation</a><span class="ltx_text ltx_font_bold" id="bib.bib50.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib50.9.1">arXiv preprint arXiv:2402.09353</em><span class="ltx_text ltx_font_bold" id="bib.bib50.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib51.5.5.1">Lozhkov et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib51.7.1">
Anton Lozhkov, Raymond Li, Loubna&nbsp;Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao&nbsp;Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.19173" title="">StarCoder 2 and The Stack v2: The next generation</a><span class="ltx_text ltx_font_bold" id="bib.bib51.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib51.9.1">arXiv preprint arXiv:2402.19173</em><span class="ltx_text ltx_font_bold" id="bib.bib51.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib52.5.5.1">Mangrulkar et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib52.7.1">
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/huggingface/peft" title="">PEFT: State-of-the-art parameter-efficient fine-tuning methods</a><span class="ltx_text ltx_font_bold" id="bib.bib52.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib53.5.5.1">Mesnard et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib53.7.1">
Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir&nbsp;Sanjay Kale, Juliette Love, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2403.08295" title="">Gemma: Open models based on gemini research and technology</a><span class="ltx_text ltx_font_bold" id="bib.bib53.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib53.9.1">arXiv preprint arXiv:2403.08295</em><span class="ltx_text ltx_font_bold" id="bib.bib53.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib54.5.5.1">Micikevicius et&nbsp;al. (2018)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib54.7.1">
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et&nbsp;al. 2018.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=r1gs9JgRZ" title="">Mixed precision training</a><span class="ltx_text ltx_font_bold" id="bib.bib54.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib54.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib54.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib54.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib55.5.5.1">Nallapati et&nbsp;al. (2016)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib55.7.1">
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/K16-1028" title="">Abstractive text summarization using sequence-to-sequence rnns and beyond</a><span class="ltx_text ltx_font_bold" id="bib.bib55.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib55.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib55.10.2">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</em><span class="ltx_text ltx_font_bold" id="bib.bib55.11.3">, pages 280–290.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib56.5.5.1">Narayan et&nbsp;al. (2018)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib56.7.1">
Shashi Narayan, Shay&nbsp;B. Cohen, and Mirella Lapata. 2018.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/D18-1206" title="">Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</a><span class="ltx_text ltx_font_bold" id="bib.bib56.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib56.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib56.10.2">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text ltx_font_bold" id="bib.bib56.11.3">, pages 1797–1807, Brussels, Belgium. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib57.5.5.1">Nguyen et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib57.7.1">
Duc&nbsp;Q. Nguyen, Sang&nbsp;T. Truong, Toan D.&nbsp;V. Nguyen, Dong&nbsp;D. Le, Nhi&nbsp;N. Truong, Tho Quan, and Sanmi Koyejo. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://huggingface.co/ura-hcmut/GemSUra-7B" title="">Crossing linguistic horizons: Finetuning and comprehensive evaluation of vietnamese large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib57.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib58.5.5.1">Ouyang et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib58.7.1">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et&nbsp;al. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html" title="">Training language models to follow instructions with human feedback</a><span class="ltx_text ltx_font_bold" id="bib.bib58.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib58.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib58.10.2">, 35:27730–27744.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib59.5.5.1">Papineni et&nbsp;al. (2002)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib59.7.1">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://aclanthology.org/P02-1040.pdf" title="">BLEU: a method for automatic evaluation of machine translation</a><span class="ltx_text ltx_font_bold" id="bib.bib59.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib59.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib59.10.2">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em><span class="ltx_text ltx_font_bold" id="bib.bib59.11.3">, pages 311–318.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib60.5.5.1">Paszke et&nbsp;al. (2019)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib60.7.1">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et&nbsp;al. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html" title="">PyTorch: An imperative style, high-performance deep learning library</a><span class="ltx_text ltx_font_bold" id="bib.bib60.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib60.9.1">Advances in neural information processing systems</em><span class="ltx_text ltx_font_bold" id="bib.bib60.10.2">, 32.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib61.5.5.1">Radford et&nbsp;al. (2018)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib61.7.1">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et&nbsp;al. 2018.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" title="">Improving language understanding by generative pre-training</a><span class="ltx_text ltx_font_bold" id="bib.bib61.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib61.9.1">OpenAI blog</em><span class="ltx_text ltx_font_bold" id="bib.bib61.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib62.5.5.1">Rafailov et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib62.7.1">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher&nbsp;D Manning, and Chelsea Finn. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=HPuSIXJaa9" title="">Direct preference optimization: Your language model is secretly a reward model</a><span class="ltx_text ltx_font_bold" id="bib.bib62.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib62.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib62.10.2">, 37.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib63.5.5.1">Rasley et&nbsp;al. (2020)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib63.7.1">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.1145/3394486.3406703" title="">DeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters</a><span class="ltx_text ltx_font_bold" id="bib.bib63.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib63.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib63.10.2">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em><span class="ltx_text ltx_font_bold" id="bib.bib63.11.3">, pages 3505–3506.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib64.5.5.1">Shao et&nbsp;al. (2019)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib64.7.1">
Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/D19-1321" title="">Long and diverse text generation with planning-based hierarchical variational model</a><span class="ltx_text ltx_font_bold" id="bib.bib64.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib64.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib64.10.2">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em><span class="ltx_text ltx_font_bold" id="bib.bib64.11.3">, pages 3257–3268, Hong Kong, China. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib65.5.5.1">Shao et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib65.7.1">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK&nbsp;Li, Y&nbsp;Wu, and Daya Guo. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2402.03300" title="">DeepSeekMath: Pushing the limits of mathematical reasoning in open language models</a><span class="ltx_text ltx_font_bold" id="bib.bib65.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib65.9.1">arXiv preprint arXiv:2402.03300</em><span class="ltx_text ltx_font_bold" id="bib.bib65.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib66.5.5.1">Taori et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib66.7.1">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/tatsu-lab/stanford_alpaca" title="">Stanford alpaca: An instruction-following llama model</a><span class="ltx_text ltx_font_bold" id="bib.bib66.8.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib67.4.4.1">Team (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib67.6.1">
InternLM Team. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf" title="">InternLM: A multilingual language model with progressively enhanced capabilities</a><span class="ltx_text ltx_font_bold" id="bib.bib67.7.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib68.5.5.1">Tillet et&nbsp;al. (2019)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib68.7.1">
Philippe Tillet, Hsiang-Tsung Kung, and David Cox. 2019.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.1145/3315508.3329973" title="">Triton: an intermediate language and compiler for tiled neural network computations</a><span class="ltx_text ltx_font_bold" id="bib.bib68.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib68.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib68.10.2">Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</em><span class="ltx_text ltx_font_bold" id="bib.bib68.11.3">, pages 10–19.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib69.5.5.1">Touvron et&nbsp;al. (2023a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib69.7.1">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2302.13971" title="">LLaMA: Open and efficient foundation language models</a><span class="ltx_text ltx_font_bold" id="bib.bib69.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib69.9.1">arXiv preprint arXiv:2302.13971</em><span class="ltx_text ltx_font_bold" id="bib.bib69.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib70.5.5.1">Touvron et&nbsp;al. (2023b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib70.7.1">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al. 2023b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a><span class="ltx_text ltx_font_bold" id="bib.bib70.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib70.9.1">arXiv preprint arXiv:2307.09288</em><span class="ltx_text ltx_font_bold" id="bib.bib70.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib71.5.5.1">Tunstall et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib71.7.1">
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2310.16944" title="">Zephyr: Direct distillation of LM alignment</a><span class="ltx_text ltx_font_bold" id="bib.bib71.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib71.9.1">arXiv preprint arXiv:2310.16944</em><span class="ltx_text ltx_font_bold" id="bib.bib71.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib72.5.5.1">von Werra et&nbsp;al. (2020)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib72.7.1">
</span><span class="ltx_text ltx_font_bold" id="bib.bib72.8.2">Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://github.com/huggingface/trl" title="">TRL: Transformer reinforcement learning</a><span class="ltx_text ltx_font_bold" id="bib.bib72.9.1">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib73.5.5.1">Wang et&nbsp;al. (2023a)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib73.7.1">
Chenglong Wang, Hang Zhou, Yimin Hu, Yifu Huo, Bei Li, Tongran Liu, Tong Xiao, and Jingbo Zhu. 2023a.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2308.02223" title="">Esrl: Efficient sampling-based reinforcement learning for sequence generation</a><span class="ltx_text ltx_font_bold" id="bib.bib73.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib73.9.1">arXiv preprint arXiv:2308.02223</em><span class="ltx_text ltx_font_bold" id="bib.bib73.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib74.5.5.1">Wang et&nbsp;al. (2023b)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib74.7.1">
Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023b.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2309.11235" title="">OpenChat: Advancing open-source language models with mixed-quality data</a><span class="ltx_text ltx_font_bold" id="bib.bib74.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib74.9.1">arXiv preprint arXiv:2309.11235</em><span class="ltx_text ltx_font_bold" id="bib.bib74.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib75.5.5.1">Wang et&nbsp;al. (2023c)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib75.7.1">
Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023c.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2023.emnlp-main.1036" title="">Document-level machine translation with large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib75.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib75.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib75.10.2">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em><span class="ltx_text ltx_font_bold" id="bib.bib75.11.3">, pages 16646–16661, Singapore. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib76.5.5.1">Wang et&nbsp;al. (2023d)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib76.7.1">
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah&nbsp;A Smith, Iz&nbsp;Beltagy, et&nbsp;al. 2023d.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=w4zZNC4ZaV" title="">How far can camels go? exploring the state of instruction tuning on open resources</a><span class="ltx_text ltx_font_bold" id="bib.bib76.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib76.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib76.10.2">, 36.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib77.5.5.1">Wei et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib77.7.1">
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams&nbsp;Wei Yu, Brian Lester, Nan Du, Andrew&nbsp;M Dai, and Quoc&nbsp;V Le. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=gEZrGCozdqR" title="">Finetuned language models are zero-shot learners</a><span class="ltx_text ltx_font_bold" id="bib.bib77.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib77.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib77.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib77.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib78.5.5.1">Wei et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib78.7.1">
Tianwen Wei, Liang Zhao, Lichang Zhang, Bo&nbsp;Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2310.19341" title="">Skywork: A more open bilingual foundation model</a><span class="ltx_text ltx_font_bold" id="bib.bib78.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib78.9.1">arXiv preprint arXiv:2310.19341</em><span class="ltx_text ltx_font_bold" id="bib.bib78.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib79.5.5.1">Wolf et&nbsp;al. (2020)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib79.7.1">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le&nbsp;Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://doi.org/10.18653/v1/2020.emnlp-demos.6" title="">Transformers: State-of-the-art natural language processing</a><span class="ltx_text ltx_font_bold" id="bib.bib79.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib79.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib79.10.2">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em><span class="ltx_text ltx_font_bold" id="bib.bib79.11.3">, pages 38–45, Online. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib80.5.5.1">Wu et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib80.7.1">
Shaohua Wu, Xudong Zhao, Shenling Wang, Jiangang Luo, Lingjun Li, Xi&nbsp;Chen, Bing Zhao, Wei Wang, Tong Yu, Rongguo Zhang, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2311.15786" title="">YUAN 2.0: A large language model with localized filtering-based attention</a><span class="ltx_text ltx_font_bold" id="bib.bib80.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib80.9.1">arXiv preprint arXiv:2311.15786</em><span class="ltx_text ltx_font_bold" id="bib.bib80.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib81.5.5.1">Yang et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib81.7.1">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce&nbsp;Bian, Chao Yin, Chenxu Lv, Da&nbsp;Pan, Dian Wang, Dong Yan, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2309.10305" title="">Baichuan 2: Open large-scale language models</a><span class="ltx_text ltx_font_bold" id="bib.bib81.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib81.9.1">arXiv preprint arXiv:2309.10305</em><span class="ltx_text ltx_font_bold" id="bib.bib81.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib82.5.5.1">Yao et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib82.7.1">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik&nbsp;R Narasimhan, and Yuan Cao. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=WE_vluYUL-X" title="">ReAct: Synergizing reasoning and acting in language models</a><span class="ltx_text ltx_font_bold" id="bib.bib82.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib82.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib82.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib82.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib83.5.5.1">Young et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib83.7.1">
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge&nbsp;Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et&nbsp;al. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2403.04652" title="">Yi: Open foundation models by 01.ai</a><span class="ltx_text ltx_font_bold" id="bib.bib83.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib83.9.1">arXiv preprint arXiv:2403.04652</em><span class="ltx_text ltx_font_bold" id="bib.bib83.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib84.5.5.1">Yu et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib84.7.1">
Hao Yu, Zachary Yang, Kellin Pelrine, Jean&nbsp;Francois Godbout, and Reihaneh Rabbany. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2308.10092" title="">Open, closed, or small language models for text classification?</a><span class="ltx_text ltx_font_bold" id="bib.bib84.8.1">
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib84.9.1">arXiv preprint arXiv:2308.10092</em><span class="ltx_text ltx_font_bold" id="bib.bib84.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib85.5.5.1">Zhang et&nbsp;al. (2022)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib85.7.1">
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu&nbsp;Qiao. 2022.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://openreview.net/forum?id=d4UiXAHN2W" title="">LLaMA-Adapter: Efficient fine-tuning of language models with zero-init attention</a><span class="ltx_text ltx_font_bold" id="bib.bib85.8.1">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib85.9.1">In </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib85.10.2">International Conference on Learning Representations</em><span class="ltx_text ltx_font_bold" id="bib.bib85.11.3">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib86.5.5.1">Zhao et&nbsp;al. (2024)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib86.7.1">
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. 2024.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2403.03507" title="">GaLore: Memory-efficient llm training by gradient low-rank projection</a><span class="ltx_text ltx_font_bold" id="bib.bib86.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib86.9.1">arXiv preprint arXiv:2403.03507</em><span class="ltx_text ltx_font_bold" id="bib.bib86.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib87.5.5.1">Zhao et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib87.7.1">
Wayne&nbsp;Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="http://arxiv.org/abs/2303.18223" title="">A survey of large language models</a><span class="ltx_text ltx_font_bold" id="bib.bib87.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib87.9.1">arXiv preprint arXiv:2303.18223</em><span class="ltx_text ltx_font_bold" id="bib.bib87.10.2">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span class="ltx_text ltx_font_bold" id="bib.bib88.5.5.1">Zheng et&nbsp;al. (2023)</span><button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_bold" id="bib.bib88.7.1">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi&nbsp;Lin, Zhuohan Li, Dacheng Li, Eric Xing, et&nbsp;al. 2023.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href ltx_font_bold" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html" title="">Judging LLM-as-a-judge with mt-bench and chatbot arena</a><span class="ltx_text ltx_font_bold" id="bib.bib88.8.1">.
</span>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="bib.bib88.9.1">Advances in Neural Information Processing Systems</em><span class="ltx_text ltx_font_bold" id="bib.bib88.10.2">, 36.
</span>
</span>
</li>
</ul>
</section>
<figure class="ltx_table" id="A0.T5">
<figcaption class="ltx_caption ltx_centering ltx_font_bold"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="A0.T5.5.1.1">Table 5</span>: </span>Comparison of features in <span class="ltx_text ltx_font_medium ltx_font_smallcaps" id="A0.T5.6.2">LlamaFactory</span> with existing LLM fine-tuning frameworks.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A0.T5.7" style="width:429.3pt;height:308.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(64.2pt,-46.1pt) scale(1.4269209217438,1.4269209217438) ;">
<table class="ltx_tabular ltx_align_middle" id="A0.T5.7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A0.T5.7.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.1.1.1.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.2"><span class="ltx_text ltx_font_smallcaps" id="A0.T5.7.1.1.1.2.1">LlamaFactory</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.1.1.3.1">FastChat</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.1.1.4.1">LitGPT</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.1.1.5.1">LMFlow</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A0.T5.7.1.1.1.6"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.1.1.6.1">Open Instruct</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.2.2.1.1">LoRA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.2"><span class="ltx_text" id="A0.T5.7.1.2.2.2.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.3"><span class="ltx_text" id="A0.T5.7.1.2.2.3.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.4"><span class="ltx_text" id="A0.T5.7.1.2.2.4.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.5"><span class="ltx_text" id="A0.T5.7.1.2.2.5.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.2.2.6"><span class="ltx_text" id="A0.T5.7.1.2.2.6.1">✓</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.3.3">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.3.3.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.3.3.1.1">QLoRA</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.3.3.2"><span class="ltx_text" id="A0.T5.7.1.3.3.2.1">✓</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.3.3.3"><span class="ltx_text" id="A0.T5.7.1.3.3.3.1">✓</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.3.3.4"><span class="ltx_text" id="A0.T5.7.1.3.3.4.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.3.3.5"></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.3.3.6"><span class="ltx_text" id="A0.T5.7.1.3.3.6.1">✓</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.4.4">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.4.4.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.4.4.1.1">LoRA+</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.4.4.2"><span class="ltx_text" id="A0.T5.7.1.4.4.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.4.4.3"></td>
<td class="ltx_td" id="A0.T5.7.1.4.4.4"></td>
<td class="ltx_td" id="A0.T5.7.1.4.4.5"></td>
<td class="ltx_td" id="A0.T5.7.1.4.4.6"></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.5.5">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.5.5.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.5.5.1.1">DoRA</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.5.5.2"><span class="ltx_text" id="A0.T5.7.1.5.5.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.5.5.3"></td>
<td class="ltx_td" id="A0.T5.7.1.5.5.4"></td>
<td class="ltx_td" id="A0.T5.7.1.5.5.5"></td>
<td class="ltx_td" id="A0.T5.7.1.5.5.6"></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.6.6">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.6.6.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.6.6.1.1">GaLore</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.6.6.2"><span class="ltx_text" id="A0.T5.7.1.6.6.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.6.6.3"></td>
<td class="ltx_td" id="A0.T5.7.1.6.6.4"></td>
<td class="ltx_td" id="A0.T5.7.1.6.6.5"></td>
<td class="ltx_td" id="A0.T5.7.1.6.6.6"></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.7.7.1.1">SFT</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.2"><span class="ltx_text" id="A0.T5.7.1.7.7.2.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.3"><span class="ltx_text" id="A0.T5.7.1.7.7.3.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.4"><span class="ltx_text" id="A0.T5.7.1.7.7.4.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.5"><span class="ltx_text" id="A0.T5.7.1.7.7.5.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.7.7.6"><span class="ltx_text" id="A0.T5.7.1.7.7.6.1">✓</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.8.8">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.8.8.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.8.8.1.1">RLHF</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.8.8.2"><span class="ltx_text" id="A0.T5.7.1.8.8.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.8.8.3"></td>
<td class="ltx_td" id="A0.T5.7.1.8.8.4"></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.8.8.5"><span class="ltx_text" id="A0.T5.7.1.8.8.5.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.8.8.6"></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.9.9">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.9.9.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.9.9.1.1">DPO</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.9.9.2"><span class="ltx_text" id="A0.T5.7.1.9.9.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.9.9.3"></td>
<td class="ltx_td" id="A0.T5.7.1.9.9.4"></td>
<td class="ltx_td" id="A0.T5.7.1.9.9.5"></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.9.9.6"><span class="ltx_text" id="A0.T5.7.1.9.9.6.1">✓</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.10.10.1.1">Flash attention</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.2"><span class="ltx_text" id="A0.T5.7.1.10.10.2.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.3"><span class="ltx_text" id="A0.T5.7.1.10.10.3.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.4"><span class="ltx_text" id="A0.T5.7.1.10.10.4.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.5"><span class="ltx_text" id="A0.T5.7.1.10.10.5.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.7.1.10.10.6"><span class="ltx_text" id="A0.T5.7.1.10.10.6.1">✓</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.11.11">
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.11.11.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.11.11.1.1">Unsloth</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.1.11.11.2"><span class="ltx_text" id="A0.T5.7.1.11.11.2.1">✓</span></td>
<td class="ltx_td" id="A0.T5.7.1.11.11.3"></td>
<td class="ltx_td" id="A0.T5.7.1.11.11.4"></td>
<td class="ltx_td" id="A0.T5.7.1.11.11.5"></td>
<td class="ltx_td" id="A0.T5.7.1.11.11.6"></td>
</tr>
<tr class="ltx_tr" id="A0.T5.7.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.1"><span class="ltx_text ltx_font_bold" id="A0.T5.7.1.12.12.1.1">DeepSpeed</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.2"><span class="ltx_text" id="A0.T5.7.1.12.12.2.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.3"><span class="ltx_text" id="A0.T5.7.1.12.12.3.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.4"><span class="ltx_text" id="A0.T5.7.1.12.12.4.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.5"><span class="ltx_text" id="A0.T5.7.1.12.12.5.1">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T5.7.1.12.12.6"><span class="ltx_text" id="A0.T5.7.1.12.12.6.1">✓</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_font_bold ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_medium" id="A1.1.1.1">Appendix A</span> </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.p1.1.1">In this section, we enumerate existing frameworks for fine-tuning LLMs, especially highlight those for efficient fine-tuning.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.p2.1.1">LLaMA-Adapter </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.2.1">(</span><span class="ltx_text ltx_font_bold">Zhang et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib85" title=""><span class="ltx_text ltx_font_bold">2022</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.4.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.5"> efficiently fine-tunes the Llama model </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.6.1">(</span><span class="ltx_text ltx_font_bold">Touvron et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.7.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib69" title=""><span class="ltx_text ltx_font_bold">2023a</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.8.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.9"> for instruction-following abilities using zero-init attention. FastChat </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.10.1">(</span><span class="ltx_text ltx_font_bold">Zheng et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.11.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib88" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.12.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.13"> is a framework focused on training and evaluating LLMs for chat completion purposes. LitGPT </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.14.1">(</span><span class="ltx_text ltx_font_bold">AI</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.15.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib2" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.16.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.17"> provides the implementation of generative models and supports various training methods. Open-Instruct </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.18.1">(</span><span class="ltx_text ltx_font_bold">Wang et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.19.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib76" title=""><span class="ltx_text ltx_font_bold">2023d</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.20.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.21"> is a fine-tuning framework that supports efficient fine-tuning. Colossal AI </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.22.1">(</span><span class="ltx_text ltx_font_bold">Li et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.23.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib46" title=""><span class="ltx_text ltx_font_bold">2023b</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.24.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.25"> takes advanced parallelism strategies for distributed training. LMFlow </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.26.1">(</span><span class="ltx_text ltx_font_bold">Diao et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.27.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib23" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.28.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.29"> is an extensible and efficient fine-tuning framework that supports decoder-only models, supporting both full-tuning and adapter tuning. GPT4All </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.p2.1.30.1">(</span><span class="ltx_text ltx_font_bold">Anand et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.p2.1.31.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib4" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.p2.1.32.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A1.p2.1.33"> allows LLMs to run on consumer devices, while also providing fine-tuning capabilities.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1"><span class="ltx_text ltx_font_bold" id="A1.p3.1.1">In contrast to the frameworks presented above, </span><span class="ltx_text ltx_font_smallcaps" id="A1.p3.1.2">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="A1.p3.1.3"> supports a broader range of efficient fine-tuning methods. We compare our framework with existing work in Table&nbsp;</span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#A0.T5" title="Table 5 ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text ltx_font_bold" id="A1.p3.1.4">.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T6">
<figcaption class="ltx_caption ltx_centering ltx_font_bold"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="A1.T6.3.1.1">Table 6</span>: </span>List of supported models.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T6.4" style="width:420.6pt;height:567.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.6pt,22.4pt) scale(0.92690966982746,0.92690966982746) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T6.4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T6.4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T6.4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T6.4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.1.1.2.1">Variant</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T6.4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.1.1.3.1">Organization</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.4.1.2.1.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.1.1">Llama </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.1.2.1">(</span><span class="ltx_text ltx_font_bold">Touvron et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib69" title=""><span class="ltx_text ltx_font_bold">2023a</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.4.1.2.1.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.2.1">7B/13B/33B/65B</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T6.4.1.2.1.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.2.1.3.1">Meta AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.3.2">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.3.2.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.1.1">Llama 2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.1.2.1">(</span><span class="ltx_text ltx_font_bold">Touvron et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib70" title=""><span class="ltx_text ltx_font_bold">2023b</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.3.2.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.2.1">7B/13B/70B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.3.2.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.3.2.3.1">Meta AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.4.3">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.4.3.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.4.3.1.1">Baichuan </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_bold">Yang et&nbsp;al.</span> <span class="ltx_text ltx_font_bold" id="A1.T6.4.1.4.3.1.2.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib81" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.4.3.1.3.2.2.1">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.4.3.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.4.3.2.1">7B/13B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.4.3.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.4.3.3.1">Baichuan Inc</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.5.4">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.5.4.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.1.1">Baichuan2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.1.2.1">(</span><span class="ltx_text ltx_font_bold">Yang et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib81" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.5.4.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.2.1">7B/13B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.5.4.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.5.4.3.1">Baichuan Inc</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.6.5">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.6.5.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.1.1">BLOOM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.1.2.1">(</span><span class="ltx_text ltx_font_bold">Le&nbsp;Scao et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib43" title=""><span class="ltx_text ltx_font_bold">2022</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.6.5.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.2.1">560M/3B/7.1B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.6.5.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.6.5.3.1">BigScience</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.7.6">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.7.6.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.1.1">BLOOMZ </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.1.2.1">(</span><span class="ltx_text ltx_font_bold">Le&nbsp;Scao et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib43" title=""><span class="ltx_text ltx_font_bold">2022</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.7.6.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.2.1">560M/3B/7.1B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.7.6.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.7.6.3.1">BigScience</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.8.7">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.8.7.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.1.1">ChatGLM2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.1.2.1">(</span><span class="ltx_text ltx_font_bold">Du et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib24" title=""><span class="ltx_text ltx_font_bold">2022</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.8.7.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.2.1">6B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.8.7.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.8.7.3.1">THUDM</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.9.8">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.9.8.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.1.1">ChatGLM3 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.1.2.1">(</span><span class="ltx_text ltx_font_bold">Du et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib24" title=""><span class="ltx_text ltx_font_bold">2022</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.9.8.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.2.1">6B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.9.8.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.9.8.3.1">THUDM</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.10.9">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.10.9.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.1.1">ChineseLLaMA2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.1.2.1">(</span><span class="ltx_text ltx_font_bold">Cui et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib16" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.10.9.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.2.1">3B/7B/13B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.10.9.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.10.9.3.1">HFL</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.11.10">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.11.10.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.1.1">DeepSeek-LLM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.1.2.1">(</span><span class="ltx_text ltx_font_bold">Bi et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib10" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.11.10.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.2.1">7B/67B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.11.10.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.11.10.3.1">DeepSeek</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.12.11">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.12.11.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.1.1">DeepSeek-Math </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.1.2.1">(</span><span class="ltx_text ltx_font_bold">Shao et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib65" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.12.11.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.2.1">7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.12.11.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.12.11.3.1">DeepSeek</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.13.12">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.13.12.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.1.1">DeepSeek-MoE </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.1.2.1">(</span><span class="ltx_text ltx_font_bold">Dai et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib17" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.13.12.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.2.1">16B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.13.12.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.13.12.3.1">DeepSeek</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.14.13">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.14.13.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.1.1">DeepSeek-Coder </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.1.2.1">(</span><span class="ltx_text ltx_font_bold">Guo et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib28" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.14.13.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.2.1">6.7B/7B/33B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.14.13.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.14.13.3.1">DeepSeek</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.15.14">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.15.14.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.1.1">Falcon </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.1.2.1">(</span><span class="ltx_text ltx_font_bold">Almazrouei et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib3" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.15.14.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.2.1">7B/40B/180B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.15.14.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.15.14.3.1">TII</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.16.15">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.16.15.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.1.1">Gemma </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.1.2.1">(</span><span class="ltx_text ltx_font_bold">Mesnard et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib53" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.16.15.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.2.1">2B/7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.16.15.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.16.15.3.1">Google</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.17.16">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.17.16.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.1.1">InternLM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.1.2.1">(</span><span class="ltx_text ltx_font_bold">Team</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib67" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.17.16.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.2.1">7B/20B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.17.16.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.17.16.3.1">Shanghai AI Lab</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.18.17">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.18.17.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.1.1">InternLM2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.1.2.1">(</span><span class="ltx_text ltx_font_bold">Team</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib67" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.18.17.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.2.1">1.8B/7B/20B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.18.17.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.18.17.3.1">Shanghai AI Lab</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.19.18">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.19.18.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.1.1">Mistral </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.1.2.1">(</span><span class="ltx_text ltx_font_bold">Jiang et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib35" title=""><span class="ltx_text ltx_font_bold">2023a</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.19.18.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.2.1">7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.19.18.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.19.18.3.1">Mistral AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.20.19">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.20.19.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.1.1">Mixtral </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.1.2.1">(</span><span class="ltx_text ltx_font_bold">Jiang et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib35" title=""><span class="ltx_text ltx_font_bold">2023a</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.20.19.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.2.1">8x7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.20.19.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.20.19.3.1">Mistral AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.21.20">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.21.20.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.1.1">OLMo </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.1.2.1">(</span><span class="ltx_text ltx_font_bold">Groeneveld et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib27" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.21.20.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.2.1">1B/7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.21.20.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.21.20.3.1">Allen AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.22.21">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.22.21.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.1.1">OpenChat </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.1.2.1">(</span><span class="ltx_text ltx_font_bold">Wang et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib74" title=""><span class="ltx_text ltx_font_bold">2023b</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.22.21.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.2.1">7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.22.21.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.22.21.3.1">OpenChat</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.23.22">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.23.22.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.1.1">Orion </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.1.2.1">(</span><span class="ltx_text ltx_font_bold">Chen et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib12" title=""><span class="ltx_text ltx_font_bold">2024a</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.23.22.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.2.1">14B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.23.22.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.23.22.3.1">OrionStar</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.24.23">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.24.23.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.1.1">Phi-1.5 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.1.2.1">(</span><span class="ltx_text ltx_font_bold">Li et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib47" title=""><span class="ltx_text ltx_font_bold">2023c</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.24.23.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.2.1">1.3B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.24.23.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.24.23.3.1">Microsoft</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.25.24">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.25.24.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.1.1">Phi-2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.1.2.1">(</span><span class="ltx_text ltx_font_bold">Li et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib47" title=""><span class="ltx_text ltx_font_bold">2023c</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.25.24.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.2.1">2.7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.25.24.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.25.24.3.1">Microsoft</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.26.25">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.26.25.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.1.1">Qwen </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.1.2.1">(</span><span class="ltx_text ltx_font_bold">Bai et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib6" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.26.25.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.2.1">1.8B/7B/14B/72B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.26.25.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.26.25.3.1">Alibaba Cloud</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.27.26">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.27.26.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.1.1">Qwen1.5 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.1.2.1">(</span><span class="ltx_text ltx_font_bold">Bai et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib6" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.27.26.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.2.1">1.8B/7B/14B/72B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.27.26.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.27.26.3.1">Alibaba Cloud</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.28.27">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.28.27.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.1.1">SOLAR </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.1.2.1">(</span><span class="ltx_text ltx_font_bold">Kim et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib40" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.28.27.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.2.1">10.7B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.28.27.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.28.27.3.1">Upstage AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.29.28">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.29.28.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.1.1">Skywork </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.1.2.1">(</span><span class="ltx_text ltx_font_bold">Wei et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib78" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.29.28.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.2.1">13B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.29.28.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.29.28.3.1">Skywork</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.30.29">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.30.29.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.1.1">StarCoder2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.1.2.1">(</span><span class="ltx_text ltx_font_bold">Lozhkov et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib51" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.30.29.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.2.1">3B/7B/15B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.30.29.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.30.29.3.1">BigCode</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.31.30">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.31.30.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.1.1">Vicuna1.5 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.1.2.1">(</span><span class="ltx_text ltx_font_bold">Zheng et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib88" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.31.30.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.2.1">7B/13B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.31.30.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.31.30.3.1">LMSYS</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.32.31">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.32.31.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.1.1">Yi </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.1.2.1">(</span><span class="ltx_text ltx_font_bold">Young et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib83" title=""><span class="ltx_text ltx_font_bold">2024</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.32.31.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.2.1">6B/9B/34B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.32.31.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.32.31.3.1">01.AI</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.33.32">
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.33.32.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.1.1">Yuan2 </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.1.2.1">(</span><span class="ltx_text ltx_font_bold">Wu et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib80" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.33.32.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.2.1">2B/51B/102B</span></td>
<td class="ltx_td ltx_align_left" id="A1.T6.4.1.33.32.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.33.32.3.1">IEIT</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.4.1.34.33">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.4.1.34.33.1">
<span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.1.1">Zephyr </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.1.2.1">(</span><span class="ltx_text ltx_font_bold">Tunstall et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib71" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.1.4.3">)</span></cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.4.1.34.33.2"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.2.1">7B</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A1.T6.4.1.34.33.3"><span class="ltx_text ltx_font_bold" id="A1.T6.4.1.34.33.3.1">Hugging Face H4</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_font_bold ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_medium" id="A2.1.1.1">Appendix B</span> </span>Supported Models in <span class="ltx_text ltx_font_medium ltx_font_smallcaps" id="A2.2.2">LlamaFactory</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.p1.1.1">We select the popular pre-trained models supported by </span><span class="ltx_text ltx_font_smallcaps" id="A2.p1.1.2">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="A2.p1.1.3"> and list them in Table&nbsp;</span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#A1.T6" title="Table 6 ‣ Appendix A Related Work ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text ltx_font_bold" id="A2.p1.1.4">, ranging from dense models to sparse mixture-of-expert (MoE) models.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A2.T7">
<figcaption class="ltx_caption ltx_centering ltx_font_bold"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_medium" id="A2.T7.5.1.1">Table 7</span>: </span>Prompt of function calling in <span class="ltx_text ltx_font_medium ltx_font_smallcaps" id="A2.T7.6.2">LlamaFactory</span>.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T7.7" style="width:429.3pt;height:186.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.8pt,1.2pt) scale(0.987304193006671,0.987304193006671) ;">
<table class="ltx_tabular ltx_align_middle" id="A2.T7.7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T7.7.1.1.1">
<td class="ltx_td ltx_align_justify ltx_border_tt" id="A2.T7.7.1.1.1.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.1.1.1.1.1">You have access to the following tools:</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.2.2">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.2.2.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.2.2.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.2.2.1.1.1">&gt; Tool Name: tool1</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.3.3">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.3.3.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.3.3.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.3.3.1.1.1">Tool Description: the usage of tool1</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.4.4">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.4.4.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.4.4.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.4.4.1.1.1">Tool Args:</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.5.5">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.5.5.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.5.5.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.5.5.1.1.1">- arg1 (dtype): the usage of arg1</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.6.6">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.6.6.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.6.6.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.6.6.1.1.1">Use the following format if using a tool:</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.7.7">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.7.7.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.7.7.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.7.7.1.1.1">```</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.8.8">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.8.8.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.8.8.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.8.8.1.1.1">Action: tool name (one of [tool1]).</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.9.9">
<td class="ltx_td ltx_align_justify" id="A2.T7.7.1.9.9.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.9.9.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.9.9.1.1.1">Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. ```{"input": "hello world", "num_beams": 5}```).</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="A2.T7.7.1.10.10">
<td class="ltx_td ltx_align_justify ltx_border_bb" id="A2.T7.7.1.10.10.1" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A2.T7.7.1.10.10.1.1"><span class="ltx_text ltx_font_bold" id="A2.T7.7.1.10.10.1.1.1">```</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_font_bold ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_medium" id="A3.1.1.1">Appendix C</span> </span>Details of Designing Chat Template</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1"><span class="ltx_text ltx_font_bold" id="A3.p1.1.1">Although Transformers 4.34.0 introduces the feature of chat templates, several models (such as ChatGLM and Qwen) have tokenizers that exhibit specific behaviours and cannot directly encode the special tokens. Therefore, we design a </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.2">Formatter</span><span class="ltx_text ltx_font_bold" id="A3.p1.1.3"> class to robustly convert the textual inputs to their embedding IDs. Specifically, we provide the </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.4">EmptyFormatter</span><span class="ltx_text ltx_font_bold" id="A3.p1.1.5">, </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.6">StringFormatter</span><span class="ltx_text ltx_font_bold" id="A3.p1.1.7">, </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.8">FunctionFormatter</span><span class="ltx_text ltx_font_bold" id="A3.p1.1.9"> and </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.10">ToolFormatter</span><span class="ltx_text ltx_font_bold" id="A3.p1.1.11">. Furthermore, </span><span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.12">LlamaFactory</span><span class="ltx_text ltx_font_bold" id="A3.p1.1.13"> supports fine-tuning models to obtain function calling capabilities. While the ReAct prompting </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_bold">Yao et&nbsp;al.</span> <span class="ltx_text ltx_font_bold" id="A3.p1.1.14.1.1.1">(</span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib82" title=""><span class="ltx_text ltx_font_bold">2023</span></a><span class="ltx_text ltx_font_bold" id="A3.p1.1.15.2.2.1">)</span></cite><span class="ltx_text ltx_font_bold" id="A3.p1.1.16"> is a popular choice for tool using, it is insufficient for nested tool parameters. We optimize the tool prompts and provided them in Table&nbsp;</span><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2403.13372v2#A2.T7" title="Table 7 ‣ Appendix B Supported Models in LlamaFactory ‣ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models"><span class="ltx_text ltx_ref_tag">7</span></a><span class="ltx_text ltx_font_bold" id="A3.p1.1.17">.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_font_bold ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix"><span class="ltx_text ltx_font_medium" id="A4.1.1.1">Appendix D</span> </span>Experiment Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_medium" id="A4.SS1.1.1.1">D.1</span> </span>Training Efficiency</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p" id="A4.SS1.p1.7"><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.1">We adopt a learning rate of </span><math alttext="10^{-5}" class="ltx_Math" display="inline" id="A4.SS1.p1.1.m1.1"><semantics id="A4.SS1.p1.1.m1.1a"><msup id="A4.SS1.p1.1.m1.1.1" xref="A4.SS1.p1.1.m1.1.1.cmml"><mn id="A4.SS1.p1.1.m1.1.1.2" xref="A4.SS1.p1.1.m1.1.1.2.cmml">10</mn><mrow id="A4.SS1.p1.1.m1.1.1.3" xref="A4.SS1.p1.1.m1.1.1.3.cmml"><mo id="A4.SS1.p1.1.m1.1.1.3a" xref="A4.SS1.p1.1.m1.1.1.3.cmml">−</mo><mn id="A4.SS1.p1.1.m1.1.1.3.2" xref="A4.SS1.p1.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.1.m1.1b"><apply id="A4.SS1.p1.1.m1.1.1.cmml" xref="A4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A4.SS1.p1.1.m1.1.1.1.cmml" xref="A4.SS1.p1.1.m1.1.1">superscript</csymbol><cn id="A4.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="A4.SS1.p1.1.m1.1.1.2">10</cn><apply id="A4.SS1.p1.1.m1.1.1.3.cmml" xref="A4.SS1.p1.1.m1.1.1.3"><minus id="A4.SS1.p1.1.m1.1.1.3.1.cmml" xref="A4.SS1.p1.1.m1.1.1.3"></minus><cn id="A4.SS1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A4.SS1.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.1.m1.1c">10^{-5}</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.1.m1.1d">10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.2">, a token batch size of </span><math alttext="512" class="ltx_Math" display="inline" id="A4.SS1.p1.2.m2.1"><semantics id="A4.SS1.p1.2.m2.1a"><mn id="A4.SS1.p1.2.m2.1.1" xref="A4.SS1.p1.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.2.m2.1b"><cn id="A4.SS1.p1.2.m2.1.1.cmml" type="integer" xref="A4.SS1.p1.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.2.m2.1c">512</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.2.m2.1d">512</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.3"> to fine-tune these models using the 8-bit AdamW optimizer </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.4.1">(</span><span class="ltx_text ltx_font_bold">Dettmers et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.5.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib21" title=""><span class="ltx_text ltx_font_bold">2022b</span></a><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.6.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.7"> in bfloat16 precision with activation checkpointing to reduce the memory footprint. In freeze-tuning, we only fine-tune the last </span><math alttext="3" class="ltx_Math" display="inline" id="A4.SS1.p1.3.m3.1"><semantics id="A4.SS1.p1.3.m3.1a"><mn id="A4.SS1.p1.3.m3.1.1" xref="A4.SS1.p1.3.m3.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.3.m3.1b"><cn id="A4.SS1.p1.3.m3.1.1.cmml" type="integer" xref="A4.SS1.p1.3.m3.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.3.m3.1c">3</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.3.m3.1d">3</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.8"> decoder layers of the model. For GaLore, we set the rank and scale to </span><math alttext="128" class="ltx_Math" display="inline" id="A4.SS1.p1.4.m4.1"><semantics id="A4.SS1.p1.4.m4.1a"><mn id="A4.SS1.p1.4.m4.1.1" xref="A4.SS1.p1.4.m4.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.4.m4.1b"><cn id="A4.SS1.p1.4.m4.1.1.cmml" type="integer" xref="A4.SS1.p1.4.m4.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.4.m4.1c">128</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.4.m4.1d">128</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.9"> and </span><math alttext="2.0" class="ltx_Math" display="inline" id="A4.SS1.p1.5.m5.1"><semantics id="A4.SS1.p1.5.m5.1a"><mn id="A4.SS1.p1.5.m5.1.1" xref="A4.SS1.p1.5.m5.1.1.cmml">2.0</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.5.m5.1b"><cn id="A4.SS1.p1.5.m5.1.1.cmml" type="float" xref="A4.SS1.p1.5.m5.1.1">2.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.5.m5.1c">2.0</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.5.m5.1d">2.0</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.10">, respectively. For LoRA and QLoRA, we attach adapters to all linear layers and set the rank and alpha to </span><math alttext="128" class="ltx_Math" display="inline" id="A4.SS1.p1.6.m6.1"><semantics id="A4.SS1.p1.6.m6.1a"><mn id="A4.SS1.p1.6.m6.1.1" xref="A4.SS1.p1.6.m6.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.6.m6.1b"><cn id="A4.SS1.p1.6.m6.1.1.cmml" type="integer" xref="A4.SS1.p1.6.m6.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.6.m6.1c">128</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.6.m6.1d">128</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.11"> and </span><math alttext="256" class="ltx_Math" display="inline" id="A4.SS1.p1.7.m7.1"><semantics id="A4.SS1.p1.7.m7.1a"><mn id="A4.SS1.p1.7.m7.1.1" xref="A4.SS1.p1.7.m7.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.7.m7.1b"><cn id="A4.SS1.p1.7.m7.1.1.cmml" type="integer" xref="A4.SS1.p1.7.m7.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.7.m7.1c">256</annotation><annotation encoding="application/x-llamapun" id="A4.SS1.p1.7.m7.1d">256</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS1.p1.7.12">, respectively. All the experiments are conducted on a single NVIDIA A100 40GB GPU. We enable flash attention in all experiments and Unsloth for LoRA and QLoRA experiments.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text ltx_font_medium" id="A4.SS2.1.1.1">D.2</span> </span>Fine-Tuning on Downstream Tasks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.SS2.p1">
<p class="ltx_p" id="A4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="A4.SS2.p1.1.1">We briefly describe the datasets used for evaluating the effectiveness of different fine-tuning methods.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A4.SS2.p2">
<ul class="ltx_itemize" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.1">CNN/DM </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.2.1">(</span><span class="ltx_text ltx_font_bold">Nallapati et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib55" title=""><span class="ltx_text ltx_font_bold">2016</span></a><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.5"> This dataset is used for the summary generation task. English articles are gathered from CNN and Daily Mail. We utilize version 1.0.0, which comprises over 300k samples.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i2.p1">
<p class="ltx_p" id="A4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.1">XSum </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.2.1">(</span><span class="ltx_text ltx_font_bold">Narayan et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib56" title=""><span class="ltx_text ltx_font_bold">2018</span></a><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.5"> This dataset is used for the abstractive summary generation task, which consists of English documents and one-sentence summaries, totaling over 200k samples.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i3.p1">
<p class="ltx_p" id="A4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.1">AdGen </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.2.1">(</span><span class="ltx_text ltx_font_bold">Shao et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.3.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib64" title=""><span class="ltx_text ltx_font_bold">2019</span></a><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.4.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.5"> This dataset is a Chinese dataset that requires models to generate advertising text based on given keywords, totalling over 100k samples.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A4.SS2.p3">
<p class="ltx_p" id="A4.SS2.p3.7"><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.1">In this experiment, we adopt a learning rate of </span><math alttext="10^{-5}" class="ltx_Math" display="inline" id="A4.SS2.p3.1.m1.1"><semantics id="A4.SS2.p3.1.m1.1a"><msup id="A4.SS2.p3.1.m1.1.1" xref="A4.SS2.p3.1.m1.1.1.cmml"><mn id="A4.SS2.p3.1.m1.1.1.2" xref="A4.SS2.p3.1.m1.1.1.2.cmml">10</mn><mrow id="A4.SS2.p3.1.m1.1.1.3" xref="A4.SS2.p3.1.m1.1.1.3.cmml"><mo id="A4.SS2.p3.1.m1.1.1.3a" xref="A4.SS2.p3.1.m1.1.1.3.cmml">−</mo><mn id="A4.SS2.p3.1.m1.1.1.3.2" xref="A4.SS2.p3.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.1.m1.1b"><apply id="A4.SS2.p3.1.m1.1.1.cmml" xref="A4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A4.SS2.p3.1.m1.1.1.1.cmml" xref="A4.SS2.p3.1.m1.1.1">superscript</csymbol><cn id="A4.SS2.p3.1.m1.1.1.2.cmml" type="integer" xref="A4.SS2.p3.1.m1.1.1.2">10</cn><apply id="A4.SS2.p3.1.m1.1.1.3.cmml" xref="A4.SS2.p3.1.m1.1.1.3"><minus id="A4.SS2.p3.1.m1.1.1.3.1.cmml" xref="A4.SS2.p3.1.m1.1.1.3"></minus><cn id="A4.SS2.p3.1.m1.1.1.3.2.cmml" type="integer" xref="A4.SS2.p3.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.1.m1.1c">10^{-5}</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.1.m1.1d">10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.2">. We set batch size to </span><math alttext="4" class="ltx_Math" display="inline" id="A4.SS2.p3.2.m2.1"><semantics id="A4.SS2.p3.2.m2.1a"><mn id="A4.SS2.p3.2.m2.1.1" xref="A4.SS2.p3.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.2.m2.1b"><cn id="A4.SS2.p3.2.m2.1.1.cmml" type="integer" xref="A4.SS2.p3.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.2.m2.1c">4</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.2.m2.1d">4</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.3"> while the maximum input length is </span><math alttext="2048" class="ltx_Math" display="inline" id="A4.SS2.p3.3.m3.1"><semantics id="A4.SS2.p3.3.m3.1a"><mn id="A4.SS2.p3.3.m3.1.1" xref="A4.SS2.p3.3.m3.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.3.m3.1b"><cn id="A4.SS2.p3.3.m3.1.1.cmml" type="integer" xref="A4.SS2.p3.3.m3.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.3.m3.1c">2048</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.3.m3.1d">2048</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.4">. We fine-tune the models using the 8-bit AdamW optimizer </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.5.1">(</span><span class="ltx_text ltx_font_bold">Dettmers et&nbsp;al.</span><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.6.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2403.13372v2#bib.bib21" title=""><span class="ltx_text ltx_font_bold">2022b</span></a><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.7.3">)</span></cite><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.8"> in bfloat16 precision with activation checkpointing to reduce the memory footprint. For GaLore, we set the rank and scale to </span><math alttext="128" class="ltx_Math" display="inline" id="A4.SS2.p3.4.m4.1"><semantics id="A4.SS2.p3.4.m4.1a"><mn id="A4.SS2.p3.4.m4.1.1" xref="A4.SS2.p3.4.m4.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.4.m4.1b"><cn id="A4.SS2.p3.4.m4.1.1.cmml" type="integer" xref="A4.SS2.p3.4.m4.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.4.m4.1c">128</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.4.m4.1d">128</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.9"> and </span><math alttext="2.0" class="ltx_Math" display="inline" id="A4.SS2.p3.5.m5.1"><semantics id="A4.SS2.p3.5.m5.1a"><mn id="A4.SS2.p3.5.m5.1.1" xref="A4.SS2.p3.5.m5.1.1.cmml">2.0</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.5.m5.1b"><cn id="A4.SS2.p3.5.m5.1.1.cmml" type="float" xref="A4.SS2.p3.5.m5.1.1">2.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.5.m5.1c">2.0</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.5.m5.1d">2.0</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.10">, respectively. For LoRA and QLoRA, we attach adapters to all linear layers and set the rank and alpha to </span><math alttext="128" class="ltx_Math" display="inline" id="A4.SS2.p3.6.m6.1"><semantics id="A4.SS2.p3.6.m6.1a"><mn id="A4.SS2.p3.6.m6.1.1" xref="A4.SS2.p3.6.m6.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.6.m6.1b"><cn id="A4.SS2.p3.6.m6.1.1.cmml" type="integer" xref="A4.SS2.p3.6.m6.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.6.m6.1c">128</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.6.m6.1d">128</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.11"> and </span><math alttext="256" class="ltx_Math" display="inline" id="A4.SS2.p3.7.m7.1"><semantics id="A4.SS2.p3.7.m7.1a"><mn id="A4.SS2.p3.7.m7.1.1" xref="A4.SS2.p3.7.m7.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p3.7.m7.1b"><cn id="A4.SS2.p3.7.m7.1.1.cmml" type="integer" xref="A4.SS2.p3.7.m7.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p3.7.m7.1c">256</annotation><annotation encoding="application/x-llamapun" id="A4.SS2.p3.7.m7.1d">256</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.7.12">, respectively. All the experimental results are carried out on the NVIDIA A100 40GB GPUs with flash attention enabled.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.01432] Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge</title><meta property="og:description" content="Large language models (LLMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.01432">

<!--Generated on Fri Apr  5 16:22:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Heydar Soudani 
<br class="ltx_break">Radboud University 
<br class="ltx_break">Nijmegen 
<br class="ltx_break">The Netherlands 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">heydar.soudani@ru.nl</span> 
<br class="ltx_break">&amp;Evangelos Kanoulas 
<br class="ltx_break">University of Amsterdam 
<br class="ltx_break">Amsterdam 
<br class="ltx_break">The Netherlands 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">e.kanoulas@uva.nl</span> 
<br class="ltx_break">&amp;Faegheh Hasibi 
<br class="ltx_break">Radboud University 
<br class="ltx_break">Nijmegen 
<br class="ltx_break">The Netherlands 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">faegheh.hasibi@ru.nl</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">대규모 언어 모델(LLM)은 방대한 양의 사실적 지식을 암기하여 다양한 작업과 영역에 걸쳐 강력한 성능을 발휘한다. 그러나, 예를 들어 도메인 특정 애플리케이션에서 덜 인기 있거나 빈도가 낮은 개념 및 엔티티를 다룰 때 성능이 감소하는 것으로 관찰되었다. 저빈도 토픽에서 LLM의 성능을 향상시키기 위한 두 가지 두드러진 접근법은 합성 데이터에 대한 검색 증강 생성(RAG)과 미세 조정(FT)이다. 본 논문에서는 저빈도 개체 처리에 있어서 RAG와 FT가 LLMs 커스터마이징에 미치는 영향을 질의 응답 태스크에서 탐색하고 평가한다. 우리의 연구 결과는 FT가 특히 가장 인기 있는 그룹과 가장 인기 없는 그룹에서 다양한 인기의 엔터티에 걸쳐 성능을 크게 향상시키는 반면 RAG는 다른 방법을 능가한다는 것을 나타낸다. 또한, RAG 및 FT 접근법의 성공은 검색 및 데이터 증강 기술의 발전에 의해 증폭된다. 코드 및 데이터는 <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/informagi/RAGvsFT" target="_blank" title="">https://github.com/informagi/RAGvsFT</a>에서 이용 가능하다.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Fine Tuning vs. 덜 인기 있는 지식을 위한 검색 증강 생성</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.2.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.1.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Heydar Soudani</span></span></span>
<span id="p1.1.2.1.1.2.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.2.1" class="ltx_td ltx_align_center">Radboud University</span></span>
<span id="p1.1.2.1.1.3.3" class="ltx_tr">
<span id="p1.1.2.1.1.3.3.1" class="ltx_td ltx_align_center">Nijmegen</span></span>
<span id="p1.1.2.1.1.4.4" class="ltx_tr">
<span id="p1.1.2.1.1.4.4.1" class="ltx_td ltx_align_center">The Netherlands</span></span>
<span id="p1.1.2.1.1.5.5" class="ltx_tr">
<span id="p1.1.2.1.1.5.5.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.5.5.1.1" class="ltx_text ltx_font_typewriter">heydar.soudani@ru.nl</span></span></span>
</span>
</span></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="p1.1.2.2" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.2.2.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.2.1.1.1" class="ltx_tr">
<span id="p1.1.2.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Evangelos Kanoulas</span></span></span>
<span id="p1.1.2.2.1.2.2" class="ltx_tr">
<span id="p1.1.2.2.1.2.2.1" class="ltx_td ltx_align_center">University of Amsterdam</span></span>
<span id="p1.1.2.2.1.3.3" class="ltx_tr">
<span id="p1.1.2.2.1.3.3.1" class="ltx_td ltx_align_center">Amsterdam</span></span>
<span id="p1.1.2.2.1.4.4" class="ltx_tr">
<span id="p1.1.2.2.1.4.4.1" class="ltx_td ltx_align_center">The Netherlands</span></span>
<span id="p1.1.2.2.1.5.5" class="ltx_tr">
<span id="p1.1.2.2.1.5.5.1" class="ltx_td ltx_align_center"><span id="p1.1.2.2.1.5.5.1.1" class="ltx_text ltx_font_typewriter">e.kanoulas@uva.nl</span></span></span>
</span>
</span></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="p1.1.2.3" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.2.3.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.3.1.1.1" class="ltx_tr">
<span id="p1.1.2.3.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Faegheh Hasibi</span></span></span>
<span id="p1.1.2.3.1.2.2" class="ltx_tr">
<span id="p1.1.2.3.1.2.2.1" class="ltx_td ltx_align_center">Radboud University</span></span>
<span id="p1.1.2.3.1.3.3" class="ltx_tr">
<span id="p1.1.2.3.1.3.3.1" class="ltx_td ltx_align_center">Nijmegen</span></span>
<span id="p1.1.2.3.1.4.4" class="ltx_tr">
<span id="p1.1.2.3.1.4.4.1" class="ltx_td ltx_align_center">The Netherlands</span></span>
<span id="p1.1.2.3.1.5.5" class="ltx_tr">
<span id="p1.1.2.3.1.5.5.1" class="ltx_td ltx_align_center"><span id="p1.1.2.3.1.5.5.1.1" class="ltx_text ltx_font_typewriter">faegheh.hasibi@ru.nl</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">대규모 언어 모델(LLM)은 사실 데이터<cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al., <a class="ltx_ref" href="#bib.bib6" title="">2023</a>)</cite>에 대한 광범위한 암기를 요구하는 작업을 실행하는 탁월한 능력을 보여줍니다. 그러나, 그들의 암기 능력들은 덜 빈번한 엔티티들 <cite class="ltx_cite ltx_citemacro_cite">Mallen et al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>); Kandpal et al. (<a class="ltx_ref" href="#bib.bib16" title="">2023</a>); Sun et al. (<a class="ltx_ref" href="#bib.bib34" title="">2023</a>)</cite>를 다룰 때 제약되고, 심지어 가장 큰 모델들조차도 잘 알려진 "hallucination" 문제 <cite class="ltx_cite ltx_citemacro_cite">Shuster et al. (<a class="ltx_ref" href="#bib.bib32" title="">2021</a>)</cite> 및 시간 열화 <cite class="ltx_cite ltx_citemacro_cite">Kasai et al. (<a class="ltx_ref" href="#bib.bib18" title="">2022</a>)</cite>에 직면할 수 있다. 따라서 LLM이 자원이 부족한 도메인에 배포되도록 의도되는 경우 최적의 성능을 보장하기 위해 사용자 지정이 필수적입니다. 일반적인 예는 산업 설정 내에 있으며, 여기서 챗봇 또는 질의 응답(QA) 시스템은 제한된 텍스트 설명으로 독점 지식 그래프 또는 회사 내 용어에 대한 사용자의 질문에 정확하게 응답해야 한다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">RAG(Retrieval-Augmented Generation)와 FT(Fine-Tuning)는 LLM을 특정 도메인에 적응시키기 위한 두 가지 두드러진 접근법으로 눈에 띈다. RAG는 문서 코퍼스에서 관련 정보를 검색하고, 인-컨텍스트 학습(ICL)의 구현을 통해 LLM의 응답 생성을 향상시킨다. 반대로, 미세 조정 접근법은 특정 정보를 회상하는 데 능숙하도록 모델 가중치를 업데이트하고 추론 동안 암기 능력을 향상시킨다. 제한된 데이터를 사용할 수 있는 덜 대중적인 지식의 맥락에서, 데이터 증강 방법은 합성 트레이닝 데이터를 생성하는 데 활용되어 미세 조정을 향한 초기 단계 역할을 한다.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.01432/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1:</span>Correlation between subject entity popularity in a question and the effects of RAG and FT on FlanT5-small performance in open-domain question answering. FT는 다른 것(분홍색 선으로 표시됨)에 비해 초기 버킷 및 최종 버킷의 정확도를 현저하게 향상시킨다.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">본 논문에서는 산업별 모형에 어떤 접근과 어떤 조건에서 더 효과적인지를 파악하고자 한다. 구체적으로 <br class="ltx_break"/> <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">(RQ1)</span> 저빈도 사실적 지식을 위한 QA에 대한 합성 데이터로 RAG와 미세조정의 효과는 무엇인가? <br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S1.p3.1.2">(RQ2)</span> 합성 샘플의 품질, 미세 조정될 방법, 모델 크기 및 검색 모델의 성능을 포함한 매개 변수가 다운스트림 성능에 영향을 미칩니다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">RQ를 해결하기 위해 덜 대중적인 지식에 특히 주의를 기울여 RAG와 미세 조정 방법을 종합적으로 비교했다. 평가 설정은 모델 크기, 검색 모델, 합성 데이터 생성의 품질, 미세 조정 방법(PEFT 대 완전 미세 조정) 등 다양한 요소를 탐구한다.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">우리의 연구 결과는 FT가 가장 인기 있는 범주와 가장 인기 없는 범주에서 관찰된 가장 실질적인 개선과 함께 인기 있는 것과 덜 인기 있는 엔터티의 성능을 일관되게 향상시킨다는 것을 나타낸다. 또한, RAG는 특히 더 작은 모델에서 FT와 결합될 때 미세 조정 방법을 일관되게 능가하며, 이는 기본 모델에서 감소하고 더 큰 모델에서는 존재하지 않는 이점이다. 마지막으로, RAG 및 FT 전략의 효과는 검색 및 데이터 증강 모델의 성능이 향상됨에 따라 증가한다.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Data Augmentation (DA). </span> 데이터 가용성은 특수 도메인에서 미세 조정에 중요 합니다. DA는 기존의 레이블이 지정되지 않은 텍스트로부터 태스크 및 도메인 관련 샘플을 생성함으로써 데이터 희소성 문제를 해결한다. QA 작업에 대한 일반적인 DA 접근법은 4단계 <em class="ltx_emph ltx_font_italic" id="S2.p1.1.2">Pipeline</em>을 통해 질문-응답 쌍을 생성하는 것이다. <cite class="ltx_cite ltx_citemacro_citet">Ushio et al. (<a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>는 Pipeline, Multitask, End-to-End(E2E)의 세 가지 질문 답변 생성 방식을 비교한 실증 연구를 수행하였고, E2E 방식이 다운스트림 태스크에서 다른 방식보다 우수함을 보였다. 최근 데이터 생성을 위한 LLM의 활용은 정보 검색, QA, 대화 생성 작업 <cite class="ltx_cite ltx_citemacro_citep">(Soudani et al., <a class="ltx_ref" href="#bib.bib33" title="">2023</a>; Askari et al., <a class="ltx_ref" href="#bib.bib4" title="">2023</a>)</cite>에서 효과적인 것으로 나타나고 있다.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Retrieval Augmented Generation. </span> RAG는 외부 지식 소스를 입력 쿼리와 통합함으로써 LLM을 향상시키며, 지식 집약적인 작업에 대한 추가 컨텍스트를 가진 모델을 강화한다. 정보 검색 시스템을 활용하여 관련 문서를 찾아 입력 프롬프트에 추가하여 LLM<cite class="ltx_cite ltx_citemacro_citep">(Asai et al., <a class="ltx_ref" href="#bib.bib2" title="">2023a</a>, <a class="ltx_ref" href="#bib.bib3" title="">b</a>)</cite>의 응답 생성을 향상시킨다.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Less popular Knowledge. </span> LLM에서 엔터티의 인기는 모델의 사전 훈련 데이터 <cite class="ltx_cite ltx_citemacro_citep">(Godbole and Jia, <a class="ltx_ref" href="#bib.bib12" title="">2023</a>; Min et al., <a class="ltx_ref" href="#bib.bib25" title="">2023</a>)</cite>에서 엔터티의 빈도로 측정되며, 종종 큰 말뭉치 <cite class="ltx_cite ltx_citemacro_citep">(Kandpal et al., <a class="ltx_ref" href="#bib.bib16" title="">2023</a>)</cite>에서 엔터티의 발생을 통해 평가된다. 직접 계수의 현실적인 난제로 인해 트래픽 메트릭 및 콘텐츠 밀도와 같은 근사치가 <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a class="ltx_ref" href="#bib.bib34" title="">2023</a>)</cite>로 사용된다. 위키피디아 페이지뷰는 <cite class="ltx_cite ltx_citemacro_citep">(Mallen et al., <a class="ltx_ref" href="#bib.bib24" title="">2023</a>; Sciavolino et al., <a class="ltx_ref" href="#bib.bib31" title="">2021</a>; Chen et al., <a class="ltx_ref" href="#bib.bib5" title="">2021</a>)</cite> 엔터티의 인기도를 측정하는 가장 널리 사용되는 방법 중 하나이다.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Comparing FT vs. RAG</span>. 특정 작업에 대해 사전 훈련된 언어 모델을 정제하는 데 관심이 증가함에 따라 공평한 조건에서 FT와 RAG 전략의 비교가 점점 더 중요해지고 있다. <cite class="ltx_cite ltx_citemacro_citet">Mosbach et al. (<a class="ltx_ref" href="#bib.bib26" title="">2023</a>)</cite>는 일반 도메인에서 분류 작업에 대한 소샷 FT 대 In-context Learning의 효과를 탐구했다. <cite class="ltx_cite ltx_citemacro_citet">de Luis Balaguer et al. (<a class="ltx_ref" href="#bib.bib10" title="">2024</a>)</cite>는 긴 질문, 농업 및 지리별 질문에 대한 답변에서 FT와 RAG를 비교했다. <cite class="ltx_cite ltx_citemacro_citet">Ovadia et al. (<a class="ltx_ref" href="#bib.bib28" title="">2023</a>)</cite>는 Anatomy, Astronomy, College Biology, Prehistory와 같은 전문 영역의 객관식 질문에 대한 성능을 평가했다. 이러한 연구와 대조적으로, 우리는 다양한 검색, 데이터 증강 및 미세 조정 방법을 비교하여 덜 인기 있는 사실 지식을 LLM에 통합하는 것을 직접 다룬다.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation Setup</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">Closed-book QA 태스크에서 LLMs를 평가합니다. <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">PopQA</span> dataset <cite class="ltx_cite ltx_citemacro_citep">(Mallen et al., <a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>는 롱테일 엔터티 분포를 다루는 질문을 특징으로 합니다. 또한 고유한 위키피디아 제목을 포함하여 페이지뷰 계산 및 관련 위키피디아 페이지 식별을 용이하게 한다. <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">PopQA</span> 데이터 세트에서 각 주제 엔터티에 대한 관련 위키피디아 페이지를 획득하고 엔터티의 인기 수준에 따라 5개의 버킷으로 나눕니다(그림 <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 Evaluation Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.01432/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="161" height="101" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2:</span>Distribution of sample counts across buckets, defined by <math alttext="log_{10}(\text{pageviews})" class="ltx_Math" display="inline" id="S3.F2.4.m1.1"><semantics id="S3.F2.4.m1.1b"><mrow id="S3.F2.4.m1.1.2" xref="S3.F2.4.m1.1.2.cmml"><mi id="S3.F2.4.m1.1.2.2" xref="S3.F2.4.m1.1.2.2.cmml">l</mi><mo id="S3.F2.4.m1.1.2.1" lspace="0em" rspace="0em" xref="S3.F2.4.m1.1.2.1.cmml">​</mo><mi id="S3.F2.4.m1.1.2.3" xref="S3.F2.4.m1.1.2.3.cmml">o</mi><mo id="S3.F2.4.m1.1.2.1b" lspace="0em" rspace="0em" xref="S3.F2.4.m1.1.2.1.cmml">​</mo><msub id="S3.F2.4.m1.1.2.4" xref="S3.F2.4.m1.1.2.4.cmml"><mi id="S3.F2.4.m1.1.2.4.2" xref="S3.F2.4.m1.1.2.4.2.cmml">g</mi><mn id="S3.F2.4.m1.1.2.4.3" xref="S3.F2.4.m1.1.2.4.3.cmml">10</mn></msub><mo id="S3.F2.4.m1.1.2.1c" lspace="0em" rspace="0em" xref="S3.F2.4.m1.1.2.1.cmml">​</mo><mrow id="S3.F2.4.m1.1.2.5.2" xref="S3.F2.4.m1.1.1a.cmml"><mo id="S3.F2.4.m1.1.2.5.2.1" stretchy="false" xref="S3.F2.4.m1.1.1a.cmml">(</mo><mtext id="S3.F2.4.m1.1.1" xref="S3.F2.4.m1.1.1.cmml">pageviews</mtext><mo id="S3.F2.4.m1.1.2.5.2.2" stretchy="false" xref="S3.F2.4.m1.1.1a.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.4.m1.1c"><apply id="S3.F2.4.m1.1.2.cmml" xref="S3.F2.4.m1.1.2"><times id="S3.F2.4.m1.1.2.1.cmml" xref="S3.F2.4.m1.1.2.1"></times><ci id="S3.F2.4.m1.1.2.2.cmml" xref="S3.F2.4.m1.1.2.2">𝑙</ci><ci id="S3.F2.4.m1.1.2.3.cmml" xref="S3.F2.4.m1.1.2.3">𝑜</ci><apply id="S3.F2.4.m1.1.2.4.cmml" xref="S3.F2.4.m1.1.2.4"><csymbol cd="ambiguous" id="S3.F2.4.m1.1.2.4.1.cmml" xref="S3.F2.4.m1.1.2.4">subscript</csymbol><ci id="S3.F2.4.m1.1.2.4.2.cmml" xref="S3.F2.4.m1.1.2.4.2">𝑔</ci><cn id="S3.F2.4.m1.1.2.4.3.cmml" type="integer" xref="S3.F2.4.m1.1.2.4.3">10</cn></apply><ci id="S3.F2.4.m1.1.1a.cmml" xref="S3.F2.4.m1.1.2.5.2"><mtext id="S3.F2.4.m1.1.1.cmml" xref="S3.F2.4.m1.1.1">pageviews</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m1.1d">log_{10}(\text{pageviews})</annotation></semantics></math>. 가장 왼쪽 빈은 <math alttext="10^{2}" class="ltx_Math" display="inline" id="S3.F2.5.m2.1"><semantics id="S3.F2.5.m2.1b"><msup id="S3.F2.5.m2.1.1" xref="S3.F2.5.m2.1.1.cmml"><mn id="S3.F2.5.m2.1.1.2" xref="S3.F2.5.m2.1.1.2.cmml">10</mn><mn id="S3.F2.5.m2.1.1.3" xref="S3.F2.5.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.F2.5.m2.1c"><apply id="S3.F2.5.m2.1.1.cmml" xref="S3.F2.5.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.5.m2.1.1.1.cmml" xref="S3.F2.5.m2.1.1">superscript</csymbol><cn id="S3.F2.5.m2.1.1.2.cmml" type="integer" xref="S3.F2.5.m2.1.1.2">10</cn><cn id="S3.F2.5.m2.1.1.3.cmml" type="integer" xref="S3.F2.5.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.m2.1d">10^{2}</annotation></semantics></math> 페이지뷰보다 적은 엔터티를 포함하는 반면, 가장 오른쪽 빈은 <math alttext="10^{5}" class="ltx_Math" display="inline" id="S3.F2.6.m3.1"><semantics id="S3.F2.6.m3.1b"><msup id="S3.F2.6.m3.1.1" xref="S3.F2.6.m3.1.1.cmml"><mn id="S3.F2.6.m3.1.1.2" xref="S3.F2.6.m3.1.1.2.cmml">10</mn><mn id="S3.F2.6.m3.1.1.3" xref="S3.F2.6.m3.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="S3.F2.6.m3.1c"><apply id="S3.F2.6.m3.1.1.cmml" xref="S3.F2.6.m3.1.1"><csymbol cd="ambiguous" id="S3.F2.6.m3.1.1.1.cmml" xref="S3.F2.6.m3.1.1">superscript</csymbol><cn id="S3.F2.6.m3.1.1.2.cmml" type="integer" xref="S3.F2.6.m3.1.1.2">10</cn><cn id="S3.F2.6.m3.1.1.3.cmml" type="integer" xref="S3.F2.6.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.m3.1d">10^{5}</annotation></semantics></math> 페이지뷰보다 많은 엔터티를 포함한다.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p" id="S3.p2.1">DA와 RAG 방법 간의 공정한 비교를 보장하기 위해 우리는 PopQA 데이터 세트에 해당 엔터티가 나타나는 위키피디아 페이지로 초점을 제한한다. 이 설정은 엔터티 및 해당 텍스트 설명이 기업의 특정 내부 개념과 관련된 실제 산업 관행을 반영하기도 합니다.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>RAG Approach</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p1.1">RAG 접근 방식을 위해 BM25 <cite class="ltx_cite ltx_citemacro_citep">(Robertson and Zaragoza, <a class="ltx_ref" href="#bib.bib30" title="">2009</a>)</cite>, Contriever <cite class="ltx_cite ltx_citemacro_citep">(Izacard et al., <a class="ltx_ref" href="#bib.bib14" title="">2022</a>)</cite>, DPR <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et al., <a class="ltx_ref" href="#bib.bib17" title="">2020</a>)</cite>, BM25와 DPR을 결합한 2단계 재순위자, 모두 BEIR 벤치마크 <cite class="ltx_cite ltx_citemacro_citep">(Thakur et al., <a class="ltx_ref" href="#bib.bib35" title="">2021</a>)</cite>에 따라 구현된다. 또한, 질문에서 언급된 엔티티에 대한 위키피디아 페이지의 요약 단락이 고려되는 이상적인 검색 설정을 활용한다. 우리는 구절에 주석을 달기 위한 진정한 증거를 사용할 수 없지만 요약 섹션에는 질문에 대한 답이 있을 수 있기 때문에 이 방법을 채택한다. 이 방법의 효능은 그림 <a class="ltx_ref" href="#A4.F5" title="Figure 5 ‣ Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">5</span></a>의 부록 <a class="ltx_ref" href="#A4" title="Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">D</span></a>에서 입증되며, 이는 이상적인 리트리버로 가장 높은 결과를 달성하는 QA 성능을 보여준다.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p2.1">질문에 가장 적합한 통로를 검색한 후, 간단한 템플릿을 사용하여 생성 예측을 위한 제로 샷 프롬프트를 적용합니다. <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS1.p2.1.1">"Context: &lt;context&gt;. 제공된 </code> <br class="ltx_break"/><code class="ltx_verbatim ltx_font_typewriter" id="S3.SS1.p2.1.2">context: &lt;question&gt;"</code>. 성능에 대한 모델 규모의 영향을 조사하기 위해 FlanT5 모델 <cite class="ltx_cite ltx_citemacro_citep">(Chung et al., <a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite>의 세 가지 크기를 실험한다. <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>에 이어 Accuracy 메트릭에 대해 보고하며, 여기서 예측은 제공된 골드 답변 중 하나와 정확히 일치하는 부분 문자열을 포함하는 경우 정확한 것으로 간주됩니다.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Fine-Tuning Approach</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p1.1">우리는 두 가지 별개의 데이터 증강 방법을 사용하여 미세 조정 접근법에 대한 훈련 데이터를 생성한다. 첫 번째는 End-to-End 접근법 <cite class="ltx_cite ltx_citemacro_citep">(Ushio et al., <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>, T5 모델 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="#bib.bib29" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">E2E</span>을 사용하여 단락 수준 QA 생성을 위해 특별히 훈련된 모델을 활용한다. 또한, QA 생성을 위해 Zephyr<cite class="ltx_cite ltx_citemacro_citep">(Tunstall et al., <a class="ltx_ref" href="#bib.bib36" title="">2023</a>)</cite>를 활용하여 LLM을 프롬프트하여 합성 학습 데이터의 생성을 탐색한다. 이 방법은 논의에서 <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.2">Prompt</span> 방법으로 지칭된다. 우리는 FT와 RAG를 이상적인 리트리버와 공정한 비교를 보장하기 위해 위키피디아 페이지의 요약 섹션을 사용하여 독점적으로 QA를 생성한다. 프롬프트, 입력 및 출력 예 및 생성된 QA의 통계를 포함한 QA 생성 프로세스에 대한 자세한 내용은 부록 <a class="ltx_ref" href="#A2" title="Appendix B QA Generation ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">B</span></a>에 나와 있다.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p2.1">QA 쌍을 생성한 후, 전체 매개변수 조정(<span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Full</span>) 및 매개변수 효율적인 미세 조정(<span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.2">PEFT</span>)의 두 가지 접근법을 사용하여 FlanT5 모델을 미세 조정했다. PEFT 기술 <cite class="ltx_cite ltx_citemacro_citep">(Zaken et al., <a class="ltx_ref" href="#bib.bib38" title="">2022</a>; Liu et al., <a class="ltx_ref" href="#bib.bib22" title="">2022</a>; Ma et al., <a class="ltx_ref" href="#bib.bib23" title="">2023</a>)</cite>의 범위 내에서, 우리는 필드 <cite class="ltx_cite ltx_citemacro_citep">(Kaddour et al., <a class="ltx_ref" href="#bib.bib15" title="">2023</a>; Naveed et al., <a class="ltx_ref" href="#bib.bib27" title="">2023</a>)</cite>에서 광범위한 수용과 우리가 처분한 계산 자원의 효율적인 사용을 위해 선택한 QLoRA <cite class="ltx_cite ltx_citemacro_citep">(Dettmers et al., <a class="ltx_ref" href="#bib.bib11" title="">2023</a>)</cite>를 활용한다. FT 접근법 및 하이퍼 매개변수에 대한 자세한 내용은 부록 <a class="ltx_ref" href="#A3" title="Appendix C Fine-Tuning Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">C</span></a>에 자세히 설명되어 있다.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2>

<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Rec@1</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Rec@3</span></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Rec@5</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BM25</th>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">36.14</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">73.76</td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">81.68</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Contriever</th>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center">72.77</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center">91.58</td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_center">96.04</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BM25+DPR</th>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.3.2.1" class="ltx_text ltx_font_bold">79.21</span></td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center">91.58</td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_center">92.57</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">DPR</th>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b">78.21</td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.1.5.4.3.1" class="ltx_text ltx_font_bold">92.57</span></td>
<td id="S4.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.1.5.4.4.1" class="ltx_text ltx_font_bold">93.64</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span> 우리 말뭉치에서 검색 모델의 성능 비교, DPR 모델이 나머지를 능가합니다. 모든 질문에 대한 관련 구절은 위키피디아 페이지의 첫 번째 단락으로 가정한다.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Retrieval performance. </span> Figure <a class="ltx_ref" href="#S4.F3" title="Figure 3 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">3</span></a>는 인기 버킷에 걸쳐 이상적인 리트리버(여기서는 ground truth로 가정됨)에 대한 검색 모델의 성능을 비교합니다. 입력 프롬프트에 가장 높은 순위의 통로만 포함되기 때문에, 본 논문에서는 Recall@1에 초점을 맞추어 분석한다. 그 결과 인기 있는 엔티티가 인기 있는 엔티티에 비해 검색 효율이 더 높다는 것을 알 수 있다. 이는 아마도 낮은 인기 엔티티에 대한 노이즈 패시지의 제한된 발생 때문일 것이다. Table <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">1</span></a>는 모든 메소드에 대한 전체 검색 점수를 제공한다.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">FT 및 RAG 비교. </span> 4가지 별개의 구성에서 RAG와 FT의 영향을 평가합니다. (i) FT나 RAG 모두 <span class="ltx_text ltx_font_italic" id="S4.p2.1.2">(-FT-RAG)</span>, (ii) FT 없이 사용된 RAG <span class="ltx_text ltx_font_italic" id="S4.p2.1.3">(-FT+RAG)</span>, (iii) RAG 없이 적용된 FT <span class="ltx_text ltx_font_italic" id="S4.p2.1.4">(+FT-RAG)</span>, (iv)) FT와 RAG 모두 <span class="ltx_text ltx_font_italic" id="S4.p2.1.5">(+FT+RAG)</span>입니다. 연구 결과는 표 <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">2</span></a>에 자세히 설명되어 있다. 일반적으로 FT는 기본 모델의 정확도를 향상시키지만 기본 모델에 대한 RAG의 효과에는 도달하지 못한다. 최적의 성능은 FT와 RAG를 모두 통합하여 달성한다.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;"></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_nopad_l ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;"></th>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">-FT/</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">-FT/</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">+FT/</span></td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">+FT/</span></td>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.2.2.1.1" class="ltx_text ltx_font_bold">FT</span></th>
<th id="S4.T2.1.2.2.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.2.2.2.1" class="ltx_text ltx_font_bold">QA</span></th>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.2.2.3.1" class="ltx_text ltx_font_bold">-RAG</span></td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.2.2.4.1" class="ltx_text ltx_font_bold">+RAG</span></td>
<td id="S4.T2.1.2.2.5" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.2.2.5.1" class="ltx_text ltx_font_bold">-RAG</span></td>
<td id="S4.T2.1.2.2.6" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.2.2.6.1" class="ltx_text ltx_font_bold">+RAG</span></td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<th id="S4.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" colspan="6"><span id="S4.T2.1.3.3.1.1" class="ltx_text ltx_font_bold">FlanT5-small</span></th>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<th id="S4.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T2.1.4.4.1.1" class="ltx_text">PEFT</span></th>
<th id="S4.T2.1.4.4.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="4"><span id="S4.T2.1.4.4.3.1" class="ltx_text">3.05</span></td>
<td id="S4.T2.1.4.4.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="4"><span id="S4.T2.1.4.4.4.1" class="ltx_text">26.13</span></td>
<td id="S4.T2.1.4.4.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">5.53</td>
<td id="S4.T2.1.4.4.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">22.91</td>
</tr>
<tr id="S4.T2.1.5.5" class="ltx_tr">
<th id="S4.T2.1.5.5.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T2.1.5.5.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">7.01</td>
<td id="S4.T2.1.5.5.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">49.85</td>
</tr>
<tr id="S4.T2.1.6.6" class="ltx_tr">
<th id="S4.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T2.1.6.6.1.1" class="ltx_text">Full</span></th>
<th id="S4.T2.1.6.6.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T2.1.6.6.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">6.35</td>
<td id="S4.T2.1.6.6.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">10.21</td>
</tr>
<tr id="S4.T2.1.7.7" class="ltx_tr">
<th id="S4.T2.1.7.7.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T2.1.7.7.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.7.7.2.1" class="ltx_text ltx_font_bold">8.52</span></td>
<td id="S4.T2.1.7.7.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.7.7.3.1" class="ltx_text ltx_font_bold">49.88</span></td>
</tr>
<tr id="S4.T2.1.8.8" class="ltx_tr">
<th id="S4.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" colspan="6"><span id="S4.T2.1.8.8.1.1" class="ltx_text ltx_font_bold">FlanT5-base</span></th>
</tr>
<tr id="S4.T2.1.9.9" class="ltx_tr">
<th id="S4.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T2.1.9.9.1.1" class="ltx_text">PEFT</span></th>
<th id="S4.T2.1.9.9.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T2.1.9.9.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="4"><span id="S4.T2.1.9.9.3.1" class="ltx_text">6.72</span></td>
<td id="S4.T2.1.9.9.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="4"><span id="S4.T2.1.9.9.4.1" class="ltx_text">63.13</span></td>
<td id="S4.T2.1.9.9.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">6.94</td>
<td id="S4.T2.1.9.9.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">51.61</td>
</tr>
<tr id="S4.T2.1.10.10" class="ltx_tr">
<th id="S4.T2.1.10.10.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T2.1.10.10.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">9.92</td>
<td id="S4.T2.1.10.10.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.10.10.3.1" class="ltx_text ltx_font_bold">63.29</span></td>
</tr>
<tr id="S4.T2.1.11.11" class="ltx_tr">
<th id="S4.T2.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T2.1.11.11.1.1" class="ltx_text">Full</span></th>
<th id="S4.T2.1.11.11.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T2.1.11.11.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">8.63</td>
<td id="S4.T2.1.11.11.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">24.17</td>
</tr>
<tr id="S4.T2.1.12.12" class="ltx_tr">
<th id="S4.T2.1.12.12.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T2.1.12.12.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.12.12.2.1" class="ltx_text ltx_font_bold">11.41</span></td>
<td id="S4.T2.1.12.12.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">60.26</td>
</tr>
<tr id="S4.T2.1.13.13" class="ltx_tr">
<th id="S4.T2.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" colspan="6"><span id="S4.T2.1.13.13.1.1" class="ltx_text ltx_font_bold">FlanT5-large</span></th>
</tr>
<tr id="S4.T2.1.14.14" class="ltx_tr">
<th id="S4.T2.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T2.1.14.14.1.1" class="ltx_text">PEFT</span></th>
<th id="S4.T2.1.14.14.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T2.1.14.14.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="4"><span id="S4.T2.1.14.14.3.1" class="ltx_text">8.41</span></td>
<td id="S4.T2.1.14.14.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="4"><span id="S4.T2.1.14.14.4.1" class="ltx_text">58.12</span></td>
<td id="S4.T2.1.14.14.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">8.22</td>
<td id="S4.T2.1.14.14.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">55.26</td>
</tr>
<tr id="S4.T2.1.15.15" class="ltx_tr">
<th id="S4.T2.1.15.15.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T2.1.15.15.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">12.15</td>
<td id="S4.T2.1.15.15.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.15.15.3.1" class="ltx_text ltx_font_bold">61.71</span></td>
</tr>
<tr id="S4.T2.1.16.16" class="ltx_tr">
<th id="S4.T2.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T2.1.16.16.1.1" class="ltx_text">Full</span></th>
<th id="S4.T2.1.16.16.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T2.1.16.16.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T2.1.16.16.3.1" class="ltx_text ltx_font_bold">16.23</span></td>
<td id="S4.T2.1.16.16.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">13.31</td>
</tr>
<tr id="S4.T2.1.17.17" class="ltx_tr">
<th id="S4.T2.1.17.17.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T2.1.17.17.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;">13.91</td>
<td id="S4.T2.1.17.17.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;">58.60</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span> RAG가 있거나 없는 기본 및 미세 조정 모델의 정확도. 제시된 RAG 결과는 이상적인 검색을 기반으로 한다.</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.01432/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="170" height="108" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3:</span> Recall@1 for four retrieval models across different popularity level. 그 결과, 검색 모델이 더 인기 있는 모델에 비해 덜 인기 있는 지식으로 더 효과적으로 수행됨을 알 수 있었다.</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p" id="S4.p3.1">그림 <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">1</span></a>는 Fine-tuning이 FlanT5-small 모델에 대한 모든 인기 수준에 걸쳐 QA 정확도를 향상시키며 가장 인기 있는 버킷과 가장 인기 없는 버킷에서 가장 큰 개선이 있음을 보여준다. 미세 조정은 더 작은 모델에서 덜 인기 있는 범주에 대한 성능을 향상시키는 반면, 이러한 이점은 기본 모델에서 감소하고 더 큰 모델에서 사라집니다. 자세한 내용은 그림 <a class="ltx_ref" href="#A4.F4" title="Figure 4 ‣ Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">4</span></a>를 참조하세요. 이는 더 큰 모델의 암기가 향상되었기 때문일 수 있습니다.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Effect of external parameters on RAG and FT. </span> 우리는 덜 대중적인 지식을 처리하는 데 있어 모델 전문화에 영향을 미치는 추가 요인을 조사한다. 검토 중인 핵심 측면은 PEFT 대비 완전 튜닝의 효과이다. 표 <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">2</span></a>는 PEFT가 전체 FT에 비해 <span class="ltx_text ltx_font_italic" id="S4.p4.1.2">+FT-RAG</span>에서 더 작은 이득을 가져오지만 <span class="ltx_text ltx_font_italic" id="S4.p4.1.3">+FT+RAG</span> 설정에서는 정확도를 크게 향상시킵니다. 이는 PEFT가 LLM이 제공된 프롬프트를 기반으로 추론 능력을 유지할 수 있음을 시사한다.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p class="ltx_p" id="S4.p5.1">우리의 조사는 또한 두 가지 QA 생성 기술을 다룬다. E2E 방법은 프롬프팅 방법보다 12배 이상 많은 QA를 생성한다(cf. Table <a class="ltx_ref" href="#A2.T4" title="Table 4 ‣ B.2 E2E Method ‣ Appendix B QA Generation ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">4</span></a> in Appendix <a class="ltx_ref" href="#A3" title="Appendix C Fine-Tuning Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">C</span></a>). 그러나 프롬프트 기반 메서드는 더 많은 품질 QA를 생성합니다. 표 <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">2</span></a>의 결과는 프롬프트 생성 데이터에 대해 훈련된 미세 조정 모델이 E2E 생성 모델보다 우수하다는 것을 보여준다. 이것은 양보다 합성 데이터 품질의 중요성을 강조한다.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p class="ltx_p" id="S4.p6.1">검색 모델의 성능은 또 다른 중요한 고려 사항이다. 표 <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">3</span></a>는 RAG 프레임워크 내에서 다양한 검색 전략을 활용한 QA 시스템의 정확도를 제시한다. 연구 결과는 검색 모델 성능과 전체 QA 정확도 사이의 직접적인 상관 관계를 보여주며, 검색 모델이 다운스트림 태스크의 효율성에 미치는 영향을 강조한다.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">FT</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">QA</span></th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<div id="S4.T3.1.1.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:26.2pt;vertical-align:-9.7pt;"><span class="ltx_transformed_inner" style="width:26.3pt;transform:translate(-9.71pt,0pt) rotate(-90deg) ;">
<p id="S4.T3.1.1.1.3.1.1" class="ltx_p"><span id="S4.T3.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">BM25</span></p>
</span></div>
</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<div id="S4.T3.1.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:45.9pt;vertical-align:-19.5pt;"><span class="ltx_transformed_inner" style="width:45.9pt;transform:translate(-19.53pt,0pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T3.1.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.4.1.1">Contriever</span></p>
</span></div>
</th>
<th id="S4.T3.1.1.1.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<div id="S4.T3.1.1.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.7pt;height:55.8pt;vertical-align:-24.9pt;"><span class="ltx_transformed_inner" style="width:55.8pt;transform:translate(-24.08pt,1.25pt) rotate(-90deg) ;">
<p id="S4.T3.1.1.1.5.1.1" class="ltx_p"><span id="S4.T3.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">BM25+DPR</span></p>
</span></div>
</th>
<th id="S4.T3.1.1.1.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<div id="S4.T3.1.1.1.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:21.8pt;vertical-align:-7.5pt;"><span class="ltx_transformed_inner" style="width:21.8pt;transform:translate(-7.49pt,0pt) rotate(-90deg) ;">
<p id="S4.T3.1.1.1.6.1.1" class="ltx_p"><span id="S4.T3.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">DPR</span></p>
</span></div>
</th>
<th id="S4.T3.1.1.1.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<div id="S4.T3.1.1.1.7.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:21.4pt;vertical-align:-7.2pt;"><span class="ltx_transformed_inner" style="width:21.4pt;transform:translate(-7.22pt,0pt) rotate(-90deg) ;">
<p id="S4.T3.1.1.1.7.1.1" class="ltx_p"><span id="S4.T3.1.1.1.7.1.1.1" class="ltx_text ltx_font_bold">Ideal</span></p>
</span></div>
</th>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" colspan="2"><span id="S4.T3.1.2.2.1.1" class="ltx_text ltx_font_bold">FlanT5-small</span></th>
<th id="S4.T3.1.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">9.76</th>
<th id="S4.T3.1.2.2.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">17.22</th>
<th id="S4.T3.1.2.2.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">20.80</th>
<th id="S4.T3.1.2.2.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">21.41</th>
<th id="S4.T3.1.2.2.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">26.13</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.3.1" class="ltx_tr">
<th id="S4.T3.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T3.1.3.1.1.1" class="ltx_text">PEFT</span></th>
<th id="S4.T3.1.3.1.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T3.1.3.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">13.13</td>
<td id="S4.T3.1.3.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">17.30</td>
<td id="S4.T3.1.3.1.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">19.74</td>
<td id="S4.T3.1.3.1.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">19.81</td>
<td id="S4.T3.1.3.1.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">22.91</td>
</tr>
<tr id="S4.T3.1.4.2" class="ltx_tr">
<th id="S4.T3.1.4.2.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T3.1.4.2.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">20.82</td>
<td id="S4.T3.1.4.2.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">34.87</td>
<td id="S4.T3.1.4.2.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">40.45</td>
<td id="S4.T3.1.4.2.5" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">41.25</td>
<td id="S4.T3.1.4.2.6" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">49.85</td>
</tr>
<tr id="S4.T3.1.5.3" class="ltx_tr">
<th id="S4.T3.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T3.1.5.3.1.1" class="ltx_text">Full</span></th>
<th id="S4.T3.1.5.3.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T3.1.5.3.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">6.48</td>
<td id="S4.T3.1.5.3.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">9.11</td>
<td id="S4.T3.1.5.3.5" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">9.31</td>
<td id="S4.T3.1.5.3.6" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">9.48</td>
<td id="S4.T3.1.5.3.7" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">10.21</td>
</tr>
<tr id="S4.T3.1.6.4" class="ltx_tr">
<th id="S4.T3.1.6.4.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T3.1.6.4.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.6.4.2.1" class="ltx_text ltx_font_bold">20.95</span></td>
<td id="S4.T3.1.6.4.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.6.4.3.1" class="ltx_text ltx_font_bold">35.09</span></td>
<td id="S4.T3.1.6.4.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.6.4.4.1" class="ltx_text ltx_font_bold">40.52</span></td>
<td id="S4.T3.1.6.4.5" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.6.4.5.1" class="ltx_text ltx_font_bold">41.53</span></td>
<td id="S4.T3.1.6.4.6" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.6.4.6.1" class="ltx_text ltx_font_bold">49.88</span></td>
</tr>
<tr id="S4.T3.1.7.5" class="ltx_tr">
<th id="S4.T3.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" colspan="2"><span id="S4.T3.1.7.5.1.1" class="ltx_text ltx_font_bold">FlanT5-base</span></th>
<td id="S4.T3.1.7.5.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">26.56</td>
<td id="S4.T3.1.7.5.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">43.42</td>
<td id="S4.T3.1.7.5.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">50.41</td>
<td id="S4.T3.1.7.5.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">51.35</td>
<td id="S4.T3.1.7.5.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">63.13</td>
</tr>
<tr id="S4.T3.1.8.6" class="ltx_tr">
<th id="S4.T3.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T3.1.8.6.1.1" class="ltx_text">PEFT</span></th>
<th id="S4.T3.1.8.6.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T3.1.8.6.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">22.72</td>
<td id="S4.T3.1.8.6.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">35.80</td>
<td id="S4.T3.1.8.6.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">42.05</td>
<td id="S4.T3.1.8.6.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">42.84</td>
<td id="S4.T3.1.8.6.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">51.61</td>
</tr>
<tr id="S4.T3.1.9.7" class="ltx_tr">
<th id="S4.T3.1.9.7.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T3.1.9.7.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.9.7.2.1" class="ltx_text ltx_font_bold">26.83</span></td>
<td id="S4.T3.1.9.7.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.9.7.3.1" class="ltx_text ltx_font_bold">43.20</span></td>
<td id="S4.T3.1.9.7.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.9.7.4.1" class="ltx_text ltx_font_bold">50.59</span></td>
<td id="S4.T3.1.9.7.5" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.9.7.5.1" class="ltx_text ltx_font_bold">51.43</span></td>
<td id="S4.T3.1.9.7.6" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.9.7.6.1" class="ltx_text ltx_font_bold">63.29</span></td>
</tr>
<tr id="S4.T3.1.10.8" class="ltx_tr">
<th id="S4.T3.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T3.1.10.8.1.1" class="ltx_text">Full</span></th>
<th id="S4.T3.1.10.8.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T3.1.10.8.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">11.78</td>
<td id="S4.T3.1.10.8.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">18.53</td>
<td id="S4.T3.1.10.8.5" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">20.59</td>
<td id="S4.T3.1.10.8.6" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">21.15</td>
<td id="S4.T3.1.10.8.7" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">24.17</td>
</tr>
<tr id="S4.T3.1.11.9" class="ltx_tr">
<th id="S4.T3.1.11.9.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T3.1.11.9.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">25.77</td>
<td id="S4.T3.1.11.9.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">41.78</td>
<td id="S4.T3.1.11.9.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">48.52</td>
<td id="S4.T3.1.11.9.5" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">49.39</td>
<td id="S4.T3.1.11.9.6" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">60.26</td>
</tr>
<tr id="S4.T3.1.12.10" class="ltx_tr">
<th id="S4.T3.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" colspan="2"><span id="S4.T3.1.12.10.1.1" class="ltx_text ltx_font_bold">FlanT5-large</span></th>
<td id="S4.T3.1.12.10.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">26.74</td>
<td id="S4.T3.1.12.10.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">40.53</td>
<td id="S4.T3.1.12.10.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">46.86</td>
<td id="S4.T3.1.12.10.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">47.57</td>
<td id="S4.T3.1.12.10.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">58.12</td>
</tr>
<tr id="S4.T3.1.13.11" class="ltx_tr">
<th id="S4.T3.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T3.1.13.11.1.1" class="ltx_text">PEFT</span></th>
<th id="S4.T3.1.13.11.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T3.1.13.11.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">25.72</td>
<td id="S4.T3.1.13.11.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">39.25</td>
<td id="S4.T3.1.13.11.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">45.34</td>
<td id="S4.T3.1.13.11.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">45.90</td>
<td id="S4.T3.1.13.11.7" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">55.26</td>
</tr>
<tr id="S4.T3.1.14.12" class="ltx_tr">
<th id="S4.T3.1.14.12.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T3.1.14.12.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.14.12.2.1" class="ltx_text ltx_font_bold">28.60</span></td>
<td id="S4.T3.1.14.12.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.14.12.3.1" class="ltx_text ltx_font_bold">43.45</span></td>
<td id="S4.T3.1.14.12.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.14.12.4.1" class="ltx_text ltx_font_bold">50.15</span></td>
<td id="S4.T3.1.14.12.5" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.14.12.5.1" class="ltx_text ltx_font_bold">50.91</span></td>
<td id="S4.T3.1.14.12.6" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="S4.T3.1.14.12.6.1" class="ltx_text ltx_font_bold">61.71</span></td>
</tr>
<tr id="S4.T3.1.15.13" class="ltx_tr">
<th id="S4.T3.1.15.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="S4.T3.1.15.13.1.1" class="ltx_text">Full</span></th>
<th id="S4.T3.1.15.13.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td id="S4.T3.1.15.13.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">7.67</td>
<td id="S4.T3.1.15.13.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">12.29</td>
<td id="S4.T3.1.15.13.5" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">12.01</td>
<td id="S4.T3.1.15.13.6" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">12.44</td>
<td id="S4.T3.1.15.13.7" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">13.31</td>
</tr>
<tr id="S4.T3.1.16.14" class="ltx_tr">
<th id="S4.T3.1.16.14.1" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td id="S4.T3.1.16.14.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;">25.54</td>
<td id="S4.T3.1.16.14.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;">40.88</td>
<td id="S4.T3.1.16.14.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;">46.82</td>
<td id="S4.T3.1.16.14.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;">47.87</td>
<td id="S4.T3.1.16.14.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;">58.60</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 3:</span> Accuracy of RAG for base and fine-tuned LLMs using different retrieval models.</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.1">본 연구에서는 덜 대중적인 지식을 중심으로 RAG 대 FT의 효과를 평가하기 위해 비교 분석을 수행했다. 우리의 결과는 FT가 모든 엔터티에 대해 일관된 성능 개선으로 이어지며 가장 인기 있는 범주와 가장 인기 없는 범주에서 가장 주목할 만한 이득을 볼 수 있음을 보여준다. 특히 미세 조정과 함께 사용할 때 RAG가 보다 효과적인 전략으로 두드러진다는 것을 발견했다. 그러나 이러한 이점은 더 큰 모델에서 감소한다. 또한, 검색 및 데이터 증강 모델의 성능이 향상됨에 따라 RAG 및 FT 전략의 성공이 향상됨을 관찰했다. 합성 데이터 품질의 중요한 중요성을 이해하면 향후 작업은 데이터 생성을 위한 효과적인 방법을 개발하는 데 중점을 둘 것이다.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1">우리의 연구는 주로 템플릿 기반 QA 작업을 다루었으며, 향후 연구가 다중 홉 QA <cite class="ltx_cite ltx_citemacro_citep">(Ho et al., <a class="ltx_ref" href="#bib.bib13" title="">2020</a>)</cite> 또는 Conversational QA <cite class="ltx_cite ltx_citemacro_citep">(Christmann et al., <a class="ltx_ref" href="#bib.bib7" title="">2022</a>)</cite>와 같은 더 복잡한 QA 문제를 해결할 수 있음을 시사한다. 또한, 합성 데이터 생성을 위해 Zephyr을 사용하는 것은 CoT(Chain of Thought)를 따르는 데 한계가 있으며, 데이터 품질을 향상시키고 미세 조정 비용을 줄이기 위한 고급 데이터 생성 기술의 잠재적인 이점을 강조한다.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p" id="Sx1.p1.1">이 간행물은 네덜란드 연구 위원회(NWO)에서 자금을 조달하는 연구 프로그램 NWA ORC 2020/21의 프로젝트 번호 NWA.1389.20.183을 가진 프로젝트 LESSEN의 일부이다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alberti et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/P19-1620" title="" class="ltx_ref ltx_href">Synthetic QA corpora generation with roundtrip consistency</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>, pages 6168–6173. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2023.ACL-TUTORIALS.6" title="" class="ltx_ref ltx_href">Retrieval-based language models and applications</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 41–46. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2310.11511" title="" class="ltx_ref ltx_href">Self-rag: Learning to retrieve, generate, and critique through self-reflection</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2310.11511.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Askari et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Arian Askari, Mohammad Aliannejadi, Chuan Meng, Evangelos Kanoulas, and Suzan Verberne. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.emnlp-main.623" title="" class="ltx_ref ltx_href">Expand, highlight, generate: Rl-driven document generation for passage reranking</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 10087–10099. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao Ling, and Sameer Singh. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2021.ACL-LONG.345" title="" class="ltx_ref ltx_href">Evaluating entity disambiguation and the role of popularity in retrieval-based NLP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>, pages 4472–4485. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, and et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://jmlr.org/papers/v24/22-1144.html" title="" class="ltx_ref ltx_href">Palm: Scaling language modeling with pathways</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>, 24:240:1–240:113.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christmann et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Philipp Christmann, Rishiraj&nbsp;Saha Roy, and Gerhard Weikum. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3477495.3531815" title="" class="ltx_ref ltx_href">Conversational question answering on heterogeneous sources</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">SIGIR ’22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022</em>, pages 144–154. ACM.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christmann et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Philipp Christmann, Rishiraj&nbsp;Saha Roy, and Gerhard Weikum. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2306.12235" title="" class="ltx_ref ltx_href">Compmix: A benchmark for heterogeneous question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.12235.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Le&nbsp;Hou, Shayne Longpre, Barret Zoph, Yi&nbsp;Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang&nbsp;Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent&nbsp;Y. Zhao, Yanping Huang, Andrew&nbsp;M. Dai, Hongkun Yu, Slav Petrov, Ed&nbsp;H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc&nbsp;V. Le, and Jason Wei. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2210.11416" title="" class="ltx_ref ltx_href">Scaling instruction-finetuned language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2210.11416.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de&nbsp;Luis&nbsp;Balaguer et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Maria&nbsp;Angels de&nbsp;Luis&nbsp;Balaguer, Vinamra Benara, Renato&nbsp;Luiz de&nbsp;Freitas&nbsp;Cunha, Roberto de&nbsp;M.&nbsp;Estevão&nbsp;Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo&nbsp;O. Nunes, Rafael Padilha, Morris Sharp, Bruno Silva, Swati Sharma, Vijay Aski, and Ranveer Chandra. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2401.08406" title="" class="ltx_ref ltx_href">RAG vs fine-tuning: Pipelines, tradeoffs, and a case study on agriculture</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2401.08406.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2305.14314" title="" class="ltx_ref ltx_href">Qlora: Efficient finetuning of quantized llms</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2305.14314.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Godbole and Jia (2023)</span>
<span class="ltx_bibblock">
Ameya Godbole and Robin Jia. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2023.FINDINGS-EACL.71" title="" class="ltx_ref ltx_href">Benchmarking long-tail generalization with likelihood splits</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023</em>, pages 933–953. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Xanh Ho, Anh-Khoa Duong&nbsp;Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.coling-main.580" title="" class="ltx_ref ltx_href">Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th International Conference on Computational Linguistics</em>, pages 6609–6625, Barcelona, Spain (Online). International Committee on Computational Linguistics.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=jKN1pXi7b0" title="" class="ltx_ref ltx_href">Unsupervised dense information retrieval with contrastive learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Trans. Mach. Learn. Res.</em>, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaddour et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2307.10169" title="" class="ltx_ref ltx_href">Challenges and applications of large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.10169.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kandpal et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v202/kandpal23a.html" title="" class="ltx_ref ltx_href">Large language models struggle to learn long-tail knowledge</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning, ICML</em>, volume 202 of <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 15696–15707. PMLR.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S.&nbsp;H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2020.EMNLP-MAIN.550" title="" class="ltx_ref ltx_href">Dense passage retrieval for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, pages 6769–6781. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasai et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan&nbsp;Le Bras, Akari Asai, Xinyan Yu, Dragomir&nbsp;R. Radev, Noah&nbsp;A. Smith, Yejin Choi, and Kentaro Inui. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2207.13332" title="" class="ltx_ref ltx_href">Realtime QA: what’s the answer right now?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2207.13332.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Patrick S.&nbsp;H. Lewis, Ludovic Denoyer, and Sebastian Riedel. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/P19-1484" title="" class="ltx_ref ltx_href">Unsupervised question answering by cloze translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>, pages 4896–4910. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Patrick S.&nbsp;H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="" class="ltx_ref ltx_href">Retrieval-augmented generation for knowledge-intensive NLP tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Patrick S.&nbsp;H. Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/TACL_A_00415" title="" class="ltx_ref ltx_href">PAQ: 65 million probably-asked questions and what you can do with them</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc. Comput. Linguistics</em>, 9:1098–1115.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2310.08319" title="" class="ltx_ref ltx_href">Fine-tuning llama for multi-stage text retrieval</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2310.08319.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2023.ACL-LONG.546" title="" class="ltx_ref ltx_href">When not to trust language models: Investigating effectiveness of parametric and non-parametric memories</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</em>, pages 9802–9822. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2023.FINDINGS-ACL.132" title="" class="ltx_ref ltx_href">Nonparametric masked language modeling</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 2097–2118. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mosbach et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2023.FINDINGS-ACL.779" title="" class="ltx_ref ltx_href">Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 12284–12314. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naveed et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Humza Naveed, Asad&nbsp;Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Nick Barnes, and Ajmal Mian. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2307.06435" title="" class="ltx_ref ltx_href">A comprehensive overview of large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.06435.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ovadia et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2312.05934" title="" class="ltx_ref ltx_href">Fine-tuning or retrieval? comparing knowledge injection in llms</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2312.05934.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://jmlr.org/papers/v21/20-074.html" title="" class="ltx_ref ltx_href">Exploring the limits of transfer learning with a unified text-to-text transformer</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>, 21:140:1–140:67.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Zaragoza (2009)</span>
<span class="ltx_bibblock">
Stephen&nbsp;E. Robertson and Hugo Zaragoza. 2009.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1561/1500000019" title="" class="ltx_ref ltx_href">The probabilistic relevance framework: BM25 and beyond</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Found. Trends Inf. Retr.</em>, 3(4):333–389.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sciavolino et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2021.EMNLP-MAIN.496" title="" class="ltx_ref ltx_href">Simple entity-centric questions challenge dense retrievers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, pages 6138–6148. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2021.FINDINGS-EMNLP.320" title="" class="ltx_ref ltx_href">Retrieval augmentation reduces hallucination in conversation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021</em>, pages 3784–3803. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soudani et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Heydar Soudani, Evangelos Kanoulas, and Faegheh Hasibi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3583780.3615291" title="" class="ltx_ref ltx_href">Data augmentation for conversational AI</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023</em>, pages 5220–5223. ACM.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Kai Sun, Yifan&nbsp;Ethan Xu, Hanwen Zha, Yue Liu, and Xin&nbsp;Luna Dong. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2308.10168" title="" class="ltx_ref ltx_href">Head-to-tail: How knowledgeable are large language models (llm)? A.K.A. will llms replace knowledge graphs?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2308.10168.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html" title="" class="ltx_ref ltx_href">BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tunstall et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander&nbsp;M. Rush, and Thomas Wolf. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2310.16944" title="" class="ltx_ref ltx_href">Zephyr: Direct distillation of LM alignment</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2310.16944.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ushio et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Asahi Ushio, Fernando Alva-Manchego, and Jose Camacho-Collados. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-acl.899" title="" class="ltx_ref ltx_href">An empirical comparison of LM-based question and answer generation methods</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 14262–14272, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaken et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Elad&nbsp;Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2022.ACL-SHORT.1" title="" class="ltx_ref ltx_href">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 1–9. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_font_bold ltx_title_appendix">Appendix</h2>

</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Data Preprocessing</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p" id="A1.p1.1">실제 QA <cite class="ltx_cite ltx_citemacro_citep">(Christmann et al., <a class="ltx_ref" href="#bib.bib8" title="">2023</a>; Sciavolino et al., <a class="ltx_ref" href="#bib.bib31" title="">2021</a>)</cite>를 제공하는 다양한 데이터 세트 중에서 <span class="ltx_text ltx_font_smallcaps" id="A1.p1.1.1">PopQA</span> <cite class="ltx_cite ltx_citemacro_citep">(Mallen et al., <a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>로 명명된 가장 최근의 데이터 세트 중 하나를 선택했다. <span class="ltx_text ltx_font_smallcaps" id="A1.p1.1.2">PopQA</span> 데이터셋은 위키피디아에서 추출한 16가지 유형의 관계를 기반으로 단일 엔터티 답변을 유도하는 것을 목표로 하는 대략 14,000개의 템플릿 질문을 포함한다. 각 엔터티의 인기를 평가하기 위해 위키피디아 페이지뷰를 인기 척도로 사용하여 이전 연구 <cite class="ltx_cite ltx_citemacro_citep">(Mallen et al., <a class="ltx_ref" href="#bib.bib24" title="">2023</a>; Sciavolino et al., <a class="ltx_ref" href="#bib.bib31" title="">2021</a>; Chen et al., <a class="ltx_ref" href="#bib.bib5" title="">2021</a>)</cite>를 따랐다. 2023년 시작부터 말까지 각 엔터티에 대한 페이지뷰를 축적했습니다.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p class="ltx_p" id="A1.p2.1">또한 각 엔터티에 대한 관련 위키피디아 페이지를 획득했습니다. 그런 다음 각 위키피디아 페이지를 요약 단락과 후속 추가 단락으로 분할하여 코퍼스를 구성했다. 이를 위해 <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/wikipedia" target="_blank" title="">https://huggingface.co/datasets/wikipedia</a>의 HuggingFace 데이터셋 리포지토리에서 사용 가능한 2022년 3월 위키피디아 덤프를 활용했다.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>QA Generation</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Prompt Method</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A2.SS1.p1.1">주어진 컨텍스트를 기반으로 합성 QA를 생성하기 위해 다음 프롬프트를 사용했습니다.  <span class="ltx_inline-block ltx_framed ltx_framed_rectangle" id="A2.SS1.p1.1.1" style="border-color: #000000;"> <span class="ltx_p" id="A2.SS1.p1.1.1.1"><span class="ltx_text ltx_font_italic" id="A2.SS1.p1.1.1.1.1">You is a question-answer generator. 귀하의 목표는 컨텍스트가 주어진 질문-응답 쌍을 생성하는 것입니다. <span> <span class="ltx_p" id="A2.SS1.p1.1.1.2.1">Example output:</span> {'question': ”, ’answer’: ”}  <br class="ltx_break"/></span> <span class="ltx_p" id="A2.SS1.p1.1.1.3.1">Context:</span> <CONTEXT  <br class="ltx_break"/></span> <span class="ltx_p" id="A2.SS1.p1.1.1.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p1.1.1.1.1.1.1">Step 1:<span class="ltx_text ltx_font_bold ltx_font_medium" id="A2.SS1.p1.1.1.1.1 <span></span></span> <span class="ltx_p" id="A2.SS1.p1.1.1.5"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p1.1.1.5.1">Step 2:<span class="ltx_text ltx_font_medium" id="A2.SS1.p1.1.1.1.1.1.1">각 식별된 스팬에 대해 질문을 생성합니다. <span></span></span> <span class="ltx_p" id="A2.SS1.p1.1.1.6"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p1.1.1.6.1">Step 3:<span class="ltx_text ltx_font_medium" id="A2.SS1.p1.1.1.6.1.1">Respond to the question in only a few tokens concisely. <span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p1.1.1.7.1">Step 4:<span class="ltx_text ltx_font_medium" id="A2.SS1.p1.1.1.7.1">Step 4:<span class="ltx_text ltx_font_medium" id="A2.SS1.p1.1.1.7.1">Output in JSON format following […]</span></span> <br class="ltx_break"/></span> <span class="ltx_p" id="A2.SS1.p1.1.1.8"><span class="ltx_text ltx_font_italic" id="A2.SS1.p1.1.1.8.1">Ensure that you distinctly label and delineate Steps 1, 2, 3, 4. Let’s think step by step:</</p>
</div>
<div id="A2.SS1.p2" class="ltx_para">
<p class="ltx_p" id="A2.SS1.p2.1">테스트 세트는 단일 엔터티 답변을 특징으로 하는 템플릿 기반 QA로 구성됩니다. 테스트 세트의 QA와 유사한 QA를 생성하기 위해 LLM에 CoT(Chain of Thought) 접근법을 활용하도록 지시했다. 그러나 생성된 모든 QA가 지침에 따르지 않는다는 것을 관찰했다. 다음은 이 프롬프트 기반 접근 방식을 사용하여 생성된 QA의 몇 가지 예입니다.</p>
</div>
<div id="A2.SS1.p3" class="ltx_para">
<span id="A2.SS1.p3.1" class="ltx_inline-block ltx_framed ltx_framed_rectangle" style="border-color: #000000;">
<span id="A2.SS1.p3.1.1" class="ltx_p"><span id="A2.SS1.p3.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Entity:</span> John Mayne 
<br class="ltx_break"></span>
<span id="A2.SS1.p3.1.2" class="ltx_p"><span id="A2.SS1.p3.1.2.1" class="ltx_text ltx_font_bold ltx_font_italic">Context:</span>
John Mayne was a Scottish printer, journalist and poet born in Dumfries. In 1780, his poem The Siller Gun appeared in its original form in Ruddiman’s Magazine, published by Walter Ruddiman in Edinburgh. It is a humorous work on an ancient custom in Dumfries of shooting for the S̈iller Gun.Ḧe also wrote a poem on Hallowe’en in 1780 which influenced Robert Burns’s 1785 poem Halloween. Mayne also wrote a version of the ballad Helen of Kirkconnel. His verses were admired by Walter Scott. 
<br class="ltx_break"></span>
<span id="A2.SS1.p3.1.3" class="ltx_p"><span id="A2.SS1.p3.1.3.1" class="ltx_text ltx_font_bold ltx_font_italic">Synthetic QAs:</span></span>
<span id="A2.SS1.p3.1.4" class="ltx_p"><span id="A2.SS1.p3.1.4.1" class="ltx_text ltx_font_italic">question 1:</span> "Who was John Mayne?"</span>
<span id="A2.SS1.p3.1.5" class="ltx_p"><span id="A2.SS1.p3.1.5.1" class="ltx_text ltx_font_italic">answer 1:</span> "John Mayne was a Scottish printer, journalist, and poet from Dumfries. (1780 poem: The Siller Gun)" 
<br class="ltx_break"></span>
<span id="A2.SS1.p3.1.6" class="ltx_p"><span id="A2.SS1.p3.1.6.1" class="ltx_text ltx_font_italic">question 2:</span> "What poem did John Mayne write in 1780 called Hallowe’en?",</span>
<span id="A2.SS1.p3.1.7" class="ltx_p"><span id="A2.SS1.p3.1.7.1" class="ltx_text ltx_font_italic">answer 2:</span> "Hallowe’en is a poem by John Mayne (1780)"</span>
</span>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>E2E Method</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A2.SS2.p1.1">QAG(Question Answer Generation) 작업을 위해 훈련된 모델을 사용했으며, 특히 End-to-End(E2E) 접근법에 최적화된 T5-large 모델을 사용하여 QA에서 다운스트림 작업인 <cite class="ltx_cite ltx_citemacro_citep">(Ushio et al., <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>로 우수한 성능을 제공하는 것으로 보고되었다. 그러나 E2E 방법의 주목할 만한 단점은 생성된 샘플의 품질과 양에 대한 제한된 제어이다. 또한 T5 모델의 입력 토큰 제한으로 인해 입력 텍스트는 512 토큰으로 제한되어야 한다. 공정한 비교를 보장하기 위해 프롬프트 기반 접근법과 함께 사용된 입력에 동일한 토큰 제한을 적용했다. 표 <a class="ltx_ref" href="#A2.T4" title="Table 4 ‣ B.2 E2E Method ‣ Appendix B QA Generation ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">4</span></a>는 동일한 코퍼스를 사용하여 프롬프트 및 E2E 방법 모두에 의해 생성된 QA에 대한 통계를 제시한다.</p>
</div>
<figure id="A2.T4" class="ltx_table">
<table id="A2.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T4.1.1.1" class="ltx_tr">
<th id="A2.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="A2.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Relationship</span></th>
<th id="A2.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" rowspan="2"><span id="A2.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">#Ent.</span></th>
<td id="A2.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" colspan="2"><span id="A2.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">Train-set</span></td>
<td id="A2.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;" colspan="2"><span id="A2.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">Dev-set</span></td>
</tr>
<tr id="A2.T4.1.2.2" class="ltx_tr">
<td id="A2.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</td>
<td id="A2.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</td>
<td id="A2.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</td>
<td id="A2.T4.1.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</td>
</tr>
<tr id="A2.T4.1.3.3" class="ltx_tr">
<th id="A2.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">occupation</th>
<th id="A2.T4.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">532</th>
<td id="A2.T4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">16398</td>
<td id="A2.T4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">1872</td>
<td id="A2.T4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">1822</td>
<td id="A2.T4.1.3.3.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">207</td>
</tr>
<tr id="A2.T4.1.4.4" class="ltx_tr">
<th id="A2.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">place of birth</th>
<th id="A2.T4.1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">584</th>
<td id="A2.T4.1.4.4.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">19993</td>
<td id="A2.T4.1.4.4.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">2043</td>
<td id="A2.T4.1.4.4.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">2221</td>
<td id="A2.T4.1.4.4.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">227</td>
</tr>
<tr id="A2.T4.1.5.5" class="ltx_tr">
<th id="A2.T4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">genre</th>
<th id="A2.T4.1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">1619</th>
<td id="A2.T4.1.5.5.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">56101</td>
<td id="A2.T4.1.5.5.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">5338</td>
<td id="A2.T4.1.5.5.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">6233</td>
<td id="A2.T4.1.5.5.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">593</td>
</tr>
<tr id="A2.T4.1.6.6" class="ltx_tr">
<th id="A2.T4.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">father</th>
<th id="A2.T4.1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">570</th>
<td id="A2.T4.1.6.6.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">37676</td>
<td id="A2.T4.1.6.6.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">2092</td>
<td id="A2.T4.1.6.6.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">4186</td>
<td id="A2.T4.1.6.6.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">406</td>
</tr>
<tr id="A2.T4.1.7.7" class="ltx_tr">
<th id="A2.T4.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">country</th>
<th id="A2.T4.1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">838</th>
<td id="A2.T4.1.7.7.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">19362</td>
<td id="A2.T4.1.7.7.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">2769</td>
<td id="A2.T4.1.7.7.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">2151</td>
<td id="A2.T4.1.7.7.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">307</td>
</tr>
<tr id="A2.T4.1.8.8" class="ltx_tr">
<th id="A2.T4.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">producer</th>
<th id="A2.T4.1.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">1520</th>
<td id="A2.T4.1.8.8.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">41992</td>
<td id="A2.T4.1.8.8.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">4235</td>
<td id="A2.T4.1.8.8.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">4665</td>
<td id="A2.T4.1.8.8.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">470</td>
</tr>
<tr id="A2.T4.1.9.9" class="ltx_tr">
<th id="A2.T4.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">director</th>
<th id="A2.T4.1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">1999</th>
<td id="A2.T4.1.9.9.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">27578</td>
<td id="A2.T4.1.9.9.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">4644</td>
<td id="A2.T4.1.9.9.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">3064</td>
<td id="A2.T4.1.9.9.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">515</td>
</tr>
<tr id="A2.T4.1.10.10" class="ltx_tr">
<th id="A2.T4.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">capital of</th>
<th id="A2.T4.1.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">363</th>
<td id="A2.T4.1.10.10.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">109141</td>
<td id="A2.T4.1.10.10.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">3388</td>
<td id="A2.T4.1.10.10.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">12126</td>
<td id="A2.T4.1.10.10.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">376</td>
</tr>
<tr id="A2.T4.1.11.11" class="ltx_tr">
<th id="A2.T4.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">screenwriter</th>
<th id="A2.T4.1.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">1999</th>
<td id="A2.T4.1.11.11.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">59680</td>
<td id="A2.T4.1.11.11.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">6932</td>
<td id="A2.T4.1.11.11.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">6631</td>
<td id="A2.T4.1.11.11.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">770</td>
</tr>
<tr id="A2.T4.1.12.12" class="ltx_tr">
<th id="A2.T4.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">composer</th>
<th id="A2.T4.1.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">978</th>
<td id="A2.T4.1.12.12.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">46574</td>
<td id="A2.T4.1.12.12.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">4064</td>
<td id="A2.T4.1.12.12.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">5174</td>
<td id="A2.T4.1.12.12.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">451</td>
</tr>
<tr id="A2.T4.1.13.13" class="ltx_tr">
<th id="A2.T4.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">color</th>
<th id="A2.T4.1.13.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">34</th>
<td id="A2.T4.1.13.13.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">4396</td>
<td id="A2.T4.1.13.13.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">160</td>
<td id="A2.T4.1.13.13.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">488</td>
<td id="A2.T4.1.13.13.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">17</td>
</tr>
<tr id="A2.T4.1.14.14" class="ltx_tr">
<th id="A2.T4.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">religion</th>
<th id="A2.T4.1.14.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">338</th>
<td id="A2.T4.1.14.14.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">18776</td>
<td id="A2.T4.1.14.14.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">1449</td>
<td id="A2.T4.1.14.14.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">2086</td>
<td id="A2.T4.1.14.14.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">161</td>
</tr>
<tr id="A2.T4.1.15.15" class="ltx_tr">
<th id="A2.T4.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">sport</th>
<th id="A2.T4.1.15.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">547</th>
<td id="A2.T4.1.15.15.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">14760</td>
<td id="A2.T4.1.15.15.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">1710</td>
<td id="A2.T4.1.15.15.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">1629</td>
<td id="A2.T4.1.15.15.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">189</td>
</tr>
<tr id="A2.T4.1.16.16" class="ltx_tr">
<th id="A2.T4.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">author</th>
<th id="A2.T4.1.16.16.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">1514</th>
<td id="A2.T4.1.16.16.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">46399</td>
<td id="A2.T4.1.16.16.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">5319</td>
<td id="A2.T4.1.16.16.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">5155</td>
<td id="A2.T4.1.16.16.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">590</td>
</tr>
<tr id="A2.T4.1.17.17" class="ltx_tr">
<th id="A2.T4.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">mother</th>
<th id="A2.T4.1.17.17.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">187</th>
<td id="A2.T4.1.17.17.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">7592</td>
<td id="A2.T4.1.17.17.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">477</td>
<td id="A2.T4.1.17.17.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">843</td>
<td id="A2.T4.1.17.17.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">52</td>
</tr>
<tr id="A2.T4.1.18.18" class="ltx_tr">
<th id="A2.T4.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">capital</th>
<th id="A2.T4.1.18.18.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">645</th>
<td id="A2.T4.1.18.18.3" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">28467</td>
<td id="A2.T4.1.18.18.4" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">1322</td>
<td id="A2.T4.1.18.18.5" class="ltx_td ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">3162</td>
<td id="A2.T4.1.18.18.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.25pt;padding-bottom:1.25pt;">146</td>
</tr>
<tr id="A2.T4.1.19.19" class="ltx_tr">
<th id="A2.T4.1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">All</th>
<th id="A2.T4.1.19.19.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">14,267</th>
<td id="A2.T4.1.19.19.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">491,525</td>
<td id="A2.T4.1.19.19.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">38,114</td>
<td id="A2.T4.1.19.19.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">56,803</td>
<td id="A2.T4.1.19.19.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">5,949</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 4:</span> 두 가지 방법을 사용하여 생성된 QA 쌍에 대한 Statistics. T5-대형을 사용하는 E2E 접근법은 프롬프트 방법에 비해 12배 이상 더 많은 QA를 생성한다.</figcaption>
</figure>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Fine-Tuning Setup</h2>

<div id="A3.p1" class="ltx_para">
<p class="ltx_p" id="A3.p1.1">FlanT5(small, base, large) <cite class="ltx_cite ltx_citemacro_citep">(Chung et al., <a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite> 모델의 세 가지 버전을 미세 조정했다. 미세 조정 실험은 전체 매개변수 조정과 PEFT를 모두 포함했다. 특히 QLoRA 방법<cite class="ltx_cite ltx_citemacro_citep">(Dettmers et al., <a class="ltx_ref" href="#bib.bib11" title="">2023</a>)</cite>에 초점을 맞추었으며, 이는 훈련 가능한 순위 분해 행렬을 트랜스포머 아키텍처의 각 계층에 통합하면서 사전 훈련된 모델 매개변수를 고정적으로 유지하는 접근법으로 널리 알려져 있다. 다양한 수의 훈련 에포크로 실험했음에도 불구하고, 우리는 결과가 특정 시점 이후에 안정되는 것을 관찰했다. 표 <a class="ltx_ref" href="#A3.T5" title="Table 5 ‣ Appendix C Fine-Tuning Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">5</span></a>에는 미세 조정을 위한 최종 하이퍼파라미터가 포함되어 있다.</p>
</div>
<figure id="A3.T5" class="ltx_table">
<table id="A3.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T5.1.1.1" class="ltx_tr">
<th id="A3.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="A3.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Hyperparameter</span></th>
<th id="A3.T5.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span id="A3.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T5.1.2.1" class="ltx_tr">
<th id="A3.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">Epochs</th>
<td id="A3.T5.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:1.25pt;padding-bottom:1.25pt;">10</td>
</tr>
<tr id="A3.T5.1.3.2" class="ltx_tr">
<th id="A3.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Batch size</th>
<td id="A3.T5.1.3.2.2" class="ltx_td ltx_align_right" style="padding-top:1.25pt;padding-bottom:1.25pt;">128</td>
</tr>
<tr id="A3.T5.1.4.3" class="ltx_tr">
<th id="A3.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">Learing rate</th>
<td id="A3.T5.1.4.3.2" class="ltx_td ltx_align_right" style="padding-top:1.25pt;padding-bottom:1.25pt;">2e-4</td>
</tr>
<tr id="A3.T5.1.5.4" class="ltx_tr">
<th id="A3.T5.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">PEFT, alpha</th>
<td id="A3.T5.1.5.4.2" class="ltx_td ltx_align_right" style="padding-top:1.25pt;padding-bottom:1.25pt;">16</td>
</tr>
<tr id="A3.T5.1.6.5" class="ltx_tr">
<th id="A3.T5.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:1.25pt;padding-bottom:1.25pt;">PEFT, rank</th>
<td id="A3.T5.1.6.5.2" class="ltx_td ltx_align_right" style="padding-top:1.25pt;padding-bottom:1.25pt;">32</td>
</tr>
<tr id="A3.T5.1.7.6" class="ltx_tr">
<th id="A3.T5.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;">PEFT, dropout</th>
<td id="A3.T5.1.7.6.2" class="ltx_td ltx_align_right ltx_border_b" style="padding-top:1.25pt;padding-bottom:1.25pt;">0.05</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span> Fine-Tuning용 하이퍼파라미터: 철저한 하이퍼파라미터 검색에 따라 모델의 모든 버전에 걸쳐 하이퍼파라미터를 표준화했습니다.</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Detailed Results</h2>

<div id="A4.p1" class="ltx_para">
<p class="ltx_p" id="A4.p1.1">이 섹션에서는 주요 결과를 확증하는 추가 결과를 제공한다. 그림 <a class="ltx_ref" href="#A4.F4" title="Figure 4 ‣ Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">4</span></a>는 RAG가 있거나 없는 미세 조정 대 비미세 조정이 FlanT5 기반 및 FlanT5 대형 모델에 미치는 영향을 보여준다. 또한 미세 조정이 가장 인기 없는 버킷과 인기 있는 버킷 모두에서 정확도를 향상시킨다는 것을 보여줍니다. 기본 모델의 경우 최소 인기 버킷에 대한 정확도 향상이 소형 및 대형 모델에 비해 덜 중요하다는 점에 유의해야 한다.</p>
</div>
<figure id="A4.F4" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.01432/assets/x4.png" id="A4.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="221" height="277" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 4:</span>질문에서의 주제 엔티티의 인기와 QA에서의 FlanT5-base 및 FlanT5-large의 성능에 대한 RAG 및 FT의 영향 사이의 상관 관계.</figcaption>
</figure>
<div id="A4.p2" class="ltx_para">
<p class="ltx_p" id="A4.p2.1">그림 <a class="ltx_ref" href="#A4.F5" title="Figure 5 ‣ Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">5</span></a>는 다양한 검색 모델에 걸쳐 미세 조정하기 전의 RAG 접근법의 정확도를 보여준다. 해당 위키피디아 페이지의 요약 섹션을 검색하는 이상 검색기가 가장 높은 정확도를 제공합니다.</p>
</div>
<figure id="A4.F5" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.01432/assets/x5.png" id="A4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="341" height="328" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 5:</span> <span class="ltx_text ltx_font_smallcaps" id="A4.F5.2.1">PopQA</span> 데이터 세트의 모든 관계에 걸쳐 미세 조정 이전의 RAG 접근법의 정확도.</figcaption>
</figure>
<div id="A4.p3" class="ltx_para">
<p class="ltx_p" id="A4.p3.1">그림 <a class="ltx_ref" href="#A4.F6" title="Figure 6 ‣ Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">6</span></a>는 모든 관계에 걸쳐 RAG가 있거나 없는 미세 조정 전후의 성능을 비교한다. 또한 미세 조정만으로는 대부분의 관계에 대해 RAG 방법과 동일한 수준의 정확도를 달성하지 못한다는 것을 보여준다.</p>
</div>
<figure id="A4.F6" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.01432/assets/x6.png" id="A4.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="341" height="328" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 6:</span> <span class="ltx_text ltx_font_smallcaps" id="A4.F6.2.1">PopQA</span> 데이터 세트의 모든 관계에 걸쳐 미세 조정 전후의 RAG 접근법의 정확도.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2403.01431" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2403.01432" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2403.01432">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.01432" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2403.01433" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 16:22:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
# Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge

 Heydar Soudani

Radboud University

Nijmegen

The Netherlands

heydar.soudani@ru.nl

Evangelos Kanoulas

University of Amsterdam

Amsterdam

The Netherlands

e.kanoulas@uva.nl

Faegheh Hasibi

Radboud University

Nijmegen

The Netherlands

faegheh.hasibi@ru.nl

###### Abstract

Large language models (LLMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LLMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LLMs in handling low-frequency entities on question answering task. Our findings indicate that FT significantly boosts the performance across entities of varying popularity, especially in the most and least popular groups, while RAG surpasses other methods. Additionally, the success of both RAG and FT approaches is amplified by advancements in retrieval and data augmentation techniques. The code and data is available at [https://github.com/informagi/RAGvsFT](https://github.com/informagi/RAGvsFT).

## 1 Introduction

Large Language Models (LLMs) exhibit outstanding capabilities in executing tasks that demand extensive memorization of factual data (Chowdhery et al., 2023). However, their memorization capabilities are constrained when dealing with less frequent entities (Mallen et al., 2023; Kandpal et al., 2023; Sun et al., 2023), and even the largest models may encounter the well-known "hallucination" problem (Shuster et al., 2021) and temporal degradation (Kasai et al., 2022). Consequently, when LLMs are intended for deployment in less resourced domains, customization becomes imperative to ensure optimal performance. A common example is within the industrial setup, where chatbots or Question Answering (QA) systems need to accurately answer users' questions about a proprietary knowledge graph or intra-company terminology with limited textual description.

Retrieval-Augmented Generation (RAG) and Fine-Tuning (FT) stand out as two prominent approaches for adapting LLMs to specific domains. RAG retrieves relevant information from a document corpus and enhances LLM's response generation through the implementation of in-context learning (ICL). Conversely, the fine-tuning approach updates model weights to become adept at recalling specific information and enhance its memorization capabilities during inference. In the context of less popular knowledge, where limited data is available, data augmentation methods are utilized to generate synthetic training data, serving as an initial step towards fine tuning.

In this paper, we aim to understand which approach and under what conditions is more effective for industry-specific models. Specifically, we seek to answer the following research questions:

**(RQ1)** What is the effectiveness of RAG and fine-tuning with synthetic data on QA for low-frequency factual knowledge?

**(RQ2)** Which parameters, including the quality of

Figure 1: Correlation between subject entity popularity in a question and the effects of RAG and FT on FlanT5-small performance in open-domain question answering. FT markedly improves accuracy in the initial and final buckets relative to others (indicated by the pink line).

synthetic samples, the method to be fine-tuned, the model size, and the performance of retrieval models affect the downstream performance?

To address our RQs, we performed a comprehensive comparison of RAG and fine tuning methods, with specific attention to less popular knowledge. Our evaluation setup explores various factors, including model size, retrieval models, the quality of synthetic data generation, and the fine-tuning method (PEFT vs. full fine tuning).

Our findings indicate that FT consistently enhances performance for entities, both popular and less popular, with the most substantial improvements observed in the most and least popular categories. Furthermore, RAG consistently outperforms fine tuning methods, particularly when combined with FT in smaller models, a benefit that diminishes in base models and is non-existent in larger models. Lastly, the effectiveness of both RAG and FT strategies increases with improvements in the performance of the retrieval and data augmentation models.

## 2 Background

**Data Augmentation (DA).** Data availability is crucial for fine-tuning in specialized domains. DA addresses data scarcity problem by generating task- and domain-relevant samples from existing unlabeled texts. A common DA approach for the QA task is generating question-answer pairs through a four-step _Pipeline_, consisting of: passage selection, answer extraction, question generation, and consistency filtering Alberti et al. (2019); Lewis et al. (2019, 2021); Ushio et al. (2023). Ushio et al. (2023) conducted an empirical study comparing three question answer generation approaches: Pipeline, Multitask, and End-to-End (E2E) and showed the E2E approach outperforms others in downstream tasks. Recently, the utilization of LLMs to generate data is shown effective in information retrieval, QA, and dialogue creation tasks Soudani et al. (2023); Askari et al. (2023).

**Retrieval Augmented Generation.** RAG enhances LLMs by integrating external knowledge sources with input queries, enriching the model with additional context for knowledge-intensive tasks Lewis et al. (2020). It utilizes an information retrieval system to find relevant documents and adds them to the input prompt to enhance response generation of an LLM Asai et al. (2023, 2023).

**Less popular Knowledge.** An entity's popularity in an LLM is gauged by its frequency in the model's pre-training data Godbole and Jia (2023); Min et al. (2023), often assessed through the entity's occurrence in a large corpus Kandpal et al. (2023). Due to the practical challenges of direct counting, approximations like traffic metrics and content density are used Sun et al. (2023). Wikipedia pageviews are among the most prevalent methods for measuring the popularity of entities Mallen et al. (2023); Sciavolino et al. (2021); Chen et al. (2021).

**Comparing FT vs. RAG**. As interest grows in refining pre-trained language models for particular tasks, the comparison of FT and RAG strategies under equitable conditions is becoming increasingly important. Mosbach et al. (2023) explored the effectiveness of few-shot FT versus In-context Learning for classification tasks in general domains. de Luis Balaguer et al. (2024) compared FT and RAG in answering long, agriculture, and geography-specific questions. Ovadia et al. (2023) assessed the performance on multiple-choice questions in specialized areas like Anatomy, Astronomy, College Biology, and Prehistory. In contrast to these studies, we directly address the integration of less popular factual knowledge into LLMs, comparing various retrieval, data augmentation, and fine tuning methods.

## 3 Evaluation Setup

We evaluate LLMs on the closed-book QA task, focusing on the PopQA dataset Mallen et al. (2023) characterized by questions that covers a long-tail entity distribution. It also includes unique Wikipedia titles, facilitating the calculation of pageviews and identification of relevant Wikipedia pages. We acquire the relevant Wikipedia page for each subject entity in the PopQA dataset and divide the entities into five buckets based on their popularity levels (Figure 2).

To ensure a fair comparison between DA and

Figure 2: Distribution of sample counts across buckets, defined by \(log_{10}\)(pageviews). The leftmost bin includes entities with fewer than \(10^{2}\) pageviews, while the right-most bin encompasses entities with over \(10^{5}\) pageviews.

[MISSING_PAGE_FAIL:3]

ments in the most and least popular buckets. While fine-tuning boosts performance for less popular categories in smaller models, this advantage decreases in base models and disappears in larger models; see Figure 4 for more details. This is likely due to the improved memorization of larger models.

**Effect of external parameters on RAG and FT.** We delve into additional factors that influence model specialization in processing less popular knowledge. A key aspect under review is the effect of full tuning versus PEFT. Table 2 shows PEFT leads to smaller gains in the _+FT-RAG_ compared to full FT, yet it significantly improves accuracy in the _+FT+RAG_ setup. This suggests that PEFT enables the LLM to maintain its reasoning abilities based on the provided prompts.

Our investigation also covers two QA generation techniques. The E2E method generates over 12 times more QA than the prompting method (cf. Table 4 in Appendix C), while prompt-based method generates more quality QAs. The results in Table 2 show that fine-tuned models trained on prompt-generated data outperform E2E-generated ones. This highlights the significance of synthetic data quality over quantity.

The performance of the retrieval model is another important consideration. Table 3 presents the QA system's accuracy utilizing various retrieval strategies within the RAG framework. The findings demonstrate a direct correlation between the retrieval model performance and the overall QA accuracy, underscoring the retrieval model's impact on the downstream task's effectiveness.

## 5 Conclusion

In this study, we performed a comparative analysis to evaluate the effectiveness of RAG versus FT, with a focus on less popular knowledge. Our results reveal that FT leads to consistent performance improvements for all entities, with the most notable gains seen in the most and least popular categories. We found that RAG stands out as a more effective strategy, especially when used in combination with fine-tuning. This advantage decreases however in larger models. Additionally, we observed that the success of both RAG and FT strategies improves with the enhancement of the retrieval and data augmentation models' performance. Understanding the critical importance of synthetic data quality, future work will focus on developing an effective method for data creation.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline  & & **-FT/** & **-FT/** & **+FT/** & **+FT/** \\
**FT** & **QA** & **-RAG** & **+RAG** & **-RAG** & **+RAG** \\ \hline \multicolumn{5}{l}{**FianT5-small**} \\ \hline \multirow{5}{*}{\begin{tabular}{l} PEFT \\ Full \\ \end{tabular} } & E2E & & & 5.53 & 22.91 \\  & Prompt & & & 7.01 & 49.85 \\  & E2E & & & 6.35 & 10.21 \\  & Prompt & & & **8.52** & **49.88** \\ \hline \multicolumn{5}{l}{**FianT5-base**} \\ \hline \multirow{5}{*}{\begin{tabular}{l} PEFT \\ Full \\ \end{tabular} } & E2E & & & 6.94 & 51.61 \\  & Prompt & & 9.92 & **63.29** \\  & E2E & & & 8.63 & 24.17 \\  & Prompt & & & **11.41** & 60.26 \\ \hline \multicolumn{5}{l}{**FianT5-large**} \\ \hline \multirow{5}{*}{
\begin{tabular}{l} PEFT \\ Full \\ \end{tabular} } & E2E & & & 8.22 & 55.26 \\  & Prompt & & 12.15 & **61.71** \\  & E2E & & **16.23** & 13.31 \\  & Prompt & & & 13.91 & 58.60 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Accuracy of base and fine-tuned models, both with and without RAG. The RAG results presented are based on ideal retrieval.

Figure 3: Recall@1 for four retrieval models across different popularity levels. The results indicate that retrieval models perform more effectively with less popular knowledge compared to more popular ones.

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline \multicolumn{1}{c}{**FT**} & **QA** & **RAG** & **RAG** & **RAG** & **RAG** \\ \hline \multicolumn{1}{c}{**FianT5-small**} & 9.76 & 17.22 & 20.80 & 21.41 & 26.13 \\ \hline \multirow{5}{*}{\begin{tabular}{l} PEFT \\ Full \\ \end{tabular} } & E2E & 13.13 & 17.30 & 19.74 & 19.81 & 22.91 \\  & Prompt & 20.82 & 34.87 & 40.45 & 41.25 & 49.85 \\  & E2E & 6.48 & 9.11 & 9.31 & 9.48 & 10.21 \\  & Prompt & **20.95** & **35.09** & **40.52** & **41.53** & **49.88** \\ \hline \multicolumn{1}{c}{**FianT5-base**} & 26.56 & 43.42 & 50.41 & 51.35 & 63.13 \\ \hline \multirow{5}{*}{\begin{tabular}{l} PEFT \\ Full \\ \end{tabular} } & E2E & 22.72 & 35.80 & 42.05 & 42.84 & 51.61 \\  & Prompt & **26.83** & **43.20** & **50.59** & **51.43** & **63.29** \\ \cline{1-1}  & E2E & 11.78 & 18.53 & 20.59 & 21.15 & 24.17 \\ \cline{1-1}  & Prompt & 25.77 & 41.78 & 48.52 & 49.39 & 60.26 \\ \hline \multicolumn{1}{c}{**FianT5-large**} & 26.74 & 40.53 & 46.86 & 47.57 & 58.12 \\ \hline \multirow{5}{*}{
\begin{tabular}{l} PEFT \\ Full \\ \end{tabular} } & E2E & 25.72 & 39.25 & 45.34 & 45.90 & 55.26 \\  & Prompt & **28.60** & **43.45** & **50.15** & **50.91** & **61.71** \\ \cline{1-1}  & E2E & 7.67 & 12.29 & 12.01 & 12.44 & 13.31 \\ \cline{1-1}  & Prompt & 25.54 & 40.88 & 46.82 & 47.87 & 58.60 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Accuracy of RAG for base and fine-tuned LLMs using different retrieval models.

## 6 Limitations

Our study primarily addressed a template-based QA task, suggesting future research could tackle more complex QA challenges, such as multi-hop QA Ho et al. (2020) or Conversational QA Christmann et al. (2022). Furthermore, we noted that our use of Zephyr for synthetic data generation has limitations in following Chain of Thought (CoT), highlighting the potential benefits of advanced data generation techniques to enhance data quality and reduce fine-tuning cost.

## Acknowledgements

This publication is part of the project LESSEN with project number NWA.1389.20.183 of the research program NWA ORC 2020/21 which is (partly) financed by the Dutch Research Council (NWO).

## References

* A. Asai, S. Min, Z. Zhong, and D. Chen (2023)Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 41-46. External Links: Link Cited by: SS1.
* A. Asai, S. Wu, Y. Wang, A. Sil, and H. Hajishirzi (2023)Self-rag: learning to retrieve, generate, and critique through self-reflection. CoRRabs/2310.11511. External Links: Link, 2310.11511 Cited by: SS1.
* A. Askari, M. Aliannejadi, C. Meng, E. Kanoulas, and S. Verberne (2023)Ex-pand, highlight, generate: rl-driven document generation for passage reranking. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 10087-10099. External Links: Link, 2007.10099 Cited by: SS1.
* A. Chen, P. Gudipati, S. Longpre, X. Ling, and S. Singh (2021)Evaluating entity disambiguation and the role of popularity in retrieval-based NLP. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Volume 1: Long Papers, Virtual Event, August 1-6, 2021, pp. 4472-4485. External Links: Link, 2106.08786 Cited by: SS1.
* A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, and et al. (2023)Palm: scaling language modeling with pathways. J. Mach. Learn. Res.24 (2), pp. 1-240:113. External Links: Link, 2304.1137 Cited by: SS1.
* 15, 2022, pp. 144-154. External Links: Link, 2106.08786 Cited by: SS1.
* P. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, S. Narang, G. Mishra, A. Yu, V. Y. Zhao, Y. Huang, A. M. Dai, H. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei (2022)Scaling instruction-finetuned language models. CoRRabs/2210.11416. External Links: Link, 2106.11416 Cited by: SS1.
* M. Angels de Luis Balaguer, V. Benara, R. Luiz de Freitas Cunha, R. de M. Estevao Filho, T. Hendry, D. Holstein, J. Marsman, N. Mecklenburg, S. Malvar, L. O. Nunes, R. Padilha, M. Sharp, B. Silva, S. Sharma, V. Aski, and R. Chandra (2024)RAG vs fine-tuning: pipelines, tradeoffs, and a case study on agriculture. CoRRabs/2401.08406. External Links: Link, 2401.08406 Cited by: SS1.
* T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer (2023)Qlora: efficient finetuning of quantized l lms. CoRRabs/2305.14314. External Links: Link, 2305.14314 Cited by: SS1.
* A. Godbole and R. Jia (2023)Benchmarking long-tail generalization with likelihood splits. In Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pp. 933-953. External Links: Link, 2305.14314 Cited by: SS1.
* X. Ho, A. Duong Nguyen, S. Sugawara, and A. Aizawa (2020)Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, Barcelona, Spain, pp. 6609-6625. External Links: Link, 2004.1137 Cited by: SS1.
* G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave (2022)Unsupervised dense information retrieval with contrastive learning. Trans. Mach. Learn. Res.22. External Links: Link, 2202.10871 Cited by: SS1.

Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. Challenges and applications of large language models. _CoRR_, abs/2307.10169.
* Kandpal et al. (2023) Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large language models struggle to learn long-tail knowledge. In _International Conference on Machine Learning, ICML_, volume 202 of _Proceedings of Machine Learning Research_, pages 15696-15707. PMLR.
* Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020_, pages 6769-6781. Association for Computational Linguistics.
* Kasai et al. (2022) Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir R. Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. 2022. Realtime QA: what's the answer right now? _CoRR_, abs/2207.13332.
* Lewis et al. (2019) Patrick S. H. Lewis, Ludovic Denoyer, and Sebastian Riedel. 2019. Unsupervised question answering by cloze translation. In _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pages 4896-4910. Association for Computational Linguistics.
* Lewis et al. (2020) Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_.
* Lewis et al. (2021) Patrick S. H. Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Kuttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021. PAQ: 65 million probably-asked questions and what you can do with them. _Trans. Assoc. Comput. Linguistics_, 9:1098-1115.
* December 9, 2022_.
* Ma et al. (2023) Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023. Fine-tuning llama for multi-stage text retrieval. _CoRR_, abs/2310.08319.
* Mallen et al. (2023) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)._, pages 9802-9822. Association for Computational Linguistics.
* Min et al. (2023) Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wentau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2023. Nonparametric masked language modeling. In _Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023_, pages 2097-2118. Association for Computational Linguistics.
* Mosbach et al. (2023) Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. 2023. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. In _Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023_, pages 12284-12314. Association for Computational Linguistics.
* Naveed et al. (2023) Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Nick Barnes, and Ajmal Mian. 2023. A comprehensive overview of large language models. _CoRR_, abs/2307.06435.
* Ovadia et al. (2023) Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023. Fine-tuning or retrieval? comparing knowledge injection in llms. _CoRR_, abs/2312.05934.
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21:140:1-140:67.
* Robertson and Zaragoza (2009) Stephen E. Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. _Found. Trends Inf. Retr._, 3(4):333-389.
* Sciavolino et al. (2021) Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021. Simple entity-centric questions challenge dense retrievers. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021_, pages 6138-6148. Association for Computational Linguistics.
* Shuster et al. (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In _Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021_, pages 3784-3803. Association for Computational Linguistics.
* Shuster et al. (2021)* Soudani et al. (2023) Heydar Soudani, Evangelos Kanoulas, and Faegheh Hasibi. 2023. Data augmentation for conversational AI. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023_, pages 5220-5223. ACM.
* Sun et al. (2023) Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. 2023. Head-to-tail: How knowledgeable are large language models (llm)? A.K.A. will lms replace knowledge graphs? _CoRR_, abs/2308.10168.
* Thakur et al. (2021) Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual_.
* Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Saneviero, Alexander M. Rush, and Thomas Wolf. 2023. Zephyr: Direct distillation of LM alignment. _CoRR_, abs/2310.16944.
* Ushio et al. (2023) Asahi Ushio, Fernando Alva-Manchego, and Jose Camacho-Collados. 2023. An empirical comparison of LM-based question and answer generation methods. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 14262-14272, Toronto, Canada. Association for Computational Linguistics.
* Zaken et al. (2022) Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 1-9. Association for Computational Linguistics.

## Appendix A Data Preprocessing

Among the various datasets offering factual QA (Christmann et al., 2023; Sciavolino et al., 2021), we selected one of the most recent ones, named PopQA (Mallen et al., 2023). PopQA dataset comprises approximately 14,000 templated questions aimed at eliciting single-entity answers, based on 16 types of relationships extracted from Wikipedia. To assess the popularity of each entity, we followed the previous studies (Mallen et al., 2023; Sciavolino et al., 2021; Chen et al., 2021), using Wikipedia pageviews as a metric for popularity. We accumulated the pageviews for each entity from the start to the end of 2023.

Furthermore, we acquired the relevant Wikipedia page for each entity. Our corpus was then constructed by segmenting each Wikipedia page into a summary paragraph and subsequent additional paragraphs. For this purpose, we utilized the Wikipedia dump from March 2022, available in the HuggingFace dataset repository at [https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia).

## Appendix B QA Generation

### Prompt Method

For generating synthetic QAs based on a given context, we used the following prompt:

_You are a question-answer generator. Your goal is to generate question-answer pairs given the Context._

_Example output:_ {'question': ", 'answer': "}

_Context:_ SCONTEXT

_Step 1:_ Identify spans that are likely to be answers to questions, identify as many as possible.

_Step 2:_ For each identified span, generate a question.

_Step 3:_ Respond to the question in only a few tokens concisely.

_Step 4:_ Output in JSON format following [...]

_Ensure that you distinctly label and delineate Steps 1, 2, 3, and 4. Let's think step by step:_

Our test set comprises template-based QAs that feature single-entity answers. To generate QAs similar to those in the test set, we instructed the LLM to utilize a Chain of Thought (CoT) approach. However, we observed that not all the generated QAs adhered to our instructions. Below are some examples of QAs generated using this prompt-based approach:

_Context:_ John Mayne was a Scottish printer, journalist and poet born in Dumfries. In 1780, his poem The Siller Gun appeared in its original form in Ruddiman's Magazine, published by Walter Ruddiman in Edinburgh. It is a humorous work on an ancient custom in Dumfries of shooting for the Siller Gun.He also wrote a poem on Hallowe'en in 1780 which influenced Robert Burns's 1785 poem Halloween. Mayne also wrote a version of the ballad Helen of Kirkcnonnel. His verses were admired by Walter Scott.

_Synthetic QAs: question 1:_ "Who was John Mayne?" _answer 1:_ "John Mayne was a Scottish printer, journalist, and poet from Dumfries. (1780 poem: The Siller Gun)" _question 2:_ "What poem did John Mayne write in 1780 called Hallowe'en?", _answer 2:_ "Hallowe'en is a poem by John Mayne (1780)"

### E2E Method

We employed models trained for the Question Answer Generation (QAG) task, specifically using the T5-large model optimized for the End-to-End (E2E) approach, which was reported to deliver superior performance on the QA as the downstream task (Ushio et al., 2023). However, a notable drawback of the E2E method is its limited control over the quality and quantity of generated samples. Additionally, due to the T5 model's input token limitation, the input text must be restricted to 512 tokens. To ensure a fair comparison, we applied the same token limit to the inputs used with the prompt-based approach. Table 4 presents statistics for the QAs generated by both the prompt and E2E methods, using the same corpus.

## Appendix C Fine-Tuning Setup

We fine-tuned three versions of FlanT5 (small, base, and large) (Chung et al., 2022) model. Our fine-tuning experiments encompassed both full parameter tuning and PEFT. Our focus was particularly on the QLoRA method (Dettmers et al., 2023), which is widely recognized for its approach of keeping the pre-trained model parameters fixed while incorporating trainable rank decomposition matrices into each layer of the Transformer architecture. Despite experimenting with various numbers of training epochs, we observed that the results plateaued after a certain point. Table 5 included presents our finalized hyperparameters for fine-tuning.

## Appendix D Detailed Results

In this section, we provide additional results that corroborate our primary findings. Figure 4 illustrates the impact of fine-tuning versus non-fine-tuning, both with and without RAG, on the FlanT5-base and FlanT5-large models. It further demonstrates that fine-tuning enhances accuracy across both the least-popular and popular buckets. It should be noted that for the base model, the improvement in accuracy for the least-popular bucket is less significant compared to that of the small and large models.

Figure 5 displays the accuracy of the RAG approach before fine-tuning across various retrieval models. The Ideal retriever, which retrieves the summary section of the corresponding Wikipedia page, yields the highest accuracy.

Figure 6 compares the performance before and after fine-tuning, both with and without RAG, across all relationships. It also demonstrates that fine-tuning alone does not achieve the same level of accuracy as the RAG method for most of relationships.

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline \multirow{2}{*}{**Relationship**} & \multirow{2}{*}{**\#Ent.**} & \multicolumn{2}{c}{**Train-set**} & \multicolumn{2}{c}{**Dev-set**} \\ \cline{3-6}  & & E2E & Prompt & E2E & Prompt \\ \hline occupation & 532 & 16398 & 1872 & 1822 & 207 \\ place of birth & 584 & 19993 & 2043 & 2221 & 227 \\ genre & 1619 & 56101 & 5338 & 6233 & 593 \\ father & 570 & 37676 & 2092 & 4186 & 406 \\ country & 838 & 19362 & 2769 & 2151 & 307 \\ producer & 1520 & 41992 & 4235 & 4665 & 470 \\ director & 1999 & 27578 & 4644 & 3064 & 515 \\ capital of & 363 & 109141 & 3388 & 12126 & 376 \\ screenwriter & 1999 & 59680 & 6932 & 6631 & 770 \\ composer & 978 & 46574 & 4064 & 5174 & 451 \\ color & 34 & 4396 & 160 & 488 & 17 \\ religion & 338 & 18776 & 1449 & 2086 & 161 \\ sport & 547 & 14760 & 1710 & 1629 & 189 \\ author & 1514 & 46399 & 5319 & 5155 & 590 \\ mother & 187 & 7592 & 477 & 843 & 52 \\ capital & 645 & 28467 & 1322 & 3162 & 146 \\ \hline All & 14,267 & 491,525 & 38,114 & 56,803 & 5,949 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Statistics for generated QA pairs using two methods. The E2E approach, utilizing T5-large, generates over 12 times more QAs compared to the prompt method.

Figure 4: Correlation between the popularity of the subject entity in a question and the impact of RAG and FT on the performance of FlanT5-base and FlanT5-large in QA.

\begin{table}
\begin{tabular}{l r} \hline \hline
**Hyperparameter** & **Value** \\ \hline Epochs & 10 \\ Batch size & 128 \\ Learing rate & 2e-4 \\ PEFT, alpha & 16 \\ PEFT, rank & 32 \\ PEFT, dropout & 0.05 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameters for Fine-Tuning: Following a thorough hyperparameter search, we standardized the hyperparameters across all versions of the model.

Figure 5: The accuracy of the RAG approach before fine-tuning across all relationships in PopQA dataset.

Figure 6: The accuracy of the RAG approach before and after fine-tuning across all relationships in PopQA dataset.
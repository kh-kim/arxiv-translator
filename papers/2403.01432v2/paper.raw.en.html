<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge</title>
<!--Generated on Thu Mar  7 11:23:20 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.01432v2/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2403.01432v2">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2403.01432v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2403.01432v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S1" title="1 Introduction ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S2" title="2 Background ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S3" title="3 Evaluation Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Evaluation Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S3.SS1" title="3.1 RAG Approach ‣ 3 Evaluation Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>RAG Approach</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S3.SS2" title="3.2 Fine-Tuning Approach ‣ 3 Evaluation Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Fine-Tuning Approach</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S4" title="4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments and Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S5" title="5 Conclusion ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S6" title="6 Limitations ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A1" title="Appendix A Data Preprocessing ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Data Preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A2" title="Appendix B QA Generation ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>QA Generation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A2.SS1" title="B.1 Prompt Method ‣ Appendix B QA Generation ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Prompt Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A2.SS2" title="B.2 E2E Method ‣ Appendix B QA Generation ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>E2E Method</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A3" title="Appendix C Fine-Tuning Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Fine-Tuning Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A4" title="Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Detailed Results</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2403.01432v2 [cs.CL] 07 Mar 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Heydar Soudani 
<br class="ltx_break">Radboud University 
<br class="ltx_break">Nijmegen 
<br class="ltx_break">The Netherlands 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">heydar.soudani@ru.nl</span>
<br class="ltx_break">&amp;Evangelos Kanoulas 
<br class="ltx_break">University of Amsterdam 
<br class="ltx_break">Amsterdam 
<br class="ltx_break">The Netherlands 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">e.kanoulas@uva.nl</span>
<br class="ltx_break">&amp;Faegheh Hasibi 
<br class="ltx_break">Radboud University 
<br class="ltx_break">Nijmegen 
<br class="ltx_break">The Netherlands 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">faegheh.hasibi@ru.nl</span>
<br class="ltx_break">
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id4.id1">Large language models (LLMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications.
The two prominent approaches to enhance the performance of LLMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data.
This paper explores and evaluates the impact of RAG and FT on customizing LLMs in handling low-frequency entities on question answering task.
Our findings indicate that FT significantly boosts the performance across entities of varying popularity, especially in the most and least popular groups, while RAG surpasses other methods.
Additionally, the success of both RAG and FT approaches is amplified by advancements in retrieval and data augmentation techniques.
The code and data is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/informagi/RAGvsFT" title="">https://github.com/informagi/RAGvsFT</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break">
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1" style="width:433.6pt;"><span class="ltx_text" id="p2.1.1.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.1.1.1.1.1.1">Heydar Soudani</span></span></span>
<span class="ltx_tr" id="p2.1.1.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.2.2.1">Radboud University</span></span>
<span class="ltx_tr" id="p2.1.1.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.3.3.1">Nijmegen</span></span>
<span class="ltx_tr" id="p2.1.1.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.4.4.1">The Netherlands</span></span>
<span class="ltx_tr" id="p2.1.1.1.1.5.5">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.1.1.1.5.5.1.1">heydar.soudani@ru.nl</span></span></span>
</span>
</span></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_text" id="p2.1.1.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.1.1.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.1.1.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p2.1.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.2.1.1.1.1.1">Evangelos Kanoulas</span></span></span>
<span class="ltx_tr" id="p2.1.1.2.1.2.2">
<span class="ltx_td ltx_align_center" id="p2.1.1.2.1.2.2.1">University of Amsterdam</span></span>
<span class="ltx_tr" id="p2.1.1.2.1.3.3">
<span class="ltx_td ltx_align_center" id="p2.1.1.2.1.3.3.1">Amsterdam</span></span>
<span class="ltx_tr" id="p2.1.1.2.1.4.4">
<span class="ltx_td ltx_align_center" id="p2.1.1.2.1.4.4.1">The Netherlands</span></span>
<span class="ltx_tr" id="p2.1.1.2.1.5.5">
<span class="ltx_td ltx_align_center" id="p2.1.1.2.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.1.2.1.5.5.1.1">e.kanoulas@uva.nl</span></span></span>
</span>
</span></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_text" id="p2.1.1.3" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.1.1.3.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.1.1.3.1.1.1">
<span class="ltx_td ltx_align_center" id="p2.1.1.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.3.1.1.1.1.1">Faegheh Hasibi</span></span></span>
<span class="ltx_tr" id="p2.1.1.3.1.2.2">
<span class="ltx_td ltx_align_center" id="p2.1.1.3.1.2.2.1">Radboud University</span></span>
<span class="ltx_tr" id="p2.1.1.3.1.3.3">
<span class="ltx_td ltx_align_center" id="p2.1.1.3.1.3.3.1">Nijmegen</span></span>
<span class="ltx_tr" id="p2.1.1.3.1.4.4">
<span class="ltx_td ltx_align_center" id="p2.1.1.3.1.4.4.1">The Netherlands</span></span>
<span class="ltx_tr" id="p2.1.1.3.1.5.5">
<span class="ltx_td ltx_align_center" id="p2.1.1.3.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.1.3.1.5.5.1.1">faegheh.hasibi@ru.nl</span></span></span>
</span>
</span></span> </span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) exhibit outstanding capabilities in executing tasks that demand extensive memorization of factual data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib6" title="">2023</a>)</cite>. However, their memorization capabilities are constrained when dealing with less frequent entities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Mallen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib24" title="">2023</a>); Kandpal et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib16" title="">2023</a>); Sun et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib34" title="">2023</a>)</cite>, and even the largest models may encounter the well-known "hallucination" problem&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Shuster et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib32" title="">2021</a>)</cite> and temporal degradation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Kasai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib18" title="">2022</a>)</cite>.
Consequently, when LLMs are intended for deployment in less resourced domains, customization becomes imperative to ensure optimal performance. A common example is within the industrial setup, where chatbots or Question Answering (QA) systems need to accurately answer users’ questions about a proprietary knowledge graph or intra-company terminology with limited textual description.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Retrieval-Augmented Generation (RAG) and Fine-Tuning (FT) stand out as two prominent approaches for adapting LLMs to specific domains. RAG retrieves relevant information from a document corpus and enhances LLM’s response generation through the implementation of in-context learning (ICL). Conversely, the fine-tuning approach updates model weights to become adept at recalling specific information and enhance its memorization capabilities during inference.
In the context of less popular knowledge, where limited data is available, data augmentation methods are utilized to generate synthetic training data, serving as an initial step towards fine tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="236" id="S1.F1.g1" src="x1.png" width="390">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Correlation between subject entity popularity in a question and the effects of RAG and FT on FlanT5-small performance in open-domain question answering. FT markedly improves accuracy in the initial and final buckets relative to others (indicated by the pink line).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we aim to understand which approach and under what conditions is more effective for industry-specific models. Specifically, we seek to answer the following research questions:

<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="S1.p3.1.1">(RQ1)</span> What is the effectiveness of RAG and fine-tuning with synthetic data on QA for low-frequency factual knowledge?

<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="S1.p3.1.2">(RQ2)</span> Which parameters, including the quality of synthetic samples, the method to be fine-tuned, the model size, and the performance of retrieval models affect the downstream performance?
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To address our RQs, we performed a comprehensive comparison of RAG and fine tuning methods, with specific attention to less popular knowledge.
Our evaluation setup explores various factors, including model size, retrieval models, the quality of synthetic data generation, and the fine-tuning method (PEFT vs. full fine tuning).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our findings indicate that FT consistently enhances performance for entities, both popular and less popular, with the most substantial improvements observed in the most and least popular categories. Furthermore, RAG consistently outperforms fine tuning methods, particularly when combined with FT in smaller models, a benefit that diminishes in base models and is non-existent in larger models. Lastly, the effectiveness of both RAG and FT strategies increases with improvements in the performance of the retrieval and data augmentation models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Data Augmentation (DA).</span>
Data availability is crucial for fine-tuning in specialized domains.
DA addresses data scarcity problem by generating task- and domain-relevant samples from existing unlabeled texts.
A common DA approach for the QA task is generating question-answer pairs through a four-step <em class="ltx_emph ltx_font_italic" id="S2.p1.1.2">Pipeline</em>, consisting of: passage selection, answer extraction, question generation, and consistency filtering&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Alberti et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib1" title="">2019</a>; Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib19" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib21" title="">2021</a>; Ushio et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib37" title="">2023</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Ushio et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib37" title="">2023</a>)</cite> conducted an empirical study comparing three question answer generation approaches: Pipeline, Multitask, and End-to-End (E2E) and showed the E2E approach outperforms others in downstream tasks.
Recently, the utilization of LLMs to generate data is shown effective in information retrieval, QA, and dialogue creation tasks&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Soudani et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib33" title="">2023</a>; Askari et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib4" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Retrieval Augmented Generation.</span>
RAG enhances LLMs by integrating external knowledge sources with input queries, enriching the model with additional context for knowledge-intensive tasks&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib20" title="">2020</a>)</cite>. It utilizes an information retrieval system to find relevant documents and adds them to the input prompt to enhance response generation of an LLM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib2" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib3" title="">b</a>)</cite>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Less popular Knowledge.</span>
An entity’s popularity in an LLM is gauged by its frequency in the model’s pre-training data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Godbole and Jia, <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib12" title="">2023</a>; Min et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib25" title="">2023</a>)</cite>, often assessed through the entity’s occurrence in a large corpus&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kandpal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib16" title="">2023</a>)</cite>. Due to the practical challenges of direct counting, approximations like traffic metrics and content density are used&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib34" title="">2023</a>)</cite>. Wikipedia pageviews are among the most prevalent methods for measuring the popularity of entities&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mallen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib24" title="">2023</a>; Sciavolino et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib31" title="">2021</a>; Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib5" title="">2021</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Comparing FT vs. RAG</span>.
As interest grows in refining pre-trained language models for particular tasks, the comparison of FT and RAG strategies under equitable conditions is becoming increasingly important.
<cite class="ltx_cite ltx_citemacro_citet">Mosbach et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib26" title="">2023</a>)</cite> explored the effectiveness of few-shot FT versus In-context Learning for classification tasks in general domains. <cite class="ltx_cite ltx_citemacro_citet">de&nbsp;Luis&nbsp;Balaguer et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib10" title="">2024</a>)</cite> compared FT and RAG in answering long, agriculture, and geography-specific questions. <cite class="ltx_cite ltx_citemacro_citet">Ovadia et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib28" title="">2023</a>)</cite> assessed the performance on multiple-choice questions in specialized areas like Anatomy, Astronomy, College Biology, and Prehistory. In contrast to these studies, we directly address the integration of less popular factual knowledge into LLMs, comparing various retrieval, data augmentation, and fine tuning methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation Setup</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We evaluate LLMs on the closed-book QA task, focusing on the <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">PopQA</span> dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mallen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib24" title="">2023</a>)</cite> characterized by questions that covers a long-tail entity distribution.
It also includes unique Wikipedia titles, facilitating the calculation of pageviews and identification of relevant Wikipedia pages. We acquire the relevant Wikipedia page for each subject entity in the <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">PopQA</span> dataset and divide the entities into five buckets based on their popularity levels (Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S3.F2" title="Figure 2 ‣ 3 Evaluation Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">2</span></a>).
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="181" id="S3.F2.g1" src="x2.png" width="290">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Distribution of sample counts across buckets, defined by <math alttext="log_{10}(\text{pageviews})" class="ltx_Math" display="inline" id="S3.F2.4.m1.1"><semantics id="S3.F2.4.m1.1b"><mrow id="S3.F2.4.m1.1.2" xref="S3.F2.4.m1.1.2.cmml"><mi id="S3.F2.4.m1.1.2.2" xref="S3.F2.4.m1.1.2.2.cmml">l</mi><mo id="S3.F2.4.m1.1.2.1" xref="S3.F2.4.m1.1.2.1.cmml">⁢</mo><mi id="S3.F2.4.m1.1.2.3" xref="S3.F2.4.m1.1.2.3.cmml">o</mi><mo id="S3.F2.4.m1.1.2.1b" xref="S3.F2.4.m1.1.2.1.cmml">⁢</mo><msub id="S3.F2.4.m1.1.2.4" xref="S3.F2.4.m1.1.2.4.cmml"><mi id="S3.F2.4.m1.1.2.4.2" xref="S3.F2.4.m1.1.2.4.2.cmml">g</mi><mn id="S3.F2.4.m1.1.2.4.3" xref="S3.F2.4.m1.1.2.4.3.cmml">10</mn></msub><mo id="S3.F2.4.m1.1.2.1c" xref="S3.F2.4.m1.1.2.1.cmml">⁢</mo><mrow id="S3.F2.4.m1.1.2.5.2" xref="S3.F2.4.m1.1.1a.cmml"><mo id="S3.F2.4.m1.1.2.5.2.1" stretchy="false" xref="S3.F2.4.m1.1.1a.cmml">(</mo><mtext id="S3.F2.4.m1.1.1" xref="S3.F2.4.m1.1.1.cmml">pageviews</mtext><mo id="S3.F2.4.m1.1.2.5.2.2" stretchy="false" xref="S3.F2.4.m1.1.1a.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.4.m1.1c"><apply id="S3.F2.4.m1.1.2.cmml" xref="S3.F2.4.m1.1.2"><times id="S3.F2.4.m1.1.2.1.cmml" xref="S3.F2.4.m1.1.2.1"></times><ci id="S3.F2.4.m1.1.2.2.cmml" xref="S3.F2.4.m1.1.2.2">𝑙</ci><ci id="S3.F2.4.m1.1.2.3.cmml" xref="S3.F2.4.m1.1.2.3">𝑜</ci><apply id="S3.F2.4.m1.1.2.4.cmml" xref="S3.F2.4.m1.1.2.4"><csymbol cd="ambiguous" id="S3.F2.4.m1.1.2.4.1.cmml" xref="S3.F2.4.m1.1.2.4">subscript</csymbol><ci id="S3.F2.4.m1.1.2.4.2.cmml" xref="S3.F2.4.m1.1.2.4.2">𝑔</ci><cn id="S3.F2.4.m1.1.2.4.3.cmml" type="integer" xref="S3.F2.4.m1.1.2.4.3">10</cn></apply><ci id="S3.F2.4.m1.1.1a.cmml" xref="S3.F2.4.m1.1.2.5.2"><mtext id="S3.F2.4.m1.1.1.cmml" xref="S3.F2.4.m1.1.1">pageviews</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m1.1d">log_{10}(\text{pageviews})</annotation><annotation encoding="application/x-llamapun" id="S3.F2.4.m1.1e">italic_l italic_o italic_g start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT ( pageviews )</annotation></semantics></math>. The leftmost bin includes entities with fewer than <math alttext="10^{2}" class="ltx_Math" display="inline" id="S3.F2.5.m2.1"><semantics id="S3.F2.5.m2.1b"><msup id="S3.F2.5.m2.1.1" xref="S3.F2.5.m2.1.1.cmml"><mn id="S3.F2.5.m2.1.1.2" xref="S3.F2.5.m2.1.1.2.cmml">10</mn><mn id="S3.F2.5.m2.1.1.3" xref="S3.F2.5.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.F2.5.m2.1c"><apply id="S3.F2.5.m2.1.1.cmml" xref="S3.F2.5.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.5.m2.1.1.1.cmml" xref="S3.F2.5.m2.1.1">superscript</csymbol><cn id="S3.F2.5.m2.1.1.2.cmml" type="integer" xref="S3.F2.5.m2.1.1.2">10</cn><cn id="S3.F2.5.m2.1.1.3.cmml" type="integer" xref="S3.F2.5.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.m2.1d">10^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.5.m2.1e">10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> pageviews, while the rightmost bin encompasses entities with over <math alttext="10^{5}" class="ltx_Math" display="inline" id="S3.F2.6.m3.1"><semantics id="S3.F2.6.m3.1b"><msup id="S3.F2.6.m3.1.1" xref="S3.F2.6.m3.1.1.cmml"><mn id="S3.F2.6.m3.1.1.2" xref="S3.F2.6.m3.1.1.2.cmml">10</mn><mn id="S3.F2.6.m3.1.1.3" xref="S3.F2.6.m3.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="S3.F2.6.m3.1c"><apply id="S3.F2.6.m3.1.1.cmml" xref="S3.F2.6.m3.1.1"><csymbol cd="ambiguous" id="S3.F2.6.m3.1.1.1.cmml" xref="S3.F2.6.m3.1.1">superscript</csymbol><cn id="S3.F2.6.m3.1.1.2.cmml" type="integer" xref="S3.F2.6.m3.1.1.2">10</cn><cn id="S3.F2.6.m3.1.1.3.cmml" type="integer" xref="S3.F2.6.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.m3.1d">10^{5}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.6.m3.1e">10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT</annotation></semantics></math> pageviews.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To ensure a fair comparison between DA and RAG methods, we limit our focus to Wikipedia pages whose corresponding entities appear in the PopQA dataset. This setup also mirrors real-world industry practices, where entities and their corresponding textual descriptions are related to companies’ specific internal concepts.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>RAG Approach</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We utilize a variety of retrieval models to retrieve relevant passages for the RAG approach: BM25&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Robertson and Zaragoza, <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib30" title="">2009</a>)</cite>, Contriever&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Izacard et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib14" title="">2022</a>)</cite>, DPR&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib17" title="">2020</a>)</cite>, and a two-stages re-ranker that combines BM25 with DPR, all implemented according to the BEIR benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Thakur et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib35" title="">2021</a>)</cite>. Moreover, we utilize an ideal retrieval setup, where the summary paragraph of the Wikipedia page for the mentioned entity in the question is considered. We adopt this method because the true evidence for annotating passages is not available, yet the summary section assumably contains the answer to the question. The efficacy of this approach is evidenced in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A4.F5" title="Figure 5 ‣ Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">5</span></a> in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A4" title="Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">D</span></a>, which shows QA performance with the the ideal retriever achieves the highest results. </p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">After retrieving the most relevant passage for a question, we apply zero-shot prompting for generative prediction using a straightforward template:
<code class="ltx_verbatim ltx_font_typewriter" id="S3.SS1.p2.1.1">"Context: &lt;context&gt;. Based on the provided </code>
<br class="ltx_break"><code class="ltx_verbatim ltx_font_typewriter" id="S3.SS1.p2.1.2"> context, answer the question: &lt;question&gt;"</code>.
We experiment with three sizes of the FlanT5 model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chung et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib9" title="">2022</a>)</cite>—small, base, and large—to examine the impact of model scale on performance. Following <cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib24" title="">2023</a>)</cite>, we report on the Accuracy metric, where a prediction is considered accurate if it contains a substring that exactly matches any of the provided gold answers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Fine-Tuning Approach</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We generate training data for the fine tuning approach using two distinct data augmentation methods.
The first one is the End-to-End approach&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ushio et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib37" title="">2023</a>)</cite>, utilizing a model specifically trained for paragraph-level QA generation, using the T5 model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib29" title="">2020</a>)</cite>, referred to as <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">E2E</span> in our paper. Additionally, we explore the generation of synthetic training data by prompting an LLM, utilizing Zephyr&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Tunstall et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib36" title="">2023</a>)</cite> for QA generation. This method is referred to as the <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.2">Prompt</span> method in our discussion.
We generate QAs exclusively using the summary section of Wikipedia pages to ensures a fair comparison between FT and RAG with ideal retriever.
Further details on the QA generation process, including the prompt, input and output examples, and statistics of the generated QAs, are provided in the Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A2" title="Appendix B QA Generation ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">B</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">After generating QA pairs, we proceeded to fine-tune the FlanT5 models using two approaches: full parameter tuning (<span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Full</span>) and Parameter Efficient Fine-Tuning (<span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.2">PEFT</span>). Within the range of PEFT techniques&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zaken et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib38" title="">2022</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib22" title="">2022</a>; Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib23" title="">2023</a>)</cite>, we utilize QLoRA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dettmers et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib11" title="">2023</a>)</cite>, chosen for its broad acceptance in the field&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kaddour et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib15" title="">2023</a>; Naveed et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib27" title="">2023</a>)</cite> and efficient use of computational resources we had at our disposal. More information about FT approaches and hyper parameters are detailed in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A3" title="Appendix C Fine-Tuning Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">C</span></a>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments and Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Rec@1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">Rec@3</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">Rec@5</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">BM25</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">36.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">73.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">81.68</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1">Contriever</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">72.77</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">91.58</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.4">96.04</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.3.1">BM25+DPR</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.3.2.1">79.21</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.3">91.58</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.4">92.57</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T1.1.5.4.1">DPR</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.5.4.2">78.21</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.5.4.3.1">92.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.5.4.4.1">93.64</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> Performance comparison of retrieval models on our corpus, with DPR models outperforming the rest. The relevant passage for every question is assumed to be the first paragraph of the Wikipedia page.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Retrieval performance.</span>
Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S4.F3" title="Figure 3 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">3</span></a> compares the performance of retrieval models against ideal retriever (assumed to be the ground truth here) across popularity buckets. Since only the highest-ranked passage is included in the input prompt, our analysis is focused on Recall@1. The results indicate that retrieval effectiveness is higher in low popular entities compare to popular entities. This is probably due to limited occurrences of noisy passages for low popular entities. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S4.T1" title="Table 1 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">1</span></a> provides the overall retrieval scores for all methods.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">FT and RAG comparison.</span>
We assess the impact of RAG and FT in four distinct configurations:
(i) Neither FT nor RAG used <span class="ltx_text ltx_font_italic" id="S4.p2.1.2">(-FT-RAG)</span>,
(ii) RAG used without FT <span class="ltx_text ltx_font_italic" id="S4.p2.1.3">(-FT+RAG)</span>,
(iii) FT applied without RAG <span class="ltx_text ltx_font_italic" id="S4.p2.1.4">(+FT-RAG)</span>, and
(iv)) both FT and RAG employed <span class="ltx_text ltx_font_italic" id="S4.p2.1.5">(+FT+RAG)</span>.
The findings are detailed in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S4.T2" title="Table 2 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">2</span></a>. Generally, FT improves accuracy of the base model, but does not reach the effectiveness of RAG on the base model. The optimal performance is achieved by integrating both FT and RAG.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.1.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"></th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.1.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">-FT/</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.4.1">-FT/</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.1">+FT/</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.6.1">+FT/</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.2.2.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.1.1">FT</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.2.2.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.2.1">QA</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.2.2.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.3.1">-RAG</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.2.2.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.4.1">+RAG</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.2.2.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.5.1">-RAG</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.2.2.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.6.1">+RAG</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="6" id="S4.T2.1.3.3.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.3.3.1.1">FlanT5-small</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.4.4.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T2.1.4.4.1.1">PEFT</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.4.4.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4.3" rowspan="4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text" id="S4.T2.1.4.4.3.1">3.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4.4" rowspan="4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text" id="S4.T2.1.4.4.4.1">26.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">5.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.4.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">22.91</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.5.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">7.01</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.5.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">49.85</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.6.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T2.1.6.6.1.1">Full</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.6.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">6.35</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.6.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">10.21&nbsp;</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.7.7.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.7.7.2.1">8.52</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.7.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.7.7.3.1">49.88</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="6" id="S4.T2.1.8.8.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.8.8.1.1">FlanT5-base</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.9.9.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T2.1.9.9.1.1">PEFT</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.9.9.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.9.9.3" rowspan="4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text" id="S4.T2.1.9.9.3.1">6.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.9.9.4" rowspan="4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text" id="S4.T2.1.9.9.4.1">63.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.9.9.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">6.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.9.9.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">51.61</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.10.10.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.10.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">9.92</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.10.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.10.10.3.1">63.29</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.11.11.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T2.1.11.11.1.1">Full</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.11.11.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.11.11.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">8.63</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.11.11.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">24.17</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.12.12.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.12.12.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.12.12.2.1">11.41</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.12.12.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">60.26</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="6" id="S4.T2.1.13.13.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.13.13.1.1">FlanT5-large</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.14.14.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T2.1.14.14.1.1">PEFT</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.14.14.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.14.14.3" rowspan="4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text" id="S4.T2.1.14.14.3.1">8.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.14.14.4" rowspan="4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text" id="S4.T2.1.14.14.4.1">58.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.14.14.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">8.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.14.14.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">55.26</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.15.15.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.15.15.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">12.15</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.15.15.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.15.15.3.1">61.71</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.1.16.16.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T2.1.16.16.1.1">Full</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.16.16.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.16.16.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.16.16.3.1">16.23</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.16.16.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">13.31</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.1.17.17.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.17.17.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">13.91</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.17.17.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">58.60</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span> Accuracy of base and fine-tuned models, both with and without RAG. The RAG results presented are based on ideal retrieval.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="195" id="S4.F3.g1" src="x3.png" width="307">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Recall@1 for four retrieval models across different popularity levels. The results indicate that retrieval models perform more effectively with less popular knowledge compared to more popular ones.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates that fine-tuning enhances QA accuracy across all popularity levels for the FlanT5-small model, with the greatest improvements in the most and least popular buckets. While fine-tuning boosts performance for less popular categories in smaller models, this advantage decreases in base models and disappears in larger models; see Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A4.F4" title="Figure 4 ‣ Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">4</span></a> for more details. This is likely due to the improved memorization of larger models.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Effect of external parameters on RAG and FT.</span>
We delve into additional factors that influence model specialization in processing less popular knowledge. A key aspect under review is the effect of full tuning versus PEFT. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S4.T2" title="Table 2 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">2</span></a> shows PEFT leads to smaller gains in the <span class="ltx_text ltx_font_italic" id="S4.p4.1.2">+FT-RAG</span> compared to full FT, yet it significantly improves accuracy in the <span class="ltx_text ltx_font_italic" id="S4.p4.1.3">+FT+RAG</span> setup. This suggests that PEFT enables the LLM to maintain its reasoning abilities based on the provided prompts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Our investigation also covers two QA generation techniques. The E2E method generates over 12 times more QA than the prompting method (cf. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A2.T4" title="Table 4 ‣ B.2 E2E Method ‣ Appendix B QA Generation ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">4</span></a> in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A3" title="Appendix C Fine-Tuning Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">C</span></a>), while prompt-based method generates more quality QAs. The results in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S4.T2" title="Table 2 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">2</span></a> show that fine-tuned models trained on prompt-generated data outperform E2E-generated ones. This highlights the significance of synthetic data quality over quantity.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">The performance of the retrieval model is another important consideration. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#S4.T3" title="Table 3 ‣ 4 Experiments and Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">3</span></a> presents the QA system’s accuracy utilizing various retrieval strategies within the RAG framework. The findings demonstrate a direct correlation between the retrieval model performance and the overall QA accuracy, underscoring the retrieval model’s impact on the downstream task’s effectiveness.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.1.1.1.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">FT</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.1.1.1.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">QA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.1.1.1.3.1" style="width:6.8pt;height:26.2pt;vertical-align:-9.7pt;"><span class="ltx_transformed_inner" style="width:26.3pt;transform:translate(-9.71pt,0pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T3.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1.1.1">BM25</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.1.1.1.4.1" style="width:6.8pt;height:45.9pt;vertical-align:-19.5pt;"><span class="ltx_transformed_inner" style="width:45.9pt;transform:translate(-19.53pt,0pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T3.1.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.4.1.1.1">Contriever</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.1.1.1.5.1" style="width:7.7pt;height:55.8pt;vertical-align:-24.9pt;"><span class="ltx_transformed_inner" style="width:55.8pt;transform:translate(-24.08pt,1.25pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T3.1.1.1.5.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.5.1.1.1">BM25+DPR</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.1.1.1.6.1" style="width:6.8pt;height:21.8pt;vertical-align:-7.5pt;"><span class="ltx_transformed_inner" style="width:21.8pt;transform:translate(-7.49pt,0pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T3.1.1.1.6.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.6.1.1.1">DPR</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.7" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.1.1.1.7.1" style="width:6.9pt;height:21.4pt;vertical-align:-7.2pt;"><span class="ltx_transformed_inner" style="width:21.4pt;transform:translate(-7.22pt,0pt) rotate(-90deg) ;">
<p class="ltx_p" id="S4.T3.1.1.1.7.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.7.1.1.1">Ideal</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div></th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="2" id="S4.T3.1.2.2.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.2.2.1.1">FlanT5-small</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">9.76</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">17.22</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">20.80</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">21.41</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">26.13</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.3.1.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T3.1.3.1.1.1">PEFT</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.3.1.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">13.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">17.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">19.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">19.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.7" style="padding-top:1.25pt;padding-bottom:1.25pt;">22.91</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.4.2.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">20.82</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">34.87</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">40.45</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">41.25</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">49.85</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.5.3.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T3.1.5.3.1.1">Full</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.5.3.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">6.48</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">9.11</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">9.31</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">9.48</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.7" style="padding-top:1.25pt;padding-bottom:1.25pt;">10.21</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.6.4.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.2.1">20.95</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.3.1">35.09</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.4.1">40.52</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.5.1">41.53</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.4.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.6.4.6.1">49.88</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2" id="S4.T3.1.7.5.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.7.5.1.1">FlanT5-base</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.7.5.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">26.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.7.5.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">43.42</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.7.5.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">50.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.7.5.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">51.35</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.7.5.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">63.13</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.8.6.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T3.1.8.6.1.1">PEFT</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.8.6.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.8.6.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">22.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.8.6.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">35.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.8.6.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">42.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.8.6.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">42.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.8.6.7" style="padding-top:1.25pt;padding-bottom:1.25pt;">51.61&nbsp;</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.9.7.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.9.7.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.9.7.2.1">26.83</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.9.7.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.9.7.3.1">43.20</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.9.7.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.9.7.4.1">50.59</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.9.7.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.9.7.5.1">51.43</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.9.7.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.9.7.6.1">63.29</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.10.8.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T3.1.10.8.1.1">Full</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.10.8.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.10.8.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">11.78</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.10.8.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">18.53</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.10.8.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">20.59</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.10.8.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">21.15</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.10.8.7" style="padding-top:1.25pt;padding-bottom:1.25pt;">24.17</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.11.9.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.11.9.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">25.77</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.11.9.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">41.78</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.11.9.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">48.52</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.11.9.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">49.39</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.11.9.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">60.26</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2" id="S4.T3.1.12.10.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.12.10.1.1">FlanT5-large</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.12.10.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">26.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.12.10.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">40.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.12.10.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">46.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.12.10.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">47.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.12.10.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">58.12</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.13.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.13.11.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T3.1.13.11.1.1">PEFT</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.13.11.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.13.11.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">25.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.13.11.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">39.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.13.11.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">45.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.13.11.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">45.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.13.11.7" style="padding-top:1.25pt;padding-bottom:1.25pt;">55.26</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.14.12.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.14.12.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.14.12.2.1">28.60</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.14.12.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.14.12.3.1">43.45</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.14.12.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.14.12.4.1">50.15</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.14.12.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.14.12.5.1">50.91</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.14.12.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.14.12.6.1">61.71</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.15.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T3.1.15.13.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;">&nbsp;<span class="ltx_text" id="S4.T3.1.15.13.1.1">Full</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.1.15.13.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.15.13.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">7.67</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.15.13.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">12.29</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.15.13.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">12.01</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.15.13.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">12.44</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.15.13.7" style="padding-top:1.25pt;padding-bottom:1.25pt;">13.31</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.16.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T3.1.16.14.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.16.14.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">25.54</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.16.14.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">40.88</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.16.14.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">46.82</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.16.14.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">47.87</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.16.14.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">58.60</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span> Accuracy of RAG for base and fine-tuned LLMs using different retrieval models.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, we performed a comparative analysis to evaluate the effectiveness of RAG versus FT, with a focus on less popular knowledge. Our results reveal that FT leads to consistent performance improvements for all entities,
with the most notable gains seen in the most and least popular categories. We found that RAG stands out as a more effective strategy, especially when used in combination with fine-tuning. This advantage decreases however in larger models. Additionally, we observed that the success of both RAG and FT strategies improves with the enhancement of the retrieval and data augmentation models’ performance. Understanding the critical importance of synthetic data quality, future work will focus on developing an effective method for data creation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our study primarily addressed a template-based QA task, suggesting future research could tackle more complex QA challenges, such as multi-hop QA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ho et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib13" title="">2020</a>)</cite> or Conversational QA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Christmann et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib7" title="">2022</a>)</cite>.
Furthermore, we noted that our use of Zephyr for synthetic data generation has limitations in following Chain of Thought (CoT), highlighting the potential benefits of advanced data generation techniques to enhance data quality and reduce fine-tuning cost.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This publication is part of the project LESSEN with project number NWA.1389.20.183 of the research program NWA ORC 2020/21 which is (partly) financed by the Dutch Research Council (NWO).
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alberti et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/P19-1620" title="">Synthetic QA corpora generation with roundtrip consistency</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>, pages 6168–6173. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.ACL-TUTORIALS.6" title="">Retrieval-based language models and applications</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 41–46. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.11511" title="">Self-rag: Learning to retrieve, generate, and critique through self-reflection</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CoRR</em>, abs/2310.11511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Askari et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Arian Askari, Mohammad Aliannejadi, Chuan Meng, Evangelos Kanoulas, and Suzan Verberne. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.emnlp-main.623" title="">Expand, highlight, generate: Rl-driven document generation for passage reranking</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>, pages 10087–10099. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao Ling, and Sameer Singh. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.ACL-LONG.345" title="">Evaluating entity disambiguation and the role of popularity in retrieval-based NLP</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>, pages 4472–4485. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, and et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v24/22-1144.html" title="">Palm: Scaling language modeling with pathways</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">J. Mach. Learn. Res.</em>, 24:240:1–240:113.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christmann et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Philipp Christmann, Rishiraj&nbsp;Saha Roy, and Gerhard Weikum. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3477495.3531815" title="">Conversational question answering on heterogeneous sources</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">SIGIR ’22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022</em>, pages 144–154. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christmann et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Philipp Christmann, Rishiraj&nbsp;Saha Roy, and Gerhard Weikum. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2306.12235" title="">Compmix: A benchmark for heterogeneous question answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">CoRR</em>, abs/2306.12235.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Le&nbsp;Hou, Shayne Longpre, Barret Zoph, Yi&nbsp;Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang&nbsp;Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent&nbsp;Y. Zhao, Yanping Huang, Andrew&nbsp;M. Dai, Hongkun Yu, Slav Petrov, Ed&nbsp;H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc&nbsp;V. Le, and Jason Wei. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2210.11416" title="">Scaling instruction-finetuned language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">CoRR</em>, abs/2210.11416.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de&nbsp;Luis&nbsp;Balaguer et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Maria&nbsp;Angels de&nbsp;Luis&nbsp;Balaguer, Vinamra Benara, Renato&nbsp;Luiz de&nbsp;Freitas&nbsp;Cunha, Roberto de&nbsp;M.&nbsp;Estevão&nbsp;Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo&nbsp;O. Nunes, Rafael Padilha, Morris Sharp, Bruno Silva, Swati Sharma, Vijay Aski, and Ranveer Chandra. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.08406" title="">RAG vs fine-tuning: Pipelines, tradeoffs, and a case study on agriculture</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">CoRR</em>, abs/2401.08406.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.14314" title="">Qlora: Efficient finetuning of quantized llms</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">CoRR</em>, abs/2305.14314.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Godbole and Jia (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ameya Godbole and Robin Jia. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.FINDINGS-EACL.71" title="">Benchmarking long-tail generalization with likelihood splits</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023</em>, pages 933–953. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xanh Ho, Anh-Khoa Duong&nbsp;Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.coling-main.580" title="">Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>, pages 6609–6625, Barcelona, Spain (Online). International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=jKN1pXi7b0" title="">Unsupervised dense information retrieval with contrastive learning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Trans. Mach. Learn. Res.</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaddour et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.10169" title="">Challenges and applications of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">CoRR</em>, abs/2307.10169.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kandpal et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/kandpal23a.html" title="">Large language models struggle to learn long-tail knowledge</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">International Conference on Machine Learning, ICML</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2">Proceedings of Machine Learning Research</em>, pages 15696–15707. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S.&nbsp;H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2020.EMNLP-MAIN.550" title="">Dense passage retrieval for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, pages 6769–6781. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasai et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan&nbsp;Le Bras, Akari Asai, Xinyan Yu, Dragomir&nbsp;R. Radev, Noah&nbsp;A. Smith, Yejin Choi, and Kentaro Inui. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2207.13332" title="">Realtime QA: what’s the answer right now?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">CoRR</em>, abs/2207.13332.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Patrick S.&nbsp;H. Lewis, Ludovic Denoyer, and Sebastian Riedel. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/P19-1484" title="">Unsupervised question answering by cloze translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers</em>, pages 4896–4910. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Patrick S.&nbsp;H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="">Retrieval-augmented generation for knowledge-intensive NLP tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Patrick S.&nbsp;H. Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/TACL_A_00415" title="">PAQ: 65 million probably-asked questions and what you can do with them</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Trans. Assoc. Comput. Linguistics</em>, 9:1098–1115.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html" title="">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.08319" title="">Fine-tuning llama for multi-stage text retrieval</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">CoRR</em>, abs/2310.08319.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.ACL-LONG.546" title="">When not to trust language models: Investigating effectiveness of parametric and non-parametric memories</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</em>, pages 9802–9822. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.FINDINGS-ACL.132" title="">Nonparametric masked language modeling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 2097–2118. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mosbach et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.FINDINGS-ACL.779" title="">Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>, pages 12284–12314. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naveed et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Humza Naveed, Asad&nbsp;Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Nick Barnes, and Ajmal Mian. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.06435" title="">A comprehensive overview of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">CoRR</em>, abs/2307.06435.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ovadia et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2312.05934" title="">Fine-tuning or retrieval? comparing knowledge injection in llms</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">CoRR</em>, abs/2312.05934.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v21/20-074.html" title="">Exploring the limits of transfer learning with a unified text-to-text transformer</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">J. Mach. Learn. Res.</em>, 21:140:1–140:67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Zaragoza (2009)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephen&nbsp;E. Robertson and Hugo Zaragoza. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1561/1500000019" title="">The probabilistic relevance framework: BM25 and beyond</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Found. Trends Inf. Retr.</em>, 3(4):333–389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sciavolino et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.EMNLP-MAIN.496" title="">Simple entity-centric questions challenge dense retrievers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, pages 6138–6148. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.FINDINGS-EMNLP.320" title="">Retrieval augmentation reduces hallucination in conversation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021</em>, pages 3784–3803. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soudani et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Heydar Soudani, Evangelos Kanoulas, and Faegheh Hasibi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3583780.3615291" title="">Data augmentation for conversational AI</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023</em>, pages 5220–5223. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kai Sun, Yifan&nbsp;Ethan Xu, Hanwen Zha, Yue Liu, and Xin&nbsp;Luna Dong. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2308.10168" title="">Head-to-tail: How knowledgeable are large language models (llm)? A.K.A. will llms replace knowledge graphs?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">CoRR</em>, abs/2308.10168.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/65b9eea6e1cc6bb9f0cd2a47751a186f-Abstract-round2.html" title="">BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tunstall et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander&nbsp;M. Rush, and Thomas Wolf. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.16944" title="">Zephyr: Direct distillation of LM alignment</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">CoRR</em>, abs/2310.16944.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ushio et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Asahi Ushio, Fernando Alva-Manchego, and Jose Camacho-Collados. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.899" title="">An empirical comparison of LM-based question and answer generation methods</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 14262–14272, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaken et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elad&nbsp;Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2022.ACL-SHORT.1" title="">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>, pages 1–9. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_font_bold ltx_title_appendix">Appendix</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Data Preprocessing</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Among the various datasets offering factual QA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Christmann et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib8" title="">2023</a>; Sciavolino et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib31" title="">2021</a>)</cite>, we selected one of the most recent ones, named <span class="ltx_text ltx_font_smallcaps" id="A1.p1.1.1">PopQA</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mallen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib24" title="">2023</a>)</cite>. <span class="ltx_text ltx_font_smallcaps" id="A1.p1.1.2">PopQA</span> dataset comprises approximately 14,000 templated questions aimed at eliciting single-entity answers, based on 16 types of relationships extracted from Wikipedia.
To assess the popularity of each entity, we followed the previous studies&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mallen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib24" title="">2023</a>; Sciavolino et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib31" title="">2021</a>; Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib5" title="">2021</a>)</cite>, using Wikipedia pageviews as a metric for popularity. We accumulated the pageviews for each entity from the start to the end of 2023.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">Furthermore, we acquired the relevant Wikipedia page for each entity. Our corpus was then constructed by segmenting each Wikipedia page into a summary paragraph and subsequent additional paragraphs.
For this purpose, we utilized the Wikipedia dump from March 2022, available in the HuggingFace dataset repository at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/wikipedia" title="">https://huggingface.co/datasets/wikipedia</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>QA Generation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Prompt Method</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">For generating synthetic QAs based on a given context, we used the following prompt:

<span class="ltx_inline-block ltx_framed_rectangle" id="A2.SS1.p1.1.1" style="border-color: black;">
<span class="ltx_p" id="A2.SS1.p1.1.1.1"><span class="ltx_text ltx_font_italic" id="A2.SS1.p1.1.1.1.1">You are a question-answer generator. Your goal is to generate question-answer pairs given the Context.</span>
<br class="ltx_break"></span>
<span class="ltx_p" id="A2.SS1.p1.1.1.2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p1.1.1.2.1">Example output:</span> {’question’: ”, ’answer’: ”} 
<br class="ltx_break"></span>
<span class="ltx_p" id="A2.SS1.p1.1.1.3"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p1.1.1.3.1">Context:</span> $CONTEXT 
<br class="ltx_break"></span>
<span class="ltx_p" id="A2.SS1.p1.1.1.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p1.1.1.4.1">Step 1:<span class="ltx_text ltx_font_medium" id="A2.SS1.p1.1.1.4.1.1"> Identify spans that are likely to be answers to questions, identify as many as possible.</span></span></span>
<span class="ltx_p" id="A2.SS1.p1.1.1.5"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p1.1.1.5.1">Step 2:<span class="ltx_text ltx_font_medium" id="A2.SS1.p1.1.1.5.1.1"> For each identified span, generate a question.</span></span></span>
<span class="ltx_p" id="A2.SS1.p1.1.1.6"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p1.1.1.6.1">Step 3:<span class="ltx_text ltx_font_medium" id="A2.SS1.p1.1.1.6.1.1"> Respond to the question in only a few tokens concisely.</span></span></span>
<span class="ltx_p" id="A2.SS1.p1.1.1.7"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p1.1.1.7.1">Step 4:<span class="ltx_text ltx_font_medium" id="A2.SS1.p1.1.1.7.1.1"> Output in JSON format following […]</span></span>
<br class="ltx_break"></span>
<span class="ltx_p" id="A2.SS1.p1.1.1.8"><span class="ltx_text ltx_font_italic" id="A2.SS1.p1.1.1.8.1">Ensure that you distinctly label and delineate Steps 1, 2, 3, and 4. Let’s think step by step:</span>
</span>
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1">Our test set comprises template-based QAs that feature single-entity answers. To generate QAs similar to those in the test set, we instructed the LLM to utilize a Chain of Thought (CoT) approach. However, we observed that not all the generated QAs adhered to our instructions. Below are some examples of QAs generated using this prompt-based approach:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.SS1.p3">
<span class="ltx_inline-block ltx_framed_rectangle" id="A2.SS1.p3.1" style="border-color: black;">
<span class="ltx_p" id="A2.SS1.p3.1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p3.1.1.1">Entity:</span> John Mayne 
<br class="ltx_break"></span>
<span class="ltx_p" id="A2.SS1.p3.1.2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p3.1.2.1">Context:</span>
John Mayne was a Scottish printer, journalist and poet born in Dumfries. In 1780, his poem The Siller Gun appeared in its original form in Ruddiman’s Magazine, published by Walter Ruddiman in Edinburgh. It is a humorous work on an ancient custom in Dumfries of shooting for the S̈iller Gun.Ḧe also wrote a poem on Hallowe’en in 1780 which influenced Robert Burns’s 1785 poem Halloween. Mayne also wrote a version of the ballad Helen of Kirkconnel. His verses were admired by Walter Scott. 
<br class="ltx_break"></span>
<span class="ltx_p" id="A2.SS1.p3.1.3"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.SS1.p3.1.3.1">Synthetic QAs:</span></span>
<span class="ltx_p" id="A2.SS1.p3.1.4"><span class="ltx_text ltx_font_italic" id="A2.SS1.p3.1.4.1">question 1:</span> "Who was John Mayne?"</span>
<span class="ltx_p" id="A2.SS1.p3.1.5"><span class="ltx_text ltx_font_italic" id="A2.SS1.p3.1.5.1">answer 1:</span> "John Mayne was a Scottish printer, journalist, and poet from Dumfries. (1780 poem: The Siller Gun)" 
<br class="ltx_break"></span>
<span class="ltx_p" id="A2.SS1.p3.1.6"><span class="ltx_text ltx_font_italic" id="A2.SS1.p3.1.6.1">question 2:</span> "What poem did John Mayne write in 1780 called Hallowe’en?",</span>
<span class="ltx_p" id="A2.SS1.p3.1.7"><span class="ltx_text ltx_font_italic" id="A2.SS1.p3.1.7.1">answer 2:</span> "Hallowe’en is a poem by John Mayne (1780)"</span>
</span>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>E2E Method</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">We employed models trained for the Question Answer Generation (QAG) task, specifically using the T5-large model optimized for the End-to-End (E2E) approach, which was reported to deliver superior performance on the QA as the downstream task&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ushio et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib37" title="">2023</a>)</cite>. However, a notable drawback of the E2E method is its limited control over the quality and quantity of generated samples. Additionally, due to the T5 model’s input token limitation, the input text must be restricted to 512 tokens. To ensure a fair comparison, we applied the same token limit to the inputs used with the prompt-based approach. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A2.T4" title="Table 4 ‣ B.2 E2E Method ‣ Appendix B QA Generation ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">4</span></a> presents statistics for the QAs generated by both the prompt and E2E methods, using the same corpus.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A2.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T4.1.1.1.1" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.1.1">Relationship</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T4.1.1.1.2" rowspan="2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.2.1">#Ent.</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A2.T4.1.1.1.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.3.1">Train-set</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A2.T4.1.1.1.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">
<span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.4.1">Dev-set</span></td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.2.2.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.2.2.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.2.2.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">E2E</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.2.2.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">Prompt</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T4.1.3.3.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">occupation</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T4.1.3.3.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">532</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.3.3.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">16398</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.3.3.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">1872</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.3.3.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">1822</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.1.3.3.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">207</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.4.4.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">place of birth</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.4.4.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">584</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.4.4.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">19993</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.4.4.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">2043</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.4.4.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">2221</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.4.4.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">227</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.5.5.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">genre</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.5.5.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">1619</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.5.5.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">56101</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.5.5.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">5338</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.5.5.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">6233</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.5.5.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">593</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.6.6.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">father</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.6.6.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">570</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.6.6.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">37676</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.6.6.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">2092</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.6.6.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">4186</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.6.6.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">406</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.7.7.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">country</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.7.7.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">838</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.7.7.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">19362</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.7.7.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">2769</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.7.7.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">2151</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.7.7.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">307</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.8.8.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">producer</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.8.8.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">1520</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.8.8.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">41992</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.8.8.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">4235</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.8.8.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">4665</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.8.8.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">470</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.9.9.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">director</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.9.9.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">1999</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.9.9.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">27578</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.9.9.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">4644</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.9.9.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">3064</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.9.9.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">515</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.10.10.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">capital of</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.10.10.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">363</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.10.10.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">109141</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.10.10.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">3388</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.10.10.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">12126</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.10.10.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">376</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.11.11.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">screenwriter</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.11.11.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">1999</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.11.11.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">59680</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.11.11.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">6932</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.11.11.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">6631</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.11.11.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">770</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.12.12.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">composer</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.12.12.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">978</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.12.12.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">46574</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.12.12.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">4064</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.12.12.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">5174</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.12.12.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">451</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.13.13.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">color</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.13.13.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">34</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.13.13.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">4396</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.13.13.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">160</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.13.13.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">488</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.13.13.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">17</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.14.14.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">religion</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.14.14.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">338</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.14.14.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">18776</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.14.14.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">1449</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.14.14.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">2086</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.14.14.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">161</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.15.15.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">sport</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.15.15.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">547</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.15.15.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">14760</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.15.15.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">1710</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.15.15.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">1629</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.15.15.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">189</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.16.16.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">author</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.16.16.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">1514</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.16.16.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">46399</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.16.16.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">5319</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.16.16.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">5155</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.16.16.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">590</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.17.17.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">mother</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.17.17.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">187</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.17.17.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">7592</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.17.17.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">477</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.17.17.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">843</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.17.17.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">52</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.18.18.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">capital</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.18.18.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">645</th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.18.18.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">28467</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.18.18.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">1322</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.18.18.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">3162</td>
<td class="ltx_td ltx_align_center" id="A2.T4.1.18.18.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">146</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.19.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="A2.T4.1.19.19.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">All</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="A2.T4.1.19.19.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">14,267</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T4.1.19.19.3" style="padding-top:1.25pt;padding-bottom:1.25pt;">491,525</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T4.1.19.19.4" style="padding-top:1.25pt;padding-bottom:1.25pt;">38,114</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T4.1.19.19.5" style="padding-top:1.25pt;padding-bottom:1.25pt;">56,803</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A2.T4.1.19.19.6" style="padding-top:1.25pt;padding-bottom:1.25pt;">5,949</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span> Statistics for generated QA pairs using two methods. The E2E approach, utilizing T5-large, generates over 12 times more QAs compared to the prompt method.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Fine-Tuning Setup</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We fine-tuned three versions of FlanT5 (small, base, and large)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chung et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib9" title="">2022</a>)</cite> model. Our fine-tuning experiments encompassed both full parameter tuning and PEFT. Our focus was particularly on the QLoRA method&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dettmers et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#bib.bib11" title="">2023</a>)</cite>, which is widely recognized for its approach of keeping the pre-trained model parameters fixed while incorporating trainable rank decomposition matrices into each layer of the Transformer architecture. Despite experimenting with various numbers of training epochs, we observed that the results plateaued after a certain point. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A3.T5" title="Table 5 ‣ Appendix C Fine-Tuning Setup ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">5</span></a> included presents our finalized hyperparameters for fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A3.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T5.1.1.1.1" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_bold" id="A3.T5.1.1.1.1.1">Hyperparameter</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="A3.T5.1.1.1.2" style="padding-top:1.25pt;padding-bottom:1.25pt;"><span class="ltx_text ltx_font_bold" id="A3.T5.1.1.1.2.1">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T5.1.2.1.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Epochs</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A3.T5.1.2.1.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">10</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T5.1.3.2.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Batch size</th>
<td class="ltx_td ltx_align_right" id="A3.T5.1.3.2.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">128</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T5.1.4.3.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">Learing rate</th>
<td class="ltx_td ltx_align_right" id="A3.T5.1.4.3.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">2e-4</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T5.1.5.4.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">PEFT, alpha</th>
<td class="ltx_td ltx_align_right" id="A3.T5.1.5.4.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">16</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T5.1.6.5.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">PEFT, rank</th>
<td class="ltx_td ltx_align_right" id="A3.T5.1.6.5.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">32</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A3.T5.1.7.6.1" style="padding-top:1.25pt;padding-bottom:1.25pt;">PEFT, dropout</th>
<td class="ltx_td ltx_align_right ltx_border_b" id="A3.T5.1.7.6.2" style="padding-top:1.25pt;padding-bottom:1.25pt;">0.05</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span> Hyperparameters for Fine-Tuning: Following a thorough hyperparameter search, we standardized the hyperparameters across all versions of the model.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Detailed Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">In this section, we provide additional results that corroborate our primary findings. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A4.F4" title="Figure 4 ‣ Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the impact of fine-tuning versus non-fine-tuning, both with and without RAG, on the FlanT5-base and FlanT5-large models. It further demonstrates that fine-tuning enhances accuracy across both the least-popular and popular buckets.
It should be noted that for the base model, the improvement in accuracy for the least-popular bucket is less significant compared to that of the small and large models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="498" id="A4.F4.g1" src="x4.png" width="399">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Correlation between the popularity of the subject entity in a question and the impact of RAG and FT on the performance of FlanT5-base and FlanT5-large in QA.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A4.F5" title="Figure 5 ‣ Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">5</span></a> displays the accuracy of the RAG approach before fine-tuning across various retrieval models. The Ideal retriever, which retrieves the summary section of the corresponding Wikipedia page, yields the highest accuracy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="591" id="A4.F5.g1" src="x5.png" width="614">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span> The accuracy of the RAG approach before fine-tuning across all relationships in <span class="ltx_text ltx_font_smallcaps" id="A4.F5.2.1">PopQA</span> dataset.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.p3">
<p class="ltx_p" id="A4.p3.1">Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.01432v2#A4.F6" title="Figure 6 ‣ Appendix D Detailed Results ‣ Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge"><span class="ltx_text ltx_ref_tag">6</span></a> compares the performance before and after fine-tuning, both with and without RAG, across all relationships. It also demonstrates that fine-tuning alone does not achieve the same level of accuracy as the RAG method for most of relationships.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="591" id="A4.F6.g1" src="x6.png" width="615">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span> The accuracy of the RAG approach before and after fine-tuning across all relationships in <span class="ltx_text ltx_font_smallcaps" id="A4.F6.2.1">PopQA</span> dataset.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
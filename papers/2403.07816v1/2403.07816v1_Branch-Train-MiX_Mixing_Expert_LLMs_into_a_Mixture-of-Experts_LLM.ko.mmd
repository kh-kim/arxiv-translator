[MISSING_PAGE_FAIL:1]

동기화된 패션이며, 전체 대 전체 커뮤니케이션으로 인해 전문가 수와 함께 커뮤니케이션 비용이 증가합니다.

본 논문에서는 브랜치-트레인-머지(Branch-Train-Merge)와 믹서-오브-전문가(Mixture-of-Experts)의 장점을 결합하여 두 세계 모두의 장점을 지향하면서도 단점을 완화하고자 한다. 이를 위해 브랜치-트레인-머지 방법과 같이 여러 개의 전문가 LLM을 개별적으로 훈련시킨 후 MoE 아키텍처를 사용하여 해당 전문가를 단일 모델로 결합한다. 보다 구체적으로, 모든 전문가 LLM들로부터의 피드포워드 하위 계층들은 각각의 계층에서 단일 MoE 모듈로 통합되고, 라우터 네트워크는 모든 토큰에서 어떤 피드포워드 전문가를 사용할지를 선택한다. 우리는 단순히 가중치를 평균하여 자체 주의 계층을 포함한 전문가 LLM의 다른 모듈을 병합한다. 그런 다음, 라우터가 전문가 피드포워드(FF) 모듈을 혼합하는 것을 학습할 수 있도록 지속적인 훈련을 통해 결합된 모든 데이터에 대해 결과 모델을 MoE-파인튜닝한다. 그림 1은 _Branch-Train-MiX_ (BTX)라고 하는 이 방법의 개요를 보여 줍니다.

MoE에 비해 BTX의 주요 장점은 전문가 훈련이 당황스러울 정도로 병렬적이고 비동기적이어서 통신 비용을 줄이고 훈련 처리량을 증가시킨다는 것이다. Branch-Train-Merge와 비교하여 최종 BTX 모델은 다른 표준 LLM과 같이 미세 조정되거나 사용될 수 있는 통합된 신경망이다. 최종 BTX 모델은 훨씬 더 많은 수의 파라미터를 가짐에도 불구하고 희박하게 활성화되기 때문에 시드 모델에 비해 추론 FLOP를 크게 증가시키지 않을 것이다.

우리는 Llama-2 7B (Touvron et al., 2023)를 시드 모델로 사용하여 실험을 수행하고 수학, 코드 및 위키피디아의 도메인에 해당하는 데이터의 서로 다른 하위 집합에 대해 전문가 LLM을 훈련한다. 원래의 라마-2 7B 가중치를 네 번째 전문가로 추가하여 사전 훈련 과정에 비해 비교적 짧은 기간 동안 결합된 MoE 모델을 미세 조정한다. 결과 BTX 모델은 다양한 도메인에 걸쳐 태스크에 대한 시드 모델보다 상당한 개선을 가져오며, 특히 수학 및 코드 관련 태스크에 대한 특수 모델과의 격차를 해소하는 동시에 특수 모델이 치명적인 망각으로 고통받는 원래 기능에 대한 성능을 유지한다. BTX는 MoE finetuning을 통해 학습된 라우팅의 이점을 보여주는 모든 태스크에서 BTM보다 우수하다. 희소 업사이클링과 같은 순전히 MoE 트레이닝과 비교하여, BTX는 상이한 도메인들에서 태스크들에 걸쳐 더 높은 트레이닝 처리량 및 더 균형 잡힌 성능으로 더 컴퓨팅 효율적이다.

## 2 관련 작업

비동기 병렬 훈련 계산 효율성을 위해 훈련 작업자 간의 통신을 줄이는 것은 딥러닝 시스템을 훈련하기 위한 주요 연구 주제이다. Zhang 등(2015)은 서로 다른 작업자에 대한 모델 인스턴스가 서로 분기할 수 있도록 하여 동기화의 지속적인 필요성을 제거하는 방법을 도입하였다. 대신에, 작업자들은 때때로 탄성 평균화를 사용하여 마스터 가중치에 느슨하게 동기화된다. Douillard et al. (2023)의 보다 최근의 연구는 체중 변화를 평균화하고 Nesterov 모멘텀을 적용함으로써 분기된 근로자의 덜 빈번한 동기화가 실제로 잘 작동한다는 것을 보여주었다.

그림 1: BTQ(Branch-Train-Mix) 방법에는 세 가지 단계가 있습니다. 사전 훈련된 시드 LLM에서 여러 복사본을 만들어 **1) 분기; **2) 전문가의 LLM을 얻기 위해 다른 데이터 하위 집합에서 해당 복사본을 별도로 학습; **3) 전문가 LLM을 FF(혼합 전문가 피드포워드) 계층을 사용하여 단일 LLM으로 결합하고 전체 통합 모델을 미세 조정함으로써 해당 전문가 LLM을 혼합합니다.

LLM 트레이닝을 위해서. Branch-Train-Merge 방법(Li et al., 2022; Gururangan et al., 2023)은 다수의 훈련 과정을 완전히 독립적으로 실행함으로써 극한까지 병렬 훈련을 수행한다. 각각의 트레이닝 프로세스는 특정 도메인 데이터를 사용하고, 따라서 대응하는 모델은 그 도메인의 전문가가 된다. 마지막으로, 이러한 전문가 모델의 출력 분포를 평균하여 다음 토큰 예측을 수행한다. 어느 전문가가 평균을 낼지는 입력을 하나 이상의 영역으로 분류하여 결정한다. Wortsman et al.(2022)은 개별적으로 훈련된 모델의 매개변수를 단순히 평균화하는 것이 성능을 향상시키지만 모델은 하이퍼파라미터만 다르다.

Mixture-of-ExperstMoE는 간단한 Top-K 라우팅 방식을 사용하여 Shazeer 등(2017)에서 심층 네트워크를 확장하는 데 사용됩니다. 라우팅 결정들은 이산적이고 따라서 경사 하강에 의해 트레이닝될 수 없기 때문에, 트랜스포머 아키텍처에 대해 다양한 트레이닝 방법들이 탐색되었다(Fedus et al., 2022; Lewis et al., 2021). 놀랍게도 Roller et al.(2021)은 입력 토큰을 기반으로 랜덤 매핑을 통해 라우팅이 수행되면 학습 없이 고정된 라우팅 방식도 잘 작동한다는 것을 보여주었다. 최근 LLM을 사용한 대규모 실험에서, Jiang 등(2024)은 MoE 접근법이 훨씬 더 적은 수의 활성 파라미터를 사용하여 조밀한 LLM 대응물의 성능과 일치할 수 있음을 입증했다. Dai et al.(2024)의 연구는 더 세밀한 전문가의 장점은 물론 항상 활동적인 공동 전문가를 보유하고 있다는 것을 보여주었다. 우리의 연구와 더 유사하게, Gururangan et al. (2021)은 피드포워드 계층의 전문가를 도메인 조건 고정 라우팅을 사용하여 특정 도메인에 전문적으로 만들지만, 우리의 접근 방식의 비동기적 훈련이 부족하다.

지속적 학습(Continual Learning: Awasthi and Sarawagi, 2019)은 도메인 전문가가 시드 모델의 학습에 사용된 초기 데이터와 다른 분포를 가진 데이터 세트에 대해 훈련되기 때문에 분기 후 지속적인 훈련으로 구현된다. 특히, 우리의 접근법은 서로 다른 도메인에 대해 서로 다른 매개변수를 갖기 때문에 매개변수 분리 방법(Lange 등, 2019)과 관련이 있다. Aljundi 등(2016)은 또한 각 도메인에서 트레이닝할 모델의 새로운 사본을 생성한다. Rusu et al.(2016)은 새로운 도메인과 함께 새로운 모델을 추가하되, 이전 모델들과 연결하여 이전에 학습된 특징들을 사용할 수 있도록 한다. Roziere et al. (2023)은 코드의 특정 도메인에 대한 종자 LLM을 지속적으로 훈련하면 강력한 도메인 전문가 모델을 생성할 수 있으며 이는 처음부터 시작하는 것보다 훨씬 빠르게 수렴한다. 수학 전문가를 양성하기 위해서는 일반적인 LLM이 아닌 코드 전문가로부터 시작하는 것이 더 유익한 것으로 나타났다(Shao et al., 2024; Azerbayev et al., 2023).

## 3 Branch-Train-Mix

기존의 LLM \(\mathcal{M}\)은 다양한 주제를 다루는 대규모 코퍼라에 대해 사전 훈련되어 왔기 때문에, 우리는 \(N\) 전문 분야에 대한 성능을 향상시키는 것을 목표로 한다. 이것은 각각 수학, 코드 등과 같은 특정 지식 영역과 관련된 해당 학습 데이터 세트 \(\mathcal{D}\coloneqq\{D_{1},\dots,D_{N}\}\)로 계속된 사전 훈련에 의해 달성된다. 제안된 방법은 분기, 열차, MiX의 세 단계로 구성된다.

### 분기 & 열차: 당황스러울 정도로 병렬적인 전문가 훈련

시드 모델 \(\mathcal{M}\)로부터 초기화하기 위해, 우리는 \(N\) 전문가 LLMs \(\{\mathcal{M}_{1},\dots,\mathcal{M}_{N}\}\)을 훈련시키고, 각 모델 \(\mathcal{M}_{i}\)은 일반적인 언어 모델링 목적을 사용하여 사전 훈련 동안과 동일한 방식으로 해당 데이터세트 \(D_{i}\)에 대해 훈련된다. 각 전문가 모델 \(\mathcal{M}_{i}\)은 다른 전문가 모델들과 완전히 분리되어 훈련될 수 있기 때문에, 전체 훈련 과정은 \(N\)길이 당황스러울 정도로 평행해진다. 이 훈련 패러다임은 대규모 분산 훈련에서 몇 가지 이점이 있다. 이는 컴퓨팅의 크기를 스케일링할 때 전체 트레이닝 처리량의 선형 스케일링을 허용하는 반면, 조인트 트레이닝은 종종 배치 크기를 증가시킴으로써 불확실한 성능에 직면한다. 전체 통신 비용이 저렴합니다. 또한 단일 훈련 실패가 전체 훈련을 중단하는 대신 \(N \) 훈련 프로세스 중 하나에만 영향을 미치기 때문에 더 탄력적입니다.

모든 전문가 교육이 끝나면 우리는 각각 특정 배포를 전문으로 하는 \(N\)개의 다른 LLM으로 끝날 것이다. 이때 Branch-Train-Merge 방법(Li et al., 2022; Gururangan et al., 2023)은 이러한 도메인 전문가들을 그대로 사용하여, 추론 시점에 입력이 어느 도메인에 속하는지를 판단하여 어떤 전문가를 사용할지를 선택한다. 일반적으로 여러 전문가를 선택하고 최종 출력 분포를 단순히 평균화하여 다음 토큰을 생성합니다. 대조적으로, 우리의 BTX 접근법은 다음 섹션에서 설명할 것처럼 이러한 도메인 전문가를 다시 미세 조정되는 단일 LLM으로 병합한다.

### MiX: 별도의 전문가를 결합하여 전문가 혼합

도메인 전문가 모델 \(\mathcal{M}_{i}\)을 결합하기 위해 전문가 혼합 접근법을 사용한다. 그러나 \(\mathcal{M}_{i}\)의 최종 출력을 혼합하는 고전적인 절차를 사용하는 대신 트랜스포머의 각 층 내에서 MoE를 수행하여 보다 세밀한 혼합을 수행한다. 특히, 도메인 전문가의 다른 피드포워드 하위 계층을 단일 MoE 하위 계층으로 결합한다. \(\mathsf{FF}^{l}_{i}(x)\)가 \(i\)번째 도메인 전문가 \(\mathcal{M}_{i}\)의 \(l\)번째 층에서 피드포워드 서브층인 경우, 층 \(l\)에서 입력 표현을 위한 결합된 MoE 층 \(x\)은 계산될 것이다:

\[\mathsf{FF}^{l}_{\text{MoE}}(x)=\sum_{i=1}^{N}g_{i}(W_{l}x)\mathsf{FF}^{l}_{i} (x).\]

여기서 \(W_{l}\)는 선형 변환이고 \(g\)는 라우팅 함수이며 일반적으로 출력이 희박하여 일부 전문가만 스위칭한다. Router 출력이 0인 경우 \(\mathsf{FF}^{l}_{i}(x)\) 계산을 생략할 수 있기 때문에, \(\mathsf{FF}^{l}_{\text{MoE}}(x)\)의 실제 계산은 모든 도메인 전문가를 계산하는 것보다 훨씬 효율적일 것이다. 그러나, 라우팅 결정은 토큰에서 토큰으로 변경될 수 있으므로, 하나의 입력 시퀀스는 임의의 주어진 토큰에서 단지 소수만이 액세스되는 경우에도 필요한 경우 모든 도메인 전문가 FF 계층을 채용할 수 있다. 실험에서는 달리 명시되지 않는 한, \(g(W_{l}x)=\text{SoftMax}(\text{TopK}(W_{l}x))\인 Top-k (k=2) 라우팅을 사용한다.

자기 주의 하위 계층의 경우 가중치를 평균하여 서로 다른 도메인 전문가를 결합한다. 이 배경의 동기는 자기 주의 계층이 피드포워드 계층보다 덜 전문화된 영역이라는 가정이다. 나머지 파라미터(임베딩 등)에 대해서도 동일한 평균화를 수행한다.

우리가 소개하는 유일한 새로운 파라미터는 라우터의 변환 파라미터 \(W_{l}\)이며, 이는 네트워크의 나머지 부분에 비해 무시할 수 있는 크기이다. 그럼에도 불구하고, 이러한 새로운 파라미터들은 세밀하게 조정되어야 하므로, 라우터는 어떤 도메인 \(\mathsf{FF}_{i}\)을 사용할지를 선택할 때 최적의 결정을 내릴 수 있다. 또한, 미세 조정은 자기 주의 가중치가 평균에 의해 구성되고, 최적이 아닐 가능성이 높기 때문에 도움이 된다. 전반적으로 전체 시스템은 당황스러울 정도로 병렬적인 훈련 프레임워크에서 함께 작업하는 데 전혀 최적화되지 않았지만, 우리의 가설은 소량의 결합된 미세 조정도 큰 개선을 가져올 수 있다는 것이다.

### Variations

또한 본 논문에서 제안한 방법을 여러 가지 방법으로 실험하였다.

MoE의 공통적인 문제는 라우터에 의해 전혀 활성화되지 않는 죽은 전문가의 출현이다. Top-k와 같은 일반적인 라우팅 방법은 죽은 전문가가 Top-k 선택에 있지 않으므로 훈련 신호를 받지 않기 때문에 그러한 상황에서 벗어날 가능성이 낮다. 부하 분산은 전문가가 동등하게 활용되도록 권장하는 추가 손실 항을 추가하여 간단한 솔루션을 제공합니다. 우리는 (Fedus et al., 2022)와 유사한 손실 항을 사용한다:

\[\mathcal{L}_{\text{LB}}=\alpha N\sum_{i=1}^{N}u_{i}p_{i}\quad\text{ 여기서 }u_{i}=\frac{1}{|\mathcal{B}|}\sum_{x\in\mathcal{B}}g_{i}(W_{l}x)\text{ 및 }p_{i}=\frac{1}{|\mathcal{B}|}\sum_{x\in\mathcal{B}}\text{SoftMax}_{i}(W_{l}x)\]\]

여기서, \(\mathcal{B}\)는 현재 데이터 배치이고, \(\alpha\)는 하이퍼파라미터이다. 이 손실은 각 층에서 계산되고 NLL 손실에 추가된다.

Top-k 라우팅 외에 다른 라우팅 방법도 실험하였다.

* Switch: Fedus et al.(2022)에서 제안한 Top-1 라우팅 방식이다.
* 소프트 라우팅: 라우팅 함수 \(g\)로 소프트max를 사용하므로 모든 전문가가 훈련 및 추론 중에 활성화됩니다. 최상의 성능을 제공할 수 있지만 컴퓨팅 증가를 희생해야 합니다.
* 샘플 Top-1: \(g\)에 gumbel softmax (Jang et al., 2016)를 사용합니다. 훈련 시간에는 검벨 소프트맥스로부터 소프트 샘플을 생성하지만 가장 큰 값을 제외한 모든 값을 0으로 제거한다. 그런 다음 다른 전문가 계산을 생략하고 이 가장 큰 값에 해당하는 하나의 전문가만 계산합니다.

추론할 때, 우리는 단순히 하드 샘플링을 한다. 우리는 훈련과 추론 사이의 불일치를 점진적으로 줄이기 위해 훈련의 끝에서 온도를 날카로운 분포로 가열냉각한다.

전문가 분리 MoE 계층의 모듈 수는 각 모듈이 하나의 도메인에 해당하기 때문에 우리가 훈련하는 도메인 수와 일치한다. 그러나, 각 도메인 FF 하위 계층을 여러 개의 청크로 분할함으로써 간단한 방법으로 모듈의 수를 증가시킬 수 있다. 주어진 \(N\) 도메인과 \(d_{\text{FF}}\)의 FF 활성화 크기를 고려하여 각 FF 레이어를 \(d_{\text{FF}}/C\ 차원을 갖는 \(C\) 청크로 분할한다. 결과적으로, 최종 MoE 층은 \(MC\) 모듈을 가질 것이다.

또한 도메인 전문가의 MoE 전문가를 일대일 방식으로 직접 초기화하는 대신 각 MoE 전문가에 모든 도메인을 포함하려고 시도한다. 이에 대한 동기는 표준적인 방식으로 훈련된 MoE 전문가들이 도메인 전문화를 보여주지 않고, 오히려 상이한 도메인들에 걸쳐 균일하게 활성화된다는 관찰이다(Jiang et al., 2024). 대조적으로, 우리의 도메인 전문가들은 그들의 훈련 데이터를 통해 특정 도메인에 특화되어 있다. 이 도메인 전문화를 깨기 위해 각 도메인 전문가의 FF 계층을 \(N\) 청크로 분할한 다음 모든 도메인에서 \(n\) 번째 청크를 병합하여 \(n\) 번째 MoE 전문가를 구축한다. 이러한 방식으로, 각각의 MoE 전문가는 모든 도메인으로부터 동일한 양의 파라미터를 포함한다.

## 4 Experiments

### Experimental Setup

우리는 Llama-2 사전 훈련에 사용된 설정을 기반으로 실험을 한다(Touvron et al., 2023). 특히, 우리는 Llama-2 7B 모델을 종자 모델로 사용합니다.

#### 4.1.1 BTX Training

우리는 시드 모델로 7B 매개변수와 함께 사전 훈련된 Llama-2(Touvron et al., 2023)를 사용한다. 시드 모델 Llama-2 7B의 3개의 사본을 만든 후, 우리는 3개의 도메인 전문가를 도출하기 위해 다음 도메인 데이터 세트에 대해 이들을 계속 훈련시킨다:

* **수학:** Llemma(Azerbayev et al., 2023) 모델 훈련에 사용된 동일한 데이터 원본 및 혼합물입니다. 릴레마와 견줄 수 있도록 동일한 양의 데이터, 즉 총 201B 토큰이 있는 48k 단계를 교육한다.
* **코드:** CodeLlama 사전 훈련에서 사용된 동일한 데이터 원본 및 코드 데이터의 혼합입니다 (Roziere et al., 2023). 코드 전문가 LLM은 수학 전문가와 비교할 수 있도록 총 210B 토큰으로 50k 단계에 대해 훈련된다.
* **위키피디아:** 2022년 6월부터 8월 사이에 추출된 위키피디아 문서. 데이터는 하이퍼링크, 주석 및 기타 형식 상용구를 제거하기 위해 전처리되었습니다. 이것은 더 작은 데이터 세트이기 때문에 우리는 총 42B 토큰을 훈련한다.

이 세 가지 도메인 전문가만 진행할 수 있지만 "일반주의자" 전문가로 원래 종자 LLM도 포함하여 일반 지식이 최종 모델로 전달되도록 한다. 따라서 섹션 3.2에서 설명한 대로 이 4개의 전문가 모델을 단일 MoE 모델로 혼합한다. 그런 다음 이 MoE 모델을 4명의 전문가(일반 전문가에 대한 원래 Llama-2 7B 사전 훈련 데이터 포함)를 훈련하고 다른 80B 토큰에 대해 훈련하는 데 사용되는 모든 데이터 소스에서 미세 조정한다. 각 도메인뿐만 아니라 도메인 전체의 데이터 세트에 대한 자세한 샘플링 비율은 부록A에 설명되어 있다. 기본 Top-2 라우팅이 있는 BTX의 경우 달리 명시되지 않는 한 \(\alpha=0.01\)로 부하 균형을 사용합니다. 샘플 Top-1 라우팅의 경우, Jang et al.(2016)의 온도 어닐링 스케줄 \(\tau\)=max\((0.5,-rt)\)과 \(r=1e-4\)를 사용하며, 여기서 \(t\)는 훈련 단계 수이다. 첫 번째 레이어에 대해서만 소프트 라우팅을 대신 사용했습니다. 샘플 Top-1 훈련은 Top-2보다 효율적이기 때문에 동일한 컴퓨팅 예산으로 160B 토큰을 훈련할 수 있습니다.

#### 4.1.2 Baselines

다음 기준선과 비교 합니다.* **Llama-2:** 시드 모델로 사용 하는 원래 Llama-2 7B 및 Llama-2 13B와 비교 합니다.
* **밀도:** 다른 도메인 데이터 세트에서 별도의 LLM을 학습 하는 대신 조밀한 기준선은 모든 데이터를 사용 하 여 시드 LLM을 계속 학습 합니다. 우리는 BTX와 정확히 동일한 훈련 데이터를 사용하며, 전문가 훈련 단계에서 사용된 새로운 도메인 특정 데이터에 대한 첫 번째 훈련 후 MoE 미세 조정 단계에서 Llama-2 사전 훈련 데이터를 포함하는 동일한 데이터 혼합을 사용한다. 이 비교를 _데이터 매칭_ (DM)이라고 합니다.
* **희소 업사이클링:** 이 기준선(Komatsuzaki 등, 2022)은 전문가로 피드포워드 모듈의 4개의 동일한 사본을 만들어 시드 모델에서 MoE 모델을 초기화합니다. 우리는 랜덤하게 초기화된 \(W_{i}\) 파라미터를 갖는 Top-2 라우터를 사용한다. BTX 및 조밀 베이스라인에서 사용되는 것과 동일한 데이터로 데이터 매칭 베이스라인을 트레이닝하는 것 외에도, 트레이닝 전체에 걸쳐 MoE 미세튜닝 데이터 혼합물을 사용하여 동일한 양의 GPU-일, 즉 컴퓨팅-매칭(CM)으로 희소 업사이클링 베이스라인을 트레이닝한다. 이는 당혹스러울 정도로 병렬적인 전문가 훈련을 포함하지 않는 BTX의 특수한 경우에 해당한다.
* **Branch-Train-Merge(BTM):** 이 기준선(Li 등, 2022)은 BTX와 동일한 전문가 LLM(원래 시드 모델 포함)을 사용하지만 MoE 모델을 빌드하지 않고 직접 사용합니다. 주어진 컨텍스트(입력)에 대해, 컨텍스트와 전문가의 훈련 데이터 사이의 유사성을 기반으로 Top-k 전문가 LLMs을 선택한다. Gururangan et al.(2023)에서 사용된 효율적인 추론 방법에 따라, 컨텍스트와 전문가의 훈련 데이터는 모두 tf-idf를 통해 임베딩된다. 각 전문가의 평균 tf-idf 임베딩에 대한 코사인 유사도를 기반으로 Top-k 전문가를 선정한다.
* **CodeLlama 7B:** 코드 데이터에 대해 동일한 시드 모델 Llama-2 7B를 계속 학습하여 코드를 전문으로 하는 언어 모델(Roziere 등, 2023)입니다. 또한 긴 컨텍스트 및 충전과 같은 다른 기능도 있습니다.
* **Llema 7B:** 수학 데이터에 대한 CodeLlama 7B의 지속적인 훈련에 의한 수학을 전문으로 하는 언어 모델(Azerbayev et al., 2023).

기준선, 전문가 모델 및 MoE 모델의 훈련에 동일한 최적화 하이퍼파라미터를 사용한다. 무게감쇠가 0.1인 AdamW 최적화기를 사용하여 100단계의 웜업으로 학습률을 \(1e-4\)의 피크까지 어닐링하고 코사인 스케줄로 피크의 10%까지 감쇠시킨다. 우리는 시퀀스 길이가 4096인 4M 토큰의 배치 크기를 사용한다.

#### 4.1.3 Evaluation

평가를 위해 서로 다른 기술을 테스트하는 여러 벤치마크에서 제로 샷 및 소수 샷 성능을 사용합니다.

* 수학: 수학 추론을 위해 GSM8K(8 shot)(Cobbe et al., 2021) 및 MATH(4 shot)(Hendrycks et al., 2021)에 대한 평균 성능을 보고한다.
* 코드: 코드 생성에 대한 HumanEval(0 shot)(Chen et al., 2021) 및 MBPP(3 shot)(Austin et al., 2021)의 평균 성능을 보고합니다.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{2}{c}{**Math**} & \multicolumn{2}{c}{**Code**} & \multicolumn{2}{c}{**General knowledge**} \\ \cline{2-9}  & **GSM8K** & **MATH** & **Human** & **MBPP** & **Natural** & **Trivia** & **MMLU** \\  & & & **Eval** & & **Questions** & **QA** & \\ \hline Llama-2 7B & 14.7 & 2.5 & 12.8 & 20.8 & 16.4 & **58.5** & 46.1 \\ Math expert & **39.5** & **18.8** & 25.0 & 33.6 & 14.4 & 37.1 & **52.0** \\ Code expert & 12.0 & 4.0 & **31.7** & **40.2** & 11.5 & 29.9 & 39.6 \\ Wikipedia expert & 11.7 & 3.1 & 11.0 & 15.2 & **21.8** & 57.2 & 43.1 \\ \hline \hline \end{tabular}
\end{table}
표 1: 시드 모델 Llama-2 7B와 비교하여 대표 업무에 대한 개별 도메인 전문가 LLM 성과. 예상대로 코드와 수학 전문가는 해당 도메인 과제에 탁월합니다. 위키피디아 전문가는 자연 질문에서 더 나은 성능을 보이지만 수학 전문가는 MMLU에서 가장 좋은 점수를 받습니다. 이는 MMLU가 많은 수학 과목을 포함하고 있고 수학 훈련이 이 과제에 도움이 되는 것으로 나타났기 때문일 수 있다(Shao et al., 2024).

* World knowledge: 우리는 Natural Questions (5 shot)(Kwiatkowski et al., 2019) 및 TriviaQA (5 shot)(Joshi et al., 2017)의 평균 성능을 보고합니다.
* 추론: ARC-Easy and ARC-Challenge (Clark et al., 2018), SIQA (Sap et al., 2019), PIQA (Bisk et al., 2020) and WinoGrande (Sakaguchi et al., 2021)의 평균 0-shot 성능을 보고한다.
* 일반: 여러 도메인을 포괄 하는 MMLU (5 shot) (Hendrycks et al., 2021)에 대 한 성능을 보고 합니다.

### Main Results

#### 4.2.1 Overall Performance

도메인 전문가는 각각의 작업에서 탁월합니다. 먼저 전문가 LLM이 특정 도메인에 전문화하는 방법을 분석합니다. 결과는 표 1에 요약되어 있다. 예상대로 개별 전문가 LLM은 수학 및 코드 도메인이 특히 큰 개선을 보이는 각 도메인에서 최상의 성능을 달성한다. 게다가, 몇 가지 흥미로운 관찰이 있다. 수학 전문가 훈련이 코드 성능도 향상되어 이러한 영역의 밀접한 관계가 있음을 알 수 있다. 그러나 이러한 단일 도메인 연속 훈련은 다른 도메인의 일부 작업에서 상당한 성능 저하와 함께 치명적인 망각으로 어려움을 겪는다. 예를 들어, 수학 및 코드 전문가는 시드 모델보다 트리비아QA에서 훨씬 더 나쁘다.

BTX는 전문가가 전문화하는 모든 작업을 개선합니다.표 2와 그림 2(오른쪽)는 여러 도메인에 걸쳐 집계된 성능을 보여줍니다. 더 자세한 작업별 결과는 부록의 표 8에 보고되어 있다. 시드 모델 Llama-2 7B와 비교하여, BTX 모델(서로 다른 수의 활성 파라미터에 대응하는 샘플 Top-1 및 Top-2 모두)은 상식 추론과 같은 다른 작업에서 회귀하지 않고 수학, 코딩 및 세계 지식과 같은 모든 전문가 도메인에서 개선된다. 상위 2명의 전문가(우리의 기본값)를 가진 BTX는 또한 수학 및 코딩 도메인에서 전문화된 모델 Llama 7B 및 CodeLama 7B의 최상의 성능에 접근하면서 세계 지식 및 상식 추론과 같은 전문적이지 않은 도메인에서 이러한 모델보다 크게 개선한다. 조밀하고 희박한 업사이클링과 같은 지속적인 사전 훈련을 위한 대체 데이터 매칭(DM) 방법에 비해 BTX는 수학 및 코딩 도메인에서 작은 갭으로 평균적으로 더 나은 성능을 달성한다. BTX는 평균적으로 BTM보다 큰 마진만큼 우수한 성능을 보이며, 이는 토큰 레벨 라우팅을 학습하기 위한 MoE 미세 조정이 유익하다는 것을 나타낸다. 전반적으로, 결과는 BTX가 다중 태스크 학습으로부터 태스크 간섭에 강건한 연속 프리트레이닝을 위한 보다 계산 효율적인 방법임을 입증한다. BTX는 또한 Llama-2 13B가 훨씬 더 많은 트레이닝 컴퓨트를 사용하고 약간 더 많은 활성 파라미터를 가지고 있음에도 불구하고 추론을 제외한 모든 태스크에서 Llama-2 13B를 능가한다.

우리는 컴퓨트 매칭(compute-matching, CM) 시나리오에서 BTX를 희소 업사이클링 베이스라인과 추가로 비교한다. 둘 다

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & **Math** & **Code** & **Knowledge** & **Reasoning** & **MMLU** & **Average** \\ \hline \hline _Specialized LLMs_ & & & & & & \\ CodeLlama 7B & 8.1 & 36.3 & 22.2 & 56.6 & 38.6 & 37.9 \\ Llemma 7B & 28.0 & 33.5 & 17.2 & 38.8 & 33.5 & 32.1 \\ \hline _Generalist LLMs_ & & & & & & \\ Llama-2 7B & 8.6 & 16.8 & 37.4 & 63.3 & 46.1 & 40.7 \\ Llama-2 13B & 16.3 & 24.5 & 40.0 & **66.1** & 52.8 & 45.4 \\ Dense (DM) & 18.3 & 25.8 & 39.6 & 63.3 & 49.8 & 44.5 \\ Sparse upcycling (DM), Top-2 & **28.1** & 34.7 & 34.0 & 62.3 & 51.1 & 46.3 \\ BTM, Top-1 & 21.3 & 36.4 & 26.5 & 61.0 & 44.3 & 43.1 \\ BTM, Top-2 & 21.5 & **36.6** & 26.9 & 61.2 & 44.3 & 43.4 \\ BTX, Sample Top-1 & 26.4 & 31.5 & 40.1 & 63.7 & **53.2** & 47.3 \\ BTX, Top-2 & 27.4 & 34.0 & **41.0** & 63.5 & 52.5 & **47.9** \\ \hline \hline \end{tabular}
\end{table}
표 2: 대중적인 벤치마크에 걸쳐 집계된 다양한 능력에 대해 테스트된 일반 및 전문 사전 훈련 모델을 모두 포함하는 다양한 기준선에 비해 BTX의 집계된 성능. 치밀하고 희박한 업사이클링, BTM 및 BTX는 BTM이 미세 조정 단계를 갖지 않는 것을 제외하고는 정확히 동일한 양과 데이터의 혼합물에 대해 훈련된다.

MoE 단계 동안 동일한 데이터 혼합물에서 훈련하지만 MoE 훈련에 사용된 컴퓨팅의 백분율 측면에서 다르다. 희소 사이클링은 BTX에 근접하여 수행되지만, 전문가의 병렬 훈련은 표 3과 같이 BTX의 훈련 처리량을 증가시킨다. 그 결과, BTX는 동일한 훈련 계산 예산을 고려할 때 순수한 MoE보다 \(2\times\) 이상의 데이터로 훈련할 수 있고, 모든 도메인에서 약간 더 높은 평균 성능을 달성한다.

#### 4.2.2 더 나은 컴퓨팅 성능 트레이드오프

우리는 그림 2 (왼쪽)의 계산 효율 측면에서 BTX를 기준선과 비교한다. X-축은 GPU 일 단위로 측정된 시드 모델에서 시작하여 MoE 모델의 도메인 전문가 훈련 및 미세 조정을 포함하는 총 훈련 계산을 보여준다. Y축은 표 2에 보고된 전체 성능을 측정한다.

MoE 훈련 단계가 사전 훈련에서 총 훈련 예산의 일부를 사용함에도 불구하고(예를 들어, Llama-2 사전 훈련은 2T 토큰을 사용함), BTX는 밀집 모델의 다중 작업 학습 및 Branch-Train-Merge와 같은 대안적인 연속 사전 훈련 접근법에 비해 일반적인 능력에 가파른 개선을 가져온다.

\begin{table}
\begin{tabular}{l c c c c|c c c c c c} \hline \hline  & MoE & Training & Total compute & \#tokens & Math & Code & Knowledge & Reasoning & MMLU & Average \\  & compute & time (days) & (GPU-days) & (B) & & & & & & \\ \hline BTX & 23\% & 7.8 & 926.1 & 533 & 27.4 & 34.0 & 41.0 & 63.5 & 52.5 & 47.9 \\ Sparse upcycling (CM) & 100\% & 7.9 & 1007.1 & 252 & 28.2 & 30.7 & 41.3 & 62.9 & 52.1 & 47.3 \\ \hline \hline \end{tabular}
\end{table}
표 3: 컴퓨팅의 100%가 MoE 트레이닝에 소비된다는 첫 번째 열에서 보여지는 바와 같이 전문가 트레이닝 스테이지가 없는 BTX의 특수한 경우인 컴퓨팅-매칭(compute-matching; CM)을 갖는 BTX와 Sparse 업사이클링의 비교. 또한 총 훈련 시간, 계산 및 훈련 토큰 수를 보고합니다. 평균뿐만 아니라 개별 도메인에 대한 두 성능을 비교하면 BTX가 더 높은 처리량 외에도 더 균형 잡힌 성능을 가짐을 알 수 있다.

그림 2: **왼쪽:** 원 크기로 표시된 추론 시간에 다른 활성 매개 변수를 사용하여 다양한 기준선과 비교한 BTX의 평균 성능 대 훈련 예산입니다. Llama-2 13B를 제외한 모든 모델은 섹션 4.1.1에 설명된 데이터 세트를 사용하여 Llama-2 7B에서 시작하여 훈련된다. X축은 GPU days1에서 측정된 시드 모델에서 시작하는 총 훈련 계산을 보여주고, Y축은 모든 태스크에 대한 평균 점수이다(표 2에서 계산된 바와 같이). BTX 모델은 라마-2 13B뿐만 아니라 동일한 시드 모델에서 시작된 기준선을 능가한다. **오른쪽:** 점수가 가장 높은 도메인으로 나누어지는 다른 도메인에 대한 정규화된 성능입니다. 우리는 시드 모델 Llama-2 7B와 비교하여 코드(특화된 모델과 일치함) 및 수학 과제에서 BTX에 대한 큰 개선을 보고 있으며, 심지어 Llama-2 13B 모델을 능가한다.

희소 업사이클링보다 더 효율적이다. BTX의 특별한 경우, 전문가 훈련이 없는 희소 업사이클링은 동일하거나 더 큰 컴퓨팅 예산을 고려할 때 조밀하고 BTM보다 우수하지만 BTX는 그렇지 않다. BTX의 계산 효율 향상은 MoE 미세 조정 전에 전문가의 당황스러울 정도로 병렬 훈련에서 비롯된다.

매개 변수의 활성 수(2(왼쪽)에서 원 크기로 표시됨) 측면에서 MoE 모델은 Llama-2 13B 모델과 유사하다. BTX는 Llama-2 13B에 비해 추가 트레이닝 컴퓨트의 절반 미만을 사용하지만, 전문가 도메인들(수학, 코드, 및 지식)에 대해 개선된 성능을 입증하고 더 나은 전체 성능을 달성한다. 이는 BTX의 훈련이 사전 훈련 전체에 걸쳐 동일한 훈련 프로토콜을 사용하는 것보다 사전 훈련의 후기 단계에 더 효과적임을 나타낸다.

### Ablations & Analysis

#### 4.3.1 BTX 훈련의 요약

먼저, 서로 다른 양의 핀튜닝에 대해 서로 다른 양의 활성 파라미터와 서로 다른 라우팅 방법을 비교한다. 공정한 비교를 위해 부하 분산은 그 중 어느 것에도 사용되지 않습니다. 결과는 표 4에 나와 있습니다. 스위치 라우팅의 경우 용량 계수를 1.5로 설정합니다(라우팅된 토큰이 삭제되는 하드 제한). 스위치 라우터가 평균 성능에서 저수준임을 발견했습니다. 소프트 라우팅은 가장 좋은 성능을 보이지만 희소성이 부족하고 활성 파라미터의 수가 가장 많기 때문에 예상된다. 전반적으로 Top-2 라우팅은 성능과 효율성 사이에서 좋은 균형을 제공합니다.

또한 표 5에 요약된 결과를 통해 BTX의 추가 설계 선택을 제거했다. 로드 밸런싱이 없는 MoE 트레이닝은 코딩 태스크(HumanEval)에서 더 나쁜 성능을 수행하지만 수학(GSM8k) 정확도가 더 높다는 것을 발견했다. 다음 섹션의 라우팅 분석은 이러한 트레이드오프에 대한 더 많은 통찰력을 제공할 것이다. 다음으로 각 전문가로부터 초기화된 피드포워드 모듈을 동결하고 나머지 MoE 모델만을 훈련시키는 것은 모든 작업에서 성능에 거의 영향을 미치지 않는다. 이는 개별 전문가가 분기-트레인 단계에서 이미 충분한 도메인 지식을 얻었고 믹스(MoE finetuning) 단계는 주로 학습한다는 것을 시사한다.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{**Routing method**} & \multicolumn{2}{c}{**Active parameters (B)**} & \multicolumn{1}{c}{**MoE Finetune**} & \multicolumn{1}{c}{**Average**} \\ \cline{2-5}  & **Training** & **Inference** & **tokens (B)** & **score** \\ \hline \hline Switch Top-1 & 6.7 & 6.7 & 10 & 24.7 \\ Sample Top-1 & 6.7 & 6.7 & 10 & 33.0 \\ Top-2 & 11.1 & 11.1 & 10 & 34.6 \\ Soft routing & 19.7 & 19.7 & 10 & 35.8 \\ \hline Sample Top-1 & 6.7 & 6.7 & 40 & 35.3 \\ Top-2 & 11.1 & 11.1 & 40 & 35.9 \\ Soft routing & 19.7 & 19.7 & 40 & 37.3 \\ \hline Sample Top-1 & 6.7 & 6.7 & 160 & 36.9 \\ Top-2 & 11.1 & 11.1 & 80 & 37.3 \\ \hline \hline \end{tabular}
\end{table}
표 4: BTX 트레이닝 동안 상이한 라우팅 방법들에 대한 블레이션들. 평균 점수는 GSM8K, HumanEval, Natural Questions, ARC Challenge 및 MMLU를 포함한 대표적인 작업에 대한 성능을 기반으로 한다.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & **GSM8K** & **Human** & **Natural** & **ARC** & **MMLU** & **Average** \\  & & **Eval** & **Questions** & **Challenge** & & **Score** \\ \hline \hline BTX & 29.8 & 27.4 & 23.0 & 43.4 & 50.0 & 34.7 \\ \hline no load-balancing (LB) & 34.6 & 19.5 & 23.2 & 44.4 & 51.6 & 34.6 \\ no LB \& freeze experts & 34.8 & 18.3 & 24.1 & 44.9 & 51.4 & 34.7 \\ blending experts & 13.9 & 17.1 & 9.9 & 34.1 & 36.2 & 22.2 \\ split experts, top-2 of 8 & 22.0 & 20.1 & 16.8 & 39.1 & 41.8 & 28.0 \\ split experts, top-4 of 8 & 29.6 & 26.8 & 22.9 & 44.0 & 49.4 & 34.5 \\ \hline \hline \end{tabular}
\end{table}
표 5: 상이한 BTX 훈련 전략에 대한 표. 모든 변형은 동일한 전문가로부터 초기화되고 MoE 미세조정 동안 총 10B 토큰에 대해 훈련된다.

다른 파라미터들, 즉 자기주목 및 라우터 변환에서 평균 가중치는 \(W_{i}\)이다.

또한 섹션 3.3에 설명된 혼합 및 분할 기술을 테스트한다. 전문가가 혼합될 때 모든 작업에 대한 성능이 떨어졌으며, 이는 도메인 FF 계층이 이러한 방식으로 혼합될 수 없음을 시사한다. MoE 계층에서 8개의 모듈을 얻기 위해 각 도메인 FF를 \(C=2\) 청크로 분할해도 Top-4 라우팅이 활성 파라미터 수와 일치하더라도 성능이 향상되지 않는다.

#### 4.3.2 라우팅 분석

BTX의 성능을 심층적으로 이해하기 위해 다운스트림 태스크에 대한 모델 평가를 실행하고 전문가 간의 라우팅 결정을 검토한다. 결과는 그림 3에 요약되어 있으며, 또한 부록C에서 다양한 BTX 설정에 대한 자세한 절제 결과를 보고한다. 로드 밸런싱을 적용한 Top-2 라우팅은 다른 라우팅 방법에 비해 전문가 간 로드의 균일한 분산을 보장한다. 토큰 확률 분포를 분석하면 부하 분산이 있는 모든 전문가에서 낮은 확률 점수로의 이동이 관찰되며, 특히 모델의 최종 계층에 더 가깝고 이는 공정한 라우팅에 기여한다. 흥미롭게도 부하 균형이 없는 모든 모델은 다른 전문가, 특히 코드 전문가의 전체 기여도가 낮은 수학 전문가에 크게 의존한다. 데드 코드 전문가는 훈련에 도입된 부하 분산과 함께 "다시 살아나기"를 한다. 실제로 가시화될 뿐만 아니라 수학과 코드 영역의 지배적인 전문가가 된다.

부하 분산이 있는 Top-2에 대 한 라우팅 결정의 예는 표 6에서 찾을 수 있습니다. 수학 도메인 작업 전반에 걸쳐 토큰은 종종 Code 및 Llama-2 7B 전문가로 라우팅됩니다. 보다 상세한 토큰 분포(부록C, 그림 6)를 살펴보면 GSM8K 태스크는 코드 및 라마-2 전문가를 선호하는 반면 MATH 태스크는 도메인 내 수학 전문가에 더 의존한다는 것을 알 수 있다. 우리는 GSM8K 데이터 세트가 상식 지식과 기본적인 산술 연산이 필요한 초등학교 수학 문제로 구성되어 있기 때문에 이러한 일이 발생한다고 가정한다. 코드 및 세계 지식 작업 모두 대부분 도메인 내 코드 및 위키피디아 전문가로 이동한다. 섹션4.3.1에서 앞서 관찰한 바와 같이 부하 분산을 도입하면 코딩 과제는 개선되지만 수학 과제는 저하되며, 이는 도메인 전문가 라우팅의 이러한 변화로 설명될 수 있다. 대조적으로 추론 과제는 유사한 행동을 나타내며 동등하게 의존한다.

도 3: BTX는 다양한 계층에서의 토큰들의 결정들을 상이한 다운스트림 태스크들에 대한 상이한 전문가들(위키, 수학, 코드, LLMa-2 7B)로 라우팅한다. 작업은 코드(인간 평가, MBPP), 수학(GSM8K, MATH), 세계 지식(자연 질문, 트리비아QA) 및 추론(ARC-Easy, ARC-Challenge, SIQA, PIQA 및 WinoGrande) 도메인별로 집계됩니다. 부하 균형(상단)을 갖는 Top-2 라우팅은 부하 균형(하단)을 갖지 않는 Top-2에 비해 전문가들 사이의 부하를 보다 균일하게 분배하는 것을 관찰한다.

수학 및 일반론자 LLM에 대한 전문 지식.

## 5 Conclusion

LLM의 능력을 향상시키기 위해 간단한 연속 사전 훈련 방법인 브랜치-트레인-MiX(BTX)를 도입했다. 종자 LLM의 여러 사본을 비동기 및 병렬 방식으로 여러 도메인에 전문화하도록 훈련하고 나중에 미세 조정을 통해 단일 혼합 전문가(MoE) 모델로 다시 병합한다. 초기 병렬 훈련 단계가 더 높은 훈련 처리량과 확장성을 가져오는 반면, 두 번째 MoE 미세 조정 단계는 최종 LLM을 더 성능 있게 만든다. 실험 결과, 일반인 LLM은 전문 지식과 기술을 가진 데이터 세트에 대한 지속적인 훈련을 통해 성능을 향상시킬 수 있음을 보여준다. 우리는 BTX 접근법이 더 큰 일반 LLM 또는 여러 별도로 특수화된 LLM을 훈련하는 것보다 계산 효율이 더 높다는 것을 발견했다. 이러한 통찰력은 강력한 일반화 모델을 달성하기 위해 늦은 사전 훈련에서 컴퓨팅을 할당하는 방법을 알려줄 수 있다.

## 6 제한 사항 및 미래 작업

BTX에 대한 우리의 실험 결과는 유망하지만 이 논문에서 그 잠재력을 완전히 탐구하지는 못했다. 계산 한계로 인해 본 논문에서는 3개의 도메인과 4명의 전문가만을 대상으로 실험하였다. 비지도 도메인 발견을 사용하는 것과 같은 더 많은 도메인에 대한 훈련(Gururangan et al., 2023)은 전문가 훈련의 병렬화의 이점을 증폭시켜야 한다. 더 많은 전문가를 두는 것도 최종 MoE 모델을 더 효율적으로 만들 것이다. 왜냐하면 활동적인 전문가의 수는 그대로 유지될 수 있지만 전체 용량은 증가하기 때문이다. 실험에서는 MoE의 간단한 구현을 사용했으며 다른 전문가를 다른 GPU에 배치하여 병렬로 실행하는 것과 같은 더 복잡한 기술을 사용하여 MoE를 최적화하지 않았다. 이러한 효율적인 MoE 구현은 BTX의 훈련 시간을 단축시킬 수 있으며, 드문드문 업사이클링 베이스라인도 단축시킬 수 있다.

\begin{table}
\begin{tabular}{l l} \hline Task & Question and generation \\ \hline \multirow{4}{*}{GSM8K} & Q: Janet’s ducks lay 16 eggs per day. She cuts three for breakfast every morning and lakes muffins for her friends every day with four. She sells the remainder at the farmers’ market daily for \$2 per fresh \\ \cline{2-3}  & duck egg. How much in dollars does she make every day at the farmers’ market? \\ \cline{2-3}  & A: Janet’s ducks lay 16 eggs per day. She cuts three for breakfast every morning. So she has 16 - 3 = 13 eggs left. She sells the remainder at the farmers’ market daily for \$2 per fresh duck egg. So she has 9 \$2 = 18 dollars. \\ \hline \multirow{4}{*}{GSM8K} & Q: from typing import List \\  & def has close elements(numbers: List[float], threshold: float)-> bool: \\  & *** Check if in given list of numbers. are any two numbers closer to each other than \\  & given threshold. \\  & \(\geq\) : has close elements([1.0, 2.0, 3.0], 0.5) \\ \cline{2-3}  & False \\ \cline{2-3}  & \(>\) : has close elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) \\ \cline{2-3}  & True \\ \cline{2-3}  & *** \\ \cline{2-3}  & A: for i in range(len(numbers) - 1): \\ \cline{2-3}  & for j in range(i + 1, len(numbers)): \\ \cline{2-3}  & if abs(numbers)[i] - numbers[j] ) \textless threshold: \\ \cline{2-3}  & return True \\ \cline{2-3}  & return False \\ \hline Natural & Q: who got the first nobel prize in physics \\ \cline{2-3}  & Questions & A: Max Planck \\ \hline \end{tabular}
\end{table}
표 6: 수학(GSM8K), 코드(Human Eval), 및 지식(Natural Questions) 도메인에서 로드 밸런싱을 갖는 Top-2 라우팅을 위한 토큰 라우팅 결정들의 예시들. 강조된 토큰은 위키피디아와 LLaMA-2 TB, 수학과 LLaMA-2 TB, 코드와 LLaMA-2 TB, 수학과 코드, 위키피디아와 수학, 위키피디아와 코드와 같은 전문가에게 전달된다. 도메인 내 전문가에게 전달된 토큰에 밑줄이 그어져 있습니다.

BTX는 BTM과 비교하여 결합된 전문가를 미세 조정하는 접근법을 제공하며, 이는 명령어 미세 조정 또는 RLHF 절차에서 직접 적용될 수 있다. 그러나, 우리는 이 논문에서 사전 훈련 단계에 초점을 맞추었기 때문에 그것을 향후 작업으로 남겨둔다.

MoE의 전문가들이 특정 영역을 전문화하는 것이 더 나은지 여부에 대한 질문은 추가 조사가 필요한 흥미로운 질문이다. 우리의 접근법은 전문가를 특정 도메인에 명시적으로 묶었지만, 그러한 전문화는 MoE 훈련 동안 자연스럽게 나타나지 않는 것 같다(Jiang et al., 2024). 일부 전문가가 해당 도메인 작업에 더 많이 사용되는 것을 관찰하여 MoE 미세 조정 후에도 도메인 전문화가 부분적으로 남아 있음을 보여준다.

BTX를 전문가 훈련에 할당된 100% 컴퓨팅과 MoE 미세 조정에 할당된 0% 컴퓨팅을 사용하는 BTM과 전문가 훈련에 할당된 0% 컴퓨팅과 MoE 미세 조정에 할당된 100% 컴퓨팅을 사용하는 희소 업사이클링이라는 두 가지 특수 변형과 비교했다. 향후 작업은 전문가 훈련과 MoE 훈련 사이의 컴퓨팅 할당 비율을 철저히 스윕할 수 있다. 또한 균일한 샘플링 이외의 MoE 미세 조정을 위해 다른 데이터 혼합물로 실험을 수행하지 않았다.

## 7 Acknowledgements

우리는 마가렛 리, 쿠샬 티루말라, 루크 제틀모예르, 아르티도로 파그노니, 수친 구루랑간, 마이크 루이스와 에밀리 디난, 훈련 시행에 도움을 준 앤드류 코헨과 아룬 바부에게 감사드린다.

## References

* Achiam 등(2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _ arXiv preprint arXiv:2303.08774_, 2023.
* Aljundi et al. (2016) Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. 전문가 게이트: 전문가 네트워크를 통한 평생 학습입니다. _ 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 7120-7129, 2016. [https://api.semanticscholar.org/CorpusID:914027](https://api.semanticscholar.org/CorpusID:914027).
* Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. 르, 찰스 서튼 대용량 언어 모델을 사용한 프로그램 합성 _ ArXiv_, abs/2108.07732, 2021. [https://api.semanticscholar.org/CorpusID:237142385](https://api.semanticscholar.org/CorpusID:237142385).
* 아와스티 및 사라와기(2019) 아비지트 아와스티 및 수니타 사라와기. 신경망을 이용한 지속적인 학습: 리뷰. "프로시빙스 오브 ACM 인도 데이터 과학 및 데이터 관리에 관한 국제 회의"에서 2019년 362-365페이지를 참조하십시오.
* Azerbayev et al. (2023) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. 장, 지아 덩, 스텔라 바이더만, 숀 웰렉 렘마: 수학을 위한 개방형 언어 모델입니다. _ ArXiv_, abs/2310.10631, 2023. [https://api.semanticscholar.org/CorpusID:264172303](https://api.semanticscholar.org/CorpusID:264172303).
* Bisk et al.(2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. <인공지능에 관한 AAAI 회의 회보>에서 34권, 2020년 7432-7439쪽입니다.
* Brown et al.(2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Illya Sutskever, Dario Amodei. 언어 모델은 거의 볼 수 없는 학습자입니다. _ ArXiv_, abs/2005.14165, 2020. [https://api.semanticscholar.org/CorpusID:218971783](https://api.semanticscholar.org/CorpusID:218971783).
* Chen et al.(2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. 나이트, 마일스 브런디지, 미라 무라티, 케이티 메이어, 피터 웰린더, 밥 맥그루, 다리오 아모데이, 샘 맥캔드리시, 일리야 수츠키버, 그리고 보이시크 자렘바. 코드에 대해 학습된 대규모 언어 모델을 평가하는 중입니다. _ ArXiv_, abs/2107.03374, 2021. [https://api.semanticscholar.org/CorpusID:235755472](https://api.semanticscholar.org/CorpusID:235755472).
* Clark 등(2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문을 풀었다고 생각해? AI2 추론 도전인 ARC를 사용해 보세요. _ arXiv preprint arXiv:1803.05457_, 2018.
* Cobbe 등(2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 수학 단어 문제를 해결하기 위한 검증자 훈련. _ arXiv preprint arXiv:2110.14168_, 2021.
* Dai et al. (2024) Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. 우, 젠다시에, Y.K. 리, 판판황, 훌리뤄, 충루앙, 지팡수이, 원펑량. 딥섹모: 전문가 혼합 언어 모델의 궁극적인 전문가 전문화를 지향합니다. _ ArXiv_, abs/2401.06066, 2024. [https://api.semanticscholar.org/CorpusID:266933338](https://api.semanticscholar.org/CorpusID:266933338).
* Douillard 등(2023) Arthur Douillard, Qixuang Feng, Andrei A. Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc'Aurelio Ranzato, Arthur Szlam, and Jiajun Shen. 딜로코: 언어 모델의 분산 저통신 교육 _ ArXiv_, abs/2311.08105, 2023. [https://api.semanticscholar.org/CorpusID:265158012](https://api.semanticscholar.org/CorpusID:265158012).
* Fedus et al.(2022) William Fedus, Barret Zoph, and Noam Shazeer. 트랜스포머 전환: 간단하고 효율적인 희소성으로 조 단위 매개변수 모델로 확장합니다. _ The Journal of Machine Learning Research_, 23(1):5232-5270, 2022.
* 팀(2023) 제미니 팀. 제미니: 매우 유능한 멀티모달 모델 가족입니다. _ arXiv preprint arXiv:2312.11805_, 2023. Team et al. (2021) Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others.

수신 구루랑간, 마이클 루이스, 아리 홀츠만, 노아 A. 스미스, 루크 제틀모이어. 디믹스 레이어: 모듈식 언어 모델링을 위한 디엔탱글링 도메인. [https://api.semanticscholar.org/CorpusID:236976189](https://api.semanticscholar.org/CorpusID:236976189).
* Gururangan et al. [2023] Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah A Smith, and Luke Zettlemoyer. Scaling expert language models with unsupervised domain discovery. _arXiv preprint arXiv:2303.14177_, 2023.
* Hendrycks et al. [2021a] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 대규모 멀티태스킹 언어 이해도 측정 제9회 국제학술대회에서, ICLR 2021, 오스트리아 가상행사, 2021년 5월 3일부터 7일까지. OpenReview.net, 2021a. [https://openreview.net/forum?id=47KBjm13GmQ] (https://openreview.net/forum?id=47KBjm13GmQ).
* Hendrycks et al. [2021b] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, and Jacob Steinhardt. 수학 데이터 세트를 사용하여 수학 문제 풀이를 측정하는 중입니다. _ ArXiv_, abs/2103.03874, 2021b. [https://api.semanticscholar.org/CorpusID:232134851] (https://api.semanticscholar.org/CorpusID:232134851).
* Jacobs et al. [1991] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. _Neural Computation_, 3:79-87, 1991. [https://api.semanticscholar.org/CorpusID:572361](https://api.semanticscholar.org/CorpusID:572361).
* Jang et al. [2016] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. _arXiv preprint arXiv:1611.01144_, 2016.
* Jiang et al. [2022] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L'elio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral of experts. _ArXiv_, abs/2401.04088, 2024. [https://api.semanticscholar.org/CorpusID:266844877](https://api.semanticscholar.org/CorpusID:266844877).
* Joshi et al. [2017] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. _ArXiv_, abs/1705.03551, 2017. [https://api.semanticscholar.org/CorpusID:26501419](https://api.semanticscholar.org/CorpusID:26501419).
* Komatsuzaki et al. [2022] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. _ArXiv_, abs/2212.05055, 2022. [https://api.semanticscholar.org/CorpusID:254535822](https://api.semanticscholar.org/CorpusID:254535822).
* Kwiatkowski et al. [2019] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. _Transactions of the Association of Computational Linguistics_, 2019.
* Lange et al. [2019] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory G. Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44:3366-3385, 2019. [https://api.semanticscholar.org/CorpusID:218889912](https://api.semanticscholar.org/CorpusID:218889912).
* Lewis et al. [2021] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In _International Conference on Machine Learning_, 2021. [https://api.semanticscholar.org/CorpusID:232428341](https://api.semanticscholar.org/CorpusID:232428341).
* Li et al. [2022a] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke Zettlemoyer. 브랜치-트레인-머지: 전문가 언어 모델의 당황스러울 정도로 병렬 훈련입니다. _ ArXiv_, abs/2208.03306, 2022a. [https://api.semanticscholar.org/CorpusID:251371375] (https://api.semanticscholar.org/CorpusID:251371375).
* 1097, 2022b. [https://api.semanticscholar.org/CorpusID:246527904] (https://api.semanticscholar.org/CorpusID:246527904).
* Ouyang et al. [2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. _ArXiv_, abs/2203.02155, 2022. [https://api.semanticscholar.org/CorpusID:246426909](https://api.semanticscholar.org/CorpusID:246426909).
* Ouyang et al. [2022]Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. 대형 희소 모델의 해시 도면층 [https://api.semanticscholar.org/CorpusID:235367626](https://api.semanticscholar.org/CorpusID:235367626).
* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D'efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 코드 라마: 코드에 대한 기초 모델을 엽니다. _ ArXiv_, abs/2308.12950, 2023. [https://api.semanticscholar.org/CorpusID:261100919](https://api.semanticscholar.org/CorpusID:261100919).
* Rusu et al.(2016) Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 진보적인 신경망입니다. _ ArXiv_, abs/1606.04671, 2016. [https://api.semanticscholar.org/CorpusID:15350923](https://api.semanticscholar.org/CorpusID:15350923).
* Sakaguchi et al.(2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 위노그란데: 규모의 적대적인 위노그라드 스키마 도전입니다. _ Communications of the ACM_, 64(9):99-106, 2021.
* Sap et al.(2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 소셜리카: 사회적 상호 작용에 대한 상식적인 추론입니다. _ arXiv preprint arXiv:1904.09728_, 2019.
* Shao et al.(2024) Zhihong Shao, Peiyi Wang, Qihao Zhu, R. X. Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daa Guo. 심층수학: 개방형 언어 모델에서 수학적 추론의 한계를 밀어붙입니다. _ ArXiv_, abs/2402.03300, 2024. [https://api.semanticscholar.org/CorpusID:267412607](https://api.semanticscholar.org/CorpusID:267412607).
* Shazeer et al.(2017) Noam M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton 그리고 Jeff Dean. 엄청나게 큰 신경망: 희박하게 게이팅된 전문가 혼합 계층입니다. _ ArXiv_, abs/1701.06538, 2017. [https://api.semanticscholar.org/CorpusID:12462234](https://api.semanticscholar.org/CorpusID:12462234).
* 투브론 외 (2023) 휴고 투브론, 루이 마틴, 케빈 스톤, 피터 알버트, 암자드 알마하리, 야스민 바바이, 니콜라이 바슐리코프, 소우마 바트라, 프라즈왈 바살레, 슈루티 바살레, 단 비켈, 루카스 블레처, 크리스티안 칸톤 페러, 모야 첸, 기옌 쿠쿠룰, 다비드 에시오부, 주드 페르난데스, 제레미 푸, 원린 푸, 브라이언 풀러, 신시아 가오, 제레미 푸, 베다누즈 고세이니, 루이 호우, 하칸 이난, 마르신 카다스, 빅토르 케르케즈, 매디안 하바사, 이사벨 클루만, 아르템 고레네프, 신시아 가오, 사그하르 호세이니, 하칸 이난, 마르신 카다스, 빅토르 케르케즈, 마디안 카다스, 마디안 라쇼, 사그하르 호세이니, 제냐 리, 다이애나 리스코비치, 잉하이 루, 윤잉 마오, 라마 2: 오픈 파운데이션과 미세 조정된 채팅 모델, 2023.
* Wortsman et al. (2022) Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. 모르코스, 홍석 남궁, 알리 파하디, 야르 카르몬, 사이먼 콘블리스, 루드비히 슈미트. 모델 수프: 여러 미세 조정 모델의 가중치를 평균화하면 추론 시간을 늘리지 않고도 정확도가 향상됩니다. _ ArXiv_, abs/2203.05482, 2022. [https://api.semanticscholar.org/CorpusID:247362886](https://api.semanticscholar.org/CorpusID:247362886).
*Xue 등(2024) Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: 개방형 전문가 혼합 언어 모델에 대한 초기 노력. _ arXiv preprint arXiv:2402.01739_, 2024.
* Zhang 등(2015) Sixin Zhang, Anna E Choromanska, and Yann LeCun. 탄성 평균 sgd를 사용한 딥 러닝. N. C. Cortes 로렌스 이동민 수기야마, R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. [https://proceedings.neurips.cc/paper_files/paper/2015/file/d18f655c3fce66ca401d5f38b48c89af-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/d18f655c3fce66ca401d5f38b48c89af-Paper.pdf)
* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. 디아브, 시안 리, 시 빅토리아 린, 토도르 미하일로프, 마일 오트, 샘 슐레이퍼, 커트 슈스터, 다니엘 시미그, 푸닛 싱 코우라, 안잘리 스리다르, 톈루 왕, 루크 제틀모이어. 옵션: 미리 훈련된 변압기 언어 모델을 엽니다. _ ArXiv_, abs/2205.01068, 2022. [https://api.semanticscholar.org/CorpusID:248496292](https://api.semanticscholar.org/CorpusID:248496292).
*Zhao et al.(2024) Jun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, and Shuanjing Huang. 영어를 넘어선 라마: 언어 능력 전이에 대한 실증적 연구 _ arXiv preprint arXiv:2401.01055_, 2024.

## 데이터 혼합물 부록

표 7은 각 도메인 전문가의 훈련에 사용된 정확한 데이터 혼합 비율을 보여준다. MoE 모델을 미세 조정하기 위해 수학 전문가, 코드 전문가, 위키피디아 전문가 및 원래 Llama-2 7B를 훈련하는 데 사용된 데이터 세트를 확률 30.16%, 40.31%, 10.30% 및 19.23%로 샘플링했다.

## 부록 B 평가

우리는 Touvron et al. (2023)과 Roziere et al. (2023)에서 사용된 것과 동일한 평가 메트릭을 사용한다: 코드 태스크(HumanEval and MBPP)에 대해 pass@1을 보고하고, 수학 태스크(GSM8k and MATH) 및 지식 태스크(Natural Questions and TriviaQA)에 대해 정확한 일치를 보고하고, MMLU 및 ARC에 대해 정확성을 보고한다. 우리는 모든 세대에 대해 탐욕스러운 디코딩을 사용한다. 모든 작업에 대한 자세한 결과는 표 8에 보고되어 있다.

## 부록 C 라우팅 분석

다양한 라우터 설계 및 태스크 도메인별로 집계된 다운스트림 태스크에 대한 라우팅 결정의 계층별 비교는 그림 4와 같다. 라우팅 분포는 처음 몇 개의 계층에서 약간 다르지만 계층마다 빠르게 구별할 수 없게 된다. 한 가지 예외는 스위치 라우팅에서 수학 전문가가 마지막 모델 계층에서 작업 전반에 걸쳐 우세하게 되는 것입니다.

\begin{table}
\begin{tabular}{l l l} \hline \hline Domain & Dataset & Sampling ratio (\%) \\ \hline \multirow{4}{*}{Math} & AlgebraicStack & 13.57 \\  & OpenWebMath & 54.27 \\  & Arxiv & 27.14 \\  & Github & 2.99 \\  & Commoncrawl & 5.01 \\ \hline \multirow{2}{*}{Code} & Code & 82.18 \\  & Natural language related to code & 9.90 \\  & Natural language & 6.93 \\ \hline \multirow{2}{*}{Wikipedia} & Wikipedia & 90.91 \\  & Commoncrawl & 9.09 \\ \hline \hline \end{tabular}
\end{table}
표 7: 도메인 전문가를 위한 데이터 소스 및 가중치.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline  & GSM8K & MATH & Human & MBPP & Natural & Trivia & ARC-e & ARC-c & Wino & SIQA & PIQA & MMLU \\  & & Eval & & & Questions & QA & & & & & \\ \hline \multicolumn{11}{l}{_Specialized LLMs_} \\ CodeLlama 7B & 13.0 & 3.3 & 31.1 & 41.4 & 11.5 & 32.8 & 67.4 & 34.0 & 62.7 & 46.1 & 72.9 & 38.6 \\ Llamma 7B & 39.3 & 16.7 & 25.6 & 41.4 & 9.4 & 24.9 & 28.7 & 26.8 & 50.1 & 37.3 & 51.0 & 33.5 \\ \hline \multicolumn{11}{l}{_General LLMs_} \\ Llama-2 7B & 14.7 & 2.5 & 12.8 & 20.8 & 16.4 & 58.5 & 76.4 & 43.8 & 69.2 & 48.3 & 78.8 & 46.1 \\ Llama-2 13B & 28.7 & 3.9 & 18.3 & 30.6 & 16.1 & 63.8 & 77.3 & 49.4 & 73.0 & 50.1 & 80.8 & 52.8 \\ Dense (DM) & 26.7 & 9.9 & 20.7 & 30.8 & 24.0 & 55.3 & 76.7 & 44.5 & 68.9 & 48.3 & 78.2 & 49.8 \\ Sparse upcveling (DM), Top-2 & 37.3 & 18.9 & 29.3 & 40.2 & 18.8 & 49.2 & 76.3 & 43.4 & 66.4 & 47.3 & 77.9 & 51.1 \\ Sparse upcveling (CM), Top-2 & 40.1 & 16.2 & 26.2 & 35.2 & 24.5 & 58.2 & 75.6 & 44.7 & 69.1 & 47.1 & 78.0 & 52.1 \\ BTM, Top-1 & 27.4 & 15.2 & 30.8 & 41.9 & 15.0 & 38.0 & 72.8 & 38.1 & 68.4 & 47.8 & 77.9 & 44.3 \\ BTM, Top-2 & 27.7 & 15.3 & 30.6 & 42.6 & 15.3 & 38.5 & 73.1 & 38.5 & 68.3 & 48.0 & 78.1 & 44.3 \\ BTX, sample Top-1 & 36.9 & 15.8 & 25.6 & 37.4 & 23.7 & 56.4 & 76.7 & 45.0 & 70.6 & 48.0 & 78.2 & 53.2 \\ BTX, Top-2 & 37.1 & 17.8 & 28.7 & 39.4 & 24.8 & 57.1 & 76.9 & 45.6 & 67.9 & 48.7 & 78.7 & 52.5 \\ \hline \hline \end{tabular}
\end{table}
표 8: BTX 및 기준선의 개별 작업 수행.

로드 밸런싱을 고려한 Top-2 라우팅에서 코드 전문가가 코드 도메인에서 지배적인 힘임을 알 수 있다. 부하 분산이 추가되지 않고 수학 전문가가 도메인 전체에서 우세한 다른 모델과의 차이점에 주목하십시오. 그림 5에서 코드 도메인을 자세히 살펴보고 로드 밸런싱이 있는 모델과 없는 모델에 대한 라우팅 확률 분포를 비교한다. 그림의 아래 세 그래프에서 코드 전문가로의 라우팅 확률이 0으로 이동한 반면 로드 밸런싱이 추가되면 전문가 간의 확률 분포가 더 유사해 보이며 코드 전문가에 대한 기대치가 약간 더 높다.

전문가가 다른 영역을 전문으로 하는지 이해하기 위해 작업별 분포를 자세히 살펴본다. Math 및 Reasoning 도메인에서 토큰의 라우팅 결정은 그림 6과 같다. GSM8K 태스크는 Code와 Llama-2 전문가를 선호하는 반면, Math 태스크는 도메인 내 전문가에 더 의존한다는 것을 관찰한다. GSM8K 데이터 세트는 상식 지식과 기본 사칙 연산이 필요한 초등학교 수학 단어 문제로 구성된 반면, 수학 과제는 대학 수준의 수학 지식이 필요하고 수학 전문가의 훈련 데이터와 더 일치하기 때문에 이러한 현상이 발생한다고 가정한다. 추론 영역에서 모든 과제는 유사한 행동을 보이며 수학 및 일반론자 LLM의 전문성에 동등하게 의존한다.

## 부록 그림 4: 다양한 계층에서 토큰의 BTX 라우팅 결정을 다른 다운스트림 작업에 대해 다른 전문가(위키, 수학, 코드, LLaMa-2 7B)에게 라우팅합니다. 작업은 코드(인간 평가, MBPP), 수학(GSM8K, MATH), 세계 지식(자연 질문, 트리비아QA) 및 추론(ARC-Easy, ARC-Challenge, SIQA, PIQA 및 WinoGrande) 도메인별로 집계됩니다. 로드 밸런싱을 이용한 Top-2 라우팅은 모든 계층에서 다른 라우팅 방법에 비해 전문가들 간의 로드의 균일한 분배를 보장한다.

그림 5: 인간 평가 작업에 대해 서로 다른 계층에 걸쳐 전문가당 확률을 라우팅합니다. 상단-2 라우팅을 (좌측)과 부하 분산 없이 (우측) 비교한다.

그림 6: 수학 및 추론 도메인에서 토큰의 라우팅 결정입니다. GSM8K 태스크는 코드 및 라마-2 전문가를 선호하는 반면 MATH 태스크는 도메인 내 전문가에 더 의존한다는 것을 관찰한다. 추론 영역에서는 Math와 LLaMa-2 7B 전문가 사이에 부하가 분산된다.

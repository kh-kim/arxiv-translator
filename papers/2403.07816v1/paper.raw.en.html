<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM</title>
<!--Generated on Tue Mar 12 16:56:50 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on March 12, 2024.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.07816v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2403.07816v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2403.07816v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2403.07816v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S1" title="1 Introduction ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S2" title="2 Related Work ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S2.SS0.SSS0.Px1" title="Asynchronous parallel training ‣ 2 Related Work ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title">Asynchronous parallel training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S2.SS0.SSS0.Px2" title="Mixture-of-Experts ‣ 2 Related Work ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title">Mixture-of-Experts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S2.SS0.SSS0.Px3" title="Continual learning ‣ 2 Related Work ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title">Continual learning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S3" title="3 Branch-Train-MiX ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Branch-Train-MiX</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S3.SS1" title="3.1 Branch &amp; Train: Embarrassingly Parallel Expert Training ‣ 3 Branch-Train-MiX ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Branch &amp; Train: Embarrassingly Parallel Expert Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S3.SS2" title="3.2 MiX: Combining Separate Experts to be a Mixture-of-Experts ‣ 3 Branch-Train-MiX ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>MiX: Combining Separate Experts to be a Mixture-of-Experts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S3.SS3" title="3.3 Variations ‣ 3 Branch-Train-MiX ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Variations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S3.SS3.SSS0.Px1" title="Load balancing ‣ 3.3 Variations ‣ 3 Branch-Train-MiX ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title">Load balancing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S3.SS3.SSS0.Px2" title="Routing method ‣ 3.3 Variations ‣ 3 Branch-Train-MiX ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title">Routing method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S3.SS3.SSS0.Px3" title="Splitting Experts ‣ 3.3 Variations ‣ 3 Branch-Train-MiX ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title">Splitting Experts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S3.SS3.SSS0.Px4" title="Blending Experts ‣ 3.3 Variations ‣ 3 Branch-Train-MiX ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title">Blending Experts</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4" title="4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS1" title="4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS1.SSS1" title="4.1.1 BTX Training ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>BTX Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS1.SSS2" title="4.1.2 Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS1.SSS3" title="4.1.3 Evaluation ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS2" title="4.2 Main Results ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS2.SSS1" title="4.2.1 Overall Performance ‣ 4.2 Main Results ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Overall Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS2.SSS1.Px1" title="Domain experts excel at their respective tasks. ‣ 4.2.1 Overall Performance ‣ 4.2 Main Results ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title">Domain experts excel at their respective tasks.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS2.SSS1.Px2" title="BTX improves all tasks where experts specialize. ‣ 4.2.1 Overall Performance ‣ 4.2 Main Results ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title">BTX improves all tasks where experts specialize.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS2.SSS2" title="4.2.2 Better compute-performance tradeoff ‣ 4.2 Main Results ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Better compute-performance tradeoff</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS2.SSS2.Px1" title="Better performance than dense and BTM. ‣ 4.2.2 Better compute-performance tradeoff ‣ 4.2 Main Results ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title">Better performance than dense and BTM.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS2.SSS2.Px2" title="More efficient than sparse upcycling. ‣ 4.2.2 Better compute-performance tradeoff ‣ 4.2 Main Results ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title">More efficient than sparse upcycling.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS3" title="4.3 Ablations &amp; Analysis ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablations &amp; Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS3.SSS1" title="4.3.1 Ablations of BTX training ‣ 4.3 Ablations &amp; Analysis ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Ablations of BTX training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS3.SSS2" title="4.3.2 Routing Analysis ‣ 4.3 Ablations &amp; Analysis ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Routing Analysis</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S5" title="5 Conclusion ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S6" title="6 Limitations &amp; Future Work ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Limitations &amp; Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S7" title="7 Acknowledgements ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Acknowledgements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S8" title="8 Data mixture ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Data mixture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S9" title="9 Evaluation ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S10" title="10 Routing analysis ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Routing analysis</span></a></li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2403.07816v1 [cs.CL] 12 Mar 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Branch-Train-MiX: 
<br class="ltx_break">Mixing Expert LLMs into a Mixture-of-Experts LLM
</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sainbayar Sukhbaatar
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Olga Golovneva
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vasu Sharma
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hu Xu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xi Victoria Lin
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Baptiste Rozière
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jacob Kahn
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Li
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wen-tau Yih
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jason Weston
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xian Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">FAIR at Meta
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_dates">(March 12, 2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id1.id1">We investigate efficient methods for training Large Language Models (LLMs) to possess capabilities in multiple specialized domains, such as coding, math reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts from a seed model, which is branched to train experts in embarrassingly parallel fashion with high throughput and reduced communication cost. After individual experts are asynchronously trained, BTX brings together their feedforward parameters as experts in Mixture-of-Expert (MoE) layers and averages the remaining parameters, followed by an MoE-finetuning stage to learn token-level routing. BTX generalizes two special cases, the Branch-Train-Merge method, which does not have the MoE finetuning stage to learn routing, and sparse upcycling, which omits the stage of training experts asynchronously. Compared to alternative approaches, BTX achieves the best accuracy-efficiency tradeoff.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<span class="ltx_ERROR undefined" id="id1">\correspondence</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">{sainbar,xianl}@meta.com
<span class="ltx_text" id="p1.1.1" lang="en"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, Large Language Models (LLMs) have shown impressive performance in a wide-range of tasks
<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib7" title="">2020</a>; Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib37" title="">2023</a>; Achiam et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib1" title="">2023</a>)</cite>, including code generation <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib28" title="">2022b</a>; Rozière et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib31" title="">2023</a>)</cite>, solving math problems <cite class="ltx_cite ltx_citemacro_citep">(Azerbayev et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib5" title="">2023</a>)</cite>, multilinguality <cite class="ltx_cite ltx_citemacro_citep">(Zhao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib42" title="">2024</a>)</cite>, etc.
Training such LLMs requires a large amount of compute and data, exceeding thousands of GPUs and trillions of tokens.
The training parallelization is typically done by maintaining multiple copies of the model on different GPUs and keeping them synchronized after each weight update.
The cost of this frequent communication is the main bottleneck in scaling the training to more GPUs.
Besides this issue, synchronized training is more vulnerable to hardware failures as a single failed GPU can cause the whole training to halt <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib41" title="">2022</a>; Gemini Team, <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib14" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recent work by <cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib27" title="">2022a</a>)</cite> proposed the Branch-Train-Merge (BTM) method for embarrassingly parallel training of LLMs without any synchronization for improving the throughput of pretraining.
It starts by creating multiple copies of a seed LLM, then separately training each copy on different subsets of data.
This results in multiple independent LLMs that do not share any parameters and each LLM is an expert specializing in its own data distribution, such as knowledge domains, languages or even modalities.
At test time, an input prompt is classified into one or more of the domains, and then the final outputs are formed from the corresponding expert models which are combined to predict the next token.
While this approach makes training more efficient, its main drawback is the lack of a unified single model making it impossible to do further supervised finetuning (SFT) or reinforcement learning from human feedback (RLHF) finetuning <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib29" title="">2022</a>)</cite>, both of which can boost performance further, and are crucial steps in building aligned LLMs. </p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">A separate line of work for reducing the computational footprint of LLMs is the Mixture-of-Experts (MoE) approach <cite class="ltx_cite ltx_citemacro_citep">(Jacobs et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib19" title="">1991</a>; Shazeer et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib36" title="">2017</a>)</cite>, where only a subset of parameteters are active at any given time.
In particular, MoE is applied to the feedforward sublayer of Transformers <cite class="ltx_cite ltx_citemacro_citep">(Fedus et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib13" title="">2022</a>; Roller et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib30" title="">2021</a>; Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib26" title="">2021</a>)</cite>, allowing the total number of parameters to grow without additional computation.
LLMs scaled in this way have shown impressive performance on downstream tasks <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib21" title="">2024</a>; Xue et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib39" title="">2024</a>)</cite>. Unlike Branch-Train-Merge, Mixture-of-Experts are often trained in a fully synchronized fashion, and the communication cost increases with the number of experts due to all-to-all communication.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we aim for the best of both worlds, combining the advantages of Branch-Train-Merge and Mixture-of-Experts, while mitigating their disadvantages.
We achieve this by training multiple expert LLMs separately as in the Branch-Train-Merge method, but subsequently combine those experts into a single model using an MoE architecture.
More specifically, the feedforward sublayers from all the expert LLMs are brought together into a single MoE module at each layer, and a router network selects which feedforward expert to use at every token.
We merge other modules of the expert LLMs, including self-attention layers, by simply averaging their weights.
Then the resulting model is MoE-finetuned
on all the combined data by continuing training, so that the router can learn to mix the expert feedforward (FF) modules.
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Figure&nbsp;1</span></a> shows an overview of this method, which we call <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">Branch-Train-MiX</em> (BTX).
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The main advantage of BTX compared to MoE is that expert training is embarrassingly parallel and asynchronous, reducing communication cost and increasing training throughput.
Compared to Branch-Train-Merge, the final BTX model is a unified neural network that can be finetuned or used like any other standard LLM.
The final BTX model will not significantly increase inference FLOPs compared to the seed model since it is sparsely activated, despite having a much larger number of parameters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">We conduct our experiments using <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.1">Llama-2 7B</span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib37" title="">2023</a>)</cite> as a seed model and train expert LLMs on different subsets of data corresponding to the domains of math, code and Wikipedia.
With the original <span class="ltx_text ltx_font_smallcaps" id="S1.p6.1.2">Llama-2 7B</span> weights added as a fourth expert, we finetune the combined MoE model for a relatively short period compared to the pretraining process.
The resulting BTX model brings significant improvements over the seed model on tasks across various domains, especially bridging the gap with specialized models on math and code related tasks, while retaining performance on the original capabilities where specialized models suffer from catastrophic forgetting. BTX outperforms BTM on all tasks demonstrating the benefits of learnt routing through MoE finetuning. Compared to purely MoE training such as sparse upcycling, BTX is more compute efficient with higher training throughput and more balanced performance across tasks in different domains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="231" id="S1.F1.g1" src="x1.png" width="664">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.6.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S1.F1.7.2" style="font-size:90%;">The Branch-Train-MiX (BTX) method<span class="ltx_text ltx_font_medium" id="S1.F1.7.2.1"> has three steps: </span>1) branch<span class="ltx_text ltx_font_medium" id="S1.F1.7.2.2"> from a pretrained seed LLM by making multiple copies of it; </span>2) train<span class="ltx_text ltx_font_medium" id="S1.F1.7.2.3"> those copies separately on different subsets of data to obtain expert LLMs; </span>3) mix<span class="ltx_text ltx_font_medium" id="S1.F1.7.2.4"> those expert LLMs by combining them into a single LLM using mixture-of-experts feedforward (FF) layers, and finetuning the overall unified model.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Asynchronous parallel training</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Reducing communication between training workers for computational efficiency is a major topic of study for training deep learning systems.
<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib40" title="">2015</a>)</cite> introduced a method that allows model instances on different workers to diverge from each other, thus eliminating the constant need of synchronization. Instead, the workers are loosely synchronized to master weights using elastic averaging from time to time.
A more recent work by <cite class="ltx_cite ltx_citemacro_citet">Douillard et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib12" title="">2023</a>)</cite> showed that less frequent synchronization of diverged workers by averaging their weight changes and applying Nesterov momentum works well in practice for training LLMs.
The Branch-Train-Merge method <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib27" title="">2022a</a>; Gururangan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib16" title="">2023</a>)</cite> takes parallel training to the extreme by running multiple training processes completely independently.
Each training process uses specific domain data, thus the corresponding model becomes an expert in that domain. Finally, the output distributions of those expert models are averaged to make a next token prediction.
Which experts to average is decided by classifying the input into one or more of the domains.
<cite class="ltx_cite ltx_citemacro_citet">Wortsman et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib38" title="">2022</a>)</cite> showed simply averaging parameters of separately trained models improves performance, but the models only differed in their hyperparameters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Mixture-of-Experts</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">MoE is used to scale deep networks in <cite class="ltx_cite ltx_citemacro_cite">Shazeer et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib36" title="">2017</a>)</cite> using a simple Top-K routing scheme.
Since the routing decisions are discrete and thus cannot be trained by gradient descent, various training methods have been explored for the Transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Fedus et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib13" title="">2022</a>; Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib26" title="">2021</a>)</cite>.
Surprisingly <cite class="ltx_cite ltx_citemacro_cite">Roller et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib30" title="">2021</a>)</cite> showed that even a fixed routing scheme without any learning works well, if the routing is done via a random mapping based on input tokens.
In larger scale experiments with recent LLMs, <cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib21" title="">2024</a>)</cite> demonstrated that the MoE approach can match the performance of dense LLM counterparts using a much smaller number of active parameters.
A study by <cite class="ltx_cite ltx_citemacro_cite">Dai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib11" title="">2024</a>)</cite> showed the advantage of more fine-grained experts, as well as having a shared expert that always stay active.
More similar to our work, <cite class="ltx_cite ltx_citemacro_cite">Gururangan et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib15" title="">2021</a>)</cite> makes experts in feedforward layers specialize to specific domains using a domain-conditioned fixed routing, but it lacks the asynchronous training of our approach.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Continual learning</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Our method relates to continual learning <cite class="ltx_cite ltx_citemacro_citep">(Awasthi and Sarawagi, <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib4" title="">2019</a>)</cite> because domain experts are trained on datasets with different distributions from the initial data used for training the seed model, which is implemented by continued training after branching.
Specifically, our approach is related to parameter isolation methods <cite class="ltx_cite ltx_citemacro_citep">(Lange et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib25" title="">2019</a>)</cite> as we have different parameters for different domains.
<cite class="ltx_cite ltx_citemacro_citet">Aljundi et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib2" title="">2016</a>)</cite> also creates a new copy of a model to train on each domain.
<cite class="ltx_cite ltx_citemacro_citet">Rusu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib32" title="">2016</a>)</cite> adds a new model with a new domain, but connects it to the previous models so the previously learned features can be used.
<cite class="ltx_cite ltx_citemacro_citet">Rozière et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib31" title="">2023</a>)</cite> showed continual training of a seed LLM on a specific domain of code can produce a strong domain expert model, and this converges much faster than starting from scratch.
For training a math expert, starting from a code expert rather than a general LLM was shown to be more beneficial <cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib35" title="">2024</a>; Azerbayev et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib5" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Branch-Train-MiX</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.3">Given an existing LLM <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">caligraphic_M</annotation></semantics></math> which has been pretrained on a large corpora covering a wide variety of topics, we aim to improve its performance on <math alttext="N" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_N</annotation></semantics></math> areas of expertise. This is achieved by continued pretraining with corresponding training datasets <math alttext="\mathcal{D}\coloneqq\{D_{1},\ldots,D_{N}\}" class="ltx_Math" display="inline" id="S3.p1.3.m3.3"><semantics id="S3.p1.3.m3.3a"><mrow id="S3.p1.3.m3.3.3" xref="S3.p1.3.m3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.3.m3.3.3.4" xref="S3.p1.3.m3.3.3.4.cmml">𝒟</mi><mo id="S3.p1.3.m3.3.3.3" xref="S3.p1.3.m3.3.3.3.cmml">≔</mo><mrow id="S3.p1.3.m3.3.3.2.2" xref="S3.p1.3.m3.3.3.2.3.cmml"><mo id="S3.p1.3.m3.3.3.2.2.3" stretchy="false" xref="S3.p1.3.m3.3.3.2.3.cmml">{</mo><msub id="S3.p1.3.m3.2.2.1.1.1" xref="S3.p1.3.m3.2.2.1.1.1.cmml"><mi id="S3.p1.3.m3.2.2.1.1.1.2" xref="S3.p1.3.m3.2.2.1.1.1.2.cmml">D</mi><mn id="S3.p1.3.m3.2.2.1.1.1.3" xref="S3.p1.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.p1.3.m3.3.3.2.2.4" xref="S3.p1.3.m3.3.3.2.3.cmml">,</mo><mi id="S3.p1.3.m3.1.1" mathvariant="normal" xref="S3.p1.3.m3.1.1.cmml">…</mi><mo id="S3.p1.3.m3.3.3.2.2.5" xref="S3.p1.3.m3.3.3.2.3.cmml">,</mo><msub id="S3.p1.3.m3.3.3.2.2.2" xref="S3.p1.3.m3.3.3.2.2.2.cmml"><mi id="S3.p1.3.m3.3.3.2.2.2.2" xref="S3.p1.3.m3.3.3.2.2.2.2.cmml">D</mi><mi id="S3.p1.3.m3.3.3.2.2.2.3" xref="S3.p1.3.m3.3.3.2.2.2.3.cmml">N</mi></msub><mo id="S3.p1.3.m3.3.3.2.2.6" stretchy="false" xref="S3.p1.3.m3.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.3b"><apply id="S3.p1.3.m3.3.3.cmml" xref="S3.p1.3.m3.3.3"><ci id="S3.p1.3.m3.3.3.3.cmml" xref="S3.p1.3.m3.3.3.3">≔</ci><ci id="S3.p1.3.m3.3.3.4.cmml" xref="S3.p1.3.m3.3.3.4">𝒟</ci><set id="S3.p1.3.m3.3.3.2.3.cmml" xref="S3.p1.3.m3.3.3.2.2"><apply id="S3.p1.3.m3.2.2.1.1.1.cmml" xref="S3.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.p1.3.m3.2.2.1.1.1.2.cmml" xref="S3.p1.3.m3.2.2.1.1.1.2">𝐷</ci><cn id="S3.p1.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S3.p1.3.m3.2.2.1.1.1.3">1</cn></apply><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">…</ci><apply id="S3.p1.3.m3.3.3.2.2.2.cmml" xref="S3.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p1.3.m3.3.3.2.2.2.1.cmml" xref="S3.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S3.p1.3.m3.3.3.2.2.2.2.cmml" xref="S3.p1.3.m3.3.3.2.2.2.2">𝐷</ci><ci id="S3.p1.3.m3.3.3.2.2.2.3.cmml" xref="S3.p1.3.m3.3.3.2.2.2.3">𝑁</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.3c">\mathcal{D}\coloneqq\{D_{1},\ldots,D_{N}\}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.3d">caligraphic_D ≔ { italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_D start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }</annotation></semantics></math>, each related to a specific knowledge domain such as math, code, etc.
The proposed method contains three stages: Branch, Train, and MiX.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Branch &amp; Train: Embarrassingly Parallel Expert Training</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.8">Initializing from the seed model <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">caligraphic_M</annotation></semantics></math>, we train <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_N</annotation></semantics></math> expert LLMs <math alttext="\{\mathcal{M}_{1},\ldots,\mathcal{M}_{N}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.3"><semantics id="S3.SS1.p1.3.m3.3a"><mrow id="S3.SS1.p1.3.m3.3.3.2" xref="S3.SS1.p1.3.m3.3.3.3.cmml"><mo id="S3.SS1.p1.3.m3.3.3.2.3" stretchy="false" xref="S3.SS1.p1.3.m3.3.3.3.cmml">{</mo><msub id="S3.SS1.p1.3.m3.2.2.1.1" xref="S3.SS1.p1.3.m3.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.2.2.1.1.2" xref="S3.SS1.p1.3.m3.2.2.1.1.2.cmml">ℳ</mi><mn id="S3.SS1.p1.3.m3.2.2.1.1.3" xref="S3.SS1.p1.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.3.m3.3.3.2.4" xref="S3.SS1.p1.3.m3.3.3.3.cmml">,</mo><mi id="S3.SS1.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p1.3.m3.1.1.cmml">…</mi><mo id="S3.SS1.p1.3.m3.3.3.2.5" xref="S3.SS1.p1.3.m3.3.3.3.cmml">,</mo><msub id="S3.SS1.p1.3.m3.3.3.2.2" xref="S3.SS1.p1.3.m3.3.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.3.3.2.2.2" xref="S3.SS1.p1.3.m3.3.3.2.2.2.cmml">ℳ</mi><mi id="S3.SS1.p1.3.m3.3.3.2.2.3" xref="S3.SS1.p1.3.m3.3.3.2.2.3.cmml">N</mi></msub><mo id="S3.SS1.p1.3.m3.3.3.2.6" stretchy="false" xref="S3.SS1.p1.3.m3.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.3b"><set id="S3.SS1.p1.3.m3.3.3.3.cmml" xref="S3.SS1.p1.3.m3.3.3.2"><apply id="S3.SS1.p1.3.m3.2.2.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.2.2.1.1.2.cmml" xref="S3.SS1.p1.3.m3.2.2.1.1.2">ℳ</ci><cn id="S3.SS1.p1.3.m3.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.2.2.1.1.3">1</cn></apply><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">…</ci><apply id="S3.SS1.p1.3.m3.3.3.2.2.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.3.3.2.2.1.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.3.3.2.2.2.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.2">ℳ</ci><ci id="S3.SS1.p1.3.m3.3.3.2.2.3.cmml" xref="S3.SS1.p1.3.m3.3.3.2.2.3">𝑁</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.3c">\{\mathcal{M}_{1},\ldots,\mathcal{M}_{N}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.3d">{ caligraphic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , caligraphic_M start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }</annotation></semantics></math>, with each model <math alttext="\mathcal{M}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">ℳ</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ℳ</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathcal{M}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> being trained on the corresponding dataset <math alttext="D_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">D</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝐷</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in the same manner as during pretraining, using the usual language modeling objective.
Since each expert model <math alttext="\mathcal{M}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><msub id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">ℳ</mi><mi id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">ℳ</ci><ci id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\mathcal{M}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> can be trained in complete separation from the others, the whole training process becomes <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_N</annotation></semantics></math>-way embarrassingly parallel. This training paradigm has several benefits in large-scale distributed training. It allows linear scaling of overall training throughput when scaling up the size of compute, while joint training often faces uncertain performance from increasing batch size. It has lower all-to-all communication cost. It is also more resilient, as a single training failure will only affect one of the <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><mi id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><ci id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">italic_N</annotation></semantics></math> training processes instead of halting the entire training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">After all the expert training is finished, we will end up with <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_N</annotation></semantics></math> different LLMs, with each specializing in a specific distribution.
At this point, the Branch-Train-Merge method <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib27" title="">2022a</a>; Gururangan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib16" title="">2023</a>)</cite> uses these domain experts as is, choosing which expert to use by determining which domain the input belongs to at inference time.
Usually multiple experts are chosen, and their final output distributions are simply averaged to generate the next token.
Our BTX approach, in contrast, merges these domain experts back into a single LLM that is finetuned further, as we will describe in the next section.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>MiX: Combining Separate Experts to be a Mixture-of-Experts</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.8">We employ a Mixture-of-Experts approach to combine the domain expert models <math alttext="\mathcal{M}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">ℳ</mi><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ℳ</ci><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathcal{M}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.
However, instead of using the classical procedure of mixing the final outputs from <math alttext="\mathcal{M}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">ℳ</mi><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ℳ</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\mathcal{M}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, we do a more fine-grained mixing by performing MoE within each layer of a Transformer.
In particular, we combine the different feedforward sublayers from the domain experts into a single MoE sublayer.
If <math alttext="\mathtt{FF}_{i}^{l}(x)" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.2" xref="S3.SS2.p1.3.m3.1.2.cmml"><msubsup id="S3.SS2.p1.3.m3.1.2.2" xref="S3.SS2.p1.3.m3.1.2.2.cmml"><mi id="S3.SS2.p1.3.m3.1.2.2.2.2" xref="S3.SS2.p1.3.m3.1.2.2.2.2.cmml">𝙵𝙵</mi><mi id="S3.SS2.p1.3.m3.1.2.2.2.3" xref="S3.SS2.p1.3.m3.1.2.2.2.3.cmml">i</mi><mi id="S3.SS2.p1.3.m3.1.2.2.3" xref="S3.SS2.p1.3.m3.1.2.2.3.cmml">l</mi></msubsup><mo id="S3.SS2.p1.3.m3.1.2.1" xref="S3.SS2.p1.3.m3.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p1.3.m3.1.2.3.2" xref="S3.SS2.p1.3.m3.1.2.cmml"><mo id="S3.SS2.p1.3.m3.1.2.3.2.1" stretchy="false" xref="S3.SS2.p1.3.m3.1.2.cmml">(</mo><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">x</mi><mo id="S3.SS2.p1.3.m3.1.2.3.2.2" stretchy="false" xref="S3.SS2.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.2.cmml" xref="S3.SS2.p1.3.m3.1.2"><times id="S3.SS2.p1.3.m3.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.2.1"></times><apply id="S3.SS2.p1.3.m3.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.2.2.1.cmml" xref="S3.SS2.p1.3.m3.1.2.2">superscript</csymbol><apply id="S3.SS2.p1.3.m3.1.2.2.2.cmml" xref="S3.SS2.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.2.2.2.1.cmml" xref="S3.SS2.p1.3.m3.1.2.2">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.2.2.2.2.cmml" xref="S3.SS2.p1.3.m3.1.2.2.2.2">𝙵𝙵</ci><ci id="S3.SS2.p1.3.m3.1.2.2.2.3.cmml" xref="S3.SS2.p1.3.m3.1.2.2.2.3">𝑖</ci></apply><ci id="S3.SS2.p1.3.m3.1.2.2.3.cmml" xref="S3.SS2.p1.3.m3.1.2.2.3">𝑙</ci></apply><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\mathtt{FF}_{i}^{l}(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">typewriter_FF start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( italic_x )</annotation></semantics></math> is the feedforward sublayer at the <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_l</annotation></semantics></math>-th layer of the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_i</annotation></semantics></math>-th domain expert <math alttext="\mathcal{M}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><msub id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">ℳ</mi><mi id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">ℳ</ci><ci id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">\mathcal{M}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, then the combined MoE layer for input representation <math alttext="x" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.1"><semantics id="S3.SS2.p1.7.m7.1a"><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m7.1d">italic_x</annotation></semantics></math> at layer <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m8.1"><semantics id="S3.SS2.p1.8.m8.1a"><mi id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><ci id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m8.1d">italic_l</annotation></semantics></math> will compute:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathtt{FF}_{\text{MoE}}^{l}(x)=\sum_{i=1}^{N}g_{i}(W_{l}x)\mathtt{FF}_{i}^{l}%
(x)." class="ltx_Math" display="block" id="S3.Ex1.m1.3"><semantics id="S3.Ex1.m1.3a"><mrow id="S3.Ex1.m1.3.3.1" xref="S3.Ex1.m1.3.3.1.1.cmml"><mrow id="S3.Ex1.m1.3.3.1.1" xref="S3.Ex1.m1.3.3.1.1.cmml"><mrow id="S3.Ex1.m1.3.3.1.1.3" xref="S3.Ex1.m1.3.3.1.1.3.cmml"><msubsup id="S3.Ex1.m1.3.3.1.1.3.2" xref="S3.Ex1.m1.3.3.1.1.3.2.cmml"><mi id="S3.Ex1.m1.3.3.1.1.3.2.2.2" xref="S3.Ex1.m1.3.3.1.1.3.2.2.2.cmml">𝙵𝙵</mi><mtext id="S3.Ex1.m1.3.3.1.1.3.2.2.3" xref="S3.Ex1.m1.3.3.1.1.3.2.2.3a.cmml">MoE</mtext><mi id="S3.Ex1.m1.3.3.1.1.3.2.3" xref="S3.Ex1.m1.3.3.1.1.3.2.3.cmml">l</mi></msubsup><mo id="S3.Ex1.m1.3.3.1.1.3.1" xref="S3.Ex1.m1.3.3.1.1.3.1.cmml">⁢</mo><mrow id="S3.Ex1.m1.3.3.1.1.3.3.2" xref="S3.Ex1.m1.3.3.1.1.3.cmml"><mo id="S3.Ex1.m1.3.3.1.1.3.3.2.1" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.3.cmml">(</mo><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">x</mi><mo id="S3.Ex1.m1.3.3.1.1.3.3.2.2" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.3.3.1.1.2" rspace="0.111em" xref="S3.Ex1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.Ex1.m1.3.3.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.cmml"><munderover id="S3.Ex1.m1.3.3.1.1.1.2" xref="S3.Ex1.m1.3.3.1.1.1.2.cmml"><mo id="S3.Ex1.m1.3.3.1.1.1.2.2.2" movablelimits="false" xref="S3.Ex1.m1.3.3.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.2.2.3" xref="S3.Ex1.m1.3.3.1.1.1.2.2.3.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.2.2.3.2" xref="S3.Ex1.m1.3.3.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.Ex1.m1.3.3.1.1.1.2.2.3.1" xref="S3.Ex1.m1.3.3.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.Ex1.m1.3.3.1.1.1.2.2.3.3" xref="S3.Ex1.m1.3.3.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex1.m1.3.3.1.1.1.2.3" xref="S3.Ex1.m1.3.3.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.Ex1.m1.3.3.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.cmml"><msub id="S3.Ex1.m1.3.3.1.1.1.1.3" xref="S3.Ex1.m1.3.3.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.1.3.2" xref="S3.Ex1.m1.3.3.1.1.1.1.3.2.cmml">g</mi><mi id="S3.Ex1.m1.3.3.1.1.1.1.3.3" xref="S3.Ex1.m1.3.3.1.1.1.1.3.3.cmml">i</mi></msub><mo id="S3.Ex1.m1.3.3.1.1.1.1.2" xref="S3.Ex1.m1.3.3.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.2.cmml">W</mi><mi id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.3" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.3.cmml">l</mi></msub><mo id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S3.Ex1.m1.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.Ex1.m1.3.3.1.1.1.1.2a" xref="S3.Ex1.m1.3.3.1.1.1.1.2.cmml">⁢</mo><msubsup id="S3.Ex1.m1.3.3.1.1.1.1.4" xref="S3.Ex1.m1.3.3.1.1.1.1.4.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.1.4.2.2" xref="S3.Ex1.m1.3.3.1.1.1.1.4.2.2.cmml">𝙵𝙵</mi><mi id="S3.Ex1.m1.3.3.1.1.1.1.4.2.3" xref="S3.Ex1.m1.3.3.1.1.1.1.4.2.3.cmml">i</mi><mi id="S3.Ex1.m1.3.3.1.1.1.1.4.3" xref="S3.Ex1.m1.3.3.1.1.1.1.4.3.cmml">l</mi></msubsup><mo id="S3.Ex1.m1.3.3.1.1.1.1.2b" xref="S3.Ex1.m1.3.3.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1.5.2" xref="S3.Ex1.m1.3.3.1.1.1.1.cmml"><mo id="S3.Ex1.m1.3.3.1.1.1.1.5.2.1" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.1.1.cmml">(</mo><mi id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml">x</mi><mo id="S3.Ex1.m1.3.3.1.1.1.1.5.2.2" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex1.m1.3.3.1.2" lspace="0em" xref="S3.Ex1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.3b"><apply id="S3.Ex1.m1.3.3.1.1.cmml" xref="S3.Ex1.m1.3.3.1"><eq id="S3.Ex1.m1.3.3.1.1.2.cmml" xref="S3.Ex1.m1.3.3.1.1.2"></eq><apply id="S3.Ex1.m1.3.3.1.1.3.cmml" xref="S3.Ex1.m1.3.3.1.1.3"><times id="S3.Ex1.m1.3.3.1.1.3.1.cmml" xref="S3.Ex1.m1.3.3.1.1.3.1"></times><apply id="S3.Ex1.m1.3.3.1.1.3.2.cmml" xref="S3.Ex1.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.1.3.2.1.cmml" xref="S3.Ex1.m1.3.3.1.1.3.2">superscript</csymbol><apply id="S3.Ex1.m1.3.3.1.1.3.2.2.cmml" xref="S3.Ex1.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.1.3.2.2.1.cmml" xref="S3.Ex1.m1.3.3.1.1.3.2">subscript</csymbol><ci id="S3.Ex1.m1.3.3.1.1.3.2.2.2.cmml" xref="S3.Ex1.m1.3.3.1.1.3.2.2.2">𝙵𝙵</ci><ci id="S3.Ex1.m1.3.3.1.1.3.2.2.3a.cmml" xref="S3.Ex1.m1.3.3.1.1.3.2.2.3"><mtext id="S3.Ex1.m1.3.3.1.1.3.2.2.3.cmml" mathsize="70%" xref="S3.Ex1.m1.3.3.1.1.3.2.2.3">MoE</mtext></ci></apply><ci id="S3.Ex1.m1.3.3.1.1.3.2.3.cmml" xref="S3.Ex1.m1.3.3.1.1.3.2.3">𝑙</ci></apply><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">𝑥</ci></apply><apply id="S3.Ex1.m1.3.3.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1"><apply id="S3.Ex1.m1.3.3.1.1.1.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.1.1.2.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.2">superscript</csymbol><apply id="S3.Ex1.m1.3.3.1.1.1.2.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.1.1.2.2.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.2">subscript</csymbol><sum id="S3.Ex1.m1.3.3.1.1.1.2.2.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.2.2.2"></sum><apply id="S3.Ex1.m1.3.3.1.1.1.2.2.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.2.2.3"><eq id="S3.Ex1.m1.3.3.1.1.1.2.2.3.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.2.2.3.1"></eq><ci id="S3.Ex1.m1.3.3.1.1.1.2.2.3.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.Ex1.m1.3.3.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.Ex1.m1.3.3.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.Ex1.m1.3.3.1.1.1.2.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.2.3">𝑁</ci></apply><apply id="S3.Ex1.m1.3.3.1.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1"><times id="S3.Ex1.m1.3.3.1.1.1.1.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.2"></times><apply id="S3.Ex1.m1.3.3.1.1.1.1.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.3.2">𝑔</ci><ci id="S3.Ex1.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1"><times id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.1"></times><apply id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.2">𝑊</ci><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.3">𝑙</ci></apply><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.3">𝑥</ci></apply><apply id="S3.Ex1.m1.3.3.1.1.1.1.4.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.1.1.1.4.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.4">superscript</csymbol><apply id="S3.Ex1.m1.3.3.1.1.1.1.4.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.1.1.1.4.2.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.4">subscript</csymbol><ci id="S3.Ex1.m1.3.3.1.1.1.1.4.2.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.4.2.2">𝙵𝙵</ci><ci id="S3.Ex1.m1.3.3.1.1.1.1.4.2.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.4.2.3">𝑖</ci></apply><ci id="S3.Ex1.m1.3.3.1.1.1.1.4.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.4.3">𝑙</ci></apply><ci id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.3c">\mathtt{FF}_{\text{MoE}}^{l}(x)=\sum_{i=1}^{N}g_{i}(W_{l}x)\mathtt{FF}_{i}^{l}%
(x).</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.3d">typewriter_FF start_POSTSUBSCRIPT MoE end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( italic_x ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT italic_x ) typewriter_FF start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( italic_x ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.13">Here <math alttext="W_{l}" class="ltx_Math" display="inline" id="S3.SS2.p1.9.m1.1"><semantics id="S3.SS2.p1.9.m1.1a"><msub id="S3.SS2.p1.9.m1.1.1" xref="S3.SS2.p1.9.m1.1.1.cmml"><mi id="S3.SS2.p1.9.m1.1.1.2" xref="S3.SS2.p1.9.m1.1.1.2.cmml">W</mi><mi id="S3.SS2.p1.9.m1.1.1.3" xref="S3.SS2.p1.9.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m1.1b"><apply id="S3.SS2.p1.9.m1.1.1.cmml" xref="S3.SS2.p1.9.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.9.m1.1.1.1.cmml" xref="S3.SS2.p1.9.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.9.m1.1.1.2.cmml" xref="S3.SS2.p1.9.m1.1.1.2">𝑊</ci><ci id="S3.SS2.p1.9.m1.1.1.3.cmml" xref="S3.SS2.p1.9.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m1.1c">W_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.9.m1.1d">italic_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> is a linear transformation and <math alttext="g" class="ltx_Math" display="inline" id="S3.SS2.p1.10.m2.1"><semantics id="S3.SS2.p1.10.m2.1a"><mi id="S3.SS2.p1.10.m2.1.1" xref="S3.SS2.p1.10.m2.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m2.1b"><ci id="S3.SS2.p1.10.m2.1.1.cmml" xref="S3.SS2.p1.10.m2.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m2.1c">g</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.10.m2.1d">italic_g</annotation></semantics></math> is a routing function, which usually has sparse output and hence switches on only some experts.
Since we can skip computing <math alttext="\mathtt{FF}_{i}^{l}(x)" class="ltx_Math" display="inline" id="S3.SS2.p1.11.m3.1"><semantics id="S3.SS2.p1.11.m3.1a"><mrow id="S3.SS2.p1.11.m3.1.2" xref="S3.SS2.p1.11.m3.1.2.cmml"><msubsup id="S3.SS2.p1.11.m3.1.2.2" xref="S3.SS2.p1.11.m3.1.2.2.cmml"><mi id="S3.SS2.p1.11.m3.1.2.2.2.2" xref="S3.SS2.p1.11.m3.1.2.2.2.2.cmml">𝙵𝙵</mi><mi id="S3.SS2.p1.11.m3.1.2.2.2.3" xref="S3.SS2.p1.11.m3.1.2.2.2.3.cmml">i</mi><mi id="S3.SS2.p1.11.m3.1.2.2.3" xref="S3.SS2.p1.11.m3.1.2.2.3.cmml">l</mi></msubsup><mo id="S3.SS2.p1.11.m3.1.2.1" xref="S3.SS2.p1.11.m3.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p1.11.m3.1.2.3.2" xref="S3.SS2.p1.11.m3.1.2.cmml"><mo id="S3.SS2.p1.11.m3.1.2.3.2.1" stretchy="false" xref="S3.SS2.p1.11.m3.1.2.cmml">(</mo><mi id="S3.SS2.p1.11.m3.1.1" xref="S3.SS2.p1.11.m3.1.1.cmml">x</mi><mo id="S3.SS2.p1.11.m3.1.2.3.2.2" stretchy="false" xref="S3.SS2.p1.11.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m3.1b"><apply id="S3.SS2.p1.11.m3.1.2.cmml" xref="S3.SS2.p1.11.m3.1.2"><times id="S3.SS2.p1.11.m3.1.2.1.cmml" xref="S3.SS2.p1.11.m3.1.2.1"></times><apply id="S3.SS2.p1.11.m3.1.2.2.cmml" xref="S3.SS2.p1.11.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.11.m3.1.2.2.1.cmml" xref="S3.SS2.p1.11.m3.1.2.2">superscript</csymbol><apply id="S3.SS2.p1.11.m3.1.2.2.2.cmml" xref="S3.SS2.p1.11.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.11.m3.1.2.2.2.1.cmml" xref="S3.SS2.p1.11.m3.1.2.2">subscript</csymbol><ci id="S3.SS2.p1.11.m3.1.2.2.2.2.cmml" xref="S3.SS2.p1.11.m3.1.2.2.2.2">𝙵𝙵</ci><ci id="S3.SS2.p1.11.m3.1.2.2.2.3.cmml" xref="S3.SS2.p1.11.m3.1.2.2.2.3">𝑖</ci></apply><ci id="S3.SS2.p1.11.m3.1.2.2.3.cmml" xref="S3.SS2.p1.11.m3.1.2.2.3">𝑙</ci></apply><ci id="S3.SS2.p1.11.m3.1.1.cmml" xref="S3.SS2.p1.11.m3.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m3.1c">\mathtt{FF}_{i}^{l}(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.11.m3.1d">typewriter_FF start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( italic_x )</annotation></semantics></math> if the corresponding router output is zero, the actual computation of <math alttext="\mathtt{FF}_{\text{MoE}}^{l}(x)" class="ltx_Math" display="inline" id="S3.SS2.p1.12.m4.1"><semantics id="S3.SS2.p1.12.m4.1a"><mrow id="S3.SS2.p1.12.m4.1.2" xref="S3.SS2.p1.12.m4.1.2.cmml"><msubsup id="S3.SS2.p1.12.m4.1.2.2" xref="S3.SS2.p1.12.m4.1.2.2.cmml"><mi id="S3.SS2.p1.12.m4.1.2.2.2.2" xref="S3.SS2.p1.12.m4.1.2.2.2.2.cmml">𝙵𝙵</mi><mtext id="S3.SS2.p1.12.m4.1.2.2.2.3" xref="S3.SS2.p1.12.m4.1.2.2.2.3a.cmml">MoE</mtext><mi id="S3.SS2.p1.12.m4.1.2.2.3" xref="S3.SS2.p1.12.m4.1.2.2.3.cmml">l</mi></msubsup><mo id="S3.SS2.p1.12.m4.1.2.1" xref="S3.SS2.p1.12.m4.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p1.12.m4.1.2.3.2" xref="S3.SS2.p1.12.m4.1.2.cmml"><mo id="S3.SS2.p1.12.m4.1.2.3.2.1" stretchy="false" xref="S3.SS2.p1.12.m4.1.2.cmml">(</mo><mi id="S3.SS2.p1.12.m4.1.1" xref="S3.SS2.p1.12.m4.1.1.cmml">x</mi><mo id="S3.SS2.p1.12.m4.1.2.3.2.2" stretchy="false" xref="S3.SS2.p1.12.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m4.1b"><apply id="S3.SS2.p1.12.m4.1.2.cmml" xref="S3.SS2.p1.12.m4.1.2"><times id="S3.SS2.p1.12.m4.1.2.1.cmml" xref="S3.SS2.p1.12.m4.1.2.1"></times><apply id="S3.SS2.p1.12.m4.1.2.2.cmml" xref="S3.SS2.p1.12.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m4.1.2.2.1.cmml" xref="S3.SS2.p1.12.m4.1.2.2">superscript</csymbol><apply id="S3.SS2.p1.12.m4.1.2.2.2.cmml" xref="S3.SS2.p1.12.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m4.1.2.2.2.1.cmml" xref="S3.SS2.p1.12.m4.1.2.2">subscript</csymbol><ci id="S3.SS2.p1.12.m4.1.2.2.2.2.cmml" xref="S3.SS2.p1.12.m4.1.2.2.2.2">𝙵𝙵</ci><ci id="S3.SS2.p1.12.m4.1.2.2.2.3a.cmml" xref="S3.SS2.p1.12.m4.1.2.2.2.3"><mtext id="S3.SS2.p1.12.m4.1.2.2.2.3.cmml" mathsize="70%" xref="S3.SS2.p1.12.m4.1.2.2.2.3">MoE</mtext></ci></apply><ci id="S3.SS2.p1.12.m4.1.2.2.3.cmml" xref="S3.SS2.p1.12.m4.1.2.2.3">𝑙</ci></apply><ci id="S3.SS2.p1.12.m4.1.1.cmml" xref="S3.SS2.p1.12.m4.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m4.1c">\mathtt{FF}_{\text{MoE}}^{l}(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.12.m4.1d">typewriter_FF start_POSTSUBSCRIPT MoE end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( italic_x )</annotation></semantics></math> will be much more efficient than computing all domain experts.
However, routing decisions can change from token to token, so one input sequence can employ all the domain expert FF layers if needed, even when only a few are accessed at any given token.
In our experiments, we use Top-k (k=2) routing where <math alttext="g(W_{l}x)=\text{SoftMax}(\text{TopK}(W_{l}x))" class="ltx_Math" display="inline" id="S3.SS2.p1.13.m5.2"><semantics id="S3.SS2.p1.13.m5.2a"><mrow id="S3.SS2.p1.13.m5.2.2" xref="S3.SS2.p1.13.m5.2.2.cmml"><mrow id="S3.SS2.p1.13.m5.1.1.1" xref="S3.SS2.p1.13.m5.1.1.1.cmml"><mi id="S3.SS2.p1.13.m5.1.1.1.3" xref="S3.SS2.p1.13.m5.1.1.1.3.cmml">g</mi><mo id="S3.SS2.p1.13.m5.1.1.1.2" xref="S3.SS2.p1.13.m5.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p1.13.m5.1.1.1.1.1" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p1.13.m5.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.13.m5.1.1.1.1.1.1" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.cmml"><msub id="S3.SS2.p1.13.m5.1.1.1.1.1.1.2" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p1.13.m5.1.1.1.1.1.1.2.2" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.2.2.cmml">W</mi><mi id="S3.SS2.p1.13.m5.1.1.1.1.1.1.2.3" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.2.3.cmml">l</mi></msub><mo id="S3.SS2.p1.13.m5.1.1.1.1.1.1.1" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p1.13.m5.1.1.1.1.1.1.3" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S3.SS2.p1.13.m5.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.13.m5.2.2.3" xref="S3.SS2.p1.13.m5.2.2.3.cmml">=</mo><mrow id="S3.SS2.p1.13.m5.2.2.2" xref="S3.SS2.p1.13.m5.2.2.2.cmml"><mtext id="S3.SS2.p1.13.m5.2.2.2.3" xref="S3.SS2.p1.13.m5.2.2.2.3a.cmml">SoftMax</mtext><mo id="S3.SS2.p1.13.m5.2.2.2.2" xref="S3.SS2.p1.13.m5.2.2.2.2.cmml">⁢</mo><mrow id="S3.SS2.p1.13.m5.2.2.2.1.1" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.cmml"><mo id="S3.SS2.p1.13.m5.2.2.2.1.1.2" stretchy="false" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.13.m5.2.2.2.1.1.1" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.cmml"><mtext id="S3.SS2.p1.13.m5.2.2.2.1.1.1.3" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.3a.cmml">TopK</mtext><mo id="S3.SS2.p1.13.m5.2.2.2.1.1.1.2" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.cmml"><mo id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.cmml"><msub id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2.2" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2.2.cmml">W</mi><mi id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2.3" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2.3.cmml">l</mi></msub><mo id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.1" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.3" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.13.m5.2.2.2.1.1.3" stretchy="false" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m5.2b"><apply id="S3.SS2.p1.13.m5.2.2.cmml" xref="S3.SS2.p1.13.m5.2.2"><eq id="S3.SS2.p1.13.m5.2.2.3.cmml" xref="S3.SS2.p1.13.m5.2.2.3"></eq><apply id="S3.SS2.p1.13.m5.1.1.1.cmml" xref="S3.SS2.p1.13.m5.1.1.1"><times id="S3.SS2.p1.13.m5.1.1.1.2.cmml" xref="S3.SS2.p1.13.m5.1.1.1.2"></times><ci id="S3.SS2.p1.13.m5.1.1.1.3.cmml" xref="S3.SS2.p1.13.m5.1.1.1.3">𝑔</ci><apply id="S3.SS2.p1.13.m5.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.13.m5.1.1.1.1.1"><times id="S3.SS2.p1.13.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.1"></times><apply id="S3.SS2.p1.13.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.13.m5.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.13.m5.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.2.2">𝑊</ci><ci id="S3.SS2.p1.13.m5.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.2.3">𝑙</ci></apply><ci id="S3.SS2.p1.13.m5.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.13.m5.1.1.1.1.1.1.3">𝑥</ci></apply></apply><apply id="S3.SS2.p1.13.m5.2.2.2.cmml" xref="S3.SS2.p1.13.m5.2.2.2"><times id="S3.SS2.p1.13.m5.2.2.2.2.cmml" xref="S3.SS2.p1.13.m5.2.2.2.2"></times><ci id="S3.SS2.p1.13.m5.2.2.2.3a.cmml" xref="S3.SS2.p1.13.m5.2.2.2.3"><mtext id="S3.SS2.p1.13.m5.2.2.2.3.cmml" xref="S3.SS2.p1.13.m5.2.2.2.3">SoftMax</mtext></ci><apply id="S3.SS2.p1.13.m5.2.2.2.1.1.1.cmml" xref="S3.SS2.p1.13.m5.2.2.2.1.1"><times id="S3.SS2.p1.13.m5.2.2.2.1.1.1.2.cmml" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.2"></times><ci id="S3.SS2.p1.13.m5.2.2.2.1.1.1.3a.cmml" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.3"><mtext id="S3.SS2.p1.13.m5.2.2.2.1.1.1.3.cmml" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.3">TopK</mtext></ci><apply id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1"><times id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.1"></times><apply id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2.2">𝑊</ci><ci id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.2.3">𝑙</ci></apply><ci id="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.13.m5.2.2.2.1.1.1.1.1.1.3">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m5.2c">g(W_{l}x)=\text{SoftMax}(\text{TopK}(W_{l}x))</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.13.m5.2d">italic_g ( italic_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT italic_x ) = SoftMax ( TopK ( italic_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT italic_x ) )</annotation></semantics></math>, unless otherwise stated.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">For the self-attention sublayers, we combine the different domain experts by simply averaging their weights. The motivation behind this is the assumption that the self-attention layers are less domain specialized than the feedforward layers.
We do the same averaging for the remaining parameters (embeddings, etc.) as well.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.2">Note that the only new parameters we introduce are the router’s transformation parameters <math alttext="W_{l}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">W</mi><mi id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝑊</ci><ci id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">W_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math>, which are negligible in size compared to the rest of the network.
Nevertheless, those new parameters need to be finetuned, so the router can make optimal decisions in selecting which domain <math alttext="\mathtt{FF}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">𝙵𝙵</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝙵𝙵</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">\mathtt{FF}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">typewriter_FF start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to use.
In addition, funetuning is helpful because the self-attention weights are constructed by averaging, and are likely not optimal.
Overall, the entire system has not been optimized for working together at all in the embarrassingly parallel training framework, but our hypothesis is that even a small amount of combined finetuning might make large improvements.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Variations</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We also experimented with several variations of our method.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Load balancing</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.3">A common problem with MoE is the emergence of dead experts, which do not get activated by the router at all.
Common routing methods like Top-k are unlikely to escape from such a situation because a dead expert is never in the top-k selection, and therefore never receives a training signal.
Load balancing offers a simple solution by adding an extra loss term that encourages the experts to be utilized equally.
We use a loss term similar to <cite class="ltx_cite ltx_citemacro_citep">(Fedus et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib13" title="">2022</a>)</cite>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{LB}}=\alpha N\sum_{i=1}^{N}u_{i}p_{i}\quad\text{where}\ u_{%
i}=\frac{1}{|\mathcal{B}|}\sum_{x\in\mathcal{B}}g_{i}(W_{l}x)\ \text{and}\ p_{%
i}=\frac{1}{|\mathcal{B}|}\sum_{x\in\mathcal{B}}\text{SoftMax}_{i}(W_{l}x)." class="ltx_Math" display="block" id="S3.Ex2.m1.3"><semantics id="S3.Ex2.m1.3a"><mrow id="S3.Ex2.m1.3.3.1"><mrow id="S3.Ex2.m1.3.3.1.1.2" xref="S3.Ex2.m1.3.3.1.1.3.cmml"><mrow id="S3.Ex2.m1.3.3.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.1.1.cmml"><msub id="S3.Ex2.m1.3.3.1.1.1.1.2" xref="S3.Ex2.m1.3.3.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.3.3.1.1.1.1.2.2" xref="S3.Ex2.m1.3.3.1.1.1.1.2.2.cmml">ℒ</mi><mtext id="S3.Ex2.m1.3.3.1.1.1.1.2.3" xref="S3.Ex2.m1.3.3.1.1.1.1.2.3a.cmml">LB</mtext></msub><mo id="S3.Ex2.m1.3.3.1.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.1.1.1.cmml">=</mo><mrow id="S3.Ex2.m1.3.3.1.1.1.1.3" xref="S3.Ex2.m1.3.3.1.1.1.1.3.cmml"><mi id="S3.Ex2.m1.3.3.1.1.1.1.3.2" xref="S3.Ex2.m1.3.3.1.1.1.1.3.2.cmml">α</mi><mo id="S3.Ex2.m1.3.3.1.1.1.1.3.1" xref="S3.Ex2.m1.3.3.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.Ex2.m1.3.3.1.1.1.1.3.3" xref="S3.Ex2.m1.3.3.1.1.1.1.3.3.cmml">N</mi><mo id="S3.Ex2.m1.3.3.1.1.1.1.3.1a" xref="S3.Ex2.m1.3.3.1.1.1.1.3.1.cmml">⁢</mo><mrow id="S3.Ex2.m1.3.3.1.1.1.1.3.4" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.cmml"><munderover id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.cmml"><mo id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.2" movablelimits="false" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.2.cmml">∑</mo><mrow id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.cmml"><mi id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.2" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.2.cmml">i</mi><mo id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.1" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.1.cmml">=</mo><mn id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.3" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.3" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.3.cmml">N</mi></munderover><mrow id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.cmml"><msub id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2.cmml"><mi id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2.2" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2.2.cmml">u</mi><mi id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2.3" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.1" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.1.cmml">⁢</mo><msub id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3.cmml"><mi id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3.2" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3.2.cmml">p</mi><mi id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3.3" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow></mrow><mspace id="S3.Ex2.m1.3.3.1.1.2.3" width="1em" xref="S3.Ex2.m1.3.3.1.1.3a.cmml"></mspace><mrow id="S3.Ex2.m1.3.3.1.1.2.2" xref="S3.Ex2.m1.3.3.1.1.2.2.cmml"><mrow id="S3.Ex2.m1.3.3.1.1.2.2.4" xref="S3.Ex2.m1.3.3.1.1.2.2.4.cmml"><mtext id="S3.Ex2.m1.3.3.1.1.2.2.4.2" xref="S3.Ex2.m1.3.3.1.1.2.2.4.2a.cmml">where</mtext><mo id="S3.Ex2.m1.3.3.1.1.2.2.4.1" lspace="0.500em" xref="S3.Ex2.m1.3.3.1.1.2.2.4.1.cmml">⁢</mo><msub id="S3.Ex2.m1.3.3.1.1.2.2.4.3" xref="S3.Ex2.m1.3.3.1.1.2.2.4.3.cmml"><mi id="S3.Ex2.m1.3.3.1.1.2.2.4.3.2" xref="S3.Ex2.m1.3.3.1.1.2.2.4.3.2.cmml">u</mi><mi id="S3.Ex2.m1.3.3.1.1.2.2.4.3.3" xref="S3.Ex2.m1.3.3.1.1.2.2.4.3.3.cmml">i</mi></msub></mrow><mo id="S3.Ex2.m1.3.3.1.1.2.2.5" xref="S3.Ex2.m1.3.3.1.1.2.2.5.cmml">=</mo><mrow id="S3.Ex2.m1.3.3.1.1.2.2.1" xref="S3.Ex2.m1.3.3.1.1.2.2.1.cmml"><mfrac id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml"><mn id="S3.Ex2.m1.1.1.3" xref="S3.Ex2.m1.1.1.3.cmml">1</mn><mrow id="S3.Ex2.m1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.2.cmml"><mo id="S3.Ex2.m1.1.1.1.3.1" stretchy="false" xref="S3.Ex2.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.cmml">ℬ</mi><mo id="S3.Ex2.m1.1.1.1.3.2" stretchy="false" xref="S3.Ex2.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.Ex2.m1.3.3.1.1.2.2.1.2" xref="S3.Ex2.m1.3.3.1.1.2.2.1.2.cmml">⁢</mo><mrow id="S3.Ex2.m1.3.3.1.1.2.2.1.1" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.cmml"><munder id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.cmml"><mo id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.2" movablelimits="false" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.2.cmml">∑</mo><mrow id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.cmml"><mi id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.2" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.2.cmml">x</mi><mo id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.1" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.3" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.3.cmml">ℬ</mi></mrow></munder><mrow id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.cmml"><msub id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3.cmml"><mi id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3.2" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3.2.cmml">g</mi><mi id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3.3" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.2" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.cmml"><msub id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2.2" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2.2.cmml">W</mi><mi id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2.3" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2.3.cmml">l</mi></msub><mo id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.3" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.2a" lspace="0.500em" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.2.cmml">⁢</mo><mtext id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.4" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.4a.cmml">and</mtext><mo id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.2b" lspace="0.500em" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.2.cmml">⁢</mo><msub id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5.cmml"><mi id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5.2" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5.2.cmml">p</mi><mi id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5.3" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5.3.cmml">i</mi></msub></mrow></mrow></mrow><mo id="S3.Ex2.m1.3.3.1.1.2.2.6" xref="S3.Ex2.m1.3.3.1.1.2.2.6.cmml">=</mo><mrow id="S3.Ex2.m1.3.3.1.1.2.2.2" xref="S3.Ex2.m1.3.3.1.1.2.2.2.cmml"><mfrac id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml"><mn id="S3.Ex2.m1.2.2.3" xref="S3.Ex2.m1.2.2.3.cmml">1</mn><mrow id="S3.Ex2.m1.2.2.1.3" xref="S3.Ex2.m1.2.2.1.2.cmml"><mo id="S3.Ex2.m1.2.2.1.3.1" stretchy="false" xref="S3.Ex2.m1.2.2.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.2.2.1.1" xref="S3.Ex2.m1.2.2.1.1.cmml">ℬ</mi><mo id="S3.Ex2.m1.2.2.1.3.2" stretchy="false" xref="S3.Ex2.m1.2.2.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.Ex2.m1.3.3.1.1.2.2.2.2" xref="S3.Ex2.m1.3.3.1.1.2.2.2.2.cmml">⁢</mo><mrow id="S3.Ex2.m1.3.3.1.1.2.2.2.1" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.cmml"><munder id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.cmml"><mo id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.2" movablelimits="false" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.2.cmml">∑</mo><mrow id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.cmml"><mi id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.2" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.2.cmml">x</mi><mo id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.1" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.3" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.3.cmml">ℬ</mi></mrow></munder><mrow id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.cmml"><msub id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.cmml"><mtext id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.2" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.2a.cmml">SoftMax</mtext><mi id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.3" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.2" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.cmml"><mo id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.2" stretchy="false" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.cmml"><msub id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.cmml"><mi id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.2" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.2.cmml">W</mi><mi id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.3" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.3.cmml">l</mi></msub><mo id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.3" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.3" stretchy="false" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.Ex2.m1.3.3.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.3b"><apply id="S3.Ex2.m1.3.3.1.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.3a.cmml" xref="S3.Ex2.m1.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S3.Ex2.m1.3.3.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1"><eq id="S3.Ex2.m1.3.3.1.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1"></eq><apply id="S3.Ex2.m1.3.3.1.1.1.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.2.2">ℒ</ci><ci id="S3.Ex2.m1.3.3.1.1.1.1.2.3a.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.2.3"><mtext id="S3.Ex2.m1.3.3.1.1.1.1.2.3.cmml" mathsize="70%" xref="S3.Ex2.m1.3.3.1.1.1.1.2.3">LB</mtext></ci></apply><apply id="S3.Ex2.m1.3.3.1.1.1.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3"><times id="S3.Ex2.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.1"></times><ci id="S3.Ex2.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.2">𝛼</ci><ci id="S3.Ex2.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.3">𝑁</ci><apply id="S3.Ex2.m1.3.3.1.1.1.1.3.4.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4"><apply id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1">superscript</csymbol><apply id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1">subscript</csymbol><sum id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.2"></sum><apply id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3"><eq id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.1"></eq><ci id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.2">𝑖</ci><cn id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.3.cmml" type="integer" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.2.3.3">1</cn></apply></apply><ci id="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.1.3">𝑁</ci></apply><apply id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2"><times id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.1"></times><apply id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2">subscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2.2">𝑢</ci><ci id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.2.3">𝑖</ci></apply><apply id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3">subscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3.2">𝑝</ci><ci id="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.3.4.2.3.3">𝑖</ci></apply></apply></apply></apply></apply><apply id="S3.Ex2.m1.3.3.1.1.2.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2"><and id="S3.Ex2.m1.3.3.1.1.2.2a.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2"></and><apply id="S3.Ex2.m1.3.3.1.1.2.2b.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2"><eq id="S3.Ex2.m1.3.3.1.1.2.2.5.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.5"></eq><apply id="S3.Ex2.m1.3.3.1.1.2.2.4.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.4"><times id="S3.Ex2.m1.3.3.1.1.2.2.4.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.4.1"></times><ci id="S3.Ex2.m1.3.3.1.1.2.2.4.2a.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.4.2"><mtext id="S3.Ex2.m1.3.3.1.1.2.2.4.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.4.2">where</mtext></ci><apply id="S3.Ex2.m1.3.3.1.1.2.2.4.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.4.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.2.2.4.3.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.4.3">subscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.2.2.4.3.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.4.3.2">𝑢</ci><ci id="S3.Ex2.m1.3.3.1.1.2.2.4.3.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.4.3.3">𝑖</ci></apply></apply><apply id="S3.Ex2.m1.3.3.1.1.2.2.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1"><times id="S3.Ex2.m1.3.3.1.1.2.2.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.2"></times><apply id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1"><divide id="S3.Ex2.m1.1.1.2.cmml" xref="S3.Ex2.m1.1.1"></divide><cn id="S3.Ex2.m1.1.1.3.cmml" type="integer" xref="S3.Ex2.m1.1.1.3">1</cn><apply id="S3.Ex2.m1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.3"><abs id="S3.Ex2.m1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.1.1.1.3.1"></abs><ci id="S3.Ex2.m1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1">ℬ</ci></apply></apply><apply id="S3.Ex2.m1.3.3.1.1.2.2.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1"><apply id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2">subscript</csymbol><sum id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.2"></sum><apply id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3"><in id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.1"></in><ci id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.2">𝑥</ci><ci id="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.2.3.3">ℬ</ci></apply></apply><apply id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1"><times id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.2"></times><apply id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3.2">𝑔</ci><ci id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.3.3">𝑖</ci></apply><apply id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1"><times id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.1"></times><apply id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2.2">𝑊</ci><ci id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.2.3">𝑙</ci></apply><ci id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.1.1.1.3">𝑥</ci></apply><ci id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.4a.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.4"><mtext id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.4.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.4">and</mtext></ci><apply id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5">subscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5.2">𝑝</ci><ci id="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.1.1.1.5.3">𝑖</ci></apply></apply></apply></apply></apply><apply id="S3.Ex2.m1.3.3.1.1.2.2c.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2"><eq id="S3.Ex2.m1.3.3.1.1.2.2.6.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.6"></eq><share href="#S3.Ex2.m1.3.3.1.1.2.2.1.cmml" id="S3.Ex2.m1.3.3.1.1.2.2d.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2"></share><apply id="S3.Ex2.m1.3.3.1.1.2.2.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2"><times id="S3.Ex2.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.2"></times><apply id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2"><divide id="S3.Ex2.m1.2.2.2.cmml" xref="S3.Ex2.m1.2.2"></divide><cn id="S3.Ex2.m1.2.2.3.cmml" type="integer" xref="S3.Ex2.m1.2.2.3">1</cn><apply id="S3.Ex2.m1.2.2.1.2.cmml" xref="S3.Ex2.m1.2.2.1.3"><abs id="S3.Ex2.m1.2.2.1.2.1.cmml" xref="S3.Ex2.m1.2.2.1.3.1"></abs><ci id="S3.Ex2.m1.2.2.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1">ℬ</ci></apply></apply><apply id="S3.Ex2.m1.3.3.1.1.2.2.2.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1"><apply id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2">subscript</csymbol><sum id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.2"></sum><apply id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3"><in id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.1"></in><ci id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.2">𝑥</ci><ci id="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.2.3.3">ℬ</ci></apply></apply><apply id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1"><times id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.2"></times><apply id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3">subscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.2a.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.2"><mtext id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.2">SoftMax</mtext></ci><ci id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.3.3">𝑖</ci></apply><apply id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1"><times id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.1"></times><apply id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.2">𝑊</ci><ci id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.2.3">𝑙</ci></apply><ci id="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.2.2.2.1.1.1.1.1.3">𝑥</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.3c">\mathcal{L}_{\text{LB}}=\alpha N\sum_{i=1}^{N}u_{i}p_{i}\quad\text{where}\ u_{%
i}=\frac{1}{|\mathcal{B}|}\sum_{x\in\mathcal{B}}g_{i}(W_{l}x)\ \text{and}\ p_{%
i}=\frac{1}{|\mathcal{B}|}\sum_{x\in\mathcal{B}}\text{SoftMax}_{i}(W_{l}x).</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.3d">caligraphic_L start_POSTSUBSCRIPT LB end_POSTSUBSCRIPT = italic_α italic_N ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT where italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG | caligraphic_B | end_ARG ∑ start_POSTSUBSCRIPT italic_x ∈ caligraphic_B end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT italic_x ) and italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG | caligraphic_B | end_ARG ∑ start_POSTSUBSCRIPT italic_x ∈ caligraphic_B end_POSTSUBSCRIPT SoftMax start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT italic_x ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.2">Here <math alttext="\mathcal{B}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">ℬ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">ℬ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">\mathcal{B}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">caligraphic_B</annotation></semantics></math> is the current data batch, and <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.2.m2.1d">italic_α</annotation></semantics></math> is a hyperparameter. This loss is computed in each layer and added to the NLL loss.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Routing method</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">Besides Top-k routing, we also experiment with other routing methods:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Switch: It is a Top-1 routing method proposed by <cite class="ltx_cite ltx_citemacro_citet">Fedus et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib13" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Soft routing: We use softmax as the routing function <math alttext="g" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">italic_g</annotation></semantics></math>, so all experts are activated both during training and inference. While it is likely to provide the best performance, it comes at the expense of increased compute.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Sample Top-1: We use the gumbel softmax <cite class="ltx_cite ltx_citemacro_citep">(Jang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib20" title="">2016</a>)</cite> for <math alttext="g" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.m1.1"><semantics id="S3.I1.i3.p1.1.m1.1a"><mi id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><ci id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">g</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.m1.1d">italic_g</annotation></semantics></math>.
At training time, we generate a soft sample from the gumbel softmax, but zero out all its values except the largest one. Then we compute only one expert corresponding to this largest value, omitting the other expert computations. At inference time, we simply do hard sampling. We anneal the temperature to a sharp distribution at the end of training to gradually reduce the discrepancy between training and inference.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Splitting Experts</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.5">The number of modules in the MoE layer matches the number of domains we train on, since each module corresponds to one domain.
However, we can increase the number of modules in a simple way by splitting each domain FF sublayer into multiple chunks.
Given <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px3.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px3.p1.1.m1.1d">italic_N</annotation></semantics></math> domains and an FF activation size of <math alttext="d_{\text{FF}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px3.p1.2.m2.1a"><msub id="S3.SS3.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.2" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.2.cmml">d</mi><mtext id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3a.cmml">FF</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.2.m2.1b"><apply id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.2">𝑑</ci><ci id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3a.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3"><mtext id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.3">FF</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.2.m2.1c">d_{\text{FF}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px3.p1.2.m2.1d">italic_d start_POSTSUBSCRIPT FF end_POSTSUBSCRIPT</annotation></semantics></math>, we split each FF layer into <math alttext="C" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px3.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px3.p1.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.3.m3.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px3.p1.3.m3.1d">italic_C</annotation></semantics></math> chunks with a dimension of <math alttext="d_{\text{FF}}/C" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p1.4.m4.1"><semantics id="S3.SS3.SSS0.Px3.p1.4.m4.1a"><mrow id="S3.SS3.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.cmml"><msub id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.cmml"><mi id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.2" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.2.cmml">d</mi><mtext id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.3" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.3a.cmml">FF</mtext></msub><mo id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.1" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.1.cmml">/</mo><mi id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.3" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.4.m4.1b"><apply id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1"><divide id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.1"></divide><apply id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.1.cmml" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.2.cmml" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.2">𝑑</ci><ci id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.3a.cmml" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.3"><mtext id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.3.cmml" mathsize="70%" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.2.3">FF</mtext></ci></apply><ci id="S3.SS3.SSS0.Px3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p1.4.m4.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.4.m4.1c">d_{\text{FF}}/C</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px3.p1.4.m4.1d">italic_d start_POSTSUBSCRIPT FF end_POSTSUBSCRIPT / italic_C</annotation></semantics></math>.
As a result, the final MoE layer will have <math alttext="MC" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p1.5.m5.1"><semantics id="S3.SS3.SSS0.Px3.p1.5.m5.1a"><mrow id="S3.SS3.SSS0.Px3.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.cmml"><mi id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.2" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.2.cmml">M</mi><mo id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.3" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.5.m5.1b"><apply id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1"><times id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.1"></times><ci id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.2">𝑀</ci><ci id="S3.SS3.SSS0.Px3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p1.5.m5.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.5.m5.1c">MC</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px3.p1.5.m5.1d">italic_M italic_C</annotation></semantics></math> modules.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Blending Experts</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px4.p1.3">Instead of directly initializing MoE experts from domain experts in a one-to-one way, we also try including all domains in each MoE expert.
The motivation behind this is an observation that MoE experts trained in a standard way do not show domain specialization, but rather are activated uniformly across different domains <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib21" title="">2024</a>)</cite>.
In contrast, our domain experts are specialized to a specific domain through their training data.
To break this domain specialization, we split each domain expert’s FF layers into <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px4.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px4.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px4.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px4.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px4.p1.1.m1.1d">italic_N</annotation></semantics></math> chunks and then merge the <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px4.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px4.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px4.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px4.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px4.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px4.p1.2.m2.1d">italic_n</annotation></semantics></math>-th chunks from all domains to build the <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px4.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px4.p1.3.m3.1a"><mi id="S3.SS3.SSS0.Px4.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px4.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px4.p1.3.m3.1b"><ci id="S3.SS3.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px4.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px4.p1.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px4.p1.3.m3.1d">italic_n</annotation></semantics></math>-th MoE expert.
This way, each MoE expert contains the same amount of parameters from all domains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We base our experiments on the setup used for <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p1.1.1">Llama-2</span> pretraining <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib37" title="">2023</a>)</cite>.
In particular, we use the <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p1.1.2">Llama-2</span> 7B model as our seed model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>BTX Training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">We use the pretrained Llama-2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib37" title="">2023</a>)</cite> with 7B parameters as our seed model.
After making three copies of the seed model <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS1.p1.1.1">Llama-2 7B</span>, we continue training them on the following domain datasets to derive three domain experts:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Math:</span> The same data sources and mixture used in Llemma <cite class="ltx_cite ltx_citemacro_citep">(Azerbayev et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib5" title="">2023</a>)</cite> model training. To be comparable to Llemma, we train on the same amount of data as well, i.e. 48k steps with 201B tokens in total.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Code:</span> The same data sources and mixture of code data used in <span class="ltx_text ltx_font_smallcaps" id="S4.I1.i2.p1.1.2">CodeLlama</span> pretraining <cite class="ltx_cite ltx_citemacro_citep">(Rozière et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib31" title="">2023</a>)</cite>.
The code expert LLM is trained for 50k steps with 210B tokens in total to be comparable with the math expert.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Wikipedia:</span> Wikipedia documents extracted between June to August 2022. The data was preprocessed to remove hyperlinks, comments and other formatting boilerplate. Since this is a smaller dataset, we train a total of 42B tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.5">While we can proceed with only these three domain experts, we also include the original seed LLM as a “generalist” expert so that its general knowledge is transferred to the final model.
Thus we mix these four expert models into a single MoE model as described in <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S3.SS2" title="3.2 MiX: Combining Separate Experts to be a Mixture-of-Experts ‣ 3 Branch-Train-MiX ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">3.2</span></a>.
Then we finetune this MoE model on all the data sources used to train the four experts (including the original <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.SSS1.p3.5.1">Llama-2 7B</span> pretraining data for the generalist expert) and train for another 80B tokens.
The detailed sampling ratio across datasets in each domain as well as across the domains is described in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S8" title="8 Data mixture ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Section&nbsp;8</span></a>.
For BTX with default Top-2 routing, we use load balancing with <math alttext="\alpha=0.01" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.1.m1.1"><semantics id="S4.SS1.SSS1.p3.1.m1.1a"><mrow id="S4.SS1.SSS1.p3.1.m1.1.1" xref="S4.SS1.SSS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p3.1.m1.1.1.2" xref="S4.SS1.SSS1.p3.1.m1.1.1.2.cmml">α</mi><mo id="S4.SS1.SSS1.p3.1.m1.1.1.1" xref="S4.SS1.SSS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS1.p3.1.m1.1.1.3" xref="S4.SS1.SSS1.p3.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.1.m1.1b"><apply id="S4.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1"><eq id="S4.SS1.SSS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1.1"></eq><ci id="S4.SS1.SSS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1.2">𝛼</ci><cn id="S4.SS1.SSS1.p3.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.SSS1.p3.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.1.m1.1c">\alpha=0.01</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.1.m1.1d">italic_α = 0.01</annotation></semantics></math>, unless otherwise stated. For the Sample Top-1 routing, we use the temperature annealing schedule <math alttext="\tau" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.2.m2.1"><semantics id="S4.SS1.SSS1.p3.2.m2.1a"><mi id="S4.SS1.SSS1.p3.2.m2.1.1" xref="S4.SS1.SSS1.p3.2.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.2.m2.1b"><ci id="S4.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.2.m2.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.2.m2.1d">italic_τ</annotation></semantics></math>=max(<math alttext="0.5,-rt" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.3.m3.2"><semantics id="S4.SS1.SSS1.p3.3.m3.2a"><mrow id="S4.SS1.SSS1.p3.3.m3.2.2.1" xref="S4.SS1.SSS1.p3.3.m3.2.2.2.cmml"><mn id="S4.SS1.SSS1.p3.3.m3.1.1" xref="S4.SS1.SSS1.p3.3.m3.1.1.cmml">0.5</mn><mo id="S4.SS1.SSS1.p3.3.m3.2.2.1.2" xref="S4.SS1.SSS1.p3.3.m3.2.2.2.cmml">,</mo><mrow id="S4.SS1.SSS1.p3.3.m3.2.2.1.1" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1.cmml"><mo id="S4.SS1.SSS1.p3.3.m3.2.2.1.1a" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1.cmml">−</mo><mrow id="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.cmml"><mi id="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.2" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.2.cmml">r</mi><mo id="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.1" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.1.cmml">⁢</mo><mi id="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.3" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.3.cmml">t</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.3.m3.2b"><list id="S4.SS1.SSS1.p3.3.m3.2.2.2.cmml" xref="S4.SS1.SSS1.p3.3.m3.2.2.1"><cn id="S4.SS1.SSS1.p3.3.m3.1.1.cmml" type="float" xref="S4.SS1.SSS1.p3.3.m3.1.1">0.5</cn><apply id="S4.SS1.SSS1.p3.3.m3.2.2.1.1.cmml" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1"><minus id="S4.SS1.SSS1.p3.3.m3.2.2.1.1.1.cmml" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1"></minus><apply id="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.cmml" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2"><times id="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.1.cmml" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.1"></times><ci id="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.2.cmml" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.2">𝑟</ci><ci id="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.3.cmml" xref="S4.SS1.SSS1.p3.3.m3.2.2.1.1.2.3">𝑡</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.3.m3.2c">0.5,-rt</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.3.m3.2d">0.5 , - italic_r italic_t</annotation></semantics></math>) from <cite class="ltx_cite ltx_citemacro_cite">Jang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib20" title="">2016</a>)</cite> with <math alttext="r=1e-4" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.4.m4.1"><semantics id="S4.SS1.SSS1.p3.4.m4.1a"><mrow id="S4.SS1.SSS1.p3.4.m4.1.1" xref="S4.SS1.SSS1.p3.4.m4.1.1.cmml"><mi id="S4.SS1.SSS1.p3.4.m4.1.1.2" xref="S4.SS1.SSS1.p3.4.m4.1.1.2.cmml">r</mi><mo id="S4.SS1.SSS1.p3.4.m4.1.1.1" xref="S4.SS1.SSS1.p3.4.m4.1.1.1.cmml">=</mo><mrow id="S4.SS1.SSS1.p3.4.m4.1.1.3" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.cmml"><mrow id="S4.SS1.SSS1.p3.4.m4.1.1.3.2" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.2.cmml"><mn id="S4.SS1.SSS1.p3.4.m4.1.1.3.2.2" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.2.2.cmml">1</mn><mo id="S4.SS1.SSS1.p3.4.m4.1.1.3.2.1" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.2.1.cmml">⁢</mo><mi id="S4.SS1.SSS1.p3.4.m4.1.1.3.2.3" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.2.3.cmml">e</mi></mrow><mo id="S4.SS1.SSS1.p3.4.m4.1.1.3.1" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.1.cmml">−</mo><mn id="S4.SS1.SSS1.p3.4.m4.1.1.3.3" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.4.m4.1b"><apply id="S4.SS1.SSS1.p3.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1"><eq id="S4.SS1.SSS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.1"></eq><ci id="S4.SS1.SSS1.p3.4.m4.1.1.2.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.2">𝑟</ci><apply id="S4.SS1.SSS1.p3.4.m4.1.1.3.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.3"><minus id="S4.SS1.SSS1.p3.4.m4.1.1.3.1.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.1"></minus><apply id="S4.SS1.SSS1.p3.4.m4.1.1.3.2.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.2"><times id="S4.SS1.SSS1.p3.4.m4.1.1.3.2.1.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.2.1"></times><cn id="S4.SS1.SSS1.p3.4.m4.1.1.3.2.2.cmml" type="integer" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.2.2">1</cn><ci id="S4.SS1.SSS1.p3.4.m4.1.1.3.2.3.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.2.3">𝑒</ci></apply><cn id="S4.SS1.SSS1.p3.4.m4.1.1.3.3.cmml" type="integer" xref="S4.SS1.SSS1.p3.4.m4.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.4.m4.1c">r=1e-4</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.4.m4.1d">italic_r = 1 italic_e - 4</annotation></semantics></math> where <math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p3.5.m5.1"><semantics id="S4.SS1.SSS1.p3.5.m5.1a"><mi id="S4.SS1.SSS1.p3.5.m5.1.1" xref="S4.SS1.SSS1.p3.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.5.m5.1b"><ci id="S4.SS1.SSS1.p3.5.m5.1.1.cmml" xref="S4.SS1.SSS1.p3.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.5.m5.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p3.5.m5.1d">italic_t</annotation></semantics></math> is the number of training steps.
For the first layer only, we used soft-routing instead.
Since the Sample Top-1 training is more efficient than Top-2, with the same compute budget it can train 160B tokens.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.2">
<tbody><tr class="ltx_tr" id="S4.T1.2.1">
<td class="ltx_td ltx_border_tt" id="S4.T1.2.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.2.1" style="font-size:90%;">Math</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.3.1" style="font-size:90%;">Code</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T1.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.1.4.1" style="font-size:90%;">General knowledge</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2">
<td class="ltx_td" id="S4.T1.2.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.1" style="font-size:90%;">GSM8K</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.3.1" style="font-size:90%;">MATH</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.4.1" style="font-size:90%;">Human</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.5.1" style="font-size:90%;">MBPP</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.6.1" style="font-size:90%;">Natural</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.7.1" style="font-size:90%;">Trivia</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.8"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.8.1" style="font-size:90%;">MMLU</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.3">
<td class="ltx_td" id="S4.T1.2.3.1"></td>
<td class="ltx_td" id="S4.T1.2.3.2"></td>
<td class="ltx_td" id="S4.T1.2.3.3"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.3.4.1" style="font-size:90%;">Eval</span></td>
<td class="ltx_td" id="S4.T1.2.3.5"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.6"><span class="ltx_text ltx_font_bold" id="S4.T1.2.3.6.1" style="font-size:90%;">Questions</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.7"><span class="ltx_text ltx_font_bold" id="S4.T1.2.3.7.1" style="font-size:90%;">QA</span></td>
<td class="ltx_td" id="S4.T1.2.3.8"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.4.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.4.1.1" style="font-size:90%;">Llama-2 7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.2"><span class="ltx_text" id="S4.T1.2.4.2.1" style="font-size:90%;">14.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.3"><span class="ltx_text" id="S4.T1.2.4.3.1" style="font-size:90%;">2.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.4"><span class="ltx_text" id="S4.T1.2.4.4.1" style="font-size:90%;">12.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.5"><span class="ltx_text" id="S4.T1.2.4.5.1" style="font-size:90%;">20.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.6"><span class="ltx_text" id="S4.T1.2.4.6.1" style="font-size:90%;">16.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.7"><span class="ltx_text ltx_font_bold" id="S4.T1.2.4.7.1" style="font-size:90%;">58.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.4.8"><span class="ltx_text" id="S4.T1.2.4.8.1" style="font-size:90%;">46.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.5">
<td class="ltx_td ltx_align_left" id="S4.T1.2.5.1"><span class="ltx_text" id="S4.T1.2.5.1.1" style="font-size:90%;">Math expert</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.2"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.2.1" style="font-size:90%;">39.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.3.1" style="font-size:90%;">18.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.4"><span class="ltx_text" id="S4.T1.2.5.4.1" style="font-size:90%;">25.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.5"><span class="ltx_text" id="S4.T1.2.5.5.1" style="font-size:90%;">33.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.6"><span class="ltx_text" id="S4.T1.2.5.6.1" style="font-size:90%;">14.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.7"><span class="ltx_text" id="S4.T1.2.5.7.1" style="font-size:90%;">37.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.5.8"><span class="ltx_text ltx_font_bold" id="S4.T1.2.5.8.1" style="font-size:90%;">52.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.6">
<td class="ltx_td ltx_align_left" id="S4.T1.2.6.1"><span class="ltx_text" id="S4.T1.2.6.1.1" style="font-size:90%;">Code expert</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.2"><span class="ltx_text" id="S4.T1.2.6.2.1" style="font-size:90%;">12.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.3"><span class="ltx_text" id="S4.T1.2.6.3.1" style="font-size:90%;">4.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.4"><span class="ltx_text ltx_font_bold" id="S4.T1.2.6.4.1" style="font-size:90%;">31.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.5"><span class="ltx_text ltx_font_bold" id="S4.T1.2.6.5.1" style="font-size:90%;">40.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.6"><span class="ltx_text" id="S4.T1.2.6.6.1" style="font-size:90%;">11.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.7"><span class="ltx_text" id="S4.T1.2.6.7.1" style="font-size:90%;">29.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.6.8"><span class="ltx_text" id="S4.T1.2.6.8.1" style="font-size:90%;">39.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.7">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.2.7.1"><span class="ltx_text" id="S4.T1.2.7.1.1" style="font-size:90%;">Wikipedia expert</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.7.2"><span class="ltx_text" id="S4.T1.2.7.2.1" style="font-size:90%;">11.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.7.3"><span class="ltx_text" id="S4.T1.2.7.3.1" style="font-size:90%;">3.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.7.4"><span class="ltx_text" id="S4.T1.2.7.4.1" style="font-size:90%;">11.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.7.5"><span class="ltx_text" id="S4.T1.2.7.5.1" style="font-size:90%;">15.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.7.6"><span class="ltx_text ltx_font_bold" id="S4.T1.2.7.6.1" style="font-size:90%;">21.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.7.7"><span class="ltx_text" id="S4.T1.2.7.7.1" style="font-size:90%;">57.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.7.8"><span class="ltx_text" id="S4.T1.2.7.8.1" style="font-size:90%;">43.1</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Individual domain expert LLM performance on representative tasks, compared to the seed model <span class="ltx_text ltx_font_smallcaps" id="S4.T1.11.1">Llama-2 7B</span>. As expected, the code and math experts excel at their corresponding domain tasks.
The Wikipedia expert performs better on Natural Questions, but the math expert has the best score on MMLU.
This could be because MMLU contains many math subjects and math training is shown to help on this task <cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib35" title="">2024</a>)</cite>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Baselines</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">We compare to the following baselines:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.I2.i1.p1.1.1">Llama-2</span><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.2">:</span> We compare to the original <span class="ltx_text ltx_font_smallcaps" id="S4.I2.i1.p1.1.3">Llama-2 7B</span> that we use as a seed model, as well as <span class="ltx_text ltx_font_smallcaps" id="S4.I2.i1.p1.1.4">Llama-2 13B</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Dense:</span> Instead of training separate LLMs on different domain datasets, the dense baseline continues to train the seed LLM with all the data.
We use exactly the same training data as BTX, first training on the new domain-specific data used in the experts training stage, followed by the same data mixture that includes the <span class="ltx_text ltx_font_smallcaps" id="S4.I2.i2.p1.1.2">Llama-2</span> pretraining data in the MoE finetuning stage.
We call this comparison <em class="ltx_emph ltx_font_italic" id="S4.I2.i2.p1.1.3">data-matching</em> (DM).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">Sparse upcycling:</span>
This baseline <cite class="ltx_cite ltx_citemacro_citep">(Komatsuzaki et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib23" title="">2022</a>)</cite>
initializes a MoE model from the seed model by making 4 identical copies of the feedforward module as experts. We use the Top-2 router with randomly initialized <math alttext="W_{i}" class="ltx_Math" display="inline" id="S4.I2.i3.p1.1.m1.1"><semantics id="S4.I2.i3.p1.1.m1.1a"><msub id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml"><mi id="S4.I2.i3.p1.1.m1.1.1.2" xref="S4.I2.i3.p1.1.m1.1.1.2.cmml">W</mi><mi id="S4.I2.i3.p1.1.m1.1.1.3" xref="S4.I2.i3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b"><apply id="S4.I2.i3.p1.1.m1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.I2.i3.p1.1.m1.1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.I2.i3.p1.1.m1.1.1.2.cmml" xref="S4.I2.i3.p1.1.m1.1.1.2">𝑊</ci><ci id="S4.I2.i3.p1.1.m1.1.1.3.cmml" xref="S4.I2.i3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">W_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.1.m1.1d">italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> parameters.
In addition to training a data matching baseline with the same data as is used in BTX and the dense baseline, we also train a sparse upcycling baseline with the same amount of GPU-days, i.e. compute-matching (CM), using the MoE finetuning data mixture throughout training. This is equivalent to a special case of BTX which does not contain embarrassingly parallel expert training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i4.p1.1.1">Branch-Train-Merge (BTM):</span> This baseline <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib27" title="">2022a</a>)</cite> uses the same expert LLMs as BTX (including the original seed model) but uses them directly without building a MoE model. For a given context (input), it selects Top-k expert LLMs based on the similarity between the context and experts’ training data. Following the efficient inference method used in <cite class="ltx_cite ltx_citemacro_citet">Gururangan et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib16" title="">2023</a>)</cite>, both context and experts’ training data are embedded via tf-idf. Top-k experts are selected based on cosine similarity to the mean tf-idf embedding of each expert.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p" id="S4.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i5.p1.1.1">CodeLlama 7B:</span> A language model specializing in code <cite class="ltx_cite ltx_citemacro_citep">(Rozière et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib31" title="">2023</a>)</cite> by continued training of the same seed model <span class="ltx_text ltx_font_smallcaps" id="S4.I2.i5.p1.1.2">Llama-2 7B</span> on code data. It also has other features such as long-context and infilling.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i6.p1">
<p class="ltx_p" id="S4.I2.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i6.p1.1.1">Llemma 7B:</span> A language model specializing in mathematics <cite class="ltx_cite ltx_citemacro_citep">(Azerbayev et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib5" title="">2023</a>)</cite> by continued training of CodeLlama 7B on math data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.2">We use the same optimization hyperparameters for training of the baselines, expert models and MoE models. We use the AdamW optimizer with weight decay 0.1, and anneal the learning rate to the peak of <math alttext="1e-4" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p3.1.m1.1"><semantics id="S4.SS1.SSS2.p3.1.m1.1a"><mrow id="S4.SS1.SSS2.p3.1.m1.1.1" xref="S4.SS1.SSS2.p3.1.m1.1.1.cmml"><mrow id="S4.SS1.SSS2.p3.1.m1.1.1.2" xref="S4.SS1.SSS2.p3.1.m1.1.1.2.cmml"><mn id="S4.SS1.SSS2.p3.1.m1.1.1.2.2" xref="S4.SS1.SSS2.p3.1.m1.1.1.2.2.cmml">1</mn><mo id="S4.SS1.SSS2.p3.1.m1.1.1.2.1" xref="S4.SS1.SSS2.p3.1.m1.1.1.2.1.cmml">⁢</mo><mi id="S4.SS1.SSS2.p3.1.m1.1.1.2.3" xref="S4.SS1.SSS2.p3.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S4.SS1.SSS2.p3.1.m1.1.1.1" xref="S4.SS1.SSS2.p3.1.m1.1.1.1.cmml">−</mo><mn id="S4.SS1.SSS2.p3.1.m1.1.1.3" xref="S4.SS1.SSS2.p3.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.1.m1.1b"><apply id="S4.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1"><minus id="S4.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.1"></minus><apply id="S4.SS1.SSS2.p3.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.2"><times id="S4.SS1.SSS2.p3.1.m1.1.1.2.1.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.2.1"></times><cn id="S4.SS1.SSS2.p3.1.m1.1.1.2.2.cmml" type="integer" xref="S4.SS1.SSS2.p3.1.m1.1.1.2.2">1</cn><ci id="S4.SS1.SSS2.p3.1.m1.1.1.2.3.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.2.3">𝑒</ci></apply><cn id="S4.SS1.SSS2.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS2.p3.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.1.m1.1c">1e-4</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p3.1.m1.1d">1 italic_e - 4</annotation></semantics></math> with 100 steps of warmup, and decay to <math alttext="10\%" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p3.2.m2.1"><semantics id="S4.SS1.SSS2.p3.2.m2.1a"><mrow id="S4.SS1.SSS2.p3.2.m2.1.1" xref="S4.SS1.SSS2.p3.2.m2.1.1.cmml"><mn id="S4.SS1.SSS2.p3.2.m2.1.1.2" xref="S4.SS1.SSS2.p3.2.m2.1.1.2.cmml">10</mn><mo id="S4.SS1.SSS2.p3.2.m2.1.1.1" xref="S4.SS1.SSS2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.2.m2.1b"><apply id="S4.SS1.SSS2.p3.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS1.SSS2.p3.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p3.2.m2.1.1.1">percent</csymbol><cn id="S4.SS1.SSS2.p3.2.m2.1.1.2.cmml" type="integer" xref="S4.SS1.SSS2.p3.2.m2.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.2.m2.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p3.2.m2.1d">10 %</annotation></semantics></math> of the peak with a cosine schedule. We use a batch size of 4M tokens with a sequence length of 4096.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.2">
<tbody><tr class="ltx_tr" id="S4.T2.2.1">
<td class="ltx_td ltx_border_tt" id="S4.T2.2.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.2.1" style="font-size:90%;">Math</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.3.1" style="font-size:90%;">Code</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.4.1" style="font-size:90%;">Knowledge</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.5.1" style="font-size:90%;">Reasoning</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.1.6"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.6.1" style="font-size:90%;">MMLU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.1.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.7.1" style="font-size:90%;">Average</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.2.2.1"><span class="ltx_text ltx_font_italic" id="S4.T2.2.2.1.1" style="font-size:80%;">Specialized LLMs</span></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.2.2.2"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.2.2.3"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.2.2.4"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.2.2.5"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.2.2.6"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.2.2.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3">
<td class="ltx_td ltx_align_left" id="S4.T2.2.3.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.3.1.1" style="font-size:90%;">CodeLlama 7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.3.2">
<span class="ltx_text ltx_phantom" id="S4.T2.2.3.2.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T2.2.3.2.2" style="font-size:90%;">8.1</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.3.3"><span class="ltx_text" id="S4.T2.2.3.3.1" style="font-size:90%;">36.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.3.4"><span class="ltx_text" id="S4.T2.2.3.4.1" style="font-size:90%;">22.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.3.5"><span class="ltx_text" id="S4.T2.2.3.5.1" style="font-size:90%;">56.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.3.6"><span class="ltx_text" id="S4.T2.2.3.6.1" style="font-size:90%;">38.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.3.7"><span class="ltx_text" id="S4.T2.2.3.7.1" style="font-size:90%;">37.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4">
<td class="ltx_td ltx_align_left" id="S4.T2.2.4.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.4.1.1" style="font-size:90%;">Llemma 7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.2"><span class="ltx_text" id="S4.T2.2.4.2.1" style="font-size:90%;">28.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.3"><span class="ltx_text" id="S4.T2.2.4.3.1" style="font-size:90%;">33.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.4"><span class="ltx_text" id="S4.T2.2.4.4.1" style="font-size:90%;">17.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.5"><span class="ltx_text" id="S4.T2.2.4.5.1" style="font-size:90%;">38.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.6"><span class="ltx_text" id="S4.T2.2.4.6.1" style="font-size:90%;">33.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.4.7"><span class="ltx_text" id="S4.T2.2.4.7.1" style="font-size:90%;">32.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.5.1"><span class="ltx_text ltx_font_italic" id="S4.T2.2.5.1.1" style="font-size:80%;">Generalist LLMs</span></td>
<td class="ltx_td ltx_border_t" id="S4.T2.2.5.2"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.2.5.3"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.2.5.4"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.2.5.5"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.2.5.6"></td>
<td class="ltx_td ltx_border_t" id="S4.T2.2.5.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6">
<td class="ltx_td ltx_align_left" id="S4.T2.2.6.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.6.1.1" style="font-size:90%;">Llama-2 7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.2">
<span class="ltx_text ltx_phantom" id="S4.T2.2.6.2.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T2.2.6.2.2" style="font-size:90%;">8.6</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.3"><span class="ltx_text" id="S4.T2.2.6.3.1" style="font-size:90%;">16.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.4"><span class="ltx_text" id="S4.T2.2.6.4.1" style="font-size:90%;">37.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.5"><span class="ltx_text" id="S4.T2.2.6.5.1" style="font-size:90%;">63.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.6"><span class="ltx_text" id="S4.T2.2.6.6.1" style="font-size:90%;">46.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.6.7"><span class="ltx_text" id="S4.T2.2.6.7.1" style="font-size:90%;">40.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.7">
<td class="ltx_td ltx_align_left" id="S4.T2.2.7.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.7.1.1" style="font-size:90%;">Llama-2 13B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.7.2"><span class="ltx_text" id="S4.T2.2.7.2.1" style="font-size:90%;">16.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.7.3"><span class="ltx_text" id="S4.T2.2.7.3.1" style="font-size:90%;">24.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.7.4"><span class="ltx_text" id="S4.T2.2.7.4.1" style="font-size:90%;">40.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.7.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.7.5.1" style="font-size:90%;">66.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.7.6"><span class="ltx_text" id="S4.T2.2.7.6.1" style="font-size:90%;">52.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.7.7"><span class="ltx_text" id="S4.T2.2.7.7.1" style="font-size:90%;">45.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.8">
<td class="ltx_td ltx_align_left" id="S4.T2.2.8.1"><span class="ltx_text" id="S4.T2.2.8.1.1" style="font-size:90%;">Dense (DM)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.2"><span class="ltx_text" id="S4.T2.2.8.2.1" style="font-size:90%;">18.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.3"><span class="ltx_text" id="S4.T2.2.8.3.1" style="font-size:90%;">25.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.4"><span class="ltx_text" id="S4.T2.2.8.4.1" style="font-size:90%;">39.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.5"><span class="ltx_text" id="S4.T2.2.8.5.1" style="font-size:90%;">63.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.6"><span class="ltx_text" id="S4.T2.2.8.6.1" style="font-size:90%;">49.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.8.7"><span class="ltx_text" id="S4.T2.2.8.7.1" style="font-size:90%;">44.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.9">
<td class="ltx_td ltx_align_left" id="S4.T2.2.9.1"><span class="ltx_text" id="S4.T2.2.9.1.1" style="font-size:90%;">Sparse upcycling (DM), Top-2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.9.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.9.2.1" style="font-size:90%;">28.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.9.3"><span class="ltx_text" id="S4.T2.2.9.3.1" style="font-size:90%;">34.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.9.4"><span class="ltx_text" id="S4.T2.2.9.4.1" style="font-size:90%;">34.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.9.5"><span class="ltx_text" id="S4.T2.2.9.5.1" style="font-size:90%;">62.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.9.6"><span class="ltx_text" id="S4.T2.2.9.6.1" style="font-size:90%;">51.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.9.7"><span class="ltx_text" id="S4.T2.2.9.7.1" style="font-size:90%;">46.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.10">
<td class="ltx_td ltx_align_left" id="S4.T2.2.10.1"><span class="ltx_text" id="S4.T2.2.10.1.1" style="font-size:90%;">BTM, Top-1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.2"><span class="ltx_text" id="S4.T2.2.10.2.1" style="font-size:90%;">21.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.3"><span class="ltx_text" id="S4.T2.2.10.3.1" style="font-size:90%;">36.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.4"><span class="ltx_text" id="S4.T2.2.10.4.1" style="font-size:90%;">26.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.5"><span class="ltx_text" id="S4.T2.2.10.5.1" style="font-size:90%;">61.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.6"><span class="ltx_text" id="S4.T2.2.10.6.1" style="font-size:90%;">44.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.10.7"><span class="ltx_text" id="S4.T2.2.10.7.1" style="font-size:90%;">43.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.11">
<td class="ltx_td ltx_align_left" id="S4.T2.2.11.1"><span class="ltx_text" id="S4.T2.2.11.1.1" style="font-size:90%;">
BTM, Top-2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.11.2"><span class="ltx_text" id="S4.T2.2.11.2.1" style="font-size:90%;">21.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.11.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.11.3.1" style="font-size:90%;">36.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.11.4"><span class="ltx_text" id="S4.T2.2.11.4.1" style="font-size:90%;">26.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.11.5"><span class="ltx_text" id="S4.T2.2.11.5.1" style="font-size:90%;">61.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.11.6"><span class="ltx_text" id="S4.T2.2.11.6.1" style="font-size:90%;">44.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.11.7"><span class="ltx_text" id="S4.T2.2.11.7.1" style="font-size:90%;">43.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.12">
<td class="ltx_td ltx_align_left" id="S4.T2.2.12.1"><span class="ltx_text" id="S4.T2.2.12.1.1" style="font-size:90%;">BTX, Sample Top-1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.12.2"><span class="ltx_text" id="S4.T2.2.12.2.1" style="font-size:90%;">26.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.12.3"><span class="ltx_text" id="S4.T2.2.12.3.1" style="font-size:90%;">31.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.12.4"><span class="ltx_text" id="S4.T2.2.12.4.1" style="font-size:90%;">40.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.12.5"><span class="ltx_text" id="S4.T2.2.12.5.1" style="font-size:90%;">63.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.12.6"><span class="ltx_text ltx_font_bold" id="S4.T2.2.12.6.1" style="font-size:90%;">53.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.12.7"><span class="ltx_text" id="S4.T2.2.12.7.1" style="font-size:90%;">47.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.13">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.2.13.1"><span class="ltx_text" id="S4.T2.2.13.1.1" style="font-size:90%;">BTX, Top-2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.13.2"><span class="ltx_text" id="S4.T2.2.13.2.1" style="font-size:90%;">27.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.13.3"><span class="ltx_text" id="S4.T2.2.13.3.1" style="font-size:90%;">34.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.13.4"><span class="ltx_text ltx_font_bold" id="S4.T2.2.13.4.1" style="font-size:90%;">41.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.13.5"><span class="ltx_text" id="S4.T2.2.13.5.1" style="font-size:90%;">63.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.13.6"><span class="ltx_text" id="S4.T2.2.13.6.1" style="font-size:90%;">52.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.13.7"><span class="ltx_text ltx_font_bold" id="S4.T2.2.13.7.1" style="font-size:90%;">47.9</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Aggregated performance of BTX compared against various baselines, including both generalist and specialized pretrained models, tested on various capabilities aggregated across popular benchmarks. Dense, sparse upcycling, BTM and BTX are trained on exactly the same amount and mixture of data with the exception that BTM does not have the finetuning stage.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">For evaluation, we use the zero- and few-shot performance on multiple benchmarks that test different skills:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1">Math: we report the average performance on GSM8K (8 shot) <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib10" title="">2021</a>)</cite> and MATH (4 shot) <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib18" title="">2021b</a>)</cite> for math reasoning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1">Code: we report the average performance of HumanEval (0 shot) <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib8" title="">2021</a>)</cite> and MBPP (3 shot) <cite class="ltx_cite ltx_citemacro_citep">(Austin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib3" title="">2021</a>)</cite> for code generation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i3.p1">
<p class="ltx_p" id="S4.I3.i3.p1.1">World knowledge: we report the average performance of Natural Questions (5 shot)<cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib24" title="">2019</a>)</cite> and TriviaQA (5 shot) <cite class="ltx_cite ltx_citemacro_citep">(Joshi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib22" title="">2017</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i4.p1">
<p class="ltx_p" id="S4.I3.i4.p1.1">Reasoning: we report the average 0-shot performance of ARC-Easy and ARC-Challenge <cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib9" title="">2018</a>)</cite>, SIQA <cite class="ltx_cite ltx_citemacro_citep">(Sap et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib34" title="">2019</a>)</cite>, PIQA <cite class="ltx_cite ltx_citemacro_citep">(Bisk et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib6" title="">2020</a>)</cite> and WinoGrande <cite class="ltx_cite ltx_citemacro_citep">(Sakaguchi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib33" title="">2021</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i5.p1">
<p class="ltx_p" id="S4.I3.i5.p1.1">General: we report performance on MMLU (5 shot) <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib17" title="">2021a</a>)</cite> which covers multiple domains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_landscape" height="373" id="S4.F2.g1" src="x2.png" width="491"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_2 ltx_img_square" height="342" id="S4.F2.g2" src="x3.png" width="327"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F2.3.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S4.F2.3.2.1">Left:</span> The average performance vs training budget of BTX compared to various baselines, with different active parameters at inference time indicated by circle size. All the models except <span class="ltx_text ltx_font_smallcaps" id="S4.F2.3.2.2">Llama-2</span> 13B are trained starting from <span class="ltx_text ltx_font_smallcaps" id="S4.F2.3.2.3">Llama-2</span> 7B using the datasets described in <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS1.SSS1" title="4.1.1 BTX Training ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.1.1</span></a>. The X-axis shows the total training compute starting from the seed model measured in GPU days<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="footnote1.1.1.1" style="font-size:111%;">1</span></span>The GPU days of Llama-2 13B is an approximate measurement, calculated by doubling the training compute of a 7B model trained with the same amount of pretraining data (according to <cite class="ltx_cite ltx_citemacro_citet">Touvron et&nbsp;al. <span class="ltx_text" id="footnote1.9.1.1.1" style="font-size:111%;">(</span><a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib37" title="">2023</a><span class="ltx_text" id="footnote1.10.2.2.1" style="font-size:111%;">)</span></cite> Table 2). Since Llama-2 13B is not trained from the seed model, we simply report their difference in GPU days.
</span></span></span>, and the Y-axis is the average score over all the tasks (as computed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S4.T2" title="Table 2 ‣ 4.1.2 Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Table&nbsp;2</span></a>). The BTX models outperform the baselines that started from the same seed model, as well as <span class="ltx_text ltx_font_smallcaps" id="S4.F2.3.2.4">Llama-2 13B</span>.
<span class="ltx_text ltx_font_bold" id="S4.F2.3.2.5">Right:</span> The normalized performance over different domains where the scores are divided by the highest one. We see large improvements for BTX in code (which matches the specialized model) and math tasks compared to the seed model <span class="ltx_text ltx_font_smallcaps" id="S4.F2.3.2.6">Llama-2 7B</span>, even outperforming the <span class="ltx_text ltx_font_smallcaps" id="S4.F2.3.2.7">Llama-2 13B</span> model.
</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Overall Performance</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Domain experts excel at their respective tasks.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px1.p1.1">We first analyze how
expert LLMs specialize to specific domains. Results are summarized in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S4.T1" title="Table 1 ‣ 4.1.1 BTX Training ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Table&nbsp;1</span></a>.
As expected, individual expert LLMs achieve the best performance in their respective domain, where the math and code domains see especially large improvements.
In addition, there are several interesting observations.
We see that the math expert training improved its code performance as well, indicating a close relation of these domains.
However, such single-domain continued training also suffers from catastrophic forgetting with significant performance drops on some tasks in other domains. For example, the math and code expert are much worse on TriviaQA than the seed model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">BTX improves all tasks where experts specialize.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS1.Px2.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S4.T2" title="Table 2 ‣ 4.1.2 Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Table&nbsp;2</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#footnote1" title="footnote 1 ‣ Figure 2 ‣ 4.1.3 Evaluation ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag"><span class="ltx_text" style="font-size:111%;">footnote&nbsp;1</span></span></a> (right) show aggregated performance across multiple domains. More detailed per-task results are reported in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S9.T8" title="Table 8 ‣ 9 Evaluation ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Table&nbsp;8</span></a> in the Appendix.
Compared to the seed model <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS1.Px2.p1.1.1">Llama-2 7B</span>, BTX models (both Sample Top-1 and Top-2 corresponding to different number of active parameters) improve on all expert domains, such as math, coding and world knowledge without regressing on other tasks such as commonsense reasoning.
BTX with Top-2 experts (our default) also approaches the best performance of the specialized models <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS1.Px2.p1.1.2">Llemma 7B</span> and <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS1.Px2.p1.1.3">CodeLlama 7B</span> in the math and coding domains, while drastically improving over those models on domains that are not their speciality such as world knowledge and commonsense reasoning.
Compared to alternative data-matching (DM) methods for continued pretraining such as dense and sparse upcycling, BTX achieves better performance on average with small gaps in the math and coding domains. BTX outperforms BTM by a large margin on average,
indicating that MoE finetuning to learn token-level routing is beneficial. Overall, the results demonstrate that BTX is a more compute efficient method for continued pretraining which is robust to task interference from multi-task learning.
BTX also outperforms
<span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS1.Px2.p1.1.4">Llama-2 13B</span> on all tasks except
reasoning, even though <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS1.Px2.p1.1.5">Llama-2 13B</span>
uses significantly more training compute and has slightly more active parameters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.2">
<tbody><tr class="ltx_tr" id="S4.T3.2.1">
<td class="ltx_td ltx_border_tt" id="S4.T3.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.1.2.1" style="font-size:80%;">MoE</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.1.3.1" style="font-size:80%;">Training</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.1.4.1" style="font-size:80%;">Total compute</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.1.5.1" style="font-size:80%;">#tokens</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.1.6.1" style="font-size:80%;">Math</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.1.7.1" style="font-size:80%;">Code</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.1.8.1" style="font-size:80%;">Knowledge</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.1.9.1" style="font-size:80%;">Reasoning</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.1.10.1" style="font-size:80%;">MMLU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.1.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.1.11.1" style="font-size:80%;">Average</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2">
<td class="ltx_td" id="S4.T3.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.2.2.1" style="font-size:80%;">compute</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.2.3.1" style="font-size:80%;">time (days)</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.2.4.1" style="font-size:80%;">(GPU-days)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.2.5.1" style="font-size:80%;">(B)</span></td>
<td class="ltx_td" id="S4.T3.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S4.T3.2.2.7" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S4.T3.2.2.8" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S4.T3.2.2.9" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S4.T3.2.2.10" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td" id="S4.T3.2.2.11" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.3.1.1" style="font-size:80%;">BTX</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_phantom" id="S4.T3.2.3.2.1" style="font-size:80%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T3.2.3.2.2" style="font-size:80%;">23%</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.3.3.1" style="font-size:80%;">7.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_phantom" id="S4.T3.2.3.4.1" style="font-size:80%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T3.2.3.4.2" style="font-size:80%;">926.1</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.3.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.3.5.1" style="font-size:80%;">533</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.3.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.3.6.1" style="font-size:80%;">27.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.3.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.3.7.1" style="font-size:80%;">34.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.3.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.3.8.1" style="font-size:80%;">41.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.3.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.3.9.1" style="font-size:80%;">63.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.3.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.3.10.1" style="font-size:80%;">52.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.3.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.3.11.1" style="font-size:80%;">47.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.2.4.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.4.1.1" style="font-size:80%;">Sparse upcycling (CM)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.4.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.4.2.1" style="font-size:80%;">100%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.4.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.4.3.1" style="font-size:80%;">7.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.4.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.4.4.1" style="font-size:80%;">1007.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.2.4.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.4.5.1" style="font-size:80%;">252</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.4.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.4.6.1" style="font-size:80%;">28.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.4.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.4.7.1" style="font-size:80%;">30.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.4.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.4.8.1" style="font-size:80%;">41.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.4.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.4.9.1" style="font-size:80%;">62.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.4.10" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.4.10.1" style="font-size:80%;">52.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.4.11" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.2.4.11.1" style="font-size:80%;">47.3</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.5.1.1" style="font-size:113%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.6.2" style="font-size:113%;">Comparison between BTX and Sparse upcycling with compute-matching (CM), which is a special case of BTX without the expert training stage as is shown by the first column that 100% of compute is spent on MoE training. We also report total training time, compute and number of training tokens. Comparing both performance on individual domains as well as the average, we can see that BTX has more balanced performance, in addition to higher throughput.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS1.Px2.p2">
<p class="ltx_p" id="S4.SS2.SSS1.Px2.p2.1">We further compare BTX with the sparse upcycling baseline in the compute-matching (CM) scenario. Both train on the same data mixture during the MoE stage, but differ in terms of the percent of compute spent on MoE training.
While sparse cycling performs close behind BTX, the parallel training of experts increases the training throughput of BTX, as is shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S4.T3" title="Table 3 ‣ BTX improves all tasks where experts specialize. ‣ 4.2.1 Overall Performance ‣ 4.2 Main Results ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Table&nbsp;3</span></a>. As a result, BTX can train with more than <math alttext="2\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS1.Px2.p2.1.m1.1"><semantics id="S4.SS2.SSS1.Px2.p2.1.m1.1a"><mrow id="S4.SS2.SSS1.Px2.p2.1.m1.1b"><mn id="S4.SS2.SSS1.Px2.p2.1.m1.1.1">2</mn><mo id="S4.SS2.SSS1.Px2.p2.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.SSS1.Px2.p2.1.m1.1c">2\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.Px2.p2.1.m1.1d">2 ×</annotation></semantics></math> the data than pure MoE given the same training compute budget, and achieves slightly higher average performance across all domains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.2">
<tbody><tr class="ltx_tr" id="S4.T4.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.2.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.1" style="font-size:90%;">Routing method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.2.1" style="font-size:90%;">Active parameters (B)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.2.1.3" rowspan="2"><span class="ltx_text ltx_font_bold ltx_align_center" id="S4.T4.2.1.3.1" style="font-size:90%;">MoE Finetune tokens (B)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.2.1.4" rowspan="2"><span class="ltx_text ltx_font_bold ltx_align_center" id="S4.T4.2.1.4.1" style="font-size:90%;">Average score</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.1.1" style="font-size:90%;">Training</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.1" style="font-size:90%;">Inference</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.3">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.2.3.1"><span class="ltx_text" id="S4.T4.2.3.1.1" style="font-size:90%;">Switch Top-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.2.3.2">
<span class="ltx_text ltx_phantom" id="S4.T4.2.3.2.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.3.2.2" style="font-size:90%;">6.7</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.2.3.3">
<span class="ltx_text ltx_phantom" id="S4.T4.2.3.3.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.3.3.2" style="font-size:90%;">6.7</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.2.3.4">
<span class="ltx_text ltx_phantom" id="S4.T4.2.3.4.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.3.4.2" style="font-size:90%;">10</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.2.3.5"><span class="ltx_text" id="S4.T4.2.3.5.1" style="font-size:90%;">24.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.4">
<td class="ltx_td ltx_align_left" id="S4.T4.2.4.1"><span class="ltx_text" id="S4.T4.2.4.1.1" style="font-size:90%;">Sample Top-1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.2">
<span class="ltx_text ltx_phantom" id="S4.T4.2.4.2.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.4.2.2" style="font-size:90%;">6.7</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.3">
<span class="ltx_text ltx_phantom" id="S4.T4.2.4.3.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.4.3.2" style="font-size:90%;">6.7</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.4">
<span class="ltx_text ltx_phantom" id="S4.T4.2.4.4.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.4.4.2" style="font-size:90%;">10</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.4.5"><span class="ltx_text" id="S4.T4.2.4.5.1" style="font-size:90%;">33.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.5">
<td class="ltx_td ltx_align_left" id="S4.T4.2.5.1"><span class="ltx_text" id="S4.T4.2.5.1.1" style="font-size:90%;">Top-2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.2"><span class="ltx_text" id="S4.T4.2.5.2.1" style="font-size:90%;">11.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.3"><span class="ltx_text" id="S4.T4.2.5.3.1" style="font-size:90%;">11.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.4">
<span class="ltx_text ltx_phantom" id="S4.T4.2.5.4.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.5.4.2" style="font-size:90%;">10</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.5.5"><span class="ltx_text" id="S4.T4.2.5.5.1" style="font-size:90%;">34.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.6">
<td class="ltx_td ltx_align_left" id="S4.T4.2.6.1"><span class="ltx_text" id="S4.T4.2.6.1.1" style="font-size:90%;">Soft routing</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.2"><span class="ltx_text" id="S4.T4.2.6.2.1" style="font-size:90%;">19.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.3"><span class="ltx_text" id="S4.T4.2.6.3.1" style="font-size:90%;">19.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.4">
<span class="ltx_text ltx_phantom" id="S4.T4.2.6.4.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.6.4.2" style="font-size:90%;">10</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.6.5"><span class="ltx_text" id="S4.T4.2.6.5.1" style="font-size:90%;">35.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.7.1"><span class="ltx_text" id="S4.T4.2.7.1.1" style="font-size:90%;">Sample Top-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.2">
<span class="ltx_text ltx_phantom" id="S4.T4.2.7.2.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.7.2.2" style="font-size:90%;">6.7</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.3">
<span class="ltx_text ltx_phantom" id="S4.T4.2.7.3.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.7.3.2" style="font-size:90%;">6.7</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.4">
<span class="ltx_text ltx_phantom" id="S4.T4.2.7.4.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.7.4.2" style="font-size:90%;">40</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.7.5"><span class="ltx_text" id="S4.T4.2.7.5.1" style="font-size:90%;">35.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.8">
<td class="ltx_td ltx_align_left" id="S4.T4.2.8.1"><span class="ltx_text" id="S4.T4.2.8.1.1" style="font-size:90%;">Top-2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.8.2"><span class="ltx_text" id="S4.T4.2.8.2.1" style="font-size:90%;">11.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.8.3"><span class="ltx_text" id="S4.T4.2.8.3.1" style="font-size:90%;">11.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.8.4">
<span class="ltx_text ltx_phantom" id="S4.T4.2.8.4.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.8.4.2" style="font-size:90%;">40</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.8.5"><span class="ltx_text" id="S4.T4.2.8.5.1" style="font-size:90%;">35.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.9">
<td class="ltx_td ltx_align_left" id="S4.T4.2.9.1"><span class="ltx_text" id="S4.T4.2.9.1.1" style="font-size:90%;">Soft routing</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.9.2"><span class="ltx_text" id="S4.T4.2.9.2.1" style="font-size:90%;">19.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.9.3"><span class="ltx_text" id="S4.T4.2.9.3.1" style="font-size:90%;">19.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.9.4">
<span class="ltx_text ltx_phantom" id="S4.T4.2.9.4.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.9.4.2" style="font-size:90%;">40</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.9.5"><span class="ltx_text" id="S4.T4.2.9.5.1" style="font-size:90%;">37.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.2.10.1"><span class="ltx_text" id="S4.T4.2.10.1.1" style="font-size:90%;">Sample Top-1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.2">
<span class="ltx_text ltx_phantom" id="S4.T4.2.10.2.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.10.2.2" style="font-size:90%;">6.7</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.3">
<span class="ltx_text ltx_phantom" id="S4.T4.2.10.3.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.10.3.2" style="font-size:90%;">6.7</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.4"><span class="ltx_text" id="S4.T4.2.10.4.1" style="font-size:90%;">160</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.10.5"><span class="ltx_text" id="S4.T4.2.10.5.1" style="font-size:90%;">36.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.2.11.1"><span class="ltx_text" id="S4.T4.2.11.1.1" style="font-size:90%;">Top-2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.11.2"><span class="ltx_text" id="S4.T4.2.11.2.1" style="font-size:90%;">11.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.11.3"><span class="ltx_text" id="S4.T4.2.11.3.1" style="font-size:90%;">11.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.11.4">
<span class="ltx_text ltx_phantom" id="S4.T4.2.11.4.1" style="font-size:90%;"><span style="visibility:hidden">0</span></span><span class="ltx_text" id="S4.T4.2.11.4.2" style="font-size:90%;">80</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.11.5"><span class="ltx_text" id="S4.T4.2.11.5.1" style="font-size:90%;">37.3</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablations on different routing methods during BTX training. Average score is based on performance on representative tasks including GSM8K, HumanEval, Natural Questions, ARC Challenge and MMLU.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T5.2">
<tbody><tr class="ltx_tr" id="S4.T5.2.1">
<td class="ltx_td ltx_border_tt" id="S4.T5.2.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.2.1" style="font-size:90%;">GSM8K</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.3.1" style="font-size:90%;">Human</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.4.1" style="font-size:90%;">Natural</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.1.5"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.5.1" style="font-size:90%;">ARC</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.1.6"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.6.1" style="font-size:90%;">MMLU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.1.7"><span class="ltx_text ltx_font_bold" id="S4.T5.2.1.7.1" style="font-size:90%;">Average</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.2">
<td class="ltx_td" id="S4.T5.2.2.1"></td>
<td class="ltx_td" id="S4.T5.2.2.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T5.2.2.3.1" style="font-size:90%;">Eval</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T5.2.2.4.1" style="font-size:90%;">Questions</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T5.2.2.5.1" style="font-size:90%;">Challenge</span></td>
<td class="ltx_td" id="S4.T5.2.2.6"></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.2.7"><span class="ltx_text ltx_font_bold" id="S4.T5.2.2.7.1" style="font-size:90%;">Score</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.3">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T5.2.3.1"><span class="ltx_text" id="S4.T5.2.3.1.1" style="font-size:90%;">BTX</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.3.2"><span class="ltx_text" id="S4.T5.2.3.2.1" style="font-size:90%;">29.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.3.3"><span class="ltx_text" id="S4.T5.2.3.3.1" style="font-size:90%;">27.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.3.4"><span class="ltx_text" id="S4.T5.2.3.4.1" style="font-size:90%;">23.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.3.5"><span class="ltx_text" id="S4.T5.2.3.5.1" style="font-size:90%;">43.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.3.6"><span class="ltx_text" id="S4.T5.2.3.6.1" style="font-size:90%;">50.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.2.3.7"><span class="ltx_text" id="S4.T5.2.3.7.1" style="font-size:90%;">34.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.2.4.1"><span class="ltx_text" id="S4.T5.2.4.1.1" style="font-size:90%;">no load-balancing (LB)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.4.2"><span class="ltx_text" id="S4.T5.2.4.2.1" style="font-size:90%;">34.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.4.3"><span class="ltx_text" id="S4.T5.2.4.3.1" style="font-size:90%;">19.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.4.4"><span class="ltx_text" id="S4.T5.2.4.4.1" style="font-size:90%;">23.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.4.5"><span class="ltx_text" id="S4.T5.2.4.5.1" style="font-size:90%;">44.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.4.6"><span class="ltx_text" id="S4.T5.2.4.6.1" style="font-size:90%;">51.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.4.7"><span class="ltx_text" id="S4.T5.2.4.7.1" style="font-size:90%;">34.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.5">
<td class="ltx_td ltx_align_left" id="S4.T5.2.5.1"><span class="ltx_text" id="S4.T5.2.5.1.1" style="font-size:90%;">no LB &amp; freeze experts</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.5.2"><span class="ltx_text" id="S4.T5.2.5.2.1" style="font-size:90%;">34.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.5.3"><span class="ltx_text" id="S4.T5.2.5.3.1" style="font-size:90%;">18.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.5.4"><span class="ltx_text" id="S4.T5.2.5.4.1" style="font-size:90%;">24.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.5.5"><span class="ltx_text" id="S4.T5.2.5.5.1" style="font-size:90%;">44.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.5.6"><span class="ltx_text" id="S4.T5.2.5.6.1" style="font-size:90%;">51.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.5.7"><span class="ltx_text" id="S4.T5.2.5.7.1" style="font-size:90%;">34.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.6">
<td class="ltx_td ltx_align_left" id="S4.T5.2.6.1"><span class="ltx_text" id="S4.T5.2.6.1.1" style="font-size:90%;">blending experts</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.6.2"><span class="ltx_text" id="S4.T5.2.6.2.1" style="font-size:90%;">13.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.6.3"><span class="ltx_text" id="S4.T5.2.6.3.1" style="font-size:90%;">17.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.6.4"><span class="ltx_text" id="S4.T5.2.6.4.1" style="font-size:90%;">9.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.6.5"><span class="ltx_text" id="S4.T5.2.6.5.1" style="font-size:90%;">34.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.6.6"><span class="ltx_text" id="S4.T5.2.6.6.1" style="font-size:90%;">36.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.6.7"><span class="ltx_text" id="S4.T5.2.6.7.1" style="font-size:90%;">22.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.7">
<td class="ltx_td ltx_align_left" id="S4.T5.2.7.1"><span class="ltx_text" id="S4.T5.2.7.1.1" style="font-size:90%;">split experts, top-2 of 8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.7.2"><span class="ltx_text" id="S4.T5.2.7.2.1" style="font-size:90%;">22.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.7.3"><span class="ltx_text" id="S4.T5.2.7.3.1" style="font-size:90%;">20.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.7.4"><span class="ltx_text" id="S4.T5.2.7.4.1" style="font-size:90%;">16.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.7.5"><span class="ltx_text" id="S4.T5.2.7.5.1" style="font-size:90%;">39.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.7.6"><span class="ltx_text" id="S4.T5.2.7.6.1" style="font-size:90%;">41.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.7.7"><span class="ltx_text" id="S4.T5.2.7.7.1" style="font-size:90%;">28.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.2.8.1"><span class="ltx_text" id="S4.T5.2.8.1.1" style="font-size:90%;">split experts, top-4 of 8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.8.2"><span class="ltx_text" id="S4.T5.2.8.2.1" style="font-size:90%;">29.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.8.3"><span class="ltx_text" id="S4.T5.2.8.3.1" style="font-size:90%;">26.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.8.4"><span class="ltx_text" id="S4.T5.2.8.4.1" style="font-size:90%;">22.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.8.5"><span class="ltx_text" id="S4.T5.2.8.5.1" style="font-size:90%;">44.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.8.6"><span class="ltx_text" id="S4.T5.2.8.6.1" style="font-size:90%;">49.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.8.7"><span class="ltx_text" id="S4.T5.2.8.7.1" style="font-size:90%;">34.5</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablations on different BTX training strategies. All variants are initialized from the same experts and trained for a total of 10B tokens during MoE finetuning.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Better compute-performance tradeoff</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">We compare BTX with baselines in terms of compute efficiency in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#footnote1" title="footnote 1 ‣ Figure 2 ‣ 4.1.3 Evaluation ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag"><span class="ltx_text" style="font-size:111%;">footnote&nbsp;1</span></span></a> (left). The X-axis shows the total training compute starting from the seed model measured in GPU days, which includes the domain expert training and finetuning of the MoE model. The Y-axis measures the overall performance reported in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S4.T2" title="Table 2 ‣ 4.1.2 Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Table&nbsp;2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Better performance than dense and BTM.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px1.p1.1">Despite that the MoE training stage uses a fraction of the total training budget in pretraining (for example, <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS2.Px1.p1.1.1">Llama-2</span> pretraining uses 2T tokens), BTX brings steep improvements on general capabilities compared to alternative continued pretraining approaches such as multi-task learning of the dense model and Branch-Train-Merge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">More efficient than sparse upcycling.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.Px2.p1.1">As a special case of BTX, sparse upcycling without expert training outperforms dense and BTM but not BTX, given the same or larger compute budget.
The compute efficiency gains of BTX are from the embarrassingly parallel training of experts before MoE finetuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.Px2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.Px2.p2.1">In terms of the active number of parameters (shown as circle sizes in <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#footnote1" title="footnote 1 ‣ Figure 2 ‣ 4.1.3 Evaluation ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag"><span class="ltx_text" style="font-size:111%;">1</span></span></a> (left)), the MoE models are similar to the <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS2.Px2.p2.1.1">Llama-2 13B</span> model. BTX uses less than half of the additional training compute compared to <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS2.Px2.p2.1.2">Llama-2 13B</span>, but demonstrates improved performance on expert domains (math, code, and knowledge) and achieves better overall performance. This indicates that BTX’s training is more effective for the late stage of pretraining than using the same training protocol throughout the entire of pretraining.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablations &amp; Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Ablations of BTX training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">First, we compare the different routing methods with varying amount of active parameters for different amounts of finetuning. For fair comparison, load balancing is not used in any of them.
Results are shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S4.T4" title="Table 4 ‣ BTX improves all tasks where experts specialize. ‣ 4.2.1 Overall Performance ‣ 4.2 Main Results ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Table&nbsp;4</span></a>.
For Switch routing, we set its capacity factor to 1.5 (a hard limit after which routed tokens will be dropped). We found the Switch router to be subpar in average performance.
The soft routing performs the best, but that is expected since it lacks sparsity and has the highest number of active parameters.
Overall, the Top-2 routing gives us a good balance between performance and efficiency.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">We also ablate additional design choices of BTX, with results summarized in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S4.T5" title="Table 5 ‣ BTX improves all tasks where experts specialize. ‣ 4.2.1 Overall Performance ‣ 4.2 Main Results ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Table&nbsp;5</span></a>.
We found that MoE training without load balancing performs worse on the coding task (HumanEval), but has higher math (GSM8k) accuracy.
The routing analysis in the next section will give more insight into this trade-off.
Next, freezing the feedforward modules initialized from each expert, and only training the rest of the MoE model has little impact on performance across all tasks.
This suggests that individual experts already gained sufficient domain knowledge during the branch-train stage, while the mix (MoE finetuning) stage mainly trains the other parameters such as averaged weights in the self-attention and the router transformations <math alttext="W_{i}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p2.1.m1.1"><semantics id="S4.SS3.SSS1.p2.1.m1.1a"><msub id="S4.SS3.SSS1.p2.1.m1.1.1" xref="S4.SS3.SSS1.p2.1.m1.1.1.cmml"><mi id="S4.SS3.SSS1.p2.1.m1.1.1.2" xref="S4.SS3.SSS1.p2.1.m1.1.1.2.cmml">W</mi><mi id="S4.SS3.SSS1.p2.1.m1.1.1.3" xref="S4.SS3.SSS1.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p2.1.m1.1b"><apply id="S4.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1.2">𝑊</ci><ci id="S4.SS3.SSS1.p2.1.m1.1.1.3.cmml" xref="S4.SS3.SSS1.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p2.1.m1.1c">W_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p2.1.m1.1d">italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p3">
<p class="ltx_p" id="S4.SS3.SSS1.p3.1">We also test our blending and splitting techniques described in <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S3.SS3" title="3.3 Variations ‣ 3 Branch-Train-MiX ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">3.3</span></a>.
The performance across all tasks dropped when experts are mixed, suggesting that domain FF layers cannot be mixed in this way.
Splitting each domain FF into <math alttext="C=2" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p3.1.m1.1"><semantics id="S4.SS3.SSS1.p3.1.m1.1a"><mrow id="S4.SS3.SSS1.p3.1.m1.1.1" xref="S4.SS3.SSS1.p3.1.m1.1.1.cmml"><mi id="S4.SS3.SSS1.p3.1.m1.1.1.2" xref="S4.SS3.SSS1.p3.1.m1.1.1.2.cmml">C</mi><mo id="S4.SS3.SSS1.p3.1.m1.1.1.1" xref="S4.SS3.SSS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS1.p3.1.m1.1.1.3" xref="S4.SS3.SSS1.p3.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p3.1.m1.1b"><apply id="S4.SS3.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1"><eq id="S4.SS3.SSS1.p3.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.1"></eq><ci id="S4.SS3.SSS1.p3.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p3.1.m1.1.1.2">𝐶</ci><cn id="S4.SS3.SSS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.SS3.SSS1.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p3.1.m1.1c">C=2</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p3.1.m1.1d">italic_C = 2</annotation></semantics></math> chunks to obtain 8 modules in the MoE layer also does not improve performance, even if Top-4 routing is used to match the active number of parameters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="198" id="S4.F3.g1" src="x4.png" width="788"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="211" id="S4.F3.g2" src="x5.png" width="788"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.4.2" style="font-size:90%;">BTX routing decisions of the tokens at various layers to different experts (Wiki, Math, Code, <span class="ltx_text ltx_font_smallcaps" id="S4.F3.4.2.1">LLaMa-2 7B</span>) for different downstream tasks. The tasks are aggregated by domain: Code (Human Eval, MBPP), Math (GSM8K, MATH), World knowledge (Natural Questions, TriviaQA), and Reasoning (ARC-Easy, ARC-Challenge, SIQA, PIQA, and WinoGrande). We observe that Top-2 routing with load balancing (top) ensures a more uniform distribution of the load between experts compared to Top-2 without load balancing (bottom).</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Routing Analysis</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">To gain an in-depth understanding of the performance of BTX, we run model evaluations on downstream tasks and examine the routing decisions among the experts. The results are summarized in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S4.F3" title="Figure 3 ‣ 4.3.1 Ablations of BTX training ‣ 4.3 Ablations &amp; Analysis ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Figure&nbsp;3</span></a>, and we also report detailed ablation results for different BTX setups in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S10" title="10 Routing analysis ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Section&nbsp;10</span></a>. Compared to other routing methods, Top-2 routing with load balancing ensures a more uniform distribution of the load between experts. Analyzing the token probability distributions, we observe a shift towards low probability scores across all experts with load balancing, especially closer to the final layers of the model, which contributes to the fair routing. Interestingly, all models without load balance heavily rely on the Math expert, with a low overall contribution from other experts, especially the Code expert. A dead Code expert comes “back to life” with load balancing introduced in training. In fact, it not only becomes visible, but becomes the dominant expert in the math and code domains.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1">Examples of the routing decisions for Top-2 with load balancing can be found in the <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S4.T6" title="Table 6 ‣ 4.3.2 Routing Analysis ‣ 4.3 Ablations &amp; Analysis ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Table&nbsp;6</span></a>. Overall across math domain tasks, tokens are often routed to the Code and <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS2.p2.1.1">Llama-2 7B</span> experts. If we look at a more detailed token distribution (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S10" title="10 Routing analysis ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Section&nbsp;10</span></a>, <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S10.F6" title="Figure 6 ‣ 10 Routing analysis ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Figure&nbsp;6</span></a>), we find that the GSM8K task prefers Code and <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS2.p2.1.2">Llama-2</span> experts, while the MATH task relies more on the in-domain Math expert. We hypothesise that this happens because the GSM8K dataset consists of grade school math problems that require common sense knowledge and basic arithmetic operations.
Both the Code and World knowledge tasks mostly route to the in-domain Code and Wikipedia experts respectively.
As observed earlier in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S4.SS3.SSS1" title="4.3.1 Ablations of BTX training ‣ 4.3 Ablations &amp; Analysis ‣ 4 Experiments ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>, when load balancing is introduced, there are improvements in coding tasks but degradation in math tasks, which can be explained with these changes in domain expert routing. The reasoning tasks in contrast exhibit similar behaviour, and rely equally on Math and generalist LLM’s expertise.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T6.2" style="width:412.4pt;height:366.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.6pt,45.8pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.2.1">
<tbody><tr class="ltx_tr" id="S4.T6.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.2.1.1.1">Task</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T6.2.1.1.2">Question and generation</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.2.1.2.1">GSM8K</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.2.1.2.2">
<span class="ltx_text" id="S4.T6.2.1.2.2.1"></span><span class="ltx_text" id="S4.T6.2.1.2.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T6.2.1.2.2.2.1">
<span class="ltx_tr" id="S4.T6.2.1.2.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.2.2.2.1.1.1">Q: <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.1.1.1" style="color:#0000FF;">Jan<span class="ltx_text" id="S4.T6.2.1.2.2.2.1.1.1.1.1" style="color:#FF00FF;">et</span>’</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.1.1.2" style="color:#FF8000;">s <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.1.1.2.1" style="color:#FF00FF;">ducks lay</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.1.1.2.2" style="color:#FF00FF;"> </span>1<span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.1.1.2.3" style="color:#FF00FF;">6 eggs <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.1.1.2.3.1" style="color:#0000FF;">per </span>day</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.1.1.2.4" style="color:#00FFFF;">. <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.1.1.2.4.1" style="color:#0000FF;">She e</span></span>ats <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.1.1.2.5" style="color:#FF00FF;">three for breakfast</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.1.1.2.6" style="color:#FF00FF;"> <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.1.1.2.6.1" style="color:#00FFFF;">every <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.1.1.2.6.1.1" style="color:#FF00FF;">morning</span></span> <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.1.1.2.6.2" style="color:#00FFFF;">and <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.1.1.2.6.2.1" style="color:#FF00FF;">bakes muff</span></span></span>ins <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.1.1.2.7" style="color:#0000FF;">for</span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.2.2.2.1.2">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.2.2.2.1.2.1"><span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.2.1.1" style="color:#0000FF;">her <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.2.1.1.1" style="color:#FF00FF;">friends </span>every <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.2.1.1.2" style="color:#FF00FF;">day</span></span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.2.1.2" style="color:#FF00FF;"> <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.2.1.2.1" style="color:#00FFFF;">with <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.2.1.2.1.1" style="color:#FF00FF;">four. <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.2.1.2.1.1.1" style="color:#0000FF;">She s</span>ells the remainder <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.2.1.2.1.1.2" style="color:#0000FF;">at </span>the farmers’ market</span></span> <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.2.1.2.2" style="color:#00FFFF;">daily for <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.2.1.2.2.1" style="color:#FF00FF;">$</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.2.1.2.2.2" style="color:#FF8000;">2 <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.2.1.2.2.2.1" style="color:#FF00FF;">per fresh</span></span></span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.2.2.2.1.3">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.2.2.2.1.3.1"><span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.3.1.1" style="color:#FF00FF;">du</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.3.1.2" style="color:#FF8000;">ck <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.3.1.2.1" style="color:#FF00FF;">egg</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.3.1.2.2" style="color:#FF0000;">. <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.3.1.2.2.1" style="color:#00FFFF;">How</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.3.1.2.2.2" style="color:#00FFFF;"> </span></span>much <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.3.1.2.3" style="color:#FF00FF;">in dollars <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.3.1.2.3.1" style="color:#0000FF;">does she <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.3.1.2.3.1.1" style="color:#008080;">make </span>every</span></span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.3.1.2.4" style="color:#0000FF;"> </span>day <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.3.1.2.5" style="color:#0000FF;">at the <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.3.1.2.5.1" style="color:#FF00FF;">far</span></span>mers’ market<span class="ltx_text" id="S4.T6.2.1.2.2.2.1.3.1.2.6" style="color:#00FFFF;">?</span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.2.2.2.1.4">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.2.2.2.1.4.1">A: <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.4.1.1" style="color:#0000FF;">Jan</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.4.1.2" style="color:#FF8000;">et<span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.4.1.2.1" style="color:#FF00FF;">’</span>s <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.4.1.2.2" style="color:#FF00FF;">du</span>cks <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.4.1.2.3" style="color:#FF00FF;">lay</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.4.1.2.4" style="color:#FF00FF;"> </span>16 eggs <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.4.1.2.5" style="color:#FF0000;">per </span>day. <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.4.1.2.6" style="color:#0000FF;">She e</span>ats <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.4.1.2.7" style="color:#FF00FF;">three</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.4.1.2.8" style="color:#FF00FF;"> <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.4.1.2.8.1" style="color:#FF0000;">for </span></span>breakfast <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.4.1.2.9" style="color:#FF0000;">every </span>morning<span class="ltx_text" id="S4.T6.2.1.2.2.2.1.4.1.2.10" style="color:#00FFFF;">. <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.4.1.2.10.1" style="color:#0000FF;">So she has</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.4.1.2.10.2" style="color:#0000FF;"> </span></span>1<span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.4.1.2.11" style="color:#FF00FF;">6 <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.4.1.2.11.1" style="color:#008080;">-</span></span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.4.1.2.12" style="color:#008080;"> </span>3 <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.4.1.2.13" style="color:#FF00FF;">=</span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.2.2.2.1.5">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.2.2.2.1.5.1"><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1" style="color:#FF8000;">13 <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.5.1.1.1" style="color:#FF00FF;">eggs <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.1.1" style="color:#0000FF;">left. She </span>b</span>akes <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.2" style="color:#FF0000;">m</span>uffins <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.5.1.1.3" style="color:#0000FF;">for</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.4" style="color:#0000FF;"> <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.4.1" style="color:#FF0000;">her <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.5.1.1.4.1.1" style="color:#FF00FF;">friends</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.4.1.2" style="color:#FF00FF;"> </span>every </span></span>day <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.5" style="color:#008080;">with </span>4<span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.5.1.1.6" style="color:#FF00FF;">.</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.7" style="color:#FF00FF;"> <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.7.1" style="color:#FF0000;">So she <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.7.1.1" style="color:#008080;">has </span></span></span>13 <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.5.1.1.8" style="color:#008080;">-</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.9" style="color:#008080;"> </span>4 = 9 <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.5.1.1.10" style="color:#FF00FF;">eggs</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.11" style="color:#FF00FF;"> <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.5.1.1.11.1" style="color:#FF0000;">left. <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.5.1.1.11.1.1" style="color:#0000FF;">She</span></span></span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.2.2.2.1.6">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.2.2.2.1.6.1"><span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.6.1.1" style="color:#008080;">s</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.6.1.2" style="color:#FF8000;">ells <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.6.1.2.1" style="color:#0000FF;">the</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.6.1.2.2" style="color:#0000FF;"> </span>remainder <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.6.1.2.3" style="color:#FF0000;">at the <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.6.1.2.3.1" style="color:#FF00FF;">far</span></span>mers’ <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.6.1.2.4" style="color:#008080;">market</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.6.1.2.5" style="color:#008080;"> <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.6.1.2.5.1" style="color:#FF0000;">daily <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.6.1.2.5.1.1" style="color:#00FFFF;">for <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.6.1.2.5.1.1.1" style="color:#008080;">$</span></span></span></span>2 per fresh duck egg<span class="ltx_text" id="S4.T6.2.1.2.2.2.1.6.1.2.6" style="color:#FF0000;">. <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.6.1.2.6.1" style="color:#0000FF;">So she has</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.6.1.2.6.2" style="color:#0000FF;"> </span></span>9 <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.6.1.2.7" style="color:#008080;">*</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.6.1.2.8" style="color:#008080;"> </span>2 <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.6.1.2.9" style="color:#FF00FF;">=</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.6.1.2.10" style="color:#FF00FF;"> </span>18 <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.6.1.2.11" style="color:#FF00FF;">dollars<span class="ltx_text" id="S4.T6.2.1.2.2.2.1.6.1.2.11.1" style="color:#0000FF;">.</span></span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.2.2.2.1.7">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.2.2.2.1.7.1"><span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.2.2.2.1.7.1.1" style="color:#0000FF;">The</span><span class="ltx_text" id="S4.T6.2.1.2.2.2.1.7.1.2" style="color:#0000FF;"> <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.7.1.2.1" style="color:#FF8000;">answer <span class="ltx_text" id="S4.T6.2.1.2.2.2.1.7.1.2.1.1" style="color:#00FFFF;">is </span>18.</span></span></span></span>
</span></span> <span class="ltx_text" id="S4.T6.2.1.2.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.2.1.3.1">Human Eval</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T6.2.1.3.2">
<span class="ltx_text" id="S4.T6.2.1.3.2.1"></span><span class="ltx_text" id="S4.T6.2.1.3.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T6.2.1.3.2.2.1">
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.1.1">Q: <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.1.1.1" style="color:#FF0000;">from typing import <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.1.1.1.1" style="color:#0000FF;">List</span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.2">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.2.1"><span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.2.1.1" style="color:#008080;">def has_close<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.2.1.1.1" style="color:#FF8000;">_</span>elements(numbers: List[float<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.2.1.1.2" style="color:#FF8000;">], </span>threshold: <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.2.1.1.3" style="color:#00FFFF;">float<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.2.1.1.3.1" style="color:#FF8000;">)</span></span>-&gt; bool<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.2.1.1.4" style="color:#FF8000;">:</span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.3">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.3.1"> <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.3.1.1" style="color:#008080;">""" Check if in given list</span><span class="ltx_text" id="S4.T6.2.1.3.2.2.1.3.1.2" style="color:#008080;"> <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.3.1.2.1" style="color:#0000FF;">of <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.3.1.2.1.1" style="color:#008080;">numbers</span>, <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.3.1.2.1.2" style="color:#008080;">are any</span></span> <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.3.1.2.2" style="color:#FF00FF;">two <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.3.1.2.2.1" style="color:#008080;">numbers closer</span></span> <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.3.1.2.3" style="color:#0000FF;">to <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.3.1.2.3.1" style="color:#FF00FF;">each <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.3.1.2.3.1.1" style="color:#FF0000;">other </span></span>than</span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.4">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.4.1"> <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.4.1.1" style="color:#008080;">given threshold.</span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.5">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.5.1"> <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.5.1.1" style="color:#008080;">&gt; &gt; &gt; has</span><span class="ltx_text" id="S4.T6.2.1.3.2.2.1.5.1.2" style="color:#FF0000;">_close_<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.5.1.2.1" style="color:#0000FF;">elements<span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.5.1.2.1.1" style="color:#008080;">([1.0<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.5.1.2.1.1.1" style="color:#FF8000;">, </span>2<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.5.1.2.1.1.2" style="color:#FF8000;">.0, 3.0], </span>0<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.5.1.2.1.1.3" style="color:#FF8000;">.</span>5)</span></span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.6">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.6.1"> <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.6.1.1" style="color:#008080;">False</span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.7">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.7.1"> <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.7.1.1" style="color:#008080;">&gt; &gt; &gt; has<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.1.1" style="color:#00FFFF;">_close</span></span><span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2" style="color:#FF0000;">_<span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.7.1.2.1" style="color:#00FFFF;">elements<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.1.1" style="color:#FF8000;">([<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.1.1.1" style="color:#008080;">1</span>.0, <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.1.1.2" style="color:#008080;">2</span>.8<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.1.1.3" style="color:#008080;">, 3</span>.<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.1.1.4" style="color:#008080;">0</span>, <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.1.1.5" style="color:#008080;">4</span>.<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.1.1.6" style="color:#008080;">0, </span>5.0<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.1.1.7" style="color:#008080;">, </span>2<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.1.1.8" style="color:#008080;">.</span>0<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.1.1.9" style="color:#008080;">],</span></span></span><span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.2" style="color:#008080;"> </span>0<span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.7.1.2.3" style="color:#008080;">.<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.7.1.2.3.1" style="color:#FF8000;">3)</span></span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.8">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.8.1"> <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.8.1.1" style="color:#FF8000;">True</span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.9">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.9.1"> <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.9.1.1" style="color:#FF8000;">"""</span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.10">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.10.1">A: <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.10.1.1" style="color:#008080;">for i in</span><span class="ltx_text" id="S4.T6.2.1.3.2.2.1.10.1.2" style="color:#008080;"> <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.10.1.2.1" style="color:#FF0000;">range<span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.10.1.2.1.1" style="color:#FF8000;">(<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.10.1.2.1.1.1" style="color:#008080;">len</span>(</span>numbers<span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.10.1.2.1.2" style="color:#008080;">) - <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.10.1.2.1.2.1" style="color:#FF8000;">1):</span></span></span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.11">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.11.1"> <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.11.1.1" style="color:#008080;">for j <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.11.1.1.1" style="color:#FF8000;">in <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.11.1.1.1.1" style="color:#0000FF;">range</span>(</span>i +</span><span class="ltx_text" id="S4.T6.2.1.3.2.2.1.11.1.2" style="color:#008080;"> <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.11.1.2.1" style="color:#FF0000;">1<span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.11.1.2.1.1" style="color:#FF8000;">, <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.11.1.2.1.1.1" style="color:#008080;">len</span>(</span><span class="ltx_text" id="S4.T6.2.1.3.2.2.1.11.1.2.1.2" style="color:#0000FF;">numbers<span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.11.1.2.1.2.1" style="color:#008080;">)):</span></span></span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.12">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.12.1">  <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.12.1.1" style="color:#008080;">if abs(numbers<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.12.1.1.1" style="color:#FF8000;">[i] - numbers<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.12.1.1.1.1" style="color:#00FFFF;">[j]</span>) </span>&lt; threshold<span class="ltx_text" id="S4.T6.2.1.3.2.2.1.12.1.1.2" style="color:#FF8000;">:</span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.13">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.13.1">   <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.13.1.1" style="color:#008080;">return <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.13.1.1.1" style="color:#00FFFF;">True</span></span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.3.2.2.1.14">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.3.2.2.1.14.1"><span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.3.2.2.1.14.1.1" style="color:#008080;">return <span class="ltx_text" id="S4.T6.2.1.3.2.2.1.14.1.1.1" style="color:#00FFFF;">False</span></span></span></span>
</span></span> <span class="ltx_text" id="S4.T6.2.1.3.2.3"></span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.1.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T6.2.1.4.1">
<span class="ltx_text" id="S4.T6.2.1.4.1.1"></span><span class="ltx_text" id="S4.T6.2.1.4.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T6.2.1.4.1.2.1">
<span class="ltx_tr" id="S4.T6.2.1.4.1.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.4.1.2.1.1.1">Natural</span></span>
<span class="ltx_tr" id="S4.T6.2.1.4.1.2.1.2">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.4.1.2.1.2.1">Questions</span></span>
</span></span> <span class="ltx_text" id="S4.T6.2.1.4.1.3"></span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T6.2.1.4.2">
<span class="ltx_text" id="S4.T6.2.1.4.2.1"></span><span class="ltx_text" id="S4.T6.2.1.4.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T6.2.1.4.2.2.1">
<span class="ltx_tr" id="S4.T6.2.1.4.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.4.2.2.1.1.1">Q: <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.4.2.2.1.1.1.1" style="color:#00FFFF;">who <span class="ltx_text" id="S4.T6.2.1.4.2.2.1.1.1.1.1" style="color:#0000FF;">got <span class="ltx_text" id="S4.T6.2.1.4.2.2.1.1.1.1.1.1" style="color:#FF0000;">the first no</span></span></span><span class="ltx_text" id="S4.T6.2.1.4.2.2.1.1.1.2" style="color:#FF00FF;">bel prize <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.4.2.2.1.1.1.2.1" style="color:#0000FF;">in</span><span class="ltx_text" id="S4.T6.2.1.4.2.2.1.1.1.2.2" style="color:#0000FF;"> </span>physics</span></span></span>
<span class="ltx_tr" id="S4.T6.2.1.4.2.2.1.2">
<span class="ltx_td ltx_align_left" id="S4.T6.2.1.4.2.2.1.2.1">A: <span class="ltx_text ltx_framed_underline" id="S4.T6.2.1.4.2.2.1.2.1.1" style="color:#FF0000;">Max</span><span class="ltx_text" id="S4.T6.2.1.4.2.2.1.2.1.2" style="color:#FF0000;"> <span class="ltx_text" id="S4.T6.2.1.4.2.2.1.2.1.2.1" style="color:#FF00FF;">Plan<span class="ltx_text" id="S4.T6.2.1.4.2.2.1.2.1.2.1.1" style="color:#FF8000;">ck</span></span></span></span></span>
</span></span> <span class="ltx_text" id="S4.T6.2.1.4.2.3"></span>
</td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.4.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S4.T6.5.2" style="font-size:90%;">Examples of the token routing decisions for the Top-2 routing with load balancing in the math (GSM8K), code (Human Eval), and knowledge (Natural Questions) domains. Tokens highlighted are routed to the following experts: <span class="ltx_text" id="S4.T6.5.2.1" style="color:#FF0000;">Wikipedia and <span class="ltx_text ltx_font_smallcaps" id="S4.T6.5.2.1.1">LLaMa-2 7B</span>, <span class="ltx_text" id="S4.T6.5.2.1.2" style="color:#FF00FF;">Math and <span class="ltx_text ltx_font_smallcaps" id="S4.T6.5.2.1.2.1">LLaMa-2 7B</span>, <span class="ltx_text" id="S4.T6.5.2.1.2.2" style="color:#FF8000;">Code and <span class="ltx_text ltx_font_smallcaps" id="S4.T6.5.2.1.2.2.1">LLaMa-2 7B</span>, <span class="ltx_text" id="S4.T6.5.2.1.2.2.2" style="color:#008080;">Math and Code, <span class="ltx_text" id="S4.T6.5.2.1.2.2.2.1" style="color:#0000FF;">Wikipedia and Math, <span class="ltx_text" id="S4.T6.5.2.1.2.2.2.1.1" style="color:#00FFFF;">Wikipedia and Code. <span class="ltx_text" id="S4.T6.5.2.1.2.2.2.1.1.1" style="color:#000000;">Tokens that were routed to the in-domain expert are underlined.</span></span></span></span></span></span></span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We introduced Branch-Train-MiX (BTX), a simple continued pretraining method to improve an LLM’s capabilities. It trains multiple copies of a seed LLM to specialize in multiple domains in an asynchronous and parallel fashion and later merges them back into a single Mixture-of-Experts (MoE) model via finetuning.
While the initial parallel training stage brings higher training throughput and scalability, the second MoE finetuning stage makes the final LLM more performant.
Our experiments suggest that a generalist LLM’s performance can be boosted by continued training on datasets with specialized knowledge and skills using our method. We find that the BTX approach is more compute efficient than training a larger generalist LLM or several separately specialized LLMs. These insights can inform how to allocate compute in late pretraining to achieve a strong generalist model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations &amp; Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Although our experimental results on BTX are promising, we have not fully explored its potential in this paper.
Due to compute limitations, we only experimented with three domains and four experts in this paper.
Training on more domains such as using unsupervised domain discovery <cite class="ltx_cite ltx_citemacro_citep">(Gururangan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib16" title="">2023</a>)</cite> should amplify the benefit of the parallelization of experts training.
Having more experts will also make the final MoE model more efficient because the number of active experts can remain the same while its overall capacity increases.
In our experiments, we used a simple implementation of MoE and did not optimize it using more complex techniques such as placing different experts on different GPUs to run them in parallel.
Such an efficient MoE implementation could shorten the training time of BTX, and the sparse upcycling baseline as well.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Compared to BTM, BTX provides an approach to finetune the combined experts, which can be directly applied in instruction finetuning or RLHF procedures.
However, we leave that for future work as we focused on the pretraining stage in this paper.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">The question of whether experts in MoE are better off specializing in specific domains or not is an interesting one that is worth further investigation.
Our approach explicitly tied experts to certain domains, but such specialization does not seem to emerge naturally during MoE training <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib21" title="">2024</a>)</cite>.
We observed that some experts are used more in their corresponding domain tasks, showing that their domain specialization partially remains even after the MoE finetuning.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">We only compared BTX to two of its special variants, i.e. BTM with 100% compute allocated to expert training and 0% on MoE finetuning, and sparse upcycling with 0% compute allocated to expert training and 100% on MoE finetuning. Future work could perform a thorough sweep of the compute allocation ratio between expert training and MoE training. Also, we did not perform experiments with different data mixtures for MoE finetuning other than uniform sampling.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S7" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We thank Margaret Li, Kushal Tirumala, Luke Zettlemoyer, Artidoro Pagnoni, Suchin Gururangan, Mike Lewis and Emily Dinan for their discussion and feedback, and Andrew Cohen and Arun Babu for their help with the training implementation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia&nbsp;Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aljundi et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars.

</span>
<span class="ltx_bibblock">Expert gate: Lifelong learning with a network of experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 7120–7129, 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:914027" title="">https://api.semanticscholar.org/CorpusID:914027</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie&nbsp;J. Cai, Michael Terry, Quoc&nbsp;V. Le, and Charles Sutton.

</span>
<span class="ltx_bibblock">Program synthesis with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">ArXiv</em>, abs/2108.07732, 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:237142385" title="">https://api.semanticscholar.org/CorpusID:237142385</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Awasthi and Sarawagi (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Abhijeet Awasthi and Sunita Sarawagi.

</span>
<span class="ltx_bibblock">Continual learning with neural networks: A review.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the ACM India Joint International Conference on Data Science and Management of Data</em>, pages 362–365, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azerbayev et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco&nbsp;Dos Santos, Stephen McAleer, Albert&nbsp;Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck.

</span>
<span class="ltx_bibblock">Llemma: An open language model for mathematics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">ArXiv</em>, abs/2310.10631, 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:264172303" title="">https://api.semanticscholar.org/CorpusID:264172303</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et&nbsp;al.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volume&nbsp;34, pages 7432–7439, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T.&nbsp;J. Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">ArXiv</em>, abs/2005.14165, 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:218971783" title="">https://api.semanticscholar.org/CorpusID:218971783</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe&nbsp;Petroski Such, David&nbsp;W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William&nbsp;H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew&nbsp;M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">ArXiv</em>, abs/2107.03374, 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:235755472" title="">https://api.semanticscholar.org/CorpusID:235755472</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think you have solved question answering? Try ARC, the AI2 reasoning challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2110.14168</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Damai Dai, Chengqi Deng, Chenggang Zhao, R.&nbsp;X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y.&nbsp;Wu, Zhenda Xie, Y.&nbsp;K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang.

</span>
<span class="ltx_bibblock">Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">ArXiv</em>, abs/2401.06066, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:266933338" title="">https://api.semanticscholar.org/CorpusID:266933338</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Douillard et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Arthur Douillard, Qixuang Feng, Andrei&nbsp;A. Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc’Aurelio Ranzato, Arthur Szlam, and Jiajun Shen.

</span>
<span class="ltx_bibblock">Diloco: Distributed low-communication training of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">ArXiv</em>, abs/2311.08105, 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:265158012" title="">https://api.semanticscholar.org/CorpusID:265158012</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fedus et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
William Fedus, Barret Zoph, and Noam Shazeer.

</span>
<span class="ltx_bibblock">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">The Journal of Machine Learning Research</em>, 23(1):5232–5270, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini Team (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemini Team.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
<span class="ltx_bibblock">Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Suchin Gururangan, Michael Lewis, Ari Holtzman, Noah&nbsp;A. Smith, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Demix layers: Disentangling domains for modular language modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">North American Chapter of the Association for Computational Linguistics</em>, 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:236976189" title="">https://api.semanticscholar.org/CorpusID:236976189</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah&nbsp;A Smith, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Scaling expert language models with unsupervised domain discovery.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2303.14177</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2021a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net, 2021a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=d7KBjmI3GmQ" title="">https://openreview.net/forum?id=d7KBjmI3GmQ</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2021b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn&nbsp;Xiaodong Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring mathematical problem solving with the math dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ArXiv</em>, abs/2103.03874, 2021b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:232134851" title="">https://api.semanticscholar.org/CorpusID:232134851</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobs et&nbsp;al. (1991)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Robert&nbsp;A. Jacobs, Michael&nbsp;I. Jordan, Steven&nbsp;J. Nowlan, and Geoffrey&nbsp;E. Hinton.

</span>
<span class="ltx_bibblock">Adaptive mixtures of local experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Neural Computation</em>, 3:79–87, 1991.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:572361" title="">https://api.semanticscholar.org/CorpusID:572361</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Eric Jang, Shixiang Gu, and Ben Poole.

</span>
<span class="ltx_bibblock">Categorical reparameterization with gumbel-softmax.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:1611.01144</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;Las&nbsp;Casas, Emma&nbsp;Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L’elio&nbsp;Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven&nbsp;Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William&nbsp;El Sayed.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ArXiv</em>, abs/2401.04088, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:266844877" title="">https://api.semanticscholar.org/CorpusID:266844877</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel&nbsp;S. Weld, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">ArXiv</em>, abs/1705.03551, 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:26501419" title="">https://api.semanticscholar.org/CorpusID:26501419</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Komatsuzaki et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos&nbsp;Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi&nbsp;Tay, Mostafa Dehghani, and Neil Houlsby.

</span>
<span class="ltx_bibblock">Sparse upcycling: Training mixture-of-experts from dense checkpoints.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">ArXiv</em>, abs/2212.05055, 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:254535822" title="">https://api.semanticscholar.org/CorpusID:254535822</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina&nbsp;N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Transactions of the Association of Computational Linguistics</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lange et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Matthias&nbsp;De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu&nbsp;Jia, Aleš Leonardis, Gregory&nbsp;G. Slabaugh, and Tinne Tuytelaars.

</span>
<span class="ltx_bibblock">A continual learning survey: Defying forgetting in classification tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 44:3366–3385, 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:218889912" title="">https://api.semanticscholar.org/CorpusID:218889912</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Base layers: Simplifying training of large, sparse models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">International Conference on Machine Learning</em>, 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:232428341" title="">https://api.semanticscholar.org/CorpusID:232428341</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah&nbsp;A. Smith, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Branch-train-merge: Embarrassingly parallel training of expert language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">ArXiv</em>, abs/2208.03306, 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:251371375" title="">https://api.semanticscholar.org/CorpusID:251371375</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yujia Li, David&nbsp;H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, Agustin&nbsp;Dal Lago, Thomas Hubert, Peter Choy, Cyprien de, Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov, James Molloy, Daniel&nbsp;Jaymin Mankowitz, Esme&nbsp;Sutherland Robson, Pushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">Competition-level code generation with alphacode.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Science</em>, 378:1092 – 1097, 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:246527904" title="">https://api.semanticscholar.org/CorpusID:246527904</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke&nbsp;E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul&nbsp;Francis Christiano, Jan Leike, and Ryan&nbsp;J. Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">ArXiv</em>, abs/2203.02155, 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:246426909" title="">https://api.semanticscholar.org/CorpusID:246426909</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roller et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston.

</span>
<span class="ltx_bibblock">Hash layers for large sparse models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Neural Information Processing Systems</em>, 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:235367626" title="">https://api.semanticscholar.org/CorpusID:235367626</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rozière et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, I.&nbsp;Evtimov, Joanna Bitton, Manish&nbsp;P Bhatt, Cristian&nbsp;Cantón Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D’efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.

</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">ArXiv</em>, abs/2308.12950, 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:261100919" title="">https://api.semanticscholar.org/CorpusID:261100919</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rusu et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andrei&nbsp;A. Rusu, Neil&nbsp;C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.

</span>
<span class="ltx_bibblock">Progressive neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">ArXiv</em>, abs/1606.04671, 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:15350923" title="">https://api.semanticscholar.org/CorpusID:15350923</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Communications of the ACM</em>, 64(9):99–106, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.

</span>
<span class="ltx_bibblock">Socialiqa: Commonsense reasoning about social interactions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:1904.09728</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhihong Shao, Peiyi Wang, Qihao Zhu, R.&nbsp;X. Xu, Jun-Mei Song, Mingchuan Zhang, Y.&nbsp;K. Li, Yu&nbsp;Wu, and Daya Guo.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">ArXiv</em>, abs/2402.03300, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:267412607" title="">https://api.semanticscholar.org/CorpusID:267412607</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Noam&nbsp;M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc&nbsp;V. Le, Geoffrey&nbsp;E. Hinton, and Jeff Dean.

</span>
<span class="ltx_bibblock">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">ArXiv</em>, abs/1701.06538, 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:12462234" title="">https://api.semanticscholar.org/CorpusID:12462234</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian&nbsp;Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wortsman et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mitchell Wortsman, Gabriel Ilharco, Samir&nbsp;Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari&nbsp;S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt.

</span>
<span class="ltx_bibblock">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">ArXiv</em>, abs/2203.05482, 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:247362886" title="">https://api.semanticscholar.org/CorpusID:247362886</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You.

</span>
<span class="ltx_bibblock">Openmoe: An early effort on open mixture-of-experts language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2402.01739</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sixin Zhang, Anna&nbsp;E Choromanska, and Yann LeCun.

</span>
<span class="ltx_bibblock">Deep learning with elastic averaging sgd.

</span>
<span class="ltx_bibblock">In C.&nbsp;Cortes, N.&nbsp;Lawrence, D.&nbsp;Lee, M.&nbsp;Sugiyama, and R.&nbsp;Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Advances in Neural Information Processing Systems</em>, volume&nbsp;28. Curran Associates, Inc., 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2015/file/d18f655c3fce66ca401d5f38b48c89af-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2015/file/d18f655c3fce66ca401d5f38b48c89af-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona&nbsp;T. Diab, Xian Li, Xi&nbsp;Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit&nbsp;Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">ArXiv</em>, abs/2205.01068, 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:248496292" title="">https://api.semanticscholar.org/CorpusID:248496292</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jun Zhao, Zhihao Zhang, Qi&nbsp;Zhang, Tao Gui, and Xuanjing Huang.

</span>
<span class="ltx_bibblock">Llama beyond english: An empirical study on language capability transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2401.01055</em>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<span class="ltx_ERROR undefined" id="id2" lang="en">\beginappendix</span>
<section class="ltx_section" id="S8" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Data mixture</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S8.T7" title="Table 7 ‣ 8 Data mixture ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Table&nbsp;7</span></a> shows the exact data mixture ratios used in training each domain expert.
For finetuning the MoE model, we sample datasets that used to train math expert, code expert, wikipedia expert and the original <span class="ltx_text ltx_font_smallcaps" id="S8.p1.1.1">Llama-2 7B</span> with probabilities 30.16%, 40.31%, 10.30% and 19.23%.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S8.T7">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S8.T7.2">
<tbody><tr class="ltx_tr" id="S8.T7.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S8.T7.2.1.1"><span class="ltx_text" id="S8.T7.2.1.1.1" style="font-size:90%;">Domain</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S8.T7.2.1.2"><span class="ltx_text" id="S8.T7.2.1.2.1" style="font-size:90%;">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S8.T7.2.1.3"><span class="ltx_text" id="S8.T7.2.1.3.1" style="font-size:90%;">Sampling ratio (%)</span></td>
</tr>
<tr class="ltx_tr" id="S8.T7.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S8.T7.2.2.1" rowspan="5"><span class="ltx_text" id="S8.T7.2.2.1.1" style="font-size:90%;">Math</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S8.T7.2.2.2"><span class="ltx_text" id="S8.T7.2.2.2.1" style="font-size:90%;">AlgebraicStack</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.2.2.3"><span class="ltx_text" id="S8.T7.2.2.3.1" style="font-size:90%;">13.57</span></td>
</tr>
<tr class="ltx_tr" id="S8.T7.2.3">
<td class="ltx_td ltx_align_left" id="S8.T7.2.3.1"><span class="ltx_text" id="S8.T7.2.3.1.1" style="font-size:90%;">OpenWebMath</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.2.3.2"><span class="ltx_text" id="S8.T7.2.3.2.1" style="font-size:90%;">54.27</span></td>
</tr>
<tr class="ltx_tr" id="S8.T7.2.4">
<td class="ltx_td ltx_align_left" id="S8.T7.2.4.1"><span class="ltx_text" id="S8.T7.2.4.1.1" style="font-size:90%;">Arxiv</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.2.4.2"><span class="ltx_text" id="S8.T7.2.4.2.1" style="font-size:90%;">27.14</span></td>
</tr>
<tr class="ltx_tr" id="S8.T7.2.5">
<td class="ltx_td ltx_align_left" id="S8.T7.2.5.1"><span class="ltx_text" id="S8.T7.2.5.1.1" style="font-size:90%;">Github</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.2.5.2"><span class="ltx_text" id="S8.T7.2.5.2.1" style="font-size:90%;">2.99</span></td>
</tr>
<tr class="ltx_tr" id="S8.T7.2.6">
<td class="ltx_td ltx_align_left" id="S8.T7.2.6.1"><span class="ltx_text" id="S8.T7.2.6.1.1" style="font-size:90%;">Commoncrawl</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.2.6.2"><span class="ltx_text" id="S8.T7.2.6.2.1" style="font-size:90%;">5.01</span></td>
</tr>
<tr class="ltx_tr" id="S8.T7.2.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S8.T7.2.7.1" rowspan="3"><span class="ltx_text" id="S8.T7.2.7.1.1" style="font-size:90%;">Code</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S8.T7.2.7.2"><span class="ltx_text" id="S8.T7.2.7.2.1" style="font-size:90%;">Code</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.2.7.3"><span class="ltx_text" id="S8.T7.2.7.3.1" style="font-size:90%;">82.18</span></td>
</tr>
<tr class="ltx_tr" id="S8.T7.2.8">
<td class="ltx_td ltx_align_left" id="S8.T7.2.8.1"><span class="ltx_text" id="S8.T7.2.8.1.1" style="font-size:90%;">Natural language related to code</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.2.8.2"><span class="ltx_text" id="S8.T7.2.8.2.1" style="font-size:90%;">9.90</span></td>
</tr>
<tr class="ltx_tr" id="S8.T7.2.9">
<td class="ltx_td ltx_align_left" id="S8.T7.2.9.1"><span class="ltx_text" id="S8.T7.2.9.1.1" style="font-size:90%;">Natural language</span></td>
<td class="ltx_td ltx_align_center" id="S8.T7.2.9.2"><span class="ltx_text" id="S8.T7.2.9.2.1" style="font-size:90%;">6.93</span></td>
</tr>
<tr class="ltx_tr" id="S8.T7.2.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S8.T7.2.10.1" rowspan="2"><span class="ltx_text" id="S8.T7.2.10.1.1" style="font-size:90%;">Wikipedia</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S8.T7.2.10.2"><span class="ltx_text" id="S8.T7.2.10.2.1" style="font-size:90%;">Wikipedia</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S8.T7.2.10.3"><span class="ltx_text" id="S8.T7.2.10.3.1" style="font-size:90%;">90.91</span></td>
</tr>
<tr class="ltx_tr" id="S8.T7.2.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S8.T7.2.11.1"><span class="ltx_text" id="S8.T7.2.11.1.1" style="font-size:90%;">Commoncrawl</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S8.T7.2.11.2"><span class="ltx_text" id="S8.T7.2.11.2.1" style="font-size:90%;">9.09</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Data sources and weights for domain experts.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S9" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">We use the same evaluation metrics as is used in <cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib37" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Rozière et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#bib.bib31" title="">2023</a>)</cite>: for code tasks (HumanEval and MBPP) we report pass@1, for math tasks (GSM8k and MATH) and knowledge tasks (Natural Questions and TriviaQA) we report exact match, we report accuracy for MMLU and ARC. We use greedy decoding for all generations. Detailed results on all tasks are reported in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2403.07816v1#S9.T8" title="Table 8 ‣ 9 Evaluation ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">Table&nbsp;8</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S9.T8">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S9.T8.2">
<tbody><tr class="ltx_tr" id="S9.T8.2.1">
<td class="ltx_td ltx_border_tt" id="S9.T8.2.1.1" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.2.1" style="font-size:80%;">GSM8K</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.3.1" style="font-size:80%;">MATH</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.4.1" style="font-size:80%;">Human</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.5.1" style="font-size:80%;">MBPP</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.6.1" style="font-size:80%;">Natural</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.7.1" style="font-size:80%;">Trivia</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.8.1" style="font-size:80%;">ARC-e</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.9.1" style="font-size:80%;">ARC-c</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.10.1" style="font-size:80%;">Wino</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.11.1" style="font-size:80%;">SIQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.12.1" style="font-size:80%;">PIQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S9.T8.2.1.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.1.13.1" style="font-size:80%;">MMLU</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.2">
<td class="ltx_td" id="S9.T8.2.2.1" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td" id="S9.T8.2.2.2" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td" id="S9.T8.2.2.3" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.2.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.2.4.1" style="font-size:80%;">Eval</span></td>
<td class="ltx_td" id="S9.T8.2.2.5" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.2.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.2.6.1" style="font-size:80%;">Questions</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.2.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.2.7.1" style="font-size:80%;">QA</span></td>
<td class="ltx_td" id="S9.T8.2.2.8" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td" id="S9.T8.2.2.9" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td" id="S9.T8.2.2.10" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td" id="S9.T8.2.2.11" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td" id="S9.T8.2.2.12" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td" id="S9.T8.2.2.13" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S9.T8.2.3.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_italic" id="S9.T8.2.3.1.1" style="font-size:80%;">Specialized LLMs</span></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.2" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.3" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.4" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.5" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.6" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.7" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.8" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.9" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.10" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.11" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.12" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.3.13" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.4">
<td class="ltx_td ltx_align_left" id="S9.T8.2.4.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_smallcaps" id="S9.T8.2.4.1.1" style="font-size:80%;">CodeLlama 7B</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.2.1" style="font-size:80%;">13.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.3.1" style="font-size:80%;">3.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.4.1" style="font-size:80%;">31.1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.5.1" style="font-size:80%;">41.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.6.1" style="font-size:80%;">11.5</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.7.1" style="font-size:80%;">32.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.8.1" style="font-size:80%;">67.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.9.1" style="font-size:80%;">34.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.10.1" style="font-size:80%;">62.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.11.1" style="font-size:80%;">46.1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.12.1" style="font-size:80%;">72.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.4.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.4.13.1" style="font-size:80%;">38.6</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.5">
<td class="ltx_td ltx_align_left" id="S9.T8.2.5.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_smallcaps" id="S9.T8.2.5.1.1" style="font-size:80%;">Llemma 7B</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.2.1" style="font-size:80%;">39.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.3.1" style="font-size:80%;">16.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.4.1" style="font-size:80%;">25.6</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.5.1" style="font-size:80%;">41.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.6.1" style="font-size:80%;">9.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.7.1" style="font-size:80%;">24.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.8.1" style="font-size:80%;">28.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.9.1" style="font-size:80%;">26.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.10.1" style="font-size:80%;">50.1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.11.1" style="font-size:80%;">37.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.12.1" style="font-size:80%;">51.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.5.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.5.13.1" style="font-size:80%;">33.5</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S9.T8.2.6.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_italic" id="S9.T8.2.6.1.1" style="font-size:80%;">Generalist LLMs</span></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.2" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.3" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.4" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.5" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.6" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.7" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.8" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.9" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.10" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.11" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.12" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
<td class="ltx_td ltx_border_t" id="S9.T8.2.6.13" style="padding-left:2.8pt;padding-right:2.8pt;"></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.7">
<td class="ltx_td ltx_align_left" id="S9.T8.2.7.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_smallcaps" id="S9.T8.2.7.1.1" style="font-size:80%;">Llama-2 7B</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.2.1" style="font-size:80%;">14.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.3.1" style="font-size:80%;">2.5</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.4.1" style="font-size:80%;">12.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.5.1" style="font-size:80%;">20.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.6.1" style="font-size:80%;">16.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.7.1" style="font-size:80%;">58.5</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.8.1" style="font-size:80%;">76.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.9.1" style="font-size:80%;">43.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.10.1" style="font-size:80%;">69.2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.11.1" style="font-size:80%;">48.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.12.1" style="font-size:80%;">78.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.7.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.7.13.1" style="font-size:80%;">46.1</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.8">
<td class="ltx_td ltx_align_left" id="S9.T8.2.8.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_smallcaps" id="S9.T8.2.8.1.1" style="font-size:80%;">Llama-2 13B</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.2.1" style="font-size:80%;">28.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.3.1" style="font-size:80%;">3.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.4.1" style="font-size:80%;">18.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.5.1" style="font-size:80%;">30.6</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.6.1" style="font-size:80%;">16.1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.7.1" style="font-size:80%;">63.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.8.1" style="font-size:80%;">77.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.9.1" style="font-size:80%;">49.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.10.1" style="font-size:80%;">73.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.11.1" style="font-size:80%;">50.1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.12.1" style="font-size:80%;">80.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.8.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.8.13.1" style="font-size:80%;">52.8</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.9">
<td class="ltx_td ltx_align_left" id="S9.T8.2.9.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.1.1" style="font-size:80%;">Dense (DM)</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.2.1" style="font-size:80%;">26.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.3.1" style="font-size:80%;">9.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.4.1" style="font-size:80%;">20.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.5.1" style="font-size:80%;">30.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.6.1" style="font-size:80%;">24.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.7.1" style="font-size:80%;">55.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.8.1" style="font-size:80%;">76.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.9.1" style="font-size:80%;">44.5</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.10.1" style="font-size:80%;">68.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.11.1" style="font-size:80%;">48.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.12.1" style="font-size:80%;">78.2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.9.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.9.13.1" style="font-size:80%;">49.8</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.10">
<td class="ltx_td ltx_align_left" id="S9.T8.2.10.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.1.1" style="font-size:80%;">Sparse upcycling (DM), Top-2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.2.1" style="font-size:80%;">37.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.3.1" style="font-size:80%;">18.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.4.1" style="font-size:80%;">29.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.5.1" style="font-size:80%;">40.2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.6.1" style="font-size:80%;">18.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.7.1" style="font-size:80%;">49.2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.8.1" style="font-size:80%;">76.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.9.1" style="font-size:80%;">43.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.10.1" style="font-size:80%;">66.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.11.1" style="font-size:80%;">47.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.12.1" style="font-size:80%;">77.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.10.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.10.13.1" style="font-size:80%;">51.1</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.11">
<td class="ltx_td ltx_align_left" id="S9.T8.2.11.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.1.1" style="font-size:80%;">Sparse upcycling (CM), Top-2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.2.1" style="font-size:80%;">40.1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.3.1" style="font-size:80%;">16.2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.4.1" style="font-size:80%;">26.2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.5.1" style="font-size:80%;">35.2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.6.1" style="font-size:80%;">24.5</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.7.1" style="font-size:80%;">58.2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.8.1" style="font-size:80%;">75.6</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.9.1" style="font-size:80%;">44.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.10.1" style="font-size:80%;">69.1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.11.1" style="font-size:80%;">47.1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.12.1" style="font-size:80%;">78.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.11.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.11.13.1" style="font-size:80%;">52.1</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.12">
<td class="ltx_td ltx_align_left" id="S9.T8.2.12.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.1.1" style="font-size:80%;">BTM, Top-1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.2.1" style="font-size:80%;">27.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.3.1" style="font-size:80%;">15.2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.4.1" style="font-size:80%;">30.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.5.1" style="font-size:80%;">41.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.6.1" style="font-size:80%;">15.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.7.1" style="font-size:80%;">38.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.8.1" style="font-size:80%;">72.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.9.1" style="font-size:80%;">38.1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.10.1" style="font-size:80%;">68.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.11.1" style="font-size:80%;">47.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.12.1" style="font-size:80%;">77.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.12.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.12.13.1" style="font-size:80%;">44.3</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.13">
<td class="ltx_td ltx_align_left" id="S9.T8.2.13.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.1.1" style="font-size:80%;">BTM, Top-2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.2.1" style="font-size:80%;">27.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.3.1" style="font-size:80%;">15.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.4.1" style="font-size:80%;">30.6</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.5.1" style="font-size:80%;">42.6</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.6.1" style="font-size:80%;">15.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.7.1" style="font-size:80%;">38.5</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.8.1" style="font-size:80%;">73.1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.9.1" style="font-size:80%;">38.5</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.10.1" style="font-size:80%;">68.3</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.11.1" style="font-size:80%;">48.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.12.1" style="font-size:80%;">78.1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.13.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.13.13.1" style="font-size:80%;">44.3</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.14">
<td class="ltx_td ltx_align_left" id="S9.T8.2.14.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.1.1" style="font-size:80%;">BTX, sample Top-1</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.2.1" style="font-size:80%;">36.9</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.3.1" style="font-size:80%;">15.8</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.4.1" style="font-size:80%;">25.6</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.5.1" style="font-size:80%;">37.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.6.1" style="font-size:80%;">23.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.7.1" style="font-size:80%;">56.4</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.8.1" style="font-size:80%;">76.7</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.9.1" style="font-size:80%;">45.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.10.1" style="font-size:80%;">70.6</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.11.1" style="font-size:80%;">48.0</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.12.1" style="font-size:80%;">78.2</span></td>
<td class="ltx_td ltx_align_center" id="S9.T8.2.14.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.14.13.1" style="font-size:80%;">53.2</span></td>
</tr>
<tr class="ltx_tr" id="S9.T8.2.15">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S9.T8.2.15.1" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.1.1" style="font-size:80%;">BTX, Top-2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.2" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.2.1" style="font-size:80%;">37.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.3.1" style="font-size:80%;">17.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.4.1" style="font-size:80%;">28.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.5" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.5.1" style="font-size:80%;">39.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.6.1" style="font-size:80%;">24.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.7.1" style="font-size:80%;">57.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.8" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.8.1" style="font-size:80%;">76.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.9.1" style="font-size:80%;">45.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.10" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.10.1" style="font-size:80%;">67.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.11" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.11.1" style="font-size:80%;">48.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.12" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.12.1" style="font-size:80%;">78.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S9.T8.2.15.13" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="S9.T8.2.15.13.1" style="font-size:80%;">52.5</span></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S9.T8.5.1.1" style="font-size:113%;">Table 8</span>: </span><span class="ltx_text" id="S9.T8.6.2" style="font-size:113%;">Individual task performance of BTX and baselines.
</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S10" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Routing analysis</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">Layer-by-layer comparison of the routing decision for different router designs and downstream tasks aggregated by task domain is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S10.F4" title="Figure 4 ‣ 10 Routing analysis ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">4</span></a>. Routing distributions slightly vary in the first few layers, but quickly become indistinguishable from layer to layer. One exception is in Switch routing where Math expert becomes dominant across tasks in the last model layer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S10.p2">
<p class="ltx_p" id="S10.p2.1">We observe that Code expert is a dominant force in Code domain in Top-2 routing with load balancing. Note the difference with other models where load balancing is not added, and Math expert prevails across domains. We look at Code domain closer and compare routing probability distribution for models with and without load balancing in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S10.F5" title="Figure 5 ‣ 10 Routing analysis ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">5</span></a>. On the bottom three graphs of the picture we can observe a phenomena of the dead expert, where routing probability to Code expert shifted to <math alttext="0" class="ltx_Math" display="inline" id="S10.p2.1.m1.1"><semantics id="S10.p2.1.m1.1a"><mn id="S10.p2.1.m1.1.1" xref="S10.p2.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S10.p2.1.m1.1b"><cn id="S10.p2.1.m1.1.1.cmml" type="integer" xref="S10.p2.1.m1.1.1">0</cn></annotation-xml></semantics></math>, while with load balancing added, probability distributions across experts look more similar, with slightly higher expectations for the Code expert.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S10.p3">
<p class="ltx_p" id="S10.p3.1">To understand if experts specialize in other domains, we look closer at per-task distribution. Routing decision of the tokens in Math and Reasoning domains are shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.07816v1#S10.F6" title="Figure 6 ‣ 10 Routing analysis ‣ Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM"><span class="ltx_text ltx_ref_tag">6</span></a>. We observe that GSM8K task prefers Code and <span class="ltx_text ltx_font_smallcaps" id="S10.p3.1.1">Llama-2</span> experts, while Math task more relies on in-domain expert. We hypothesise that this happens because GSM8K dataset consists of grade school math word problems that require common sense knowledge and basic arithmetic operations, while Math task requires college-level math knowledge, and more aligned with Math expert’s training data. In the Reasoning domain, all tasks exhibit similar behaviour and equally rely on Math
and generalist LLM’s expertise.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S10.2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="345" id="S10.1.g1" src="extracted/5463999/figs/top2_lb_all.png" width="568"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="345" id="S10.2.g2" src="extracted/5463999/figs/top2_all.png" width="568"></div>
</div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S10.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="345" id="S10.F4.g1" src="extracted/5463999/figs/soft_top1_all.png" width="568"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="343" id="S10.F4.g2" src="extracted/5463999/figs/swith_all.png" width="568"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S10.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S10.F4.4.2" style="font-size:90%;">BTX routing decisions of the tokens at various layers to different experts (Wiki, Math, Code, <span class="ltx_text ltx_font_smallcaps" id="S10.F4.4.2.1">LLaMa-2 7B</span>) for different downstream tasks. The tasks are aggregated by domain: Code (Human Eval, MBPP), Math (GSM8K, MATH), World knowledge (Natural Questions, TriviaQA), and Reasoning (ARC-Easy, ARC-Challenge, SIQA, PIQA, and WinoGrande). We observe that top-2 routing with load balancing ensures more uniform distribution of the load between experts compared to the other routing methods across all layers.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S10.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="120" id="S10.F5.g1" src="extracted/5463999/figs/top2_lb_hist_0.png" width="479"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="120" id="S10.F5.g2" src="extracted/5463999/figs/top2_lb_hist_15.png" width="479"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="120" id="S10.F5.g3" src="extracted/5463999/figs/top2_lb_hist_31.png" width="479"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="120" id="S10.F5.g4" src="extracted/5463999/figs/BTX_top2_cs_hist_0.png" width="479"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="120" id="S10.F5.g5" src="extracted/5463999/figs/BTX_top2_cs_hist_15.png" width="479"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="120" id="S10.F5.g6" src="extracted/5463999/figs/BTX_top2_cs_hist_31.png" width="479"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S10.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S10.F5.3.2" style="font-size:90%;">Routing probabilities per expert across different layers for Human Eval task. We compare top-2 routing with (left) and without load balancing (right).</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S10.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="167" id="S10.F6.g1" src="extracted/5463999/figs/BTX_top2_math_3.png" width="479"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" height="170" id="S10.F6.g2" src="extracted/5463999/figs/BTX_top2_reasoning_3.png" width="479"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S10.F6.4.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S10.F6.5.2" style="font-size:90%;">Routing decision of the tokens in Math and Reasoning domains. We observe that GSM8K task prefers Code and <span class="ltx_text ltx_font_smallcaps" id="S10.F6.5.2.1">Llama-2</span> experts, while MATH task relies more on in-domain expert. In the Reasoning domain, the load is distributed between Math and <span class="ltx_text ltx_font_smallcaps" id="S10.F6.5.2.2">LLaMa-2 7B</span> experts.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
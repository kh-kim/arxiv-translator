<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.16819] Nemotron-4 15B Technical Report</title><meta property="og:description" content="We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Nemotron-4 15B Technical Report">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Nemotron-4 15B Technical Report">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.16819">

<!--Generated on Tue Mar  5 18:37:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on  %“date–February 2024˝ .-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Nemotron-4 15B Technical Report</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jupinder Parmar<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
Equal contribution, corresponding authors: <span id="footnote1.1" class="ltx_text ltx_font_typewriter">{jupinderp,sprabhumoye,jjennings,mpatwary}@nvidia.com</span>.
</span></span></span> &nbsp;&nbsp;Shrimai Prabhumoye<sup id="id4.4.id1" class="ltx_sup"><span id="id4.4.id1.1" class="ltx_text ltx_font_italic">∗</span></sup> &nbsp;&nbsp;Joseph Jennings<sup id="id5.5.id2" class="ltx_sup"><span id="id5.5.id2.1" class="ltx_text ltx_font_italic">∗</span></sup> &nbsp;&nbsp;Mostofa Patwary<sup id="id6.6.id3" class="ltx_sup"><span id="id6.6.id3.1" class="ltx_text ltx_font_italic">∗</span></sup> 
<br class="ltx_break">Sandeep Subramanian<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Work done while at NVIDIA.</span></span></span> &nbsp;&nbsp;Dan Su &nbsp;&nbsp;Chen Zhu &nbsp;&nbsp;Deepak Narayanan &nbsp;&nbsp; Aastha Jhunjhunwala &nbsp;&nbsp; Ayush Dattagupta &nbsp;&nbsp;Vibhu Jawa &nbsp;&nbsp;Jiwei Liu &nbsp;&nbsp;Ameya Mahabaleshwarkar &nbsp;&nbsp;Osvald Nitski &nbsp;&nbsp; Annika Brundyn &nbsp;&nbsp;James Maki &nbsp;&nbsp;Miguel Martinez &nbsp;&nbsp;Jiaxuan You &nbsp;&nbsp;John Kamalu &nbsp;&nbsp;Patrick LeGresley &nbsp;&nbsp;Denys Fridman &nbsp;&nbsp;Jared Casper &nbsp;&nbsp; Ashwath Aithal &nbsp;&nbsp;Oleksii Kuchaiev &nbsp;&nbsp;Mohammad Shoeybi &nbsp;&nbsp;Jonathan Cohen &nbsp;&nbsp;Bryan Catanzaro <span id="id7.7.id4" class="ltx_text ltx_font_bold">NVIDIA</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id8.id1">우리는 8조 개의 텍스트 토큰으로 훈련된 150억 개의 매개 변수 대형 다국어 언어 모델인 네모트론-4 15B를 소개한다. 네모트론-4 15B는 영어, 다국어 및 코딩 작업에 대해 평가될 때 강력한 성능을 보여주며, 이는 7개의 다운스트림 평가 영역 중 4개에서 기존의 유사한 크기의 모든 오픈 모델을 능가하고 나머지 영역에서 선두 오픈 모델과 경쟁 성능을 달성한다. 특히, 네모트론-4 15B는 유사하게 크기가 큰 모든 모델의 최상의 다국어 능력을 보여주며, 심지어 4배 이상 큰 모델과 다국어 작업에 명시적으로 특화된 모델을 능가한다.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section">1&nbsp;&nbsp;&nbsp;Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S1.p1.1">언어 모델 사전 훈련에서 최근 발표된 노력 <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et al., <a class="ltx_ref" href="#bib.bib21" title="">2022</a>; Touvron et al., <a class="ltx_ref" href="#bib.bib45" title="">2023a</a>, <a class="ltx_ref" href="#bib.bib46" title="">b</a>; Yang et al., <a class="ltx_ref" href="#bib.bib49" title="">2023</a>; Jiang et al., <a class="ltx_ref" href="#bib.bib23" title="">2023</a>)</cite>는 고정된 컴퓨팅 예산이 주어진 모델 크기와 함께 데이터를 스케일링한다고 주장하는 Chinchilla 스케일링 법칙 <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et al., <a class="ltx_ref" href="#bib.bib21" title="">2022</a>)</cite>에서 영감을 받아 모델 <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al., <a class="ltx_ref" href="#bib.bib24" title="">2020</a>; Brown et al., <a class="ltx_ref" href="#bib.bib7" title="">2020</a>; Smith et al., <a class="ltx_ref" href="#bib.bib42" title="">2022</a>; Rae et al., <a class="ltx_ref" href="#bib.bib33" title="">2022</a>; Scao et al., <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>의 크기만 스케일링한 과거 작업과 비교된다. 예를 들어, <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et al., <a class="ltx_ref" href="#bib.bib21" title="">2022</a>)</cite>는 유사한 데이터 분포를 가진 두 개의 대략적인 IsoFLOP GPT 모델, 1.4조 토큰의 650억 매개 변수 모델 및 3,000억 토큰의 280억 매개 변수 모델이 주어진 경우 65B 모델이 다운스트림 태스크에서 더 나은 정확도를 갖는다는 것을 보여준다.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S1.p2.1">모델 크기를 늘리는 대신 더 많은 데이터에 대한 훈련에 컴퓨팅을 할당하는 이러한 트레이드오프는 추론 관점에서 특히 매력적이며, 지연 시간과 모델을 서비스하는 데 필요한 컴퓨팅의 양을 줄인다. 그 결과 언어 모델링 훈련 노력의 주요 초점은 커먼 크롤과 같은 공개 소스에서 고품질 수십조 토큰 데이터 세트를 수집하는 것으로 전환되었다. 우리는 영어, 다국어, 코딩 텍스트의 8조 토큰에 대해 훈련되었고 단일 NVIDIA A100 또는 H100 GPU에 적합할 수 있는 최고의 범용 대용량 언어 모델(LLM)로 개발된 네모트론-4 15B를 도입하여 이러한 추세를 지속한다.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S1.p3.1">도표 <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">1</span></a>에서 입증된 바와 같이, 네모트론-4 15B는 광범위한 영어, 코드 및 다국어 평가 영역에 걸쳐 높은 다운스트림 정확도를 나타낸다. 유사한 크기의 개방형 모델에 비해 모든 영어 평가 영역에서 Nemotron-4 15B가 LLaMA-2 34B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="#bib.bib46" title="">2023b</a>)</cite>보다 두 배 이상의 매개 변수를 가지며, Mistral 7B <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="#bib.bib23" title="">2023</a>)</cite>보다 우수함을 보인다. 추가적으로, Nemotron-4 15B는 QWEN 14B <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="#bib.bib5" title="">2023</a>)</cite> 및 Gemma 7B <cite class="ltx_cite ltx_citemacro_citep">(Gemma Team, <a class="ltx_ref" href="#bib.bib17" title="">2024</a>)</cite>에 대한 경쟁적 정확도를 달성한다. 광범위한 프로그래밍 언어에 걸친 비교에서, 우리는 Nemotron-4 15B가 스타코더 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="#bib.bib27" title="">2023</a>)</cite>, 코드 특정 모델 및 Mistral 7B보다 특히 낮은 리소스 프로그래밍 언어에서 더 나은 평균 정확도를 달성한다는 것을 발견했다. 네모트론-4 15B는 상당한 양의 다국어 데이터에 대해 훈련되었기 때문에 현재 모든 다국어 벤치마크에 대한 크기 클래스에서 최첨단 범용 모델이다. 네모트론-4가 PALM 62B-Cont<cite class="ltx_cite ltx_citemacro_citep">(Slav Petrov and et al., <a class="ltx_ref" href="#bib.bib41" title="">2023</a>)</cite>보다 우수하다는 것을 알 수 있었고, XGLM<cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="#bib.bib28" title="">2022</a>)</cite>와 mGPT<cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al., <a class="ltx_ref" href="#bib.bib39" title="">2022</a>)</cite>와 같은 다국어 특화 모델보다 우수하다는 것을 알 수 있었다.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2402.16819/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="284" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2402.16819/assets/x2.png" id="S1.F1.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="287" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.4.1.1" style="font-size:90%;">Figure 1</span>:</span><span class="ltx_text" id="S1.F1.5.2" style="font-size:90%;">Comparison of Nemotron-4 15B across seven evaluation areas against similarly sized models. 각 평가 영역을 형성하는 과제의 구성은, 보다 상세한 평가 결과와 함께, Section <a class="ltx_ref" href="#S3" title="3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">3</span></a></span>에서 찾을 수 있다.</figcaption>
</figure>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.2.1.1" class="ltx_tr">
<td id="S1.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Number of</td>
<td id="S1.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Hidden</td>
<td id="S1.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Number of</td>
<td id="S1.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Number of</td>
<td id="S1.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">Sequence</td>
<td id="S1.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Vocabulary</td>
</tr>
<tr id="S1.T1.2.2.2" class="ltx_tr">
<td id="S1.T1.2.2.2.1" class="ltx_td ltx_align_center">transformer layers</td>
<td id="S1.T1.2.2.2.2" class="ltx_td ltx_align_center">dimension</td>
<td id="S1.T1.2.2.2.3" class="ltx_td ltx_align_center">attention heads</td>
<td id="S1.T1.2.2.2.4" class="ltx_td ltx_align_center">KV heads</td>
<td id="S1.T1.2.2.2.5" class="ltx_td ltx_align_center">length</td>
<td id="S1.T1.2.2.2.6" class="ltx_td ltx_align_center">size</td>
</tr>
<tr id="S1.T1.2.3.3" class="ltx_tr">
<td id="S1.T1.2.3.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">32</td>
<td id="S1.T1.2.3.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">6144</td>
<td id="S1.T1.2.3.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">48</td>
<td id="S1.T1.2.3.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">8</td>
<td id="S1.T1.2.3.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">4096</td>
<td id="S1.T1.2.3.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">256,000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S1.T1.3.1.1" style="font-size:90%;">Table 1</span>:</span><span class="ltx_text" id="S1.T1.4.2" style="font-size:90%;">Key hyper-parameters affecting size of Nemotron-4 15B. </span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section">2&nbsp;&nbsp;&nbsp;Architecture Details</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.p1.1">네모트론-4는 인과적 주의 마스크와 함께 표준 디코더 전용 트랜스포머 아키텍처 <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="#bib.bib47" title="">2017</a>)</cite>를 사용한다. 크기에 영향을 미치는 정확한 하이퍼-파라미터는 표 <a class="ltx_ref" href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">1</span></a>와 같다. 네모트론-4는 32억 개의 임베딩 파라미터와 125억 개의 비 임베딩 파라미터를 갖는다. Rotary Position Embeddings (RoPE) <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a class="ltx_ref" href="#bib.bib43" title="">2021</a>)</cite>, SentencePiece tokenizer <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a class="ltx_ref" href="#bib.bib26" title="">2018</a>)</cite>, MLP 계층에서 제곱 ReLU 활성화, 바이어스 항 없음, 드롭아웃 비율 0, 언타이징 입력-출력 임베딩을 사용한다. 더 빠른 추론을 위해 그룹화된 질의어텐션(GQA) <cite class="ltx_cite ltx_citemacro_citep">(Ainslie et al., <a class="ltx_ref" href="#bib.bib2" title="">2023</a>)</cite>를 사용한다.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">우리는 8조 개의 토큰으로 구성된 사전 훈련 데이터 세트에서 네모트론-4 15B를 훈련한다. 높은 수준에서 데이터 혼합은 영어 자연어 데이터(70%), 다국어 자연어 데이터(15%), 소스 코드 데이터(15%)의 세 가지 다른 유형의 데이터로 분할된다.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2402.16819/assets/plots/new_full_distr.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="280" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">그림 2</span>:</span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Pre-training에 사용된 English tokens의 Data composition of the pre-training</span></figcaption>
</figure>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">영어 코퍼스는 웹 문서, 뉴스 기사, 과학 논문, 책 등을 포함한 다양한 출처와 도메인의 선별된 문서로 구성되며 사전 훈련 세트에 사용된 분포는 그림 <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">2</span></a>에서 강조 표시된다. 코드 및 다국어 데이터는 다양한 자연 언어 및 프로그래밍 언어 집합으로 구성된다. 우리는 이러한 언어에서 토큰을 적절하게 샘플링하는 것이 이러한 도메인에서 강력한 정확도의 핵심이라는 것을 발견했다. 우리는 그림 <a class="ltx_ref" href="#S2.F3" title="Figure 3 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">3</span></a> 및 그림 <a class="ltx_ref" href="#S2.F4" title="Figure 4 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">4</span></a>의 사전 훈련 데이터 세트에서 코드 및 다국어 토큰 모두에 사용된 분포를 각각 공유한다.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2402.16819/assets/plots/test_code.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>:</span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Pre-training에 사용된 43개의 프로그래밍 언어의 데이터 배포. 각 막대 내의 숫자는 개별 언어가 구성하는 전체 코드 분포의 백분율을 나타냅니다. </span></figcaption>
</figure>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p3.1">사전 학습 코퍼스를 구성할 때 문서 수준의 정확도와 근중복제거 <cite class="ltx_cite ltx_citemacro_citep">(Jennings et al., <a class="ltx_ref" href="#bib.bib22" title="">2023</a>)</cite>를 통해 가능한 중복을 제거한다. 우리는 <cite class="ltx_cite ltx_citemacro_citep">(Rae et al., <a class="ltx_ref" href="#bib.bib33" title="">2022</a>)</cite> 및 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="#bib.bib34" title="">2020</a>)</cite>에 설명된 일련의 휴리스틱 필터 외에도 <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et al., <a class="ltx_ref" href="#bib.bib48" title="">2019</a>)</cite>와 유사한 언어 모델 기반 필터링 접근법을 사용하여 말뭉치에 문서 수준 품질 필터링을 추가로 적용했다.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p4.1">최종 8T 토큰 데이터 세트에서 무작위로 샘플링된 데이터에 대해 SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a class="ltx_ref" href="#bib.bib26" title="">2018</a>)</cite>에서 BPE 토큰화기를 학습한다. 토큰나이저에서 낮은 리소스 언어의 적용 범위를 높이기 위해 최종 훈련 데이터 세트 분포에 비해 영어가 아닌 데이터를 업샘플링한다. 토키나이저는 화이트 스페이스(선행 및 후행 포함)를 보존하고, 숫자를 개별 숫자로 분할합니다. <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>)</cite> 그리고 바이트 수준 백오프에 의존하여 알려지지 않은 문자 시퀀스를 처리합니다. 최종 어휘 크기는 256,000 토큰입니다.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2402.16819/assets/plots/test_multi.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="274" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>:</span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">Data distribution of the 53 natural languages, aside from English,we used for pre-training. 각 막대 내의 숫자는 개별 언어가 구성하는 전체 다국어 분포의 백분율을 나타냅니다. </span></figcaption>
</figure>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Pre-training.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Nemotron-4는 384개의 DGX H100 노드를 사용하여 훈련되었으며, 각 노드는 NVIDIA Hopper 아키텍쳐 <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA, <a class="ltx_ref" href="#bib.bib29" title="">2022</a>)</cite>를 기반으로 8개의 H100 80GB SXM5 GPU를 포함한다. 각 H100 GPU는 희소성이 없는 16비트 부동 소수점(<span class="ltx_text ltx_font_typewriter" id="S2.SS0.SSS0.Px2.p1.1.1">bfloat16</span>) 산술을 수행할 때 989 teraFLOP/s의 피크 처리량을 갖는다. 각 노드 내에서 GPU는 NVLink와 NVSwitch<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="#bib.bib1" title="">nvl, </a>)</cite>로 연결 되며 GPU 대 GPU 대역폭은 900 GB/s (각 방향으로 450 GB/s)입니다. 각 노드에는 노드 간 통신을 위한 8개의 NVIDIA Mellanox 400 Gbps HDR InfiniBand 호스트 채널 어댑터(HCAs)가 있다.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">8-way tensor parallelism <cite class="ltx_cite ltx_citemacro_citep">(Shoeybi et al., <a class="ltx_ref" href="#bib.bib40" title="">2019</a>)</cite>와 data parallelism의 조합을 사용하여 모델을 학습시켰으며, 분산 최적화기를 사용하여 데이터-병렬 복제본에 대한 최적화기 상태를 분할하였다. 데이터 병렬성의 정도는 배치 크기가 증가함에 따라 96에서 384까지 다양했다. 표 <a class="ltx_ref" href="#S2.T2" title="Table 2 ‣ Pre-training. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">2</span></a>는 배치 크기 램프의 3단계를 요약하고, 반복당 시간 및 모델 FLOP/s 활용률(MFU) <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>; Korthikanti et al., <a class="ltx_ref" href="#bib.bib25" title="">2022</a>)</cite>를 포함한다. MFU는 GPU가 모델 훈련에서 얼마나 효율적으로 활용되는지를 정량화한다. 교육은 약 13일 만에 완료되었다.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.2.1.1" class="ltx_tr">
<th id="S2.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Data-parallel size</th>
<th id="S2.T2.2.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">GPUs</th>
<th id="S2.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Iteration time (secs)</th>
<th id="S2.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MFU (%)</th>
<th id="S2.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Batch size</th>
<th id="S2.T2.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Tokens (B)</th>
<th id="S2.T2.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Time (days)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.2.2.1" class="ltx_tr">
<td id="S2.T2.2.2.1.1" class="ltx_td ltx_align_center ltx_border_tt">96</td>
<td id="S2.T2.2.2.1.2" class="ltx_td ltx_align_right ltx_border_tt">768</td>
<td id="S2.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">0.57</td>
<td id="S2.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">34.3</td>
<td id="S2.T2.2.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">384</td>
<td id="S2.T2.2.2.1.6" class="ltx_td ltx_align_center ltx_border_tt">200</td>
<td id="S2.T2.2.2.1.7" class="ltx_td ltx_align_center ltx_border_tt">0.8</td>
</tr>
<tr id="S2.T2.2.3.2" class="ltx_tr">
<td id="S2.T2.2.3.2.1" class="ltx_td ltx_align_center">192</td>
<td id="S2.T2.2.3.2.2" class="ltx_td ltx_align_right">1,536</td>
<td id="S2.T2.2.3.2.3" class="ltx_td ltx_align_center">0.58</td>
<td id="S2.T2.2.3.2.4" class="ltx_td ltx_align_center">33.3</td>
<td id="S2.T2.2.3.2.5" class="ltx_td ltx_align_center">768</td>
<td id="S2.T2.2.3.2.6" class="ltx_td ltx_align_center">200</td>
<td id="S2.T2.2.3.2.7" class="ltx_td ltx_align_center">0.4</td>
</tr>
<tr id="S2.T2.2.4.3" class="ltx_tr">
<td id="S2.T2.2.4.3.1" class="ltx_td ltx_align_center ltx_border_bb">288</td>
<td id="S2.T2.2.4.3.2" class="ltx_td ltx_align_right ltx_border_bb">2,304</td>
<td id="S2.T2.2.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">0.64</td>
<td id="S2.T2.2.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">30.5</td>
<td id="S2.T2.2.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">1,152</td>
<td id="S2.T2.2.4.3.6" class="ltx_td ltx_align_center ltx_border_bb">7,600</td>
<td id="S2.T2.2.4.3.7" class="ltx_td ltx_align_center ltx_border_bb">11.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.3.1.1" style="font-size:90%;">Table 2</span>:</span><span class="ltx_text" id="S2.T2.4.2" style="font-size:90%;">Batch size rampup schedule, along time and efficiency metrics for the Nemotron-4 15B parameter model. </span></figcaption>
</figure>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Continued Training.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">최근 작업 <cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="#bib.bib18" title="">2023</a>)</cite>와 유사하게 모델 트레이닝이 끝날 때 데이터 분포와 학습률 감소 스케줄을 전환하면 모델 품질이 크게 향상됨을 알 수 있다. 구체적으로, 8T 사전 훈련 데이터 세트 전체에 대해 훈련한 후 동일한 손실 목표를 사용하고 사전 훈련 토큰과 비교하여 적은 수의 토큰에 대해 지속적인 훈련을 수행한다.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p2.1">계속된 훈련의 이 추가 단계에서 우리는 두 가지 별개의 데이터 분포를 활용한다. 첫 번째 분포는 지속적인 훈련 동안 대부분의 토큰이 샘플링되는 곳입니다. 사전 훈련 중에 이미 도입되었지만 더 높은 품질의 소스에 더 큰 샘플링 가중치를 두는 분포와 함께 토큰을 활용한다. 두 번째 분포는 모델이 다운스트림 평가에서 이러한 질문에 더 잘 응답할 수 있도록 하는 동시에 모델 성능이 낮은 영역에서 나오는 데이터 소스를 가중화할 수 있도록 소수의 벤치마크 스타일 정렬 예를 도입한다. 학습 속도의 크기보다 더 가파른 감쇠 기울기를 우선시하는 학습 속도 스케줄과 함께, 이러한 데이터 분포의 순서와 스타일이 모델이 사전 훈련 데이터 세트로부터 부드럽게 전이하고 새로 강조된 데이터 영역을 더 잘 학습할 수 있게 한다는 것을 발견한다.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section">3&nbsp;&nbsp;&nbsp;Results</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.p1.1">우리는 다양한 범위의 작업과 영역을 포함하는 다양한 다운스트림 평가 영역에서 네모트론-4 15B를 평가한다. 모든 평가에서 표준화된 작업 설정을 준수하고 사용된 정확한 설정을 공유합니다. 상기 커버링된 평가 카테고리들은,</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Commonsense Reasoning (0-shot):</span> SIQA <cite class="ltx_cite ltx_citemacro_citep">(Sap et al., <a class="ltx_ref" href="#bib.bib36" title="">2019</a>)</cite>, ARC easy and challenge <cite class="ltx_cite ltx_citemacro_citep">(Clark et al., <a class="ltx_ref" href="#bib.bib14" title="">2018</a>)</cite>, PIQA <cite class="ltx_cite ltx_citemacro_citep">(Bisk et al., <a class="ltx_ref" href="#bib.bib6" title="">2020</a>)</cite>, Winogrande <cite class="ltx_cite ltx_citemacro_citep">(Sakaguchi et al., <a class="ltx_ref" href="#bib.bib35" title="">2020</a>)</cite>, and Hellaswag <cite class="ltx_cite ltx_citemacro_citep">(Zellers et al., <a class="ltx_ref" href="#bib.bib50" title="">2019</a>)</cite></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Popular Aggregated Benchmarks:</span> MMLU (5-shot) <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="#bib.bib20" title="">2020</a>)</cite> and BBH (3-shot) <cite class="ltx_cite ltx_citemacro_citep">(Suzgun et al., <a class="ltx_ref" href="#bib.bib44" title="">2022</a>)</cite></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Math:</span> GSM8K (8-shot with maj@1) <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al., <a class="ltx_ref" href="#bib.bib15" title="">2021</a>)</cite></p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Code:</span> Pass@1 scores on HumanEval (0-shot) <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="#bib.bib11" title="">2021</a>)</cite>, MBPP (3-shot) <cite class="ltx_cite ltx_citemacro_citep">(Austin et al., <a class="ltx_ref" href="#bib.bib4" title="">2021</a>)</cite>, and MultiPL-E (0-shot) <cite class="ltx_cite ltx_citemacro_citep">(Cassano et al., <a class="ltx_ref" href="#bib.bib8" title="">2023a</a>)</cite></p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">Multilingual:</span> classification via XCOPA (0 and 4-shot) <cite class="ltx_cite ltx_citemacro_citep">(Ponti et al., <a class="ltx_ref" href="#bib.bib31" title="">2020</a>)</cite>, machine translation with FLORES-101 (8-shot) <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a class="ltx_ref" href="#bib.bib19" title="">2021</a>)</cite>, generation tasks such as MGSM (8-shot) <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="#bib.bib38" title="">2022</a>)</cite> and TyDiQA (1-shot) <cite class="ltx_cite ltx_citemacro_citep">(Clark et al., <a class="ltx_ref" href="#bib.bib13" title="">2020</a>)</cite></p>
</div>
</li>
</ul>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.p3.1">우리의 평가에서 우리는 많은 외부 디코더 전용 변압기 언어 모델과 비교하고 달리 명시되지 않는 한 해당 모델의 보고서에 게시된 번호를 사용한다. 영어 및 코드 작업의 경우 Nemotron-4 15B, LlaMA-2 13B 및 34B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="#bib.bib46" title="">2023b</a>)</cite>, Mistral 7B <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="#bib.bib23" title="">2023</a>)</cite>, Baichuan-2 13B <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib49" title="">2023</a>)</cite>, QWEN 14B <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="#bib.bib5" title="">2023</a>)</cite>, Gemma 7B <cite class="ltx_cite ltx_citemacro_citep">(Gemma Team, <a class="ltx_ref" href="#bib.bib17" title="">2024</a>)</cite>에 대한 자세한 결과를 공유한다. 다국어 벤치마크의 경우 PaLM 62B 및 62B-cont <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>)</cite>와 mGPT 13B <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al., <a class="ltx_ref" href="#bib.bib39" title="">2022</a>)</cite> 및 XGLM 7.5B <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="#bib.bib28" title="">2022</a>)</cite>와 같은 다국어 기능을 위해 특별히 훈련된 모델에 대한 결과를 보고한다.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Commonsense Reasoning</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS1.p1.1">LM-Evaluation Harness<cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="#bib.bib16" title="">2021</a>)</cite>를 사용하여 앞서 언급한 모든 작업에서 Nemotron-4 15B를 평가한다. 표 <a class="ltx_ref" href="#S3.T3" title="Table 3 ‣ 3.1 Commonsense Reasoning ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">3</span></a>는 네모트론-4 15B가 이 다양한 작업 집합에서 가장 강력한 평균 성능을 달성한다는 것을 보여준다.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:413.9pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S3.T3.1.1" class="ltx_p"><span id="S3.T3.1.1.1" class="ltx_text"> <span id="S3.T3.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:413.9pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"> <span id="S3.T3.1.1.1.1.1" class="ltx_p"><span id="S3.T3.1.1.1.1.1.1" class="ltx_text"> <span id="S3.T3.1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="S3.T3.1.1.1.1.1.1.1.2.1" class="ltx_tr"> <span id="S3.T3.1.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></span> <span id="S3.T3.1.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">Size</span> <span id="S3.T3.1.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SIQA</span> <span id="S3.T3.1.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ARC-c</span> <span id="S3.T3.1.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ARC-e</span> <span id="S3.T3.1.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PIQA</span> <span id="S3.T3.1.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Winogrande</span> <span id="S3.T3.1.1.1.1.1.1.1.2.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Hellaswag</span> <span id="S3.T3.1.1.1.1.1.1.1.2.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVG</span></span> </span> <span class="ltx_tbody"> <span id="S3.T3.1.1.1.1.1.1.1.3.1" class="ltx_tr"> <span id="S3.T3.1.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S3.T3.1.1.1.1.1.1.1.3.1.1.1" class="ltx_text">LLaMA-2</span></span> <span id="S3.T3.1.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</span> <span id="S3.T3.1.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">50.3</span> <span id="S3.T3.1.1.1.1.1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">49.4</span> <span id="S3.T3.1.1.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">77.3</span> <span id="S3.T3.1.1.1.1.1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">79.8</span> <span id="S3.T3.1.1.1.1.1.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">72.8</span> <span id="S3.T3.1.1.1.1.1.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">80.7</span> <span id="S3.T3.1.1.1.1.1.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">68.4</span></span> <span id="S3.T3.1.1.1.1.1.1.1.4.2" class="ltx_tr"> <span id="S3.T3.1.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">34B</span> <span id="S3.T3.1.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_center">50.9</span> <span id="S3.T3.1.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_center">54.5</span> <span id="S3.T3.1.1.1.1.1.1.1.4.2.4" class="ltx_td ltx_align_center">79.4</span> <span id="S3.T3.1.1.1.1.1.1.1.4.2.5" class="ltx_td ltx_align_center">81.9</span> <span id="S3.T3.1.1.1.1.1.1.1.4.2.6" class="ltx_td ltx_align_center">76.7</span> <span id="S3.T3.1.1.1.1.1.1.1.4.2.7" class="ltx_td ltx_align_center">83.3</span> <span id="S3.T3.1.1.1.1.1.1.1.4.2.8" class="ltx_td ltx_align_center">71.1</span></span> <span id="S3.T3.1.1.1.1.1.1.1.5.3" class="ltx_tr"> <span id="S3.T3.1.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T3.1.1.1.1.1.1.1.5.3.1.1" class="ltx_text">Baichuan-2</span></span> <span id="S3.T3.1.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</span> <span id="S3.T3.1.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S3.T3.1.1.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S3.T3.1.1.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S3.T3.1.1.1.1.1.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_t">78.1</span> <span id="S3.T3.1.1.1.1.1.1.1.5.3.7" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S3.T3.1.1.1.1.1.1.1.5.3.8" class="ltx_td ltx_align_center ltx_border_t">70.8</span> <span id="S3.T3.1.1.1.1.1.1.1.5.3.9" class="ltx_td ltx_align_center ltx_border_t">-</span></span> <span id="S3.T3.1.1.1.1.1.1.1.6.4" class="ltx_tr"> <span id="S3.T3.1.1.1.1.1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">QWEN</span> <span id="S3.T3.1.1.1.1.1.1.1.6.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">14B</span> <span id="S3.T3.1.1.1.1.1.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_t">77.9</span> <span id="S3.T3.1.1.1.1.1.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t">84.4</span> <span id="S3.T3.1.1.1.1.1.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_t">90.3</span> <span id="S3.T3.1.1.1.1.1.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_t">79.9</span> <span id="S3.T3.1.1.1.1.1.1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S3.T3.1.1.1.1.1.1.1.6.4.8" class="ltx_td ltx_align_center ltx_border_t">80.2</span> <span id="S3.T3.1.1.1.1.1.1.1.6.4.9" class="ltx_td ltx_align_center ltx_border_t">-</span></span> <span id="S3.T3.1.1.1.1.1.1.1.1" class="ltx_tr"> <span id="S3.T3.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Mistral</span> <span id="S3.T3.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</span> <span id="S3.T3.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">&nbsp;&nbsp;47.0<sup id="S3.T3.1.1.1.1.1.1.1.1.1.1" class="ltx_sup"><span id="S3.T3.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup></span> <span id="S3.T3.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">55.5</span> <span id="S3.T3.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">80.0</span> <span id="S3.T3.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">83.0</span> <span id="S3.T3.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">75.3</span> <span id="S3.T3.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t">81.3</span> <span id="S3.T3.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">70.4</span></span> <span id="S3.T3.1.1.1.1.1.1.1.7.5" class="ltx_tr"> <span id="S3.T3.1.1.1.1.1.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Gemma</span> <span id="S3.T3.1.1.1.1.1.1.1.7.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</span> <span id="S3.T3.1.1.1.1.1.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_t">51.8</span> <span id="S3.T3.1.1.1.1.1.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_t">53.2</span> <span id="S3.T3.1.1.1.1.1.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_t">81.5</span> <span id="S3.T3.1.1.1.1.1.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_t">81.2</span> <span id="S3.T3.1.1.1.1.1.1.1.7.5.7" class="ltx_td ltx_align_center ltx_border_t">72.3</span> <span id="S3.T3.1.1.1.1.1.1.1.7.5.8" class="ltx_td ltx_align_center ltx_border_t">81.2</span> <span id="S3.T3.1.1.1.1.1.1.1.7.5.9" class="ltx_td ltx_align_center ltx_border_t">70.2</span></span> <span id="S3.T3.1.1.1.1.1.1.1.8.6" class="ltx_tr"> <span id="S3.T3.1.1.1.1.1.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Nemotron-4</span> <span id="S3.T3.1.1.1.1.1.1.1.8.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t">15B</span> <span id="S3.T3.1.1.1.1.1.1.1.8.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">60.9</span> <span id="S3.T3.1.1.1.1.1.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">55.5</span> <span id="S3.T3.1.1.1.1.1.1.1.8.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">80.9</span> <span id="S3.T3.1.1.1.1.1.1.1.8.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">82.4</span> <span id="S3.T3.1.1.1.1.1.1.1.8.6.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">78.0</span> <span id="S3.T3.1.1.1.1.1.1.1.8.6.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">82.4</span> <span id="S3.T3.1.1.1.1.1.1.1.8.6.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T3.1.1.1.1.1.1.1.8.6.9.1" class="ltx_text ltx_font_bold">73.4</span></span></span> </span> </span></span></span> </span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.5.2.1" style="font-size:90%;">Table 3</span>:</span><span class="ltx_text" id="S3.T3.3.1" style="font-size:90%;">Results on standard reasoning benchmarks in the zero-shot setting. 우리는 공정한 비교를 위해 가능한 모든 작업에서 평균을 보고한다. <math alttext="*" class="ltx_Math" display="inline" id="S3.T3.3.1.m1.1"><semantics id="S3.T3.3.1.m1.1b"><mo id="S3.T3.3.1.m1.1.1" xref="S3.T3.3.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.1.m1.1c"><times id="S3.T3.3.1.m1.1.1.cmml" xref="S3.T3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.1.m1.1d">*</annotation></semantics></math>로 표시된 값은 <cite class="ltx_cite ltx_citemacro_citet">Gemma Team (<a class="ltx_ref" href="#bib.bib17" title="">2024</a>)</cite></span>에서 읽힌다.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Popular Aggregated Benchmarks</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS2.p1.1">MLU<cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="#bib.bib20" title="">2020</a>)</cite> 및 Big Bench Hard(BBH)<cite class="ltx_cite ltx_citemacro_citep">(Suzgun et al., <a class="ltx_ref" href="#bib.bib44" title="">2022</a>)</cite> 벤치마크는 광범위한 작업 및 도메인에 대한 언어 모델의 능력에 대한 도전적인 평가로 개발되었다. 표 <a class="ltx_ref" href="#S3.T4" title="Table 4 ‣ 3.2 Popular Aggregated Benchmarks ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">4</span></a>에서 볼 수 있듯이, 네모트론-4 15B는 그 규모에서 기존 모델에 걸쳐 BBH에서 거의 7%의 최고 점수를 달성한다. 추가적으로, Nemotron-4는 LLaMA-2 70B가 51.2의 점수를 획득하고 Nemotron-4가 58.7인 BBH 벤치마크에서 LLaMA-2 70B 모델보다 상당히 우수하다. Nemotron-4 15B는 추가적으로 매우 경쟁적인 MMLU 점수를 획득하고 MMLU에 대한 카테고리별 성능은 표 <a class="ltx_ref" href="#Ax1.T11" title="Table 11 ‣ Supplementary Materials ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">11</span></a>에서 찾을 수 있다.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.2.1.1" class="ltx_tr">
<th id="S3.T4.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T4.2.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">Size</th>
<th id="S3.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">BBH</th>
<th id="S3.T4.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MMLU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.2.2.1" class="ltx_tr">
<th id="S3.T4.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S3.T4.2.2.1.1.1" class="ltx_text">LLaMA-2</span></th>
<th id="S3.T4.2.2.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</th>
<td id="S3.T4.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">39.4</td>
<td id="S3.T4.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">54.8</td>
</tr>
<tr id="S3.T4.2.3.2" class="ltx_tr">
<th id="S3.T4.2.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">34B</th>
<td id="S3.T4.2.3.2.2" class="ltx_td ltx_align_center">44.1</td>
<td id="S3.T4.2.3.2.3" class="ltx_td ltx_align_center">62.6</td>
</tr>
<tr id="S3.T4.2.4.3" class="ltx_tr">
<th id="S3.T4.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T4.2.4.3.1.1" class="ltx_text">Baichuan-2</span></th>
<th id="S3.T4.2.4.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</th>
<td id="S3.T4.2.4.3.3" class="ltx_td ltx_align_center ltx_border_t">48.8</td>
<td id="S3.T4.2.4.3.4" class="ltx_td ltx_align_center ltx_border_t">59.2</td>
</tr>
<tr id="S3.T4.2.5.4" class="ltx_tr">
<th id="S3.T4.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">QWEN</th>
<th id="S3.T4.2.5.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">14B</th>
<td id="S3.T4.2.5.4.3" class="ltx_td ltx_align_center ltx_border_t">53.4</td>
<td id="S3.T4.2.5.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T4.2.5.4.4.1" class="ltx_text ltx_font_bold">66.3</span></td>
</tr>
<tr id="S3.T4.2.6.5" class="ltx_tr">
<th id="S3.T4.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Mistral</th>
<th id="S3.T4.2.6.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</th>
<td id="S3.T4.2.6.5.3" class="ltx_td ltx_align_center ltx_border_t">39.5</td>
<td id="S3.T4.2.6.5.4" class="ltx_td ltx_align_center ltx_border_t">60.1</td>
</tr>
<tr id="S3.T4.2.7.6" class="ltx_tr">
<th id="S3.T4.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Gemma</th>
<th id="S3.T4.2.7.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</th>
<td id="S3.T4.2.7.6.3" class="ltx_td ltx_align_center ltx_border_t">55.1</td>
<td id="S3.T4.2.7.6.4" class="ltx_td ltx_align_center ltx_border_t">64.3</td>
</tr>
<tr id="S3.T4.2.8.7" class="ltx_tr">
<th id="S3.T4.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Nemotron-4</th>
<th id="S3.T4.2.8.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t">15B</th>
<td id="S3.T4.2.8.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T4.2.8.7.3.1" class="ltx_text ltx_font_bold">58.7</span></td>
<td id="S3.T4.2.8.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">64.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.3.1.1" style="font-size:90%;">Table 4</span>:</span><span class="ltx_text" id="S3.T4.4.2" style="font-size:90%;">Nemotron-4 15B는 인기 있는 집계 벤치마크에서 높은 경쟁 성능을 달성합니다. <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="#bib.bib23" title="">2023</a>)</cite>의 그림에서 미스트랄에 대한 BBH 결과를 읽는다. </span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Math and Code</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS3.p1.1">최근 대규모 언어 모델은 수학적 추론과 다양한 코딩 작업 <cite class="ltx_cite ltx_citemacro_citep">(Allal et al., <a class="ltx_ref" href="#bib.bib3" title="">2023</a>; Chowdhery et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>; Touvron et al., <a class="ltx_ref" href="#bib.bib45" title="">2023a</a>)</cite> 모두에 효과적인 것으로 나타났다. 표<a class="ltx_ref" href="#S3.T5" title="Table 5 ‣ 3.3 Math and Code ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">5</span></a>는 이러한 작업에 대한 네모트론-4 15B의 성능을 강조한다. 특히, 수학적 추론에서 네모트론-4 15B는 젬마 7B와 유사한 점수를 얻으면서 강한 성능을 달성하지만 바이촨-2 및 QWEN과 같은 모델보다 뒤처진다는 것을 발견했다. 코드 작업에서 우리는 네모트론-4가 젬마 7B 뒤에 약간 남아 있는 동안 QWEN 14B와 동등하게 수행하는 것을 본다. 두 가지 유형의 작업 모두에서 네모트론-4 15B는 미스트랄 7B 및 LlaMA-2 13B/34B를 능가할 수 있다.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<div id="S3.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:249.7pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S3.T5.2.2" class="ltx_p"><span id="S3.T5.2.2.2" class="ltx_text"> <span id="S3.T5.2.2.2.2" class="ltx_inline-block ltx_transformed_outer" style="width:249.7pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"> <span id="S3.T5.2.2.2.2.2" class="ltx_p"><span id="S3.T5.2.2.2.2.2.2" class="ltx_text"> <span id="S3.T5.2.2.2.2.2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="S3.T5.2.2.2.2.2.2.2.3.1" class="ltx_tr"> <span id="S3.T5.2.2.2.2.2.2.2.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></span> <span id="S3.T5.2.2.2.2.2.2.2.3.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">Size</span> <span id="S3.T5.2.2.2.2.2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">GSM8K</span> <span id="S3.T5.2.2.2.2.2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">HumanEval</span> <span id="S3.T5.2.2.2.2.2.2.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MBPP</span></span> </span> <span class="ltx_tbody"> <span id="S3.T5.2.2.2.2.2.2.2.4.1" class="ltx_tr"> <span id="S3.T5.2.2.2.2.2.2.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S3.T5.2.2.2.2.2.2.2.4.1.1.1" class="ltx_text">LlaMA-2</span></span> <span id="S3.T5.2.2.2.2.2.2.2.4.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</span> <span id="S3.T5.2.2.2.2.2.2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">28.7</span> <span id="S3.T5.2.2.2.2.2.2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">18.3</span> <span id="S3.T5.2.2.2.2.2.2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t">30.6</span></span> <span id="S3.T5.2.2.2.2.2.2.2.5.2" class="ltx_tr"> <span id="S3.T5.2.2.2.2.2.2.2.5.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">34B</span> <span id="S3.T5.2.2.2.2.2.2.2.5.2.2" class="ltx_td ltx_align_center">42.2</span> <span id="S3.T5.2.2.2.2.2.2.2.5.2.3" class="ltx_td ltx_align_center">22.6</span> <span id="S3.T5.2.2.2.2.2.2.2.5.2.4" class="ltx_td ltx_align_center">33.0</span></span> <span id="S3.T5.2.2.2.2.2.2.2.6.3" class="ltx_tr"> <span id="S3.T5.2.2.2.2.2.2.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.6.3.1.1" class="ltx_text">Baichuan-2</span></span> <span id="S3.T5.2.2.2.2.2.2.2.6.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</span> <span id="S3.T5.2.2.2.2.2.2.2.6.3.3" class="ltx_td ltx_align_center ltx_border_t">52.8</span> <span id="S3.T5.2.2.2.2.2.2.2.6.3.4" class="ltx_td ltx_align_center ltx_border_t">17.1</span> <span id="S3.T5.2.2.2.2.2.2.2.6.3.5" class="ltx_td ltx_align_center ltx_border_t">30.2</span></span> <span id="S3.T5.2.2.2.2.2.2.2.7.4" class="ltx_tr"> <span id="S3.T5.2.2.2.2.2.2.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.7.4.1.1" class="ltx_text">QWEN</span></span> <span id="S3.T5.2.2.2.2.2.2.2.7.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">14B</span> <span id="S3.T5.2.2.2.2.2.2.2.7.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.7.4.3.1" class="ltx_text ltx_font_bold">60.1</span></span> <span id="S3.T5.2.2.2.2.2.2.2.7.4.4" class="ltx_td ltx_align_center ltx_border_t">32.2</span> <span id="S3.T5.2.2.2.2.2.2.2.7.4.5" class="ltx_td ltx_align_center ltx_border_t">40.8</span></span> <span id="S3.T5.2.2.2.2.2.2.2.2" class="ltx_tr"> <span id="S3.T5.2.2.2.2.2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.2.3.1" class="ltx_text">Mistral</span></span> <span id="S3.T5.2.2.2.2.2.2.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</span> <span id="S3.T5.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">35.4<sup id="S3.T5.1.1.1.1.1.1.1.1.1.1" class="ltx_sup"><span id="S3.T5.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup></span> <span id="S3.T5.2.2.2.2.2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t">30.5</span> <span id="S3.T5.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">&nbsp;&nbsp;40.2<sup id="S3.T5.2.2.2.2.2.2.2.2.2.1" class="ltx_sup"><span id="S3.T5.2.2.2.2.2.2.2.2.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup></span></span> <span id="S3.T5.2.2.2.2.2.2.2.8.5" class="ltx_tr"> <span id="S3.T5.2.2.2.2.2.2.2.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.8.5.1.1" class="ltx_text">Gemma</span></span> <span id="S3.T5.2.2.2.2.2.2.2.8.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</span> <span id="S3.T5.2.2.2.2.2.2.2.8.5.3" class="ltx_td ltx_align_center ltx_border_t">46.4</span> <span id="S3.T5.2.2.2.2.2.2.2.8.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.8.5.4.1" class="ltx_text ltx_font_bold">32.3</span></span> <span id="S3.T5.2.2.2.2.2.2.2.8.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.8.5.5.1" class="ltx_text ltx_font_bold">44.4</span></span></span> <span id="S3.T5.2.2.2.2.2.2.2.9.6" class="ltx_tr"> <span id="S3.T5.2.2.2.2.2.2.2.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.9.6.1.1" class="ltx_text">Nemotron-4</span></span> <span id="S3.T5.2.2.2.2.2.2.2.9.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t">15B</span> <span id="S3.T5.2.2.2.2.2.2.2.9.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">46.0</span> <span id="S3.T5.2.2.2.2.2.2.2.9.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">31.6</span> <span id="S3.T5.2.2.2.2.2.2.2.9.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">40.6</span></span> </span> </span></span></span> </span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T5.4.1.1" style="font-size:90%;">Table 5</span>:</span><span class="ltx_text" id="S3.T5.5.2" style="font-size:90%;">math and code benchmarks에 대한 비교 결과. Mistral 7B가 MBPP 성능을 다른 평가 분할에 보고하고 GSM8K에 대해 다른 평가 설정을 사용함에 따라, 우리는 <cite class="ltx_cite ltx_citemacro_citep">(Gemma Team, <a class="ltx_ref" href="#bib.bib17" title="">2024</a>)</cite></span>에 보고된 해당 번호를 사용한다.</figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS3.p2.1">비슷한 크기의 거의 모든 개방형 모델은 다른 프로그래밍 언어에 대한 성능 평가를 무시하고 파이썬 관련 작업에 대한 성능만을 기반으로 코드 능력을 결정한다. 표 <a class="ltx_ref" href="#S3.T6" title="Table 6 ‣ 3.3 Math and Code ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">6</span></a>에서는 11개의 다양한 프로그래밍 언어에 걸쳐 Multiple-E <cite class="ltx_cite ltx_citemacro_citep">(Cassano et al., <a class="ltx_ref" href="#bib.bib9" title="">2023b</a>)</cite> 벤치마크 상에서 Nemotron-4 15B의 결과를 보여주고, 코드를 위해 특별히 훈련된 15B 파라미터 모델인 Mistral 7B 및 Starcoder <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="#bib.bib27" title="">2023</a>)</cite>와 비교한다. 우리는 네모트론-4 15B가 다양한 프로그래밍 언어에 걸쳐 강력한 코딩 성능을 달성하고 스타코더와 미스트랄 7B보다 평균적으로 더 우수하다는 것을 발견했다. 특히 Scala, Julia 및 R과 같은 자원이 적은 프로그래밍 언어에서 Nemotron-4 15B의 우수한 성능을 강조한다.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<div id="S3.T6.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:553.3pt;height:73pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<div id="S3.T6.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:57.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-59.9pt,7.8pt) scale(0.78367,0.78367) ;">
<p id="S3.T6.2.1.1" class="ltx_p"><span id="S3.T6.2.1.1.1" class="ltx_text"> <span id="S3.T6.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:553.3pt;height:73pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"> <span id="S3.T6.2.1.1.1.1.1" class="ltx_p"><span id="S3.T6.2.1.1.1.1.1.1" class="ltx_text"> <span id="S3.T6.2.1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1" class="ltx_tr"> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Size</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">JavaScript</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Julia</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Java</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Lua</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">C++</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">C-Sharp</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PHP</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Shell</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">TypeScript</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">R</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Scala</span> <span id="S3.T6.2.1.1.1.1.1.1.1.1.1.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVG</span></span> </span> <span class="ltx_tbody"> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1" class="ltx_tr"> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Starcoder</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">15B</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">30.8</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">23.0</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">30.2</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">23.9</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">31.6</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">21.0</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">26.1</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">10.5</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t">32.3</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.12" class="ltx_td ltx_align_center ltx_border_t">15.5</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.13" class="ltx_td ltx_align_center ltx_border_t">27.6</span> <span id="S3.T6.2.1.1.1.1.1.1.1.2.1.14" class="ltx_td ltx_align_center ltx_border_t">24.2</span></span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2" class="ltx_tr"> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Mistral</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_center">7B</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_center">34.2</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_center">22.0</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_center">26.0</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.6" class="ltx_td ltx_align_center">25.3</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.7" class="ltx_td ltx_align_center">29.1</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.8" class="ltx_td ltx_align_center">22.8</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.9" class="ltx_td ltx_align_center">27.9</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.10" class="ltx_td ltx_align_center">8.9</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.11" class="ltx_td ltx_align_center">28.5</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.12" class="ltx_td ltx_align_center">11.8</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.13" class="ltx_td ltx_align_center">22.2</span> <span id="S3.T6.2.1.1.1.1.1.1.1.3.2.14" class="ltx_td ltx_align_center">23.6</span></span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3" class="ltx_tr"> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Nemotron-4</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">15B</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">28.6</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">24.8</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">24.8</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb">24.2</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_bb">35.4</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_bb">21.1</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_bb">27.3</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.10" class="ltx_td ltx_align_center ltx_border_bb">8.9</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.11" class="ltx_td ltx_align_center ltx_border_bb">32.9</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.12" class="ltx_td ltx_align_center ltx_border_bb">18.6</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.13" class="ltx_td ltx_align_center ltx_border_bb">27.3</span> <span id="S3.T6.2.1.1.1.1.1.1.1.4.3.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T6.2.1.1.1.1.1.1.1.4.3.14.1" class="ltx_text ltx_font_bold">24.5</span></span></span> </span> </span></span></span> </span></span></span></p>
</span></div>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T6.3.1.1" style="font-size:90%;">Table 6</span>:</span><span class="ltx_text" id="S3.T6.4.2" style="font-size:90%;">Nemotron-4 15B는 광범위한 프로그래밍 언어에 걸쳐 코딩 성능에서 높은 역량을 달성한다. 미스트랄에 대한 결과는 네모트론-4와 동일한 환경에서 미스트랄의 실행에서 가져온 것이다. </span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multilingual</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS4.p1.1">우리는 다양한 범위의 높은 자원에서 낮은 자원 자연 언어를 다루는 이전 연구에서 널리 연구된 4개의 벤치마크를 사용하여 네모트론-4 15B의 뛰어난 다국어 능력을 보여준다. 분류는 정확도를 메트릭으로 사용하고, 생성 작업의 경우 정확한 매칭을 사용하며, 기계 번역의 경우 spBLEU 점수를 얻기 위해 <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.1.1">sacreBLEU</span> <cite class="ltx_cite ltx_citemacro_citep">(Post, <a class="ltx_ref" href="#bib.bib32" title="">2018</a>)</cite> <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.1.2">BLEU</span> <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="#bib.bib30" title="">2002</a>)</cite> <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.1.3">spm-flores-101</span> tokenization을 사용하여 평가한다.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">1. Classification:</span> Cross-lingual Choice of Plausible Alternatives (XCOPA) <cite class="ltx_cite ltx_citemacro_citep">(Ponti et al., <a class="ltx_ref" href="#bib.bib31" title="">2020</a>)</cite> tests causal commonsense reasoning in 11 languages</p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS4.p3.1">네모트론-4 15B를 기존의 다국어 언어 모델인 XGLM <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="#bib.bib28" title="">2022</a>)</cite>, mGPT <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al., <a class="ltx_ref" href="#bib.bib39" title="">2022</a>)</cite>, BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Scao et al., <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>와 비교한다. XGLM 및 mGPT는 훈련 데이터에 비영어 언어의 존재를 업 샘플링하여 향상된 다국어 능력을 갖도록 특별히 훈련된 모델이다. 대조적으로, BLOOM은 네모트론-4와 마찬가지로 영어, 다국어 및 코드 데이터의 조합으로 훈련된 범용 언어 모델이다. 표 <a class="ltx_ref" href="#S3.T7" title="Table 7 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">7</span></a>에서 우리는 네모트론-4가 모든 모델 중에서 최고의 성능을 달성한다는 것을 분명히 볼 수 있는데, 이는 4샷 설정에서 거의 12% 개선을 실현한다.</p>
</div>
<figure id="S3.T7" class="ltx_table">
<div id="S3.T7.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:536.2pt;height:145pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<div id="S3.T7.5.5" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:117.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.3pt,13.8pt) scale(0.80868,0.80868) ;">
<p id="S3.T7.5.5.5" class="ltx_p"><span id="S3.T7.5.5.5.5" class="ltx_text"> <span id="S3.T7.5.5.5.5.5" class="ltx_inline-block ltx_transformed_outer" style="width:536.2pt;height:145pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"> <span id="S3.T7.5.5.5.5.5.5" class="ltx_p"><span id="S3.T7.5.5.5.5.5.5.5" class="ltx_text"> <span id="S3.T7.5.5.5.5.5.5.5.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1" class="ltx_tr"> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Mode</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">Size</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ET</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">HT</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ID</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">IT</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">QU</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SW</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">TA</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">TH</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">TR</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VI</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ZH</span> <span id="S3.T7.5.5.5.5.5.5.5.5.6.1.15" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVG</span></span> </span> <span class="ltx_tbody"> <span id="S3.T7.5.5.5.5.5.5.5.5.5" class="ltx_tr"> <span id="S3.T7.5.5.5.5.5.5.5.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_4"><span id="S3.T7.5.5.5.5.5.5.5.5.5.6.1" class="ltx_text">Zero-Shot</span></span> <span id="S3.T7.5.5.5.5.5.5.5.5.5.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">BLOOM</span> <span id="S3.T7.5.5.5.5.5.5.5.5.5.8" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">176B</span> <span id="S3.T7.5.5.5.5.5.5.5.5.5.9" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S3.T7.5.5.5.5.5.5.5.5.5.10" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S3.T7.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="57.5^{*}" display="inline"><semantics id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1a"><msup id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">57.5</mn><mo id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><cn type="float" id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">57.5</cn><times id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1c">57.5^{*}</annotation></semantics></math></span> <span id="S3.T7.5.5.5.5.5.5.5.5.5.11" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S3.T7.5.5.5.5.5.5.5.5.5.12" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S3.T7.2.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="59.5^{*}" display="inline"><semantics id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1a"><msup id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml"><mn id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.2" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.2.cmml">59.5</mn><mo id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.3" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1b"><apply id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.1.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1">superscript</csymbol><cn type="float" id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.2.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.2">59.5</cn><times id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.3.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1c">59.5^{*}</annotation></semantics></math></span> <span id="S3.T7.3.3.3.3.3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1" class="ltx_Math" alttext="54.7^{*}" display="inline"><semantics id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1a"><msup id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml"><mn id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.2" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.2.cmml">54.7</mn><mo id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.3" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1b"><apply id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.1.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1">superscript</csymbol><cn type="float" id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.2.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.2">54.7</cn><times id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.3.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1c">54.7^{*}</annotation></semantics></math></span> <span id="S3.T7.5.5.5.5.5.5.5.5.5.13" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S3.T7.5.5.5.5.5.5.5.5.5.14" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S3.T7.4.4.4.4.4.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1" class="ltx_Math" alttext="58.2^{*}" display="inline"><semantics id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1a"><msup id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.cmml"><mn id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.2" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.2.cmml">58.2</mn><mo id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.3" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1b"><apply id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.1.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1">superscript</csymbol><cn type="float" id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.2.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.2">58.2</cn><times id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.3.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1c">58.2^{*}</annotation></semantics></math></span> <span id="S3.T7.5.5.5.5.5.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1" class="ltx_Math" alttext="57.7^{*}" display="inline"><semantics id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1a"><msup id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.cmml"><mn id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.2" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.2.cmml">57.7</mn><mo id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.3" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1b"><apply id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.1.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1">superscript</csymbol><cn type="float" id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.2.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.2">57.7</cn><times id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.3.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1c">57.7^{*}</annotation></semantics></math></span> <span id="S3.T7.5.5.5.5.5.5.5.5.5.15" class="ltx_td ltx_align_center ltx_border_t">-</span></span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1" class="ltx_tr"> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">XGLM</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">7.5B</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.3" class="ltx_td ltx_align_center">57.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.4" class="ltx_td ltx_align_center">57.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.5" class="ltx_td ltx_align_center">59.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.6" class="ltx_td ltx_align_center">49.2</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.7" class="ltx_td ltx_align_center">52.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.8" class="ltx_td ltx_align_center">55.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.9" class="ltx_td ltx_align_center">55.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.10" class="ltx_td ltx_align_center">57.8</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.11" class="ltx_td ltx_align_center">55.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.12" class="ltx_td ltx_align_center">59.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.13" class="ltx_td ltx_align_center">53.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.7.1.14" class="ltx_td ltx_align_center">55.6</span></span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2" class="ltx_tr"> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">mGPT</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">13B</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.3" class="ltx_td ltx_align_center">49.8</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.4" class="ltx_td ltx_align_center">50.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.5" class="ltx_td ltx_align_center">63.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.6" class="ltx_td ltx_align_center">61.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.7" class="ltx_td ltx_align_center">50.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.8" class="ltx_td ltx_align_center">57.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.9" class="ltx_td ltx_align_center">57.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.10" class="ltx_td ltx_align_center">54.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.11" class="ltx_td ltx_align_center">58.2</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.12" class="ltx_td ltx_align_center">60.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.13" class="ltx_td ltx_align_center">54.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.8.2.14" class="ltx_td ltx_align_center">56.1</span></span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3" class="ltx_tr"> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Nemotron-4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">15B</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.3" class="ltx_td ltx_align_center">62.8</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.4" class="ltx_td ltx_align_center">47.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.5" class="ltx_td ltx_align_center">66.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.6" class="ltx_td ltx_align_center">67.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.7" class="ltx_td ltx_align_center">53.8</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.8" class="ltx_td ltx_align_center">50.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.9" class="ltx_td ltx_align_center">62.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.10" class="ltx_td ltx_align_center">59.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.11" class="ltx_td ltx_align_center">57.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.12" class="ltx_td ltx_align_center">65.2</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.13" class="ltx_td ltx_align_center">62.2</span> <span id="S3.T7.5.5.5.5.5.5.5.5.9.3.14" class="ltx_td ltx_align_center"><span id="S3.T7.5.5.5.5.5.5.5.5.9.3.14.1" class="ltx_text ltx_font_bold">59.5</span></span></span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4" class="ltx_tr"> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3"><span id="S3.T7.5.5.5.5.5.5.5.5.10.4.1.1" class="ltx_text">4-Shot</span></span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">XGLM</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.3" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7.5B</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.4" class="ltx_td ltx_align_center ltx_border_t">64.7</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.5" class="ltx_td ltx_align_center ltx_border_t">60.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.6" class="ltx_td ltx_align_center ltx_border_t">67.3</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.7" class="ltx_td ltx_align_center ltx_border_t">64.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.8" class="ltx_td ltx_align_center ltx_border_t">50.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.9" class="ltx_td ltx_align_center ltx_border_t">61.8</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.10" class="ltx_td ltx_align_center ltx_border_t">56.7</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.11" class="ltx_td ltx_align_center ltx_border_t">61.5</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.12" class="ltx_td ltx_align_center ltx_border_t">60.1</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.13" class="ltx_td ltx_align_center ltx_border_t">68.5</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.14" class="ltx_td ltx_align_center ltx_border_t">59.9</span> <span id="S3.T7.5.5.5.5.5.5.5.5.10.4.15" class="ltx_td ltx_align_center ltx_border_t">61.4</span></span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5" class="ltx_tr"> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">mGPT</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">13B</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.3" class="ltx_td ltx_align_center">48.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.4" class="ltx_td ltx_align_center">48.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.5" class="ltx_td ltx_align_center">62.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.6" class="ltx_td ltx_align_center">60.8</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.7" class="ltx_td ltx_align_center">50.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.8" class="ltx_td ltx_align_center">56.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.9" class="ltx_td ltx_align_center">55.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.10" class="ltx_td ltx_align_center">54.8</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.11" class="ltx_td ltx_align_center">57.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.12" class="ltx_td ltx_align_center">61.8</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.13" class="ltx_td ltx_align_center">58.4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.11.5.14" class="ltx_td ltx_align_center">56.0</span></span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6" class="ltx_tr"> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Nemotron-4</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb">15B</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.3" class="ltx_td ltx_align_center ltx_border_bb">72.9</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.4" class="ltx_td ltx_align_center ltx_border_bb">52.8</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.5" class="ltx_td ltx_align_center ltx_border_bb">79.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.6" class="ltx_td ltx_align_center ltx_border_bb">79.2</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.7" class="ltx_td ltx_align_center ltx_border_bb">50.2</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.8" class="ltx_td ltx_align_center ltx_border_bb">52.2</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.9" class="ltx_td ltx_align_center ltx_border_bb">72.8</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.10" class="ltx_td ltx_align_center ltx_border_bb">66.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.11" class="ltx_td ltx_align_center ltx_border_bb">77.2</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.12" class="ltx_td ltx_align_center ltx_border_bb">78.6</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.13" class="ltx_td ltx_align_center ltx_border_bb">76.0</span> <span id="S3.T7.5.5.5.5.5.5.5.5.12.6.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T7.5.5.5.5.5.5.5.5.12.6.14.1" class="ltx_text ltx_font_bold">68.9</span></span></span> </span> </span></span></span> </span></span></span></p>
</span></div>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T7.9.2.1" style="font-size:90%;">Table 7</span>:</span><span class="ltx_text" id="S3.T7.7.1" style="font-size:90%;">Comparison of Nemotron-4 15B against existing large language models on XCOPA under the zero- and four-shot setting. XGLM에 대해 보고된 결과는 mGPT에서 사용하는 동일한 프롬프트 템플릿을 사용한다는 점에서 <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al., <a class="ltx_ref" href="#bib.bib39" title="">2022</a>)</cite>의 모델 실행에서 나온 것이다. <math alttext="*" class="ltx_Math" display="inline" id="S3.T7.7.1.m1.1"><semantics id="S3.T7.7.1.m1.1b"><mo id="S3.T7.7.1.m1.1.1" xref="S3.T7.7.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.T7.7.1.m1.1c"><times id="S3.T7.7.1.m1.1.1.cmml" xref="S3.T7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.7.1.m1.1d">*</annotation></semantics></math>로 표시된 값은 <cite class="ltx_cite ltx_citemacro_citep">(Scao et al., <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>의 도형으로부터 판독된다. </span></figcaption>
</figure>
<div id="S3.SS4.p4" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p4.1.1">2. Generation:</span> We consider two generationative tasks: TyDiQA-GoldP <cite class="ltx_cite ltx_citemacro_citep">(Clark et al., <a class="ltx_ref" href="#bib.bib13" title="">2020</a>)</cite> and Multilingual Grade School Math (MGSM) <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="#bib.bib38" title="">2022</a>)</cite>. TyDiQA-GoldP는 질의 응답 작업이며 MGSM은 10개 언어로 언어 모델의 산술 추론 능력을 평가한다.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS4.p5.1">TyDiQA-GoldP 상의 네모트론-4 15B의 성능을 다양한 모델과 비교할 때, 표 <a class="ltx_ref" href="#S3.T8" title="Table 8 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">8</span></a>는 네모트론-4 15B가 최상의 성능을 달성함을 보여준다. 인상적으로, 네모트론-4 15B는 차선책인 PaLM 62B-cont에서 크게 개선할 수 있다.</p>
</div>
<figure id="S3.T8" class="ltx_table">
<div id="S3.T8.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.0pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S3.T8.2.1" class="ltx_p"><span id="S3.T8.2.1.1" class="ltx_text"> <span id="S3.T8.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:390.0pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"> <span id="S3.T8.2.1.1.1.1" class="ltx_p"><span id="S3.T8.2.1.1.1.1.1" class="ltx_text"> <span id="S3.T8.2.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="S3.T8.2.1.1.1.1.1.1.1.1" class="ltx_tr"> <span id="S3.T8.2.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</span> <span id="S3.T8.2.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Size</span> <span id="S3.T8.2.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">AR</span> <span id="S3.T8.2.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">BN</span> <span id="S3.T8.2.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">FI</span> <span id="S3.T8.2.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ID</span> <span id="S3.T8.2.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">KO</span> <span id="S3.T8.2.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">RU</span> <span id="S3.T8.2.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">SW</span> <span id="S3.T8.2.1.1.1.1.1.1.1.1.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">TE</span> <span id="S3.T8.2.1.1.1.1.1.1.1.1.11" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">AVG</span></span> </span> <span class="ltx_tbody"> <span id="S3.T8.2.1.1.1.1.1.1.2.1" class="ltx_tr"> <span id="S3.T8.2.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S3.T8.2.1.1.1.1.1.1.2.1.1.1" class="ltx_text">PaLM</span></span> <span id="S3.T8.2.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">62B</span> <span id="S3.T8.2.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">31.2</span> <span id="S3.T8.2.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t">42.5</span> <span id="S3.T8.2.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t">41.7</span> <span id="S3.T8.2.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_right ltx_border_t">41.6</span> <span id="S3.T8.2.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_right ltx_border_t">49.3</span> <span id="S3.T8.2.1.1.1.1.1.1.2.1.8" class="ltx_td ltx_align_right ltx_border_t">29.2</span> <span id="S3.T8.2.1.1.1.1.1.1.2.1.9" class="ltx_td ltx_align_right ltx_border_t">58.1</span> <span id="S3.T8.2.1.1.1.1.1.1.2.1.10" class="ltx_td ltx_align_right ltx_border_t">30.6</span> <span id="S3.T8.2.1.1.1.1.1.1.2.1.11" class="ltx_td ltx_align_right ltx_border_t">40.5</span></span> <span id="S3.T8.2.1.1.1.1.1.1.3.2" class="ltx_tr"> <span id="S3.T8.2.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_right">62B-cont</span> <span id="S3.T8.2.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_right">39.4</span> <span id="S3.T8.2.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_right">48.7</span> <span id="S3.T8.2.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_right">44.0</span> <span id="S3.T8.2.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_right">49.2</span> <span id="S3.T8.2.1.1.1.1.1.1.3.2.6" class="ltx_td ltx_align_right">52.5</span> <span id="S3.T8.2.1.1.1.1.1.1.3.2.7" class="ltx_td ltx_align_right">35.6</span> <span id="S3.T8.2.1.1.1.1.1.1.3.2.8" class="ltx_td ltx_align_right">60.9</span> <span id="S3.T8.2.1.1.1.1.1.1.3.2.9" class="ltx_td ltx_align_right">35.3</span> <span id="S3.T8.2.1.1.1.1.1.1.3.2.10" class="ltx_td ltx_align_right">45.7</span></span> <span id="S3.T8.2.1.1.1.1.1.1.4.3" class="ltx_tr"> <span id="S3.T8.2.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">LLaMA-2</span> <span id="S3.T8.2.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_right">13B</span> <span id="S3.T8.2.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.4.3.6" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.4.3.7" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.4.3.8" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.4.3.9" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.4.3.10" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.4.3.11" class="ltx_td ltx_align_right">33.2</span></span> <span id="S3.T8.2.1.1.1.1.1.1.5.4" class="ltx_tr"> <span id="S3.T8.2.1.1.1.1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Baichuan-2</span> <span id="S3.T8.2.1.1.1.1.1.1.5.4.2" class="ltx_td ltx_align_right">13B</span> <span id="S3.T8.2.1.1.1.1.1.1.5.4.3" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.5.4.4" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.5.4.5" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.5.4.6" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.5.4.7" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.5.4.8" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.5.4.9" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.5.4.10" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.5.4.11" class="ltx_td ltx_align_right">30.8</span></span> <span id="S3.T8.2.1.1.1.1.1.1.6.5" class="ltx_tr"> <span id="S3.T8.2.1.1.1.1.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">QWEN</span> <span id="S3.T8.2.1.1.1.1.1.1.6.5.2" class="ltx_td ltx_align_right">14B</span> <span id="S3.T8.2.1.1.1.1.1.1.6.5.3" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.6.5.4" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.6.5.5" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.6.5.6" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.6.5.7" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.6.5.8" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.6.5.9" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.6.5.10" class="ltx_td ltx_align_right">-</span> <span id="S3.T8.2.1.1.1.1.1.1.6.5.11" class="ltx_td ltx_align_right">39.8</span></span> <span id="S3.T8.2.1.1.1.1.1.1.7.6" class="ltx_tr"> <span id="S3.T8.2.1.1.1.1.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Nemotron-4</span> <span id="S3.T8.2.1.1.1.1.1.1.7.6.2" class="ltx_td ltx_align_right ltx_border_bb">15B</span> <span id="S3.T8.2.1.1.1.1.1.1.7.6.3" class="ltx_td ltx_align_right ltx_border_bb">39.1</span> <span id="S3.T8.2.1.1.1.1.1.1.7.6.4" class="ltx_td ltx_align_right ltx_border_bb">55.8</span> <span id="S3.T8.2.1.1.1.1.1.1.7.6.5" class="ltx_td ltx_align_right ltx_border_bb">52.2</span> <span id="S3.T8.2.1.1.1.1.1.1.7.6.6" class="ltx_td ltx_align_right ltx_border_bb">54.5</span> <span id="S3.T8.2.1.1.1.1.1.1.7.6.7" class="ltx_td ltx_align_right ltx_border_bb">55.1</span> <span id="S3.T8.2.1.1.1.1.1.1.7.6.8" class="ltx_td ltx_align_right ltx_border_bb">37.8</span> <span id="S3.T8.2.1.1.1.1.1.1.7.6.9" class="ltx_td ltx_align_right ltx_border_bb">54.5</span> <span id="S3.T8.2.1.1.1.1.1.1.7.6.10" class="ltx_td ltx_align_right ltx_border_bb">55.0</span> <span id="S3.T8.2.1.1.1.1.1.1.7.6.11" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T8.2.1.1.1.1.1.1.7.6.11.1" class="ltx_text ltx_font_bold">50.5</span></span></span> </span> </span></span></span> </span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T8.3.1.1" style="font-size:90%;">Table 8</span>:</span><span class="ltx_text" id="S3.T8.4.2" style="font-size:90%;">Comparison results in the one-shot setting on TyDiQA-GoldP. LLaMA-2 13B, 바이촨-2 13B 및 QWEN 14B에 대한 결과는 <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="#bib.bib10" title="">2024</a>)</cite>에서 가져왔다. </span></figcaption>
</figure>
<div id="S3.SS4.p6" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS4.p6.1">네모트론-4 15B의 인상적인 다국어 능력을 추가로 입증하기 위해, 표 <a class="ltx_ref" href="#S3.T9" title="Table 9 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">9</span></a>는 MGSM에 대한 성능을 보여준다. <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="#bib.bib38" title="">2022</a>)</cite>에 소개된 영어 사고 연쇄 설정을 사용하여 보고하며, 여기서 모든 사고 연쇄 설명은 과제의 언어가 아닌 영어로 모델에 제시된다. 수학적 능력과 다국어 능력의 교차점을 평가하는 이 도전적인 과제에서 네모트론-4 15B는 비교된 모델 중에서 최고의 성능을 달성하고 가장 가까운 점수를 거의 30% 향상시킨다.</p>
</div>
<figure id="S3.T9" class="ltx_table">
<div id="S3.T9.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:516.2pt;height:91pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<div id="S3.T9.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:76.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-41.3pt,7.2pt) scale(0.84001,0.84001) ;">
<p id="S3.T9.2.1.1" class="ltx_p"><span id="S3.T9.2.1.1.1" class="ltx_text"> <span id="S3.T9.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:516.2pt;height:91pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"> <span id="S3.T9.2.1.1.1.1.1" class="ltx_p"><span id="S3.T9.2.1.1.1.1.1.1" class="ltx_text"> <span id="S3.T9.2.1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1" class="ltx_tr"> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Mode</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Size</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">DE</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">FR</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ES</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">RU</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">JA</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">TH</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.11" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">TE</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.12" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">BN</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.13" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">SW</span> <span id="S3.T9.2.1.1.1.1.1.1.1.1.1.14" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">AVG</span></span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2" class="ltx_tr"> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S3.T9.2.1.1.1.1.1.1.1.2.2.1.1" class="ltx_text">Native-COT</span></span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S3.T9.2.1.1.1.1.1.1.1.2.2.2.1" class="ltx_text">PaLM</span></span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">62B</span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">24.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">24.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">26.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">22.8</span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">24.8</span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.9" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">14.8</span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">18.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.11" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">11.6</span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.12" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">13.6</span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.13" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">9.6</span> <span id="S3.T9.2.1.1.1.1.1.1.1.2.2.14" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">18.9</span></span> </span> <span class="ltx_tbody"> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1" class="ltx_tr"> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3"><span id="S3.T9.2.1.1.1.1.1.1.1.3.1.1.1" class="ltx_text">English-COT</span></span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T9.2.1.1.1.1.1.1.1.3.1.2.1" class="ltx_text">PALM</span></span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_right ltx_border_t">62B-cont</span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.4" class="ltx_td ltx_align_right ltx_border_t">44.8</span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t">39.2</span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.6" class="ltx_td ltx_align_right ltx_border_t">44.4</span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.7" class="ltx_td ltx_align_right ltx_border_t">36.8</span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.8" class="ltx_td ltx_align_right ltx_border_t">33.6</span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.9" class="ltx_td ltx_align_right ltx_border_t">24.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.10" class="ltx_td ltx_align_right ltx_border_t">28.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.11" class="ltx_td ltx_align_right ltx_border_t">19.6</span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.12" class="ltx_td ltx_align_right ltx_border_t">28.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.13" class="ltx_td ltx_align_right ltx_border_t">21.2</span> <span id="S3.T9.2.1.1.1.1.1.1.1.3.1.14" class="ltx_td ltx_align_right ltx_border_t">32.0</span></span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2" class="ltx_tr"> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T9.2.1.1.1.1.1.1.1.4.2.1.1" class="ltx_text">Mistral</span></span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_right">7B</span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_right">33.2</span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.4" class="ltx_td ltx_align_right">35.2</span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.5" class="ltx_td ltx_align_right">35.6</span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.6" class="ltx_td ltx_align_right">35.2</span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.7" class="ltx_td ltx_align_right">33.2</span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.8" class="ltx_td ltx_align_right">18.8</span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.9" class="ltx_td ltx_align_right">10.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.10" class="ltx_td ltx_align_right">0.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.11" class="ltx_td ltx_align_right">8.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.12" class="ltx_td ltx_align_right">9.2</span> <span id="S3.T9.2.1.1.1.1.1.1.1.4.2.13" class="ltx_td ltx_align_right">21.8</span></span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3" class="ltx_tr"> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S3.T9.2.1.1.1.1.1.1.1.5.3.1.1" class="ltx_text">Nemotron-4</span></span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_right ltx_border_bb">15B</span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_right ltx_border_bb">46.8</span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_right ltx_border_bb">46.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_right ltx_border_bb">50.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.6" class="ltx_td ltx_align_right ltx_border_bb">45.6</span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.7" class="ltx_td ltx_align_right ltx_border_bb">40.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.8" class="ltx_td ltx_align_right ltx_border_bb">40.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.9" class="ltx_td ltx_align_right ltx_border_bb">43.6</span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.10" class="ltx_td ltx_align_right ltx_border_bb">41.6</span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.11" class="ltx_td ltx_align_right ltx_border_bb">43.6</span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.12" class="ltx_td ltx_align_right ltx_border_bb">16.0</span> <span id="S3.T9.2.1.1.1.1.1.1.1.5.3.13" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T9.2.1.1.1.1.1.1.1.5.3.13.1" class="ltx_text ltx_font_bold">41.3</span></span></span> </span> </span></span></span> </span></span></span></p>
</span></div>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T9.3.1.1" style="font-size:90%;">Table 9</span>:</span><span class="ltx_text" id="S3.T9.4.2" style="font-size:90%;">MGSM에 대한 8-shot accuracy results. 미스트랄에 대한 결과는 네모트론-4와 동일한 환경에서 미스트랄의 실행에서 가져온 것이다. </span></figcaption>
</figure>
<div id="S3.SS4.p7" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS4.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p7.1.1">3. Machine Translation:</span> FLORES-101 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a class="ltx_ref" href="#bib.bib19" title="">2021</a>)</cite> 벤치마크를 통해 모델의 번역 능력을 추가로 평가합니다. 언어 간 번역 능력은 언어 간의 의미적 관계를 관계화하고 이해하는 모델의 능력에 대한 좋은 테스트이다.</p>
</div>
<div id="S3.SS4.p8" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S3.SS4.p8.1">표 <a class="ltx_ref" href="#S3.T10" title="Table 10 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">10</span></a>에서 볼 수 있듯이, 네모트론-4 15B는 LLaMA-2 13B와 바이촨-2 13B 모두에 비해 성능이 각각 90.2%와 44.1% 향상되었습니다. 네모트론-4 15B는 중국어에서 영어로 번역하는 데만 좋은 성과를 내는 것이 아니라 중국어를 다른 언어로 직접 번역하는 데에도 인상적인 결과를 얻을 수 있다. 이 능력은 네모트론-4 15B가 광범위한 자연 언어에 걸쳐 가지고 있는 강력한 이해를 강조한다.</p>
</div>
<figure id="S3.T10" class="ltx_table">
<div id="S3.T10.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:429.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S3.T10.2.1" class="ltx_p"><span id="S3.T10.2.1.1" class="ltx_text"> <span id="S3.T10.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:429.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"> <span id="S3.T10.2.1.1.1.1" class="ltx_p"><span id="S3.T10.2.1.1.1.1.1" class="ltx_text"> <span id="S3.T10.2.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="S3.T10.2.1.1.1.1.1.1.1.1" class="ltx_tr"> <span id="S3.T10.2.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></span> <span id="S3.T10.2.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Size</span> <span id="S3.T10.2.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-EN</span> <span id="S3.T10.2.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-FR</span> <span id="S3.T10.2.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-ES</span> <span id="S3.T10.2.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-AR</span> <span id="S3.T10.2.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-RU</span> <span id="S3.T10.2.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-JA</span> <span id="S3.T10.2.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-DE</span> <span id="S3.T10.2.1.1.1.1.1.1.1.1.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">AVG</span></span> </span> <span class="ltx_tbody"> <span id="S3.T10.2.1.1.1.1.1.1.2.1" class="ltx_tr"> <span id="S3.T10.2.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T10.2.1.1.1.1.1.1.2.1.1.1" class="ltx_text">LLaMA-2</span></span> <span id="S3.T10.2.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">13B</span> <span id="S3.T10.2.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">25.4</span> <span id="S3.T10.2.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t">19.2</span> <span id="S3.T10.2.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t">17.5</span> <span id="S3.T10.2.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_right ltx_border_t">1.4</span> <span id="S3.T10.2.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_right ltx_border_t">10.3</span> <span id="S3.T10.2.1.1.1.1.1.1.2.1.8" class="ltx_td ltx_align_right ltx_border_t">0.1</span> <span id="S3.T10.2.1.1.1.1.1.1.2.1.9" class="ltx_td ltx_align_right ltx_border_t">11.1</span> <span id="S3.T10.2.1.1.1.1.1.1.2.1.10" class="ltx_td ltx_align_right ltx_border_t">12.2</span></span> <span id="S3.T10.2.1.1.1.1.1.1.3.2" class="ltx_tr"> <span id="S3.T10.2.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T10.2.1.1.1.1.1.1.3.2.1.1" class="ltx_text">Baichuan-2</span></span> <span id="S3.T10.2.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">13B</span> <span id="S3.T10.2.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_right ltx_border_t">30.6</span> <span id="S3.T10.2.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_right ltx_border_t">22.1</span> <span id="S3.T10.2.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_right ltx_border_t">17.3</span> <span id="S3.T10.2.1.1.1.1.1.1.3.2.6" class="ltx_td ltx_align_right ltx_border_t">2.4</span> <span id="S3.T10.2.1.1.1.1.1.1.3.2.7" class="ltx_td ltx_align_right ltx_border_t">14.2</span> <span id="S3.T10.2.1.1.1.1.1.1.3.2.8" class="ltx_td ltx_align_right ltx_border_t">11.6</span> <span id="S3.T10.2.1.1.1.1.1.1.3.2.9" class="ltx_td ltx_align_right ltx_border_t">14.5</span> <span id="S3.T10.2.1.1.1.1.1.1.3.2.10" class="ltx_td ltx_align_right ltx_border_t">16.1</span></span> <span id="S3.T10.2.1.1.1.1.1.1.4.3" class="ltx_tr"> <span id="S3.T10.2.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Nemotron-4</span> <span id="S3.T10.2.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">15B</span> <span id="S3.T10.2.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">34.0</span> <span id="S3.T10.2.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">28.1</span> <span id="S3.T10.2.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">21.3</span> <span id="S3.T10.2.1.1.1.1.1.1.4.3.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">16.8</span> <span id="S3.T10.2.1.1.1.1.1.1.4.3.7" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">21.2</span> <span id="S3.T10.2.1.1.1.1.1.1.4.3.8" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">23.1</span> <span id="S3.T10.2.1.1.1.1.1.1.4.3.9" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">18.1</span> <span id="S3.T10.2.1.1.1.1.1.1.4.3.10" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S3.T10.2.1.1.1.1.1.1.4.3.10.1" class="ltx_text ltx_font_bold">23.2</span></span></span> </span> </span></span></span> </span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T10.3.1.1" style="font-size:90%;">Table 10</span>:</span><span class="ltx_text" id="S3.T10.4.2" style="font-size:90%;">Eight-shot results on Flores sub-tasks translating out of Chinese. 외부 모델에 대한 모든 결과는 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib49" title="">2023</a>)</cite></span>에서 얻었다.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section">4&nbsp;&nbsp;&nbsp;Conclusion</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p" id="S4.p1.1">우리는 디코더 전용 트랜스포머 기반 대용량 언어 모델인 네모트론-4 15B를 제시한다. 영어에 걸쳐 있는 8조 개의 토큰, 53개의 추가 자연 언어 및 43개의 프로그래밍 언어로 교육됩니다. 네모트론-4 15B는 규모에서 모든 범용 언어 모델의 가장 강력한 다국어 성능을 나타내며, 심지어 다국어 도메인에 특화된 모델도 능가한다. 네모트론-4는 모델의 능력을 향상시키기 위해 대형 언어 모델에 대한 사전 훈련 세트가 계속해서 더 확장될 수 있음을 보여준다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
NVLink and NVSwitch.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.nvidia.com/en-us/data-center/nvlink/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nvidia.com/en-us/data-center/nvlink/</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ainslie et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Joshua Ainslie, James Lee-Thorp, Michiel de&nbsp;Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.

</span>
<span class="ltx_bibblock">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13245</em>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allal et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Loubna&nbsp;Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos&nbsp;Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh&nbsp;Kumar Umapathi, Carolyn&nbsp;Jane Anderson, Yangtian Zi, Joel&nbsp;Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco&nbsp;De Toni, Bernardo&nbsp;García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry&nbsp;Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de&nbsp;Vries, and Leandro von Werra.

</span>
<span class="ltx_bibblock">SantaCoder: Don’t Reach for the Stars!, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.

</span>
<span class="ltx_bibblock">Program Synthesis with Large Language Models, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Qwen Technical Report.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan&nbsp;Le Bras, Jianfeng Gao, and Yejin Choi.

</span>
<span class="ltx_bibblock">PIQA: Reasoning about Physical Commonsense in Natural Language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassano et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn&nbsp;Jane Anderson, Molly&nbsp;Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.

</span>
<span class="ltx_bibblock">MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Software Engineering</em>, pages 1–17, 2023a.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TSE.2023.3267446</span>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassano et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn&nbsp;Jane Anderson, Molly&nbsp;Q Feldman, et&nbsp;al.

</span>
<span class="ltx_bibblock">Multipl-e: a scalable and polyglot approach to benchmarking neural code generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Software Engineering</em>, 2023b.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Du&nbsp;Chen, Yi&nbsp;Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, and Kun Han.

</span>
<span class="ltx_bibblock">Orion-14B: Open-source Multilingual Large Language Models, 2024.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique&nbsp;Ponde de&nbsp;Oliveira&nbsp;Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe&nbsp;Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William&nbsp;Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew&nbsp;N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.

</span>
<span class="ltx_bibblock">Evaluating Large Language Models Trained on Code, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, et&nbsp;al.

</span>
<span class="ltx_bibblock">PaLM: Scaling Language Modeling with Pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jonathan&nbsp;H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki.

</span>
<span class="ltx_bibblock">TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2003.05002, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2003.05002" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2003.05002</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think You have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.

</span>
<span class="ltx_bibblock">Training Verifiers to Solve Math Word Problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2110.14168, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2110.14168" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2110.14168</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.

</span>
<span class="ltx_bibblock">A Framework for Few-shot Language Model Evaluation, September 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.5371628" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.5371628</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemma&nbsp;Team (2024)</span>
<span class="ltx_bibblock">
Google&nbsp;DeepMind Gemma&nbsp;Team.

</span>
<span class="ltx_bibblock">Gemma: Open Models Based on Gemini Research and Technology, 2024.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2023)</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Gemini: A Family of Highly Capable Multimodal Models, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da&nbsp;Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan.

</span>
<span class="ltx_bibblock">The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2106.03193, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2106.03193" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2106.03193</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring Massive Multitask Language Understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.03300</em>, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training Compute-Optimal Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jennings et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Shrimai Prabhumoye, Ayush Dattagupta, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Curating Trillion-Token Datasets: Introducing NVIDIA NeMo Data Curator.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://developer.nvidia.com/blog/curating-trillion-token-datasets-introducing-nemo-data-curator/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.nvidia.com/blog/curating-trillion-token-datasets-introducing-nemo-data-curator/</a>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling Laws for Neural Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korthikanti et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Reducing Activation Recomputation in Large Transformer Models, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.06226</em>, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Raymond Li, Loubna&nbsp;Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry&nbsp;Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh&nbsp;Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva&nbsp;Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn&nbsp;Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos&nbsp;Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von
Werra, and Harm de&nbsp;Vries.

</span>
<span class="ltx_bibblock">StarCoder: May the Source be with You!, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit&nbsp;Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li.

</span>
<span class="ltx_bibblock">Few-shot Learning with Multilingual Language Models, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA (2022)</span>
<span class="ltx_bibblock">
NVIDIA.

</span>
<span class="ltx_bibblock">H100 Tensor Core GPU Architecture Overview, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et&nbsp;al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">BLEU: A Method for Automatic Evaluation of Machine Translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</em>, ACL ’02, page 311–318, USA, 2002. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3115/1073083.1073135</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.3115/1073083.1073135" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3115/1073083.1073135</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ponti et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Edoardo&nbsp;Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen.

</span>
<span class="ltx_bibblock">XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2005.00333, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2005.00333" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2005.00333</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post.

</span>
<span class="ltx_bibblock">A Call for Clarity in Reporting BLEU Scores.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1804.08771, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1804.08771" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1804.08771</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jack&nbsp;W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van&nbsp;den Driessche, Lisa&nbsp;Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang&nbsp;Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de&nbsp;Masson&nbsp;d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de&nbsp;Las&nbsp;Casas, Aurelia Guy, Chris Jones,
James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed&nbsp;Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, 21(1):5485–5551, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.

</span>
<span class="ltx_bibblock">Socialiqa: Commonsense reasoning about social interactions, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander&nbsp;M. Rush, Stella Biderman, Albert Webson, Pawan&nbsp;Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert&nbsp;Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz&nbsp;Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro&nbsp;Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham&nbsp;Fikri Aji, Amit Alfassy, Anna Rogers, Ariel&nbsp;Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David&nbsp;Ifeoluwa Adelani, Dragomir Radev, Eduardo&nbsp;González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal&nbsp;Bar Natan, Francesco&nbsp;De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin,
Isaac Johnson, Itziar Gonzalez-Dios, Javier de&nbsp;la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro&nbsp;Von Werra, Leon Weber, Long Phan, Loubna&nbsp;Ben allal, Ludovic Tanguy, Manan Dey, Manuel&nbsp;Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh&nbsp;Chien Vu, Mohammad&nbsp;A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de&nbsp;Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto&nbsp;Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen&nbsp;Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago&nbsp;Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut&nbsp;Emre Taşar, Elizabeth Salesky, Sabrina&nbsp;J. Mielke, Wilson&nbsp;Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason&nbsp;Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M&nbsp;Saiful Bari, Maged&nbsp;S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen&nbsp;H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung&nbsp;Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette,
Pierre&nbsp;François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta&nbsp;Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica&nbsp;Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van&nbsp;der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol,
Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos&nbsp;Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong&nbsp;A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio&nbsp;Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav&nbsp;Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo&nbsp;Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu,
Clémentine Fourrier, Daniel&nbsp;León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena&nbsp;U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose&nbsp;David Posada, Karthik&nbsp;Rangasai Sivaraman, Lokesh Bulchandani, Lu&nbsp;Liu, Luisa Shinzato, Madeleine&nbsp;Hahn de&nbsp;Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria&nbsp;A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel&nbsp;De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas&nbsp;Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok&nbsp;S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,
Yash&nbsp;Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu&nbsp;Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf.

</span>
<span class="ltx_bibblock">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung&nbsp;Won Chung, Yi&nbsp;Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.

</span>
<span class="ltx_bibblock">Language Models are Multilingual Chain-of-Thought Reasoners, 2022.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shliazhko et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina.

</span>
<span class="ltx_bibblock">mGPT: Few-Shot Learners Go Multilingual, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Megatron-LM: Training Multi-Billion Parameter Language Models using Model Parallelism.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Slav&nbsp;Petrov and et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Andrew M.&nbsp;Dai Slav&nbsp;Petrov, Yonghui&nbsp;Wu and et&nbsp;al.

</span>
<span class="ltx_bibblock">PaLM 2 Technical Report, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://ai.google/static/documents/palm2techreport.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.google/static/documents/palm2techreport.pdf</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zheng, Rewon Child, Reza&nbsp;Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2201.11990, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2201.11990" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2201.11990</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Ahmed Murtadha, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced Transformer with Rotary Position Embedding.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.09864</em>, 2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzgun et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V. Le, Ed&nbsp;H. Chi, Denny Zhou, and Jason Wei.

</span>
<span class="ltx_bibblock">Challenging big-bench tasks and whether chain-of-thought can solve them, 2022.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language Models, 2023a.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-tuned Chat Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1706.03762, 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1706.03762" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1706.03762</a>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00359</em>, 2019.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da&nbsp;Pan, Dian Wang, Dong Yan, Fan Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Baichuan 2: Open Large-scale Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10305</em>, 2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">HellaSwag: Can a Machine Really Finish Your Sentence?

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2019.

</span>
</li>
</ul>
</section>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Supplementary Materials</h2>

<figure id="Ax1.T11" class="ltx_table">
<table id="Ax1.T11.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Ax1.T11.2.1.1" class="ltx_tr">
<th id="Ax1.T11.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="Ax1.T11.2.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt">Size</th>
<td id="Ax1.T11.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Humanities</td>
<td id="Ax1.T11.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Social sciences</td>
<td id="Ax1.T11.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">STEM</td>
<td id="Ax1.T11.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Other</td>
<td id="Ax1.T11.2.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">Average</td>
</tr>
<tr id="Ax1.T11.2.2.2" class="ltx_tr">
<th id="Ax1.T11.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Nemotron-4</th>
<th id="Ax1.T11.2.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t">15B</th>
<td id="Ax1.T11.2.2.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">69.2</td>
<td id="Ax1.T11.2.2.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">74.1</td>
<td id="Ax1.T11.2.2.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">53.4</td>
<td id="Ax1.T11.2.2.2.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">67.5</td>
<td id="Ax1.T11.2.2.2.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">64.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ax1.T11.3.1.1" style="font-size:90%;">Table 11</span>:</span><span class="ltx_text" id="Ax1.T11.4.2" style="font-size:90%;">Per-category breakdown accuracy for MMLU</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2402.16818" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2402.16819" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2402.16819">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.16819" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2402.16820" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 18:37:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Nemotron-4 15B Technical Report</title>
<!--Generated on Tue Feb 27 15:22:54 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2402.16819v2/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2402.16819v2">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2402.16819v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2402.16819v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S1" title="1 Introduction ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2" title="2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Architecture Details</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.SS0.SSS0.Px1" title="Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title">Data.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.SS0.SSS0.Px2" title="Pre-training. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title">Pre-training.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.SS0.SSS0.Px3" title="Continued Training. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title">Continued Training.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3" title="3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.SS1" title="3.1 Commonsense Reasoning ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Commonsense Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.SS2" title="3.2 Popular Aggregated Benchmarks ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Popular Aggregated Benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.SS3" title="3.3 Math and Code ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Math and Code</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.SS4" title="3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Multilingual</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S4" title="4 Conclusion ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2402.16819v2 [cs.CL] 27 Feb 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Nemotron-4 15B Technical Report</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jupinder Parmar<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
Equal contribution, corresponding authors: <span class="ltx_text ltx_font_typewriter" id="footnote1.1">{jupinderp,sprabhumoye,jjennings,mpatwary}@nvidia.com</span>.
</span></span></span> &nbsp;&nbsp;Shrimai Prabhumoye<math alttext="{}^{*}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><times id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math> &nbsp;&nbsp;Joseph Jennings<math alttext="{}^{*}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><times id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math> &nbsp;&nbsp;Mostofa Patwary<math alttext="{}^{*}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mo id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><times id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break">Sandeep Subramanian<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Work done while at NVIDIA.</span></span></span> &nbsp;&nbsp;Dan Su &nbsp;&nbsp;Chen Zhu &nbsp;&nbsp;Deepak Narayanan &nbsp;&nbsp; Aastha Jhunjhunwala &nbsp;&nbsp; Ayush Dattagupta &nbsp;&nbsp;Vibhu Jawa &nbsp;&nbsp;Jiwei Liu &nbsp;&nbsp;Ameya Mahabaleshwarkar &nbsp;&nbsp;Osvald Nitski &nbsp;&nbsp; Annika Brundyn &nbsp;&nbsp;James Maki &nbsp;&nbsp;Miguel Martinez &nbsp;&nbsp;Jiaxuan You &nbsp;&nbsp;John Kamalu &nbsp;&nbsp;Patrick LeGresley &nbsp;&nbsp;Denys Fridman &nbsp;&nbsp;Jared Casper &nbsp;&nbsp; Ashwath Aithal &nbsp;&nbsp;Oleksii Kuchaiev &nbsp;&nbsp;Mohammad Shoeybi &nbsp;&nbsp;Jonathan Cohen &nbsp;&nbsp;Bryan Catanzaro <span class="ltx_text ltx_font_bold" id="id4.4.id1">NVIDIA</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id5.id1">우리는 8조 개의 텍스트 토큰으로 훈련된 150억 개의 매개 변수 대형 다국어 언어 모델인 네모트론-4 15B를 소개한다. 네모트론-4 15B는 영어, 다국어 및 코딩 작업에 대해 평가될 때 강력한 성능을 보여주며, 이는 7개의 다운스트림 평가 영역 중 4개에서 기존의 유사한 크기의 모든 오픈 모델을 능가하고 나머지 영역에서 선두 오픈 모델과 경쟁 성능을 달성한다. 특히, 네모트론-4 15B는 유사하게 크기가 큰 모든 모델의 최상의 다국어 능력을 보여주며, 심지어 4배 이상 큰 모델과 다국어 작업에 명시적으로 특화된 모델을 능가한다.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section">1&nbsp;&nbsp;&nbsp;Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">최근 발표된 노력들 <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib21" title="">2022</a>; Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib45" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib46" title="">b</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib49" title="">2023</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib23" title="">2023</a>)</cite>는 고정된 컴퓨팅 예산이 주어진 모델 크기와 함께 스케일링 데이터를 주장한 Chinchilla 스케일링 법칙 <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib24" title="">2020</a>; Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib7" title="">2020</a>; Smith et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib42" title="">2022</a>; Rae et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib33" title="">2022</a>; Scao et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib37" title="">2023</a>)</cite>에서 영감을 받았다. 예를 들어, <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib21" title="">2022</a>)</cite>는 유사한 데이터 분포를 갖는 두 개의 대략적인 IsoFLOP GPT 모델, 1.4조 토큰에 대한 65-빌리언-파라미터 모델 및 3천억 토큰에 대한 280-빌리언-파라미터 모델이 주어진 경우, 65B 모델이 다운스트림 태스크에서 더 나은 정확도를 갖는다는 것을 보여준다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">모델 크기를 늘리는 대신 더 많은 데이터에 대한 훈련에 컴퓨팅을 할당하는 이러한 트레이드오프는 추론 관점에서 특히 매력적이며, 지연 시간과 모델을 서비스하는 데 필요한 컴퓨팅의 양을 줄인다. 그 결과 언어 모델링 훈련 노력의 주요 초점은 커먼 크롤과 같은 공개 소스에서 고품질 수십조 토큰 데이터 세트를 수집하는 것으로 전환되었다. 우리는 영어, 다국어, 코딩 텍스트의 8조 토큰에 대해 훈련되었고 단일 NVIDIA A100 또는 H100 GPU에 적합할 수 있는 최고의 범용 대용량 언어 모델(LLM)로 개발된 네모트론-4 15B를 도입하여 이러한 추세를 지속한다.</p>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">도표 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">1</span></a>에서 입증된 바와 같이, 네모트론-4 15B는 광범위한 영어, 코드 및 다국어 평가 영역에 걸쳐 높은 다운스트림 정확도를 나타낸다. 유사한 크기의 오픈 모델과 비교하여, 모든 영어 평가 영역에서 Nemotron-4 15B가 LLaMA-2 34B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib46" title="">2023b</a>)</cite>보다 훨씬 낫고, Mistral 7B <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib23" title="">2023</a>)</cite>보다 더 낫다는 것을 보여준다. 추가적으로, Nemotron-4 15B는 QWEN 14B <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib5" title="">2023</a>)</cite> 및 Gemma 7B <cite class="ltx_cite ltx_citemacro_citep">(Gemma Team, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib17" title="">2024</a>)</cite>에 대한 경쟁적 정확도를 달성한다. 광범위한 프로그래밍 언어에 걸친 비교에서, 우리는 Nemotron-4 15B가 Starcoder <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib27" title="">2023</a>)</cite>, 코드 특정 모델 및 Mistral 7B보다 더 나은 평균 정확도를 달성하고, 특히 낮은 리소스 프로그래밍 언어에서 더 나은 평균 정확도를 달성한다는 것을 발견했다. 네모트론-4 15B는 상당한 양의 다국어 데이터에 대해 훈련되었기 때문에 현재 모든 다국어 벤치마크에 대한 크기 클래스에서 최첨단 범용 모델이다. 우리는 Nemotron-4가 PALM 62B-Cont <cite class="ltx_cite ltx_citemacro_citep">(Slav Petrov and et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib41" title="">2023</a>)</cite>보다 낫고, 또한 XGLM <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib28" title="">2022</a>)</cite> 및 mGPT <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib39" title="">2022</a>)</cite>와 같은 다국어 특화 모델보다 우수하다는 것을 알아냈다.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S1.F1.1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="512" id="S1.F1.1.g1" src="https://arxiv.org/html/2402.16819v2/x1.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S1.F1.2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="517" id="S1.F1.2.g1" src="https://arxiv.org/html/2402.16819v2/x2.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.4.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.5.2" style="font-size:90%;">Comparison of Nemotron-4 15B across seven evaluation areas against similarly sized models. The composition of tasks that form each evaluation area can be found, along with more detailed evaluation results, in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3" title="3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">3</span></a></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S1.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.2.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.1">Number of</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.2">Hidden</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.3">Number of</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.4">Number of</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.5">Sequence</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.6">Vocabulary</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.2.2">
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.1">transformer layers</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.2">dimension</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.3">attention heads</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.4">KV heads</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.5">length</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.6">size</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.3.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.1">32</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.2">6144</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.3">48</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.4">8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.5">4096</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.6">256,000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S1.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S1.T1.4.2" style="font-size:90%;">Key hyper-parameters affecting size of Nemotron-4 15B.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section">2&nbsp;&nbsp;&nbsp;Architecture Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Nemotron-4는 표준 디코더 전용 Transformer 아키텍처 <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib47" title="">2017</a>)</cite>,인과 주의 마스크를 사용한다. 크기에 영향을 미치는 정확한 하이퍼-파라미터는 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">1</span></a>와 같다. 네모트론-4는 32억 개의 임베딩 파라미터와 125억 개의 비 임베딩 파라미터를 갖는다. Rotary Position Embeddings (RoPE) <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib43" title="">2021</a>)</cite>, SentencePiece tokenizer <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib26" title="">2018</a>)</cite>, squared ReLU activations in the MLP layers, no bias terms, dropout rate of zero, and unied input-output embeddings. 그룹화된 쿼리 어텐션(GQA) <cite class="ltx_cite ltx_citemacro_citep">(Ainslie et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib2" title="">2023</a>)</cite>를 사용하여 더 빠른 추론과 낮은 메모리 풋프린트를 제공합니다.</p>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Data.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">우리는 8조 개의 토큰으로 구성된 사전 훈련 데이터 세트에서 네모트론-4 15B를 훈련한다. 높은 수준에서 데이터 혼합은 영어 자연어 데이터(70%), 다국어 자연어 데이터(15%), 소스 코드 데이터(15%)의 세 가지 다른 유형의 데이터로 분할된다.</p>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="280" id="S2.F2.g1" src="https://arxiv.org/html/2402.16819v2/extracted/5435317/plots/new_full_distr.png" width="509">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Data composition of the English tokens used for pre-training</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">영어 코퍼스는 웹 문서, 뉴스 기사, 과학 논문, 책 등을 포함한 다양한 출처와 도메인의 선별된 문서로 구성되며 사전 훈련 세트에 사용된 분포는 그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.F2" title="Figure 2 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">2</span></a>에서 강조 표시된다. 코드 및 다국어 데이터는 다양한 자연 언어 및 프로그래밍 언어 집합으로 구성된다. 우리는 이러한 언어에서 토큰을 적절하게 샘플링하는 것이 이러한 도메인에서 강력한 정확도의 핵심이라는 것을 발견했다. 우리는 그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.F3" title="Figure 3 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">3</span></a> 및 그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.F4" title="Figure 4 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">4</span></a>의 사전 훈련 데이터 세트에서 코드 및 다국어 토큰 모두에 사용된 분포를 각각 공유한다.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="347" id="S2.F3.g1" src="https://arxiv.org/html/2402.16819v2/extracted/5435317/plots/test_code.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Data distribution of the 43 programming languages used for pre-training. The number within each bar indicates the percent of the overall code distribution that an individual language comprises.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p3.1">사전 학습 코퍼스를 구성할 때 문서 수준의 정확성과 근접 중복 제거 <cite class="ltx_cite ltx_citemacro_citep">(Jennings et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib22" title="">2023</a>)</cite>를 통해 가능한 중복을 제거한다. 우리는 <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib48" title="">2019</a>)</cite>에 설명된 일련의 휴리스틱 필터 외에도 <cite class="ltx_cite ltx_citemacro_citep">(Rae et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib33" title="">2022</a>)</cite> 및 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib34" title="">2020</a>)</cite>와 유사한 언어 모델 기반 필터링 접근법을 사용하여 코퍼스에 걸쳐 문서 수준 품질 필터링을 추가로 적용했다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p4">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p4.1">최종 8T 토큰 데이터셋에서 무작위로 샘플링된 데이터에 SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib26" title="">2018</a>)</cite>에서 BPE tokenizer를 학습한다. 토큰나이저에서 낮은 리소스 언어의 적용 범위를 높이기 위해 최종 훈련 데이터 세트 분포에 비해 영어가 아닌 데이터를 업샘플링한다. 토큰사이저는 화이트 스페이스(선행 및 후행 포함)를 보존하고, 숫자를 개별 숫자로 분할합니다. <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib12" title="">2022</a>)</cite>, 바이트 수준 백오프에 의존하여 알려지지 않은 문자 시퀀스를 처리합니다. 최종 어휘 크기는 256,000 토큰입니다.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="274" id="S2.F4.g1" src="https://arxiv.org/html/2402.16819v2/extracted/5435317/plots/test_multi.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">Data distribution of the 53 natural languages, aside from English,we used for pre-training. The number within each bar indicates the percent of the overall multilingual distribution that an individual language comprises.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Pre-training.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Nemotron-4는 384개의 DGX H100 노드를 사용하여 훈련되었으며, 각 노드는 NVIDIA Hopper 아키텍처에 기반한 8개의 H100 80GB SXM5 GPU를 포함한다. 각 H100 GPU는 희소성이 없는 16비트 부동 소수점(<span class="ltx_text ltx_font_typewriter" id="S2.SS0.SSS0.Px2.p1.1.1">bfloat16</span>) 산술을 수행할 때 989 teraFLOP/s의 피크 처리량을 갖는다. 각 노드 내에서 GPU는 NVLink와 NVSwitch <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib1" title="">nvl, </a>)</cite>로 연결 되며 GPU 대 GPU 대역폭은 900 GB/s (각 방향으로 450 GB/s)입니다. 각 노드에는 노드 간 통신을 위한 8개의 NVIDIA Mellanox 400 Gbps HDR InfiniBand 호스트 채널 어댑터(HCAs)가 있다.</p>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">8-way tensor parallelism <cite class="ltx_cite ltx_citemacro_citep">(Shoeybi et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib40" title="">2019</a>)</cite>와 데이터 parallelism의 조합을 사용하여 모델을 학습하고, 분산 최적화기를 사용하여 데이터 병렬 복제본 위에 최적화기 상태를 샤드한다. 데이터 병렬성의 정도는 배치 크기가 증가함에 따라 96에서 384까지 다양했다. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.T2" title="Table 2 ‣ Pre-training. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">2</span></a>는 batch size ramp의 3단계를 요약한 것으로, per-iteration time and model FLOP/s utilization (MFU) <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib12" title="">2022</a>; Korthikanti et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib25" title="">2022</a>)</cite>를 포함한다. MFU는 GPU가 모델 훈련에서 얼마나 효율적으로 활용되는지를 정량화한다. 교육은 약 13일 만에 완료되었다.</p>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T2.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.1">Data-parallel size</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.2">GPUs</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.3">Iteration time (secs)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.4">MFU (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.5">Batch size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.6">Tokens (B)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.7">Time (days)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.1">96</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T2.2.2.1.2">768</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.3">0.57</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.4">34.3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.5">384</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.6">200</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.7">0.8</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.3.2">
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.1">192</td>
<td class="ltx_td ltx_align_right" id="S2.T2.2.3.2.2">1,536</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.3">0.58</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.4">33.3</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.5">768</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.6">200</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.7">0.4</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.1">288</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S2.T2.2.4.3.2">2,304</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.3">0.64</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.4">30.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.5">1,152</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.6">7,600</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.7">11.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S2.T2.4.2" style="font-size:90%;">Batch size rampup schedule, along with time and efficiency metrics for the Nemotron-4 15B parameter model.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Continued Training.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">최근 작업 <cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib18" title="">2023</a>)</cite>와 유사하게 모델 학습이 끝날 때 데이터 분포와 학습률 감소 스케줄을 전환하면 모델 품질이 크게 향상됨을 알 수 있다. 구체적으로, 8T 사전 훈련 데이터 세트 전체에 대해 훈련한 후 동일한 손실 목표를 사용하고 사전 훈련 토큰과 비교하여 적은 수의 토큰에 대해 지속적인 훈련을 수행한다.</p>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p2.1">계속된 훈련의 이 추가 단계에서 우리는 두 가지 별개의 데이터 분포를 활용한다. 첫 번째 분포는 지속적인 훈련 동안 대부분의 토큰이 샘플링되는 곳입니다. 사전 훈련 중에 이미 도입되었지만 더 높은 품질의 소스에 더 큰 샘플링 가중치를 두는 분포와 함께 토큰을 활용한다. 두 번째 분포는 모델이 다운스트림 평가에서 이러한 질문에 더 잘 응답할 수 있도록 하는 동시에 모델 성능이 낮은 영역에서 나오는 데이터 소스를 가중화할 수 있도록 소수의 벤치마크 스타일 정렬 예를 도입한다. 학습 속도의 크기보다 더 가파른 감쇠 기울기를 우선시하는 학습 속도 스케줄과 함께, 이러한 데이터 분포의 순서와 스타일이 모델이 사전 훈련 데이터 세트로부터 부드럽게 전이하고 새로 강조된 데이터 영역을 더 잘 학습할 수 있게 한다는 것을 발견한다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section">3&nbsp;&nbsp;&nbsp;Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">우리는 다양한 범위의 작업과 영역을 포함하는 다양한 다운스트림 평가 영역에서 네모트론-4 15B를 평가한다. 모든 평가에서 표준화된 작업 설정을 준수하고 사용된 정확한 설정을 공유합니다. 상기 커버링된 평가 카테고리들은,</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Commonsense Reasoning (0-shot):</span> SIQA <cite class="ltx_cite ltx_citemacro_citep">(Sap et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib36" title="">2019</a>)</cite>, ARC easy and challenge <cite class="ltx_cite ltx_citemacro_citep">(Clark et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib14" title="">2018</a>)</cite>, PIQA <cite class="ltx_cite ltx_citemacro_citep">(Bisk et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib6" title="">2020</a>)</cite>, Winogrande <cite class="ltx_cite ltx_citemacro_citep">(Sakaguchi et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib35" title="">2020</a>)</cite>, Hell</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Popular Aggregated Benchmarks:</span> MMLU (5-shot) <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib20" title="">2020</a>)</cite> and BBH (3-shot) <cite class="ltx_cite ltx_citemacro_citep">(Suzgun et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib44" title="">2022</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Math:</span> GSM8K (8-shot with maj@1) <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib15" title="">2021</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Code:</span> Pass@1 scores on HumanEval (0-shot) <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib11" title="">2021</a>)</cite>, MBPP (3-shot) <cite class="ltx_cite ltx_citemacro_citep">(Austin et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib4" title="">2021</a>)</cite>, and MultiPL-E (0-shot) <cite class="ltx_cite ltx_citemacro_citep">(Cassano et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib8" title="">2023a</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">Multilingual:</span> classification via XCOPA (0 and 4-shot) <cite class="ltx_cite ltx_citemacro_citep">(Ponti et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib31" title="">2020</a>)</cite>, machine translation with FLORES-101 (8-shot) <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib19" title="">2021</a>)</cite> 및 MGSM (8-shot) <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib38" title="">2022</a>)</cite>와 TyDiQA (1-shot) <cite class="ltx_cite ltx_citemacro_citep">(</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">우리의 평가에서 우리는 많은 외부 디코더 전용 변압기 언어 모델과 비교하고 달리 명시되지 않는 한 해당 모델의 보고서에 게시된 번호를 사용한다. 영어 및 코드 작업의 경우, Nemotron-4 15B, LlaMA-2 13B 및 34B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib46" title="">2023b</a>)</cite>, Mistral 7B <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib23" title="">2023</a>)</cite>, Baichuan-2 13B <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib49" title="">2023</a>)</cite>, QWEN 14B <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib5" title="">2023</a>)</cite>, Gemma 7B <cite class="ltx_cite ltx_citemacro_citep"> 다국어 벤치마크의 경우 PaLM 62B 및 62B-cont <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib12" title="">2022</a>)</cite>뿐만 아니라 mGPT 13B <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib39" title="">2022</a>)</cite> 및 XGLM 7.5B <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib28" title="">2022</a>)</cite>와 같은 다국어 능력을 위해 특별히 훈련된 모델에 대한 결과를 보고한다.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Commonsense Reasoning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">LM-Evaluation Harness <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib16" title="">2021</a>)</cite>를 사용하여 앞서 언급한 모든 작업에 걸쳐 Nemotron-4 15B를 평가한다. 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T3" title="Table 3 ‣ 3.1 Commonsense Reasoning ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">3</span></a>는 네모트론-4 15B가 이 다양한 작업 집합에서 가장 강력한 평균 성능을 달성한다는 것을 보여준다.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.1" style="width:305.9pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T3.1.1"><span class="ltx_text" id="S3.T3.1.1.1"></span></p>
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T3.1.1.1.1" style="width:305.9pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T3.1.1.1.1.1"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.1.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.1"></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.2">Size</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.3">SIQA</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.4">ARC-c</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.5">ARC-e</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.6">PIQA</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.7">Winogrande</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.8">Hellaswag</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.9">AVG</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.3.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T3.1.1.1.1.1.1.1.3.1.1"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1.1.3.1.1.1">LLaMA-2</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.2">13B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.3">50.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.4">49.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.5">77.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.6">79.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.7">72.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.8">80.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.9">68.4</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.4.2">
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T3.1.1.1.1.1.1.1.4.2.1">34B</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.2">50.9</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.3">54.5</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.4">79.4</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.5">81.9</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.6">76.7</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.7">83.3</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.8">71.1</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.5.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.1"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1.1.5.3.1.1">Baichuan-2</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.2">13B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.3">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.4">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.5">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.6">78.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.7">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.8">70.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.9">-</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.6.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.1">QWEN</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.2">14B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.3">77.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.4">84.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.5">90.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.6">79.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.7">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.8">80.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.9">-</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.2">Mistral</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.3">7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.1">&nbsp;&nbsp;47.0<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1a"><msup id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1a" xref="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.1" mathcolor="#000000" xref="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1"><times id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.4">55.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.5">80.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.6">83.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.7">75.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.8">81.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.9">70.4</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.7.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.1">Gemma</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.2">7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.3">51.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.4">53.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.5">81.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.6">81.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.7">72.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.8">81.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.9">70.2</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.8.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.1">Nemotron-4</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.2">15B</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.3">60.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.4">55.5</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.5">80.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.6">82.4</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.7">78.0</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.8">82.4</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.9"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1.1.1.8.6.9.1">73.4</span></span></span>
</span>
</span>
</span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.5.2.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.3.1" style="font-size:90%;"> Results on standard reasoning benchmarks in the zero-shot setting. We report the average across all tasks where possible for a fair comparison. The values marked with <math alttext="*" class="ltx_Math" display="inline" id="S3.T3.3.1.m1.1"><semantics id="S3.T3.3.1.m1.1b"><mo id="S3.T3.3.1.m1.1.1" xref="S3.T3.3.1.m1.1.1.cmml">*</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.1.m1.1c"><times id="S3.T3.3.1.m1.1.1.cmml" xref="S3.T3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.1.m1.1d">*</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.1.m1.1e">*</annotation></semantics></math> are read from &nbsp;<cite class="ltx_cite ltx_citemacro_citet">Gemma&nbsp;Team (<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib17" title="">2024</a>)</cite> </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Popular Aggregated Benchmarks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">MLU<cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib20" title="">2020</a>)</cite> and Big Bench Hard (BBH)<cite class="ltx_cite ltx_citemacro_citep">(Suzgun et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib44" title="">2022</a>)</cite> 벤치마크는 광범위한 작업 및 도메인에 대한 언어 모델의 능력에 대한 도전적인 평가로서 개발되었다. 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T4" title="Table 4 ‣ 3.2 Popular Aggregated Benchmarks ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">4</span></a>에서 볼 수 있듯이, 네모트론-4 15B는 거의 7%의 규모에서 기존 모델에 걸쳐 BBH에서 최상의 점수를 달성한다. 추가적으로, Nemotron-4는 LLaMA-2 70B가 51.2의 점수를 획득하고 Nemotron-4가 58.7인 BBH 벤치마크에서 LLaMA-2 70B 모델보다 상당히 우수하다. Nemotron-4 15B는 추가적으로 매우 경쟁적인 MMLU 점수를 획득하고 MMLU에 대한 카테고리별 성능은 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#Ax1.T11" title="Table 11 ‣ Supplementary Materials ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">11</span></a>에서 찾을 수 있다.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.2.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T4.2.1.1.1"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T4.2.1.1.2">Size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.1.1.3">BBH</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.1.1.4">MMLU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.1.1" rowspan="2"><span class="ltx_text" id="S3.T4.2.2.1.1.1">LLaMA-2</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.1.2">13B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.1.3">39.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.1.4">54.8</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.3.2">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T4.2.3.2.1">34B</th>
<td class="ltx_td ltx_align_center" id="S3.T4.2.3.2.2">44.1</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.3.2.3">62.6</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.4.3.1"><span class="ltx_text" id="S3.T4.2.4.3.1.1">Baichuan-2</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.4.3.2">13B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.4.3.3">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.4.3.4">59.2</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.5.4.1">QWEN</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.5.4.2">14B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.5.4.3">53.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.5.4.4"><span class="ltx_text ltx_font_bold" id="S3.T4.2.5.4.4.1">66.3</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.6.5.1">Mistral</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.6.5.2">7B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.6.5.3">39.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.6.5.4">60.1</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.7.6.1">Gemma</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.7.6.2">7B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.7.6.3">55.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.7.6.4">64.3</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T4.2.8.7.1">Nemotron-4</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T4.2.8.7.2">15B</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.2.8.7.3"><span class="ltx_text ltx_font_bold" id="S3.T4.2.8.7.3.1">58.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.2.8.7.4">64.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S3.T4.4.2" style="font-size:90%;"> Nemotron-4 15B attains highly competitive performance on popular aggregate benchmarks. The BBH result for Mistral is read from the figure in&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib23" title="">2023</a>)</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Math and Code</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">최근, 대형 언어 모델들은 수학적 추론과 다양한 코딩 작업들 모두에서 효과적인 것으로 보여지고 있다 <cite class="ltx_cite ltx_citemacro_citep">(Allal et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib3" title="">2023</a>; Chowdhery et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib12" title="">2022</a>; Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib45" title="">2023a</a>)</cite>. 표<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T5" title="Table 5 ‣ 3.3 Math and Code ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">5</span></a>는 이러한 작업에 대한 네모트론-4 15B의 성능을 강조한다. 특히, 수학적 추론에서 네모트론-4 15B는 젬마 7B와 유사한 점수를 얻으면서 강한 성능을 달성하지만 바이촨-2 및 QWEN과 같은 모델보다 뒤처진다는 것을 발견했다. 코드 작업에서 우리는 네모트론-4가 젬마 7B 뒤에 약간 남아 있는 동안 QWEN 14B와 동등하게 수행하는 것을 본다. 두 가지 유형의 작업 모두에서 네모트론-4 15B는 미스트랄 7B 및 LlaMA-2 13B/34B를 능가할 수 있다.</p>
</div>
<figure class="ltx_table" id="S3.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T5.2" style="width:189.7pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T5.2.2"><span class="ltx_text" id="S3.T5.2.2.2"></span></p>
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T5.2.2.2.2" style="width:189.7pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T5.2.2.2.2.2"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T5.2.2.2.2.2.2.2">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.3.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T5.2.2.2.2.2.2.2.3.1.1"></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T5.2.2.2.2.2.2.2.3.1.2">Size</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.2.2.2.2.2.2.2.3.1.3">GSM8K</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.2.2.2.2.2.2.2.3.1.4">HumanEval</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.2.2.2.2.2.2.2.3.1.5">MBPP</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.4.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T5.2.2.2.2.2.2.2.4.1.1"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.4.1.1.1">LlaMA-2</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.4.1.2">13B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.4.1.3">28.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.4.1.4">18.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.4.1.5">30.6</span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.5.2">
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T5.2.2.2.2.2.2.2.5.2.1">34B</span>
<span class="ltx_td ltx_align_center" id="S3.T5.2.2.2.2.2.2.2.5.2.2">42.2</span>
<span class="ltx_td ltx_align_center" id="S3.T5.2.2.2.2.2.2.2.5.2.3">22.6</span>
<span class="ltx_td ltx_align_center" id="S3.T5.2.2.2.2.2.2.2.5.2.4">33.0</span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.6.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.6.3.1"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.6.3.1.1">Baichuan-2</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.6.3.2">13B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.6.3.3">52.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.6.3.4">17.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.6.3.5">30.2</span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.7.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.7.4.1"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.7.4.1.1">QWEN</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.7.4.2">14B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.7.4.3"><span class="ltx_text ltx_font_bold" id="S3.T5.2.2.2.2.2.2.2.7.4.3.1">60.1</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.7.4.4">32.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.7.4.5">40.8</span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.2.3"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.2.3.1">Mistral</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.2.4">7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.1.1.1.1.1.1.1.1">35.4<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1a"><msup id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1a" xref="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.1" mathcolor="#000000" xref="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1"><times id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.2.5">30.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.2.2">&nbsp;&nbsp;40.2<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1"><semantics id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1a"><msup id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1" xref="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.cmml"><mi id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1a" xref="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.cmml"></mi><mo id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.1" mathcolor="#000000" xref="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1b"><apply id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1"><times id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.1.cmml" xref="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.8.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.8.5.1"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.8.5.1.1">Gemma</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.8.5.2">7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.8.5.3">46.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.8.5.4"><span class="ltx_text ltx_font_bold" id="S3.T5.2.2.2.2.2.2.2.8.5.4.1">32.3</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.8.5.5"><span class="ltx_text ltx_font_bold" id="S3.T5.2.2.2.2.2.2.2.8.5.5.1">44.4</span></span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.9.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.9.6.1"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.9.6.1.1">Nemotron-4</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.9.6.2">15B</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.9.6.3">46.0</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.9.6.4">31.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.9.6.5">40.6</span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T5.4.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S3.T5.5.2" style="font-size:90%;"> Comparative results on math and code benchmarks. As Mistral 7B reports MBPP performance on a different eval split and uses a different evaluation setting for GSM8K , we use the corresponding numbers reported in&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gemma&nbsp;Team, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib17" title="">2024</a>)</cite></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">비슷한 크기의 거의 모든 개방형 모델은 다른 프로그래밍 언어에 대한 성능 평가를 무시하고 파이썬 관련 작업에 대한 성능만을 기반으로 코드 능력을 결정한다. 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T6" title="Table 6 ‣ 3.3 Math and Code ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">6</span></a>에서는 11개의 다양한 프로그래밍 언어에 걸쳐 Multiple-E <cite class="ltx_cite ltx_citemacro_citep">(Cassano et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib9" title="">2023b</a>)</cite> 벤치마크에 대한 Nemotron-4 15B의 결과를 보여주고, 이를 Mistral 7B 및 Starcoder <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib27" title="">2023</a>)</cite>, 코드에 대해 특별히 훈련된 15B 파라미터 모델과 비교한다. 우리는 네모트론-4 15B가 다양한 프로그래밍 언어에 걸쳐 강력한 코딩 성능을 달성하고 스타코더와 미스트랄 7B보다 평균적으로 더 우수하다는 것을 발견했다. 특히 Scala, Julia 및 R과 같은 자원이 적은 프로그래밍 언어에서 Nemotron-4 15B의 우수한 성능을 강조한다.</p>
</div>
<figure class="ltx_table" id="S3.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T6.2" style="width:388.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T6.2.1"><span class="ltx_text" id="S3.T6.2.1.1"></span></p>
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T6.2.1.1.1" style="width:388.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T6.2.1.1.1.1"><span class="ltx_text" id="S3.T6.2.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T6.2.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T6.2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.1"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.2">Size</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.3">JavaScript</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.4">Julia</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.5">Java</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.6">Lua</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.7">C++</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.8">C-Sharp</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.9">PHP</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.10">Shell</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.11">TypeScript</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.12">R</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.13">Scala</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.14">AVG</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T6.2.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.1">Starcoder</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.2">15B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.3">30.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.4">23.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.5">30.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.6">23.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.7">31.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.8">21.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.9">26.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.10">10.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.11">32.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.12">15.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.13">27.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.14">24.2</span></span>
<span class="ltx_tr" id="S3.T6.2.1.1.1.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T6.2.1.1.1.1.1.1.3.2.1">Mistral</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.2">7B</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.3">34.2</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.4">22.0</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.5">26.0</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.6">25.3</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.7">29.1</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.8">22.8</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.9">27.9</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.10">8.9</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.11">28.5</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.12">11.8</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.13">22.2</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.14">23.6</span></span>
<span class="ltx_tr" id="S3.T6.2.1.1.1.1.1.1.4.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.1">Nemotron-4</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.2">15B</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.3">28.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.4">24.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.5">24.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.6">24.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.7">35.4</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.8">21.1</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.9">27.3</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.10">8.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.11">32.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.12">18.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.13">27.3</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.14"><span class="ltx_text ltx_font_bold" id="S3.T6.2.1.1.1.1.1.1.4.3.14.1">24.5</span></span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T6.3.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S3.T6.4.2" style="font-size:90%;"> Nemotron-4 15B attains high competency in coding performance across a broad range of programming languages. Results for Mistral are from our runs of Mistral in the same setting as Nemotron-4. </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multilingual</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">우리는 다양한 범위의 높은 자원에서 낮은 자원 자연 언어를 다루는 이전 연구에서 널리 연구된 4개의 벤치마크를 사용하여 네모트론-4 15B의 뛰어난 다국어 능력을 보여준다. 분류는 정확도를 메트릭으로 사용 합니다. 생성 작업의 경우 정확한 일치를 사용 합니다. 그리고 기계 번역의 경우 <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.1.1">sacreBLEU</span> <cite class="ltx_cite ltx_citemacro_citep">(Post, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib32" title="">2018</a>)</cite> <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.1.2">BLEU</span> <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib30" title="">2002</a>)</cite> <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.1.3">spm-flores-101</span> tokenization을 사용하여 spBLEU 점수를 얻습니다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">1. Classification:</span> Cross-lingual Choice of Plausible Alternatives (XCOPA) <cite class="ltx_cite ltx_citemacro_citep">(Ponti et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib31" title="">2020</a>)</cite> tests causal commonsense reasoning in 11 languages</p>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">Nemotron-4 15B를 기존의 다국어 언어 모델인 XGLM <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib28" title="">2022</a>)</cite>, mGPT <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib39" title="">2022</a>)</cite> 및 BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Scao et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib37" title="">2023</a>)</cite>와 비교한다. XGLM 및 mGPT는 훈련 데이터에 비영어 언어의 존재를 업 샘플링하여 향상된 다국어 능력을 갖도록 특별히 훈련된 모델이다. 대조적으로, BLOOM은 네모트론-4와 마찬가지로 영어, 다국어 및 코드 데이터의 조합으로 훈련된 범용 언어 모델이다. 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T7" title="Table 7 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">7</span></a>에서 우리는 네모트론-4가 모든 모델 중에서 최고의 성능을 달성한다는 것을 분명히 볼 수 있는데, 이는 4샷 설정에서 거의 12% 개선을 실현한다.</p>
</div>
<figure class="ltx_table" id="S3.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T7.5" style="width:359.5pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T7.5.5"><span class="ltx_text" id="S3.T7.5.5.5"></span></p>
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T7.5.5.5.5" style="width:359.5pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T7.5.5.5.5.5"><span class="ltx_text" id="S3.T7.5.5.5.5.5.5" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T7.5.5.5.5.5.5.5">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.6.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.1">Mode</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.2">Model</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.3">Size</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.4">ET</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.5">HT</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.6">ID</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.7">IT</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.8">QU</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.9">SW</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.10">TA</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.11">TH</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.12">TR</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.13">VI</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.14">ZH</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.15">AVG</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_4" id="S3.T7.5.5.5.5.5.5.5.5.6"><span class="ltx_text" id="S3.T7.5.5.5.5.5.5.5.5.6.1">Zero-Shot</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.7">BLOOM</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.8">176B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.9">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.10">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.1.1.1.1.1.1.1.1.1"><math alttext="57.5^{*}" class="ltx_Math" display="inline" id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1a"><msup id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2" mathcolor="#000000" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">57.5</mn><mo id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3" mathcolor="#000000" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><cn id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2">57.5</cn><times id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1c">57.5^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1d">57.5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.11">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.12">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.2.2.2.2.2.2.2.2.2"><math alttext="59.5^{*}" class="ltx_Math" display="inline" id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1"><semantics id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1a"><msup id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.cmml"><mn id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.2" mathcolor="#000000" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.2.cmml">59.5</mn><mo id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.3" mathcolor="#000000" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1b"><apply id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.1.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1">superscript</csymbol><cn id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.2.cmml" type="float" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.2">59.5</cn><times id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.3.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1c">59.5^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1d">59.5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.3.3.3.3.3.3.3.3.3"><math alttext="54.7^{*}" class="ltx_Math" display="inline" id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1"><semantics id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1a"><msup id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.cmml"><mn id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.2" mathcolor="#000000" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.2.cmml">54.7</mn><mo id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.3" mathcolor="#000000" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1b"><apply id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.1.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1">superscript</csymbol><cn id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.2.cmml" type="float" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.2">54.7</cn><times id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.3.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1c">54.7^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1d">54.7 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.13">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.14">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.4.4.4.4.4.4.4.4.4"><math alttext="58.2^{*}" class="ltx_Math" display="inline" id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1"><semantics id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1a"><msup id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.cmml"><mn id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.2" mathcolor="#000000" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.2.cmml">58.2</mn><mo id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.3" mathcolor="#000000" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1b"><apply id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.1.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1">superscript</csymbol><cn id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.2.cmml" type="float" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.2">58.2</cn><times id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.3.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1c">58.2^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1d">58.2 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.5"><math alttext="57.7^{*}" class="ltx_Math" display="inline" id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1"><semantics id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1a"><msup id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.cmml"><mn id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.2" mathcolor="#000000" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.2.cmml">57.7</mn><mo id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.3" mathcolor="#000000" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1b"><apply id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.1.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1">superscript</csymbol><cn id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.2.cmml" type="float" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.2">57.7</cn><times id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.3.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1c">57.7^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1d">57.7 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.15">-</span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.7.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.7.1.1">XGLM</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.7.1.2">7.5B</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.3">57.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.4">57.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.5">59.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.6">49.2</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.7">52.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.8">55.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.9">55.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.10">57.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.11">55.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.12">59.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.13">53.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.14">55.6</span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.8.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.8.2.1">mGPT</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.8.2.2">13B</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.3">49.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.4">50.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.5">63.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.6">61.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.7">50.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.8">57.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.9">57.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.10">54.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.11">58.2</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.12">60.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.13">54.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.14">56.1</span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.9.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.9.3.1">Nemotron-4</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.9.3.2">15B</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.3">62.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.4">47.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.5">66.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.6">67.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.7">53.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.8">50.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.9">62.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.10">59.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.11">57.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.12">65.2</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.13">62.2</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.14"><span class="ltx_text ltx_font_bold" id="S3.T7.5.5.5.5.5.5.5.9.3.14.1">59.5</span></span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.10.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3" id="S3.T7.5.5.5.5.5.5.5.10.4.1"><span class="ltx_text" id="S3.T7.5.5.5.5.5.5.5.10.4.1.1">4-Shot</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.2">XGLM</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.3">7.5B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.4">64.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.5">60.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.6">67.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.7">64.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.8">50.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.9">61.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.10">56.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.11">61.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.12">60.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.13">68.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.14">59.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.15">61.4</span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.11.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.11.5.1">mGPT</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.11.5.2">13B</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.3">48.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.4">48.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.5">62.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.6">60.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.7">50.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.8">56.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.9">55.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.10">54.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.11">57.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.12">61.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.13">58.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.14">56.0</span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.12.6">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.1">Nemotron-4</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.2">15B</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.3">72.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.4">52.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.5">79.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.6">79.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.7">50.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.8">52.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.9">72.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.10">66.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.11">77.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.12">78.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.13">76.0</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.14"><span class="ltx_text ltx_font_bold" id="S3.T7.5.5.5.5.5.5.5.12.6.14.1">68.9</span></span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T7.9.2.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S3.T7.7.1" style="font-size:90%;"> Comparison of Nemotron-4 15B against existing large language models on XCOPA under the zero- and four-shot setting. Our reported results for XGLM are from the runs of the model in <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib39" title="">2022</a>)</cite> given that we use the same prompt template used by mGPT. The values marked with <math alttext="*" class="ltx_Math" display="inline" id="S3.T7.7.1.m1.1"><semantics id="S3.T7.7.1.m1.1b"><mo id="S3.T7.7.1.m1.1.1" xref="S3.T7.7.1.m1.1.1.cmml">*</mo><annotation-xml encoding="MathML-Content" id="S3.T7.7.1.m1.1c"><times id="S3.T7.7.1.m1.1.1.cmml" xref="S3.T7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.7.1.m1.1d">*</annotation><annotation encoding="application/x-llamapun" id="S3.T7.7.1.m1.1e">*</annotation></semantics></math> are read from figures in <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib37" title="">2023</a>)</cite>. </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p4.1.1">2. Generation:</span> We consider two generationative tasks: TyDiQA-GoldP <cite class="ltx_cite ltx_citemacro_citep">(Clark et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib13" title="">2020</a>)</cite> and Multilingual Grade School Math (MGSM) <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib38" title="">2022</a>)</cite>. TyDiQA-GoldP는 질의 응답 작업이며 MGSM은 10개 언어로 언어 모델의 산술 추론 능력을 평가한다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1">TyDiQA-GoldP 상의 네모트론-4 15B의 성능을 다양한 모델과 비교할 때, 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T8" title="Table 8 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">8</span></a>는 네모트론-4 15B가 최상의 성능을 달성함을 보여준다. 인상적으로, 네모트론-4 15B는 차선책인 PaLM 62B-cont에서 크게 개선할 수 있다.</p>
</div>
<figure class="ltx_table" id="S3.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T8.2" style="width:258.0pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T8.2.1"><span class="ltx_text" id="S3.T8.2.1.1"></span></p>
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T8.2.1.1.1" style="width:258.0pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T8.2.1.1.1.1"><span class="ltx_text" id="S3.T8.2.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T8.2.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.1">Model</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.2">Size</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.3">AR</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.4">BN</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.5">FI</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.6">ID</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.7">KO</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.8">RU</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.9">SW</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.10">TE</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.11">AVG</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T8.2.1.1.1.1.1.1.2.1.1"><span class="ltx_text" id="S3.T8.2.1.1.1.1.1.1.2.1.1.1">PaLM</span></span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.2">62B</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.3">31.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.4">42.5</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.5">41.7</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.6">41.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.7">49.3</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.8">29.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.9">58.1</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.10">30.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.11">40.5</span></span>
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.3.2">
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.1">62B-cont</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.2">39.4</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.3">48.7</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.4">44.0</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.5">49.2</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.6">52.5</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.7">35.6</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.8">60.9</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.9">35.3</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.10">45.7</span></span>
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.4.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T8.2.1.1.1.1.1.1.4.3.1">LLaMA-2</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.2">13B</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.3">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.4">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.5">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.6">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.7">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.8">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.9">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.10">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.11">33.2</span></span>
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.5.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T8.2.1.1.1.1.1.1.5.4.1">Baichuan-2</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.2">13B</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.3">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.4">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.5">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.6">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.7">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.8">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.9">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.10">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.11">30.8</span></span>
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.6.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T8.2.1.1.1.1.1.1.6.5.1">QWEN</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.2">14B</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.3">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.4">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.5">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.6">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.7">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.8">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.9">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.10">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.11">39.8</span></span>
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.7.6">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.1">Nemotron-4</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.2">15B</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.3">39.1</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.4">55.8</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.5">52.2</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.6">54.5</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.7">55.1</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.8">37.8</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.9">54.5</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.10">55.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.11"><span class="ltx_text ltx_font_bold" id="S3.T8.2.1.1.1.1.1.1.7.6.11.1">50.5</span></span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T8.3.1.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S3.T8.4.2" style="font-size:90%;"> Comparative results in the one-shot setting on TyDiQA-GoldP. Results for LLaMA-2 13B, Baichuan-2 13B and QWEN 14B are taken from <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib10" title="">2024</a>)</cite>. </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.p6">
<p class="ltx_p" id="S3.SS4.p6.1">네모트론-4 15B의 인상적인 다국어 능력을 추가로 입증하기 위해, 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T9" title="Table 9 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">9</span></a>는 MGSM에 대한 성능을 보여준다. <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib38" title="">2022</a>)</cite>에 소개된 영어 연쇄 사상 설정을 사용하여 보고하며, 여기서 모든 연쇄 사상 설명은 과제의 언어가 아닌 영어로 모델에 제시된다. 수학적 능력과 다국어 능력의 교차점을 평가하는 이 도전적인 과제에서 네모트론-4 15B는 비교된 모델 중에서 최고의 성능을 달성하고 가장 가까운 점수를 거의 30% 향상시킨다.</p>
</div>
<figure class="ltx_table" id="S3.T9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T9.2" style="width:351.5pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T9.2.1"><span class="ltx_text" id="S3.T9.2.1.1"></span></p>
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T9.2.1.1.1" style="width:351.5pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T9.2.1.1.1.1"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T9.2.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T9.2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.1">Mode</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.2">Model</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.3">Size</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.4">DE</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.5">FR</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.6">ES</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.7">RU</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.8">ZH</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.9">JA</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.10">TH</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.11">TE</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.12">BN</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.13">SW</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.14">AVG</span></span>
<span class="ltx_tr" id="S3.T9.2.1.1.1.1.1.1.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.1"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.2.2.1.1">Native-COT</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.2"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.2.2.2.1">PaLM</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.3">62B</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.4">24.0</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.5">24.0</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.6">26.0</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.7">22.8</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.8">24.8</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.9">14.8</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.10">18.0</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.11">11.6</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.12">13.6</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.13">9.6</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.14">18.9</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T9.2.1.1.1.1.1.1.3.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3" id="S3.T9.2.1.1.1.1.1.1.3.1.1"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.3.1.1.1">English-COT</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.2"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.3.1.2.1">PALM</span></span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.3">62B-cont</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.4">44.8</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.5">39.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.6">44.4</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.7">36.8</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.8">33.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.9">24.0</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.10">28.0</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.11">19.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.12">28.0</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.13">21.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.14">32.0</span></span>
<span class="ltx_tr" id="S3.T9.2.1.1.1.1.1.1.4.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T9.2.1.1.1.1.1.1.4.2.1"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.4.2.1.1">Mistral</span></span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.2">7B</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.3">33.2</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.4">35.2</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.5">35.6</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.6">35.2</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.7">33.2</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.8">18.8</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.9">10.0</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.10">0.0</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.11">8.0</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.12">9.2</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.13">21.8</span></span>
<span class="ltx_tr" id="S3.T9.2.1.1.1.1.1.1.5.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.1"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.5.3.1.1">Nemotron-4</span></span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.2">15B</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.3">46.8</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.4">46.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.5">50.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.6">45.6</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.7">40.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.8">40.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.9">43.6</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.10">41.6</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.11">43.6</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.12">16.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.13"><span class="ltx_text ltx_font_bold" id="S3.T9.2.1.1.1.1.1.1.5.3.13.1">41.3</span></span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T9.3.1.1" style="font-size:90%;">Table 9</span>: </span><span class="ltx_text" id="S3.T9.4.2" style="font-size:90%;"> Eight-shot accuracy results on MGSM. Results for Mistral are from our runs of Mistral in the same setting as Nemotron-4. </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.p7">
<p class="ltx_p" id="S3.SS4.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p7.1.1">3. Machine Translation:</span> FLORES-101 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib19" title="">2021</a>)</cite> 벤치마크를 통해 모델의 번역 능력을 추가로 평가합니다. 언어 간 번역 능력은 언어 간의 의미적 관계를 관계화하고 이해하는 모델의 능력에 대한 좋은 테스트이다.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p8">
<p class="ltx_p" id="S3.SS4.p8.1">표 <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T10" title="Table 10 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">10</span></a>에서 볼 수 있듯이, 네모트론-4 15B는 LLaMA-2 13B와 바이촨-2 13B 모두에 비해 성능이 각각 90.2%와 44.1% 향상되었습니다. 네모트론-4 15B는 중국어에서 영어로 번역하는 데만 좋은 성과를 내는 것이 아니라 중국어를 다른 언어로 직접 번역하는 데에도 인상적인 결과를 얻을 수 있다. 이 능력은 네모트론-4 15B가 광범위한 자연 언어에 걸쳐 가지고 있는 강력한 이해를 강조한다.</p>
</div>
<figure class="ltx_table" id="S3.T10">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T10.2" style="width:309.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T10.2.1"><span class="ltx_text" id="S3.T10.2.1.1"></span></p>
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T10.2.1.1.1" style="width:309.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T10.2.1.1.1.1"><span class="ltx_text" id="S3.T10.2.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T10.2.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T10.2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.1"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.2">Size</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.3">ZH-EN</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.4">ZH-FR</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.5">ZH-ES</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.6">ZH-AR</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.7">ZH-RU</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.8">ZH-JA</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.9">ZH-DE</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.10">AVG</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T10.2.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.1"><span class="ltx_text" id="S3.T10.2.1.1.1.1.1.1.2.1.1.1">LLaMA-2</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.2">13B</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.3">25.4</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.4">19.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.5">17.5</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.6">1.4</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.7">10.3</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.8">0.1</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.9">11.1</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.10">12.2</span></span>
<span class="ltx_tr" id="S3.T10.2.1.1.1.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.1"><span class="ltx_text" id="S3.T10.2.1.1.1.1.1.1.3.2.1.1">Baichuan-2</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.2">13B</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.3">30.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.4">22.1</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.5">17.3</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.6">2.4</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.7">14.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.8">11.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.9">14.5</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.10">16.1</span></span>
<span class="ltx_tr" id="S3.T10.2.1.1.1.1.1.1.4.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.1">Nemotron-4</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.2">15B</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.3">34.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.4">28.1</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.5">21.3</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.6">16.8</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.7">21.2</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.8">23.1</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.9">18.1</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.10"><span class="ltx_text ltx_font_bold" id="S3.T10.2.1.1.1.1.1.1.4.3.10.1">23.2</span></span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T10.3.1.1" style="font-size:90%;">Table 10</span>: </span><span class="ltx_text" id="S3.T10.4.2" style="font-size:90%;"> Eight-shot results on Flores sub-tasks translating out of Chinese. All results for external models were obtained from <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib49" title="">2023</a>)</cite></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section">4&nbsp;&nbsp;&nbsp;Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">우리는 디코더 전용 트랜스포머 기반 대용량 언어 모델인 네모트론-4 15B를 제시한다. 영어에 걸쳐 있는 8조 개의 토큰, 53개의 추가 자연 언어 및 43개의 프로그래밍 언어로 교육됩니다. 네모트론-4 15B는 규모에서 모든 범용 언어 모델의 가장 강력한 다국어 성능을 나타내며, 심지어 다국어 도메인에 특화된 모델도 능가한다. 네모트론-4는 모델의 능력을 향상시키기 위해 대형 언어 모델에 대한 사전 훈련 세트가 계속해서 더 확장될 수 있음을 보여준다.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
NVLink and NVSwitch.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nvidia.com/en-us/data-center/nvlink/" title="">https://www.nvidia.com/en-us/data-center/nvlink/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ainslie et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joshua Ainslie, James Lee-Thorp, Michiel de&nbsp;Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.

</span>
<span class="ltx_bibblock">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2305.13245</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allal et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Loubna&nbsp;Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos&nbsp;Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh&nbsp;Kumar Umapathi, Carolyn&nbsp;Jane Anderson, Yangtian Zi, Joel&nbsp;Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco&nbsp;De Toni, Bernardo&nbsp;García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry&nbsp;Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de&nbsp;Vries, and Leandro von Werra.

</span>
<span class="ltx_bibblock">SantaCoder: Don’t Reach for the Stars!, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.

</span>
<span class="ltx_bibblock">Program Synthesis with Large Language Models, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Qwen Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan&nbsp;Le Bras, Jianfeng Gao, and Yejin Choi.

</span>
<span class="ltx_bibblock">PIQA: Reasoning about Physical Commonsense in Natural Language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">AAAI</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassano et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn&nbsp;Jane Anderson, Molly&nbsp;Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.

</span>
<span class="ltx_bibblock">MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE Transactions on Software Engineering</em>, pages 1–17, 2023a.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1109/TSE.2023.3267446" title="">10.1109/TSE.2023.3267446</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassano et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn&nbsp;Jane Anderson, Molly&nbsp;Q Feldman, et&nbsp;al.

</span>
<span class="ltx_bibblock">Multipl-e: a scalable and polyglot approach to benchmarking neural code generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">IEEE Transactions on Software Engineering</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Du&nbsp;Chen, Yi&nbsp;Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, and Kun Han.

</span>
<span class="ltx_bibblock">Orion-14B: Open-source Multilingual Large Language Models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique&nbsp;Ponde de&nbsp;Oliveira&nbsp;Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe&nbsp;Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William&nbsp;Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew&nbsp;N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.

</span>
<span class="ltx_bibblock">Evaluating Large Language Models Trained on Code, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, et&nbsp;al.

</span>
<span class="ltx_bibblock">PaLM: Scaling Language Modeling with Pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonathan&nbsp;H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki.

</span>
<span class="ltx_bibblock">TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">CoRR</em>, abs/2003.05002, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2003.05002" title="">https://arxiv.org/abs/2003.05002</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think You have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.

</span>
<span class="ltx_bibblock">Training Verifiers to Solve Math Word Problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">CoRR</em>, abs/2110.14168, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2110.14168" title="">https://arxiv.org/abs/2110.14168</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.

</span>
<span class="ltx_bibblock">A Framework for Few-shot Language Model Evaluation, September 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5281/zenodo.5371628" title="">https://doi.org/10.5281/zenodo.5371628</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemma&nbsp;Team (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Google&nbsp;DeepMind Gemma&nbsp;Team.

</span>
<span class="ltx_bibblock">Gemma: Open Models Based on Gemini Research and Technology, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Gemini: A Family of Highly Capable Multimodal Models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da&nbsp;Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan.

</span>
<span class="ltx_bibblock">The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">CoRR</em>, abs/2106.03193, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2106.03193" title="">https://arxiv.org/abs/2106.03193</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring Massive Multitask Language Understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2009.03300</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training Compute-Optimal Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jennings et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Shrimai Prabhumoye, Ayush Dattagupta, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Curating Trillion-Token Datasets: Introducing NVIDIA NeMo Data Curator.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/blog/curating-trillion-token-datasets-introducing-nemo-data-curator/" title="">https://developer.nvidia.com/blog/curating-trillion-token-datasets-introducing-nemo-data-curator/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling Laws for Neural Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korthikanti et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Reducing Activation Recomputation in Large Transformer Models, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:1808.06226</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Raymond Li, Loubna&nbsp;Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry&nbsp;Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh&nbsp;Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva&nbsp;Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn&nbsp;Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos&nbsp;Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von
Werra, and Harm de&nbsp;Vries.

</span>
<span class="ltx_bibblock">StarCoder: May the Source be with You!, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit&nbsp;Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li.

</span>
<span class="ltx_bibblock">Few-shot Learning with Multilingual Language Models, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
NVIDIA.

</span>
<span class="ltx_bibblock">H100 Tensor Core GPU Architecture Overview, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et&nbsp;al. (2002)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">BLEU: A Method for Automatic Evaluation of Machine Translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</em>, ACL ’02, page 311–318, USA, 2002. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.3115/1073083.1073135" title="">10.3115/1073083.1073135</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3115/1073083.1073135" title="">https://doi.org/10.3115/1073083.1073135</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ponti et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edoardo&nbsp;Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen.

</span>
<span class="ltx_bibblock">XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">CoRR</em>, abs/2005.00333, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2005.00333" title="">https://arxiv.org/abs/2005.00333</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Matt Post.

</span>
<span class="ltx_bibblock">A Call for Clarity in Reporting BLEU Scores.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">CoRR</em>, abs/1804.08771, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1804.08771" title="">http://arxiv.org/abs/1804.08771</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jack&nbsp;W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van&nbsp;den Driessche, Lisa&nbsp;Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang&nbsp;Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de&nbsp;Masson&nbsp;d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de&nbsp;Las&nbsp;Casas, Aurelia Guy, Chris Jones,
James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed&nbsp;Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">The Journal of Machine Learning Research</em>, 21(1):5485–5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">AAAI</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.

</span>
<span class="ltx_bibblock">Socialiqa: Commonsense reasoning about social interactions, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander&nbsp;M. Rush, Stella Biderman, Albert Webson, Pawan&nbsp;Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert&nbsp;Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz&nbsp;Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro&nbsp;Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham&nbsp;Fikri Aji, Amit Alfassy, Anna Rogers, Ariel&nbsp;Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David&nbsp;Ifeoluwa Adelani, Dragomir Radev, Eduardo&nbsp;González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal&nbsp;Bar Natan, Francesco&nbsp;De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin,
Isaac Johnson, Itziar Gonzalez-Dios, Javier de&nbsp;la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro&nbsp;Von Werra, Leon Weber, Long Phan, Loubna&nbsp;Ben allal, Ludovic Tanguy, Manan Dey, Manuel&nbsp;Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh&nbsp;Chien Vu, Mohammad&nbsp;A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de&nbsp;Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto&nbsp;Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen&nbsp;Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago&nbsp;Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut&nbsp;Emre Taşar, Elizabeth Salesky, Sabrina&nbsp;J. Mielke, Wilson&nbsp;Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason&nbsp;Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M&nbsp;Saiful Bari, Maged&nbsp;S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen&nbsp;H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung&nbsp;Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette,
Pierre&nbsp;François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta&nbsp;Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica&nbsp;Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van&nbsp;der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol,
Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos&nbsp;Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong&nbsp;A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio&nbsp;Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav&nbsp;Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo&nbsp;Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu,
Clémentine Fourrier, Daniel&nbsp;León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena&nbsp;U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose&nbsp;David Posada, Karthik&nbsp;Rangasai Sivaraman, Lokesh Bulchandani, Lu&nbsp;Liu, Luisa Shinzato, Madeleine&nbsp;Hahn de&nbsp;Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria&nbsp;A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel&nbsp;De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas&nbsp;Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok&nbsp;S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,
Yash&nbsp;Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu&nbsp;Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf.

</span>
<span class="ltx_bibblock">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung&nbsp;Won Chung, Yi&nbsp;Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.

</span>
<span class="ltx_bibblock">Language Models are Multilingual Chain-of-Thought Reasoners, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shliazhko et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina.

</span>
<span class="ltx_bibblock">mGPT: Few-Shot Learners Go Multilingual, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Megatron-LM: Training Multi-Billion Parameter Language Models using Model Parallelism.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Slav&nbsp;Petrov and et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andrew M.&nbsp;Dai Slav&nbsp;Petrov, Yonghui&nbsp;Wu and et&nbsp;al.

</span>
<span class="ltx_bibblock">PaLM 2 Technical Report, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.google/static/documents/palm2techreport.pdf" title="">https://ai.google/static/documents/palm2techreport.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zheng, Rewon Child, Reza&nbsp;Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">CoRR</em>, abs/2201.11990, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.11990" title="">https://arxiv.org/abs/2201.11990</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Ahmed Murtadha, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced Transformer with Rotary Position Embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2104.09864</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzgun et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V. Le, Ed&nbsp;H. Chi, Denny Zhou, and Jason Wei.

</span>
<span class="ltx_bibblock">Challenging big-bench tasks and whether chain-of-thought can solve them, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language Models, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-tuned Chat Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">CoRR</em>, abs/1706.03762, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1706.03762" title="">http://arxiv.org/abs/1706.03762</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:1911.00359</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da&nbsp;Pan, Dian Wang, Dong Yan, Fan Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Baichuan 2: Open Large-scale Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2309.10305</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">HellaSwag: Can a Machine Really Finish Your Sentence?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">ACL</em>, 2019.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">Supplementary Materials</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="Ax1.T11">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ax1.T11.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ax1.T11.2.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="Ax1.T11.2.1.1.1"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" id="Ax1.T11.2.1.1.2">Size</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ax1.T11.2.1.1.3">Humanities</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ax1.T11.2.1.1.4">Social sciences</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ax1.T11.2.1.1.5">STEM</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ax1.T11.2.1.1.6">Other</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ax1.T11.2.1.1.7">Average</td>
</tr>
<tr class="ltx_tr" id="Ax1.T11.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.1">Nemotron-4</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.2">15B</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.3">69.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.4">74.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.5">53.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.6">67.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.7">64.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ax1.T11.3.1.1" style="font-size:90%;">Table 11</span>: </span><span class="ltx_text" id="Ax1.T11.4.2" style="font-size:90%;"> Per-category breakdown accuracy for MMLU </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
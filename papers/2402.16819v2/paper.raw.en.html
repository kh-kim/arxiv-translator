<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Nemotron-4 15B Technical Report</title>
<!--Generated on Tue Feb 27 15:22:54 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.16819v2/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2402.16819v2">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2402.16819v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2402.16819v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S1" title="1 Introduction ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2" title="2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Architecture Details</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.SS0.SSS0.Px1" title="Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title">Data.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.SS0.SSS0.Px2" title="Pre-training. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title">Pre-training.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.SS0.SSS0.Px3" title="Continued Training. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title">Continued Training.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3" title="3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.SS1" title="3.1 Commonsense Reasoning ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Commonsense Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.SS2" title="3.2 Popular Aggregated Benchmarks ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Popular Aggregated Benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.SS3" title="3.3 Math and Code ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Math and Code</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.SS4" title="3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Multilingual</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S4" title="4 Conclusion ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a></li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2402.16819v2 [cs.CL] 27 Feb 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Nemotron-4 15B Technical Report</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jupinder Parmar<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
Equal contribution, corresponding authors: <span class="ltx_text ltx_font_typewriter" id="footnote1.1">{jupinderp,sprabhumoye,jjennings,mpatwary}@nvidia.com</span>.
</span></span></span> &nbsp;&nbsp;Shrimai Prabhumoye<math alttext="{}^{*}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><times id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math> &nbsp;&nbsp;Joseph Jennings<math alttext="{}^{*}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><times id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math> &nbsp;&nbsp;Mostofa Patwary<math alttext="{}^{*}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mo id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><times id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break">Sandeep Subramanian<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Work done while at NVIDIA.</span></span></span> &nbsp;&nbsp;Dan Su &nbsp;&nbsp;Chen Zhu &nbsp;&nbsp;Deepak Narayanan &nbsp;&nbsp; Aastha Jhunjhunwala &nbsp;&nbsp; Ayush Dattagupta &nbsp;&nbsp;Vibhu Jawa &nbsp;&nbsp;Jiwei Liu &nbsp;&nbsp;Ameya Mahabaleshwarkar &nbsp;&nbsp;Osvald Nitski &nbsp;&nbsp; Annika Brundyn &nbsp;&nbsp;James Maki &nbsp;&nbsp;Miguel Martinez &nbsp;&nbsp;Jiaxuan You &nbsp;&nbsp;John Kamalu &nbsp;&nbsp;Patrick LeGresley &nbsp;&nbsp;Denys Fridman &nbsp;&nbsp;Jared Casper &nbsp;&nbsp; Ashwath Aithal &nbsp;&nbsp;Oleksii Kuchaiev &nbsp;&nbsp;Mohammad Shoeybi &nbsp;&nbsp;Jonathan Cohen &nbsp;&nbsp;Bryan Catanzaro <span class="ltx_text ltx_font_bold" id="id4.4.id1">NVIDIA</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id5.id1">We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section">1&nbsp;&nbsp;&nbsp;Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recently published efforts <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib21" title="">2022</a>; Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib45" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib46" title="">b</a>; Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib49" title="">2023</a>; Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib23" title="">2023</a>)</cite> in language model pre-training have been inspired by Chinchilla scaling laws <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib21" title="">2022</a>)</cite>, which argue for scaling data along with model size given a fixed compute budget, compared to past work that only scaled the size of the model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kaplan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib24" title="">2020</a>; Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib7" title="">2020</a>; Smith et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib42" title="">2022</a>; Rae et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib33" title="">2022</a>; Scao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib37" title="">2023</a>)</cite>.
For example, <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib21" title="">2022</a>)</cite> shows that given two roughly IsoFLOP GPT models with a similar data distribution, a 65-billion-parameter model on 1.4 trillion tokens and a 280-billion-parameter model on 300 billion tokens, the 65B model has better accuracy on downstream tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">This trade-off of allocating compute towards training on more data as opposed to increasing model size is particularly appealing from an inference perspective, reducing latency and the amount of compute needed to serve models. As a consequence, a major focus of language modeling training efforts has shifted to collecting high-quality multi-trillion token datasets from public sources such as Common Crawl. We continue this trend by introducing Nemotron-4 15B which was trained on 8 trillion tokens of English, multilingual, and coding text and was developed to be the best general-purpose large language model (LLM) that can fit on a single NVIDIA A100 or H100 GPU.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">As demonstrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">1</span></a>, Nemotron-4 15B exhibits high downstream accuracies across a wide range of English, code, and multilingual evaluation areas. In comparison to leading similarly-sized, open models we show that Nemotron-4 15B is significantly better than LLaMA-2 34B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib46" title="">2023b</a>)</cite>, which has over twice the number of parameters, and is better than Mistral 7B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib23" title="">2023</a>)</cite> on all English evaluation areas. Additionally, Nemotron-4 15B achieves competitive accuracies to QWEN 14B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib5" title="">2023</a>)</cite> and Gemma 7B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gemma&nbsp;Team, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib17" title="">2024</a>)</cite>. In a comparison across a wide range of programming languages, we find that Nemotron-4 15B achieves better average accuracy, and in particular on low-resource programming languages, than Starcoder&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib27" title="">2023</a>)</cite>, a code-specific model, and Mistral 7B. As Nemotron-4 15B was trained on significant amount of multilingual data, it is currently the state-of-the-art general purpose model in its size class on all multilingual benchmarks. We find that Nemotron-4 is better than PALM 62B-Cont&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Slav&nbsp;Petrov and et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib41" title="">2023</a>)</cite>, and also outperforms multilingual-specific models such as XGLM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib28" title="">2022</a>)</cite> and mGPT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib39" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S1.F1.1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="512" id="S1.F1.1.g1" src="x1.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S1.F1.2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="517" id="S1.F1.2.g1" src="x2.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.4.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.5.2" style="font-size:90%;">Comparison of Nemotron-4 15B across seven evaluation areas against similarly sized models. The composition of tasks that form each evaluation area can be found, along with more detailed evaluation results, in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3" title="3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">3</span></a></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S1.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.2.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.1">Number of</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.2">Hidden</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.3">Number of</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.4">Number of</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.5">Sequence</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1.6">Vocabulary</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.2.2">
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.1">transformer layers</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.2">dimension</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.3">attention heads</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.4">KV heads</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.5">length</td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2.6">size</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.3.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.1">32</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.2">6144</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.3">48</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.4">8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.5">4096</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S1.T1.2.3.3.6">256,000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S1.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S1.T1.4.2" style="font-size:90%;">Key hyper-parameters affecting size of Nemotron-4 15B.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section">2&nbsp;&nbsp;&nbsp;Architecture Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Nemotron-4 uses a standard decoder-only Transformer architecture&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib47" title="">2017</a>)</cite>, with causal attention masks. Exact hyper-parameters affecting size are shown in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">1</span></a>.
Nemotron-4 has 3.2 billion embedding parameters and 12.5 billion non-embedding parameters.
We use Rotary Position Embeddings (RoPE)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Su et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib43" title="">2021</a>)</cite>, SentencePiece tokenizer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib26" title="">2018</a>)</cite>, squared ReLU activations in the MLP layers, no bias terms, dropout rate of zero, and untied input-output embeddings.
We use grouped query attention (GQA)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ainslie et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib2" title="">2023</a>)</cite> for faster inference and lower memory footprint.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Data.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">We train Nemotron-4 15B on a pre-training dataset consisting of 8 trillion tokens. At a high-level, the data blend is split into three different types of data: English natural language data (70%), multilingual natural language data (15%), and source-code data (15%).
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="280" id="S2.F2.g1" src="extracted/5435317/plots/new_full_distr.png" width="509">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Data composition of the English tokens used for pre-training</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p2.1">The English corpus consists of curated documents from a variety of sources and domains including web documents, news articles, scientific papers, books, etc and the distribution used in our pre-training set is highlighted in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.F2" title="Figure 2 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">2</span></a>. The code and multilingual data consists of a diverse set of natural and programming languages. We find that appropriately sampling tokens from these languages is key to strong accuracies in these domains. We share the distributions used for both code and multilingual tokens in our pre-training dataset in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.F3" title="Figure 3 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">3</span></a> and Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.F4" title="Figure 4 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">4</span></a> respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="347" id="S2.F3.g1" src="extracted/5435317/plots/test_code.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Data distribution of the 43 programming languages used for pre-training. The number within each bar indicates the percent of the overall code distribution that an individual language comprises.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p3.1">In constructing the pre-training corpus, we remove any possible duplicates via document-level exact and near-deduplication&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jennings et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib22" title="">2023</a>)</cite>. We additionally applied document-level quality filtering across our corpus using a language-model based filtering approach similar to <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib48" title="">2019</a>)</cite> in addition to a series of heuristic filters as described in <cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib33" title="">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib34" title="">2020</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p4">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p4.1">We train a BPE tokenizer in SentencePiece&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib26" title="">2018</a>)</cite> on data that is randomly sampled from the final 8T token dataset. To have better coverage of low-resource languages in the tokenizer, we upsample non-English data relative to the final training dataset distribution. Our tokenizer preserves whitespaces (including leading and trailing ones), splits numbers into their individual digits&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib12" title="">2022</a>)</cite>, and relies on byte-level backoff to handle unknown character sequences. The final vocabulary size is 256,000 tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="274" id="S2.F4.g1" src="extracted/5435317/plots/test_multi.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">Data distribution of the 53 natural languages, aside from English,we used for pre-training. The number within each bar indicates the percent of the overall multilingual distribution that an individual language comprises.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Pre-training.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Nemotron-4 was trained using 384 DGX H100 nodes; each node contains 8 H100 80GB SXM5 GPUs based on the NVIDIA Hopper architecture&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(NVIDIA, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib29" title="">2022</a>)</cite>.
Each H100 GPU has a peak throughput of 989 teraFLOP/s when doing 16-bit floating point (<span class="ltx_text ltx_font_typewriter" id="S2.SS0.SSS0.Px2.p1.1.1">bfloat16</span>) arithmetic without sparsity. Within each node, GPUs are connected by NVLink and NVSwitch&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib1" title="">nvl, </a>)</cite>; the GPU-to-GPU bandwidth is 900 GB/s (450 GB/s in each direction). Each node has 8 NVIDIA Mellanox 400 Gbps HDR InfiniBand Host Channel Adapters (HCAs) for inter-node communication.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">We used a combination of 8-way tensor parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shoeybi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib40" title="">2019</a>)</cite> and data parallelism to train the model; we also use a distributed optimizer to shard the optimizer state over the data-parallel replicas. The degree of data parallelism was varied from 96 to 384 as the batch size was ramped up.
Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S2.T2" title="Table 2 ‣ Pre-training. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the 3 stages of batch size ramp, and includes the per-iteration time and model FLOP/s utilization (MFU)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib12" title="">2022</a>; Korthikanti et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib25" title="">2022</a>)</cite>. MFU quantifies how efficiently the GPUs are utilized in model training. Training was completed in approximately 13 calendar days.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T2.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.1">Data-parallel size</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.2">GPUs</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.3">Iteration time (secs)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.4">MFU (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.5">Batch size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.6">Tokens (B)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T2.2.1.1.7">Time (days)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.1">96</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T2.2.2.1.2">768</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.3">0.57</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.4">34.3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.5">384</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.6">200</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T2.2.2.1.7">0.8</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.3.2">
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.1">192</td>
<td class="ltx_td ltx_align_right" id="S2.T2.2.3.2.2">1,536</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.3">0.58</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.4">33.3</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.5">768</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.6">200</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.3.2.7">0.4</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.1">288</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S2.T2.2.4.3.2">2,304</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.3">0.64</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.4">30.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.5">1,152</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.6">7,600</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.4.3.7">11.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S2.T2.4.2" style="font-size:90%;">Batch size rampup schedule, along with time and efficiency metrics for the Nemotron-4 15B parameter model.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Continued Training.</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Similar to recent work&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib18" title="">2023</a>)</cite>, we find that switching the data distribution and learning rate decay schedule at the end of model training greatly improves model quality. Concretely, after having trained over the entirety of our 8T pre-training dataset, we use the same loss objective and perform continued training on small number of tokens in comparison to the pre-training tokens.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p2.1">In this additional phase of continued training, we utilize two distinct data distributions.
The first distribution is where the majority of tokens during continued training are sampled from. It utilizes tokens that have already been introduced during pre-training but with a distribution that places larger sampling weight on higher quality sources.
The second distribution introduces a small number of benchmark-style alignment examples to better allow the model to respond to such questions in downstream evaluations while also up-weighting data sources that come from areas of low model performance.
In accompaniment with a learning rate schedule that prioritizes a steeper slope of decay than magnitude of learning rate, we find that such an ordering and style of data distributions allows for the model to gently transition from the pre-training dataset and better learn newly emphasized data areas.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section">3&nbsp;&nbsp;&nbsp;Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We evaluate Nemotron-4 15B on a variety of downstream evaluation areas covering a diverse range of tasks and domains. In all evaluations, we adhere to the standardized task setup and share the exact settings used. The covered evaluation categories include:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Commonsense Reasoning (0-shot):</span> SIQA <cite class="ltx_cite ltx_citemacro_citep">(Sap et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib36" title="">2019</a>)</cite>, ARC easy and challenge <cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib14" title="">2018</a>)</cite>, PIQA <cite class="ltx_cite ltx_citemacro_citep">(Bisk et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib6" title="">2020</a>)</cite>, Winogrande <cite class="ltx_cite ltx_citemacro_citep">(Sakaguchi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib35" title="">2020</a>)</cite>, and Hellaswag <cite class="ltx_cite ltx_citemacro_citep">(Zellers et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib50" title="">2019</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Popular Aggregated Benchmarks:</span> MMLU (5-shot) <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib20" title="">2020</a>)</cite> and BBH (3-shot) <cite class="ltx_cite ltx_citemacro_citep">(Suzgun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib44" title="">2022</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Math:</span> GSM8K (8-shot with maj@1) <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib15" title="">2021</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Code:</span> Pass@1 scores on HumanEval (0-shot) <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib11" title="">2021</a>)</cite>, MBPP (3-shot) <cite class="ltx_cite ltx_citemacro_citep">(Austin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib4" title="">2021</a>)</cite>, and MultiPL-E (0-shot) <cite class="ltx_cite ltx_citemacro_citep">(Cassano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib8" title="">2023a</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">Multilingual:</span> classification via XCOPA (0 and 4-shot) <cite class="ltx_cite ltx_citemacro_citep">(Ponti et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib31" title="">2020</a>)</cite>, machine translation with FLORES-101 (8-shot)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Goyal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib19" title="">2021</a>)</cite>, and generation tasks such as MGSM (8-shot)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib38" title="">2022</a>)</cite> and TyDiQA (1-shot)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib13" title="">2020</a>)</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">In our evaluations, we compare against a number of external decoder-only transformer language models and unless otherwise stated we use the numbers published in the reports of the corresponding models. For English and code tasks, we share detailed results for Nemotron-4 15B, LlaMA-2 13B and 34B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib46" title="">2023b</a>)</cite>, Mistral 7B <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib23" title="">2023</a>)</cite>, Baichuan-2 13B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib49" title="">2023</a>)</cite>, QWEN 14B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib5" title="">2023</a>)</cite>, and Gemma 7B <cite class="ltx_cite ltx_citemacro_citep">(Gemma&nbsp;Team, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib17" title="">2024</a>)</cite>. For multilingual benchmarks, we report results against PaLM 62B and 62B-cont <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib12" title="">2022</a>)</cite> as well as models specially trained for multilingual capabilities such as mGPT 13B <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib39" title="">2022</a>)</cite> and XGLM 7.5B <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib28" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Commonsense Reasoning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We use the LM-Evaluation Harness&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib16" title="">2021</a>)</cite> to evaluate Nemotron-4 15B across all aforementioned tasks. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T3" title="Table 3 ‣ 3.1 Commonsense Reasoning ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">3</span></a> showcases that Nemotron-4 15B achieves the strongest average performance on this diverse set of tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.1" style="width:305.9pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T3.1.1"><span class="ltx_text" id="S3.T3.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T3.1.1.1.1" style="width:305.9pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T3.1.1.1.1.1"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.1.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.1"></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.2">Size</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.3">SIQA</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.4">ARC-c</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.5">ARC-e</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.6">PIQA</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.7">Winogrande</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.8">Hellaswag</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1.1.1.2.1.9">AVG</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.3.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T3.1.1.1.1.1.1.1.3.1.1"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1.1.3.1.1.1">LLaMA-2</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.2">13B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.3">50.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.4">49.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.5">77.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.6">79.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.7">72.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.8">80.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.3.1.9">68.4</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.4.2">
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T3.1.1.1.1.1.1.1.4.2.1">34B</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.2">50.9</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.3">54.5</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.4">79.4</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.5">81.9</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.6">76.7</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.7">83.3</span>
<span class="ltx_td ltx_align_center" id="S3.T3.1.1.1.1.1.1.1.4.2.8">71.1</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.5.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.1"><span class="ltx_text" id="S3.T3.1.1.1.1.1.1.1.5.3.1.1">Baichuan-2</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.2">13B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.3">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.4">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.5">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.6">78.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.7">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.8">70.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.5.3.9">-</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.6.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.1">QWEN</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.2">14B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.3">77.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.4">84.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.5">90.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.6">79.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.7">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.8">80.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.6.4.9">-</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.2">Mistral</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.3">7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.1">&nbsp;&nbsp;47.0<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1a"><msup id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1a" xref="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.1" mathcolor="#000000" xref="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1"><times id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.4">55.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.5">80.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.6">83.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.7">75.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.8">81.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.1.9">70.4</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.7.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.1">Gemma</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.2">7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.3">51.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.4">53.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.5">81.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.6">81.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.7">72.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.8">81.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.7.5.9">70.2</span></span>
<span class="ltx_tr" id="S3.T3.1.1.1.1.1.1.1.8.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.1">Nemotron-4</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.2">15B</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.3">60.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.4">55.5</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.5">80.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.6">82.4</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.7">78.0</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.8">82.4</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.1.1.1.1.1.1.8.6.9"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1.1.1.8.6.9.1">73.4</span></span></span>
</span>
</span>
</span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.5.2.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.3.1" style="font-size:90%;"> Results on standard reasoning benchmarks in the zero-shot setting. We report the average across all tasks where possible for a fair comparison. The values marked with <math alttext="*" class="ltx_Math" display="inline" id="S3.T3.3.1.m1.1"><semantics id="S3.T3.3.1.m1.1b"><mo id="S3.T3.3.1.m1.1.1" xref="S3.T3.3.1.m1.1.1.cmml">*</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.1.m1.1c"><times id="S3.T3.3.1.m1.1.1.cmml" xref="S3.T3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.1.m1.1d">*</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.1.m1.1e">*</annotation></semantics></math> are read from &nbsp;<cite class="ltx_cite ltx_citemacro_citet">Gemma&nbsp;Team (<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib17" title="">2024</a>)</cite> </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Popular Aggregated Benchmarks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib20" title="">2020</a>)</cite> and Big Bench Hard (BBH)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Suzgun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib44" title="">2022</a>)</cite> benchmarks have been developed as a challenging assessment of language models’ capabilities on a wide range of tasks and domains. As seen from Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T4" title="Table 4 ‣ 3.2 Popular Aggregated Benchmarks ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">4</span></a>, Nemotron-4 15B achieves the best score on BBH across existing models at its scale by nearly 7%.
Additionally, Nemotron-4 is significantly better than LLaMA-2 70B model on BBH benchmark where LLaMA-2 70B attains a score of 51.2 and Nemotron-4 is 58.7.
Nemotron-4 15B additionally attains a highly competitive MMLU score and its per-category performance on MMLU can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#Ax1.T11" title="Table 11 ‣ Supplementary Materials ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">11</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.2.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T4.2.1.1.1"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T4.2.1.1.2">Size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.1.1.3">BBH</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.1.1.4">MMLU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.1.1" rowspan="2"><span class="ltx_text" id="S3.T4.2.2.1.1.1">LLaMA-2</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.2.1.2">13B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.1.3">39.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.1.4">54.8</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.3.2">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T4.2.3.2.1">34B</th>
<td class="ltx_td ltx_align_center" id="S3.T4.2.3.2.2">44.1</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.3.2.3">62.6</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.4.3.1"><span class="ltx_text" id="S3.T4.2.4.3.1.1">Baichuan-2</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.4.3.2">13B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.4.3.3">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.4.3.4">59.2</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.5.4.1">QWEN</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.5.4.2">14B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.5.4.3">53.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.5.4.4"><span class="ltx_text ltx_font_bold" id="S3.T4.2.5.4.4.1">66.3</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.6.5.1">Mistral</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.6.5.2">7B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.6.5.3">39.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.6.5.4">60.1</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.7.6.1">Gemma</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T4.2.7.6.2">7B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.7.6.3">55.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.7.6.4">64.3</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T4.2.8.7.1">Nemotron-4</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T4.2.8.7.2">15B</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.2.8.7.3"><span class="ltx_text ltx_font_bold" id="S3.T4.2.8.7.3.1">58.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T4.2.8.7.4">64.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S3.T4.4.2" style="font-size:90%;"> Nemotron-4 15B attains highly competitive performance on popular aggregate benchmarks. The BBH result for Mistral is read from the figure in&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib23" title="">2023</a>)</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Math and Code</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Recently, large language models have been shown to be effective at both mathematical reasoning and a variety of coding tasks <cite class="ltx_cite ltx_citemacro_citep">(Allal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib3" title="">2023</a>; Chowdhery et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib12" title="">2022</a>; Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib45" title="">2023a</a>)</cite>. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T5" title="Table 5 ‣ 3.3 Math and Code ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">5</span></a> highlights the performance of Nemotron-4 15B on such tasks. Specifically, on mathematical reasoning we find that Nemotron-4 15B achieves strong performance as it attains a similar score to Gemma 7B, but lags behind models such as Baichuan-2 and QWEN. On code tasks, we see that Nemotron-4 performs on par with QWEN 14B while remaining slightly behind Gemma 7B. Across both types of tasks, Nemotron-4 15B is able to outperform Mistral 7B and LlaMA-2 13B/34B.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T5.2" style="width:189.7pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T5.2.2"><span class="ltx_text" id="S3.T5.2.2.2">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T5.2.2.2.2" style="width:189.7pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T5.2.2.2.2.2"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T5.2.2.2.2.2.2.2">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.3.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T5.2.2.2.2.2.2.2.3.1.1"></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T5.2.2.2.2.2.2.2.3.1.2">Size</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.2.2.2.2.2.2.2.3.1.3">GSM8K</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.2.2.2.2.2.2.2.3.1.4">HumanEval</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.2.2.2.2.2.2.2.3.1.5">MBPP</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.4.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T5.2.2.2.2.2.2.2.4.1.1"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.4.1.1.1">LlaMA-2</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.4.1.2">13B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.4.1.3">28.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.4.1.4">18.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.4.1.5">30.6</span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.5.2">
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T5.2.2.2.2.2.2.2.5.2.1">34B</span>
<span class="ltx_td ltx_align_center" id="S3.T5.2.2.2.2.2.2.2.5.2.2">42.2</span>
<span class="ltx_td ltx_align_center" id="S3.T5.2.2.2.2.2.2.2.5.2.3">22.6</span>
<span class="ltx_td ltx_align_center" id="S3.T5.2.2.2.2.2.2.2.5.2.4">33.0</span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.6.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.6.3.1"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.6.3.1.1">Baichuan-2</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.6.3.2">13B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.6.3.3">52.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.6.3.4">17.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.6.3.5">30.2</span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.7.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.7.4.1"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.7.4.1.1">QWEN</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.7.4.2">14B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.7.4.3"><span class="ltx_text ltx_font_bold" id="S3.T5.2.2.2.2.2.2.2.7.4.3.1">60.1</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.7.4.4">32.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.7.4.5">40.8</span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.2.3"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.2.3.1">Mistral</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.2.4">7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.1.1.1.1.1.1.1.1">35.4<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1a"><msup id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1a" xref="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.1" mathcolor="#000000" xref="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1"><times id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T5.1.1.1.1.1.1.1.1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.1.1.1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.2.5">30.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.2.2">&nbsp;&nbsp;40.2<math alttext="{}^{*}" class="ltx_Math" display="inline" id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1"><semantics id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1a"><msup id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1" xref="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.cmml"><mi id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1a" xref="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.cmml"></mi><mo id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.1" mathcolor="#000000" xref="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1b"><apply id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1"><times id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.1.cmml" xref="S3.T5.2.2.2.2.2.2.2.2.2.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T5.2.2.2.2.2.2.2.2.2.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.8.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.8.5.1"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.8.5.1.1">Gemma</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.8.5.2">7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.8.5.3">46.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.8.5.4"><span class="ltx_text ltx_font_bold" id="S3.T5.2.2.2.2.2.2.2.8.5.4.1">32.3</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.8.5.5"><span class="ltx_text ltx_font_bold" id="S3.T5.2.2.2.2.2.2.2.8.5.5.1">44.4</span></span></span>
<span class="ltx_tr" id="S3.T5.2.2.2.2.2.2.2.9.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.9.6.1"><span class="ltx_text" id="S3.T5.2.2.2.2.2.2.2.9.6.1.1">Nemotron-4</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.9.6.2">15B</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.9.6.3">46.0</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.9.6.4">31.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T5.2.2.2.2.2.2.2.9.6.5">40.6</span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T5.4.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S3.T5.5.2" style="font-size:90%;"> Comparative results on math and code benchmarks. As Mistral 7B reports MBPP performance on a different eval split and uses a different evaluation setting for GSM8K , we use the corresponding numbers reported in&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gemma&nbsp;Team, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib17" title="">2024</a>)</cite></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Nearly all similarly-sized open models determine their code abilities solely based on performance on Python related tasks – disregarding an evaluation of their capabilities on other programming languages. In Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T6" title="Table 6 ‣ 3.3 Math and Code ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">6</span></a>, we demonstrate results of Nemotron-4 15B on the Multiple-E&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cassano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib9" title="">2023b</a>)</cite> benchmark across 11 diverse programming languages and compare it against Mistral 7B and Starcoder&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib27" title="">2023</a>)</cite>, a 15B parameter model that has been specially trained for code. We find that Nemotron-4 15B attains strong coding performance across a wide assortment of programming languages and outperforms both Starcoder and Mistral 7B on average. We especially highlight the superior performance of Nemotron-4 15B on low-resource programming languages such as Scala, Julia, and R.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T6.2" style="width:388.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T6.2.1"><span class="ltx_text" id="S3.T6.2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T6.2.1.1.1" style="width:388.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T6.2.1.1.1.1"><span class="ltx_text" id="S3.T6.2.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T6.2.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T6.2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.1"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.2">Size</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.3">JavaScript</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.4">Julia</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.5">Java</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.6">Lua</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.7">C++</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.8">C-Sharp</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.9">PHP</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.10">Shell</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.11">TypeScript</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.12">R</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.13">Scala</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.2.1.1.1.1.1.1.1.1.14">AVG</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T6.2.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.1">Starcoder</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.2">15B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.3">30.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.4">23.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.5">30.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.6">23.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.7">31.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.8">21.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.9">26.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.10">10.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.11">32.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.12">15.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.13">27.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.2.1.1.1.1.1.1.2.1.14">24.2</span></span>
<span class="ltx_tr" id="S3.T6.2.1.1.1.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T6.2.1.1.1.1.1.1.3.2.1">Mistral</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.2">7B</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.3">34.2</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.4">22.0</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.5">26.0</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.6">25.3</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.7">29.1</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.8">22.8</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.9">27.9</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.10">8.9</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.11">28.5</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.12">11.8</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.13">22.2</span>
<span class="ltx_td ltx_align_center" id="S3.T6.2.1.1.1.1.1.1.3.2.14">23.6</span></span>
<span class="ltx_tr" id="S3.T6.2.1.1.1.1.1.1.4.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.1">Nemotron-4</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.2">15B</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.3">28.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.4">24.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.5">24.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.6">24.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.7">35.4</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.8">21.1</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.9">27.3</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.10">8.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.11">32.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.12">18.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.13">27.3</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.2.1.1.1.1.1.1.4.3.14"><span class="ltx_text ltx_font_bold" id="S3.T6.2.1.1.1.1.1.1.4.3.14.1">24.5</span></span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T6.3.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S3.T6.4.2" style="font-size:90%;"> Nemotron-4 15B attains high competency in coding performance across a broad range of programming languages. Results for Mistral are from our runs of Mistral in the same setting as Nemotron-4. </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multilingual</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We demonstrate the outstanding multilingual ability of Nemotron-4 15B using four widely-studied benchmarks in previous works that cover a diverse range of high to low resource natural languages. For classification we use accuracy as the metric; for generative tasks, we use exact match; and for machine translation, we evaluate using the <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.1.1">sacreBLEU</span> <cite class="ltx_cite ltx_citemacro_citep">(Post, <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib32" title="">2018</a>)</cite> implementation of <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.1.2">BLEU</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Papineni et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib30" title="">2002</a>)</cite>, using <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.1.3">spm-flores-101</span> tokenization to obtain spBLEU scores.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">1. Classification:</span> Cross-lingual Choice of Plausible Alternatives (XCOPA) <cite class="ltx_cite ltx_citemacro_citep">(Ponti et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib31" title="">2020</a>)</cite> tests causal commonsense reasoning in 11 languages
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">We compare Nemotron-4 15B to existing multilingual language models: XGLM <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib28" title="">2022</a>)</cite> , mGPT <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib39" title="">2022</a>)</cite>, and BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib37" title="">2023</a>)</cite>.
XGLM and mGPT are models specially trained to have improved multilingual ability by up-sampling the presence of non-English languages in the training data.
In contrast, BLOOM, like Nemotron-4, is a general purpose language model that was trained on a combination of English, multilingual, and code data.
In Table <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T7" title="Table 7 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">7</span></a>, we clearly see that Nemotron-4 achieves the best performance amongst all models – realizing almost a 12% improvement in the four-shot setting.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T7.5" style="width:359.5pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T7.5.5"><span class="ltx_text" id="S3.T7.5.5.5">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T7.5.5.5.5" style="width:359.5pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T7.5.5.5.5.5"><span class="ltx_text" id="S3.T7.5.5.5.5.5.5" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T7.5.5.5.5.5.5.5">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.6.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.1">Mode</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.2">Model</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.3">Size</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.4">ET</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.5">HT</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.6">ID</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.7">IT</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.8">QU</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.9">SW</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.10">TA</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.11">TH</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.12">TR</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.13">VI</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.14">ZH</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T7.5.5.5.5.5.5.5.6.1.15">AVG</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_4" id="S3.T7.5.5.5.5.5.5.5.5.6"><span class="ltx_text" id="S3.T7.5.5.5.5.5.5.5.5.6.1">Zero-Shot</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.7">BLOOM</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.8">176B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.9">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.10">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.1.1.1.1.1.1.1.1.1"><math alttext="57.5^{*}" class="ltx_Math" display="inline" id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1a"><msup id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2" mathcolor="#000000" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">57.5</mn><mo id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3" mathcolor="#000000" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><cn id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.2">57.5</cn><times id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1c">57.5^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T7.1.1.1.1.1.1.1.1.1.m1.1d">57.5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.11">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.12">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.2.2.2.2.2.2.2.2.2"><math alttext="59.5^{*}" class="ltx_Math" display="inline" id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1"><semantics id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1a"><msup id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.cmml"><mn id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.2" mathcolor="#000000" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.2.cmml">59.5</mn><mo id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.3" mathcolor="#000000" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1b"><apply id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.1.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1">superscript</csymbol><cn id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.2.cmml" type="float" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.2">59.5</cn><times id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.3.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1c">59.5^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T7.2.2.2.2.2.2.2.2.2.m1.1d">59.5 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.3.3.3.3.3.3.3.3.3"><math alttext="54.7^{*}" class="ltx_Math" display="inline" id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1"><semantics id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1a"><msup id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.cmml"><mn id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.2" mathcolor="#000000" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.2.cmml">54.7</mn><mo id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.3" mathcolor="#000000" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1b"><apply id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.1.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1">superscript</csymbol><cn id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.2.cmml" type="float" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.2">54.7</cn><times id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.3.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1c">54.7^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T7.3.3.3.3.3.3.3.3.3.m1.1d">54.7 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.13">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.14">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.4.4.4.4.4.4.4.4.4"><math alttext="58.2^{*}" class="ltx_Math" display="inline" id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1"><semantics id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1a"><msup id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.cmml"><mn id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.2" mathcolor="#000000" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.2.cmml">58.2</mn><mo id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.3" mathcolor="#000000" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1b"><apply id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.1.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1">superscript</csymbol><cn id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.2.cmml" type="float" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.2">58.2</cn><times id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.3.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1c">58.2^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T7.4.4.4.4.4.4.4.4.4.m1.1d">58.2 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.5"><math alttext="57.7^{*}" class="ltx_Math" display="inline" id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1"><semantics id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1a"><msup id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.cmml"><mn id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.2" mathcolor="#000000" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.2.cmml">57.7</mn><mo id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.3" mathcolor="#000000" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1b"><apply id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.1.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1">superscript</csymbol><cn id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.2.cmml" type="float" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.2">57.7</cn><times id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.3.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1c">57.7^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.T7.5.5.5.5.5.5.5.5.5.m1.1d">57.7 start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.5.15">-</span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.7.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.7.1.1">XGLM</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.7.1.2">7.5B</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.3">57.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.4">57.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.5">59.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.6">49.2</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.7">52.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.8">55.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.9">55.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.10">57.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.11">55.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.12">59.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.13">53.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.7.1.14">55.6</span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.8.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.8.2.1">mGPT</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.8.2.2">13B</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.3">49.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.4">50.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.5">63.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.6">61.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.7">50.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.8">57.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.9">57.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.10">54.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.11">58.2</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.12">60.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.13">54.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.8.2.14">56.1</span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.9.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.9.3.1">Nemotron-4</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.9.3.2">15B</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.3">62.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.4">47.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.5">66.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.6">67.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.7">53.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.8">50.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.9">62.0</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.10">59.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.11">57.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.12">65.2</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.13">62.2</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.9.3.14"><span class="ltx_text ltx_font_bold" id="S3.T7.5.5.5.5.5.5.5.9.3.14.1">59.5</span></span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.10.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3" id="S3.T7.5.5.5.5.5.5.5.10.4.1"><span class="ltx_text" id="S3.T7.5.5.5.5.5.5.5.10.4.1.1">4-Shot</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.2">XGLM</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.3">7.5B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.4">64.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.5">60.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.6">67.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.7">64.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.8">50.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.9">61.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.10">56.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.11">61.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.12">60.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.13">68.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.14">59.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T7.5.5.5.5.5.5.5.10.4.15">61.4</span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.11.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.11.5.1">mGPT</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S3.T7.5.5.5.5.5.5.5.11.5.2">13B</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.3">48.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.4">48.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.5">62.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.6">60.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.7">50.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.8">56.6</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.9">55.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.10">54.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.11">57.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.12">61.8</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.13">58.4</span>
<span class="ltx_td ltx_align_center" id="S3.T7.5.5.5.5.5.5.5.11.5.14">56.0</span></span>
<span class="ltx_tr" id="S3.T7.5.5.5.5.5.5.5.12.6">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.1">Nemotron-4</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.2">15B</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.3">72.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.4">52.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.5">79.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.6">79.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.7">50.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.8">52.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.9">72.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.10">66.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.11">77.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.12">78.6</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.13">76.0</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T7.5.5.5.5.5.5.5.12.6.14"><span class="ltx_text ltx_font_bold" id="S3.T7.5.5.5.5.5.5.5.12.6.14.1">68.9</span></span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T7.9.2.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S3.T7.7.1" style="font-size:90%;"> Comparison of Nemotron-4 15B against existing large language models on XCOPA under the zero- and four-shot setting. Our reported results for XGLM are from the runs of the model in <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib39" title="">2022</a>)</cite> given that we use the same prompt template used by mGPT. The values marked with <math alttext="*" class="ltx_Math" display="inline" id="S3.T7.7.1.m1.1"><semantics id="S3.T7.7.1.m1.1b"><mo id="S3.T7.7.1.m1.1.1" xref="S3.T7.7.1.m1.1.1.cmml">*</mo><annotation-xml encoding="MathML-Content" id="S3.T7.7.1.m1.1c"><times id="S3.T7.7.1.m1.1.1.cmml" xref="S3.T7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.7.1.m1.1d">*</annotation><annotation encoding="application/x-llamapun" id="S3.T7.7.1.m1.1e">*</annotation></semantics></math> are read from figures in <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib37" title="">2023</a>)</cite>. </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p4.1.1">2. Generation:</span> We consider two generative tasks: TyDiQA-GoldP&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib13" title="">2020</a>)</cite> and Multilingual Grade School Math (MGSM)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib38" title="">2022</a>)</cite>. TyDiQA-GoldP is a question answering task while MGSM evaluates the arithmetic reasoning ability of language models in 10 languages.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1">In comparing the performance of Nemotron-4 15B on TyDiQA-GoldP to a range of models, Table <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T8" title="Table 8 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">8</span></a> shows that Nemotron-4 15B achieves the best performance. Impressively, Nemotron-4 15B is able to significantly improve upon the next best model, PaLM 62B-cont.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T8.2" style="width:258.0pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T8.2.1"><span class="ltx_text" id="S3.T8.2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T8.2.1.1.1" style="width:258.0pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T8.2.1.1.1.1"><span class="ltx_text" id="S3.T8.2.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T8.2.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.1">Model</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.2">Size</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.3">AR</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.4">BN</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.5">FI</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.6">ID</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.7">KO</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.8">RU</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.9">SW</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.10">TE</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T8.2.1.1.1.1.1.1.1.1.11">AVG</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2" id="S3.T8.2.1.1.1.1.1.1.2.1.1"><span class="ltx_text" id="S3.T8.2.1.1.1.1.1.1.2.1.1.1">PaLM</span></span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.2">62B</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.3">31.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.4">42.5</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.5">41.7</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.6">41.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.7">49.3</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.8">29.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.9">58.1</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.10">30.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T8.2.1.1.1.1.1.1.2.1.11">40.5</span></span>
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.3.2">
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.1">62B-cont</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.2">39.4</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.3">48.7</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.4">44.0</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.5">49.2</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.6">52.5</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.7">35.6</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.8">60.9</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.9">35.3</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.3.2.10">45.7</span></span>
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.4.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T8.2.1.1.1.1.1.1.4.3.1">LLaMA-2</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.2">13B</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.3">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.4">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.5">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.6">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.7">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.8">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.9">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.10">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.4.3.11">33.2</span></span>
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.5.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T8.2.1.1.1.1.1.1.5.4.1">Baichuan-2</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.2">13B</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.3">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.4">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.5">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.6">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.7">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.8">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.9">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.10">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.5.4.11">30.8</span></span>
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.6.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T8.2.1.1.1.1.1.1.6.5.1">QWEN</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.2">14B</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.3">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.4">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.5">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.6">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.7">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.8">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.9">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.10">-</span>
<span class="ltx_td ltx_align_right" id="S3.T8.2.1.1.1.1.1.1.6.5.11">39.8</span></span>
<span class="ltx_tr" id="S3.T8.2.1.1.1.1.1.1.7.6">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.1">Nemotron-4</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.2">15B</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.3">39.1</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.4">55.8</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.5">52.2</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.6">54.5</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.7">55.1</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.8">37.8</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.9">54.5</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.10">55.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T8.2.1.1.1.1.1.1.7.6.11"><span class="ltx_text ltx_font_bold" id="S3.T8.2.1.1.1.1.1.1.7.6.11.1">50.5</span></span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T8.3.1.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S3.T8.4.2" style="font-size:90%;"> Comparative results in the one-shot setting on TyDiQA-GoldP. Results for LLaMA-2 13B, Baichuan-2 13B and QWEN 14B are taken from <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib10" title="">2024</a>)</cite>. </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.p6">
<p class="ltx_p" id="S3.SS4.p6.1">Further demonstrating the impressive multilingual ability of Nemotron-4 15B, Table <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T9" title="Table 9 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">9</span></a> shows the performance on MGSM.
We report using the English chain-of-thought setting introduced in <cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib38" title="">2022</a>)</cite> where all chain of thought explanations are presented to the model in English rather than in the language of the task. On this challenging task which assesses the intersection of mathematical and multilingual ability, Nemotron-4 15B achieves the best performance amongst compared models and improves upon the closest score by nearly 30%.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T9.2" style="width:351.5pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T9.2.1"><span class="ltx_text" id="S3.T9.2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T9.2.1.1.1" style="width:351.5pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T9.2.1.1.1.1"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T9.2.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T9.2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.1">Mode</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.2">Model</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.3">Size</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.4">DE</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.5">FR</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.6">ES</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.7">RU</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.8">ZH</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.9">JA</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.10">TH</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.11">TE</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.12">BN</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.13">SW</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T9.2.1.1.1.1.1.1.1.1.14">AVG</span></span>
<span class="ltx_tr" id="S3.T9.2.1.1.1.1.1.1.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.1"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.2.2.1.1">Native-COT</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.2"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.2.2.2.1">PaLM</span></span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.3">62B</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.4">24.0</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.5">24.0</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.6">26.0</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.7">22.8</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.8">24.8</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.9">14.8</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.10">18.0</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.11">11.6</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.12">13.6</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.13">9.6</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.2.2.14">18.9</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T9.2.1.1.1.1.1.1.3.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3" id="S3.T9.2.1.1.1.1.1.1.3.1.1"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.3.1.1.1">English-COT</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.2"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.3.1.2.1">PALM</span></span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.3">62B-cont</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.4">44.8</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.5">39.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.6">44.4</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.7">36.8</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.8">33.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.9">24.0</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.10">28.0</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.11">19.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.12">28.0</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.13">21.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T9.2.1.1.1.1.1.1.3.1.14">32.0</span></span>
<span class="ltx_tr" id="S3.T9.2.1.1.1.1.1.1.4.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T9.2.1.1.1.1.1.1.4.2.1"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.4.2.1.1">Mistral</span></span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.2">7B</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.3">33.2</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.4">35.2</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.5">35.6</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.6">35.2</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.7">33.2</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.8">18.8</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.9">10.0</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.10">0.0</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.11">8.0</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.12">9.2</span>
<span class="ltx_td ltx_align_right" id="S3.T9.2.1.1.1.1.1.1.4.2.13">21.8</span></span>
<span class="ltx_tr" id="S3.T9.2.1.1.1.1.1.1.5.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.1"><span class="ltx_text" id="S3.T9.2.1.1.1.1.1.1.5.3.1.1">Nemotron-4</span></span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.2">15B</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.3">46.8</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.4">46.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.5">50.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.6">45.6</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.7">40.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.8">40.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.9">43.6</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.10">41.6</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.11">43.6</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.12">16.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb" id="S3.T9.2.1.1.1.1.1.1.5.3.13"><span class="ltx_text ltx_font_bold" id="S3.T9.2.1.1.1.1.1.1.5.3.13.1">41.3</span></span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T9.3.1.1" style="font-size:90%;">Table 9</span>: </span><span class="ltx_text" id="S3.T9.4.2" style="font-size:90%;"> Eight-shot accuracy results on MGSM. Results for Mistral are from our runs of Mistral in the same setting as Nemotron-4. </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.p7">
<p class="ltx_p" id="S3.SS4.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p7.1.1">3. Machine Translation:</span> We additionally evaluate the translation ability of our models through the FLORES-101 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib19" title="">2021</a>)</cite> benchmark. The ability to translate between languages is a good test of the model’s ability to relate and understand semantic relationships between languages.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p8">
<p class="ltx_p" id="S3.SS4.p8.1">As seen in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#S3.T10" title="Table 10 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report"><span class="ltx_text ltx_ref_tag">10</span></a>, Nemotron-4 15B heftily outperforms both LLaMA-2 13B and Baichuan-2 13B – improving upon their performance by 90.2% and 44.1% respectively. Nemotron-4 15B does not solely perform well on translating from Chinese into English but is able to attain impressive results on the direct translation of Chinese into other languages. This ability highlights the strong understanding that Nemotron-4 15B has across a broad spectrum of natural languages.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T10">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T10.2" style="width:309.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p class="ltx_p" id="S3.T10.2.1"><span class="ltx_text" id="S3.T10.2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T10.2.1.1.1" style="width:309.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S3.T10.2.1.1.1.1"><span class="ltx_text" id="S3.T10.2.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T10.2.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S3.T10.2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.1"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.2">Size</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.3">ZH-EN</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.4">ZH-FR</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.5">ZH-ES</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.6">ZH-AR</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.7">ZH-RU</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.8">ZH-JA</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.9">ZH-DE</span>
<span class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S3.T10.2.1.1.1.1.1.1.1.1.10">AVG</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T10.2.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.1"><span class="ltx_text" id="S3.T10.2.1.1.1.1.1.1.2.1.1.1">LLaMA-2</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.2">13B</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.3">25.4</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.4">19.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.5">17.5</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.6">1.4</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.7">10.3</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.8">0.1</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.9">11.1</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.2.1.10">12.2</span></span>
<span class="ltx_tr" id="S3.T10.2.1.1.1.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.1"><span class="ltx_text" id="S3.T10.2.1.1.1.1.1.1.3.2.1.1">Baichuan-2</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.2">13B</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.3">30.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.4">22.1</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.5">17.3</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.6">2.4</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.7">14.2</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.8">11.6</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.9">14.5</span>
<span class="ltx_td ltx_align_right ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.3.2.10">16.1</span></span>
<span class="ltx_tr" id="S3.T10.2.1.1.1.1.1.1.4.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.1">Nemotron-4</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.2">15B</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.3">34.0</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.4">28.1</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.5">21.3</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.6">16.8</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.7">21.2</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.8">23.1</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.9">18.1</span>
<span class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S3.T10.2.1.1.1.1.1.1.4.3.10"><span class="ltx_text ltx_font_bold" id="S3.T10.2.1.1.1.1.1.1.4.3.10.1">23.2</span></span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T10.3.1.1" style="font-size:90%;">Table 10</span>: </span><span class="ltx_text" id="S3.T10.4.2" style="font-size:90%;"> Eight-shot results on Flores sub-tasks translating out of Chinese. All results for external models were obtained from <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.16819v2#bib.bib49" title="">2023</a>)</cite></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section">4&nbsp;&nbsp;&nbsp;Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We present Nemotron-4 15B, a decoder-only transformer-based large language model.
It is trained on 8 trillion tokens spanning English, 53 additional natural languages as well as 43 programming languages. Nemotron-4 15B exhibits the strongest multilingual performance of any general purpose language model at its scale – even outperforming models specialized for the multilingual domain.
Nemotron-4 demonstrates that pre-training sets for large language models can continue to be scaled up even further in order to improve the abilities of models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
NVLink and NVSwitch.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nvidia.com/en-us/data-center/nvlink/" title="">https://www.nvidia.com/en-us/data-center/nvlink/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ainslie et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joshua Ainslie, James Lee-Thorp, Michiel de&nbsp;Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.

</span>
<span class="ltx_bibblock">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2305.13245</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allal et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Loubna&nbsp;Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos&nbsp;Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh&nbsp;Kumar Umapathi, Carolyn&nbsp;Jane Anderson, Yangtian Zi, Joel&nbsp;Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco&nbsp;De Toni, Bernardo&nbsp;García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry&nbsp;Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de&nbsp;Vries, and Leandro von Werra.

</span>
<span class="ltx_bibblock">SantaCoder: Don’t Reach for the Stars!, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.

</span>
<span class="ltx_bibblock">Program Synthesis with Large Language Models, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Qwen Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan&nbsp;Le Bras, Jianfeng Gao, and Yejin Choi.

</span>
<span class="ltx_bibblock">PIQA: Reasoning about Physical Commonsense in Natural Language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">AAAI</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassano et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn&nbsp;Jane Anderson, Molly&nbsp;Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.

</span>
<span class="ltx_bibblock">MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE Transactions on Software Engineering</em>, pages 1–17, 2023a.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1109/TSE.2023.3267446" title="">10.1109/TSE.2023.3267446</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassano et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn&nbsp;Jane Anderson, Molly&nbsp;Q Feldman, et&nbsp;al.

</span>
<span class="ltx_bibblock">Multipl-e: a scalable and polyglot approach to benchmarking neural code generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">IEEE Transactions on Software Engineering</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Du&nbsp;Chen, Yi&nbsp;Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, and Kun Han.

</span>
<span class="ltx_bibblock">Orion-14B: Open-source Multilingual Large Language Models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique&nbsp;Ponde de&nbsp;Oliveira&nbsp;Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe&nbsp;Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William&nbsp;Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew&nbsp;N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.

</span>
<span class="ltx_bibblock">Evaluating Large Language Models Trained on Code, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, et&nbsp;al.

</span>
<span class="ltx_bibblock">PaLM: Scaling Language Modeling with Pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonathan&nbsp;H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki.

</span>
<span class="ltx_bibblock">TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">CoRR</em>, abs/2003.05002, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2003.05002" title="">https://arxiv.org/abs/2003.05002</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think You have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.

</span>
<span class="ltx_bibblock">Training Verifiers to Solve Math Word Problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">CoRR</em>, abs/2110.14168, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2110.14168" title="">https://arxiv.org/abs/2110.14168</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.

</span>
<span class="ltx_bibblock">A Framework for Few-shot Language Model Evaluation, September 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5281/zenodo.5371628" title="">https://doi.org/10.5281/zenodo.5371628</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemma&nbsp;Team (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Google&nbsp;DeepMind Gemma&nbsp;Team.

</span>
<span class="ltx_bibblock">Gemma: Open Models Based on Gemini Research and Technology, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Gemini: A Family of Highly Capable Multimodal Models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da&nbsp;Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan.

</span>
<span class="ltx_bibblock">The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">CoRR</em>, abs/2106.03193, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2106.03193" title="">https://arxiv.org/abs/2106.03193</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring Massive Multitask Language Understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2009.03300</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training Compute-Optimal Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jennings et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Shrimai Prabhumoye, Ayush Dattagupta, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Curating Trillion-Token Datasets: Introducing NVIDIA NeMo Data Curator.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/blog/curating-trillion-token-datasets-introducing-nemo-data-curator/" title="">https://developer.nvidia.com/blog/curating-trillion-token-datasets-introducing-nemo-data-curator/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling Laws for Neural Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korthikanti et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Reducing Activation Recomputation in Large Transformer Models, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:1808.06226</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Raymond Li, Loubna&nbsp;Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry&nbsp;Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh&nbsp;Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva&nbsp;Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn&nbsp;Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos&nbsp;Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von
Werra, and Harm de&nbsp;Vries.

</span>
<span class="ltx_bibblock">StarCoder: May the Source be with You!, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit&nbsp;Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li.

</span>
<span class="ltx_bibblock">Few-shot Learning with Multilingual Language Models, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
NVIDIA.

</span>
<span class="ltx_bibblock">H100 Tensor Core GPU Architecture Overview, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et&nbsp;al. (2002)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">BLEU: A Method for Automatic Evaluation of Machine Translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</em>, ACL ’02, page 311–318, USA, 2002. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.3115/1073083.1073135" title="">10.3115/1073083.1073135</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.3115/1073083.1073135" title="">https://doi.org/10.3115/1073083.1073135</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ponti et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edoardo&nbsp;Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen.

</span>
<span class="ltx_bibblock">XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">CoRR</em>, abs/2005.00333, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2005.00333" title="">https://arxiv.org/abs/2005.00333</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Matt Post.

</span>
<span class="ltx_bibblock">A Call for Clarity in Reporting BLEU Scores.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">CoRR</em>, abs/1804.08771, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1804.08771" title="">http://arxiv.org/abs/1804.08771</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jack&nbsp;W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van&nbsp;den Driessche, Lisa&nbsp;Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang&nbsp;Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de&nbsp;Masson&nbsp;d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de&nbsp;Las&nbsp;Casas, Aurelia Guy, Chris Jones,
James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed&nbsp;Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">The Journal of Machine Learning Research</em>, 21(1):5485–5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">AAAI</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.

</span>
<span class="ltx_bibblock">Socialiqa: Commonsense reasoning about social interactions, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander&nbsp;M. Rush, Stella Biderman, Albert Webson, Pawan&nbsp;Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert&nbsp;Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz&nbsp;Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro&nbsp;Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham&nbsp;Fikri Aji, Amit Alfassy, Anna Rogers, Ariel&nbsp;Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David&nbsp;Ifeoluwa Adelani, Dragomir Radev, Eduardo&nbsp;González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal&nbsp;Bar Natan, Francesco&nbsp;De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin,
Isaac Johnson, Itziar Gonzalez-Dios, Javier de&nbsp;la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro&nbsp;Von Werra, Leon Weber, Long Phan, Loubna&nbsp;Ben allal, Ludovic Tanguy, Manan Dey, Manuel&nbsp;Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh&nbsp;Chien Vu, Mohammad&nbsp;A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de&nbsp;Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto&nbsp;Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen&nbsp;Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago&nbsp;Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut&nbsp;Emre Taşar, Elizabeth Salesky, Sabrina&nbsp;J. Mielke, Wilson&nbsp;Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason&nbsp;Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M&nbsp;Saiful Bari, Maged&nbsp;S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen&nbsp;H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung&nbsp;Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette,
Pierre&nbsp;François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta&nbsp;Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica&nbsp;Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van&nbsp;der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol,
Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos&nbsp;Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong&nbsp;A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio&nbsp;Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav&nbsp;Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo&nbsp;Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu,
Clémentine Fourrier, Daniel&nbsp;León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena&nbsp;U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose&nbsp;David Posada, Karthik&nbsp;Rangasai Sivaraman, Lokesh Bulchandani, Lu&nbsp;Liu, Luisa Shinzato, Madeleine&nbsp;Hahn de&nbsp;Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria&nbsp;A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel&nbsp;De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas&nbsp;Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok&nbsp;S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,
Yash&nbsp;Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu&nbsp;Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf.

</span>
<span class="ltx_bibblock">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung&nbsp;Won Chung, Yi&nbsp;Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.

</span>
<span class="ltx_bibblock">Language Models are Multilingual Chain-of-Thought Reasoners, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shliazhko et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina.

</span>
<span class="ltx_bibblock">mGPT: Few-Shot Learners Go Multilingual, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Megatron-LM: Training Multi-Billion Parameter Language Models using Model Parallelism.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Slav&nbsp;Petrov and et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andrew M.&nbsp;Dai Slav&nbsp;Petrov, Yonghui&nbsp;Wu and et&nbsp;al.

</span>
<span class="ltx_bibblock">PaLM 2 Technical Report, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.google/static/documents/palm2techreport.pdf" title="">https://ai.google/static/documents/palm2techreport.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zheng, Rewon Child, Reza&nbsp;Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">CoRR</em>, abs/2201.11990, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.11990" title="">https://arxiv.org/abs/2201.11990</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Ahmed Murtadha, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced Transformer with Rotary Position Embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2104.09864</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzgun et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V. Le, Ed&nbsp;H. Chi, Denny Zhou, and Jason Wei.

</span>
<span class="ltx_bibblock">Challenging big-bench tasks and whether chain-of-thought can solve them, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language Models, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-tuned Chat Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">CoRR</em>, abs/1706.03762, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1706.03762" title="">http://arxiv.org/abs/1706.03762</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:1911.00359</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da&nbsp;Pan, Dian Wang, Dong Yan, Fan Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Baichuan 2: Open Large-scale Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2309.10305</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">HellaSwag: Can a Machine Really Finish Your Sentence?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">ACL</em>, 2019.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">Supplementary Materials</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="Ax1.T11">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ax1.T11.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ax1.T11.2.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="Ax1.T11.2.1.1.1"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt" id="Ax1.T11.2.1.1.2">Size</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ax1.T11.2.1.1.3">Humanities</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ax1.T11.2.1.1.4">Social sciences</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ax1.T11.2.1.1.5">STEM</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ax1.T11.2.1.1.6">Other</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Ax1.T11.2.1.1.7">Average</td>
</tr>
<tr class="ltx_tr" id="Ax1.T11.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.1">Nemotron-4</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.2">15B</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.3">69.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.4">74.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.5">53.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.6">67.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Ax1.T11.2.2.2.7">64.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ax1.T11.3.1.1" style="font-size:90%;">Table 11</span>: </span><span class="ltx_text" id="Ax1.T11.4.2" style="font-size:90%;"> Per-category breakdown accuracy for MMLU </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
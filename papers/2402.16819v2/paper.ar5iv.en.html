<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.16819] Nemotron-4 15B Technical Report</title><meta property="og:description" content="We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Nemotron-4 15B Technical Report">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Nemotron-4 15B Technical Report">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.16819">

<!--Generated on Tue Mar  5 18:37:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on  %“date–February 2024˝ .-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Nemotron-4 15B Technical Report</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jupinder Parmar<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
Equal contribution, corresponding authors: <span id="footnote1.1" class="ltx_text ltx_font_typewriter">{jupinderp,sprabhumoye,jjennings,mpatwary}@nvidia.com</span>.
</span></span></span> &nbsp;&nbsp;Shrimai Prabhumoye<sup id="id4.4.id1" class="ltx_sup"><span id="id4.4.id1.1" class="ltx_text ltx_font_italic">∗</span></sup> &nbsp;&nbsp;Joseph Jennings<sup id="id5.5.id2" class="ltx_sup"><span id="id5.5.id2.1" class="ltx_text ltx_font_italic">∗</span></sup> &nbsp;&nbsp;Mostofa Patwary<sup id="id6.6.id3" class="ltx_sup"><span id="id6.6.id3.1" class="ltx_text ltx_font_italic">∗</span></sup> 
<br class="ltx_break">Sandeep Subramanian<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Work done while at NVIDIA.</span></span></span> &nbsp;&nbsp;Dan Su &nbsp;&nbsp;Chen Zhu &nbsp;&nbsp;Deepak Narayanan &nbsp;&nbsp; Aastha Jhunjhunwala &nbsp;&nbsp; Ayush Dattagupta &nbsp;&nbsp;Vibhu Jawa &nbsp;&nbsp;Jiwei Liu &nbsp;&nbsp;Ameya Mahabaleshwarkar &nbsp;&nbsp;Osvald Nitski &nbsp;&nbsp; Annika Brundyn &nbsp;&nbsp;James Maki &nbsp;&nbsp;Miguel Martinez &nbsp;&nbsp;Jiaxuan You &nbsp;&nbsp;John Kamalu &nbsp;&nbsp;Patrick LeGresley &nbsp;&nbsp;Denys Fridman &nbsp;&nbsp;Jared Casper &nbsp;&nbsp; Ashwath Aithal &nbsp;&nbsp;Oleksii Kuchaiev &nbsp;&nbsp;Mohammad Shoeybi &nbsp;&nbsp;Jonathan Cohen &nbsp;&nbsp;Bryan Catanzaro <span id="id7.7.id4" class="ltx_text ltx_font_bold">NVIDIA</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section">1&nbsp;&nbsp;&nbsp;Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Recently published efforts <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>; Touvron et&nbsp;al., <a href="#bib.bib45" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib46" title="" class="ltx_ref">b</a>; Yang et&nbsp;al., <a href="#bib.bib49" title="" class="ltx_ref">2023</a>; Jiang et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite> in language model pre-training have been inspired by Chinchilla scaling laws <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>, which argue for scaling data along with model size given a fixed compute budget, compared to past work that only scaled the size of the model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kaplan et&nbsp;al., <a href="#bib.bib24" title="" class="ltx_ref">2020</a>; Brown et&nbsp;al., <a href="#bib.bib7" title="" class="ltx_ref">2020</a>; Smith et&nbsp;al., <a href="#bib.bib42" title="" class="ltx_ref">2022</a>; Rae et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2022</a>; Scao et&nbsp;al., <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>.
For example, <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite> shows that given two roughly IsoFLOP GPT models with a similar data distribution, a 65-billion-parameter model on 1.4 trillion tokens and a 280-billion-parameter model on 300 billion tokens, the 65B model has better accuracy on downstream tasks.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">This trade-off of allocating compute towards training on more data as opposed to increasing model size is particularly appealing from an inference perspective, reducing latency and the amount of compute needed to serve models. As a consequence, a major focus of language modeling training efforts has shifted to collecting high-quality multi-trillion token datasets from public sources such as Common Crawl. We continue this trend by introducing Nemotron-4 15B which was trained on 8 trillion tokens of English, multilingual, and coding text and was developed to be the best general-purpose large language model (LLM) that can fit on a single NVIDIA A100 or H100 GPU.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">As demonstrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, Nemotron-4 15B exhibits high downstream accuracies across a wide range of English, code, and multilingual evaluation areas. In comparison to leading similarly-sized, open models we show that Nemotron-4 15B is significantly better than LLaMA-2 34B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib46" title="" class="ltx_ref">2023b</a>)</cite>, which has over twice the number of parameters, and is better than Mistral 7B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite> on all English evaluation areas. Additionally, Nemotron-4 15B achieves competitive accuracies to QWEN 14B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> and Gemma 7B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gemma&nbsp;Team, <a href="#bib.bib17" title="" class="ltx_ref">2024</a>)</cite>. In a comparison across a wide range of programming languages, we find that Nemotron-4 15B achieves better average accuracy, and in particular on low-resource programming languages, than Starcoder&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>, a code-specific model, and Mistral 7B. As Nemotron-4 15B was trained on significant amount of multilingual data, it is currently the state-of-the-art general purpose model in its size class on all multilingual benchmarks. We find that Nemotron-4 is better than PALM 62B-Cont&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Slav&nbsp;Petrov and et&nbsp;al., <a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>, and also outperforms multilingual-specific models such as XGLM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite> and mGPT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.16819/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="284" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2402.16819/assets/x2.png" id="S1.F1.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="287" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:90%;">Comparison of Nemotron-4 15B across seven evaluation areas against similarly sized models. The composition of tasks that form each evaluation area can be found, along with more detailed evaluation results, in Section <a href="#S3" title="3 Results ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></span></figcaption>
</figure>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.2.1.1" class="ltx_tr">
<td id="S1.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Number of</td>
<td id="S1.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Hidden</td>
<td id="S1.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Number of</td>
<td id="S1.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Number of</td>
<td id="S1.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">Sequence</td>
<td id="S1.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Vocabulary</td>
</tr>
<tr id="S1.T1.2.2.2" class="ltx_tr">
<td id="S1.T1.2.2.2.1" class="ltx_td ltx_align_center">transformer layers</td>
<td id="S1.T1.2.2.2.2" class="ltx_td ltx_align_center">dimension</td>
<td id="S1.T1.2.2.2.3" class="ltx_td ltx_align_center">attention heads</td>
<td id="S1.T1.2.2.2.4" class="ltx_td ltx_align_center">KV heads</td>
<td id="S1.T1.2.2.2.5" class="ltx_td ltx_align_center">length</td>
<td id="S1.T1.2.2.2.6" class="ltx_td ltx_align_center">size</td>
</tr>
<tr id="S1.T1.2.3.3" class="ltx_tr">
<td id="S1.T1.2.3.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">32</td>
<td id="S1.T1.2.3.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">6144</td>
<td id="S1.T1.2.3.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">48</td>
<td id="S1.T1.2.3.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">8</td>
<td id="S1.T1.2.3.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">4096</td>
<td id="S1.T1.2.3.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">256,000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.4.2" class="ltx_text" style="font-size:90%;">Key hyper-parameters affecting size of Nemotron-4 15B.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section">2&nbsp;&nbsp;&nbsp;Architecture Details</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Nemotron-4 uses a standard decoder-only Transformer architecture&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite>, with causal attention masks. Exact hyper-parameters affecting size are shown in Table&nbsp;<a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Nemotron-4 has 3.2 billion embedding parameters and 12.5 billion non-embedding parameters.
We use Rotary Position Embeddings (RoPE)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Su et&nbsp;al., <a href="#bib.bib43" title="" class="ltx_ref">2021</a>)</cite>, SentencePiece tokenizer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite>, squared ReLU activations in the MLP layers, no bias terms, dropout rate of zero, and untied input-output embeddings.
We use grouped query attention (GQA)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ainslie et&nbsp;al., <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite> for faster inference and lower memory footprint.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">We train Nemotron-4 15B on a pre-training dataset consisting of 8 trillion tokens. At a high-level, the data blend is split into three different types of data: English natural language data (70%), multilingual natural language data (15%), and source-code data (15%).</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2402.16819/assets/plots/new_full_distr.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="280" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Data composition of the English tokens used for pre-training</span></figcaption>
</figure>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">The English corpus consists of curated documents from a variety of sources and domains including web documents, news articles, scientific papers, books, etc and the distribution used in our pre-training set is highlighted in Figure&nbsp;<a href="#S2.F2" title="Figure 2 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The code and multilingual data consists of a diverse set of natural and programming languages. We find that appropriately sampling tokens from these languages is key to strong accuracies in these domains. We share the distributions used for both code and multilingual tokens in our pre-training dataset in Figure&nbsp;<a href="#S2.F3" title="Figure 3 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Figure&nbsp;<a href="#S2.F4" title="Figure 4 ‣ Data. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> respectively.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2402.16819/assets/plots/test_code.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Data distribution of the 43 programming languages used for pre-training. The number within each bar indicates the percent of the overall code distribution that an individual language comprises.</span></figcaption>
</figure>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p3.1" class="ltx_p">In constructing the pre-training corpus, we remove any possible duplicates via document-level exact and near-deduplication&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jennings et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>. We additionally applied document-level quality filtering across our corpus using a language-model based filtering approach similar to <cite class="ltx_cite ltx_citemacro_citep">(Wenzek et&nbsp;al., <a href="#bib.bib48" title="" class="ltx_ref">2019</a>)</cite> in addition to a series of heuristic filters as described in <cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p4.1" class="ltx_p">We train a BPE tokenizer in SentencePiece&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite> on data that is randomly sampled from the final 8T token dataset. To have better coverage of low-resource languages in the tokenizer, we upsample non-English data relative to the final training dataset distribution. Our tokenizer preserves whitespaces (including leading and trailing ones), splits numbers into their individual digits&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>, and relies on byte-level backoff to handle unknown character sequences. The final vocabulary size is 256,000 tokens.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2402.16819/assets/plots/test_multi.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="274" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.3.2" class="ltx_text" style="font-size:90%;">Data distribution of the 53 natural languages, aside from English,we used for pre-training. The number within each bar indicates the percent of the overall multilingual distribution that an individual language comprises.</span></figcaption>
</figure>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Pre-training.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Nemotron-4 was trained using 384 DGX H100 nodes; each node contains 8 H100 80GB SXM5 GPUs based on the NVIDIA Hopper architecture&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(NVIDIA, <a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite>.
Each H100 GPU has a peak throughput of 989 teraFLOP/s when doing 16-bit floating point (<span id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">bfloat16</span>) arithmetic without sparsity. Within each node, GPUs are connected by NVLink and NVSwitch&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib1" title="" class="ltx_ref">nvl, </a>)</cite>; the GPU-to-GPU bandwidth is 900 GB/s (450 GB/s in each direction). Each node has 8 NVIDIA Mellanox 400 Gbps HDR InfiniBand Host Channel Adapters (HCAs) for inter-node communication.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">We used a combination of 8-way tensor parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shoeybi et&nbsp;al., <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite> and data parallelism to train the model; we also use a distributed optimizer to shard the optimizer state over the data-parallel replicas. The degree of data parallelism was varied from 96 to 384 as the batch size was ramped up.
Table&nbsp;<a href="#S2.T2" title="Table 2 ‣ Pre-training. ‣ 2 Architecture Details ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the 3 stages of batch size ramp, and includes the per-iteration time and model FLOP/s utilization (MFU)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>; Korthikanti et&nbsp;al., <a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>. MFU quantifies how efficiently the GPUs are utilized in model training. Training was completed in approximately 13 calendar days.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.2.1.1" class="ltx_tr">
<th id="S2.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Data-parallel size</th>
<th id="S2.T2.2.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">GPUs</th>
<th id="S2.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Iteration time (secs)</th>
<th id="S2.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MFU (%)</th>
<th id="S2.T2.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Batch size</th>
<th id="S2.T2.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Tokens (B)</th>
<th id="S2.T2.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Time (days)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.2.2.1" class="ltx_tr">
<td id="S2.T2.2.2.1.1" class="ltx_td ltx_align_center ltx_border_tt">96</td>
<td id="S2.T2.2.2.1.2" class="ltx_td ltx_align_right ltx_border_tt">768</td>
<td id="S2.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">0.57</td>
<td id="S2.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">34.3</td>
<td id="S2.T2.2.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">384</td>
<td id="S2.T2.2.2.1.6" class="ltx_td ltx_align_center ltx_border_tt">200</td>
<td id="S2.T2.2.2.1.7" class="ltx_td ltx_align_center ltx_border_tt">0.8</td>
</tr>
<tr id="S2.T2.2.3.2" class="ltx_tr">
<td id="S2.T2.2.3.2.1" class="ltx_td ltx_align_center">192</td>
<td id="S2.T2.2.3.2.2" class="ltx_td ltx_align_right">1,536</td>
<td id="S2.T2.2.3.2.3" class="ltx_td ltx_align_center">0.58</td>
<td id="S2.T2.2.3.2.4" class="ltx_td ltx_align_center">33.3</td>
<td id="S2.T2.2.3.2.5" class="ltx_td ltx_align_center">768</td>
<td id="S2.T2.2.3.2.6" class="ltx_td ltx_align_center">200</td>
<td id="S2.T2.2.3.2.7" class="ltx_td ltx_align_center">0.4</td>
</tr>
<tr id="S2.T2.2.4.3" class="ltx_tr">
<td id="S2.T2.2.4.3.1" class="ltx_td ltx_align_center ltx_border_bb">288</td>
<td id="S2.T2.2.4.3.2" class="ltx_td ltx_align_right ltx_border_bb">2,304</td>
<td id="S2.T2.2.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">0.64</td>
<td id="S2.T2.2.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">30.5</td>
<td id="S2.T2.2.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">1,152</td>
<td id="S2.T2.2.4.3.6" class="ltx_td ltx_align_center ltx_border_bb">7,600</td>
<td id="S2.T2.2.4.3.7" class="ltx_td ltx_align_center ltx_border_bb">11.9</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S2.T2.4.2" class="ltx_text" style="font-size:90%;">Batch size rampup schedule, along with time and efficiency metrics for the Nemotron-4 15B parameter model.</span></figcaption>
</figure>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Continued Training.</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">Similar to recent work&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Google, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, we find that switching the data distribution and learning rate decay schedule at the end of model training greatly improves model quality. Concretely, after having trained over the entirety of our 8T pre-training dataset, we use the same loss objective and perform continued training on small number of tokens in comparison to the pre-training tokens.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px3.p2.1" class="ltx_p">In this additional phase of continued training, we utilize two distinct data distributions.
The first distribution is where the majority of tokens during continued training are sampled from. It utilizes tokens that have already been introduced during pre-training but with a distribution that places larger sampling weight on higher quality sources.
The second distribution introduces a small number of benchmark-style alignment examples to better allow the model to respond to such questions in downstream evaluations while also up-weighting data sources that come from areas of low model performance.
In accompaniment with a learning rate schedule that prioritizes a steeper slope of decay than magnitude of learning rate, we find that such an ordering and style of data distributions allows for the model to gently transition from the pre-training dataset and better learn newly emphasized data areas.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section">3&nbsp;&nbsp;&nbsp;Results</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">We evaluate Nemotron-4 15B on a variety of downstream evaluation areas covering a diverse range of tasks and domains. In all evaluations, we adhere to the standardized task setup and share the exact settings used. The covered evaluation categories include:</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Commonsense Reasoning (0-shot):</span> SIQA <cite class="ltx_cite ltx_citemacro_citep">(Sap et&nbsp;al., <a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite>, ARC easy and challenge <cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a href="#bib.bib14" title="" class="ltx_ref">2018</a>)</cite>, PIQA <cite class="ltx_cite ltx_citemacro_citep">(Bisk et&nbsp;al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>, Winogrande <cite class="ltx_cite ltx_citemacro_citep">(Sakaguchi et&nbsp;al., <a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite>, and Hellaswag <cite class="ltx_cite ltx_citemacro_citep">(Zellers et&nbsp;al., <a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Popular Aggregated Benchmarks:</span> MMLU (5-shot) <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> and BBH (3-shot) <cite class="ltx_cite ltx_citemacro_citep">(Suzgun et&nbsp;al., <a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Math:</span> GSM8K (8-shot with maj@1) <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et&nbsp;al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite></p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Code:</span> Pass@1 scores on HumanEval (0-shot) <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite>, MBPP (3-shot) <cite class="ltx_cite ltx_citemacro_citep">(Austin et&nbsp;al., <a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>, and MultiPL-E (0-shot) <cite class="ltx_cite ltx_citemacro_citep">(Cassano et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2023a</a>)</cite></p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Multilingual:</span> classification via XCOPA (0 and 4-shot) <cite class="ltx_cite ltx_citemacro_citep">(Ponti et&nbsp;al., <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite>, machine translation with FLORES-101 (8-shot)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Goyal et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>, and generation tasks such as MGSM (8-shot)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite> and TyDiQA (1-shot)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite></p>
</div>
</li>
</ul>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">In our evaluations, we compare against a number of external decoder-only transformer language models and unless otherwise stated we use the numbers published in the reports of the corresponding models. For English and code tasks, we share detailed results for Nemotron-4 15B, LlaMA-2 13B and 34B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib46" title="" class="ltx_ref">2023b</a>)</cite>, Mistral 7B <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>, Baichuan-2 13B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a href="#bib.bib49" title="" class="ltx_ref">2023</a>)</cite>, QWEN 14B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, and Gemma 7B <cite class="ltx_cite ltx_citemacro_citep">(Gemma&nbsp;Team, <a href="#bib.bib17" title="" class="ltx_ref">2024</a>)</cite>. For multilingual benchmarks, we report results against PaLM 62B and 62B-cont <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> as well as models specially trained for multilingual capabilities such as mGPT 13B <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> and XGLM 7.5B <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Commonsense Reasoning</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">We use the LM-Evaluation Harness&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> to evaluate Nemotron-4 15B across all aforementioned tasks. Table&nbsp;<a href="#S3.T3" title="Table 3 ‣ 3.1 Commonsense Reasoning ‣ 3 Results ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> showcases that Nemotron-4 15B achieves the strongest average performance on this diverse set of tasks.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:413.9pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S3.T3.1.1" class="ltx_p"><span id="S3.T3.1.1.1" class="ltx_text">
<span id="S3.T3.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:413.9pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T3.1.1.1.1.1" class="ltx_p"><span id="S3.T3.1.1.1.1.1.1" class="ltx_text">
<span id="S3.T3.1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S3.T3.1.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="S3.T3.1.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></span>
<span id="S3.T3.1.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">Size</span>
<span id="S3.T3.1.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SIQA</span>
<span id="S3.T3.1.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ARC-c</span>
<span id="S3.T3.1.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ARC-e</span>
<span id="S3.T3.1.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PIQA</span>
<span id="S3.T3.1.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Winogrande</span>
<span id="S3.T3.1.1.1.1.1.1.1.2.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Hellaswag</span>
<span id="S3.T3.1.1.1.1.1.1.1.2.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVG</span></span>
</span>
<span class="ltx_tbody">
<span id="S3.T3.1.1.1.1.1.1.1.3.1" class="ltx_tr">
<span id="S3.T3.1.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S3.T3.1.1.1.1.1.1.1.3.1.1.1" class="ltx_text">LLaMA-2</span></span>
<span id="S3.T3.1.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</span>
<span id="S3.T3.1.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">50.3</span>
<span id="S3.T3.1.1.1.1.1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">49.4</span>
<span id="S3.T3.1.1.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">77.3</span>
<span id="S3.T3.1.1.1.1.1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">79.8</span>
<span id="S3.T3.1.1.1.1.1.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">72.8</span>
<span id="S3.T3.1.1.1.1.1.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">80.7</span>
<span id="S3.T3.1.1.1.1.1.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">68.4</span></span>
<span id="S3.T3.1.1.1.1.1.1.1.4.2" class="ltx_tr">
<span id="S3.T3.1.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">34B</span>
<span id="S3.T3.1.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_center">50.9</span>
<span id="S3.T3.1.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_center">54.5</span>
<span id="S3.T3.1.1.1.1.1.1.1.4.2.4" class="ltx_td ltx_align_center">79.4</span>
<span id="S3.T3.1.1.1.1.1.1.1.4.2.5" class="ltx_td ltx_align_center">81.9</span>
<span id="S3.T3.1.1.1.1.1.1.1.4.2.6" class="ltx_td ltx_align_center">76.7</span>
<span id="S3.T3.1.1.1.1.1.1.1.4.2.7" class="ltx_td ltx_align_center">83.3</span>
<span id="S3.T3.1.1.1.1.1.1.1.4.2.8" class="ltx_td ltx_align_center">71.1</span></span>
<span id="S3.T3.1.1.1.1.1.1.1.5.3" class="ltx_tr">
<span id="S3.T3.1.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T3.1.1.1.1.1.1.1.5.3.1.1" class="ltx_text">Baichuan-2</span></span>
<span id="S3.T3.1.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</span>
<span id="S3.T3.1.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S3.T3.1.1.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S3.T3.1.1.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S3.T3.1.1.1.1.1.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_t">78.1</span>
<span id="S3.T3.1.1.1.1.1.1.1.5.3.7" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S3.T3.1.1.1.1.1.1.1.5.3.8" class="ltx_td ltx_align_center ltx_border_t">70.8</span>
<span id="S3.T3.1.1.1.1.1.1.1.5.3.9" class="ltx_td ltx_align_center ltx_border_t">-</span></span>
<span id="S3.T3.1.1.1.1.1.1.1.6.4" class="ltx_tr">
<span id="S3.T3.1.1.1.1.1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">QWEN</span>
<span id="S3.T3.1.1.1.1.1.1.1.6.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">14B</span>
<span id="S3.T3.1.1.1.1.1.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_t">77.9</span>
<span id="S3.T3.1.1.1.1.1.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t">84.4</span>
<span id="S3.T3.1.1.1.1.1.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_t">90.3</span>
<span id="S3.T3.1.1.1.1.1.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_t">79.9</span>
<span id="S3.T3.1.1.1.1.1.1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S3.T3.1.1.1.1.1.1.1.6.4.8" class="ltx_td ltx_align_center ltx_border_t">80.2</span>
<span id="S3.T3.1.1.1.1.1.1.1.6.4.9" class="ltx_td ltx_align_center ltx_border_t">-</span></span>
<span id="S3.T3.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S3.T3.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Mistral</span>
<span id="S3.T3.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</span>
<span id="S3.T3.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">&nbsp;&nbsp;47.0<sup id="S3.T3.1.1.1.1.1.1.1.1.1.1" class="ltx_sup"><span id="S3.T3.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup></span>
<span id="S3.T3.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">55.5</span>
<span id="S3.T3.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">80.0</span>
<span id="S3.T3.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">83.0</span>
<span id="S3.T3.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">75.3</span>
<span id="S3.T3.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t">81.3</span>
<span id="S3.T3.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">70.4</span></span>
<span id="S3.T3.1.1.1.1.1.1.1.7.5" class="ltx_tr">
<span id="S3.T3.1.1.1.1.1.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Gemma</span>
<span id="S3.T3.1.1.1.1.1.1.1.7.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</span>
<span id="S3.T3.1.1.1.1.1.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_t">51.8</span>
<span id="S3.T3.1.1.1.1.1.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_t">53.2</span>
<span id="S3.T3.1.1.1.1.1.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_t">81.5</span>
<span id="S3.T3.1.1.1.1.1.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_t">81.2</span>
<span id="S3.T3.1.1.1.1.1.1.1.7.5.7" class="ltx_td ltx_align_center ltx_border_t">72.3</span>
<span id="S3.T3.1.1.1.1.1.1.1.7.5.8" class="ltx_td ltx_align_center ltx_border_t">81.2</span>
<span id="S3.T3.1.1.1.1.1.1.1.7.5.9" class="ltx_td ltx_align_center ltx_border_t">70.2</span></span>
<span id="S3.T3.1.1.1.1.1.1.1.8.6" class="ltx_tr">
<span id="S3.T3.1.1.1.1.1.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Nemotron-4</span>
<span id="S3.T3.1.1.1.1.1.1.1.8.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t">15B</span>
<span id="S3.T3.1.1.1.1.1.1.1.8.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">60.9</span>
<span id="S3.T3.1.1.1.1.1.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">55.5</span>
<span id="S3.T3.1.1.1.1.1.1.1.8.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">80.9</span>
<span id="S3.T3.1.1.1.1.1.1.1.8.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">82.4</span>
<span id="S3.T3.1.1.1.1.1.1.1.8.6.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">78.0</span>
<span id="S3.T3.1.1.1.1.1.1.1.8.6.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">82.4</span>
<span id="S3.T3.1.1.1.1.1.1.1.8.6.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T3.1.1.1.1.1.1.1.8.6.9.1" class="ltx_text ltx_font_bold">73.4</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.5.2.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S3.T3.3.1" class="ltx_text" style="font-size:90%;"> Results on standard reasoning benchmarks in the zero-shot setting. We report the average across all tasks where possible for a fair comparison. The values marked with <math id="S3.T3.3.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S3.T3.3.1.m1.1b"><mo id="S3.T3.3.1.m1.1.1" xref="S3.T3.3.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.1.m1.1c"><times id="S3.T3.3.1.m1.1.1.cmml" xref="S3.T3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.1.m1.1d">*</annotation></semantics></math> are read from &nbsp;<cite class="ltx_cite ltx_citemacro_citet">Gemma&nbsp;Team (<a href="#bib.bib17" title="" class="ltx_ref">2024</a>)</cite> </span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Popular Aggregated Benchmarks</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">The MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> and Big Bench Hard (BBH)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Suzgun et&nbsp;al., <a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite> benchmarks have been developed as a challenging assessment of language models’ capabilities on a wide range of tasks and domains. As seen from Table&nbsp;<a href="#S3.T4" title="Table 4 ‣ 3.2 Popular Aggregated Benchmarks ‣ 3 Results ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, Nemotron-4 15B achieves the best score on BBH across existing models at its scale by nearly 7%.
Additionally, Nemotron-4 is significantly better than LLaMA-2 70B model on BBH benchmark where LLaMA-2 70B attains a score of 51.2 and Nemotron-4 is 58.7.
Nemotron-4 15B additionally attains a highly competitive MMLU score and its per-category performance on MMLU can be found in Table <a href="#Ax1.T11" title="Table 11 ‣ Supplementary Materials ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.2.1.1" class="ltx_tr">
<th id="S3.T4.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T4.2.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">Size</th>
<th id="S3.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">BBH</th>
<th id="S3.T4.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MMLU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.2.2.1" class="ltx_tr">
<th id="S3.T4.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S3.T4.2.2.1.1.1" class="ltx_text">LLaMA-2</span></th>
<th id="S3.T4.2.2.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</th>
<td id="S3.T4.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">39.4</td>
<td id="S3.T4.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">54.8</td>
</tr>
<tr id="S3.T4.2.3.2" class="ltx_tr">
<th id="S3.T4.2.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">34B</th>
<td id="S3.T4.2.3.2.2" class="ltx_td ltx_align_center">44.1</td>
<td id="S3.T4.2.3.2.3" class="ltx_td ltx_align_center">62.6</td>
</tr>
<tr id="S3.T4.2.4.3" class="ltx_tr">
<th id="S3.T4.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T4.2.4.3.1.1" class="ltx_text">Baichuan-2</span></th>
<th id="S3.T4.2.4.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</th>
<td id="S3.T4.2.4.3.3" class="ltx_td ltx_align_center ltx_border_t">48.8</td>
<td id="S3.T4.2.4.3.4" class="ltx_td ltx_align_center ltx_border_t">59.2</td>
</tr>
<tr id="S3.T4.2.5.4" class="ltx_tr">
<th id="S3.T4.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">QWEN</th>
<th id="S3.T4.2.5.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">14B</th>
<td id="S3.T4.2.5.4.3" class="ltx_td ltx_align_center ltx_border_t">53.4</td>
<td id="S3.T4.2.5.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T4.2.5.4.4.1" class="ltx_text ltx_font_bold">66.3</span></td>
</tr>
<tr id="S3.T4.2.6.5" class="ltx_tr">
<th id="S3.T4.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Mistral</th>
<th id="S3.T4.2.6.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</th>
<td id="S3.T4.2.6.5.3" class="ltx_td ltx_align_center ltx_border_t">39.5</td>
<td id="S3.T4.2.6.5.4" class="ltx_td ltx_align_center ltx_border_t">60.1</td>
</tr>
<tr id="S3.T4.2.7.6" class="ltx_tr">
<th id="S3.T4.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Gemma</th>
<th id="S3.T4.2.7.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</th>
<td id="S3.T4.2.7.6.3" class="ltx_td ltx_align_center ltx_border_t">55.1</td>
<td id="S3.T4.2.7.6.4" class="ltx_td ltx_align_center ltx_border_t">64.3</td>
</tr>
<tr id="S3.T4.2.8.7" class="ltx_tr">
<th id="S3.T4.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Nemotron-4</th>
<th id="S3.T4.2.8.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t">15B</th>
<td id="S3.T4.2.8.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T4.2.8.7.3.1" class="ltx_text ltx_font_bold">58.7</span></td>
<td id="S3.T4.2.8.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">64.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S3.T4.4.2" class="ltx_text" style="font-size:90%;"> Nemotron-4 15B attains highly competitive performance on popular aggregate benchmarks. The BBH result for Mistral is read from the figure in&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Math and Code</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Recently, large language models have been shown to be effective at both mathematical reasoning and a variety of coding tasks <cite class="ltx_cite ltx_citemacro_citep">(Allal et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2023</a>; Chowdhery et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>; Touvron et&nbsp;al., <a href="#bib.bib45" title="" class="ltx_ref">2023a</a>)</cite>. Table&nbsp;<a href="#S3.T5" title="Table 5 ‣ 3.3 Math and Code ‣ 3 Results ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> highlights the performance of Nemotron-4 15B on such tasks. Specifically, on mathematical reasoning we find that Nemotron-4 15B achieves strong performance as it attains a similar score to Gemma 7B, but lags behind models such as Baichuan-2 and QWEN. On code tasks, we see that Nemotron-4 performs on par with QWEN 14B while remaining slightly behind Gemma 7B. Across both types of tasks, Nemotron-4 15B is able to outperform Mistral 7B and LlaMA-2 13B/34B.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<div id="S3.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:249.7pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S3.T5.2.2" class="ltx_p"><span id="S3.T5.2.2.2" class="ltx_text">
<span id="S3.T5.2.2.2.2" class="ltx_inline-block ltx_transformed_outer" style="width:249.7pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T5.2.2.2.2.2" class="ltx_p"><span id="S3.T5.2.2.2.2.2.2" class="ltx_text">
<span id="S3.T5.2.2.2.2.2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S3.T5.2.2.2.2.2.2.2.3.1" class="ltx_tr">
<span id="S3.T5.2.2.2.2.2.2.2.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></span>
<span id="S3.T5.2.2.2.2.2.2.2.3.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">Size</span>
<span id="S3.T5.2.2.2.2.2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">GSM8K</span>
<span id="S3.T5.2.2.2.2.2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">HumanEval</span>
<span id="S3.T5.2.2.2.2.2.2.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MBPP</span></span>
</span>
<span class="ltx_tbody">
<span id="S3.T5.2.2.2.2.2.2.2.4.1" class="ltx_tr">
<span id="S3.T5.2.2.2.2.2.2.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S3.T5.2.2.2.2.2.2.2.4.1.1.1" class="ltx_text">LlaMA-2</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.4.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</span>
<span id="S3.T5.2.2.2.2.2.2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">28.7</span>
<span id="S3.T5.2.2.2.2.2.2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">18.3</span>
<span id="S3.T5.2.2.2.2.2.2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t">30.6</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.5.2" class="ltx_tr">
<span id="S3.T5.2.2.2.2.2.2.2.5.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">34B</span>
<span id="S3.T5.2.2.2.2.2.2.2.5.2.2" class="ltx_td ltx_align_center">42.2</span>
<span id="S3.T5.2.2.2.2.2.2.2.5.2.3" class="ltx_td ltx_align_center">22.6</span>
<span id="S3.T5.2.2.2.2.2.2.2.5.2.4" class="ltx_td ltx_align_center">33.0</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.6.3" class="ltx_tr">
<span id="S3.T5.2.2.2.2.2.2.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.6.3.1.1" class="ltx_text">Baichuan-2</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.6.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">13B</span>
<span id="S3.T5.2.2.2.2.2.2.2.6.3.3" class="ltx_td ltx_align_center ltx_border_t">52.8</span>
<span id="S3.T5.2.2.2.2.2.2.2.6.3.4" class="ltx_td ltx_align_center ltx_border_t">17.1</span>
<span id="S3.T5.2.2.2.2.2.2.2.6.3.5" class="ltx_td ltx_align_center ltx_border_t">30.2</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.7.4" class="ltx_tr">
<span id="S3.T5.2.2.2.2.2.2.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.7.4.1.1" class="ltx_text">QWEN</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.7.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">14B</span>
<span id="S3.T5.2.2.2.2.2.2.2.7.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.7.4.3.1" class="ltx_text ltx_font_bold">60.1</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.7.4.4" class="ltx_td ltx_align_center ltx_border_t">32.2</span>
<span id="S3.T5.2.2.2.2.2.2.2.7.4.5" class="ltx_td ltx_align_center ltx_border_t">40.8</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.2" class="ltx_tr">
<span id="S3.T5.2.2.2.2.2.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.2.3.1" class="ltx_text">Mistral</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</span>
<span id="S3.T5.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">35.4<sup id="S3.T5.1.1.1.1.1.1.1.1.1.1" class="ltx_sup"><span id="S3.T5.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup></span>
<span id="S3.T5.2.2.2.2.2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t">30.5</span>
<span id="S3.T5.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">&nbsp;&nbsp;40.2<sup id="S3.T5.2.2.2.2.2.2.2.2.2.1" class="ltx_sup"><span id="S3.T5.2.2.2.2.2.2.2.2.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup></span></span>
<span id="S3.T5.2.2.2.2.2.2.2.8.5" class="ltx_tr">
<span id="S3.T5.2.2.2.2.2.2.2.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.8.5.1.1" class="ltx_text">Gemma</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.8.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7B</span>
<span id="S3.T5.2.2.2.2.2.2.2.8.5.3" class="ltx_td ltx_align_center ltx_border_t">46.4</span>
<span id="S3.T5.2.2.2.2.2.2.2.8.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.8.5.4.1" class="ltx_text ltx_font_bold">32.3</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.8.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.8.5.5.1" class="ltx_text ltx_font_bold">44.4</span></span></span>
<span id="S3.T5.2.2.2.2.2.2.2.9.6" class="ltx_tr">
<span id="S3.T5.2.2.2.2.2.2.2.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S3.T5.2.2.2.2.2.2.2.9.6.1.1" class="ltx_text">Nemotron-4</span></span>
<span id="S3.T5.2.2.2.2.2.2.2.9.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t">15B</span>
<span id="S3.T5.2.2.2.2.2.2.2.9.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">46.0</span>
<span id="S3.T5.2.2.2.2.2.2.2.9.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">31.6</span>
<span id="S3.T5.2.2.2.2.2.2.2.9.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">40.6</span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T5.4.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S3.T5.5.2" class="ltx_text" style="font-size:90%;"> Comparative results on math and code benchmarks. As Mistral 7B reports MBPP performance on a different eval split and uses a different evaluation setting for GSM8K , we use the corresponding numbers reported in&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gemma&nbsp;Team, <a href="#bib.bib17" title="" class="ltx_ref">2024</a>)</cite></span></figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">Nearly all similarly-sized open models determine their code abilities solely based on performance on Python related tasks – disregarding an evaluation of their capabilities on other programming languages. In Table&nbsp;<a href="#S3.T6" title="Table 6 ‣ 3.3 Math and Code ‣ 3 Results ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we demonstrate results of Nemotron-4 15B on the Multiple-E&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cassano et&nbsp;al., <a href="#bib.bib9" title="" class="ltx_ref">2023b</a>)</cite> benchmark across 11 diverse programming languages and compare it against Mistral 7B and Starcoder&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>, a 15B parameter model that has been specially trained for code. We find that Nemotron-4 15B attains strong coding performance across a wide assortment of programming languages and outperforms both Starcoder and Mistral 7B on average. We especially highlight the superior performance of Nemotron-4 15B on low-resource programming languages such as Scala, Julia, and R.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<div id="S3.T6.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:553.3pt;height:73pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<div id="S3.T6.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:57.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-59.9pt,7.8pt) scale(0.78367,0.78367) ;">
<p id="S3.T6.2.1.1" class="ltx_p"><span id="S3.T6.2.1.1.1" class="ltx_text">
<span id="S3.T6.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:553.3pt;height:73pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T6.2.1.1.1.1.1" class="ltx_p"><span id="S3.T6.2.1.1.1.1.1.1" class="ltx_text">
<span id="S3.T6.2.1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Size</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">JavaScript</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Julia</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Java</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Lua</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">C++</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">C-Sharp</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PHP</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Shell</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">TypeScript</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">R</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Scala</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.1.1.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVG</span></span>
</span>
<span class="ltx_tbody">
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Starcoder</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">15B</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">30.8</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">23.0</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">30.2</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">23.9</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">31.6</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">21.0</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">26.1</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">10.5</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t">32.3</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.12" class="ltx_td ltx_align_center ltx_border_t">15.5</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.13" class="ltx_td ltx_align_center ltx_border_t">27.6</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.2.1.14" class="ltx_td ltx_align_center ltx_border_t">24.2</span></span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2" class="ltx_tr">
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Mistral</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_center">7B</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_center">34.2</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_center">22.0</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_center">26.0</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.6" class="ltx_td ltx_align_center">25.3</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.7" class="ltx_td ltx_align_center">29.1</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.8" class="ltx_td ltx_align_center">22.8</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.9" class="ltx_td ltx_align_center">27.9</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.10" class="ltx_td ltx_align_center">8.9</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.11" class="ltx_td ltx_align_center">28.5</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.12" class="ltx_td ltx_align_center">11.8</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.13" class="ltx_td ltx_align_center">22.2</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.3.2.14" class="ltx_td ltx_align_center">23.6</span></span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3" class="ltx_tr">
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Nemotron-4</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">15B</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">28.6</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">24.8</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">24.8</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb">24.2</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_bb">35.4</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_bb">21.1</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.9" class="ltx_td ltx_align_center ltx_border_bb">27.3</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.10" class="ltx_td ltx_align_center ltx_border_bb">8.9</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.11" class="ltx_td ltx_align_center ltx_border_bb">32.9</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.12" class="ltx_td ltx_align_center ltx_border_bb">18.6</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.13" class="ltx_td ltx_align_center ltx_border_bb">27.3</span>
<span id="S3.T6.2.1.1.1.1.1.1.1.4.3.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T6.2.1.1.1.1.1.1.1.4.3.14.1" class="ltx_text ltx_font_bold">24.5</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T6.3.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S3.T6.4.2" class="ltx_text" style="font-size:90%;"> Nemotron-4 15B attains high competency in coding performance across a broad range of programming languages. Results for Mistral are from our runs of Mistral in the same setting as Nemotron-4. </span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multilingual</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">We demonstrate the outstanding multilingual ability of Nemotron-4 15B using four widely-studied benchmarks in previous works that cover a diverse range of high to low resource natural languages. For classification we use accuracy as the metric; for generative tasks, we use exact match; and for machine translation, we evaluate using the <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_typewriter">sacreBLEU</span> <cite class="ltx_cite ltx_citemacro_citep">(Post, <a href="#bib.bib32" title="" class="ltx_ref">2018</a>)</cite> implementation of <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_typewriter">BLEU</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Papineni et&nbsp;al., <a href="#bib.bib30" title="" class="ltx_ref">2002</a>)</cite>, using <span id="S3.SS4.p1.1.3" class="ltx_text ltx_font_typewriter">spm-flores-101</span> tokenization to obtain spBLEU scores.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">1. Classification:</span> Cross-lingual Choice of Plausible Alternatives (XCOPA) <cite class="ltx_cite ltx_citemacro_citep">(Ponti et&nbsp;al., <a href="#bib.bib31" title="" class="ltx_ref">2020</a>)</cite> tests causal commonsense reasoning in 11 languages</p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.1" class="ltx_p">We compare Nemotron-4 15B to existing multilingual language models: XGLM <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite> , mGPT <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>, and BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al., <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>.
XGLM and mGPT are models specially trained to have improved multilingual ability by up-sampling the presence of non-English languages in the training data.
In contrast, BLOOM, like Nemotron-4, is a general purpose language model that was trained on a combination of English, multilingual, and code data.
In Table <a href="#S3.T7" title="Table 7 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we clearly see that Nemotron-4 achieves the best performance amongst all models – realizing almost a 12% improvement in the four-shot setting.</p>
</div>
<figure id="S3.T7" class="ltx_table">
<div id="S3.T7.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:536.2pt;height:145pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<div id="S3.T7.5.5" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:117.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.3pt,13.8pt) scale(0.80868,0.80868) ;">
<p id="S3.T7.5.5.5" class="ltx_p"><span id="S3.T7.5.5.5.5" class="ltx_text">
<span id="S3.T7.5.5.5.5.5" class="ltx_inline-block ltx_transformed_outer" style="width:536.2pt;height:145pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T7.5.5.5.5.5.5" class="ltx_p"><span id="S3.T7.5.5.5.5.5.5.5" class="ltx_text">
<span id="S3.T7.5.5.5.5.5.5.5.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1" class="ltx_tr">
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Mode</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">Size</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ET</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">HT</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ID</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">IT</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">QU</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SW</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">TA</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">TH</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">TR</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VI</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ZH</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.6.1.15" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVG</span></span>
</span>
<span class="ltx_tbody">
<span id="S3.T7.5.5.5.5.5.5.5.5.5" class="ltx_tr">
<span id="S3.T7.5.5.5.5.5.5.5.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_4"><span id="S3.T7.5.5.5.5.5.5.5.5.5.6.1" class="ltx_text">Zero-Shot</span></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.5.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">BLOOM</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.5.8" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">176B</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.5.9" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.5.10" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S3.T7.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="57.5^{*}" display="inline"><semantics id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1a"><msup id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">57.5</mn><mo id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><cn type="float" id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">57.5</cn><times id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.1.1.1.1.1.1.1.1.1.1.m1.1c">57.5^{*}</annotation></semantics></math></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.5.11" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.5.12" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S3.T7.2.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="59.5^{*}" display="inline"><semantics id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1a"><msup id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml"><mn id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.2" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.2.cmml">59.5</mn><mo id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.3" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1b"><apply id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.1.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1">superscript</csymbol><cn type="float" id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.2.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.2">59.5</cn><times id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.3.cmml" xref="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.2.2.2.2.2.2.2.2.2.2.m1.1c">59.5^{*}</annotation></semantics></math></span>
<span id="S3.T7.3.3.3.3.3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1" class="ltx_Math" alttext="54.7^{*}" display="inline"><semantics id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1a"><msup id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml"><mn id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.2" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.2.cmml">54.7</mn><mo id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.3" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1b"><apply id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.1.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1">superscript</csymbol><cn type="float" id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.2.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.2">54.7</cn><times id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.3.cmml" xref="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.3.3.3.3.3.3.3.3.3.3.m1.1c">54.7^{*}</annotation></semantics></math></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.5.13" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.5.14" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S3.T7.4.4.4.4.4.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1" class="ltx_Math" alttext="58.2^{*}" display="inline"><semantics id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1a"><msup id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.cmml"><mn id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.2" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.2.cmml">58.2</mn><mo id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.3" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1b"><apply id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.1.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1">superscript</csymbol><cn type="float" id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.2.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.2">58.2</cn><times id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.3.cmml" xref="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.4.4.4.4.4.4.4.4.4.4.m1.1c">58.2^{*}</annotation></semantics></math></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1" class="ltx_Math" alttext="57.7^{*}" display="inline"><semantics id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1a"><msup id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.cmml"><mn id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.2" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.2.cmml">57.7</mn><mo id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.3" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1b"><apply id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1"><csymbol cd="ambiguous" id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.1.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1">superscript</csymbol><cn type="float" id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.2.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.2">57.7</cn><times id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.3.cmml" xref="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.5.5.5.5.5.5.5.5.5.5.m1.1c">57.7^{*}</annotation></semantics></math></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.5.15" class="ltx_td ltx_align_center ltx_border_t">-</span></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1" class="ltx_tr">
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">XGLM</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">7.5B</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.3" class="ltx_td ltx_align_center">57.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.4" class="ltx_td ltx_align_center">57.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.5" class="ltx_td ltx_align_center">59.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.6" class="ltx_td ltx_align_center">49.2</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.7" class="ltx_td ltx_align_center">52.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.8" class="ltx_td ltx_align_center">55.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.9" class="ltx_td ltx_align_center">55.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.10" class="ltx_td ltx_align_center">57.8</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.11" class="ltx_td ltx_align_center">55.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.12" class="ltx_td ltx_align_center">59.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.13" class="ltx_td ltx_align_center">53.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.7.1.14" class="ltx_td ltx_align_center">55.6</span></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2" class="ltx_tr">
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">mGPT</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">13B</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.3" class="ltx_td ltx_align_center">49.8</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.4" class="ltx_td ltx_align_center">50.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.5" class="ltx_td ltx_align_center">63.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.6" class="ltx_td ltx_align_center">61.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.7" class="ltx_td ltx_align_center">50.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.8" class="ltx_td ltx_align_center">57.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.9" class="ltx_td ltx_align_center">57.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.10" class="ltx_td ltx_align_center">54.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.11" class="ltx_td ltx_align_center">58.2</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.12" class="ltx_td ltx_align_center">60.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.13" class="ltx_td ltx_align_center">54.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.8.2.14" class="ltx_td ltx_align_center">56.1</span></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3" class="ltx_tr">
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Nemotron-4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">15B</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.3" class="ltx_td ltx_align_center">62.8</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.4" class="ltx_td ltx_align_center">47.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.5" class="ltx_td ltx_align_center">66.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.6" class="ltx_td ltx_align_center">67.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.7" class="ltx_td ltx_align_center">53.8</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.8" class="ltx_td ltx_align_center">50.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.9" class="ltx_td ltx_align_center">62.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.10" class="ltx_td ltx_align_center">59.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.11" class="ltx_td ltx_align_center">57.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.12" class="ltx_td ltx_align_center">65.2</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.13" class="ltx_td ltx_align_center">62.2</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.9.3.14" class="ltx_td ltx_align_center"><span id="S3.T7.5.5.5.5.5.5.5.5.9.3.14.1" class="ltx_text ltx_font_bold">59.5</span></span></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4" class="ltx_tr">
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3"><span id="S3.T7.5.5.5.5.5.5.5.5.10.4.1.1" class="ltx_text">4-Shot</span></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">XGLM</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.3" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">7.5B</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.4" class="ltx_td ltx_align_center ltx_border_t">64.7</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.5" class="ltx_td ltx_align_center ltx_border_t">60.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.6" class="ltx_td ltx_align_center ltx_border_t">67.3</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.7" class="ltx_td ltx_align_center ltx_border_t">64.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.8" class="ltx_td ltx_align_center ltx_border_t">50.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.9" class="ltx_td ltx_align_center ltx_border_t">61.8</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.10" class="ltx_td ltx_align_center ltx_border_t">56.7</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.11" class="ltx_td ltx_align_center ltx_border_t">61.5</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.12" class="ltx_td ltx_align_center ltx_border_t">60.1</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.13" class="ltx_td ltx_align_center ltx_border_t">68.5</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.14" class="ltx_td ltx_align_center ltx_border_t">59.9</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.10.4.15" class="ltx_td ltx_align_center ltx_border_t">61.4</span></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5" class="ltx_tr">
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">mGPT</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">13B</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.3" class="ltx_td ltx_align_center">48.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.4" class="ltx_td ltx_align_center">48.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.5" class="ltx_td ltx_align_center">62.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.6" class="ltx_td ltx_align_center">60.8</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.7" class="ltx_td ltx_align_center">50.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.8" class="ltx_td ltx_align_center">56.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.9" class="ltx_td ltx_align_center">55.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.10" class="ltx_td ltx_align_center">54.8</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.11" class="ltx_td ltx_align_center">57.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.12" class="ltx_td ltx_align_center">61.8</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.13" class="ltx_td ltx_align_center">58.4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.11.5.14" class="ltx_td ltx_align_center">56.0</span></span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6" class="ltx_tr">
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Nemotron-4</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb">15B</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.3" class="ltx_td ltx_align_center ltx_border_bb">72.9</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.4" class="ltx_td ltx_align_center ltx_border_bb">52.8</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.5" class="ltx_td ltx_align_center ltx_border_bb">79.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.6" class="ltx_td ltx_align_center ltx_border_bb">79.2</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.7" class="ltx_td ltx_align_center ltx_border_bb">50.2</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.8" class="ltx_td ltx_align_center ltx_border_bb">52.2</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.9" class="ltx_td ltx_align_center ltx_border_bb">72.8</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.10" class="ltx_td ltx_align_center ltx_border_bb">66.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.11" class="ltx_td ltx_align_center ltx_border_bb">77.2</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.12" class="ltx_td ltx_align_center ltx_border_bb">78.6</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.13" class="ltx_td ltx_align_center ltx_border_bb">76.0</span>
<span id="S3.T7.5.5.5.5.5.5.5.5.12.6.14" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T7.5.5.5.5.5.5.5.5.12.6.14.1" class="ltx_text ltx_font_bold">68.9</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T7.9.2.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S3.T7.7.1" class="ltx_text" style="font-size:90%;"> Comparison of Nemotron-4 15B against existing large language models on XCOPA under the zero- and four-shot setting. Our reported results for XGLM are from the runs of the model in <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> given that we use the same prompt template used by mGPT. The values marked with <math id="S3.T7.7.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S3.T7.7.1.m1.1b"><mo id="S3.T7.7.1.m1.1.1" xref="S3.T7.7.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.T7.7.1.m1.1c"><times id="S3.T7.7.1.m1.1.1.cmml" xref="S3.T7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T7.7.1.m1.1d">*</annotation></semantics></math> are read from figures in <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al., <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>. </span></figcaption>
</figure>
<div id="S3.SS4.p4" class="ltx_para ltx_noindent">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_bold">2. Generation:</span> We consider two generative tasks: TyDiQA-GoldP&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite> and Multilingual Grade School Math (MGSM)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite>. TyDiQA-GoldP is a question answering task while MGSM evaluates the arithmetic reasoning ability of language models in 10 languages.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para ltx_noindent">
<p id="S3.SS4.p5.1" class="ltx_p">In comparing the performance of Nemotron-4 15B on TyDiQA-GoldP to a range of models, Table <a href="#S3.T8" title="Table 8 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows that Nemotron-4 15B achieves the best performance. Impressively, Nemotron-4 15B is able to significantly improve upon the next best model, PaLM 62B-cont.</p>
</div>
<figure id="S3.T8" class="ltx_table">
<div id="S3.T8.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.0pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S3.T8.2.1" class="ltx_p"><span id="S3.T8.2.1.1" class="ltx_text">
<span id="S3.T8.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:390.0pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T8.2.1.1.1.1" class="ltx_p"><span id="S3.T8.2.1.1.1.1.1" class="ltx_text">
<span id="S3.T8.2.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S3.T8.2.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S3.T8.2.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</span>
<span id="S3.T8.2.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Size</span>
<span id="S3.T8.2.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">AR</span>
<span id="S3.T8.2.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">BN</span>
<span id="S3.T8.2.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">FI</span>
<span id="S3.T8.2.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ID</span>
<span id="S3.T8.2.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">KO</span>
<span id="S3.T8.2.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">RU</span>
<span id="S3.T8.2.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">SW</span>
<span id="S3.T8.2.1.1.1.1.1.1.1.1.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">TE</span>
<span id="S3.T8.2.1.1.1.1.1.1.1.1.11" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">AVG</span></span>
</span>
<span class="ltx_tbody">
<span id="S3.T8.2.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="S3.T8.2.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S3.T8.2.1.1.1.1.1.1.2.1.1.1" class="ltx_text">PaLM</span></span>
<span id="S3.T8.2.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">62B</span>
<span id="S3.T8.2.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">31.2</span>
<span id="S3.T8.2.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t">42.5</span>
<span id="S3.T8.2.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t">41.7</span>
<span id="S3.T8.2.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_right ltx_border_t">41.6</span>
<span id="S3.T8.2.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_right ltx_border_t">49.3</span>
<span id="S3.T8.2.1.1.1.1.1.1.2.1.8" class="ltx_td ltx_align_right ltx_border_t">29.2</span>
<span id="S3.T8.2.1.1.1.1.1.1.2.1.9" class="ltx_td ltx_align_right ltx_border_t">58.1</span>
<span id="S3.T8.2.1.1.1.1.1.1.2.1.10" class="ltx_td ltx_align_right ltx_border_t">30.6</span>
<span id="S3.T8.2.1.1.1.1.1.1.2.1.11" class="ltx_td ltx_align_right ltx_border_t">40.5</span></span>
<span id="S3.T8.2.1.1.1.1.1.1.3.2" class="ltx_tr">
<span id="S3.T8.2.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_right">62B-cont</span>
<span id="S3.T8.2.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_right">39.4</span>
<span id="S3.T8.2.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_right">48.7</span>
<span id="S3.T8.2.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_right">44.0</span>
<span id="S3.T8.2.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_right">49.2</span>
<span id="S3.T8.2.1.1.1.1.1.1.3.2.6" class="ltx_td ltx_align_right">52.5</span>
<span id="S3.T8.2.1.1.1.1.1.1.3.2.7" class="ltx_td ltx_align_right">35.6</span>
<span id="S3.T8.2.1.1.1.1.1.1.3.2.8" class="ltx_td ltx_align_right">60.9</span>
<span id="S3.T8.2.1.1.1.1.1.1.3.2.9" class="ltx_td ltx_align_right">35.3</span>
<span id="S3.T8.2.1.1.1.1.1.1.3.2.10" class="ltx_td ltx_align_right">45.7</span></span>
<span id="S3.T8.2.1.1.1.1.1.1.4.3" class="ltx_tr">
<span id="S3.T8.2.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">LLaMA-2</span>
<span id="S3.T8.2.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_right">13B</span>
<span id="S3.T8.2.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.4.3.6" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.4.3.7" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.4.3.8" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.4.3.9" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.4.3.10" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.4.3.11" class="ltx_td ltx_align_right">33.2</span></span>
<span id="S3.T8.2.1.1.1.1.1.1.5.4" class="ltx_tr">
<span id="S3.T8.2.1.1.1.1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Baichuan-2</span>
<span id="S3.T8.2.1.1.1.1.1.1.5.4.2" class="ltx_td ltx_align_right">13B</span>
<span id="S3.T8.2.1.1.1.1.1.1.5.4.3" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.5.4.4" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.5.4.5" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.5.4.6" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.5.4.7" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.5.4.8" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.5.4.9" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.5.4.10" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.5.4.11" class="ltx_td ltx_align_right">30.8</span></span>
<span id="S3.T8.2.1.1.1.1.1.1.6.5" class="ltx_tr">
<span id="S3.T8.2.1.1.1.1.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">QWEN</span>
<span id="S3.T8.2.1.1.1.1.1.1.6.5.2" class="ltx_td ltx_align_right">14B</span>
<span id="S3.T8.2.1.1.1.1.1.1.6.5.3" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.6.5.4" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.6.5.5" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.6.5.6" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.6.5.7" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.6.5.8" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.6.5.9" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.6.5.10" class="ltx_td ltx_align_right">-</span>
<span id="S3.T8.2.1.1.1.1.1.1.6.5.11" class="ltx_td ltx_align_right">39.8</span></span>
<span id="S3.T8.2.1.1.1.1.1.1.7.6" class="ltx_tr">
<span id="S3.T8.2.1.1.1.1.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Nemotron-4</span>
<span id="S3.T8.2.1.1.1.1.1.1.7.6.2" class="ltx_td ltx_align_right ltx_border_bb">15B</span>
<span id="S3.T8.2.1.1.1.1.1.1.7.6.3" class="ltx_td ltx_align_right ltx_border_bb">39.1</span>
<span id="S3.T8.2.1.1.1.1.1.1.7.6.4" class="ltx_td ltx_align_right ltx_border_bb">55.8</span>
<span id="S3.T8.2.1.1.1.1.1.1.7.6.5" class="ltx_td ltx_align_right ltx_border_bb">52.2</span>
<span id="S3.T8.2.1.1.1.1.1.1.7.6.6" class="ltx_td ltx_align_right ltx_border_bb">54.5</span>
<span id="S3.T8.2.1.1.1.1.1.1.7.6.7" class="ltx_td ltx_align_right ltx_border_bb">55.1</span>
<span id="S3.T8.2.1.1.1.1.1.1.7.6.8" class="ltx_td ltx_align_right ltx_border_bb">37.8</span>
<span id="S3.T8.2.1.1.1.1.1.1.7.6.9" class="ltx_td ltx_align_right ltx_border_bb">54.5</span>
<span id="S3.T8.2.1.1.1.1.1.1.7.6.10" class="ltx_td ltx_align_right ltx_border_bb">55.0</span>
<span id="S3.T8.2.1.1.1.1.1.1.7.6.11" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T8.2.1.1.1.1.1.1.7.6.11.1" class="ltx_text ltx_font_bold">50.5</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T8.3.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="S3.T8.4.2" class="ltx_text" style="font-size:90%;"> Comparative results in the one-shot setting on TyDiQA-GoldP. Results for LLaMA-2 13B, Baichuan-2 13B and QWEN 14B are taken from <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib10" title="" class="ltx_ref">2024</a>)</cite>. </span></figcaption>
</figure>
<div id="S3.SS4.p6" class="ltx_para ltx_noindent">
<p id="S3.SS4.p6.1" class="ltx_p">Further demonstrating the impressive multilingual ability of Nemotron-4 15B, Table <a href="#S3.T9" title="Table 9 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows the performance on MGSM.
We report using the English chain-of-thought setting introduced in <cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite> where all chain of thought explanations are presented to the model in English rather than in the language of the task. On this challenging task which assesses the intersection of mathematical and multilingual ability, Nemotron-4 15B achieves the best performance amongst compared models and improves upon the closest score by nearly 30%.</p>
</div>
<figure id="S3.T9" class="ltx_table">
<div id="S3.T9.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:516.2pt;height:91pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<div id="S3.T9.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:76.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-41.3pt,7.2pt) scale(0.84001,0.84001) ;">
<p id="S3.T9.2.1.1" class="ltx_p"><span id="S3.T9.2.1.1.1" class="ltx_text">
<span id="S3.T9.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:516.2pt;height:91pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T9.2.1.1.1.1.1" class="ltx_p"><span id="S3.T9.2.1.1.1.1.1.1" class="ltx_text">
<span id="S3.T9.2.1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Mode</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Size</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">DE</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">FR</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ES</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">RU</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">JA</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">TH</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.11" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">TE</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.12" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">BN</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.13" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">SW</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.1.1.14" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">AVG</span></span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2" class="ltx_tr">
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S3.T9.2.1.1.1.1.1.1.1.2.2.1.1" class="ltx_text">Native-COT</span></span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S3.T9.2.1.1.1.1.1.1.1.2.2.2.1" class="ltx_text">PaLM</span></span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">62B</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">24.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">24.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">26.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">22.8</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">24.8</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.9" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">14.8</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">18.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.11" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">11.6</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.12" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">13.6</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.13" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">9.6</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.2.2.14" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">18.9</span></span>
</span>
<span class="ltx_tbody">
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1" class="ltx_tr">
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3"><span id="S3.T9.2.1.1.1.1.1.1.1.3.1.1.1" class="ltx_text">English-COT</span></span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T9.2.1.1.1.1.1.1.1.3.1.2.1" class="ltx_text">PALM</span></span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_right ltx_border_t">62B-cont</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.4" class="ltx_td ltx_align_right ltx_border_t">44.8</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t">39.2</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.6" class="ltx_td ltx_align_right ltx_border_t">44.4</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.7" class="ltx_td ltx_align_right ltx_border_t">36.8</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.8" class="ltx_td ltx_align_right ltx_border_t">33.6</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.9" class="ltx_td ltx_align_right ltx_border_t">24.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.10" class="ltx_td ltx_align_right ltx_border_t">28.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.11" class="ltx_td ltx_align_right ltx_border_t">19.6</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.12" class="ltx_td ltx_align_right ltx_border_t">28.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.13" class="ltx_td ltx_align_right ltx_border_t">21.2</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.3.1.14" class="ltx_td ltx_align_right ltx_border_t">32.0</span></span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2" class="ltx_tr">
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T9.2.1.1.1.1.1.1.1.4.2.1.1" class="ltx_text">Mistral</span></span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_right">7B</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_right">33.2</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.4" class="ltx_td ltx_align_right">35.2</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.5" class="ltx_td ltx_align_right">35.6</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.6" class="ltx_td ltx_align_right">35.2</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.7" class="ltx_td ltx_align_right">33.2</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.8" class="ltx_td ltx_align_right">18.8</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.9" class="ltx_td ltx_align_right">10.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.10" class="ltx_td ltx_align_right">0.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.11" class="ltx_td ltx_align_right">8.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.12" class="ltx_td ltx_align_right">9.2</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.4.2.13" class="ltx_td ltx_align_right">21.8</span></span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3" class="ltx_tr">
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S3.T9.2.1.1.1.1.1.1.1.5.3.1.1" class="ltx_text">Nemotron-4</span></span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_right ltx_border_bb">15B</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_right ltx_border_bb">46.8</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_right ltx_border_bb">46.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_right ltx_border_bb">50.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.6" class="ltx_td ltx_align_right ltx_border_bb">45.6</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.7" class="ltx_td ltx_align_right ltx_border_bb">40.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.8" class="ltx_td ltx_align_right ltx_border_bb">40.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.9" class="ltx_td ltx_align_right ltx_border_bb">43.6</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.10" class="ltx_td ltx_align_right ltx_border_bb">41.6</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.11" class="ltx_td ltx_align_right ltx_border_bb">43.6</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.12" class="ltx_td ltx_align_right ltx_border_bb">16.0</span>
<span id="S3.T9.2.1.1.1.1.1.1.1.5.3.13" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T9.2.1.1.1.1.1.1.1.5.3.13.1" class="ltx_text ltx_font_bold">41.3</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T9.3.1.1" class="ltx_text" style="font-size:90%;">Table 9</span>: </span><span id="S3.T9.4.2" class="ltx_text" style="font-size:90%;"> Eight-shot accuracy results on MGSM. Results for Mistral are from our runs of Mistral in the same setting as Nemotron-4. </span></figcaption>
</figure>
<div id="S3.SS4.p7" class="ltx_para ltx_noindent">
<p id="S3.SS4.p7.1" class="ltx_p"><span id="S3.SS4.p7.1.1" class="ltx_text ltx_font_bold">3. Machine Translation:</span> We additionally evaluate the translation ability of our models through the FLORES-101 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> benchmark. The ability to translate between languages is a good test of the model’s ability to relate and understand semantic relationships between languages.</p>
</div>
<div id="S3.SS4.p8" class="ltx_para ltx_noindent">
<p id="S3.SS4.p8.1" class="ltx_p">As seen in Table&nbsp;<a href="#S3.T10" title="Table 10 ‣ 3.4 Multilingual ‣ 3 Results ‣ Nemotron-4 15B Technical Report" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, Nemotron-4 15B heftily outperforms both LLaMA-2 13B and Baichuan-2 13B – improving upon their performance by 90.2% and 44.1% respectively. Nemotron-4 15B does not solely perform well on translating from Chinese into English but is able to attain impressive results on the direct translation of Chinese into other languages. This ability highlights the strong understanding that Nemotron-4 15B has across a broad spectrum of natural languages.</p>
</div>
<figure id="S3.T10" class="ltx_table">
<div id="S3.T10.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:429.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S3.T10.2.1" class="ltx_p"><span id="S3.T10.2.1.1" class="ltx_text">
<span id="S3.T10.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:429.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T10.2.1.1.1.1" class="ltx_p"><span id="S3.T10.2.1.1.1.1.1" class="ltx_text">
<span id="S3.T10.2.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S3.T10.2.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S3.T10.2.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></span>
<span id="S3.T10.2.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">Size</span>
<span id="S3.T10.2.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-EN</span>
<span id="S3.T10.2.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-FR</span>
<span id="S3.T10.2.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-ES</span>
<span id="S3.T10.2.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-AR</span>
<span id="S3.T10.2.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-RU</span>
<span id="S3.T10.2.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-JA</span>
<span id="S3.T10.2.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">ZH-DE</span>
<span id="S3.T10.2.1.1.1.1.1.1.1.1.10" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">AVG</span></span>
</span>
<span class="ltx_tbody">
<span id="S3.T10.2.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="S3.T10.2.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T10.2.1.1.1.1.1.1.2.1.1.1" class="ltx_text">LLaMA-2</span></span>
<span id="S3.T10.2.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">13B</span>
<span id="S3.T10.2.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">25.4</span>
<span id="S3.T10.2.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t">19.2</span>
<span id="S3.T10.2.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t">17.5</span>
<span id="S3.T10.2.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_right ltx_border_t">1.4</span>
<span id="S3.T10.2.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_right ltx_border_t">10.3</span>
<span id="S3.T10.2.1.1.1.1.1.1.2.1.8" class="ltx_td ltx_align_right ltx_border_t">0.1</span>
<span id="S3.T10.2.1.1.1.1.1.1.2.1.9" class="ltx_td ltx_align_right ltx_border_t">11.1</span>
<span id="S3.T10.2.1.1.1.1.1.1.2.1.10" class="ltx_td ltx_align_right ltx_border_t">12.2</span></span>
<span id="S3.T10.2.1.1.1.1.1.1.3.2" class="ltx_tr">
<span id="S3.T10.2.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S3.T10.2.1.1.1.1.1.1.3.2.1.1" class="ltx_text">Baichuan-2</span></span>
<span id="S3.T10.2.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">13B</span>
<span id="S3.T10.2.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_right ltx_border_t">30.6</span>
<span id="S3.T10.2.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_right ltx_border_t">22.1</span>
<span id="S3.T10.2.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_right ltx_border_t">17.3</span>
<span id="S3.T10.2.1.1.1.1.1.1.3.2.6" class="ltx_td ltx_align_right ltx_border_t">2.4</span>
<span id="S3.T10.2.1.1.1.1.1.1.3.2.7" class="ltx_td ltx_align_right ltx_border_t">14.2</span>
<span id="S3.T10.2.1.1.1.1.1.1.3.2.8" class="ltx_td ltx_align_right ltx_border_t">11.6</span>
<span id="S3.T10.2.1.1.1.1.1.1.3.2.9" class="ltx_td ltx_align_right ltx_border_t">14.5</span>
<span id="S3.T10.2.1.1.1.1.1.1.3.2.10" class="ltx_td ltx_align_right ltx_border_t">16.1</span></span>
<span id="S3.T10.2.1.1.1.1.1.1.4.3" class="ltx_tr">
<span id="S3.T10.2.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Nemotron-4</span>
<span id="S3.T10.2.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">15B</span>
<span id="S3.T10.2.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">34.0</span>
<span id="S3.T10.2.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">28.1</span>
<span id="S3.T10.2.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">21.3</span>
<span id="S3.T10.2.1.1.1.1.1.1.4.3.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">16.8</span>
<span id="S3.T10.2.1.1.1.1.1.1.4.3.7" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">21.2</span>
<span id="S3.T10.2.1.1.1.1.1.1.4.3.8" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">23.1</span>
<span id="S3.T10.2.1.1.1.1.1.1.4.3.9" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">18.1</span>
<span id="S3.T10.2.1.1.1.1.1.1.4.3.10" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S3.T10.2.1.1.1.1.1.1.4.3.10.1" class="ltx_text ltx_font_bold">23.2</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T10.3.1.1" class="ltx_text" style="font-size:90%;">Table 10</span>: </span><span id="S3.T10.4.2" class="ltx_text" style="font-size:90%;"> Eight-shot results on Flores sub-tasks translating out of Chinese. All results for external models were obtained from <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a href="#bib.bib49" title="" class="ltx_ref">2023</a>)</cite></span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section">4&nbsp;&nbsp;&nbsp;Conclusion</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">We present Nemotron-4 15B, a decoder-only transformer-based large language model.
It is trained on 8 trillion tokens spanning English, 53 additional natural languages as well as 43 programming languages. Nemotron-4 15B exhibits the strongest multilingual performance of any general purpose language model at its scale – even outperforming models specialized for the multilingual domain.
Nemotron-4 demonstrates that pre-training sets for large language models can continue to be scaled up even further in order to improve the abilities of models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
NVLink and NVSwitch.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.nvidia.com/en-us/data-center/nvlink/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nvidia.com/en-us/data-center/nvlink/</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ainslie et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Joshua Ainslie, James Lee-Thorp, Michiel de&nbsp;Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.

</span>
<span class="ltx_bibblock">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13245</em>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allal et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Loubna&nbsp;Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos&nbsp;Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh&nbsp;Kumar Umapathi, Carolyn&nbsp;Jane Anderson, Yangtian Zi, Joel&nbsp;Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco&nbsp;De Toni, Bernardo&nbsp;García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry&nbsp;Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de&nbsp;Vries, and Leandro von Werra.

</span>
<span class="ltx_bibblock">SantaCoder: Don’t Reach for the Stars!, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.

</span>
<span class="ltx_bibblock">Program Synthesis with Large Language Models, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Qwen Technical Report.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan&nbsp;Le Bras, Jianfeng Gao, and Yejin Choi.

</span>
<span class="ltx_bibblock">PIQA: Reasoning about Physical Commonsense in Natural Language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassano et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn&nbsp;Jane Anderson, Molly&nbsp;Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.

</span>
<span class="ltx_bibblock">MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Software Engineering</em>, pages 1–17, 2023a.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/TSE.2023.3267446</span>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassano et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn&nbsp;Jane Anderson, Molly&nbsp;Q Feldman, et&nbsp;al.

</span>
<span class="ltx_bibblock">Multipl-e: a scalable and polyglot approach to benchmarking neural code generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Software Engineering</em>, 2023b.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Du&nbsp;Chen, Yi&nbsp;Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, and Kun Han.

</span>
<span class="ltx_bibblock">Orion-14B: Open-source Multilingual Large Language Models, 2024.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique&nbsp;Ponde de&nbsp;Oliveira&nbsp;Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe&nbsp;Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William&nbsp;Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew&nbsp;N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.

</span>
<span class="ltx_bibblock">Evaluating Large Language Models Trained on Code, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, et&nbsp;al.

</span>
<span class="ltx_bibblock">PaLM: Scaling Language Modeling with Pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jonathan&nbsp;H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki.

</span>
<span class="ltx_bibblock">TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2003.05002, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2003.05002" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2003.05002</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think You have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.

</span>
<span class="ltx_bibblock">Training Verifiers to Solve Math Word Problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2110.14168, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2110.14168" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2110.14168</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.

</span>
<span class="ltx_bibblock">A Framework for Few-shot Language Model Evaluation, September 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.5371628" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.5371628</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemma&nbsp;Team (2024)</span>
<span class="ltx_bibblock">
Google&nbsp;DeepMind Gemma&nbsp;Team.

</span>
<span class="ltx_bibblock">Gemma: Open Models Based on Gemini Research and Technology, 2024.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2023)</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Gemini: A Family of Highly Capable Multimodal Models, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da&nbsp;Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan.

</span>
<span class="ltx_bibblock">The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2106.03193, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2106.03193" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2106.03193</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring Massive Multitask Language Understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.03300</em>, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training Compute-Optimal Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jennings et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Shrimai Prabhumoye, Ayush Dattagupta, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Curating Trillion-Token Datasets: Introducing NVIDIA NeMo Data Curator.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://developer.nvidia.com/blog/curating-trillion-token-datasets-introducing-nemo-data-curator/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.nvidia.com/blog/curating-trillion-token-datasets-introducing-nemo-data-curator/</a>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling Laws for Neural Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korthikanti et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Reducing Activation Recomputation in Large Transformer Models, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.06226</em>, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Raymond Li, Loubna&nbsp;Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry&nbsp;Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh&nbsp;Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva&nbsp;Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn&nbsp;Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos&nbsp;Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von
Werra, and Harm de&nbsp;Vries.

</span>
<span class="ltx_bibblock">StarCoder: May the Source be with You!, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit&nbsp;Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li.

</span>
<span class="ltx_bibblock">Few-shot Learning with Multilingual Language Models, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA (2022)</span>
<span class="ltx_bibblock">
NVIDIA.

</span>
<span class="ltx_bibblock">H100 Tensor Core GPU Architecture Overview, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et&nbsp;al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">BLEU: A Method for Automatic Evaluation of Machine Translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</em>, ACL ’02, page 311–318, USA, 2002. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3115/1073083.1073135</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.3115/1073083.1073135" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3115/1073083.1073135</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ponti et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Edoardo&nbsp;Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen.

</span>
<span class="ltx_bibblock">XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2005.00333, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2005.00333" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2005.00333</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post.

</span>
<span class="ltx_bibblock">A Call for Clarity in Reporting BLEU Scores.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1804.08771, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1804.08771" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1804.08771</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jack&nbsp;W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van&nbsp;den Driessche, Lisa&nbsp;Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang&nbsp;Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de&nbsp;Masson&nbsp;d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de&nbsp;Las&nbsp;Casas, Aurelia Guy, Chris Jones,
James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed&nbsp;Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, 21(1):5485–5551, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.

</span>
<span class="ltx_bibblock">Socialiqa: Commonsense reasoning about social interactions, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander&nbsp;M. Rush, Stella Biderman, Albert Webson, Pawan&nbsp;Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert&nbsp;Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz&nbsp;Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro&nbsp;Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham&nbsp;Fikri Aji, Amit Alfassy, Anna Rogers, Ariel&nbsp;Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David&nbsp;Ifeoluwa Adelani, Dragomir Radev, Eduardo&nbsp;González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal&nbsp;Bar Natan, Francesco&nbsp;De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin,
Isaac Johnson, Itziar Gonzalez-Dios, Javier de&nbsp;la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro&nbsp;Von Werra, Leon Weber, Long Phan, Loubna&nbsp;Ben allal, Ludovic Tanguy, Manan Dey, Manuel&nbsp;Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh&nbsp;Chien Vu, Mohammad&nbsp;A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de&nbsp;Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto&nbsp;Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen&nbsp;Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago&nbsp;Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette
Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut&nbsp;Emre Taşar, Elizabeth Salesky, Sabrina&nbsp;J. Mielke, Wilson&nbsp;Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason&nbsp;Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M&nbsp;Saiful Bari, Maged&nbsp;S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen&nbsp;H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung&nbsp;Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette,
Pierre&nbsp;François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta&nbsp;Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica&nbsp;Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van&nbsp;der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol,
Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos&nbsp;Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong&nbsp;A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio&nbsp;Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav&nbsp;Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo&nbsp;Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu,
Clémentine Fourrier, Daniel&nbsp;León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena&nbsp;U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose&nbsp;David Posada, Karthik&nbsp;Rangasai Sivaraman, Lokesh Bulchandani, Lu&nbsp;Liu, Luisa Shinzato, Madeleine&nbsp;Hahn de&nbsp;Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria&nbsp;A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel&nbsp;De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas&nbsp;Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok&nbsp;S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak,
Yash&nbsp;Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu&nbsp;Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf.

</span>
<span class="ltx_bibblock">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung&nbsp;Won Chung, Yi&nbsp;Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.

</span>
<span class="ltx_bibblock">Language Models are Multilingual Chain-of-Thought Reasoners, 2022.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shliazhko et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina.

</span>
<span class="ltx_bibblock">mGPT: Few-Shot Learners Go Multilingual, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Megatron-LM: Training Multi-Billion Parameter Language Models using Model Parallelism.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Slav&nbsp;Petrov and et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Andrew M.&nbsp;Dai Slav&nbsp;Petrov, Yonghui&nbsp;Wu and et&nbsp;al.

</span>
<span class="ltx_bibblock">PaLM 2 Technical Report, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://ai.google/static/documents/palm2techreport.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.google/static/documents/palm2techreport.pdf</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zheng, Rewon Child, Reza&nbsp;Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2201.11990, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2201.11990" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2201.11990</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Ahmed Murtadha, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced Transformer with Rotary Position Embedding.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.09864</em>, 2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzgun et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V. Le, Ed&nbsp;H. Chi, Denny Zhou, and Jason Wei.

</span>
<span class="ltx_bibblock">Challenging big-bench tasks and whether chain-of-thought can solve them, 2022.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language Models, 2023a.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-tuned Chat Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1706.03762, 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1706.03762" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1706.03762</a>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00359</em>, 2019.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da&nbsp;Pan, Dian Wang, Dong Yan, Fan Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Baichuan 2: Open Large-scale Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10305</em>, 2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">HellaSwag: Can a Machine Really Finish Your Sentence?

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2019.

</span>
</li>
</ul>
</section>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Supplementary Materials</h2>

<figure id="Ax1.T11" class="ltx_table">
<table id="Ax1.T11.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Ax1.T11.2.1.1" class="ltx_tr">
<th id="Ax1.T11.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="Ax1.T11.2.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt">Size</th>
<td id="Ax1.T11.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Humanities</td>
<td id="Ax1.T11.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Social sciences</td>
<td id="Ax1.T11.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">STEM</td>
<td id="Ax1.T11.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Other</td>
<td id="Ax1.T11.2.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">Average</td>
</tr>
<tr id="Ax1.T11.2.2.2" class="ltx_tr">
<th id="Ax1.T11.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Nemotron-4</th>
<th id="Ax1.T11.2.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t">15B</th>
<td id="Ax1.T11.2.2.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">69.2</td>
<td id="Ax1.T11.2.2.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">74.1</td>
<td id="Ax1.T11.2.2.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">53.4</td>
<td id="Ax1.T11.2.2.2.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">67.5</td>
<td id="Ax1.T11.2.2.2.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">64.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="Ax1.T11.3.1.1" class="ltx_text" style="font-size:90%;">Table 11</span>: </span><span id="Ax1.T11.4.2" class="ltx_text" style="font-size:90%;"> Per-category breakdown accuracy for MMLU </span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.16818" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.16819" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2402.16819">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.16819" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.16820" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 18:37:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Chinchilla Scaling: 복제 시도\n' +
      '\n' +
      ' 타메이 베시로그루\n' +
      '\n' +
      'Ege Erdil\n' +
      '\n' +
      'Matthew Barnett\n' +
      '\n' +
      'Josh You\n' +
      '\n' +
      'Epoch AI\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Hoffmann et al.(2022)은 계산-최적 스케일링 법칙을 추정하기 위한 세 가지 방법을 제안한다. 우리는 모수 손실 함수를 그림의 데이터 재구성에 맞추는 세 번째 추정 절차를 복제하려고 시도한다. 그 결과, 제안된 추정치들은 처음 두 추정 방법들과 일치하지 않고, 추출된 데이터를 맞추는데 실패하며, 믿을 수 없을 정도로 좁은 신뢰구간을 보고하는데, 이 좁은 간격은 600,000개 이상의 실험이 필요하지만 500개 미만으로만 실행될 가능성이 있다. 반면, 세 번째 접근법을 사용하여 스케일링 법칙을 재유도하면 호프만 등이 설명한 첫 두 추정 절차의 결과와 양립할 수 있는 결과를 얻을 수 있다.\n' +
      '\n' +
      '+\n' +
      '각주 †: [https://epochai.org/code/analyzing-chinchilla](https://epochai.org/code/analyzing-chinchilla) 주소를 사용하여 모든 분석을 복제할 수 있습니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Hoffmann et al.(2022)은 주어진 계산 예산 하에서 변압기 언어 모델을 훈련시키기 위한 최적의 모델 크기 및 훈련 토큰의 수를 조사한다. 저자들은 5,000억에서 5,000억 토큰에 대해 7,000만에서 160억 이상의 매개변수에 이르는 400개 이상의 언어 모델을 훈련하고 컴퓨팅 최적화 훈련을 위해 모델 크기와 훈련 토큰의 수가 동일한 비율로 확장되어야 한다는 것을 발견했다. 모델 크기의 두 배가 될 때마다 훈련 토큰의 수도 두 배가 되어야 한다. 그런 다음 결과에 따라 컴퓨팅 최적인 친칠라라는 모델을 훈련시킵니다. 이 때문에 이들이 제안하는 스케일링 법칙을 흔히 "친칠라 스케일링 법칙"이라고 부른다.\n' +
      '\n' +
      '저자는 컴퓨팅-최적 프론티어를 추정하기 위해 세 가지 다른 방법을 사용한다(Approaches 1-3):\n' +
      '\n' +
      '1. 다양한 수의 토큰에 대한 고정된 크기의 트레이닝 모델.\n' +
      '2. 고정된 컴퓨트 예산(IsoFLOP 프로파일)을 타겟으로 하는 다양한 크기의 트레이닝 모델.\n' +
      '3. 모델 크기 및 트레이닝 토큰의 함수로서 손실의 파라메트릭 모델을 피팅하는 단계.\n' +
      '\n' +
      '주요 결과는 컴퓨팅-최적 트레이닝을 위해, 모델 크기와 트레이닝 토큰의 수가 동일하게 스케일링되어야 한다는 것이다: 모델 크기의 두 배마다 트레이닝 토큰의 수 또한 두 배로 증가되어야 한다.\n' +
      '\n' +
      '스케일링 훈련 토큰이 모델 파라미터의 수와 거의 동일한 속도로 성장해야 한다는 결과는 Anil 등(2023)과 같은 다른 사람들에 의해 복제되었다. 유사하게, Bi 등(2024)은 트레이닝 토큰들 및 모델 파라미터들이 대략 비례적으로 스케일링되어야 한다는 것을 발견하지만, 이것이 데이터의 품질에 민감하고, 더 높은 품질의 데이터에 대해 트레이닝할 때 더 낮은 토큰-파라미터당 비율이 최적이라는 것을 발견한다.\n' +
      '\n' +
      '최적 스케일링에 대해 알려주는 것 외에도 접근법 3은 고밀도 변압기에 대한 스케일링 법칙의 매개변수 형태를 조명하기 때문에 특히 중요하다. Hoffmann et al.의 특정 매개변수 추정치는 신경 스케일링 법칙의 이론적 설명에서와 같이 독립적인 과학적 관심사였다(예를 들어, Michaud et al., 2024; Bordelon et al., 2024).\n' +
      '\n' +
      'Hoffmann et al.의 데이터 세트를 부분적으로 재구성하고 접근법 3을 복제하려고 시도한다. 이것은 최종 훈련 전 손실을 \\(L(N,D)=E+\\frac{A}{N^{a}}+\\frac{B}{D^{2}}}\\로 모델링하기 위해 매개변수 함수를 피팅하는 것을 포함한다. 여기서 \\(N\\)는 모델 매개변수의 수를 나타내고 \\(D\\)는 훈련 토큰의 수를 나타낸다. 우리의 분석에서는 우리의 추정된 모델이 호프만 등에 보고된 적합도와 상당히 다르며 그들의 적합도가 재구성된 데이터를 적절하게 설명하지 못한다는 것을 보여준다. 우리는 호프만 등이 보고한 신뢰 구간이 믿을 수 없을 정도로 빡빡하고 데이터 세트의 크기를 감안할 때 적절한 통계 절차에서 얻을 가능성이 없음을 보여준다. 마지막으로, 우리는 그들의 적합성이 다른 접근법을 통해 도출된 스케일링 정책 및 우리의 적합성이 제안한 스케일링 정책과 일치하지 않음을 보여준다.\n' +
      '\n' +
      '## 2 Hoffmann et al.\'s Figure 4에서 데이터 추출\n' +
      '\n' +
      '본 논문에서 재현된 Hoffmann et al.의 Hoffmann et al.1 그림 4에서 그림 4의 데이터를 재구성하여 언어 모델의 산점도를 표시한다(그림 1 참조). \\(x\\)-축은 모델 크기를 나타내고, \\(y\\)-축은 훈련 FLOP을 나타내며, 각 점의 색상은 손실 값을 인코딩한다. 그림에서 데이터를 추출하기 위해 먼저 호프만 등의 arXiv 제출에서 PDF를 다운로드하고 SVG 형식으로 저장했다. 그런 다음 SVG 콘텐츠를 구문 분석하여 SVG 구조를 탐색하고 검색했다. SVG 내에서 산점도 데이터를 나타내는 점 그룹을 식별하고 각 점에 대해 반복하여 해당 SVG 요소의 속성을 사용하여 채우기 색상 및 위치(\\(x\\) 및 \\(y\\) 좌표)를 추출했다.\n' +
      '\n' +
      'SVG 좌표를 모델 크기에 매핑하고 FLOP 값을 훈련하기 위해 각 축의 레이블 또는 눈금의 위치를 사용했다. 이를 통해 SVG 좌표와 그림에 표시된 실제 데이터 값 사이의 대응 관계를 설정할 수 있었다.\n' +
      '\n' +
      '각 산란점과 관련된 손실 값을 도출하기 위해 그래프에 제공된 색상 척도에서 색상 값(육각 형식)을 추가로 추출했다. 그래프의 색상 스케일은 2.00에서 5.00 범위의 로그 스케일을 사용하여 색상을 손실 값으로 매핑한다. 색상 스케일은 위에서 아래로 픽셀을 반복하고 각 픽셀에서 육각 색상 값을 읽고 스케일 내에서 픽셀의 수직 위치를 기반으로 해당 손실 값을 계산하여 처리했다. 이 프로세스는 육각 색상 값과 해당 손실 값 사이의 매핑을 초래했다. 그런 다음 이 매핑을 사용하여 채우기 색상을 기반으로 각 산란점에 대한 손실 값을 결정했다.\n' +
      '\n' +
      '디지털화 프로세스는 다음의 요인들로 인해 일부 노이즈 또는 에러를 도입할 수 있다:\n' +
      '\n' +
      '1. _정확하지 않은 \\(y\\)-좌표 추출_ 입니다. 산점도의 \\(y\\)-축에는 눈금 표시가 부족하여 특정 모델 크기에 해당하는 정확한 \\(y\\)-좌표를 결정하기 어렵습니다. 모델 크기를 나타내는 레이블이 각 텍스트 레이블의 수직 중심이 \\(y\\)-축의 각 값과 정렬되도록 위치한다고 가정했다.\n' +
      '2. _손실 대 색상 매핑으로 인한 정밀 손실_. 컬러 스케일은 256 헥스 값으로 제한되며, 이는 손실 값을 대략 0.01.2 각주 2의 정확도로 정밀하게 추정하는 우리의 능력을 제한한다: 로그 스케일로 인해, 정밀도는 손실 값이 높을수록 더 낮다. 예를 들어, 손실 값 5에서 추정된 부정확성은 약 0.015이다.\n' +
      '\n' +
      '몇 가지 데이터 포인트가 눈에 띕니다. 원(10^{19}\\) 주위에 훈련 FLOP이 있는 점의 열이 있으며, 여기서 더 큰 모델 중 일부는 높은 손실 값(유사한 계산 예산을 가진 모델에 비해 최대 70% 더 높음)을 얻는다. 이는 모수에 대한 훈련 토큰의 가장 낮은 비율에 해당한다(상위 5개는 비율 \\(<0.4\\)). 이러한 훈련 실행이 왜 그렇게 높은 손실을 가져왔는지는 불분명하다. 우리의 분석에서, 우리는 이 열에서 다섯 점을 떨어뜨린다. 부록 A.1에서 우리는 이것이 호프만 등의 추정된 친칠라 스케일링 법칙의 부적합한 적합성에 대한 결론을 실질적으로 변경하지 않는다는 것을 보여준다.\n' +
      '\n' +
      '## 3 Hoffmann et al.의 접근법 3 복제 시도\n' +
      '\n' +
      '우리는 데이터의 재구성된 서브세트에 Hoffmann 등 파라메트릭 스케일링 법칙을 적합시킨다:\n' +
      '\n' +
      '\\[L(N,D)=E+\\frac{A}{N^{\\alpha}}+\\frac{B}{D^{\\beta}} \\tag{1}\\]\n' +
      '\n' +
      '\\(N\\)는 매개 변수의 수이고 \\(D\\)는 훈련 토큰의 수입니다. Hoffmann 등이 사용한 이 함수를 맞추기 위해, 우리는 Huber 손실을 최소화한다:\n' +
      '\n' +
      '\\[\\min_{a,b,c,\\alpha,\\beta}\\sum_{\\text{Run }i}\\text{Huber}_{i}\\Big{(}\\text{ LSE}\\left(a-\\alpha\\log N_{i},b-\\beta\\log D_{i},e\\right)-\\log L_{i}\\Big{}, \\tag{2}\\]\n' +
      '\n' +
      '여기서 \\(LSE\\)는 log-sum-exp 연산자이다. Hoffmann et al.의 부록 D.2의 방법론을 따르고 \\(\\delta\\)를 \\(10^{-3}\\)로 설정하고 \\(\\alpha\\in\\{0,0.5,\\dots,2\\}\\), \\(\\beta\\in\\{0,0.5,\\dots,2\\}\\), \\(e\\in\\{-1,-0.5,\\dots,1\\}\\), \\(a\\in\\{0,5,\\dots,25\\}\\), \\(b\\in\\{0,5,\\dots,25\\}\\). 이렇게 하면 다음과 같은 추정 모델이 생성됩니다.\n' +
      '\n' +
      '[left=\\begin{array}{c}\\text{One estimated model}\\\\ \\\\ L(N,D)=1.82+\\frac{514.0}{N^{0.35}}+\\frac{2115.2}{D^{0.37}}\\end{array}\\right.\\] (3)\n' +
      '\n' +
      '[left=\\begin{array}{c}\\text{H Hoffmann et al.\'s model}\\\\ \\\\ L(N,D)=1.69+\\frac{406.4}{N^{0.34}}+\\frac{410.7}{D^{0.28}}\\end{array}\\right.\\] (4)\n' +
      '\n' +
      '우리는 우리의 추정치와 호프만 등의 추정치 사이의 차이가 얼마나 중요한지 테스트한다. 이 시험은\n' +
      '\n' +
      '도 1: 호프만 등으로부터 취해진 도 4의 좌측 서브-도형으로부터의 디스플레이 윤곽.\n' +
      '\n' +
      '그림 2: 호프만 등의 그림 4의 데이터를 재구성합니다.\n' +
      '\n' +
      '다음 관찰에 기초하여: Hoffmann 등의 추정치가 전체 데이터 세트에서 최적이고 우리의 데이터 세트가 그들이 사용한 원래 데이터 세트의 일반 또는 무작위 부분 집합이라면, 우리의 추정치는 일부 공분산 행렬 \\(\\Sigma\\)을 사용하여 추정치를 중심으로 하는 정규 분포를 대략 따라야 한다.\n' +
      '\n' +
      '즉, \\(\\mu\\)가 Hoffmann et al.의 추정치를 나타내고 \\(\\nu\\)가 우리의 최적 적합치를 나타내는 경우, 일부 공분산 행렬 \\(\\Sigma\\)에 대한 차이 \\(\\mu-\\nu\\)는 \\(\\mathcal{N}(0,\\Sigma)\\)를 따라야 한다. 이 경우를 고려할 때, 우리는 \\((\\mu-\\nu)^{T}\\Sigma^{-1}(\\mu-\\nu)\\)가 벡터의 차원 수와 동일한 자유도를 갖는 \\(\\chi^{2}\\) 분포를 따를 것으로 예상한다 \\(\\mu,\\nu\\).\n' +
      '\n' +
      '공분산 행렬 \\(\\Sigma\\)은 알려져 있지 않지만 부트스트랩 철학을 따라 추정할 수 있다. "부트스트랩은 모집단에 대한 표본과 같다." 240개의 데이터 포인트로 구성된 전체 데이터 세트에서 교체하여 \\(n=240\\)회 샘플링한 다음 이 부트스트랩 데이터 세트에 스케일링 법칙을 피팅하여 각 부트스트랩을 구성한다. 이를 4000개의 서로 다른 부트스트랩에 대해 반복하면 매개변수 \\((\\log A,\\log B,\\log E,\\alpha,\\beta)\\)의 벡터에 대한 샘플링 분포를 얻을 수 있으며 이로부터 샘플 공분산 행렬을 계산할 수 있다. 이를 통해 위의 \\(\\chi^{2}\\) 검정에 필요한 \\(\\Sigma\\)를 얻을 수 있으며, 결과적으로 \\(\\chi^{2}\\) 검정의 \\(p\\)-값은 \\(<10^{-60}\\)이 된다. 이는 우리의 매개변수와 호프만 등의 매개변수 간의 차이가 통계적으로 매우 유의하다는 것을 의미한다.\n' +
      '\n' +
      '매개변수를 개별적으로 살펴보면 \\(E\\)와 \\(\\beta\\)는 \\(p\\)-값이 각각 \\(1.5\\times 10^{-6}\\)와 \\(4.3\\times 10^{-5}\\)로 매우 유의한 차이를 보였다. 이러한 작은 \\(p\\)-값은 \\(E\\) 및 \\(\\beta\\)에 대한 추정 값이 각각의 실제 값과 상당히 다르다는 것을 나타내며, 이는 모델에 기초한 예상 결과에서 이러한 매개변수의 추정치의 상당한 편차를 시사한다. 반면에 매개변수 \\(A\\), \\(B\\) 및 \\(\\alpha\\)는 Hoffmann 등에 보고된 값과 통계적으로 크게 다르지 않은 추정 값을 가지고 있다.\n' +
      '\n' +
      '### Chinchilla 스케일링 법칙이 보고된 데이터에 맞지 않습니다.\n' +
      '\n' +
      '이 적합 스케일링 법칙의 잔차와 동일한 스케일링 법칙에 대한 추정치의 잔차를 그릴 때 추정된 스케일링 법칙이 데이터에 잘 맞지 않는다는 것이 분명해진다.\n' +
      '\n' +
      '추정된 스케일링 법칙에 대한 잔차는 0 부근에서 단단히 집중되어 있어 실질적으로 더 나은 적합도를 나타낸다. 우리는 Huber 손실 값의 98%가 Hoffmann 등의 중간 손실 값보다 작으며, 우리 모델이 모든 관찰의 90%에 대해 더 낮은 손실을 얻는다는 것을 발견했다. 우도비 검정을 통해 호프만 등의 추정이 우리의 적합도(\\(p<10^{-135}\\), 부록 A.3 참조)만큼 잘 수행된다는 가설을 자신 있게 기각할 수 있다. 즉, 우리의 데이터 재구성이 정확하다면, Hoffmann 등 추정 스케일링 법칙은 데이터에 맞지 않는다.\n' +
      '\n' +
      '이것은 데이터 재구성의 노이즈 때문일 수 있습니까? 이것은 매우 가능성이 없다. 그림으로 인해 손실된 정밀도로 인한 손실값의 잡음은 0.015 이하이고, Hoffmann et al. fit의 평균 잔차는 -0.05이다. Hoffmann et al.의 \\(E\\) 추정치를 그들이 얻은 값이 아닌 0-평균 잔차를 산출하는 값으로 설정하더라도 여전히 훨씬 더 분산된 잔차 분포와 더 높은 손실값을 산출한다. Kolmogorov-Smirnov 검정을 통해 \\(p=1.4\\times 10^{-10}\\)에서 손실 값의 분포의 동일성을 거부할 수 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Parameter & Our estimate & Hoffman et al’s estimate \\\\ \\hline \\(A\\) & 482.01 & 406.4 \\\\ \\((124.58)\\) & & \\\\ \\(B\\) & 2085.43 & 410.7 \\\\ \\(E\\) & 1.82 & 1.69 \\\\ \\(\\alpha\\) & 0.35 & 0.34 \\\\ \\(\\beta\\) & 0.37 & 0.28 \\\\ \\(a=\\beta/(\\alpha+\\beta)\\) & 0.512 & 0.454 \\\\ \\hline Data points & 240 & \\(>400\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 모수 추정치와 표준 오차입니다. 표준 오차는 괄호 안에 표시되며 부트스트래핑에 의해 얻어진다. 비교를 위한 추정치와 함께 호프만 등의 추정치를 보여준다.\n' +
      '\n' +
      '그림 4: Hoffmann 등의 추정 모델과 우리의 추정 모델의 Huber 손실 값 그림. Kolmolgorov-Smirnoff 검정은 이들 분포의 동일성을 거부한다(\\(p=3.4\\times 10^{-71}\\)).\n' +
      '\n' +
      '그림 3: Hoffmann et al.의 추정 모형과 우리의 추정 모형의 잔차 그림.\n' +
      '\n' +
      '### Hoffmann et al. 보고 믿을 수 없을 정도로 좁은 신뢰 구간\n' +
      '\n' +
      'Hoffmann 등의 추정치의 더 복잡한 특징은 다음과 같이 정의되는 매개변수 \\(a\\) 및 \\(b\\)에 대한 매우 좁은 신뢰 구간이다.\n' +
      '\n' +
      '\\[a\\equiv\\frac{\\beta}{\\alpha+\\beta},\\b\\equiv 1-a. \\tag{5}\\]\n' +
      '\n' +
      '이러한 계수들의 중요성은 최적 스케일링 관계, 특히 계산 예산 \\(C\\)의 최적 할당이 어떻게 모델링될 수 있는지 이해하는 데 있다. \\(N_{opt}\\propto C^{a}\\)와 \\(D_{opt}\\propto C^{a}\\)의 관계는 이러한 계수를 사용하여 훈련 계산을 할당하는 방법을 설명합니다.\n' +
      '\n' +
      '특히 \\(a\\)와 \\(b\\)에 대해 각각 0.454~0.455와 0.542~0.543의 신뢰구간을 보고한다. 이들은 \\(\\alpha\\)와 \\(\\beta\\)를 추정할 수 있는 400개 정도의 관측치를 가졌을 가능성이 높기 때문에 매우 빡빡하다.\n' +
      '\n' +
      '대조적으로 우리는 \\(a\\)에 대해 0.018의 표준 오차를 추정한다(표 2 참조). 이것은 \\(2\\cdot z_{0.9}\\cdot 0.02\\approx 0.05\\)의 80% 신뢰 구간의 너비에 해당한다. 따라서 우리의 80% 신뢰 구간은 호프만 등이 보고한 것보다 50배 더 넓다.\n' +
      '\n' +
      '0.001의 신뢰구간을 얻기 위해 얼마나 많은 훈련 런을 관찰해야 하는가? 표준오차가 \\(\\sqrt{N}\\)에서 줄어들기 때문에 실험 횟수를 \\(50^{2}\\) 또는 2500배 증가시켜야 한다. 이는 Hoffmann 등이 보고한 신뢰구간을 얻기 위해 거의 \\(240\\times 2116=600,000\\) 훈련 런의 결과에 접근할 필요가 있다는 것을 의미한다.\n' +
      '\n' +
      '400개 이상의 관측치를 가졌다는 호프만 등의 보고서에 기초하여, 우리는 이것이 400~500개의 데이터 포인트를 가졌을 가능성이 있음을 의미하는 것으로 해석한다. 그들이 주장한 대로 훈련된 각 모델에 대한 최종 손실 값만 사용했다면 수십만 개의 관찰이 있었을 가능성은 거의 없어 보인다. 그러나 이 점에 대한 저자의 추가 설명이 도움이 될 것이다.\n' +
      '\n' +
      '### Hoffmann 등의 접근법 3 스케일링 정책은 Chinchilla 및 우리의 추정치와 일치하지 않습니다.\n' +
      '\n' +
      '\\(a\\)에 대한 신뢰 구간은 우리의 신뢰 구간이 그들의 신뢰 구간과 겹친다는 점에서 호프만 등의 접근법 1과 2와 일치한다. 이는 우리의 최적 스케일링 정책이 이러한 접근법의 스케일링 권장 사항과 일치함을 의미한다.\n' +
      '\n' +
      '계산 최적화 크기 조정 정책이 무엇인지 해결합니다. 그렇게 하고 추정치의 불확실성을 고려하여 그림 5에 예시된 정책 범위를 얻는다.\n' +
      '\n' +
      '이 분석에서 두 가지 주요 관찰이 나타난다. 첫째, Hoffmann 등의 추정치에 근거한 최적 스케일링 정책에 대한 신뢰구간은 매우 좁다. 섹션 3.2에서 논의한 바와 같이 보고된 데이터 포인트 수를 감안할 때 이러한 간격이 부당하게 빡빡하다고 주장한다.\n' +
      '\n' +
      '둘째, Hoffmann 등의 추정 파라미터로부터 도출된 스케일링 정책은 최적의 성능을 위해 파라미터당 약 70개의 토큰을 사용하는 것을 제안한다. 이 처방은 호프만 등이 70B 친칠라 모델을 훈련시키기 위해 실제로 사용한 모수당 토큰 비율 20개와 일치하지 않는다. 흥미로운 사실은 이 20개의 매개변수당 토큰 비율은 논문에서 보고된 두 가지 다른 접근법(접근법 1 및 2)의 결과와 밀접하게 일치한다.\n' +
      '\n' +
      '대조적으로, 우리의 적합 모델은 매개변수당 약 20 토큰의 최적 비율을 의미하며, 이는 친칠라 모델이 훈련된 방법과 호프만 등의 접근법 1과 2의 결과와 일치한다(그림 5 참조). 추정된 스케일링 법칙(Approach 3)에 기초한 처방과 Hoffmann 등의 접근법 1과 2의 결과 사이의 불일치는 매개변수 추정의 정확성에 대한 추가 우려를 제기한다. 실제로 사용되고 다른 접근법에서 얻은 토큰 대 매개변수 비율과 추정치의 정렬은 적합성의 유효성을 강화한다.\n' +
      '\n' +
      '## 4 Discussion\n' +
      '\n' +
      '접근법 3에 의존하는 친칠라 스케일링 법칙의 호프만 등의 추정치에서 세 가지 잠재적인 문제를 발견했다.\n' +
      '\n' +
      '1. 이들의 추정 모델은 재구성된 데이터에 매우 잘 맞지 않는다. 이러한 결론은 데이터 재구성에서 잠재적인 노이즈를 고려하고 이상치 모델을 제외하는 경우에도 성립한다.\n' +
      '2. 데이터 포인트의 수를 고려할 때 신뢰는 믿을 수 없을 정도로 빡빡하다. 신뢰 구간을 엄격하게 얻으려면 수십만 개의 관찰이 필요하지만 \\(\\sim\\)400만 있을 가능성이 있습니다.\n' +
      '3. 그들의 추정된 모델은 그들의 다른 접근법들 및 그들의 20-tokens-per-parameter rule-of-thumb과 일치하지 않는 스케일링 정책을 의미한다.\n' +
      '\n' +
      '우리가 관찰한 불일치는 호프만 등이 데이터를 플로팅한 방법의 오류 때문일 수 있으며, 이는\n' +
      '\n' +
      '그림 5: 추정치를 사용하여 모델 모수에 대한 훈련 토큰의 최적 비율입니다. 음영 영역은 80% 신뢰 구간을 나타냅니다. 우리의 추정치는 친칠라에 사용된 스케일링 정책과 일치하지만 모수 모델에 대한 추정치는 그렇지 않다.\n' +
      '\n' +
      'would mean our reconstructed dataset is not a accurate representation of their original data.4\n' +
      '\n' +
      '각주 4: 추가 가능성은 접근법 3에 대한 설명에 따라 최종 손실을 사용하기보다는 훈련 중 중간 손실도 사용했다는 것이다. 그렇다면 신뢰 구간은 실질적으로 더 좁지만 표준 오차가 적절하게 군집링되는 경우 크기 정도는 아닐 수 있다. 전반적으로, 우리는 이것이 호프만 등이 보고한 타이트한 간격을 설명할 것 같지 않다고 생각한다(부록 A.2 참조).\n' +
      '\n' +
      '우리의 분석에서는 추가 조사 및 설명이 필요한 호프만 등의 보고된 결과에 대한 몇 가지 잠재적인 문제를 강조한다.\n' +
      '\n' +
      'Hoffmann 등의 논문은 언어 모델링 커뮤니티에서 영향력이 매우 컸다. 그 결과는 구글의 제미니 스위트룸(구글, 2023)과 같은 주목할 만한 모델의 스케일링 정책에 알려졌고, 다른 많은 모델의 개발을 이끌었을 가능성이 있다. 더욱이, 본 논문에서 제시된 스케일링 법칙의 특정 매개변수 추정치는 기계 학습 이론과 같은 중요한 과학적 관심사이다. 이 연구의 광범위한 영향을 감안할 때 작업의 견고성과 재현성을 철저히 조사하는 것이 중요하다.\n' +
      '\n' +
      '우리의 작업은 계산 최적 스케일링에 대한 불확실성을 더욱 강조한다. 계산-최적 스케일링 정책의 플롯된 범위(그림 5 참조)에서, 피팅된 모델이 1e26 FLOP 이상에서 트레이닝되는 모델에서 4 내지 40 사이의 파라미터들에 대한 토큰들의 비율과 일치한다는 것을 발견한다. 이 크기의 실험을 실행하기 전에 더 엄격한 추정치를 얻으면 예상대로 해당 계산량의 의미 있는 부분을 절약할 수 있습니다.\n' +
      '\n' +
      '## 부록 A Appendix\n' +
      '\n' +
      '### 5개의 이상치를 삭제 하지 않으면 어떻게 됩니까?\n' +
      '\n' +
      '이전 분석에서 이러한 이상치가 친칠라 스케일링 법칙을 사용하여 적합하기 어렵기 때문에 모수에 대한 훈련 토큰의 비율이 가장 낮은 5개의 실험을 제외했다. 연구 결과의 견고성을 보장하기 위해 이제 이러한 이상치를 제거하지 않고 분석의 주요 부분을 반복한다. 이 추가 분석에서는 이러한 이상치가 포함될 때 매개변수 추정치가 더 불확실하지만 주요 결론은 변하지 않음을 보여준다.\n' +
      '\n' +
      '모델의 품질 모델의 전체 유의성은 \\(\\chi^{2}\\)\\(p\\)-값이 \\(2.0\\times 10^{-35}\\)로 강조되어 특정 매개변수 추정치의 상당한 편차를 나타낸다. 세부적으로 \\(E\\)와 \\(\\beta\\)는 각각 \\(1.4\\times 10^{-5}\\)와 \\(1.7\\times 10^{-3}\\)로 예상 참값과 매우 유의한 차이를 보여 이들 매개변수 추정치의 상당한 편차를 강조한다.\n' +
      '\n' +
      '적합치를 비교하면 추정된 친칠라 모델이 데이터에 잘 맞지 않는다는 것을 다시 찾을 수 있다. 3.1절에서 우도비 검정을 반복하면 \\(5.4\\times 10^{135}\\)의 t-통계량이 산출된다. 손실값을 검토하기 위해 Kolmolgorov-Smirnoff 검정을 통해 이들 분포의 동일성을 거부할 수 있다(\\(p=1.6\\times 10^{-54}\\)).\n' +
      '\n' +
      '최적 크기 조정은 매개변수 경험 규칙당 20개의 토큰과 일치하는 범위를 찾습니다. 실제로, 우리의 포인트 추정치는 매개변수당 25.6 토큰이 최적임을 의미한다.\n' +
      '\n' +
      '### 크기 조정 법칙에 적합 하는 데 중간 손실이 사용 되는 경우 어떻게 하나요?\n' +
      '\n' +
      '3.2에서 언급된 바와 같이, 매개변수 \\(a\\) 및 \\(b\\)에 대한 Hoffmann 등의 보고된 신뢰 구간은 스케일링 법칙에 적합하도록 400개의 손실 값 순서로 사용되는 경우 믿을 수 없을 정도로 좁다. 실제로 0.001의 신뢰 구간을 산출하기 위해서는 거의 \\(600,000\\)의 서로 다른 손실 값에 액세스할 수 있어야 했을 것이다. 이는 논문에 보도된 \'400여 기종의 손실\'보다 훨씬 높은 수치다.\n' +
      '\n' +
      '이러한 명백한 불일치를 설명하기 위해 한 가지 가능성은 Hoffmann 등이 훈련된 각 모델에 대한 최종 손실 값만 사용하는 것이 아니라 각 훈련 실행 동안 중간 손실 값을 사용하여 적합을 얻었을 수 있다는 것이다. 데이터 세트에는 모델당 최소 수백 개의 중간 손실 값이 있을 가능성이 높기 때문에 이론적으로 이 추가 데이터는 보고된 매우 좁은 신뢰 구간을 얻는 데 필요한 수십만 개의 데이터 포인트를 제공할 수 있었다.\n' +
      '\n' +
      '그러나 호프만 등이 접근법 3에 대한 설명에서 "접근법 1 및 2의 실험에서 얻은 모든 최종 손실을 모델 매개변수 수와 보이는 토큰 수의 매개변수 함수로 모델링"한다고 명시적으로 언급한 것을 감안할 때 이러한 가능성을 고려하지 않는다. 더욱이, Hoffmann 등의 방법론적 접근의 핵심 요소는 학습 속도 감쇠 스케줄을 훈련 토큰의 수와 대략 일치하도록 설정하는 것이었다. 그들은 말했습니다.\n' +
      '\n' +
      '코사인 사이클 길이와 그에 대응하는 학습률 하락에 대해 하나의 핵심 가정이 이루어진다(우리는 Rae et al. (2022)에 따라 \\(10\\times\\) 학습률 하락을 사용한다). 우리는 그림 A1과 같이 코사인 사이클 길이를 목표 훈련 단계 수보다 너무 길게 설정하면 차선책으로 훈련된 모델이 생성된다는 것을 발견한다. 그 결과, 우리는 최적으로 훈련된 모델이 코사인 사이클 길이를 정확하게 교정할 것이라고 가정한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Parameter & Our estimate & Chinchilla’s estimate \\\\ \\hline \\(A\\) & \\(\\begin{subarray}{c}463.29\\\\ (144.95)\\end{subarray}\\) & 406.4 \\\\ \\(B\\) & \\(\\begin{subarray}{c}12529.51\\\\ (6157.03)\\end{subarray}\\) & 410.7 \\\\ \\(E\\) & \\(\\begin{subarray}{c}1.89\\\\ (0.04)\\end{subarray}\\) & 1.69 \\\\ \\(\\alpha\\) & \\(\\begin{subarray}{c}0.35\\\\ (0.02)\\end{subarray}\\) & 0.34 \\\\ \\(\\beta\\) & \\(\\begin{subarray}{c}0.45\\\\ (0.05)\\end{subarray}\\) & 0.28 \\\\ \\(a=\\beta/(\\alpha+\\beta)\\) & \\(\\begin{subarray}{c}0.512\\\\ (0.032)\\end{subarray}\\) & 0.454 \\\\ \\hline Data points & 245 & \\(>400\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 모수 추정치와 표준 오차입니다. 표준 오차는 괄호 안에 표시되며 부트스트래핑에 의해 얻어진다. 우리는 FLOP 예산을 감안할 때 Hoffmann 등의 추정치와 비교.5 최대 단계 수를 위한 추정치를 보여주며, 주요 분석에서 이 규칙을 따른다.\n' +
      '\n' +
      '목표 훈련 단계 수와 일치하도록 설정된 고정된 코사인 사이클 길이를 갖는 주어진 훈련 실행에 대해, 호프만 등의 자체 결과에 의해, 초기 중간 손실은 동일한 수의 훈련 단계를 갖는 최적으로 훈련된 모델의 최종 손실을 과대평가할 것이다. 이는 최종 손실 값에서 스케일링 법칙을 맞추기 위해 초기 중간 손실 값을 사용해서는 안 된다는 것을 의미한다.\n' +
      '\n' +
      '호프만 등이 각 훈련 실행의 마지막(아마도 15%)에서 나중에 중간 손실 값을 사용했다고 생각할 수 있으며, 이는 이 기본 문제를 피할 수 있었다. 그러나 이것이 그들이 수행한 작업이라면 각 훈련 실행과 함께 중간 손실 값은 서로 높은 상관 관계가 있어 모든 데이터 포인트를 독립적으로 취급하기보다는 표준 오류를 클러스터링할 필요가 있다. 호프만 등이 표준 오차를 클러스터링했다는 표시가 없기 때문에 6 우리는 이 가능성을 매우 신뢰할 수 있다고 생각하지 않는다.\n' +
      '\n' +
      '각주 6: 예를 들어, 호프만 등이 각각 1,000개의 중간 손실 값을 생성하는 500개의 훈련 실행을 수행했다고 가정하자. 이러한 훈련 실행 간의 분산을 설명하기 위해 \\(\\rho\\)로 표시된 그룹 내 상관 계수를 사용한다. 보존적으로 \\(\\rho=0.5\\)를 가정하면, 유효 데이터 포인트 수는 수식을 사용하여 계산될 수 있다:\n' +
      '\n' +
      '\\[N_{\\text{eff}}=\\frac{N}{1+(n-1)\\cdot\\rho},\\]\n' +
      '\n' +
      ' 여기서 \\(N\\)은 총 관측치 수이고 \\(n\\)은 그룹당 관측치 수입니다. \\(N=500,000\\) 및 \\(n=1,000\\)을 사용하여 이 계산은 \\(N_{\\text{eff}}}\\approx 1,000\\)을 산출하며, 이는 Hoffmann 등이 보고한 엄격한 신뢰 구간을 얻기에는 여전히 너무 낮다.\n' +
      '\n' +
      '### 우도 비율 테스트\n' +
      '\n' +
      '우리는 휴버 손실 최소화 문제를 최대 가능성 문제로 재구성하고 가능성 비율 테스트를 수행하여 이러한 좋은 적합성의 부족을 추가로 정량화할 수 있다. 임의의 값 \\(\\delta>0\\)에 대해 함수 \\(p:\\mathbb{R}\\rightarrow\\mathbb{R}^{\\geq 0}\\)\n' +
      '\n' +
      '\\[p_{\\delta}(x)=\\frac{\\exp(-\\text{Huber}_{\\delta}(x))}{\\int_{-\\infty}^{\\infty} \\exp(-\\text{Huber}_{\\delta}(x))\\,dx} \\tag{6}\\]\n' +
      '\n' +
      '이 함수는 허버 손실 함수와 가산 상수.7까지의 음의 로그 우도를 갖는 실수들에 대한 합법적인 확률 밀도 함수이다. 따라서, 우리는 손실 최소화 문제를 \\(p\\)로 정의된 분포에 대한 우도 최대화 문제로 변환할 수 있다. Introducing location and scale parameters \\(\\mu\\) and \\(\\sigma\\), we augment this distribution to\n' +
      '\n' +
      '각주 7: 분모에서 적분을 \\(\\sqrt{2\\pi}(2\\Phi(\\delta)-1)+2e^{-\\delta^{2}/2}/\\delta\\)로 명시적으로 계산할 수 있으며, 여기서 \\(\\Phi\\)는 표준 정규 분포의 누적 분포 함수이다.\n' +
      '\n' +
      '\\[p_{\\mu,\\sigma,\\delta}(x)=\\frac{1}{\\sigma}\\cdot p_{\\delta}\\left(\\frac{x-\\mu}{ \\sigma}\\right) \\tag{7}\\]\n' +
      '\n' +
      '그 다음, 수학식 2로부터의 손실 최소화 문제를 다음의 음의 로그-가능성 최소화 문제로 변환한다:\n' +
      '\n' +
      '\\[\\min_{a,b,e,\\alpha,\\beta,\\sigma} \\sum_{\\text{Run }i}-\\log p_{\\mu=0,\\sigma=\\sigma,\\delta=\\delta}\\bigg{(} \\tag{8}\\] \\[\\text{LSE}\\left(a-\\alpha\\log N_{i},b-\\beta\\log D_{i},e\\right)- \\log L_{i}\\bigg{)}}.\\]\n' +
      '\n' +
      '우리는 두 가지 적합을 수행합니다. 하나는 식 8에 설명된 대로 정확히 구속되지 않은 적합이고 다른 하나는 \\(\\sigma\\)는 자유롭게 변할 수 있지만 다른 매개변수는 Hoffmann 등에 보고된 값에 고정됩니다. 그리고 우리는 우도비 검정을 사용하여 얻은 두 로그 우도를 비교합니다. 우리는 Hoffmann 등 매개변수를 귀무가설로 가정하고 \\(\\chi^{2}\\) 분포와 \\(6-1=5\\) 자유도 하에서 로그 우도 차이에 대한 p-값을 보고합니다. 그 결과는 표 3에 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Anil et al.(2023) Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, et al.(2023). 팜 2 기술 보고서입니다. _ arXiv preprint arXiv:2305.10403_.\n' +
      '* Bi 등 (2024) Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z., et al. (2024). 딥뷰 llm: 장기주의를 가진 오픈 소스 언어 모델 크기 조정 _ arXiv preprint arXiv:2401.02954_.\n' +
      '* Bordelon 등(2024) Bordelon, B., Atanasov, A., and Pehlevan, C. (2024). 신경 스케일링 법칙의 동적 모델입니다. _ arXiv preprint arXiv:2402.01092_.\n' +
      '* Google (2023) Google, G. T. (2023). 제미니: 매우 유능한 멀티모달 모델 가족입니다. _ arXiv preprint arXiv:2312.11805_.\n' +
      '* Hoffmann et al.(2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al.(2022). 컴퓨팅 최적화 대용량 언어 모델 교육 _ arXiv preprint arXiv:2203.15556_.\n' +
      '* Michaud et al. (2024) Michaud, E., Liu, Z., Girit, U., and Tegmark, M. (2024). 신경 스케일링의 양자화 모델입니다. _ Neural Information Processing Systems_, 36에서의 진보.\n' +
      '* Rae et al.(2022) Rae, J. W., Borgeaud, S., Cai, T., Millican, K., and et al., J. H. (2022). 스케일링 언어 모델: 고퍼 훈련의 방법, 분석 및 통찰력.\n' +
      '* Wilks (1938) Wilks, S. S. (1938). 복합 가설 검정을 위한 우도 비율의 대표본 분포입니다. _ The annals of mathematical statistics_, 9(1):60-62.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Parameters & Log likelihood & Parameters fitted \\\\ \\hline Hoffmann et al.’s fit & 635.04 & 1 \\\\ Our best fit & 879.77 & 6 \\\\ \\hline Likelihood ratio test p-value & \\(5\\cdot 10^{-135}\\) & \\\\ Data points & 240 & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Hoffmann 등 매개변수 및 데이터 세트에서 가장 적합한 매개변수에 대한 로그 가능성 값입니다. 호프만 등 매개변수를 귀무가설로 가정하면 우도비 검정을 수행하여 로그 우도 간의 차이가 매우 유의함을 보여준다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
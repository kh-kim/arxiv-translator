Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation

Haoran Xu

Amr Sharaf

Yunmo Chen

Weiting Tan

Lingfeng Shen

Benjamin Van Durme

Kenton Murray

Young Jin Kim

Equal contribution. Work done during an internship at Microsoft. \({}^{\spadesuit}\)Johns Hopkins University \({}^{\diamondsuit}\)Microsoft. Correspondence to: Haoran Xu \(<\)hxu6@jhu.edu\(>\), Kenton Murray \(<\)kenton@jhu.edu\(>\), Young Jin Kim \(<\)youki@microsoft.com\(>\).

stance, one may notice that some strong translation models are capable of producing translations superior to the gold reference, as illustrated in Figure 1. Secondly, SFT lacks a mechanism to prevent the model from rejecting mistakes in translations. While strong translation models can produce high-quality translations, they occasionally exhibit minor errors, such as omitting parts of the translation. _Preventing the production of these near-perfect but ultimately flawed translation is essential_. To overcome these issues, we introduce Contrastive Preference Optimization (CPO) to train the ALMA model using specially curated preference data. After CPO training, the ALMA-R model shows marked improvements, achieving performance levels that match or even surpass those of GPT-4 and WMT competition winners.

Our main contributions are summarized as follows:

**Are reference Gold or Gilded?** We conducted an in-depth analysis of the training data (FLORES-200 data) utilized by the ALMA model. We meticulously compared the quality of the reference translations with those generated by strong translation models. Our findings reveal that, in numerous instances, the quality of human-written parallel data is even inferior to that of system-generated translations. This observation underscores a critical insight: training models exclusively towards replicating reference translations may not be the most effective approach, and reliance on reference-based evaluation could be flawed.

**Pushing the Performance Boundary of SFT** We introduce Contrastive Preference Optimization, which offers advantages in terms of memory efficiency, speed, and, crucially, enhanced effectiveness in improving translation quality. CPO breaks the performance bottleneck inherent in SFT's reference-mimicking learning process and push the performance boundary of models that have reached saturation through SFT training.

**Preference Data** We build and release a high-quality preference data for the machine translation area.

## 2 Gold or Gilded? Scrutinizing Gold Reference Quality

The significance of target references is paramount in machine translation tasks. The paradigm of training models on the machine translation task heavily relies on the quality of the references since the model is commonly optimized using a loss that is defined to minimize the difference between the predicted outputs and gold reference. Consider a dataset \(\mathcal{D}\), comprising pairs of source sentences \(x\) and their corresponding target sentences (gold references) \(y\), represented as \(\mathcal{D}=\left\{x^{(i)},y^{(i)}\right\}_{i=1}^{N}\), where \(N\) is the total number of parallel sentences. The negative log-likelihood loss for these parallel sentences, in relation to a model \(\pi_{\theta}\) parameterized by \(\theta\), is defined as follows:

\[\mathcal{L}_{\text{NLL}}=-\mathbb{E}_{(x,y)\sim\mathcal{D}}[\log\pi_{\theta}(y |x)]. \tag{1}\]

Hence, the ability of models to effectively translate is contingent upon the availability of high-quality translation pairs (Xu et al., 2023; Maillard et al., 2023). Furthermore, prevalent evaluation tools such as BLEU (Papineni et al., 2002) and COMET-22 (Rei et al., 2022) predominantly rely on reference-based metrics. However, the precision of these evaluations is sensitive to and compromised by substandard references (Kocmi et al., 2023; Freitag et al., 2023). Recent research (Xu et al., 2023; Kocmi et al., 2023; Freitag et al., 2023) has shifted attention towards assessing the quality of parallel datasets, indicating that target references may not consistently represent the highest quality. In Figure 2, we take a translation example from the FLORES-200 dataset, and compare the gold reference translation with outputs from the best ALMA model and GPT-4. This comparison reveals that the gold reference is a flawed translation, as it omits part of information, whereas the system-generated outputs demonstrate superior quality. This prompts an inquiry: _Are references (even though human-written) truly equivalent to gold standards?_ To thoroughly assess the quality of both the gold standard references and the outputs from contemporary high-performance translation models,

Figure 1: A performance comparison featuring our proposed model ALMA-13B-R against other recently released 13B LLM-based models, as well as top-performing translation systems like GPT-4 and WMT winners. This evaluation covers the WMT’22 test data across 8 directions, involving translations to and from English for German, Czech, Chinese, and Russian. Scores are averaged by three different reference-free models: wmt23-cometkiwi-da-xxl, XCOMET-XXL, and wmt22-cometkiwi-da, and are also averaged across all directions. The gold reference is also evaluated due to the reference-free approach. Our model, ALMA-13B-R, developed by further training ALMA-13B-LoRA using our proposed CPO method, either matches or surpasses the most advanced translation models, We show the detailed numerical data for all systems presented in the figure in Appendix A.

we propose evaluating these outputs utilizing reference-free evaluation frameworks.

**Models** We scrutinize the translation outputs from ALMA-13B-LoRA2, as well as zero-shot translations from the most recent GPT-4 (gpt-4-1106-preview). To assess the quality of these outputs, we employ two of the latest and largest reference-free models, each with a 10B parameter size and demonstrating very high correlation with human judgements (Freitag et al., 2023). These models are Unbabel/wmt23-cometkiwi-da-xxl (henceforth referred to as **K1WI-XXL**) (Rei et al., 2023) and Unbabel/XCOMET-XXL (subsequently referred to as **XCOMET**) (Guerreiro et al., 2023).

Footnote 2: ALMA-13B-LoRA is the best 13B translation model in the ALMA families. It initially undergoes _full-weight_ fine-tuning on monolingual data, followed by fine-tuning on high-quality human-written parallel data using _low-rank adaptation_ (LoRA) (Hu et al., 2022).

**Data** we consider the high-quality and human-written FLORES-200 dataset (NLLB TEAM et al., 2022), comprising both development and test data, amounting to a total of 2009 samples for each language direction, to compare the gold references with the outputs generated by the models. We employed ALMA-13B-LoRA and GPT-4 to perform translations across five English-centric language pairs, covering both translations from and to English. These pairs include German (de), Czech (cs), Icelandic (is), Chinese (zh), and Russian (ru), with Icelandic (is) categorized as a low-resource language and the others as high-resource languages.

**Prompt** The prompt employed for generating translations with ALMA models is consistent with the one used in Xu et al. (2023). For GPT-4 translation generation, we follow the guidelines suggested by Hendy et al. (2023). The specifics of these prompts are detailed in Appendix B.

**Model Outputs Can Be Better References** In Table 1, we present the evaluation scores of K1WI-XXL and XCOMET for the gold references, ALMA-13B-LoRA outputs, and GPT-4 outputs. Additionally, we report _Win Ratio_, reflecting the proportion of instances where model outputs surpass the gold standard references. These metrics are calculated as an average across five languages. Remarkably, even comparing with the high-quality Flores-200 dataset, the average performance of translation models in xx\(\rightarrow\)en translations significantly exceeds that of the references, showing approximately 3-4 point increases in K1WI-XXL and 4-6 point gains in XCOMET. Notably, a significant proportion of outputs are rated higher than the references by K1WI-XXL (e.g., **73.24%** for ALMA), with a slightly reduced yet still substantial percentage when assessed using XCOMET (**60.17%** for ALMA). In the en\(\rightarrow\)xx direction, while the overall performance between the translations from reference and two systems is comparable, approximately 40% are still deemed superior to the reference translations.

**Motivation: Help The Model Learn Rejection** The aforementioned findings illustrate that translations produced by advanced models can sometimes surpass the quality of gold standard references. This raises the question of how to effectively utilize such data. A straightforward approach would involve fine-tuning the model using the source and the superior translations as references. While this could enhance the model's translation abilities, it does not equip the model with the discernment to identify and avoid generating suboptimal translations, exemplified by the "good but not perfect" translations depicted in Figure 2. Consequently, this situation motivates us to develop a new training objective, which aims to instruct the model in prioritizing the generation of higher-quality translations and rejecting lesser ones, in a style of contrastive learning with hard negative examples (Oord et al., 2018; Chen et al., 2020; He et al., 2020; Robinson et al., 2021; Tan et al., 2023). This objective moves beyond the traditional focus on merely minimizing cross-entropy loss towards the reference.

## 3 Contrastive Preference Optimization

To learn an objective that fosters superior translations and rejects inferior ones, access to labeled preference data is

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & K1WI-XXL & Win Ratio (\%) & XCOMET & Win Ratio (\%) \\ \hline \multicolumn{5}{c}{_Translating to English_ (xx\(\rightarrow\)en)} \\ \multicolumn{5}{c}{Reference} & 85.31 & 85.82 & - \\ ALMA-13B-LoRA & 88.33 & 73.24 & 92.68 & 60.17 \\ GPT-4 & 89.21 & 79.43 & 94.66 & 54.25 \\ \hline \multicolumn{5}{c}{_Translating from English_ (xx\(\rightarrow\)xx)} \\ \multicolumn{5}{c}{_Translessential, yet such data is scarce in machine translation. In this section, we first describe the construction of our preference data and then introduces a preference learning technique, contrastive preference optimization (CPO).

### Triplet Preference Data

We here details our methodology for constructing preference data \(\mathcal{D}\). This dataset is developed using the FLORES-200 data (both development and test sets) and encompasses the same language pairs as discussed in Section 2. For each language pair, the dataset comprises 2009 parallel sentences.

For a given source sentence \(x\), whether translated from or to English, we utilize both GPT-4 and ALMA-13B-LoRA to generate respective translations, denoted as \(y_{\text{grt-4}}\) and \(y_{\text{alma}}\). Together with the original target reference \(y_{\text{ref}}\), this forms a triplet \(\mathbf{y}=(y_{\text{ref}},y_{\text{grt-4}},y_{\text{alma}})\), representing three different translation outputs for the input \(x\). The reference-free evaluation models KIWI-XXL and XCOMET are then employed to score these translations, with the average scores represented as \(\mathbf{s}=(s_{\text{ref}},s_{\text{grt-4}},s_{\text{alma}})\).3 The highest-scoring translation is labeled as the preferred translation \(y_{w}\), and the lowest-scoring as the dis-preferred translation \(y_{l}\), i.e., \(y_{w}=\mathbf{y}_{\arg\max_{i}(\mathbf{s})},y_{l}=\mathbf{y}_{\arg\min_{i}( \mathbf{s})}\), where \(i\) represents the index in the triplet. Translations with intermediate scores are not considered. An illustrative example of this selection process is depicted in Figure 3. It is important to note that even the dis-preferred translations may be of high-quality. The designation 'dis-preferred' indicates that there is still room for improvement, perhaps through the addition of minor details. This approach of using high-quality but not flawless translations as dis-preferred data aids in training the model to refine details and achieve perfection in generated translations.

Footnote 3: The impact of using different evaluation models, such as only using XCOMET or KIWI-XXL, is explored in Section 5.1.

### Deriving the CPO Objective

We discuss the derivation of CPO objective, beginning with an analysis of Direct Preference Optimization (DPO) (Rafailov et al., 2023). DPO represents a more direct optimization objective utilized in reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019; Ouyang et al., 2022). Given a set of source sentences \(x\), alongside preferred translation targets \(y_{w}\) and less preferred ones \(y_{l}\), we can access a static dataset of comparisons, denoted as \(\mathcal{D}=\left\{x^{(i)},y_{w}^{(i)},y_{l}^{(i)}\right\}_{i=1}^{N}\). The loss function for DPO is constructed as a maximum likelihood objective for a parameterized policy \(\pi_{\theta}\):

\[\mathcal{L}(\pi_{\theta};\pi_{\text{ref}})= -\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\Big{[}\log\sigma \Big{(}\beta\log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{\text{ref}}(y_{w}|x)}\] \[-\beta\log\frac{\pi_{\theta}(y_{l}|x)}{\pi_{\text{ref}}(y_{l}|x)} \Big{)}\Big{]}, \tag{2}\]

where \(\pi_{\text{ref}}\) is a pre-trained language (translation) model \(\sigma\) is the Sigmoid function, and \(\beta\) is a hyperparameter. DPO training can be conducted in a supervised fine-tuning style, as it relies exclusively on labeled preference data and does not require interaction between agents and their environment.

However, DPO has notable drawbacks compared to common SFT. Firstly, DPO is **memory-inefficient**: it necessitates twice the memory capacity to simultaneously store both the parameterized policy and the reference policy. Secondly, it is **speed-inefficient**: executing the model sequentially for two policies doubles the processing time. To address these inefficiencies, we introduce contrastive preference optimization.

The memory- or speed- inefficiency can be resolved when \(\pi_{\text{ref}}\) is set as a uniform prior \(U\), as the terms \(\pi_{\text{ref}}(y_{w}|x)\) and \(\pi_{\text{ref}}(y_{l}|x)\) cancel each other out. This negates the need for additional computations and storage beyond the policy model itself. Thus, we initially demonstrate that the DPO loss can be effectively approximated using a uniform reference model:

\[\mathcal{L}(\pi_{\theta};U)= -\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\Big{[}\log\sigma \Big{(}\beta\log\pi_{\theta}(y_{w}|x)\] \[-\beta\log\pi_{\theta}(y_{l}|x)\Big{)}\Big{]}. \tag{3}\]

Specifically, we prove the below Theorem in Appendix C.

**Theorem 1**.: _When \(\pi_{\text{ref}}\) is defined as \(\pi_{w}\), an ideal policy that precisely aligns with the true data distribution of preferred data, the DPO loss \(\mathcal{L}(\pi_{\theta};\pi_{w})+C\) is upper bounded by \(\mathcal{L}(\pi_{\theta};U)\), where \(C\) is a constant._

The approximation in Equation 3 is effective because it minimizes the upper boundary of the DPO loss. The proof

Figure 3: A triplet of translations, either model-generated or derived from a reference, accompanied by their respective scores as assessed by reference-free models. For a given source sentence, the translation with the highest score is designated as the preferred translation, while the one with the lowest score is considered dis-preferred, and the translation with a middle score is disregarded.

relies on an important assumption of \(\pi_{\text{ref}}=\pi_{w}\). Contrary to common practices where \(\pi_{\text{ref}}\) is set as the initial SFT checkpoint, our approach considers it as the ideal policy we aim to reach. Although the ideal policy \(\pi_{w}\) is unknown and unattainable during model training, it is not engaged in the loss after our approximation.

Furthermore, we incorporate a behavior cloning (BC) regularizer (Hejna et al., 2023) to ensure that \(\pi_{\theta}\) does not deviate from the preferred data distribution:

\[\min_{\theta}\mathcal{L}(\pi_{\theta},U)\] \[\text{s.t.}\ \mathbb{E}_{(x,y_{w})\sim\mathcal{D}}\Big{[}\mathbb{KL}( \pi_{w}(y_{w}|x)||\pi_{\theta}(y_{w}|x))\Big{]}<\epsilon, \tag{4}\]

where \(\epsilon\) is a small positive constant and \(\mathbb{KL}\) is Kullback-Leibler (KL) divergence. The regularizer can boil down to adding a SFT term on the preferred data (a detailed explanation is provided in Appendix C):

\[\min_{\theta}\underbrace{\mathcal{L}(\pi_{\theta},U)}_{\mathcal{L}_{\text{ prefer}}}\underbrace{-\mathbb{E}_{(x,y_{w})\sim\mathcal{D}}[\log\pi_{\theta}(y_{w}|x )]}_{\mathcal{L}_{\text{KL}}}. \tag{5}\]

The above is the formulation of our CPO loss, which includes one preference learning term \(\mathcal{L}_{\text{prefer}}\) and one negative log likelihood term \(\mathcal{L}_{\text{NLL}}\).

## 4 Experiments

### Data

Following Section 2, we consider 10 translation directions in the paper: cs+en, de+en, is+en, zh+en, ru+en. Building on the ALMA models' (Xu et al., 2023) insights that a small quantity of high-quality data can yield impressive translation results, our training dataset is even more compact. As detailed in Section 3.1, our preference training data is derived from the FLORES-200 dataset, a subset of which has been also employed in the training of ALMA models. This results in a total of \(2\text{K}\times 10\) directions \(=20\text{K}\) paired sentences. In addition to preference data assessed by large evaluation models, our dataset incorporates 1K internal human-labeled preference data, containing preferred and dis-preferred translations along with human preference. However, the human-labeled data is limited to just two translation directions: en\(\rightarrow\)zh and en\(\rightarrow\)de. The details regarding the composition and influence of human-labeled data are explored in Appendix D.4 In alignment with Xu et al. (2023), our primary focus is on the test set drawn from WMT'21 for is and WMT'22 for other languages. Additionally, we conduct auxiliary experiments evaluating models on WMT'23, covering six directions: de+en, zh+en, and ru+en.

Footnote 4: TL;DR: A brief overview of the impact of this human-labeled data suggests a minimal effect.

### Training Setup

We train the model in a _many-to-many_ multilingual machine translation manner, starting with ALMA-13B-LoRA as the initial checkpoint. During the training phase, we focus exclusively on updating the weights of the added LoRA parameters. These weights have a rank of 16 and only add an additional 12M parameters to the original 13B size of the model. We adhere to the default \(\beta\) value of 0.1 as suggested by Rafailov et al. (2023). The fine-tuning process of ALMA-13B-LoRA involves a batch size of 128, a warm-up ratio of 0.01, spanning a single epoch, and accommodating sequences with a maximum length of 512 tokens. To optimize training efficiency, we integrate the deepspeed tool (Rasley et al., 2020). We utilize the same prompt as Xu et al. (2023) and do not compute the loss for the prompt. While our primary focus is on the performance of 13B models, CPO markedly benefits 7B models as well. Consequently, we also release ALMA-7B-R and provide a detailed discussion of its performance in Appendix A.

### Baselines

**SoTA Models** In this category, our benchmarks are established against, to the best of our knowledge, the strongest publicly available translation models. We first compare with **ALMA-13B-LoRA**, recognized as one of the top moderate-size language-model based translation systems, surpassing notable conventional models such as NLLB-54B in both WMT'21 and WMT'22. We also compare our results with **TowerInstruct5**, a recently released LLM-based translation model and a contemporary work in the field.6 Additionally, we evaluate against the zero-shot performance of the latest **GPT-4** (gpt-4-1106-preview), currently shown to be the best translation model among all LLM-based translation systems (Xu et al., 2023; Zhang et al., 2023; Zeng et al., 2023; Jiao et al., 2023). Lastly, we include comparisons with the **WMT competition winners**, representing the highest standard of translation models within the competition, though it is noted that the winning models vary across different language directions.7

Footnote 5: [https://huggingface.co/datasets/Unbabel/TowerBlocks-v0.1](https://huggingface.co/datasets/Unbabel/TowerBlocks-v0.1).

Footnote 6: Note that TowerInstruct has used WMT’22 test data for training, so we exclude it from comparison on the WMT’22 test dataset.

**SFT and DPO** We also compare different training objectives. Given that CPO is designed to steer learning towards preferred data, a straightforward benchmark is to compare its performance against directly SFT on the same preferred data set. Furthermore, considering that CPO is an evolution of DPO, we also include a comparative analysis with DPO.

### WMT'21 and WMT'22 Results

We present the primary results for en\(\rightarrow\)xx and xx\(\rightarrow\)en in Table 2 and Table 3, respectively. Our emphasis is primarily on reference-free evaluation models, due to our analysis in Section 2, which questions the reliability of gold references and highlights that evaluations can be compromised by poor-quality references (Kocmi et al., 2023; Freitag et al., 2023). These models include KIWI-XXL, XCOMET, and a smaller yet popular model, Unbabel/wmt22-cometkiwi-da (hereinafter referred to as **KIWI-22**). Scores highlighted in **bold** represent the highest achieved across all systems. For a comprehensive comparison, we also include reference-based evaluations using sacreBLEU (Post, 2018) and COMET-22 (Unbabel/wmt22-comet-da) (Rei et al., 2022) in Appendix A.

**Comparing With SoTA Models** While ALMA-13B-LoRA ranks as one of the top moderate-size LLM translation models, it slightly trails behind GPT-4 and the WMT competition winners. However, the incorporation of CPO significantly enhances ALMA's capabilities, bringing its performance to a level that is comparable to or even surpasses that of GPT-4 and WMT winners. For example, ALMA-13B-R achieves an average score of 85.74 on KIWI-XXL and 94.05 on XCOMET for en\(\rightarrow\)xx translations. These scores outperform GPT-4, which scores 83.83 on KIWI-XXL and 93.23 on XCOMET, as well as the WMT winners, who score 84.81 on KIWI-XXL and 93.78 on XCOMET.

**Comparing With SFT and DPO** All training objectives in our study are fine-tuned using the ALMA-13B-LoRA model as a base. In Table 2 and 3, we observe that SFT on preferred data marginally enhances the ALMA model's translation capability for xx\(\rightarrow\)en, and results in a slight deterioration for en\(\rightarrow\)xx. Similarly, DPO slightly decreases model performance. In contrast, CPO demonstrates significant improvements across all translation directions.

### WMT'23 Results

We show the average results across all six directions in Table 4, and provide the performance in each direction in Appendix G due to the space constraint. Consistent with observations from WMT'21 and WMT'22, ALMA-13B-R surpasses contemporary moderate-size LLM-based translators such as ALMA-13B-LoRA and TowerInstruct, and either matches or exceeds WMT winners.

## 5 Analyses

All analyses use the WMT'21 and WMT'22 test sets, with their averaged performance being reported.

### Are Translations Really Better or Just Metric-Preferred?

In our study, since the preferred data is selected by reference-free models and the same models are used for evaluation, we investigate the potential for 'cheating' in the scoring process. Specifically, we question whether the improved

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{de} & \multicolumn{3}{c}{cs} & \multicolumn{3}{c}{is} \\ \cline{2-10} Models & KIWI-22 & KIWI-XXL & XCOMET & KIWI-22 & KIWI-XXL & XCOMET & KIWI-22 & KIWI-XXL & XCOMET \\ \hline Gold Reference & 82.67 & 84.01 & **97.85** & 83.19 & 81.83 & 90.27 & 80.51 & 85.20 & 91.52 \\ WMT Winners & **83.56** & 83.70 & 96.99 & 85.31 & **87.27** & **94.38** & 81.77 & 84.94 & 91.61 \\ GPT-4 & 83.48 & **84.91** & 97.56 & 84.81 & 85.35 & 93.48 & 81.03 & 81.21 & 90.00 \\ ALMA-13B-LoRA & 82.62 & 81.64 & 96.49 & 84.14 & 84.24 & 92.38 & 81.71 & 83.31 & 91.20 \\ + SFT on preferred data & 82.75 & 81.85 & 96.67 & 84.14 & 83.46 & 91.99 & 81.48 & 82.11 & 90.30 \\ + DPO & 82.40 & 81.20 & 96.40 & 83.86 & 83.45 & 91.68 & 81.43 & 82.66 & 90.33 \\ + CPO (Ours, ALMA-13B-R) & 83.28 & 84.25 & 97.48 & **84.99** & 87.06 & **93.61** & **82.18** & **85.68** & **91.93** \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c c c c c c c} \hline \hline Models & KIWI-22 & KIWI-XXL & XCOMET & KIWI-22 & KIWI-XXL & XCOMET & KIWI-22 & KIWI-XXL & XCOMET \\ \hline Gold Reference & 80.92 & 81.70 & 90.42 & 82.96 & 84.62 & 94.17 & 82.05 & 83.47 & 92.85 \\ WMT Winners & 82.04 & 81.13 & 91.14 & **84.35** & 87.01 & 94.79 & **83.41** & 84.81 & 93.78 \\ GPT-4 & 81.73 & 81.53 & 90.79 & 83.64 & 86.15 & 94.3 & 82.94 & 83.83 & 93.23 \\ ALMA-13B-LoRA & 80.82 & 79.96 & 89.92 & 83.10 & 84.17 & 93.79 & 82.48 & 82.66 & 92.76 \\ + SFT on preferred data & 81.25 & 80.51 & 90.18 & 83.23 & 84.15 & 93.54 & 82.57 & 82.42 & 92.54 \\ + DPO & 80.74 & 79.64 & 89.58 & 82.94 & 83.40 & 93.25 & 82.27 & 82.07 & 92.25 \\ + CPO (Ours, ALMA-13B-R) & 82.25 & 84.32 & **92.03** & 83.98 & **87.37** & **95.22** & 83.34 & **85.74** & **94.05** \\ \hline \hline \end{tabular}
\end{table}
Table 2: The overall results in en\(\rightarrow\)xx for WNT’21 and WMT’22. The application of the CPO method to fine-tune the ALMA-13B-LoRA model leads to a significant enhancement in performance, equalling or surpassing that of WMT competition winners and GPT-4. **bold** numbers denote the highest scores across all systems. Dark blue boxes indicates that the improvement over the original ALMA model achieves _at least 80% estimated accuracy_ with the human judgement (Kocmi et al., 2024). Specifically, this denotes that for an agreement rate of 80% with human decisions, the improvement needs a minimum of \(\geq 1.24\) for both KIWI-XXL and XCOMET, and \(\geq 0.53\) for KIWI-22. Further details on estimatied accuracy are provided in Appendix F. The lesser improvements are highlighted in shallow blue boxes. Decreases in performance are marked with yellow boxes.

translation scores reflect genuinely better translations or if they simply align more closely with the evaluation model's preferences. This inquiry is addressed in two parts:

At the metric level, we examine if training a model on data preferred by a specific metric (such as KIWI-XXL) yields improvements that are consistent across other metrics. To investigate this, we reconstruct the preference data using only KIWI-XXL or XCOMET and re-train the ALMA-13B-LoRA model using the CPO method. The results, presented in Table 5, do not indicate a significant bias towards the metric used for selecting preferred data. We observed similar and consistent improvements across all metrics, regardless of the specific metric used to select the preferred data. Considering Comet-series models may be positive correlated, we further evaluate ALMA-R using a non-comet metric, BLEURT (Sellam et al., 2020), and also observe significant improvements in Appendix H. The inclusion of a third-party evaluation metric further substantiates the superior translation quality of ALMA-R.

At the method level, we question whether training on metric-preferred data always leads to better scores on that metric, regardless of the method we use. However, the connection is not straightforward; for instance, SFT on preferred data paradoxically results in diminished performance across all three metrics as shown in Table 2.

Consequently, our analysis supports the robustness and validity of using reference-free models like KIWI-XXL and XCOMET both for constructing preference data and for evaluation purposes, underscoring the absence of bias in this approach. Furthermore, Table 5 demonstrates that the choice between using KIWI-XXL, XCOMET, or an ensemble of both has a minimal impact on the results.

### Ablation Study

**CPO Loss Components** The CPO loss function consists of two components: \(\mathcal{L}_{\text{prefer}}\) for preference learning, and \(\mathcal{L}_{\text{NLL}}\), which ensures the model does not deviate significantly from the preferred data distribution. To illustrate the significance of each term, we re-train the model exclusively with one of the components. It is important to note that training solely with \(\mathcal{L}_{\text{NLL}}\) equates to the baseline scenario of SFT on preferred data. As depicted in the left of Figure 4, the inclusion of both terms yields the optimal performance, while the absence of either leads to a decrease in performance. In Appendix I, we also show that incorporating \(\mathcal{L}_{\text{NLL}}\) into the

\begin{table}
\begin{tabular}{l c c} \hline \hline  & KIWI-22 & KIWI-XXL & XCOMET \\ \hline Gold Reference & 78.74 & 75.56 & 86.30 \\ WMT Winners & **80.57** & 77.72 & 88.24 \\ TowerInfructr & 80.31 & 77.18 & 88.11 \\ ALMA-13B-LoRA & 79.48 & 76.00 & 87.16 \\ + CPG (Ours, ALMA-13B-K) & 80.55 & **78.97** & **89.74** \\ \hline \hline \end{tabular}
\end{table}
Table 4: The average performance in WMT’23 across all 6 directions, with the highest score highlighted in bold.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline Models & \multicolumn{3}{c}{de} & \multicolumn{3}{c}{c} \(\texttt{c8}\) & \multicolumn{3}{c}{\(\texttt{1}\texttt{.8}\)} \\ \cline{2-10}  & KIWI-22 & KIWI-XXL & XCOMET & KIWI-22 & KIWI-XXL & XCOMET & KIWI-22 & KIWI-XXL & XCOMET \\ \hline Gold Reference & 78.74 & 78.56 & 88.82 & 82.08 & 83.11 & 84.60 & 80.88 & 85.04 & 76.16 \\ WMT Winners & 81.38 & 83.59 & 93.74 & 82.47 & 82.53 & 85.65 & 81.39 & 85.60 & 78.14 \\ GPT-4 & **81.50** & **84.58** & **94.47** & 82.52 & 83.55 & **88.48** & 81.49 & **85.90** & **81.11** \\ ALMA-13B-LoRA & 81.14 & 83.57 & 93.30 & 81.96 & 82.97 & 83.95 & 80.90 & 85.49 & 76.68 \\ + SPT on preferred data & 81.36 & 83.98 & 93.84 & 82.36 & 83.15 & **86.61** & 81.32 & 85.61 & 80.20 \\ + DPO & 81.13 & 83.52 & 93.25 & 81.82 & 82.69 & 83.84 & 80.89 & 85.22 & 76.09 \\ + CPO (Ours, ALMA-13B-R) & **81.50** & 83.97 & 94.20 & **82.63** & **83.75** & **88.03** & **81.57** & 85.73 & 80.49 \\  & \multicolumn{3}{c}{zh} & \multicolumn{3}{c}{tu} & \multicolumn{3}{c}{Avg.} \\ \cline{2-10}  & KIWI-22 & KIWI-XXL & XCOMET & KIWI-22 & KIWI-XXL & XCOMET & KIWI-22 & KIWI-XXL & XCOMET \\ \hline Gold Reference & 77.09 & 74.19 & 90.70 & 80.74 & 79.59 & 88.56 & 79.91 & 80.10 & 85.77 \\ WMT Winners & 77.66 & 73.28 & 87.2 & 81.71 & 80.97 & 90.91 & 80.92 & 81.19 & 87.13 \\ GPT-4 & **79.33** & **77.65** & **92.60** & 81.57 & 81.34 & 90.95 & 81.28 & **82.60** & **89.41** \\ ALMA-13B-LoRA & 77.32 & 74.41 & 89.88 & 81.31 & 81.05 & 89.89 & 80.53 & 81.50 & 86.74 \\ + SFT on preferred data & 78.32 & 76.03 & 90.65 & 81.46 & 81.17 & 90.65 & 80.96 & 81.99 & 88.40 \\ + DPO & 77.50 & 74.50 & 89.94 & 81.19 & 80.88 & 89.76 & 80.51 & 81.36 & 86.58 \\ + CPO (Ours, ALMA-13B-R) & **79.24** & 77.17 & 91.65 & **81.72** & **81.54** & **91.18** & **81.33** & 82.43 & 89.11 \\ \hline \hline \end{tabular}
\end{table}
Table 3: The overall results in xx\(\rightarrow\)en for WMT’21 and WMT’22. The usage of color and boldface are the same in Table 2.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Models for Building Preference Data & KIWI-22 & KIWI-XXL & XCOMET \\ \hline \multicolumn{3}{c}{_Translating to English_ (xx\(\rightarrow\)en)} \\ N/A (ALMA-13B-LoRA baseline) & 80.53 & 81.50 & 86.74 \\ KIWI-XXL & **81.33** & **82.59** & 88.82 \\ XCOMET & 81.27 & 82.33 & **89.17** \\ Ensemble of above (Original) & **81.33** & 82.43 & 89.11 \\ \hline \multicolumn{3}{c}{_Translating from English_ (en\(\rightarrow\)xx)} \\ N/A (ALMA-13B-LoRA baseline) & 82.48 & 82.66 & 92.76 \\ KIWI-XXL & 83.31 & **85.87** & 93.97 \\ XCOMET & 83.09 & 85.43 & **94.09** \\ Ensemble of above (Original) & **83.34** & 85.74 & 94.05 \\ \hline \hline \end{tabular}
\end{table}
Table 5: The influence of employing various reference-free models for creating preference data. The results illustrates that the final performance disparities are minimal whether using solely KIWI-XXL, XCOMET, or their combined ensemble.

DPO loss yields significant improvements.

**Preference Data Components**: Our preference data selection involves choosing preferred and dis-preferred translations from a triplet consisting of outputs from GPT-4, ALMA, and the gold reference. In the right of Figure 4, we emphasize the significance of the data generated by both ALMA and GPT-4. The results indicate a notable decline in performance when ALMA data is excluded in the en\(\rightarrow\)xx direction. Conversely, omitting GPT-4 data leads to a significant performance decrease in the xx\(\rightarrow\)en direction. This demonstrates that data generated by both systems plays a helpful role in enhancing model performance.

### Does The Quality of Dis-preferred Data Matter?

In our experimental setup, dis-preferred data, though originating from strong translation models, receives the lowest scores when compared with two other translation outputs. A pertinent question arises: does the quality of dis-preferred data significantly impact model performance, and can high-quality (albeit imperfect) dis-preferred data aid in translation improvement? To explore this, we constructed a new set of preference data where the dis-preferred translations (\(y_{l}\)) are artificially generated, as opposed to being naturally derived high-quality translations.

In this new dataset, the preferred translation (\(y_{w}\)) remains the best of the three translation candidates, selected in the same manner as in Section 3.1. However, the dis-preferred translation is intentionally modified to be a noised version of \(y_{w}\). We applied random deletions of words with a probability of 0.15 and word swaps within a range of 1 with a probability of 0.3, following the method suggested by Zeng et al. (2023) for creating manually noised dis-preferred data. This approach produces worse translations that are artificial.

Table 6 compares the performance when using these manually noised dis-preferred data versus the original, naturally occurring high-quality dis-preferred data. The results show a substantial decline in performance across all three metrics and both translation directions when the dis-preferred data is manually noised, underscoring the importance of the quality of dis-preferred data in enhancing translation performance.

## 6 Conclusion

In this study, we initially proposed the potential quality issues of gold references in the MT task, highlighting instances where advanced translation models can outperform these references. This finding not only challenges model training via SFT, but also the evaluation procedure that uses reference-based metrics. Subsequently, we introduce Contrastive Preference Optimization, a more efficient variant of of DPO. This method leverages both model-generated and reference data to guide the model in avoiding near-perfect yet flawed translations and learning superior ones. Our developed model, ALMA-13B-R, stands out as the first moderate-size LLM-based translation model to match, and in some cases surpass, the performance of GPT-4 and WMT competition winners, marking a significant advancement in the field of MT.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Dis-Preferred Data & KIWI-22 & KIWI-XXL & XCOMET \\ \hline \multicolumn{4}{c}{_Translating to English_ (xx\(\rightarrow\)en)} \\ Manually Noised & 81.01 & 82.18 & 88.23 \\ Natural (Ours) & **81.33** & **82.43** & **89.11** \\ \hline \multicolumn{4}{c}{_Translating from English_ (en\(\rightarrow\)xx)} \\ Manually Noised & 82.71 & 83.13 & 92.80 \\ Natural (Ours) & **83.34** & **85.74** & **94.05** \\ \hline \hline \end{tabular}
\end{table}
Table 6: An examination of the impact of dis-preferred data quality, contrasting noised data with natural, high-quality translations receiving the lowest scores as dis-preferred data. The findings underscore the importance of the quality of dis-preferred data.

Figure 4: **Left:** an ablation study evaluating the significance of individual components in the CPO loss function, specifically analyzing how the preference learning loss \(\mathcal{L}_{\text{prefer}}\) and the log-likelihood loss \(\mathcal{L}_{\text{NLL}}\) each contribute to enhancing translation performance. **Right:** An ablation study assessing the significance of each component in the translation triplet. By excluding either ALMA or GPT-4 generated data from the preference triplet and re-training the model, we evaluate their respective impacts. The findings highlight the importance of ALMA-generated data for en\(\rightarrow\)xx translations and GPT-4 generated data for xx\(\rightarrow\)en translations.

## Impact Statements

This paper presents work whose goal is to advance the field of Machine Translation. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

## Acknowledgements

We express our profound appreciation to Hieu Hoang, Marcin Junczys-Dowmunt, Huda Khayrallah, Thamme Gowda, Vikas Raunak, Matt Post, Anoop Kunchukuttan, Roman Grundkiewicz, Philipp Koehn, Hany Hassan Awadalla, Arul Menezes, and Vishal Chowdhary for their engaging and valuable discussions that greatly enriched our work. Special thanks to Tom Kocmi for his innovative suggestion to enhance numerical data visibility using a dynamic threshold determined by estimated accuracy. Our gratitude also extends to Pushpendre Rastogi and Joey Hejna for their insightful recommendations on the CPO theory. Furthermore, we acknowledge the Unbabel Team for their valuable advice on incorporating non-COMET metrics into our analysis.

## References

* Almazrouei et al. (2023) Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, E., Heslow, D., Launay, J., Malartic, Q., Noune, B., Pannier, B., and Penedo, G. Falcon-40B: an open large language model with state-of-the-art performance. External Links: 2302.0214 Cited by: SS1.
* T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G., Askell, et al. (2020)Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* T. Chen, S. Kornblith, M. Norouzi, and G. Hinton (2020)A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. Cited by: SS1.
* Y. Chen, Y. Liu, F. Meng, Y. Chen, J. Xu, and J. Zhou (2023)Improving translation faithfulness of large language models via augmenting instructions. arXiv preprint arXiv:2308.12674. Cited by: SS1.
* A. Fan, S. Bhosale, H. Schwenk, Z. Ma, A. El-Kishky, S. Goyal, M. Baines, O. Celebi, G. Wenzek, V. Chaudhary, et al. (2021)Beyond english-centric multilingual machine translation. Journal of Machine Learning Research22, pp. 1-48. Cited by: SS1.
* M. Freitag, N. Mathur, C. Lo, E. Avramidis, R. Rei, B. Thompson, T. Kocmi, F. Blain, D. Deutsch, C. Stewart, C. Zerva, S. Castilho, A. Lavie, and G. Foster (2020)Results of WMT23 metrics shared task: metrics might be guilty but references are not innocent. In Proceedings of the Eighth Conference on Machine Translation, pp. 578-628. Cited by: SS1.
* N. M. Guerreiro, R. Rei, D. van Stigt, L., P. Coheur, P. Colombo, and A. F. Martins (2023)xcomet: transparent machine translation evaluation through fine-grained error detection. arXiv preprint arXiv:2310.10482. Cited by: SS1.
* K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick (2020)Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9729-9738. Cited by: SS1.
* J. Hejna, R. Rafailov, H. Sikchi, C. Finn, S. Niekum, W. B. Knox, and D. Sadigh (2023)Contrastive preference learning: learning from human feedback without rl. arXiv preprint arXiv:2310.13639. Cited by: SS1.
* A. Hendy, M. Abdelrehim, A. Sharaf, V. Raunak, M. Gabr, H. Matsushita, Y. J. Kim, M. Afify, and H. Awadalla (2023)How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210. Cited by: SS1.
* E. J. Hu, S. yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen (2022)LoRA: low-rank adaptation of large language models. In International Conference on Learning Representations, External Links: 2202.08177 Cited by: SS1.
* A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. I. Casas, F. Bressand, G. Lample, L. Saulnier, et al. (2023)Mistral 7b. arXiv preprint arXiv:2310.06825. Cited by: SS1.
* W. Jiao, J. Huang, W. Wang, Z. He, T. Liang, X. Wang, S. Shi, and Z. Tu (2020)ParroT: translating during chat using large language models tuned with human translation and feedback. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 15009-15020. Cited by: SS1.
* W. Jiao, W. Huang, W. Wang, Z. He, T. Liang, X. Wang, S. Shi, and Z. Tu (2020)ParroT: translating during chat using large language models tuned with human translation and feedback. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 15009-15020. Cited by: SS1.
* W. Jiao, W. Wang, J. Huang, X. Wang, and Z. Tu (2023)Is chatopt a good translator? a preliminary study. arXiv preprint arXiv:2301.08745. Cited by: SS1.

* Kocmi et al. (2023) Kocmi, T., Avramidis, E., Bawden, R., Bojar, O., Dvorkovich, A., Federmann, C., Fishel, M., Freitag, M., Gowda, T., Grundkiewicz, R., Haddow, B., Koehn, P., Marie, B., Monz, C., Morishita, M., Murray, K., Nagata, M., Nakazawa, T., Popel, M., Popovic, M., and Shmatova, M. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Koehn, P., Haddow, B., Kocmi, T., and Monz, C. (eds.), _Proceedings of the Eighth Conference on Machine Translation_, pp. 1-42, Singapore, December 2023. Association for Computational Linguistics. URL [https://aclanthology.org/2023.wmt-1.1](https://aclanthology.org/2023.wmt-1.1).
* Kocmi et al. (2024) Kocmi, T., Zouhar, V., Federmann, C., and Post, M. Navigating the metrics maze: Reconciling score magnitudes and accuracies. _arXiv preprint arXiv:2401.06760_, 2024.
* Kudugunta et al. (2023) Kudugunta, S., Caswell, I., Zhang, B., Garcia, X., Choquette-Choo, C. A., Lee, K., Xin, D., Kusupati, A., Stella, R., Bapna, A., and Firat, O. Madlad-400: A multilingual and document-level large audited dataset, 2023.
* Li et al. (2023) Li, J., Zhou, H., Huang, S., Chen, S., and Chen, J. Eliciting the translation ability of large language models via multilingual finetuning with translation instructions. _arXiv preprint arXiv:2305.15083_, 2023.
* Maillard et al. (2023) Maillard, J., Gao, C., Kalbassi, E., Sadagopan, K. R., Goswami, V., Koehn, P., Fan, A., and Guzman, F. Small data, big impact: Leveraging minimal data for effective machine translation. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 2740-2756, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.154. URL [https://aclanthology.org/2023.acl-long.154](https://aclanthology.org/2023.acl-long.154).
* NLLB TEAM et al. (2022) NLLB TEAM, Costa-jussa, M. R., Cross, J., Celebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, J., Licht, D., Maillard, J., et al. No language left behind: Scaling human-centered machine translation. _arXiv preprint arXiv:2207.04672_, 2022.
* Oord et al. (2018) Oord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
* Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* Papineni et al. (2002) Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine translation. In Isabelle, P., Charniak, E., and Lin, D. (eds.), _Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, pp. 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL [https://aclanthology.org/P02-1040](https://aclanthology.org/P02-1040).
* Post (2018) Post, M. A call for clarity in reporting BLEU scores. In _Proceedings of the Third Conference on Machine Translation: Research Papers_, pp. 186-191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL [https://aclanthology.org/W18-6319](https://aclanthology.org/W18-6319).
* Rafailov et al. (2023) Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint arXiv:2305.18290_, 2023.
* Rasley et al. (2020) Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deep-speed: System optimizations enable training deep learning models with over 100 billion parameters. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pp. 3505-3506, 2020.
* Rei et al. (2022) Rei, R., C. de Souza, J. G., Alves, D., Zerva, C., Farinha, A. C., Glushkova, T., Lavie, A., Coheur, L., and Martins, A. F. T. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In _Proceedings of the Seventh Conference on Machine Translation (WMT)_, pp. 578-585, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL [https://aclanthology.org/2022.wmt-1.52](https://aclanthology.org/2022.wmt-1.52).
* Rei et al. (2023) Rei, R., Guerreiro, N. M., Pombal, J., van Stigt, D., Treviso, M., Coheur, L., de Souza, J. G., and Martins, A. F. Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task. _arXiv preprint arXiv:2309.11925_, 2023.
* Robinson et al. (2021) Robinson, J. D., Chuang, C.-Y., Sra, S., and Jegelka, S. Contrastive learning with hard negative samples. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=CR1XQ0QUTh-](https://openreview.net/forum?id=CR1XQ0QUTh-).
* Sellam et al. (2020) Sellam, T., Das, D., and Parikh, A. BLEURT: Learning robust metrics for text generation. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 7881-7892, Online, July 2020. Association for Computational Linguistics.
* Sellam et al. (2020)tics. External Links: Link Cited by: SS2.
* W. Tan, K. Hefferman, H. Schwenk, and P. Koehn (2023)Multilingual representation distillation with contrastive learning. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 1469-1482. Cited by: SS2.
* H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. (2023)Llama: open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Cited by: SS2.
* H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhagava, S. Bhosale, et al. (2023)Llama 2: open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Cited by: SS2.
* A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin (2017)Attention is all you need. Advances in neural information processing systems30. Cited by: SS2.
* Y. Wu and G. Hu (2023)Exploring prompt engineering with GPT language models for document-level machine translation: insights and findings. In Proceedings of the Eighth Conference on Machine Translation, pp. 166-169. Cited by: SS2.
* H. Xu, B. Van Durme, and K. Murray (2021)BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6663-6675. External Links: Link Cited by: SS2.
* H. Xu, Y. J. Kim, A. Sharaf, and H. H. Awadalla (2023)A paradigm shift in machine translation: boosting translation performance of large language models. External Links: 2303.0301 Cited by: SS2.
* L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel (2021)mT5: a massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online, pp. 483-498. External Links: Link Cited by: SS2.
* W. Yang, C. Li, J. Zhang, and C. Zong (2023)Bigtrans: augmenting large language models with multilingual translation capability over 100 languages. arXiv preprint arXiv:2305.18098. Cited by: SS2.
* J. Zeng, F. Meng, Y. Yin, and J. Zhou (2023)TEM: teaching large language models to translate with comparison. arXiv preprint arXiv:2307.04408. Cited by: SS2.
* S. Zhang, Q. Fang, Z. Zhang, Z. Ma, Y. Zhou, L. Huang, M. Bu, S. Gui, Y. Chen, X. Chen, et al. (2023)Bayling: bridging cross-lingual alignment and instruction following through interactive translation for large language models. arXiv preprint arXiv:2306.10968. Cited by: SS2.
* W. Zhu, H. Liu, Q. Dong, J. Xu, L. Kong, J. Chen, L. Li, and S. Huang (2023)Multilingual machine translation with large language models: empirical results and analysis. arXiv preprint arXiv:2304.04675. Cited by: SS2.
* W. Zhu, Y. Lv, Q. Dong, F. Yuan, J. Xu, S. Huang, L. Kong, J. Chen, and L. Li (2023)Extrapolating large language models to non-english by aligning languages. arXiv preprint arXiv:2308.04948. Cited by: SS2.
* D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving (2019)Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593. Cited by: SS2.

## Appendix A Comprehensive Results of WMT'21 And WMT'22

We show the comprehensive results for en\(\rightarrow\)xx In Table 7 and xx\(\rightarrow\)en in Table 8. In this section, our study additionally includes results from recently released LLM-based translators, including Bayling-13B (Zhang et al., 2023), BigTranslate (Yang et al., 2023), ALMA-13B-LoRA (Xu et al., 2023), the zero-shot performances of LLaMA-1-13B (Touvron et al., 2023a) and LLaMA-2-13B (Touvron et al., 2023b). We also compare these with the most advanced current translation models, such as WMT competition winners, GPT-4, GPT-3.5-text-davinci-003, Google Translate, NLLB-3.3B, and MADLAD-10B (Kudugunta et al., 2023). Importantly, we also present the performance of **ALMA-7B-R** here, which is fine-tuning on AMLA-7B-LoRA with CPO method. Except for reference-free evaluation, we also report two commonly used reference-based metrics, sacreBLEU (Post, 2018; Papineni et al., 2002) and COMET-22 (Rei et al., 2022).

**Introducing ALMA-7B-R** In this study, we extend the ALMA-13B-R training methodology to a 7B model size, specifically fine-tuning ALMA-7B-LoRA using the CPO method with the same preference data as ALMA-13B-R. Consistent with our findings from ALMA-13B-R, the application of CPO significantly enhances performance.

**Comparing with Advanced Translation Models** Our model, ALMA-13B-R, is benchmarked against the most advanced current models, demonstrating performance comparable to GPT-4 and WMT winners. It surpasses leading commercial translation tools such as Google Translate in many cases and top multilingual translation models like NLLB, MADLAD-10B and GPT-3.5.

**Stop Using BLEU** BLEU, a metric extensively utilized for decades, often diverges from neural-based and reference-free metrics, a phenomenon also observed in previous studies (Xu et al., 2023; Freitag et al., 2023). For instance, WMT competition winners often exhibit superior performance according to BLEU (or COMET-22), yet this is not corroborated by reference-free models. A case in point is the WMT winners scoring an exceptionally high 64.14 BLEU in cs\(\rightarrow\)en translations, significantly outperforming other models by 20 BLEU points. However, reference-free evaluations suggest these translations are inferior to those generated by our models and GPT-4. We hypothesize that this discrepancy may arise from WMT models being trained on domain-specific data closely related to the WMT test set, leading to high lexical matches but lacking semantic depth as evaluated by neural-based metrics. While BLEU scores are effective for assessing basic functionality in weaker models, their utility diminishes with advanced translation models capable of generating diverse translations. In such contexts, relying solely on BLEU for evaluation appears increasingly outdated.

**Towards Reference-Free Metrics** Neural-based, reference-dependent metrics like COMET-22 demonstrate greater consistency with reference-free metrics and robustness compared to BLEU. For instance, with COMET-22, our models show significant improvements like other reference-free models over ALMA-13B-LoRA and comparable performance to GPT-4, e.g., 87.74 (Ours) vs. 87.68 (GPT-4) when en\(\rightarrow\)xx. However, it is important to note that, according to reference-free metrics, gold references are often inferior to system-generated translations, potentially indicating quality issues in the references that could impact COMET-22 evaluations. Consequently, inconsistencies still exist between COMET-22 and reference-free models like XCOMET. For example, XCOMET rates ALMA-R model on average higher than WMT winners (89.11 vs. 87.13), while COMET-22 favors WMT winners (85.21 vs. 85.60). In line with the recommendations in Freitag et al. (2023), we advocate for the use of reference-free models to circumvent the potential quality issues of references.

[MISSING_PAGE_FAIL:13]

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline  & \multicolumn{6}{c}{\(\mathrm{de}\)} & \multicolumn{6}{c}{\(\mathrm{c\leq 5}\)} \\ \cline{2-10}  & BLEU & COMET-22 & KINW-22 & KINW-XXL & XCOMET & BLEU & COMET-22 & KINW-22 & KINW-XXL & XCOMET \\ \hline Gold Reference & - & - & 78.74 & 78.56 & 88.82 & - & - & 82.08 & 83.11 & 84.60 \\ WMT Winners & 33.34 & 85.04 & 81.38 & 83.59 & 93.74 & **64.14** & **89.00** & 82.47 & 82.53 & 85.65 \\ GPT-4 & 32.41 & 85.35 & **81.50** & **84.58** & **94.47** & 46.86 & 87.26 & 82.52 & 83.55 & **88.48** \\ GPT-3.5-text-davinci-003 & 30.78 & 84.79 & 81.24 & 83.97 & 92.78 & 44.51 & 86.16 & 82.02 & 82.19 & 83.51 \\ Google Translate* & **33.25** & 84.78 & 81.36 & 83.74 & 93.71 & 49.40 & 86.95 & 82.60 & 81.99 & 86.74 \\ NLB-3.3B* & 29.46 & 83.43 & 80.98 & 82.04 & 91.26 & 49.05 & 85.92 & 81.72 & 80.27 & 82.94 \\ MADAD-10B & 32.77 & 84.80 & 81.13 & 83.33 & 93.53 & 51.17 & 87.18 & 82.29 & 82.37 & 86.16 \\ LLMA-1.13B & 52.66 & 82.42 & 75.77 & 77.98 & 58.99 & 56.05 & 81.57 & 77.75 & 70.80 & 73.71 \\ LLMA-2.13B & 31.06 & 83.01 & 79.47 & 79.27 & 91.10 & 40.02 & 83.27 & 79.29 & 74.21 & 78.50 \\ Bayling-13B* & 27.26 & 83.03 & 79.88 & 80.02 & 89.84 & 33.81 & 81.65 & 78.04 & 71.44 & 71.68 \\ BigTranslate & 25.16 & 81.54 & 78.24 & 77.73 & 86.79 & 34.81 & 82.02 & 77.91 & 72.69 & 71.38 \\ \hline ALMA-7B-LoRA & 29.56 & 83.95 & 80.63 & 82.58 & 92.35 & 43.49 & 85.93 & 81.32 & 81.42 & 81.34 \\ + SFT on preferred data & 30.51 & 84.39 & 80.86 & 82.72 & 93.19 & 44.44 & 86.17 & 81.91 & 81.95 & 84.58 \\ + DPO & 29.38 & 84.02 & 80.63 & 82.47 & 92.26 & 42.60 & 85.87 & 81.33 & 81.30 & 81.10 \\ + CPO (Ours, ALMA-7B-R) & 30.52 & **84.61** & 81.13 & 83.11 & **83.85** & 42.92 & 86.29 & 82.16 & 82.29 & 85.76 \\ \hline ALMA-13B-LoRA & 31.14 & 84.56 & 81.14 & 83.57 & 93.30 & 45.28 & 86.47 & 81.96 & 82.97 & 83.95 \\ + SFT on preferred data & 31.80 & 84.83 & 81.36 & 83.98 & 93.84 & 46.17 & 86.83 & 82.36 & 83.15 & 86.67 \\ + DPO & 30.99 & 84.51 & 81.13 & 83.52 & 93.25 & 44.95 & 86.36 & 81.82 & 82.69 & 83.84 \\ + CPO (Ours, ALMA-13B-R) & 30.89 & **84.95** & **81.50** & 83.97 & 94.20 & 44.39 & 86.85 & 82.63 & 83.75 & 88.03 \\ \hline \hline  & \multicolumn{6}{c}{\(\mathrm{i}\)} & \multicolumn{6}{c}{\(\mathrm{i}\)} & \multicolumn{6}{c}{\(\mathrm{i}\)} & \multicolumn{6}{c}{\(\mathrm{i}\)} & \multicolumn{6}{c}{\(\mathrm{i}\)} & \multicolumn{6}{c}{\(\mathrm{i}\)} & \multicolumn{6}{c}{\(\mathrm{i}\)} \\ \cline{2-10}  & BLEU & COMET-22 & KINW-22 & KINW-XXL & XCOMET & BLEU & COMET-22 & KINW-22 & KINW-XXL & XCOMET \\ \hline Gold Reference & - & - & 80.88 & 85.04 & 76.16 & - & - & 77.09 & 74.19 & 90.70 \\ WMT Winners & **41.60** & 86.98 & 81.39 & 85.60 & 78.14 & **33.49** & 81.02 & 77.66 & 73.28 & 87.20 \\ GPT-4 & 41.29 & 87.21 & 81.49 & **85.90** & **81.11** & 23.82 & **82.46** & **79.33** & **77.65** & **92.06** \\ GPT-3.5-text-davinci-003 & 31.88 & 82.13 & 78.72 & 77.53 & 66.44 & 24.98 & 81.62 & 78.91 & 76.64 & 90.92 \\ Google Translate* & - & - & - & - & - & 28.60 & 80.82 & 77.87 & 74.27 & 87.69 \\ NLB-3.3B* & - & - & - & - & - & 21.08 & 76.93 & 75.40 & 68.83 & 84.43 \\ MADAD-10B & 39.49 & 87.06 & 81.40 & 85.52 & 80.43 & 21.29 & 78.53 & 76.72 & 72.10 & 87.12 \\ LLMA-1.3B & 11.01 & 60.82 & 57.76 & 30.38 & 20.87 & 16.81 & 74.32 & 70.93 & 62.37 & 80.13 \\ LLMA-2.13B & 15.77 & 66.35 & 63.91 & 42.75 & 28.03 & 21.81 & 78.10 & 75.09 & 70.31 & 85.68 \\ Bayling-13B* & - & - & - & - & - & 20.10 & 77.72 & 75.08 & 68.32 & 86.51 \\ BigTranslate & 6.45 & 54.65 & 50.55 & 18.77 & 17.44 & 14.94 & 75.11 & 71.94 & 65.25 & 85.00 \\ \hline ALMA-7B-LoRA & 35.64 & 86.09 & 80.57 & 84.65 & 75.02 & 23.64 & 79.78 & 76.81 & 73.65 & 83.94 \\ + SFT on preferred data & 88.58 & 86.47 & 81.09 & 85.23 & 78.87 & 23.19 & 80.50 & 77.74 & 74.91 & 89.81 \\ + DPO & 35.25 & 85.96 & 80.53 & 84.44 & 75.19 & 23.20 & 79.91 & 76.83 & 73.51 & 89.22 \\ + CPO (Ours, ALMA-7B-R) & 86.44 & 86.66 & 81.24 & 85.13 & 79.14 & 22.45 & 80.95 & 78.47 & 87.21 & 89.74 \\ \hline ALMA-13B-LoRA & 95.56 & 86.42 & 80.90 & 85.49 & 76.68 & 25.46 & 80.21 & 77.32 & 74.41 & 89.88 \\ + SFT on preferred data & 99.60 & 86.88 & 81.32 & 85.61 & 80.20 & 24.54 & 81.08 & 78.32 & 76.03 &

## Appendix B Prompts for Translations

Adhering to the prompt format for translation as utilized by Hendy et al. (2023) for GPT models, we employ the same prompt for GPT-4 in our study. Similarly, we use the same prompt employed by Xu et al. (2023) for ALMA models. Prompts are depicted in Figure 5.

## Appendix C Theory

### Proof of The Upper Boundary

**Theorem 1**.: _When \(\pi_{\text{ref}}\) is defined as \(\pi_{w}\), an ideal policy that precisely aligns with the true data distribution of preferred data, the DPO loss \(\mathcal{L}(\pi_{\theta};\pi_{w})+C\) is upper bounded by \(\mathcal{L}(\pi_{\theta};U)\), where \(C\) is a constant._

Proof.: \(\pi_{w}\) represents an ideal policy that perfectly aligns the true data distribution of the preferred data. Hence, for any given data point \((x,y_{w},y_{l})\) from the preference dataset \(\mathcal{D}\), the conditions \(\pi_{w}(y_{w}|x)=1\) and \(0\leq\pi_{w}(y_{l}|x)\leq 1\) hold true. Consequently, under this setup, the predictions for preferred data do not require reweighting by the reference model, and the DPO loss \(\mathcal{L}(\pi_{\theta};\pi_{w})\) can be reformulated as follows :

\[\mathcal{L}(\pi_{\theta};\pi_{w}) =-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\Big{[}\log\sigma \Big{(}\beta\log\frac{\pi_{\theta}(y_{w}|x)}{\pi_{w}(y_{w}|x)}-\beta\log\frac{ \pi_{\theta}(y_{l}|x)}{\pi_{w}(y_{l}|x)}\Big{)}\Big{]}\] \[=-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\Big{[}\log\sigma \Big{(}\beta\log\pi_{\theta}(y_{w}|x)-\beta\log\pi_{\theta}(y_{l}|x)+\beta\log \pi_{w}(y_{l}|x)\Big{)}\Big{]}.\]

After expanding the Sigmoid function, the loss becomes to:

\[\mathcal{L}(\pi_{\theta};\pi_{w})\] \[=-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\Big{[}\log\pi_{ \theta}(y_{w}|x)^{\beta}+\log\pi_{w}(y_{l}|x)^{\beta}-\log\Big{(}\pi_{\theta}(y _{w}|x)^{\beta}\cdot\pi_{w}(y_{l}|x)^{\beta}+\pi_{\theta}(y_{l}|x)^{\beta} \Big{)}\Big{]}.\]

Given that \(\pi_{w}\) is a fixed model and \(\log\pi_{w}(y_{l}|x)^{\beta}\) does not participate in gradient calculations or parameter updates, the above loss function is equivalent when we omit the term \(\log\pi_{w}(y_{l}|x)^{\beta}\). Therefore, optimizing \(\mathcal{L}(\pi_{\theta};\pi_{w})\) is equivalent to

Figure 5: The prompts employed for GPT-4 and ALMA models to perform translations.

optimizing \(\mathcal{L}^{\prime}(\pi_{\theta};\pi_{w})\) as we define below:

\[\mathcal{L}^{\prime}(\pi_{\theta};\pi_{w}) \stackrel{{\Delta}}{{=}}\mathcal{L}(\pi_{\theta};\pi_{ w})+\underbrace{\mathbb{E}_{(x,y_{l})\sim\mathcal{D}}\Big{[}\log\pi_{w}(y_{l}|x)^{ \beta}\Big{]}}_{C\text{ in the Theorem}}\] \[=-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\Big{[}\log\pi_{ \theta}(y_{w}|x)^{\beta}-\log\Big{(}\pi_{\theta}(y_{w}|x)^{\beta}\cdot\pi_{w}( y_{l}|x)^{\beta}+\pi_{\theta}(y_{l}|x)^{\beta}\Big{)}\Big{]}.\]

Considering that \(0\leq\pi_{w}(y_{l}|x)\leq 1\), the loss can be upper bounded as follows:

\[\mathcal{L}^{\prime}(\pi_{\theta};\pi_{w}) \leq-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\Big{[}\log\pi_{ \theta}(y_{w}|x)^{\beta}-\log\Big{(}\pi_{\theta}(y_{w}|x)^{\beta}\cdot 1+\pi_{ \theta}(y_{l}|x)^{\beta}\Big{)}\Big{]}\] \[=-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\Big{[}\log\sigma \Big{(}\beta\log\pi_{\theta}(y_{w}|x)-\beta\log\pi_{\theta}(y_{l}|x)\Big{)} \Big{]}\] \[=\mathcal{L}(\pi_{\theta};U).\]

Therefore, \(\mathcal{L}(\pi_{\theta};\pi_{w})+C\) is upper bounded by \(\mathcal{L}(\pi_{\theta};U)\), where \(C=\mathbb{E}_{(x,y_{l})\sim\mathcal{D}}\Big{[}\log\pi_{w}(y_{l}|x)^{\beta} \Big{]}\).

### BC Regularizer Simplification

The contrastive preference optimization is originally defined as minimizing \(\mathcal{L}(\pi_{\theta};U)\) under the constraint of minimizing the difference between preferred data distribution and outputs of the learnable policy:

\[\min_{\theta}\mathcal{L}(\pi_{\theta},U)\text{ s.t. }\mathbb{E}_{(x,y_{w})\sim \mathcal{D}}\Big{[}\mathbb{KL}(\pi_{w}(y_{w}|x)||\pi_{\theta}(y_{w}|x))\Big{]} <\epsilon.\]

This is equivalent to the following objective via Lagrangian duality:

\[\min_{\theta}\mathcal{L}(\pi_{\theta},U)+\lambda\cdot\mathbb{E}_{(x,y_{w}) \sim\mathcal{D}}\Big{[}\mathbb{KL}(\pi_{w}(y_{w}|x)||\pi_{\theta}(y_{w}|x)) \Big{]},\]

where \(\lambda\) is a hyperparamter and we set to 1. The optimization can be further optimized by expanding the KL divergence:

\[\mathcal{L}_{\text{CPO}} =\mathcal{L}(\pi_{\theta},U)+\mathbb{E}_{(x,y_{w})\sim\mathcal{D} }\Big{[}\mathbb{KL}(\pi_{w}(y_{w}|x)||\pi_{\theta}(y_{w}|x))\Big{]}\] \[=\mathcal{L}(\pi_{\theta},U)+\mathbb{E}_{(x,y_{w})\sim\mathcal{D} }\Big{[}\pi_{w}(y_{w}|x)\cdot\log\Big{(}\pi_{w}(y_{w}|x)\Big{)}-\pi_{w}(y_{w}| x)\cdot\log\Big{(}\pi_{\theta}(y_{w}|x)\Big{)}\Big{]}\] \[=\mathcal{L}(\pi_{\theta},U)+\mathbb{E}_{(x,y_{w})\sim\mathcal{D} }\Big{[}1\cdot 0-1\cdot\log\Big{(}\pi_{\theta}(y_{w}|x)\Big{)}\Big{]}\] \[=\mathcal{L}(\pi_{\theta},U)-\mathbb{E}_{(x,y_{w})\sim\mathcal{D} }\Big{[}\log\Big{(}\pi_{\theta}(y_{w}|x)\Big{)}\Big{]}.\]

This results in the final formulation of our CPO loss function.

## Appendix D Details And Influence of Human-Labeled Preference Data

_TL;DR: Our analysis indicates that our human-labeled data has a relatively minimal impact, probably due to a high proportion of tied translations and potential human bias in the evaluation process._

### Data Construction Details

The human-labeled dataset we used is pair-wise and differs from the triplet format of our main dataset. It focuses exclusively on two language directions, en\(\rightarrow\)de and en\(\rightarrow\)zh, resulting in an additional 2K sentences. The English source sentences, selected from Wikipedia, undergo a filtering process to remove time stamps and URLs. Each sentence is translated using Google Translate and GPT-4, with human evaluators then assigning their preference between these two translations. The distribution of preferences, indicating the number of times translations from Google or GPT-4 were favored or instances where they tied, is detailed in Table 9.

### Influence on Performance

Given that our model operates in a many-to-many translation format and the additional data is specific to only de and zh directions, we anticipate changes in performance when translating into these languages, but not in others. To assess the impact of the human-labeled data, we conducted a comparison between models exclusively fine-tuned on triplet data and those fine-tuned on both triplet and human-labeled data. The training approach remained consistent, utilizing the ALMA-13B-LoRA model fine-tuned via CPO. It's important to note that tied data were excluded from this analysis due to their lack of clear preference.

**Results and Analysis** We show the detailed results for en\(\rightarrow\)xx and xx\(\rightarrow\)en in Table 10 and 11, respectively. The inclusion of human-labeled preference data does not significantly enhance overall translation performance. For en\(\rightarrow\)zh, marginal improvements are observed, though they are minimal. Conversely, for en\(\rightarrow\)de, a slight decline in performance is noted. In summary, the addition of human-labeled data shows no substantial difference in the en\(\rightarrow\)xx direction, and a minor decrease in performance for xx\(\rightarrow\)en on average. We hypothesize that the limited impact of these human-labeled data may stem from a high proportion of tied evaluations and potential human bias in the evaluation process. For instance, there are instances where the author consider GPT-4's translations to be superior, while human evaluators favor those produced by Google Translate.

## Appendix E WMT Winner Systems

### Systems For WMT'21 And WMT'22

The WMT competition winners for each direction as reported in WMT'21 and WMT'22 correspond to those used by Hendy et al. (2023). For more detailed information, we direct readers to this paper.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{3}{c}{de} & \multicolumn{3}{c}{cs} & \multicolumn{3}{c}{is} \\ \cline{2-10}  & KWI-22 & KWI-XXL & XCOMET & KWI-22 & KWI-XXL & XCOMET & KWI-22 & KWI-XXL & XCOMET \\ \hline Only Triplet Data & **83.43** & **84.63** & **97.56** & 84.97 & **87.24** & 93.50 & 82.05 & 85.37 & 91.83 \\ Triplet Data + Human-Labeled Data & 83.28 & 84.25 & 97.48 & **84.99** & 87.06 & **93.61** & **82.18** & **85.68** & **91.93** \\ \hline \multicolumn{10}{c}{zh} & \multicolumn{3}{c}{ru} & \multicolumn{3}{c}{Avg.} \\ \cline{2-10}  & KWI-22 & KWI-XXL & XCOMET & KWI-22 & KWI-XXL & XCOMET & KWI-22 & KWI-XXL & XCOMET \\ \hline Only Triplet Data & 82.15 & 84.08 & 91.59 & **84.05** & **87.43** & **95.26** & 83.33 & **85.75** & 93.95 \\ Triplet Data + Human-Labeled Data & **82.25** & **84.32** & **92.03** & 83.98 & 87.37 & 95.22 & **83.34** & 85.74 & **94.05** \\ \hline \hline \end{tabular}
\end{table}
Table 10: A comparison of translation performance when utilizing solely triplet data versus a combination of triplet data and human-labeled data (our original setup) in the en\(\rightarrow\)xx direction. The **bold** number indicates superior performance. Interestingly, the inclusion of our human-labeled data results in a slight decrease in average performance.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Google Wins & GPT-4 Wins & Ties \\ \hline en\(\rightarrow\)de & 418 & 435 & 203 \\ en\(\rightarrow\)zh & 362 & 412 & 282 \\ \hline \hline \end{tabular}
\end{table}
Table 9: The statistic of how many translations win or tie by each system evaluated by human.

### Systems For WMT'23

For the de+en and zh+en language pairs, we selected the translation systems that attained the highest human rankings based on source-based Direct Assessment and Scalar Quality Metrics (DA+SQM). For de+ru, in the absence of human rankings for these directions in Kocmi et al. (2023), we opted for the model with the highest COMET-22 scores as reported in Kocmi et al. (2023). Details about these models are available in Table 12.

## Appendix F Estimated Accuracy with Human Agreements

In the paper, we adopt a new approach for highlighting improvements within tables, moving beyond the standard practice of specifying a static improvement threshold in metric \(y\) by score \(x\). Instead, our threshold is dynamic, calibrated to the minimal metric difference \(x\) in metric \(y\) that yields a perceptible distinction between two systems as recognized by humans (Kocmi et al., 2024). For instance, to align with human judgments at an 80% concordance rate, the required improvement margin is \(\geq 1.24\) for both KWI-XXL and COMET-XXL, and \(\geq 0.53\) for KWI-22. A comprehensive delineation of these thresholds can be found in Table 13.

## Appendix G Full Results of WMT'23

The comprehensive results of WMT'23 are presented in Table 14. Similar to its performance in WMT'21 and WMT'22, ALMA-13B-R performs best on average among the SoTA translation models.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**Estimated Accuracy** & 
\begin{tabular}{c} **Coin toss** \\ **50\%** \\ \end{tabular} & **55\%** & **60\%** & **65\%** & **70\%** & **75\%** & **80\%** & **85\%** & **90\%** & **95\%** \\ \hline BLEU & 0.27 & 0.52 & 0.78 & 1.06 & 1.39 & 1.79 & 2.34 & 3.35 & - & - \\ Comet-22 & 0.03 & 0.10 & 0.18 & 0.26 & 0.35 & 0.45 & 0.56 & 0.71 & 0.94 & 1.53 \\ KWI-22 & 0.01 & 0.08 & 0.16 & 0.24 & 0.33 & 0.42 & 0.53 & 0.67 & 0.85 & 1.18 \\ XCOMET-XXL & 0.02 & 0.19 & 0.37 & 0.56 & 0.76 & 0.98 & 1.24 & 1.55 & 1.99 & 2.74 \\ KWI-XXL & 0.06 & 0.22 & 0.39 & 0.57 & 0.77 & 0.98 & 1.24 & 1.58 & 2.08 & 3.39 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Thresholds and estimated accuracies for each metric used in our paper.

\begin{table}
\begin{tabular}{l l} \hline \hline Systems & Language Pair \\ \hline ONLINE-B & en-de \\ ONLINE-A & de-en \\ Lan-BridgeMT (Wu \& Hu, 2023) & en-zh \\ Lan-BridgeMT (Wu \& Hu, 2023) & zh-en \\ ONLINE-G & en-ru \\ ONLINE-Y & ru-en \\ \hline \hline \end{tabular}
\end{table}
Table 12: The list of WMT’23 winners served for each language direction.

## Appendix H Evaluation on ALMA-R with Non-Comet Metric

Concerns may arise regarding the similar training procedure of COMET metrics, leading to high correlation among COMET models, which potentially undermine the validity of our analysis in Section 5.1. To address this, we also consider BLEURT-20 (Sellam et al., 2020), a non-COMET and neural-based (but reference-based evaluation) metric. We present BLEURT scores for ALMA-13B-LoRA and ALMA-13B-R in Table 15. Notably, even when preference data is constructed using COMET-based evaluations, significant improvements in non-COMET scores are observed. This strengthens our findings that translations produced by ALMA-R are indeed superior and robust.

## Appendix I The Effectiveness of The BC Regularizer for DPO

The DPO loss \(\mathcal{L}_{\text{DPO}}=\mathcal{L}(\pi_{\theta},\pi_{\text{ref}})\) can also be utilized by adding our additional BC regularizer:

\[\min_{\theta}\mathcal{L}(\pi_{\theta},\pi_{\text{ref}})-\mathbb{E}_{(x,y_{w} )\sim\mathcal{D}}\Big{[}\log\Big{(}\pi_{\theta}(y_{w}|x)\Big{)}\Big{]}.\]

In Table 16, we demonstrate that incorporating \(\mathcal{L}_{\text{NLL}}\) into the DPO objective results in notable enhancements for translations both to and from English. This observation hints at why \(\mathcal{L}\)prefer, as an approximation of \(\mathcal{L}_{\text{DPO}}\), performs effectively, while the original DPO loss does not. It appears that the DPO loss lacks the BC regularizer, which steers the model towards the preferred data distribution. Although combining DPO with the BC regularizer could yield similar performance to CPO, it incurs double the memory cost and FLOPs per token in the forward pass. The original DPO loss shows the possibility of failure to improve the model performance in preference learning, so we here highlight the significance of incorporating BC regularization. Importantly, Table 16 shows that \(\mathcal{L}_{\text{prefer}}\) is a successful approximation of the DPO loss, offering savings in memory and speed, and it can even outperform the original BC-regularized DPO loss \(\mathcal{L}_{\text{DPO}}+\mathcal{L}_{\text{NLL}}\).

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{de\(\rightarrow\)en} & \multicolumn{3}{c}{zh\(\rightarrow\)en} & \multicolumn{3}{c}{ru\(\rightarrow\)en} \\ \cline{2-9}  & KIVI-22 & KIVI-XXL & XCOMET & KIVI-22 & KIVI-XXL & XCOMET & KIVI-22 & KIVI-XXL & XCOMET \\ \hline Gold Reference & 78.93 & 75.96 & 84.23 & 74.46 & 68.80 & 83.51 & 79.46 & 77.84 & 83.60 \\ WMT Winners & 79.37 & 76.18 & 84.35 & **80.17** & **79.53** & 92.25 & 80.88 & 79.21 & 86.22 \\ TowerInstruct & 79.67 & 77.60 & 86.28 & 79.84 & 78.13 & 91.75 & 80.85 & 80.03 & 87.76 \\ MADLAD-10B & 78.52 & 75.50 & 83.85 & 77.68 & 73.72 & 88.07 & 79.65 & 77.58 & 85.15 \\ ALMA-13B-LoRA & 79.36 & 76.79 & 85.07 & 78.83 & 76.71 & 90.73 & 80.79 & 80.14 & 86.94 \\ + CPO (Ours, ALMA-13B-R) & **79.87** & **77.69** & **86.62** & **80.01** & **88.42** & **92.36** & **81.11** & **80.95** & **88.75** \\ \hline  & \multicolumn{3}{c}{en\(\rightarrow\)2h} & \multicolumn{3}{c}{en\(\rightarrow\)ru} & \multicolumn{3}{c}{en\(\rightarrow\)ru} \\ \cline{2-9}  & KIVI-22 & KIVI-XXL & XCOMET & KIVI-22 & KIVI-XXL & XCOMET & KIVI-22 & KIVI-XXL & XCOMET \\ \hline Gold Reference & 80.12 & **77.93** & 88.91 & 79.60 & 73.47 & 86.15 & 79.87 & 79.36 & 91.41 \\ WMT Winners & **80.80** & 77.26 & 87.94 & 79.70 & 74.20 & 87.24 & **82.51** & 79.95 & 91.41 \\ TowerInstruct & 80.13 & 75.34 & 86.55 & 80.03 & 74.85 & 86.74 & 81.33 & 77.14 & 89.59 \\ MADLAD-10B & 77.48 & 70.87 & 86.18 & 74.63 & 62.07 & 79.12 & 79.24 & 72.40 & 86.64 \\ ALMA-13B-LoRA & 78.79 & 73.40 & 85.61 & 78.92 & 72.95 & 85.13 & 80.21 & 76.02 & 89.48 \\ + CPO (Ours, ALMA-13B-R) & 79.85 & 77.05 & 89.79 & 80.48 & 78.17 & 88.34 & 81.97 & 81.52 & 92.56 \\ \hline \hline \end{tabular}
\end{table}
Table 14: The full results of WMT’23. The highest score among all systems are bold. (darkblue boxes) indicates that the improvement over the original ALMA model achieves _at least_ 80% estimated accuracy with the human judgement (Kocmi et al., 2024), while the lesser improvements are highlighted in shallow blue boxes.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline BLEURT-20 & de & cs & is & zh & ru & Avg. \\ \hline \multicolumn{8}{c}{_Translating to English (xx\(\rightarrow\)en)_} \\ \multicolumn{8}{c}{ALMA-13B-LoRA} & 73.20 & 76.65 & 75.87 & 67.37 & 76.7 & 73.96 \\ ALMA-13B-R & **73.62** & **76.94** & **76.98** & **69.48** & **76.91** & **74.79** \\ \hline \multicolumn{8}{c}{_Translating from English (en\(\rightarrow\)xx)_} \\ \multicolumn{8}{c}{ALMA-13B-LoRA} & 75.51 & 80.93 & 73.19 & 70.54 & 74.94 & 75.02 \\ ALMA-13B-R & **77.20** & **81.87** & **73.43** & **71.51** & **76.19** & **76.04** \\ \hline \hline \end{tabular}
\end{table}
Table 15: The BLEURT-20 score comparison between ALMA-13B-LoRA and ALMA-13B-R

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Loss Objective & KIWI-22 & KIWI-XXL & XCOMET & Memory Cost & FLOPs/tok \\ \hline \multicolumn{5}{c}{_Translating to English_ (xx\(\rightarrow\)en)} \\ \(\mathcal{L}_{\text{DPO}}\) & 80.51 & 81.36 & 86.58 & 2\(\times\) & 2\(\times\) \\ \(\mathcal{L}_{\text{DPO}}+\mathcal{L}_{\text{NLL}}\) & 81.28 & 82.42 & 89.05 & 2\(\times\) & 2\(\times\) \\ \(\mathcal{L}_{\text{prefer}}+\mathcal{L}_{\text{NLL}}\) (CPO) & **81.33** & **82.43** & **89.11** & 1\(\times\) & 1\(\times\) \\ \hline \multicolumn{5}{c}{_Translating from English_ (en\(\rightarrow\)xx)} \\ \(\mathcal{L}_{\text{DPO}}\) & 82.27 & 82.07 & 92.25 & 2\(\times\) & 2\(\times\) \\ \(\mathcal{L}_{\text{DPO}}+\mathcal{L}_{\text{NLL}}\) & 83.13 & 84.74 & 93.53 & 2\(\times\) & 2\(\times\) \\ \(\mathcal{L}_{\text{prefer}}+\mathcal{L}_{\text{NLL}}\) (CPO) & **83.34** & **85.74** & **94.05** & 1\(\times\) & 1\(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 16: The impact of applying \(\mathcal{L}_{\text{NLL}}\) to the original DPO loss.
{
    "2309.05463": {
        "paper_id": "2309.05463",
        "abs_url": "https://arxiv.org/abs/2309.05463",
        "pdf_url": "https://arxiv.org/pdf/2309.05463.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2309.05463_Textbooks_Are_All_You_Need_II_phi-15_technical_report.pdf",
        "title": "Textbooks Are All You Need II: phi-1.5 technical report",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Yuanzhi Li",
            "S\u00e9bastien Bubeck",
            "Ronen Eldan",
            "Allie Del Giorno",
            "Suriya Gunasekar",
            "Yin Tat Lee"
        ],
        "abstract": "We continue the investigation into the power of smaller Transformer-based language models as initiated by \\textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality\" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need\" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \\textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step\" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \\textbf{phi-1.5} to promote further research on these urgent topics.",
        "comments": "",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/textbooks-are-all-you-need-ii-phi-1-5",
        "bibtex": "@misc{li2023textbooks,\n      title={Textbooks Are All You Need II: phi-1.5 technical report}, \n      author={Yuanzhi Li and S\u00e9bastien Bubeck and Ronen Eldan and Allie Del Giorno and Suriya Gunasekar and Yin Tat Lee},\n      year={2023},\n      eprint={2309.05463},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}
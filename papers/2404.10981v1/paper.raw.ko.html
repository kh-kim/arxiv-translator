<html lang="en" data-theme="dark"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>A Survey on Retrieval-Augmented Text Generation for Large Language Models</title>
<!--Generated on Wed May  1 15:57:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons_new.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2404.10981v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2404.10981v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="Dark mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2404.10981v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2404.10981v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S1" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>RAG Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS1" title="In 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Basic RAG Workflow</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS1.SSS1" title="In 2.1 Basic RAG Workflow ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Indexing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS1.SSS2" title="In 2.1 Basic RAG Workflow ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS1.SSS3" title="In 2.1 Basic RAG Workflow ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2" title="In 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>RAG Paradigm</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS1" title="In 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Pre-Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS1.Px1" title="In 2.2.1 Pre-Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Indexing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS1.Px2" title="In 2.2.1 Pre-Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Query Manipulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS1.Px3" title="In 2.2.1 Pre-Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Data Modification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS2" title="In 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS2.Px1" title="In 2.2.2 Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Search &amp; Ranking</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS3" title="In 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Post-Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS3.Px1" title="In 2.2.3 Post-Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Re-Ranking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS3.Px2" title="In 2.2.3 Post-Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Filtering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS4" title="In 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.4 </span>Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS4.Px1" title="In 2.2.4 Generation ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Enhancing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS4.Px2" title="In 2.2.4 Generation ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Customization</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S3" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Pre-Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S3.SS1" title="In 3 Pre-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Indexing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S3.SS2" title="In 3 Pre-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Query Manipulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S3.SS3" title="In 3 Pre-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Data Modification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S4" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S4.SS1" title="In 4 Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Search &amp; Ranking</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S5" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Post-Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S5.SS1" title="In 5 Post-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Re-Ranking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S5.SS2" title="In 5 Post-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Filtering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S6" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Generation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S6.SS1" title="In 6 Generation ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Enhancing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S6.SS2" title="In 6 Generation ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Customization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S7" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Comparisons of RAG</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S7.SS1" title="In 7 Comparisons of RAG ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>The Comprehensive Summary of RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S7.SS2" title="In 7 Comparisons of RAG ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Retriever and Generator</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S8" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Evaluation in RAG</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S8.SS1" title="In 8 Evaluation in RAG ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Retrieval-based Aspect</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S8.SS2" title="In 8 Evaluation in RAG ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Generation-based Aspect</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Future Directions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS1" title="In 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1 </span>Retrieval Quality</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS1.SSS0.Px1" title="In 9.1 Retrieval Quality ‣ 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Differentiable Search Indices</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS1.SSS0.Px2" title="In 9.1 Retrieval Quality ‣ 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Generative Models for Search</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS1.SSS0.Px3" title="In 9.1 Retrieval Quality ‣ 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Fine-tuning Pre-trained Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS1.SSS0.Px4" title="In 9.1 Retrieval Quality ‣ 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Noise Power</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS2" title="In 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2 </span>Multimodal RAG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S10" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S11" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div class="package-alerts ltx_document" role="status" aria-label="Conversion errors have been found">
      <button aria-label="Dismiss alert" onclick="closePopup()">
          <span aria-hidden="true"><svg role="presentation" width="20" height="20" viewBox="0 0 44 44" aria-hidden="true" focusable="false">
          <path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
          <path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
          </svg></span>
      </button>
      <p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
          <ul arial-label="Unsupported packages used in this paper">
              <li>failed: inconsolata</li>,<li>failed: tabularray</li>,<li>failed: forest</li>
          </ul>
      <p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
    </div><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: arXiv.org perpetual non-exclusive license</a><div id="watermark-tr">arXiv:2404.10981v1 [cs.IR] 17 Apr 2024</div></div>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">A Survey on Retrieval-Augmented Text Generation for Large Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yizheng Huang 
<br class="ltx_break">York University 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">hyz@yorku.ca</span>
<br class="ltx_break"><span class="ltx_ERROR undefined" id="id2.2.id2">\And</span>Jimmy X. Huang 
<br class="ltx_break">York University 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">jhuang@yorku.ca</span>
<br class="ltx_break">
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id4.id1">검색-증강 생성(retrieval-Augmented Generation, RAG)은 최신 외부 정보의 동적 통합을 가능하게 하여 대용량 언어 모델(LLM)의 정적 한계를 해결하기 위해 검색 방법을 딥러닝 발전과 병합한다. 주로 텍스트 도메인에 초점을 맞춘 이 방법론은 LLM에 의한 그럴듯하지만 잘못된 응답 생성에 대한 비용 효율적인 솔루션을 제공하여 실제 데이터를 사용하여 출력의 정확성과 신뢰성을 향상시킨다. 본 논문은 RAG의 복잡성이 증가하고 성능에 영향을 미칠 수 있는 여러 개념을 통합함에 따라 RAG 패러다임을 검색 전, 검색 후, 생성의 네 가지 범주로 구성하여 검색 관점에서 세부적인 관점을 제공한다. 그것은 RAG의 진화에 대한 개요를 설명하고 중요한 연구의 분석을 통해 현장의 진행에 대해 논의한다. 또한, RAG에 대한 평가 방법을 소개하고, 당면한 과제를 해결하고 향후 연구 방향을 제안한다. 조직화된 프레임워크와 범주화를 제공함으로써 RAG에 대한 기존 연구를 통합하고 기술 기반을 명확히 하며 LLM의 적응성과 응용 가능성을 강조하는 것을 목표로 한다.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">A Survey on Retrieval-Augmented Text Generation for Large Language Models</span></p>
<br class="ltx_break ltx_centering">
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Yizheng Huang</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">York University</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.3.3.1.1">hyz@yorku.ca</span></span></span>
</span>
</span></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_text ltx_inline-block" id="p1.1.2.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.2.1.1.1.1.1">Jimmy X. Huang</span></span></span>
<span class="ltx_tr" id="p1.1.2.2.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.2.2.1">York University</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.2.1.3.3.1.1">jhuang@yorku.ca</span></span></span>
</span>
</span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<br class="ltx_break ltx_centering">
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">ChatGPT의 등장은 대화형 능력과 광범위한 응용으로 인해 학계와 산업계 모두에 상당한 영향을 미쳤으며, 선도적인 인공지능 도구 <cite class="ltx_cite ltx_citemacro_citep">(Laskar et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib57" title="">2023</a>; Jahan et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib45" title="">2023</a>; Huang and Huang, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib40" title="">2024</a>)</cite>로 자리매김했다. ChatGPT의 핵심에는 대형 언어 모델(LLM) GPT-4가 있는데, <cite class="ltx_cite ltx_citemacro_citep">(OpenAI et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib76" title="">2023</a>)</cite>에 자세히 설명되어 있으며, 이는 이전 작업보다 많은 향상을 보임으로써 다양한 자연어 처리(NLP) 작업 <cite class="ltx_cite ltx_citemacro_citep">(Laskar et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib58" title="">2020</a>)</cite>에서 탁월한 능력을 보여준다. 이러한 발전에도 불구하고 LLM의 채택은 주로 광범위한 데이터 세트에 대한 의존으로 인해 몇 가지 중요한 문제를 강조했다. 이러한 의존은 교육 후 새로운 정보를 통합할 수 있는 능력을 제한하여 세 가지 주요 과제로 이어진다. 첫째, 접근성과 적용성을 극대화하기 위한 광범위하고 일반적인 데이터에 초점을 맞추면 전문 영역에서 하위 성능의 결과가 나타난다. 둘째, 데이터 주석 및 모델 학습에 필요한 중요한 리소스와 결합된 온라인 데이터의 빠른 생성은 LLM의 업데이트 상태를 유지하는 능력을 방해한다. 셋째, LLM은 "환각"으로 알려진 설득력이 있지만 부정확한 응답을 생성하는 데 취약하여 사용자를 오도할 수 있다.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="70" id="S1.F1.g1" src="https://arxiv.org/html/2404.10981v1/extracted/2404.10981v1/RAG_example.png" width="281">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">그림 1:</span>RAG 혜택의 예시 ChatGPT는 학습 데이터의 범위를 넘어서는 답변할 수 없는 질문을 해결하고 올바른 결과를 생성한다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">이러한 문제를 해결하는 것은 LLM이 다양한 도메인에 걸쳐 효과적으로 활용되기 위해 중요하다. 유망한 해결책은 검색-증강 생성(RAG: Retrieval-Augmented Generation) 기술의 통합이며, 이는 질의에 응답하여 외부 데이터를 가져와 보다 정확하고 현재 출력을 보장함으로써 모델을 보완한다. 그림 <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>는 RAG가 ChatGPT가 초기 학습 데이터를 넘어 정확한 답변을 제공할 수 있도록 하는 방법을 보여준다.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">2020년 Lewis et al. <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>)</cite>에 의해 도입된 이후, RAG 기술은 상당한 발전을 거쳤으며, 특히 ChatGPT의 성공에 영향을 받았다. 그러나 RAG의 메커니즘에 대한 철저한 분석과 후속 연구에 의한 진행에 관한 문헌에는 눈에 띄는 격차가 있다. 또한, 이 분야는 다양한 연구 집중과 유사한 방법에 대한 모호한 용어의 사용으로 인해 혼란을 초래하는 것이 특징이다. 본 논문은 RAG에 대한 구조화된 개요를 제공하고 다양한 방법을 분류하고 이 연구 영역에 대한 심층적인 이해를 제공함으로써 이러한 측면을 명확히 하는 것을 목표로 한다. 이 조사는 주로 RAG의 텍스트 적용에 초점을 맞출 것이며, 이는 이 분야에서 연구 노력의 현재 강조를 반영한다.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">RAG는 검색 방법과 고급 딥러닝을 결합하여 관련 정보를 효과적으로 검색하고 정확한 응답을 생성하는 두 가지 주요 질문을 해결한다. RAG의 워크플로우는 섹션 2에 요약되어 방법론을 사전 검색, 검색, 사후 검색 및 생성 단계로 분류한다. 이 섹션은 3에서 6까지 이러한 단계 내의 기술에 대한 심층 분석을 제공한다. 섹션 7은 사용된 검색기 및 생성기와 함께 검토된 연구의 요약을 제공한다. 섹션 8은 RAG에 대한 평가 방법론을 자세히 설명한다. 9절에서는 텍스트 기반 연구에 집중하고 이미지 및 다중 모드 데이터 고려 사항으로 확장하여 향후 연구 방향을 탐구한다. 그 결론은 제10절에서 제시된다.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">본 논문의 공헌은 다음과 같다. 본 논문은 RAG 영역을 이해하기 위한 포괄적인 프레임워크를 제공하고 개선 영역과 향후 연구를 위한 과제를 식별한다. 그것은 RAG의 핵심 기술에 대한 상세한 분석을 제공하여 검색과 생성을 다루는 데 있어 그들의 강점을 조사한다. 또한 RAG 연구에 사용된 평가 방법을 소개하고 현재 과제를 강조하며 향후 연구에 대한 유망한 방향을 제시한다.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="246" id="S1.F2.g1" src="https://arxiv.org/html/2404.10981v1/extracted/2404.10981v1/RAG_framework.png" width="598">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">그림 2:</span>기본 워크플로 및 패러다임을 가진 통합 RAG 프레임워크.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>RAG Framework</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">환각은 주로 LLM이 최신 정보에 접근할 수 없기 때문이다. 이러한 한계는 모델이 훈련 데이터 세트에 의존하는 것에서 비롯된다. RAG는 LLM의 학습 데이터에 외부 소스의 현재 정보를 검색 모델을 통해 보완함으로써 정확한 응답 생성을 가능하게 함으로써 이 문제에 대한 해결책을 제시한다. RAG는 일반적으로 LLM에 필요한 광범위한 훈련 및 미세 조정 프로세스에 대한 보다 비용 효율적인 대안을 제시한다. 이 새로운 데이터를 LLM에 직접 통합할 필요 없이 전통적인 검색 방법 또는 사전 훈련된 LM을 통해 새로운 정보를 동적으로 통합할 수 있다. 이 기능은 RAG를 유연하고 확장 가능하게 만들어 다양한 목적을 위해 다양한 LLM에 걸쳐 적용을 용이하게 한다. RAG를 통해 검색된 정보는 인간이 저작한 실제 데이터에서 파생되며, 이는 생성 프로세스를 단순화할 뿐만 아니라 생성된 응답의 신뢰성을 증가시킨다. 그림 <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>는 기본 워크플로 및 패러다임이 있는 통합 RAG 프레임워크를 나타낸다.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Khandelwal 등의 연구 <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>)</cite>는 훈련 데이터셋 자체에서 관련 정보에 접근하는 것이 LLM 성능을 크게 향상시켜 RAG의 효과를 부각시킬 수 있음을 보여준다. 시간이 지남에 따라 RAG는 보충 정보를 제공하는 수단에서 검색 및 생성 구성 요소 간의 다중 상호 작용을 가능하게 하는 수단으로 발전했다. 이것은 검색된 정보의 정확도를 정제하고 생성된 출력의 품질을 반복적으로 개선하기 위해 여러 라운드의 검색을 수행하는 것을 포함한다. LangChain<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.langchain.com</span></span></span> 및 LlamaIndex<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.llamaindex.ai</span></span></span>과 같은 플랫폼은 RAG 접근법을 모듈화하여 적응성을 높이고 응용 범위를 확장했다. 여러 검색 반복에서 반복 생성에 이르기까지 RAG의 다양한 측면을 다루기 위해 다양한 방법론을 사용하는 이러한 플랫폼에도 불구하고 기본 RAG 워크플로우를 준수합니다. 이러한 일관성은 운영을 이해하고 추가 개발을 위한 기회를 찾는 데 중요하다.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Basic RAG Workflow</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">RAG의 기본 워크플로우는 외부 소스를 포함하는 인덱스를 만드는 것으로 시작합니다. 이 인덱스는 특정 쿼리를 기반으로 하는 리트리버 모델을 통해 관련 정보를 검색할 수 있는 기반이 된다. 최종 단계는 생성기 모델을 포함하며, 생성기 모델은 검색된 정보를 질의와 결합하여 원하는 출력을 생성한다.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Indexing</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">효율적인 검색은 포괄적인 인덱싱으로 시작되며, 여기서 데이터 준비가 핵심이다. 이 단계는 <cite class="ltx_cite ltx_citemacro_citep">(Manning et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib72" title="">2008</a>)</cite>를 색인하기 위한 텍스트의 적합성을 높이기 위해 토큰화, 형태소 분석, 불용어 제거 등의 텍스트 정규화 과정을 포함한다. 그런 다음 텍스트 세그먼트는 보다 집중된 검색을 용이하게 하기 위해 문장 또는 단락으로 조직되어 관련 키워드를 포함하는 세그먼트를 정확하게 찾아낼 수 있다. 딥러닝의 통합은 텍스트의 의미 벡터 표현을 생성하기 위한 사전 훈련된 LM의 사용을 통해 인덱싱에 혁명을 일으켰다. 이러한 벡터는 저장되므로 광범위한 데이터 모음에서 신속하고 정확한 검색을 가능하게 하여 검색 효율성을 크게 향상시킨다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Retrieval</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">BM25 알고리즘 <cite class="ltx_cite ltx_citemacro_citep">(Hancock-Beaulieu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib33" title="">1996</a>)</cite>와 같은 전통적인 검색 방법은 문서 순위를 위한 용어 빈도와 존재에 초점을 맞추지만, 종종 질의의 의미 정보를 간과한다. 현재 전략은 BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib22" title="">2019</a>)</cite>와 같은 사전 훈련된 LMs를 활용하여 쿼리의 의미적 본질을 보다 효과적으로 포착한다. 이 모델들은 동의어와 어구의 구조를 고려하여 검색 정확도를 향상시키며, 이를 통해 의미적 유사도 검출을 통해 문서 순위를 정제한다. 이것은 일반적으로 문서들과 쿼리들 사이의 벡터 거리들을 측정함으로써 달성되며, 전통적인 검색 메트릭들을 의미론적 이해와 결합하여 관련되고 사용자 의도와 정렬된 검색 결과들을 산출한다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Generation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">생성 단계는 질의와 관련이 있고 검색된 문서에서 발견된 정보를 반영하는 텍스트를 생성하는 작업을 수행한다. 통상적인 방법은 질의를 검색된 정보와 연결시키는 것을 포함하며, 이는 텍스트 생성 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib62" title="">2022</a>)</cite>를 위해 LLM에 공급된다. 생성된 텍스트와 검색된 콘텐츠의 정렬 및 정확성을 보장하는 것은 과제를 제시하지만, 소스 자료에 밀접하게 밀착하는 것과 결과물을 창의성으로 주입하는 것 사이의 균형을 맞추는 것도 필수적이다. 생성된 텍스트는 검색된 문서에서 정보를 정확하게 전달하고 쿼리의 의도와 일치해야 하는 동시에 검색된 데이터 내에 명시적으로 포함되지 않은 새로운 통찰력 또는 관점을 도입할 수 있는 유연성을 제공해야 한다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>RAG Paradigm</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">RAG 패러다임은 도메인 내에서 연구를 조직하여 LLM 성능을 향상시키기 위한 간단하면서도 강력한 프레임워크를 제공한다. RAG의 핵심은 고품질 결과를 생성하는 데 중요한 검색 메커니즘이다. 따라서 이 패러다임은 검색의 관점에서 사전 검색, 검색, 사후 검색, 생성의 네 가지 주요 단계로 구조화된다. 반복적인 검색 생성 주기를 포함하는 단일 홉 및 다중 홉 검색 접근법 모두 이 4단계 구조를 따른다. <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.F3" title="Figure 3 ‣ Customization ‣ 2.2.4 Generation ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>는 RAG의 핵심 기술의 분류 트리이다.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Pre-Retrieval</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">검색 증강 생성의 사전 검색 단계는 성공적인 데이터 및 쿼리 준비를 위한 기초를 제공하여 효율적인 정보 검색을 보장한다. 이 단계에는 효과적인 데이터 액세스를 준비하는 필수 작업이 포함됩니다.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Indexing</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p1.1">프로세스는 인덱싱으로 시작되며, 인덱싱은 정보를 빠르고 정확하게 검색할 수 있도록 조직화된 시스템을 구축한다. 인덱싱의 특수성은 작업 및 데이터 유형에 따라 다릅니다. 예를 들어, 문장-레벨 인덱싱은 질문-답변 시스템들이 정확하게 답변들을 찾는데 유익한 반면, 문서-레벨 인덱싱은 그들의 주요 개념들 및 아이디어들을 이해하기 위해 문서들을 요약하는데 더 적절하다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Query Manipulation</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p1.1">인덱싱 후, 인덱싱된 데이터와의 더 나은 매칭을 위해 사용자 쿼리들을 조정하기 위해 쿼리 조작이 수행된다. 여기에는 사용자의 의도에 더 가깝게 정렬하기 위해 쿼리를 재작성하는 쿼리 재구성 <cite class="ltx_cite ltx_citemacro_citep">(Jansen et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib46" title="">2009</a>; Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib110" title="">2020</a>)</cite>, 동의어 또는 관련 용어를 통해 더 관련된 결과를 캡처하기 위해 쿼리를 확장하는 쿼리 확장 <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib37" title="">2013</a>)</cite>, 일관된 쿼리 매칭을 위해 철자 또는 용어의 차이를 해결하는 쿼리 정규화가 포함된다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Data Modification</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p1.1">데이터 수정은 검색 효율성을 높이는 데에도 중요합니다. 이 단계에는 결과의 품질을 향상시키기 위해 관련 없는 정보 또는 중복 정보를 제거하고 검색된 콘텐츠의 관련성 및 다양성을 높이기 위해 메타데이터와 같은 추가 정보로 데이터를 풍부하게 하는 전처리 기술이 포함된다.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Retrieval</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Search &amp; Ranking</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS2.Px1.p1.1">검색 단계는 검색과 순위의 조합이다. 생성 모델의 출력 품질을 향상시키기 위해 데이터 세트에서 문서를 선택하고 우선순위를 지정하는 데 중점을 둡니다. 이 단계에서는 검색 알고리즘을 사용하여 인덱싱된 데이터를 탐색하여 사용자의 쿼리와 일치하는 문서를 찾습니다. 관련 문서들을 식별한 후, 이들 문서들을 초기에 순위화하는 프로세스는 질의에 대한 그들의 관련성에 따라 정렬하기 시작한다.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Post-Retrieval</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">검색 후 단계는 초기에 검색된 문서를 정제하여 텍스트 생성의 품질을 향상시키는 역할을 한다. 이 단계는 최종 생성 작업을 위해 문서 선택을 최적화하는 것을 목표로 하는 재순위화 및 필터링으로 구성된다.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">Re-Ranking</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS3.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS3.Px1.p1.1">재순위화 단계에서, 이전에 검색된 문서들은 재평가되고, 점수가 매겨지고, 재구성된다. 목적은 질의와 가장 관련성이 높은 문서를 보다 정확하게 강조하여 관련성이 낮은 문서의 중요성을 줄이는 것이다. 이 단계는 정밀도를 향상시키기 위해 추가 메트릭과 외부 지식 소스를 통합하는 것을 포함한다. 이러한 맥락에서, 우수한 정확도를 갖지만 더 낮은 효율성을 갖는 사전 훈련된 모델들은 이용가능한 후보 문서들의 제한된 세트 <cite class="ltx_cite ltx_citemacro_citep">(Huang and Hu, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib39" title="">2009</a>)</cite>로 인해 효과적으로 채용될 수 있다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">Filtering</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS3.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS3.Px2.p1.1">필터링은 지정된 품질 또는 관련성 표준을 충족하지 못하는 문서를 제거하는 것을 목표로 합니다. 이는 특정 관련성 레벨 미만의 문서들을 배제하기 위해 최소 관련성 스코어 임계치를 설정하는 것과 같은 여러 접근법들을 통해 행해질 수 있다. 더욱이, 사용자들로부터의 피드백 또는 사전 관련성 평가들의 사용은 필터링 프로세스를 조정하는 것을 보조하여, 텍스트 생성 <cite class="ltx_cite ltx_citemacro_citep">(Khattab and Zaharia, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib55" title="">2020</a>; Huang and Huang, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib41" title="">2023</a>)</cite>에 대해 가장 관련성이 높은 문서들만이 보유되도록 보장한다.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>Generation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS4.p1">
<p class="ltx_p" id="S2.SS2.SSS4.p1.1">생성 단계는 생성된 응답의 품질을 향상시키기 위해 검색된 정보를 활용하는 역할을 하는 RAG 프로세스의 중요한 구성 요소이다. 이 단계는 읽을 수 있고 매력적이며 유익한 콘텐츠를 제작하는 것을 목표로 하는 여러 하위 단계를 포함한다.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS4.Px1">
<h5 class="ltx_title ltx_title_paragraph">Enhancing</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS4.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS4.Px1.p1.1">생성 단계의 핵심은 향상 단계이며, 여기서 목표는 검색 정보를 사용자의 쿼리와 병합하여 일관성 있고 관련 있는 응답을 생성하는 것이다. 여기에는 검색된 콘텐츠에 추가 세부 정보를 추가하여 정교화하는 과정이 포함된다. 리프레이징, 리구조화 등의 방법을 통해 산출물의 명확성, 일관성, 양식적 어필을 높여 산출물의 질을 높이려는 노력이 집중되고 있다. 다양한 출처의 정보를 결합하여 종합적인 시각을 제공하고, 내용의 정확성과 관련성을 확보하기 위한 검증을 수행한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS4.Px2">
<h5 class="ltx_title ltx_title_paragraph">Customization</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS4.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS4.Px2.p1.1">사용자 지정은 사용자의 특정 환경 설정 또는 요청 컨텍스트에 맞게 콘텐츠를 조정하는 것을 포함하는 선택적 단계입니다. 이 맞춤법에는 대상 청중의 요구 또는 콘텐츠가 제시될 형식에 맞게 콘텐츠를 조정하고 콘텐츠의 본질을 간결하게 전달하기 위해 정보를 압축하는 것이 포함된다. 이 프로세스는 또한 핵심 사항이나 주장을 강조하는 요약 또는 초록을 만드는 것을 수반하며, 산출물이 유익하고 간결하다는 것을 보장한다.</p>
</div>
<figure class="ltx_figure" id="S2.F3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.F3.1" style="width:433.6pt;height:385.6pt;vertical-align:-380.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-99.2pt,1.1pt) scale(0.685985672853256,0.685985672853256) ;"><span class="ltx_ERROR undefined" id="S2.F3.1.1">{forest}</span>
<p class="ltx_p" id="S2.F3.1.2">(!u.parent anchor)<span class="ltx_ERROR undefined" id="S2.F3.1.2.1">\forestoption</south, parent anchor=south, child anchor=north, draw, minimum height=1cm, rounded corners=10pt, drop shadow, node options=align=center, text width=3cm, l sep=10mm, s sep=3mm, edge=very thick, draw=black, tier/.wrap pgfmath arg=tier #1level(), edge path=[<span_ERROR undefined" id="S2.F3.1.2">\forestoption</span>edge label;  [RAG, root, for tree=parent anchor=south [Pre-Retrieval, xnode, for tree=parent anchor=south [REALM <cite class="ltx_cite ltx_citemacro_citep">(Guu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib31" title="">2020</a>)</cite>; kNN-LMs <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>)</cite>; RAG <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>)</cite>; RETRO <cite idx=</p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 3:</span>RAG의 핵심 기술들의 분류 트리</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Pre-Retrieval</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Indexing</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">kNN-LMs <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>)</cite>에서 증명된 바와 같이 k-최근접 이웃(kNN) 알고리즘과 사전 훈련된 신경 LMs의 통합은 언어 모델링의 상당한 진전을 나타낸다. 이 방법은 텍스트 모음에서 생성된 데이터 저장소를 사용하여 문맥적으로 관련된 예제의 동적 검색을 가능하게 하여 추가적인 훈련 없이 복잡성을 개선할 수 있다.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">효율성으로 알려진 FAISS <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib49" title="">2021</a>)</cite>는 인덱싱 목적으로 많은 연구에서 채택되었다 <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>; Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>; Khattab et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib54" title="">2022</a>)</cite> 일부 연구는 HNSW(Hierarchical Navigable Small World) 근사 <cite class="ltx_cite ltx_citemacro_cite">Malkov and Yashunin (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib71" title="">2020</a>)</cite>와 같은 향상된 기능을 통합하여 더 빠른 검색 <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>)</cite>를 달성한다. 또한 Webgpt <cite class="ltx_cite ltx_citemacro_citep">(Nakano et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib73" title="">2021</a>)</cite>에 요약된 실제 사용자 검색 이력을 기반으로 인덱싱하기 위해 Bing API <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.microsoft.com/en-us/bing/apis/bing-web-search-api</span></span></span>을 활용하는 것과 같은 대체 도구는 조사 중인 다양한 인덱싱 기술을 보여줍니다.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">또한, MEMWALKER <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib8" title="">2023a</a>)</cite>는 입력 텍스트로부터 메모리 트리를 생성함으로써 LLM에서 컨텍스트 윈도우 크기의 한계를 극복하기 위한 혁신적인 방법을 소개한다. 이 트리는 처음에 텍스트를 더 작은 조각으로 분할한 다음 이러한 세그먼트를 요약 노드의 계층적 구조로 요약하여 많은 양의 정보를 효율적으로 인덱싱하고 관리함으로써 형성된다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Query Manipulation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">FiD <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib43" title="">2021</a>)</cite>, COK<cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib63" title="">2023</a>)</cite>, Query2doc <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib102" title="">2023a</a>)</cite>와 같은 연구는 더 적절한 검색 결과를 얻기 위해 새로운 쿼리를 만들거나 기존 쿼리를 정제하는 것의 중요성을 강조한다. 이러한 연구 노력은 구조화된 것이든 비구조화된 것이든 다양한 지식 소스에 적합하도록 여러 지문에서 증거를 효율적으로 수집하고 쿼리를 맞춤화할 필요성을 강조한다. 유사 문서의 생성에서부터 질의 향상을 위한 기술들은 다양한 정보 검색 데이터 세트에 걸쳐 검색 성능을 강화시키는 것으로 나타났다.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">쿼리 조작에 대한 추가 탐색은 Step-Back <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib116" title="">2023</a>)</cite>와 PROMPTAGATOR <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib21" title="">2023</a>)</cite>에 의해 수행되었으며, 이는 고수준의 개념을 추상화하거나 프롬프트 기반 쿼리 생성을 위해 LLMs을 활용하는 데 중점을 둔다. 이러한 전략은 태스크를 보다 일반화된 버전으로 변경하거나 제한된 예제에서 태스크별 쿼리를 만들어 검색 시스템의 기능과 쿼리를 더 잘 정렬하기 위해 노력한다. 이러한 방법론은 질의와 색인된 데이터 간의 일관성을 향상시켜 보다 적절하고 통찰력 있는 정보의 검색을 용이하게 한다.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">또한 KnowledGPT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib103" title="">2023b</a>)</cite>와 Rewrite-Retrieve-Read <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>는 "program of thought" 프롬프트 및 혁신적인 쿼리 재쓰기 기술을 통해 쿼리 조작을 위한 접근 방식을 소개한다. KnowledGPT는 지식베이스와 인터페이스할 코드를 생성하여 사용자 질의를 구조화된 검색 명령어로 변환함으로써 혁신한다. 대조적으로, Rewrite-Retrieve-Read는 쿼리 재구성을 위해 훈련 가능한 컴팩트 LM을 활용하여, 사용자의 의도 및 컨텍스트를 보다 효과적으로 반영하도록 조정한다.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">마지막으로 FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib48" title="">2023</a>)</cite>는 정보 요구 사항을 정확하게 반영하는 쿼리를 만드는 데 중점을 둔 쿼리 공식화에 대한 신뢰도에 기반한 전략을 제시한다. 이 방법은 생성된 문장들 또는 그 단편들의 사용을 검색 질의들의 기초로서 통합한다. 문장을 직접 사용하도록 선택하거나, 낮은 신뢰도의 토큰을 모호하게 하거나, 명시적인 질문을 공식화함으로써, 이 접근법은 검색 프로세스의 효율성을 높여, 검색된 정보가 생성 프로세스의 요구 사항을 충실히 만족하도록 하는 것을 목표로 한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data Modification</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">RA-DIT <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib66" title="">2023b</a>)</cite>와 RECITE <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib92" title="">2023</a>)</cite>는 내부 데이터 수정을 통한 향상을 강조한다. RA-DIT는 LLM과 리트리버에 대한 미세 조정 데이터 세트를 구별하여 LLM의 문맥 이해와 리트리버의 쿼리 정렬 능력을 강화한다. 반면에 RECITE는 생성된 인용 및 응답의 다양성과 관련성을 높이기 위해 통로 힌트와 합성 질문-통로 쌍을 활용한다. 이 접근법은 모델의 지식 기반을 넓히고 응답 정확도를 향상시키려고 한다.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">UPRISE <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib15" title="">2023a</a>)</cite> 및 GENREAD <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib111" title="">2023a</a>)</cite>는 외부 데이터의 리파인먼트를 대상으로 한다. UPRISE는 원시 작업 데이터를 구조화된 형식으로 변환하고 프롬프트의 선택을 세분화하여 검색 결과를 향상시킵니다. 대조적으로, GENREAD에서 사용하는 클러스터링 기반 프롬프트 방법은 질문에서 문서를 생성하고 관련 없는 데이터를 제거하기 위해 클러스터링하여 다양한 컨텍스트 통찰력으로 입력을 풍부하게 한다. 이 기법은 생성 모델을 보다 풍부한 정보 집합으로 제공함으로써 생성 모델의 성능을 향상시키는 것을 목적으로 한다.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">또한 KnowledGPT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib103" title="">2023b</a>)</cite>는 개체 연결을 통해 구조화된 의미적으로 풍부한 정보로 원시 텍스트 데이터를 증강하는 데 전용된다. 이 강화 프로세스는 데이터를 더 응집적으로 구조화하고 쿼리에 더 적합하도록 만들 뿐만 아니라 모델의 검색 효율성을 높입니다. 정확하고 연결된 지식을 활용하여 모델의 이해와 관련 응답을 생성하는 능력을 향상시켜 전체 성능을 향상시킵니다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Retrieval</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Search &amp; Ranking</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Atlas <cite class="ltx_cite ltx_citemacro_citep">(Izacard et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib44" title="">2023</a>)</cite>는 Attention Distillation 및 Perplexity Distillation을 포함한 소수의 샷 학습 접근 방식을 조사하여 검색기를 더 많은 관련 문서를 검색하는 방향으로 조정합니다. IRCOT <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib98" title="">2023</a>)</cite>는 검색과 추론을 통합하여 검색의 효율성을 향상시킨다. SURGE <cite class="ltx_cite ltx_citemacro_citep">(Kang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib51" title="">2023</a>)</cite>는 지식 그래프에서 관련 하위 그래프를 추출하기 위해 하위 그래프 검색기를 사용하는 반면, AAR <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib113" title="">2023b</a>)</cite>는 관련 문서를 가져오는 데 LLMs를 돕기 위해 검색 선호도를 수정한다.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">PRCA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib107" title="">2023a</a>)</cite>는 정확한 질의 응답에 중요한 내용의 우선순위를 결정하기 위해 지도 학습 전략을 사용하여 문서로부터 관련되고 컨텍스트가 풍부한 정보를 추출하기 위해 도메인별 추상적 요약 기법을 사용하는 데 중점을 둔다. 한편 MEMWALKER <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib8" title="">2023a</a>)</cite>는 구축된 메모리 트리에서 내부 검색 및 랭킹 메커니즘을 활용하여 긴 컨텍스트 질의 응답을 위한 관련 정보를 식별한다. 또한 FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib48" title="">2023</a>)</cite>의 Confidence-based Active Retrieval 접근 방식은 낮은 신뢰 토큰이 외부 지식의 필요성을 나타내는 통찰력을 활용하여 생성된 문장의 신뢰 수준에 따라 정보 검색을 동적으로 트리거한다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Post-Retrieval</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Re-Ranking</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Re2G <cite class="ltx_cite ltx_citemacro_citep">(Glass et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib29" title="">2022</a>)</cite>는 질의와 통과를 동시에 분석하기 위해 BERT 변환기를 활용하여 재순위화를 위한 시퀀스 쌍 분류 방법을 소개한다. 수열 간의 교차 주의를 사용하는 이 상호 작용 모델은 초기 검색 단계에서 일반적으로 사용되는 표현 모델과 대조를 제공한다. PROMPTAGATOR <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib21" title="">2023</a>)</cite>는 또한 재채점을 위해 교차 주의 모델을 사용한다. 그것의 "Lift Yourself Up" 전략은 추가 생성 라운드를 위해 풀에서 최적의 후보를 반복적으로 선택하여 자가 생성 콘텐츠를 통해 콘텐츠 품질을 점진적으로 향상시킨다.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Re-ranking은 In-Context RALM <cite class="ltx_cite ltx_citemacro_citep">(Ram et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib83" title="">2023</a>)</cite>의 중요한 초점이기도 하다. 재순위에 대한 두 가지 접근법, 즉 언어 모델을 이용한 제로샷 재순위와 학습된 모델을 통한 예측 재순위를 탐색한다. 이 단계는 언어 모델 성능 향상을 위한 기대 효용을 기반으로 문서 선택을 정제하는 것을 목표로 한다. 특히 ITER-RETGEN <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib90" title="">2023</a>)</cite>는 LLM 출력의 관련성 신호를 기반으로 재순위자에서 고밀도 리트리버로 지식 증류를 활용하여 검색 노력을 미세 조정한다. 검색 모델의 이러한 최적화는 쿼리 뉘앙스를 보다 정확하게 캡처하여 문서 선택을 개선하는 것을 목표로 한다.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">DKS-RAC <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib38" title="">2023</a>)</cite>는 시퀀스 수준에서 답변과 검색된 지문 사이의 지식을 정렬하기 위한 DKS(Dense Knowledge Similarity)를 제시한다. 이 접근법은 지식 유사성을 기반으로 한 통과 선택에 직접적인 영향을 미치기 때문에 재순위로 분류되어 쿼리와 문서 간의 일치를 정제한다.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">FiD-light <cite class="ltx_cite ltx_citemacro_citep">(Hofstätter et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib34" title="">2023</a>)</cite>는 순위 순서를 최적화하기 위해 소스 포인터를 사용하는 목록형 자기회귀 재순위화 방법을 소개한다. 이 방법은 생성된 텍스트와 소스 패시지 사이의 링크를 유지하여, 보다 구조화된 생성 프로세스를 가능하게 한다. 관련 정보 소스에 대한 포인터로서 모델의 출력 내에 텍스트 인용을 통합함으로써, 이 접근법은 조직화된 검색 및 생성 프로세스를 용이하게 하여 생성된 콘텐츠의 전반적인 일관성 및 관련성을 향상시킨다.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Filtering</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">COK <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib63" title="">2023</a>)</cite>는 검색된 지식으로 근거를 반복적으로 정제하는 것을 목표로 하는 점진적 합리화 교정 기법을 제시한다. 이 방법은 연속 최적화 프로세스를 구성하여 콘텐츠 생성에 사용되는 정보의 관련성과 품질을 크게 향상시킨다. Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib1" title="">2023</a>)</cite>는 관련 없는 내용을 효율적으로 필터링하기 위한 자기 반영 메커니즘을 소개한다. 비판 토큰을 사용하여 이 접근법은 검색된 구절의 관련성, 지원성 및 유용성을 평가하여 고품질 정보만 콘텐츠 생성 프로세스에 통합되도록 한다.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">추가적으로, FiD-TF <cite class="ltx_cite ltx_citemacro_citep">(Berchansky et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib2" title="">2023</a>)</cite> 및 RECOMP <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib106" title="">2023</a>)</cite>는 검색된 문서로부터 관련 없거나 중복되는 토큰 및 정보의 제거에 전용된다. FiD-TF는 불필요한 토큰을 식별하고 제거하는 동적 메커니즘을 채택하여 정보 처리의 효율성을 향상시킨다. 반면에 RECOMP는 문서를 간결한 요약으로 압축하여 생성 프로세스에 가장 적합한 내용만을 선택하는 데 중점을 둔다. 이러한 방법들은 관련되고 지지적인 정보만이 활용되도록 보장함으로써 콘텐츠 생성 워크플로우를 간소화하여, 생성된 콘텐츠의 전반적인 품질 및 관련성을 향상시킨다.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.1" style="width:433.6pt;height:255.6pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-364.4pt,214.4pt) scale(0.373036932542922,0.373036932542922) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1.1">Research</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.2.1">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.3.1">Retrieval Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.4.1">Multi-hop</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.5" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.5.1">Training</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S5.T1.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.6.1">Pre-Retrieval</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.7.1">Retrieval</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T1.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.8.1">Post-Retrieval</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T1.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.9.1">Generation</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.1.1">Internal</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.2.1">External</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.3.1">Indexing</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.4.1">Query Manipulation</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.5.1">Data Modification</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.6.1">Search &amp; Ranking</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.7.1">Re-Ranking</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.8"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.8.1">Filtering</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.9"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.9.1">Enhancing</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.10"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.10.1">Customization</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.1">REALM <cite class="ltx_cite ltx_citemacro_citep">(Guu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib31" title="">2020</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.2">2020</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.1">kNN-LMs <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.1">RAG <cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.2">2020</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.1">FiD <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib43" title="">2021</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.2">2021</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.1">Webgpt <cite class="ltx_cite ltx_citemacro_citep">(Nakano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib73" title="">2021</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.2">2021</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.7">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.12">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.1">Re2G <cite class="ltx_cite ltx_citemacro_citep">(Glass et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib29" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.1">RETRO <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib6" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.2">2022</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.1">DSP <cite class="ltx_cite ltx_citemacro_citep">(Khattab et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib54" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.2">2022</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.1">COK <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib63" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.1">IRCOT <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib98" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.1">ITRG <cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib26" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.1">PKG <cite class="ltx_cite ltx_citemacro_citep">(Luo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib69" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.14">✓</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.1">RA-DIT <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib66" title="">2023b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.9">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.1">Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib1" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.12">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.14">✓</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.17.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.1">SURGE <cite class="ltx_cite ltx_citemacro_citep">(Kang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib51" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.14">✓</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.18.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.1">FiD-TF <cite class="ltx_cite ltx_citemacro_citep">(Berchansky et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib2" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.11">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.12">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.19.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.1">PRCA <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib107" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.20.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.1">REPLUG <cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib91" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.14">✓</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.21.21">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.1">AAR <cite class="ltx_cite ltx_citemacro_cite">Yu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib113" title="">2023b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.22.22">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.1">Query2doc <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib102" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.23.23">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.1">Step-Back <cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib116" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.24.24">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.1">ITER-RETGEN <cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib90" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.10">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.25.25">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.1">RECITE <cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib92" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.9">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.26.26">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.1">PROMPTAGATOR <cite class="ltx_cite ltx_citemacro_cite">Dai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib21" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.11">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.12">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.27.27">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.1">UPRISE <cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib15" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.9">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.28.28">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.1">GENREAD <cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib111" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.9">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.29.29">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.1">KnowledGPT <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib103" title="">2023b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.8">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.9">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.30.30">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.1">Selfmem <cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib16" title="">2023b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.31.31">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.1">MEMWALKER <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib8" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.32.32">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.1">RECOMP <cite class="ltx_cite ltx_citemacro_citep">(Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib106" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.12">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.33.33">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.1">Rewrite-Retrieve-Read <cite class="ltx_cite ltx_citemacro_cite">Ma et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.34.34">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.1">Atlas <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.10">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.35.35">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.1">DKS-RAC <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib38" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.11">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.12">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.36.36">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.1">In-Context RALM <cite class="ltx_cite ltx_citemacro_citep">(Ram et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib83" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.37.37">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.1">Fid-light <cite class="ltx_cite ltx_citemacro_citep">(Hofstätter et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib34" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.38.38">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.1">FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib48" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.2">2023</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.3"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.4">✓</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.5"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.6"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.7"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.8">✓</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.9"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.10">✓</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.11"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.12"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.13"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.14"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1:</span>The comprehensive summary of RAG studies. "다중 홉" 열의 ✓in은 연구가 여러 검색 라운드를 포함한다는 것을 나타냅니다. 유사하게, "훈련" 열의 ✓in은 연구가 훈련 단계를 포함했음을 나타낸다. 이러한 맥락에서 "훈련"은 초기 모델 훈련과 미세 조정 과정을 모두 포함한다는 점에 유의하는 것이 중요하다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Generation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Enhancing</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">DSP <cite class="ltx_cite ltx_citemacro_citep">(Khattab et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib54" title="">2022</a>)</cite>는 다양한 구절에서 집계된 정보를 바탕으로 질문을 요약하고 답변하기 위해 여러 검색 쿼리를 생성하도록 설계된 프레임워크를 소개한다. 이 프레임워크는 CombSUM <cite class="ltx_cite ltx_citemacro_citep">(Fox and Shaw, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib27" title="">1994</a>)</cite>를 사용하여 서로 다른 검색 목록에 걸쳐 패시지에 대한 누적 확률 점수를 계산하여 여러 소스로부터 포괄적인 응답의 컴파일을 용이하게 한다.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">PRCA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib107" title="">2023a</a>)</cite>는 Reward-Driven Stage를 설명하며, 여기서 증류된 컨텍스트는 생성기의 피드백에 기초하여 정제된다. 강화 학습을 활용하여, 이 단계는 관련 컨텍스트를 제공하기 위해 수신된 보상에 따라 PRCA의 파라미터를 조정한다. 목적은 추출된 컨텍스트를 발전기의 특정 요구 사항을 충족하도록 미세 조정함으로써, 발전 프로세스를 최적화하는 것이다.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">REPLUG <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib91" title="">2023</a>)</cite>는 Black-box LM에 의한 최종 예측 이전에 검색된 문서를 입력 컨텍스트로 프리펜딩하는 방법을 제안한다. 검색된 문서를 병렬로 인코딩하기 위해 앙상블 전략을 도입하여 LM 문맥 길이의 한계를 극복하고 증가된 계산 자원의 할당을 통해 정확도를 향상시킨다. 이 접근법은 LM이 더 넓은 범위의 관련 정보에 액세스할 수 있도록 보장함으로써 생성 프로세스를 개선한다.</p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">RECITE <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib92" title="">2023</a>)</cite>는 복수의 인용문을 독립적으로 생성하고 가장 적절한 답변을 결정하기 위해 복수의/다수결 투표 시스템을 사용하는 자체 일관성 기술을 구현한다. 이 방법은 답변의 신뢰성과 정확성을 높이고, 이에 따라 출력의 품질과 신뢰성을 향상시키도록 설계된다.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Customization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib69" title="">2023</a>)</cite>에 의해 소개된 PKG 프레임워크는 LM의 출력을 커스터마이징하는 접근법을 나타낸다. 사전 학습된 모델을 사용하여 내부적으로 배경 지식을 생성함으로써, PKG는 전통적인 외부 검색 프로세스의 필요성을 제거한다. 이 방법은 도메인 또는 태스크 특정 지식을 생성 단계에 직접 통합하여 주어진 컨텍스트 또는 요구 사항에 특별히 맞춤화된 응답을 생성하는 LM의 능력을 크게 향상시킨다.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib1" title="">2023</a>)</cite>는 사용자 정의 가능한 디코딩 알고리즘 내에 반사 토큰을 통합하는 전략을 제공한다. 이 기법은 특정 작업을 기반으로 모델의 검색 및 생성 동작을 동적으로 조정하여 보다 다양한 응답 생성을 용이하게 한다. 요구 사항에 따라 이 접근법은 정확성 또는 창의성을 위해 튜닝될 수 있어 다양한 요구를 충족하는 출력을 생성하는 데 유연성을 제공한다.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">SURGE <cite class="ltx_cite ltx_citemacro_citep">(Kang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib51" title="">2023</a>)</cite>는 그래프-텍스트 대비 학습의 적용을 통해 커스터마이징을 달성한다. 이 방법은 생성된 대화 응답들이 검색된 서브그래프에 포함된 지식과 밀접하게 정렬되어, 대화 컨텍스트에 구체적이고 관련되며 깊게 뿌리내린 응답들을 산출하도록 보장한다. 검색된 지식과 생성된 텍스트 사이의 일관성을 유지함으로써 SURGE는 서브그래프의 세부 지식을 정확하게 반영하는 출력을 생성할 수 있어 응답의 관련성과 특수성을 향상시킨다.</p>
</div>
<figure class="ltx_table" id="S6.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T2.1" style="width:433.6pt;height:4525.6pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-209.5pt,2186.1pt) scale(0.50859446856813,0.50859446856813) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T2.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.1.1">Research</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.2.1">Year</span></th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T2.1.1.1.1.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.1.1.3.1">
<span class="ltx_p" id="S6.T2.1.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.3.1.1.1">Retriever</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T2.1.1.1.1.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.1.1.4.1">
<span class="ltx_p" id="S6.T2.1.1.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.4.1.1.1">Generator</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.2.1.1">REALM <cite class="ltx_cite ltx_citemacro_citep">(Guu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib31" title="">2020</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.2.1.2">2020</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.2.1.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.2.1.3.1">
<span class="ltx_p" id="S6.T2.1.1.2.1.3.1.1">BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib22" title="">2019</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.2.1.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.2.1.4.1">
<span class="ltx_p" id="S6.T2.1.1.2.1.4.1.1">Transformers <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib99" title="">2017</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.3.2.1">kNN-LMs <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.3.2.2">2020</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.3.2.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.3.2.3.1">
<span class="ltx_p" id="S6.T2.1.1.3.2.3.1.1">FAISS <cite class="ltx_cite ltx_citemacro_citep">(Johnson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib49" title="">2021</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.3.2.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.3.2.4.1">
<span class="ltx_p" id="S6.T2.1.1.3.2.4.1.1">Transformers</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.4.3.1">RAG <cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.4.3.2">2020</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.4.3.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.4.3.3.1">
<span class="ltx_p" id="S6.T2.1.1.4.3.3.1.1">DPR <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib52" title="">2020</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.4.3.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.4.3.4.1">
<span class="ltx_p" id="S6.T2.1.1.4.3.4.1.1">BART-Large <cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib59" title="">2020a</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.5.4.1">FiD <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib43" title="">2021</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.5.4.2">2021</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.5.4.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.5.4.3.1">
<span class="ltx_p" id="S6.T2.1.1.5.4.3.1.1">BM25 <cite class="ltx_cite ltx_citemacro_citep">(Robertson and Zaragoza, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib86" title="">2009</a>)</cite>, DPR</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.5.4.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.5.4.4.1">
<span class="ltx_p" id="S6.T2.1.1.5.4.4.1.1">T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib82" title="">2020</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.6.5.1">Webgpt <cite class="ltx_cite ltx_citemacro_citep">(Nakano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib73" title="">2021</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.6.5.2">2021</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.6.5.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.6.5.3.1">
<span class="ltx_p" id="S6.T2.1.1.6.5.3.1.1">Bing</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.6.5.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.6.5.4.1">
<span class="ltx_p" id="S6.T2.1.1.6.5.4.1.1">GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib7" title="">2020</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.7.6.1">Re2G <cite class="ltx_cite ltx_citemacro_citep">(Glass et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib29" title="">2022</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.7.6.2">2022</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.7.6.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.7.6.3.1">
<span class="ltx_p" id="S6.T2.1.1.7.6.3.1.1">BM25, DPR</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.7.6.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.7.6.4.1">
<span class="ltx_p" id="S6.T2.1.1.7.6.4.1.1">BART</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.8.7.1">RETRO <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib6" title="">2022</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.8.7.2">2022</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.8.7.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.8.7.3.1">
<span class="ltx_p" id="S6.T2.1.1.8.7.3.1.1">BERT</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.8.7.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.8.7.4.1">
<span class="ltx_p" id="S6.T2.1.1.8.7.4.1.1">Transformer</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.9.8.1">DSP <cite class="ltx_cite ltx_citemacro_citep">(Khattab et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib54" title="">2022</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.9.8.2">2022</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.9.8.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.9.8.3.1">
<span class="ltx_p" id="S6.T2.1.1.9.8.3.1.1">ColBERTv2 <cite class="ltx_cite ltx_citemacro_citep">(Khattab and Zaharia, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib55" title="">2020</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.9.8.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.9.8.4.1">
<span class="ltx_p" id="S6.T2.1.1.9.8.4.1.1">GPT-3.5 (text-davinci-002)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.10.9.1">COK <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib63" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.10.9.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.10.9.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.10.9.3.1">
<span class="ltx_p" id="S6.T2.1.1.10.9.3.1.1">LLaMA2-7B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib97" title="">2023b</a>)</cite>, ChatGPT (gpt-3.5-turbo-0613)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.10.9.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.10.9.4.1">
<span class="ltx_p" id="S6.T2.1.1.10.9.4.1.1">ChatGPT (gpt-3.5-turbo-0613)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.11.10.1">IRCOT <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib98" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.11.10.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.11.10.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.11.10.3.1">
<span class="ltx_p" id="S6.T2.1.1.11.10.3.1.1">BM25</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.11.10.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.11.10.4.1">
<span class="ltx_p" id="S6.T2.1.1.11.10.4.1.1">GPT-3 (code-davinci-002), Flan-T5&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chung et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib18" title="">2022</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.12.11.1">ITRG <cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib26" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.12.11.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.12.11.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.12.11.3.1">
<span class="ltx_p" id="S6.T2.1.1.12.11.3.1.1">Atlas <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.12.11.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.12.11.4.1">
<span class="ltx_p" id="S6.T2.1.1.12.11.4.1.1">LLaMA 33B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib96" title="">2023a</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.13.12.1">PKG <cite class="ltx_cite ltx_citemacro_citep">(Luo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib69" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.13.12.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.13.12.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.13.12.3.1">
<span class="ltx_p" id="S6.T2.1.1.13.12.3.1.1">LLaMa-7B</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.13.12.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.13.12.4.1">
<span class="ltx_p" id="S6.T2.1.1.13.12.4.1.1">InstructGPT-3.5 (text-davinic-002) <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib77" title="">2022</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.14.13.1">RA-DIT <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib66" title="">2023b</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.14.13.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.14.13.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.14.13.3.1">
<span class="ltx_p" id="S6.T2.1.1.14.13.3.1.1">DRAGON+ <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib65" title="">2023a</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.14.13.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.14.13.4.1">
<span class="ltx_p" id="S6.T2.1.1.14.13.4.1.1">LLama</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.15.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.15.14.1">Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib1" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.15.14.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.15.14.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.15.14.3.1">
<span class="ltx_p" id="S6.T2.1.1.15.14.3.1.1">Contriever <cite class="ltx_cite ltx_citemacro_citep">(Izacard et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib42" title="">2022</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.15.14.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.15.14.4.1">
<span class="ltx_p" id="S6.T2.1.1.15.14.4.1.1">Llama2 (7B and 13B) , GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib76" title="">2023</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.16.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.16.15.1">SURGE <cite class="ltx_cite ltx_citemacro_citep">(Kang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib51" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.16.15.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.16.15.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.16.15.3.1">
<span class="ltx_p" id="S6.T2.1.1.16.15.3.1.1">Graph Neural Networks (GNN) <cite class="ltx_cite ltx_citemacro_citep">(Hamilton, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib32" title="">2020</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.16.15.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.16.15.4.1">
<span class="ltx_p" id="S6.T2.1.1.16.15.4.1.1">Transformers</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.17.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.17.16.1">FiD-TF <cite class="ltx_cite ltx_citemacro_citep">(Berchansky et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib2" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.17.16.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.17.16.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.17.16.3.1">
<span class="ltx_p" id="S6.T2.1.1.17.16.3.1.1">BM25, Sentence Transformers</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.17.16.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.17.16.4.1">
<span class="ltx_p" id="S6.T2.1.1.17.16.4.1.1">T5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.18.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.18.17.1">PRCA <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib107" title="">2023a</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.18.17.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.18.17.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.18.17.3.1">
<span class="ltx_p" id="S6.T2.1.1.18.17.3.1.1">BM25, DPR, Contriver, SimCSE <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib28" title="">2021</a>)</cite>, SBERT <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib85" title="">2019</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.18.17.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.18.17.4.1">
<span class="ltx_p" id="S6.T2.1.1.18.17.4.1.1">T5-large, Phoenix-7B <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib14" title="">2023d</a>)</cite>, Vicuna-7B <cite class="ltx_cite ltx_citemacro_citep">(Peng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib79" title="">2023</a>)</cite>, ChatGLM <cite class="ltx_cite ltx_citemacro_citep">(Du et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib24" title="">2022</a>)</cite>, GPT-3.5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.19.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.19.18.1">REPLUG <cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib91" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.19.18.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.19.18.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.19.18.3.1">
<span class="ltx_p" id="S6.T2.1.1.19.18.3.1.1">Contriever</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.19.18.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.19.18.4.1">
<span class="ltx_p" id="S6.T2.1.1.19.18.4.1.1">GPT-3</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.20.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.20.19.1">AAR <cite class="ltx_cite ltx_citemacro_cite">Yu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib113" title="">2023b</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.20.19.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.20.19.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.20.19.3.1">
<span class="ltx_p" id="S6.T2.1.1.20.19.3.1.1">ANCE <cite class="ltx_cite ltx_citemacro_citep">(Xiong et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib105" title="">2021</a>)</cite>,&nbsp;Contriever</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.20.19.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.20.19.4.1">
<span class="ltx_p" id="S6.T2.1.1.20.19.4.1.1">Flan-T5, InstructGPT</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.21.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.21.20.1">Query2doc <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib102" title="">2023a</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.21.20.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.21.20.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.21.20.3.1">
<span class="ltx_p" id="S6.T2.1.1.21.20.3.1.1">BM25, DPR</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.21.20.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.21.20.4.1">
<span class="ltx_p" id="S6.T2.1.1.21.20.4.1.1">GPT-3 (text-davinci-003)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.22.21">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.22.21.1">Step-Back <cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib116" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.22.21.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.22.21.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.22.21.3.1">
<span class="ltx_p" id="S6.T2.1.1.22.21.3.1.1">PaLM-2L <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib17" title="">2023</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.22.21.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.22.21.4.1">
<span class="ltx_p" id="S6.T2.1.1.22.21.4.1.1">PaLM-2L, GPT-4</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.23.22">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.23.22.1">ITER-RETGEN <cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib90" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.23.22.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.23.22.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.23.22.3.1">
<span class="ltx_p" id="S6.T2.1.1.23.22.3.1.1">Contriever</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.23.22.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.23.22.4.1">
<span class="ltx_p" id="S6.T2.1.1.23.22.4.1.1">InstructGPT (text-davinci-003), Llama-2</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.24.23">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.24.23.1">RECITE <cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib92" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.24.23.2">2023</th>
<td class="ltx_td ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.24.23.3" style="width:142.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.24.23.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.24.23.4.1">
<span class="ltx_p" id="S6.T2.1.1.24.23.4.1.1">PaLM, UL2 <cite class="ltx_cite ltx_citemacro_citep">(Tay et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib93" title="">2023</a>)</cite>, OPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib115" title="">2022</a>)</cite>, Codex <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib11" title="">2021</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.25.24">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.25.24.1">PROMPTAGATOR <cite class="ltx_cite ltx_citemacro_citep">(Dai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib21" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.25.24.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.25.24.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.25.24.3.1">
<span class="ltx_p" id="S6.T2.1.1.25.24.3.1.1">T5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.25.24.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.25.24.4.1">
<span class="ltx_p" id="S6.T2.1.1.25.24.4.1.1">FLAN</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.26.25">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.26.25.1">UPRISE <cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib15" title="">2023a</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.26.25.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.26.25.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.26.25.3.1">
<span class="ltx_p" id="S6.T2.1.1.26.25.3.1.1">GPT-Neo-2.7B <cite class="ltx_cite ltx_citemacro_citep">(Black et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib5" title="">2021</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.26.25.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.26.25.4.1">
<span class="ltx_p" id="S6.T2.1.1.26.25.4.1.1">BLOOM-7.1B <cite class="ltx_cite ltx_citemacro_citep">(Workshop et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib104" title="">2022</a>)</cite>, OPT-66B, GPT-3-175B</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.27.26">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.27.26.1">GENREAD <cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib111" title="">2023a</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.27.26.2">2023</th>
<td class="ltx_td ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.27.26.3" style="width:142.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.27.26.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.27.26.4.1">
<span class="ltx_p" id="S6.T2.1.1.27.26.4.1.1">InstructGPT</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.28.27">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.28.27.1">KnowledGPT <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib103" title="">2023b</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.28.27.2">2023</th>
<td class="ltx_td ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.28.27.3" style="width:142.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.28.27.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.28.27.4.1">
<span class="ltx_p" id="S6.T2.1.1.28.27.4.1.1">GPT-4</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.29.28">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.29.28.1">Selfmem <cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib16" title="">2023b</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.29.28.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.29.28.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.29.28.3.1">
<span class="ltx_p" id="S6.T2.1.1.29.28.3.1.1">BM25</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.29.28.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.29.28.4.1">
<span class="ltx_p" id="S6.T2.1.1.29.28.4.1.1">XGLM <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib67" title="">2022</a>)</cite>, XLM-Rbase <cite class="ltx_cite ltx_citemacro_citep">(Conneau et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib19" title="">2020</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.30.29">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.30.29.1">MEMWALKER <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib8" title="">2023a</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.30.29.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.30.29.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.30.29.3.1">
<span class="ltx_p" id="S6.T2.1.1.30.29.3.1.1">LLaMA-2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.30.29.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.30.29.4.1">
<span class="ltx_p" id="S6.T2.1.1.30.29.4.1.1">LLaMA-2</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.31.30">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.31.30.1">RECOMP <cite class="ltx_cite ltx_citemacro_citep">(Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib106" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.31.30.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.31.30.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.31.30.3.1">
<span class="ltx_p" id="S6.T2.1.1.31.30.3.1.1">BM25</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.31.30.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.31.30.4.1">
<span class="ltx_p" id="S6.T2.1.1.31.30.4.1.1">T5-Large</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.32.31">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.32.31.1">Rewrite-Retrieve-Read <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.32.31.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.32.31.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.32.31.3.1">
<span class="ltx_p" id="S6.T2.1.1.32.31.3.1.1">Bing</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.32.31.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.32.31.4.1">
<span class="ltx_p" id="S6.T2.1.1.32.31.4.1.1">T5-Large, ChatGPT(gpt-3.5-turbo), Vicuna-13B</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.33.32">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.33.32.1">Atlas <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.33.32.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.33.32.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.33.32.3.1">
<span class="ltx_p" id="S6.T2.1.1.33.32.3.1.1">Contriever</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.33.32.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.33.32.4.1">
<span class="ltx_p" id="S6.T2.1.1.33.32.4.1.1">T5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.34.33">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.34.33.1">DKS-RAC <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib38" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.34.33.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.34.33.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.34.33.3.1">
<span class="ltx_p" id="S6.T2.1.1.34.33.3.1.1">DPR</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.34.33.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.34.33.4.1">
<span class="ltx_p" id="S6.T2.1.1.34.33.4.1.1">BART</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.35.34">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.35.34.1">In-Context RALM <cite class="ltx_cite ltx_citemacro_citep">(Ram et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib83" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.35.34.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.35.34.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.35.34.3.1">
<span class="ltx_p" id="S6.T2.1.1.35.34.3.1.1">BM25, BERT-base, Contriever, Spider <cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib84" title="">2022</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.35.34.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.35.34.4.1">
<span class="ltx_p" id="S6.T2.1.1.35.34.4.1.1">GPT-2, GPT-Neo, GPT-J <cite class="ltx_cite ltx_citemacro_citep">(Wang and Komatsuzaki, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib101" title="">2021</a>)</cite>, OPT, and LLaMA</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.36.35">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.36.35.1">Fid-light <cite class="ltx_cite ltx_citemacro_citep">(Hofstätter et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib34" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.36.35.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.36.35.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.36.35.3.1">
<span class="ltx_p" id="S6.T2.1.1.36.35.3.1.1">GTR-Base <cite class="ltx_cite ltx_citemacro_citep">(Ni et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib74" title="">2022</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.36.35.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.36.35.4.1">
<span class="ltx_p" id="S6.T2.1.1.36.35.4.1.1">T5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.37.36">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.37.36.1">FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib48" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S6.T2.1.1.37.36.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" id="S6.T2.1.1.37.36.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.37.36.3.1">
<span class="ltx_p" id="S6.T2.1.1.37.36.3.1.1">BM25, Bing</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" id="S6.T2.1.1.37.36.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.37.36.4.1">
<span class="ltx_p" id="S6.T2.1.1.37.36.4.1.1">GPT-3.5 (text-davinci-003)</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span>The summary of Retrievers and Generators. 이들 연구에서 명시적으로 언급된 검색 모델 및 사전 훈련된 언어 모델이 기록되었다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Comparisons of RAG</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>The Comprehensive Summary of RAG</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">표 <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S5.T1" title="Table 1 ‣ 5.2 Filtering ‣ 5 Post-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>는 본 논문에서 논의된 RAG 연구의 상세한 분석을 제시한다. 분석에 따르면 이러한 연구의 대부분은 LLM의 내용을 풍부하게 하기 위해 외부 데이터 소스를 활용했다. 단일 홉 검색보다 다중 홉 검색에 대한 선호가 언급되었으며, 이는 반복 검색 라운드가 일반적으로 우수한 결과를 산출한다는 것을 나타낸다. 즉, 대부분의 방법들은 높은 품질의 후보 문서들을 확보하기 위해 밀집 검색을 사용한다. 검색 전 단계에서 데이터 세트를 수정하는 것과 비교하여 검색 성능을 향상시키기 위해 쿼리를 조작하는 데 중점을 두는 연구가 더 많다. 또한 검색 단계를 최적화하는 데 상당한 중점을 두고 있어 연구에서 중요한 역할을 강조한다. 그러나 이를 미래 탐구의 잠재적 영역으로 지적하면서 세대 단계에서 커스터마이징에 초점을 맞춘 연구는 부족한 것으로 보인다. 전반적으로 RAG의 목표는 LLM의 응답 품질을 향상시키는 것이지만 검색 측면을 개선하기 위한 더 많은 노력이 이루어졌다.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Retriever and Generator</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">RAG에서는 리트리버와 생성기가 주요 구성 요소입니다. 표 <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S6.T2" title="Table 2 ‣ 6.2 Customization ‣ 6 Generation ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>는 본 논문에서 논의된 연구에서 사용된 검색기 및 생성기를 요약한 것이다. 표에서 대부분의 생성기가 고급 언어 모델을 사용하지만 상당수의 검색기가 효율성 때문에 여전히 전통적인 BM25를 사용하고 있음이 분명하다. 검색 방법은 RAG에서 중요한 측면이며, 효율성을 손상시키지 않으면서 검색 성능을 향상시키는 방법을 탐색하는 것의 중요성을 강조한다. 유사하게, LLaMA2, GPT-3.5 또는 GPT-4와 같은 강력한 LLM을 생성기로 채택한 연구는 많지 않다. T5와 같은 LLM은 여전히 인기 있지만, 2023년에는 BERT나 트랜스포머와 같은 기본 모델이 거의 사용되지 않는다. 리트리버에 IR 기반 LLM이 많이 사용되지 않는다는 것은 발전기에 비해 분명하며, 이는 향후 이러한 모델을 개발할 수 있는 유망한 방향을 나타낸다.</p>
</div>
<figure class="ltx_table" id="S7.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S7.T3.1" style="width:433.6pt;height:131.3pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-287.1pt,86.7pt) scale(0.430278599735059,0.430278599735059) ;">
<table class="ltx_tabular ltx_align_middle" id="S7.T3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T3.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S7.T3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S7.T3.1.1.1.1.1.1">Evaluation Framework</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S7.T3.1.1.1.1.2.1">Aspects</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S7.T3.1.1.1.1.3.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S7.T3.1.1.1.1.4.1">Metrics</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S7.T3.1.1.1.1.5.1">Datasets</span></td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S7.T3.1.1.2.2.1" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.2.2.1.1">RAGAS <cite class="ltx_cite ltx_citemacro_citep">(Shahul et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib89" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.2.2.2" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.2.2.2.1">Quality of RAG Systems</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.2.2.3">Context Relevance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.2.2.4">Extracted Sentences / Total Sentences</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.2.2.5" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.2.2.5.1">WikiEval <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://huggingface.co/datasets/explodinggradients/WikiEval</span></span></span></span></td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.3.3.1">Answer Relevance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.3.3.2">Average Cosine Similarity</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.4.4.1">Faithfulness</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.4.4.2">Supported Statements / Total Statements</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S7.T3.1.1.5.5.1" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.5.5.1.1">ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib87" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.5.5.2" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.5.5.2.1">Improving RAGAS</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.5.5.3">Context Relevance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.5.5.4" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.5.5.4.1">Confidence Intervals</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.5.5.5" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.5.5.5.1">
<span class="ltx_tabular ltx_align_middle" id="S7.T3.1.1.5.5.5.1.1">
<span class="ltx_tr" id="S7.T3.1.1.5.5.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.5.5.5.1.1.1.1">KILT <cite class="ltx_cite ltx_citemacro_citep">(Petroni et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib80" title="">2021</a>)</cite></span></span>
<span class="ltx_tr" id="S7.T3.1.1.5.5.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.5.5.5.1.1.2.1">SuperGLUE <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib100" title="">2019</a>)</cite></span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.6.6.1">Answer Relevance</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.7.7.1">Answer Faithfulness</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S7.T3.1.1.8.8.1" rowspan="2"><span class="ltx_text" id="S7.T3.1.1.8.8.1.1">RECALL <cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib68" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.8.8.2" rowspan="2"><span class="ltx_text" id="S7.T3.1.1.8.8.2.1">Counterfactual Robustness</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.8.8.3">Response Quality</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.8.8.4">
<table class="ltx_tabular ltx_align_middle" id="S7.T3.1.1.8.8.4.1">
<tbody><tr class="ltx_tr" id="S7.T3.1.1.8.8.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.8.8.4.1.1.1">Accuracy (QA)</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.8.8.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.8.8.4.1.2.1">BLEU, ROUGE-L (Generation)</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.8.8.5" rowspan="2"><span class="ltx_text" id="S7.T3.1.1.8.8.5.1">
<span class="ltx_tabular ltx_align_middle" id="S7.T3.1.1.8.8.5.1.1">
<span class="ltx_tr" id="S7.T3.1.1.8.8.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.8.8.5.1.1.1.1">EventKG <cite class="ltx_cite ltx_citemacro_citep">(Gottschalk and Demidova, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib30" title="">2018</a>)</cite></span></span>
<span class="ltx_tr" id="S7.T3.1.1.8.8.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.8.8.5.1.1.2.1">UJ <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib36" title="">2022</a>)</cite></span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.9.9.1">Robustness</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.9.9.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T3.1.1.9.9.2.1">
<tbody><tr class="ltx_tr" id="S7.T3.1.1.9.9.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.9.9.2.1.1.1">Misleading Rate (QA)</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.9.9.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.9.9.2.1.2.1">Mistake Reappearance Rate (Generation)</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S7.T3.1.1.10.10.1" rowspan="4"><span class="ltx_text" id="S7.T3.1.1.10.10.1.1">RGB <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib10" title="">2023b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T3.1.1.10.10.2" rowspan="4"><span class="ltx_text" id="S7.T3.1.1.10.10.2.1">Impact of RAG on LLMs</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.10.10.3">Noise Robustness</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.10.10.4">Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T3.1.1.10.10.5" rowspan="4"><span class="ltx_text" id="S7.T3.1.1.10.10.5.1">Synthetic Dataset including English and Chinese</span></td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.11.11.1">Negative Rejection</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.11.11.2">Rejection Rate</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.12.12.1">Information Integration</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.12.12.2">Accuracy</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.13.13">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T3.1.1.13.13.1">Counterfactual Robustness</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T3.1.1.13.13.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T3.1.1.13.13.2.1">
<tbody><tr class="ltx_tr" id="S7.T3.1.1.13.13.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.13.13.2.1.1.1">Error Detection Rate</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.13.13.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.13.13.2.1.2.1">Error Correction Rate</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3:</span>The Comparison of Different RAG Evaluation Frameworks</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Evaluation in RAG</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">외부 지식을 활용하여 보다 정확하고 관련 있으며 강력한 응답을 생성하는 LM의 효과를 이해하기 위해 RAG 시스템의 평가는 중요한 연구 영역이 되었다. 대화 기반 상호작용의 인기로 인해 최근 연구는 Exact Match(EM) 및 F1 점수와 같은 확립된 메트릭을 사용하여 이러한 다운스트림 태스크에 대한 RAG 모델의 성능을 평가하는 데 중점을 두었다. 나아가, TriviaQA <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib50" title="">2017</a>)</cite>, HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib108" title="">2018</a>)</cite>, FEVER <cite class="ltx_cite ltx_citemacro_citep">(Thorne et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib95" title="">2018</a>)</cite>, Natural Questions <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib56" title="">2019</a>)</cite>, Wikipedia <cite class="ltx_cite ltx_citemacro_citep">(Dinan et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib23" title="">2019</a>)</cite>, T-REX <cite class="ltx_cite ltx_citemacro_citep">(ElSahar et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib25" title="">2018</a>)</cite> 등의 광범위한 데이터셋이 이를 위해 활용되고 있다.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">그러나 다운스트림 태스크의 관점에서의 평가만으로는 RAG 개발의 진화하는 요구를 해결하는 데 부족한다. 최근 연구는 표 <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S7.T3" title="Table 3 ‣ 7.2 Retriever and Generator ‣ 7 Comparisons of RAG ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>와 같이 생성된 텍스트의 품질, 검색된 문서의 관련성, 잘못된 정보에 대한 모델의 복원력을 포함하여 여러 차원에 걸쳐 이러한 시스템을 평가하는 것을 목표로 하는 다양한 프레임워크와 벤치마크를 도입했다. 이러한 평가는 노이즈 견고성, 부정적인 프롬프트, 정보 통합 및 반사실적 견고성과 같은 특정 기능을 평가하는 데 중점을 두어 실제 응용 프로그램에서 RAG 시스템이 직면한 복잡한 문제를 강조한다. 평가 프레임워크와 메트릭의 지속적인 개발은 분야를 발전시키고 RAG 시스템의 적용 가능성을 넓히고 복잡하고 진화하는 정보 경관의 요구를 충족하도록 하는 데 중요하다.</p>
</div>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Retrieval-based Aspect</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.SS1.p1">
<p class="ltx_p" id="S8.SS1.p1.1">정보 검색에서 검색 결과의 품질은 일반적으로 MAP(Mean Average Precision), Precision, Reciprocal Rank, NDCG(Normalized Discounted Cumulative Gain) <cite class="ltx_cite ltx_citemacro_citep">(Radlinski and Craswell, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib81" title="">2010</a>; Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib85" title="">2019</a>; Nogueira et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib75" title="">2019</a>)</cite>와 같은 표준 메트릭을 사용하여 평가된다. 이러한 메트릭은 주로 주어진 쿼리에 대한 검색된 문서의 관련성을 평가합니다.</p>
</div>
<div class="ltx_para" id="S8.SS1.p2">
<p class="ltx_p" id="S8.SS1.p2.1">RAG의 검색 기반 메트릭은 생성 작업을 지원하기 위해 관련 정보를 검색하는 효과에 중점을 둡니다. 여기에는 쿼리에 대한 올바른 정보를 제공하는 데 있어 검색된 문서의 정밀도를 측정하는 Accuracy와 관련 정보가 발견되지 않을 때 응답을 거부하는 시스템의 능력을 평가하는 Rejection Rate <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib10" title="">2023b</a>)</cite>가 포함된다. 또한 오류 탐지율 <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib10" title="">2023b</a>)</cite>는 검색된 문서에서 올바르지 않거나 오판의 소지가 있는 정보를 식별하고 무시하는 모델의 기능을 평가합니다. 컨텍스트 관련성(Context Relevance)은 검색한 문서의 쿼리와 관련성을 평가하는 또 다른 필수 메트릭입니다. 응답을 생성하는 데 사용되는 정보가 쿼리의 컨텍스트와 직접 관련이 있는지 확인하는 것이 중요합니다. Faithfulness <cite class="ltx_cite ltx_citemacro_citep">(Shahul et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib89" title="">2023</a>)</cite>는 생성된 내용이 검색된 문서의 정보를 반영하는 정확도를 측정하여 잘못된 정보가 없는 생성 과정을 보장한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span>Generation-based Aspect</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.SS2.p1">
<p class="ltx_p" id="S8.SS2.p1.1">LLM에 의해 생성된 텍스트의 품질을 평가하는 것은 표준 메트릭을 사용하여 다양한 다운스트림 태스크에 대한 성능을 분석하는 것을 포함한다. 이러한 메트릭은 언어적 품질, 일관성, 정확성 및 생성된 텍스트가 지상 진실 데이터를 반영하는 정도를 평가한다. 언어적 품질과 일관성은 인간이 만든 텍스트와의 유창성과 유사성을 측정하는 BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib78" title="">2002</a>)</cite>, 주요 아이디어와 구문을 캡슐화하는 텍스트의 용량을 측정하기 위해 참조 요약과의 중첩을 정량화하는 ROUGE-L <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib64" title="">2004</a>)</cite> 등의 메트릭을 통해 평가된다. 정확도 및 지상 진실 데이터와의 중첩은 EM 및 F1 Score와 같은 메트릭을 사용하여 측정되며, 이는 각각 완전히 정확한 답변의 백분율을 결정하고 부정확성을 최소화하면서 관련 답변을 검색하는 데 있어 정밀도 및 재현율의 균형 있는 평가를 제공한다.</p>
</div>
<div class="ltx_para" id="S8.SS2.p2">
<p class="ltx_p" id="S8.SS2.p2.1">이러한 표준 메트릭들 외에도, 평가는 또한 태스크-특정 기준들 및 특정 애플리케이션들에 맞춤화된 신규 메트릭들을 통합할 수 있다. 예를 들어, 대화 생성에서는 응답 다양성과 자연스러움을 평가하기 위해 복잡성과 엔트로피를 사용한다. 또한 오류 발생률 및 오류 재발률 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib68" title="">2023</a>)</cite>와 같은 메트릭은 잘못된 정보 및 부정확성을 방지하는 모델의 능력을 측정합니다. 다른 전문 메트릭으로는 질문에 대한 응답의 정밀도를 평가하는 Answer Relevance <cite class="ltx_cite ltx_citemacro_citep">(Shahul et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib89" title="">2023</a>)</cite>, RAG 시스템 순위의 정확도를 평가하는 Kendall의 tau <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib87" title="">2023</a>)</cite>, 다수의 정답이 있는 태스크에서 정확도 평가를 미세 조정하는 Micro-F1 <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib87" title="">2023</a>)</cite>, 및 예측 정확도는 생성된 답변과 예상 응답의 정렬을 직접 측정하여 정확한 콘텐츠를 생성하는 시스템의 효율성에 대한 직접적인 통찰력을 제공한다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Future Directions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S9.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span>Retrieval Quality</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS1.p1">
<p class="ltx_p" id="S9.SS1.p1.1">RAG를 LLM에 통합하는 것은 가짜 뉴스를 포함한 인터넷에서 신뢰할 수 없는 방대한 정보로 인해 심각한 장애물에 직면해 있다. 이것은 유용한 지식을 정확하게 검색하기 위한 과제를 제시하여 LLM에 의한 응답의 신뢰할 수 없는 생성으로 이어진다. 결과적으로, LLMs는 부정확한 정보에 기초하여 콘텐츠를 생성할 수 있어, 그들의 신뢰성을 훼손할 수 있다. 최근의 연구 노력은 정확하고 신뢰할 수 있는 응답을 생성하는 LLM의 효율성, 확장성 및 효율성을 개선하기 위해 검색 방법을 향상시키는 데 초점을 맞추고 있다.</p>
</div>
<section class="ltx_paragraph" id="S9.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Differentiable Search Indices</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S9.SS1.SSS0.Px1.p1.1"><cite class="ltx_cite ltx_citemacro_citep">(Tay et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib94" title="">2022</a>)</cite>와 <cite class="ltx_cite ltx_citemacro_citep">(Bevilacqua et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib4" title="">2022b</a>)</cite>는 Transformer 모델 내에서 검색 프로세스를 통합하여 텍스트 쿼리를 문서 식별자에 직접 매핑할 수 있는 차별성 있는 검색 인덱스를 개발했다. 이러한 접근 방식은 보다 효율적이고 확장 가능한 검색을 위해 우수한 성능과 잠재력을 제공한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Generative Models for Search</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S9.SS1.SSS0.Px2.p1.1">GERE <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib9" title="">2022a</a>)</cite>는 사실 검증 작업을 위한 문서 제목과 증거 문장을 직접 생성할 수 있다. PARADE <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib61" title="">2024</a>)</cite>는 통로 표현을 통일된 문서 관련성 점수로 집계하는 문서 재순위화 방법이다. 이 두 가지 방법 모두 기존 방법보다 검색 품질이 크게 향상되었음을 보여준다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Fine-tuning Pre-trained Language Models</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S9.SS1.SSS0.Px3.p1.1">RankT5 <cite class="ltx_cite ltx_citemacro_citep">(Zhuang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib117" title="">2023</a>)</cite>는 텍스트 랭킹을 위해 T5 프레임워크를 구체적으로 미세 조정하는 모델이다. 순위 손실을 활용하여 성능 메트릭을 최적화하고 도메인 외 데이터에서 유망한 제로샷 성능을 보여줍니다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Noise Power</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S9.SS1.SSS0.Px4.p1.1"><cite class="ltx_cite ltx_citemacro_cite">Cuconasu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib20" title="">2024</a>)</cite>는 IR 성분이 RAG 시스템에 미치는 영향에 대한 포괄적인 분석을 제공하여 관련 없는 문서를 포함하면 정확도가 크게 향상될 수 있음을 보여준다. 기존의 검색 전략에 도전하고 검색과 언어 생성 모델을 통합하는 전문 접근 방식을 개발할 수 있는 가능성을 강조한다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S9.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span>Multimodal RAG</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS2.p1">
<p class="ltx_p" id="S9.SS2.p1.1">멀티모달 RAG 도메인은 텍스트와 시각적 이해의 융합에서 중추적인 발전을 강조하면서 상당한 성장을 경험했다. MuRAG <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib12" title="">2022b</a>)</cite>의 도입은 언어 생성을 위한 텍스트 정보와 시각 정보를 융합하여 멀티모달 데이터 세트에 대한 새로운 표준을 확립함으로써 획기적인 발전을 이루었다. 이 모델은 질의 응답 및 추론 작업에서 정확도를 높이기 위해 멀티모달 메모리 시스템을 사용하는 것의 효율성을 보여주었다.</p>
</div>
<div class="ltx_para" id="S9.SS2.p2">
<p class="ltx_p" id="S9.SS2.p2.1">MuRAG 이후, REVEAL <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib35" title="">2023</a>)</cite> 및 Re-Imagen <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib13" title="">2023c</a>)</cite>와 같은 연구는 시각적 질의 응답과 텍스트 대 이미지 생성을 향상시키는 데 중점을 두었다. 그들은 각각 동적 검색 메커니즘의 통합과 이미지 충실도의 개선을 통해 이를 달성했다. 이러한 발전은 이미지 캡션을 위한 Sarto et al. <cite class="ltx_cite ltx_citemacro_citep">(Sarto et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib88" title="">2022</a>)</cite>와 텍스트 대 오디오 생성을 위한 Yuan et al. <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib114" title="">2023</a>)</cite>와 같은 연구자들에 의해 추가 모델의 기반을 마련하여 다양한 모달리티에 걸쳐 RAG의 적용 범위를 넓히고 생성된 출력의 품질과 사실성을 향상시켰다. 또한, Re-ViLM <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib109" title="">2023b</a>)</cite> 검색 증강 시각 언어 모델을 통해 정제된 이미지 캡션 기능을 제공한다. 모델 매개변수를 미세 조정하고 혁신적인 필터링 전략을 구현함으로써 보다 정확하고 맥락적으로 적절한 캡션을 생성하는 데 진전을 이루었다. 외부 리소스를 활용함으로써 이러한 모델은 전통적인 벤치마크에 비해 상당한 향상을 제공하여 다양한 지식 소스를 통합하는 이점을 강조했습니다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Conclusions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">본 논문에서는 RAG 영역을 이해하기 위한 포괄적인 프레임워크를 제시하여 LLM의 능력 향상에 대한 중요성을 강조하였다. 본 연구는 RAG에 대한 구조화된 개요, 다양한 방법의 범주화, 핵심 기술과 평가 방법에 대한 심층 분석을 통해 향후 연구의 방향을 조명한다. 그것은 개선을 위한 중요한 영역을 식별하고 특히 텍스트 컨텍스트에서 RAG 응용 프로그램을 발전시키기 위한 잠재적인 방향을 설명한다. 이 조사는 RAG 분야의 핵심 개념을 검색 관점에서 해명하는 것을 목표로 하며, 정확한 검색과 정보 생성에 있어 추가적인 탐색과 혁신을 촉진하고자 한다.</p>
</div>
</section>
<section class="ltx_section" id="S11">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>Limitations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S11.p1">
<p class="ltx_p" id="S11.p1.1">이 조사는 기존 RAG 모델을 종합적으로 검토하여 핵심 기술을 검색 관점에서 4가지 주요 단계로 요약한다. 일부 방법은 여러 단계를 포함할 수 있으며 이러한 단계를 분리하면 고유한 연결이 잠재적으로 모호해질 수 있음을 인식한다. 그럼에도 불구하고, 주요 목표는 접근법의 복잡성을 단순화하고 그것이 다루는 특정 문제를 명확하게 설명하는 것이다. 이를 통해 추가 최적화 및 개선을 위해 익은 영역을 보다 명확하게 식별할 수 있다. 철저한 조사에도 불구하고 현장과 페이지 제한의 급속한 진화는 특정 측면이 완전히 분석되고 탐구되지 않았거나 최근 개발이 누락되었을 수 있음을 의미한다. 이 논문은 RAG 개발에 도움이 될 수 있는 평가 방법을 언급하고 있지만, 랭체인이나 라마인덱스와 같은 성숙한 도구도 유용한 자원으로 인정하고 있다. 그러나 이 조사의 초점은 평가 파이프라인을 자세히 설명하거나 이러한 도구가 구체적으로 사용되는 방법이 아니라 평가 측면이 RAG의 발전을 지원할 수 있는 방법을 설명하는 데 있다. 이 선택은 RAG 모델을 정제하고 개선하는 데 방법론적 명확성과 평가 도구의 적용의 중요성을 강조하면서 향후 작업을 위한 영역을 강조한다.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">이 작업은 캐나다의 자연 과학 및 공학 연구 위원회(NSERC)와 요크 연구 의장(YRC) 프로그램의 지원을 받았다.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock">Self-RAG: Learning to Retrieve, Generate, and Critique
through Self-Reflection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv</em>, abs/2310.11511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berchansky et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat.
2023.

</span>
<span class="ltx_bibblock">Optimizing Retrieval-augmented Reader Models via Token
Elimination.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em>, pages 1506–1524.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bevilacqua et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Michele Bevilacqua, Giuseppe Ottaviano, Patrick S.&nbsp;H. Lewis, Scott Yih,
Sebastian Riedel, and Fabio Petroni. 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2022/hash/cd88d62a2063fdaf7ce6f9068fb15dcd-Abstract-Conference.html" title="">Autoregressive search engines: Generating substrings as document
identifiers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Advances in Neural Information Processing Systems 35: Annual
Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New
Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bevilacqua et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Michele Bevilacqua, Giuseppe Ottaviano, Patrick S.&nbsp;H. Lewis, Scott Yih,
Sebastian Riedel, and Fabio Petroni. 2022b.

</span>
<span class="ltx_bibblock">Autoregressive Search Engines: Generating Substrings as
Document Identifiers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.5281/zenodo.5297715" title="">GPT-Neo: Large Scale
Autoregressive Language Modeling with Mesh-Tensorflow</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
Rutherford, Katie Millican, George van&nbsp;den Driessche, Jean-Baptiste Lespiau,
Bogdan Damoc, Aidan Clark, Diego de&nbsp;Las Casas, Aurelia Guy, Jacob Menick,
Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin
Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon
Osindero, Karen Simonyan, Jack&nbsp;W. Rae, Erich Elsen, and Laurent Sifre. 2022.

</span>
<span class="ltx_bibblock">Improving Language Models by Retrieving from Trillions of
Tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">International Conference on Machine Learning
(ICML)</em>, pages 2206–2240.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>, volume abs/2005.14165.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz.
2023a.

</span>
<span class="ltx_bibblock">Walking Down the Memory Maze: Beyond Context Limit through
Interactive Reading.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv</em>, abs/2310.05029.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng.
2022a.

</span>
<span class="ltx_bibblock">Gere: Generative Evidence Retrieval for Fact Verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval</em>.
ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le&nbsp;Sun. 2023b.

</span>
<span class="ltx_bibblock">Benchmarking large language models in retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv</em>, abs/2309.01431.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de&nbsp;Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, and others. 2021.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv</em>, abs/2107.03374.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenhu Chen, Hexiang Hu, Xi&nbsp;Chen, Pat Verga, and William Cohen.
2022b.

</span>
<span class="ltx_bibblock">Murag: Multimodal Retrieval-Augmented Generator for Open
Question Answering over Images and Text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William&nbsp;W. Cohen.
2023c.

</span>
<span class="ltx_bibblock">Re-Imagen: Retrieval-Augmented Text-to-Image Generator.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023d)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen,
Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, and others.
2023d.

</span>
<span class="ltx_bibblock">Phoenix: Democratizing chatgpt across languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv</em>, abs/2304.10453.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing
Wang, Hao Sun, Furu Wei, Weiwei Deng, and Qi&nbsp;Zhang. 2023a.

</span>
<span class="ltx_bibblock">Uprise: Universal Prompt Retrieval for Improving Zero-Shot
Evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em>, pages 12318–12337.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xin Cheng, Di&nbsp;Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan.
2023b.

</span>
<span class="ltx_bibblock">Lift Yourself Up: Retrieval-augmented Text Generation with
Self-Memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Thirty-seventh Conference on Neural Information
Processing Systems</em>, volume abs/2305.02437.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian
Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
Abhishek Rao, Parker Barnes, Yi&nbsp;Tay, Noam Shazeer, Vinodkumar Prabhakaran,
Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
Dohan, Shivani Agrawal, Mark Omernick, Andrew&nbsp;M. Dai,
Thanumalayan&nbsp;Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
2023.

</span>
<span class="ltx_bibblock">Palm: Scaling Language Modeling with Pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Journal of Machine Learning Research (JMLR)</em>,
24:240:1–240:113.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Le&nbsp;Hou, S.&nbsp;Longpre, Barret Zoph, Yi&nbsp;Tay, W.&nbsp;Fedus, Eric Li,
Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, S.&nbsp;Gu,
Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter,
Sharan Narang, Gaurav Mishra, Adams&nbsp;Wei Yu, Vincent Zhao, Yanping Huang,
Andrew&nbsp;M. Dai, Hongkun Yu, Slav Petrov, E.&nbsp;Chi, J.&nbsp;Dean, Jacob Devlin, Adam
Roberts, Denny Zhou, Quoc&nbsp;V. Le, and Jason Wei. 2022.

</span>
<span class="ltx_bibblock">Scaling Instruction-Finetuned Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv</em>, abs/2210.11416.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock">Unsupervised Cross-lingual Representation Learning at Scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics</em>, pages 8440–8451.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuconasu et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare
Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024.

</span>
<span class="ltx_bibblock">The Power of Noise: Redefining Retrieval for RAG Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv</em>, abs/2401.14887.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhuyun Dai, Vincent&nbsp;Y. Zhao, Ji&nbsp;Ma, Yi&nbsp;Luan, Jianmo Ni, Jing Lu, Anton Bakalov,
Kelvin Guu, Keith&nbsp;B. Hall, and Ming-Wei Chang. 2023.

</span>
<span class="ltx_bibblock">Promptagator: Few-shot Dense Retrieval From 8 Examples.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Bert: Pre-training of Deep Bidirectional Transformers for
Language Understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2019 Conference of the North</em>, pages
4171–4186. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinan et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason
Weston. 2019.

</span>
<span class="ltx_bibblock">Wizard of Wikipedia: Knowledge-Powered Conversational Agents.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
Jie Tang. 2022.

</span>
<span class="ltx_bibblock">Glm: General Language Model Pretraining with Autoregressive
Blank Infilling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)</em>.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ElSahar et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier,
Jonathon&nbsp;S. Hare, Frédérique Laforest, and Elena Simperl. 2018.

</span>
<span class="ltx_bibblock">T-REx: A Large Scale Alignment of Natural Language with
Knowledge Base Triples.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">International Conference on Language Resources and
Evaluation (LREC)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2023.

</span>
<span class="ltx_bibblock">Retrieval-generation synergy augmented large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv</em>, abs/2310.05149.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fox and Shaw (1994)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edward&nbsp;A. Fox and Joseph&nbsp;A. Shaw. 1994.

</span>
<span class="ltx_bibblock">Combination of multiple searches.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">TREC-2: Text retrieval conference</em>, 500215, pages 105–108.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.

</span>
<span class="ltx_bibblock">Simcse: Simple Contrastive Learning of Sentence Embeddings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing</em>, pages 6894–6910.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glass et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Michael Glass, Gaetano Rossiello, Md&nbsp;Faisal&nbsp;Mahbub Chowdhury, Ankita Naik,
Pengshan Cai, and Alfio Gliozzo. 2022.

</span>
<span class="ltx_bibblock">Re2g: Retrieve, Rerank, Generate.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies</em>, pages 2701–2715. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gottschalk and Demidova (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Simon Gottschalk and Elena Demidova. 2018.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">EventKG: A Multilingual Event-Centric Temporal
Knowledge Graph</em>.

</span>
<span class="ltx_bibblock">Springer International Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.

</span>
<span class="ltx_bibblock">Retrieval Augmented Language Model Pre-Training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">International Conference on Machine Learning
(ICML)</em>, pages 3929–3938.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamilton (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
William&nbsp;L. Hamilton. 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Graph representation learning</em>.

</span>
<span class="ltx_bibblock">Springer International Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hancock-Beaulieu et&nbsp;al. (1996)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Micheline Hancock-Beaulieu, Mike Gatford, Xiangji Huang, Stephen&nbsp;E.
Robertson, Steve Walker, and P.&nbsp;W. Williams. 1996.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://trec.nist.gov/pubs/trec5/papers/city.procpaper.ps.gz" title="">Okapi at TREC-5</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of The Fifth Text REtrieval Conference, TREC
1996, Gaithersburg, Maryland, USA, November 20-22, 1996</em>, volume 500-238 of
<em class="ltx_emph ltx_font_italic" id="bib.bib33.2.2">NIST Special Publication</em>. National Institute of Standards and
Technology (NIST).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hofstätter et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock">Fid-light: Efficient and effective retrieval-augmented text
generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Information Retrieval</em>,
pages 1437–1447.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun,
Cordelia Schmid, David&nbsp;A. Ross, and Alireza Fathi. 2023.

</span>
<span class="ltx_bibblock">Reveal: Retrieval-Augmented Visual-Language Pre-Training
with Multi-Source Multimodal Knowledge Memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">2023 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, pages 23369–23379. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang, Jinjun Xiong, and Wen-mei Hwu.
2022.

</span>
<span class="ltx_bibblock">Understanding Jargon: Combining Extraction and Generation for
Definition Modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing</em>. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2013)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jimmy&nbsp;Xiangji Huang, Jun Miao, and Ben He. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/J.IPM.2012.08.002" title="">High performance
query expansion using adaptive co-training</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Inf. Process. Manag.</em>, 49(2):441–453.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenyu Huang, Mirella Lapata, Pavlos Vougiouklis, Nikos Papasarantopoulos, and
Jeff&nbsp;Z Pan. 2023.

</span>
<span class="ltx_bibblock">Retrieval Augmented Generation with Rich Answer Encoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proc. of IJCNLP-AACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Hu (2009)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiangji Huang and Qinmin Hu. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/1571941.1571995" title="">A bayesian learning
approach to promoting diversity in ranking for biomedical information
retrieval</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 32nd Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval, SIGIR
2009, Boston, MA, USA, July 19-23, 2009</em>, pages 307–314. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Huang (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yizheng Huang and Jimmy Huang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2402.11203" title="">Exploring chatgpt
for next-generation information retrieval: Opportunities and challenges</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">CoRR</em>, abs/2402.11203.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Huang (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yizheng Huang and Jimmy&nbsp;X. Huang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3233/FAIA230385" title="">Diversified prior
knowledge enhanced general language model for biomedical information
retrieval</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">ECAI 2023 - 26th European Conference on Artificial
Intelligence, September 30 - October 4, 2023, Kraków, Poland -
Including 12th Conference on Prestigious Applications of Intelligent Systems
(PAIS 2023)</em>, volume 372 of <em class="ltx_emph ltx_font_italic" id="bib.bib41.2.2">Frontiers in Artificial Intelligence and
Applications</em>, pages 1109–1115. IOS Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr
Bojanowski, Armand Joulin, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock">Unsupervised Dense Information Retrieval with Contrastive
Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Transactions on Machine Learning Research (TMLR)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2021.

</span>
<span class="ltx_bibblock">Leveraging Passage Retrieval with Generative Models for
Open Domain Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 16th Conference of the European
Chapter of the Association for Computational Linguistics: Main
Volume</em>, pages 874–880. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick S.&nbsp;H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio
Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2023.

</span>
<span class="ltx_bibblock">Atlas: Few-shot Learning with Retrieval Augmented Language
Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Journal of Machine Learning Research (JMLR)</em>, 24:251:1–251:43.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jahan et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Israt Jahan, Md. Tahmid&nbsp;Rahman Laskar, Chun Peng, and Jimmy&nbsp;Xiangji Huang.
2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2306.04504" title="">Evaluation of
chatgpt on biomedical tasks: A zero-shot comparison with fine-tuned
generative transformers</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CoRR</em>, abs/2306.04504.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jansen et&nbsp;al. (2009)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bernard&nbsp;J. Jansen, Danielle&nbsp;L. Booth, and Amanda Spink. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1002/ASI.21071" title="">Patterns of query
reformulation during web searching</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">J. Assoc. Inf. Sci. Technol.</em>, 60(7):1358–1371.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
Yejin Bang, Andrea Madotto, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3571730" title="">Survey of hallucination in
natural language generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">ACM Comput. Surv.</em>, 55(12):248:1–248:38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengbao Jiang, Frank&nbsp;F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu,
Yiming Yang, Jamie Callan, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock">Active Retrieval Augmented Generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Conference on Empirical Methods in Natural Language
Processing (EMNLP)</em>, pages 7969–7992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TBDATA.2019.2921572" title="">Billion-scale
similarity search with gpus</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">IEEE Transactions on Big Data</em>, 7(3):535–547.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock">Triviaqa: A Large Scale Distantly Supervised Challenge
Dataset for Reading Comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)</em>,
pages 1601–1611. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Minki Kang, Jin&nbsp;Myung Kwak, Jinheon Baek, and Sung&nbsp;Ju Hwang. 2023.

</span>
<span class="ltx_bibblock">Knowledge Graph-Augmented Language Models for
Knowledge-Grounded Dialogue Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv</em>, abs/2305.18846.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S.&nbsp;H. Lewis, Ledell Wu,
Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock">Dense Passage Retrieval for Open-Domain Question
Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Conference on Empirical Methods in Natural Language
Processing (EMNLP)</em>, pages 6769–6781.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
2020.

</span>
<span class="ltx_bibblock">Generalization through Memorization: Nearest Neighbor Language
Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Khattab, Keshav Santhanam, Xiang&nbsp;Lisa Li, David Leo&nbsp;Wright Hall, Percy
Liang, Christopher Potts, and M.&nbsp;Zaharia. 2022.

</span>
<span class="ltx_bibblock">Demonstrate-Search-Predict: Composing retrieval and language
models for knowledge-intensive NLP.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv</em>, abs/2212.14024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab and Zaharia (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Omar Khattab and Matei Zaharia. 2020.

</span>
<span class="ltx_bibblock">Colbert - Efficient and Effective Passage Search via
Contextualized Late Interaction over BERT.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 43rd International ACM SIGIR
Conference on Research and Development in Information Retrieval</em>,
pages 39–48. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang,
Andrew&nbsp;M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock">Natural Questions: A Benchmark for Question Answering
Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Transactions of the Association for Computational Linguistics</em>,
7:453–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laskar et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Md. Tahmid&nbsp;Rahman Laskar, M.&nbsp;Saiful Bari, Mizanur Rahman, Md&nbsp;Amran&nbsp;Hossen
Bhuiyan, Shafiq Joty, and Jimmy&nbsp;Xiangji Huang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.18486" title="">A systematic study
and comprehensive evaluation of chatgpt on benchmark datasets</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">CoRR</em>, abs/2305.18486.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laskar et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Md. Tahmid&nbsp;Rahman Laskar, Enamul Hoque, and Jimmy&nbsp;X. Huang. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-030-47358-7_35" title="">Query focused
abstractive summarization via incorporating query relevance and transfer
learning with transformer models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Advances in Artificial Intelligence - 33rd Canadian
Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON,
Canada, May 13-15, 2020, Proceedings</em>, volume 12109 of <em class="ltx_emph ltx_font_italic" id="bib.bib58.2.2">Lecture Notes in
Computer Science</em>, pages 342–348. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a.

</span>
<span class="ltx_bibblock">Bart: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics</em>, pages 7871–7880.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Patrick S.&nbsp;H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020b.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP
Tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. 2024.

</span>
<span class="ltx_bibblock">Parade: Passage Representation Aggregation forDocument
Reranking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">ACM Transactions on Information Systems</em>, 42(2):1–26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022.

</span>
<span class="ltx_bibblock">A Survey on Retrieval-Augmented Text Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv</em>, abs/2202.01110.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xingxuan Li, Ruochen Zhao, Yew&nbsp;Ken Chia, Bosheng Ding, Shafiq&nbsp;R. Joty, Soujanya
Poria, and Lidong Bing. 2023.

</span>
<span class="ltx_bibblock">Chain-of-Knowledge: Grounding Large Language Models via
Dynamic Knowledge Adapting over Heterogeneous Sources.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W04-1013" title="">ROUGE: A package for
automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Text Summarization Branches Out</em>, pages 74–81, Barcelona,
Spain. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad,
Wen-tau Yih, and Xilun Chen. 2023a.

</span>
<span class="ltx_bibblock">How to Train Your Dragon: Diverse Augmentation Towards
Generalizable Dense Retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Findings of the Association for Computational
Linguistics: EMNLP 2023</em>, pages 6385–6400. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James,
Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, and others.
2023b.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv</em>, abs/2310.01352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,
Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth
Pasunuru, Sam Shleifer, Punit&nbsp;Singh Koura, Vishrav Chaudhary, Brian
O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona
Diab, Veselin Stoyanov, and Xian Li. 2022.

</span>
<span class="ltx_bibblock">Few-shot Learning with Multilingual Generative Language
Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing</em>. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie
Zhou, and Xu&nbsp;Sun. 2023.

</span>
<span class="ltx_bibblock">Recall: A Benchmark for LLMs Robustness against External
Counterfactual Knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">arXiv</em>, abs/2311.08147.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ziyang Luo, Can Xu, Pu&nbsp;Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin,
and Daxin Jiang. 2023.

</span>
<span class="ltx_bibblock">Augmented Large Language Models with Parametric Knowledge
Guiding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv</em>, abs/2305.04757.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock">Query Rewriting in Retrieval-Augmented Large Language
Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em>, pages 5303–5315.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malkov and Yashunin (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yu&nbsp;A. Malkov and D.&nbsp;A. Yashunin. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TPAMI.2018.2889473" title="">Efficient and
robust approximate nearest neighbor search using hierarchical navigable small
world graphs</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 42(4):824–836.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manning et&nbsp;al. (2008)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher&nbsp;D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Introduction to Information Retrieval</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and
others. 2021.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv</em>, abs/2112.09332.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo&nbsp;Hernandez Abrego, Ji&nbsp;Ma,
Vincent Zhao, Yi&nbsp;Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.

</span>
<span class="ltx_bibblock">Large Dual Encoders Are Generalizable Retrievers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing</em>, pages 9844–9855.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nogueira et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019.

</span>
<span class="ltx_bibblock">Multi-stage document ranking with BERT.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">CoRR</em>, abs/1910.14424.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
Florencia&nbsp;Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom,
Paul Baltescu, Haiming Bao, Mo&nbsp;Bavarian, Jeff Belgum, Irwan Bello, Jake
Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg
Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany
Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis
Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben
Chess, Chester Cho, Casey Chu, Hyung&nbsp;Won Chung, Dave Cummings, Jeremiah
Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien
Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien
Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
Simón&nbsp;Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges,
Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross,
Shixiang&nbsp;Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen
He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,
Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost
Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang,
Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan,
Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish&nbsp;Shirish Keskar,
Tabarak Khan, Logan Kilpatrick, Jong&nbsp;Wook Kim, Christina Kim, Yongjik Kim,
Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz
Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade
Leung, Daniel Levy, Chak&nbsp;Ming Li, Rachel Lim, Molly Lin, Stephanie Lin,
Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim
Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott&nbsp;Mayer McKinney, Christine McLeavey,
Paul McMillan, Jake McNeil, and others. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">PREPRINT</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul&nbsp;F. Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et&nbsp;al. (2002)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for
automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the 40th Annual Meeting on Association for
Computational Linguistics</em>, ACL ’02, page 311–318, USA. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.

</span>
<span class="ltx_bibblock">Instruction tuning with gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">arXiv</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani,
Nicola&nbsp;De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean
Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel.
2021.

</span>
<span class="ltx_bibblock">Kilt: a Benchmark for Knowledge Intensive Language Tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies</em>, pages 2523–2544. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radlinski and Craswell (2010)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Filip Radlinski and Nick Craswell. 2010.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/1835449.1835560" title="">Comparing the
sensitivity of information retrieval metrics</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Proceedings of the 33rd International ACM SIGIR Conference
on Research and Development in Information Retrieval</em>, SIGIR ’10, page
667–674, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam&nbsp;M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified
Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Journal of Machine Learning Research (JMLR)</em>, 21:140:1–140:67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin
Leyton-Brown, and Yoav Shoham. 2023.

</span>
<span class="ltx_bibblock">In-Context Retrieval-Augmented Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">Transactions of the Association for Computational Linguistics</em>,
11:1316–1331.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. 2022.

</span>
<span class="ltx_bibblock">Learning to Retrieve Passages without Supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence Embeddings using Siamese
BERT-Networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP)</em>, pages 3980–3990. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Zaragoza (2009)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephen Robertson and Hugo Zaragoza. 2009.

</span>
<span class="ltx_bibblock">The Probabilistic Relevance Framework: Bm25 and Beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">Foundations and Trends® in Information
Retrieval</em>, 3(4):333–389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saad-Falcon et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jon Saad-Falcon, O.&nbsp;Khattab, Christopher Potts, and Matei Zaharia. 2023.

</span>
<span class="ltx_bibblock">Ares: An Automated Evaluation Framework for
Retrieval-Augmented Generation Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">arXiv</em>, abs/2311.09476.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarto et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 2022.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Transformer for Image Captioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">International Conference on Content-based Multimedia
Indexing</em>. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shahul et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
ES&nbsp;Shahul, Jithin James, Luis&nbsp;Espinosa Anke, and S.&nbsp;Schockaert. 2023.

</span>
<span class="ltx_bibblock">Ragas: Automated Evaluation of Retrieval Augmented
Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">arXiv</em>, abs/2309.15217.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen.
2023.

</span>
<span class="ltx_bibblock">Enhancing Retrieval-Augmented Large Language Models with
Iterative Retrieval-Generation Synergy.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Findings of the Association for Computational
Linguistics: EMNLP 2023</em>, pages 9248–9274. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis,
Luke Zettlemoyer, and Wen-tau Yih. 2023.

</span>
<span class="ltx_bibblock">Replug: Retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">arXiv</em>, abs/2301.12652.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiqing Sun, Xuezhi Wang, Yi&nbsp;Tay, Yiming Yang, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Recitation-Augmented Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Tay, Mostafa Dehghani, Vinh&nbsp;Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang,
Hyung&nbsp;Won Chung, Dara Bahri, Tal Schuster, Huaixiu&nbsp;Steven Zheng, Denny Zhou,
Neil Houlsby, and Donald Metzler. 2023.

</span>
<span class="ltx_bibblock">Ul2: Unifying Language Learning Paradigms.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen
Qin, Kai Hui, Zhe Zhao, Jai&nbsp;Prakash Gupta, Tal Schuster, William&nbsp;W. Cohen,
and Donald Metzler. 2022.

</span>
<span class="ltx_bibblock">Transformer Memory as a Differentiable Search Index.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.
2018.

</span>
<span class="ltx_bibblock">Fever: a Large-scale Dataset for Fact Extraction and
VERification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers)</em>, pages
809–819. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal
Azhar, and others. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">arXiv</em>, abs/2302.13971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
and others. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">arxiv</em>, abs/2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.
2023.

</span>
<span class="ltx_bibblock">Interleaving Retrieval with Chain-of-Thought Reasoning for
Knowledge-Intensive Multi-Step Questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)</em>,
pages 10014–10037. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam&nbsp;M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is All you Need.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">Neural Information Processing Systems</em>, pages
5998–6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel&nbsp;R. Bowman. 2019.

</span>
<span class="ltx_bibblock">Superglue: A Stickier Benchmark for General-Purpose
Language Understanding Systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>, pages 3261–3275.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Komatsuzaki (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ben Wang and Aran Komatsuzaki. 2021.

</span>
<span class="ltx_bibblock">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/kingoflolz/mesh-transformer-jax" title="">https://github.com/kingoflolz/mesh-transformer-jax</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, and Furu Wei. 2023a.

</span>
<span class="ltx_bibblock">Query2doc: Query Expansion with Large Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em>, pages 9414–9423.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xintao Wang, Qian Yang, Yongting Qiu, Jiaqing Liang, Qi&nbsp;He, Zhouhong Gu,
Yanghua Xiao, and W.&nbsp;Wang. 2023b.

</span>
<span class="ltx_bibblock">Knowledgpt: Enhancing Large Language Models with Retrieval
and Storage Access on Knowledge Bases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">arXiv</em>, abs/2308.11761.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Workshop et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
BigScience Workshop, Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie
Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha
Luccioni, François Yvon, and others. 2022.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">arXiv</em>, abs/2211.05100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lee Xiong, Chenyan Xiong, Ye&nbsp;Li, Kwok-Fung Tang, Jialin Liu, Paul&nbsp;N. Bennett,
Junaid Ahmed, and Arnold Overwijk. 2021.

</span>
<span class="ltx_bibblock">Approximate Nearest Neighbor Negative Contrastive Learning
for Dense Text Retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.

</span>
<span class="ltx_bibblock">Recomp: Improving Retrieval-Augmented LMs with Compression
and Selective Augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">arXiv</em>, abs/2310.04408.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and
Jing Xiao. 2023a.

</span>
<span class="ltx_bibblock">Prca: Fitting Black-Box Large Language Models for
Retrieval Question Answering via Pluggable Reward-Driven
Contextual Adapter.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em>, pages 5364–5375.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
Salakhutdinov, and Christopher&nbsp;D. Manning. 2018.

</span>
<span class="ltx_bibblock">Hotpotqa: A Dataset for Diverse, Explainable Multi-hop
Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing</em>, pages 2369–2380.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang,
Linxi Fan, Zhiding Yu, Shiyi Lan, Bo&nbsp;Li, Mohammad Shoeybi, Ming-Yu Liu, Yuke
Zhu, Bryan Catanzaro, Chaowei Xiao, and Anima Anandkumar. 2023b.

</span>
<span class="ltx_bibblock">Re-ViLM: Retrieval-Augmented Visual Language Model for
Zero and Few-Shot Image Captioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">Findings of the Association for Computational
Linguistics: EMNLP 2023</em>, pages 11844–11857. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul&nbsp;N. Bennett, Jianfeng Gao,
and Zhiyuan Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3397271.3401323" title="">Few-shot generative
conversational query rewriting</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">Proceedings of the 43rd International ACM SIGIR
conference on research and development in Information Retrieval, SIGIR
2020, Virtual Event, China, July 25-30, 2020</em>, pages 1933–1936. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal,
Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023a.

</span>
<span class="ltx_bibblock">Generate rather than Retrieve: Large Language Models are
Strong Context Generators.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and
Meng Jiang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3512467" title="">A survey of
knowledge-enhanced text generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">ACM Comput. Surv.</em>, 54(11s):227:1–227:38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. 2023b.

</span>
<span class="ltx_bibblock">Augmentation-Adapted Retriever Improves Generalization of
Language Models as Generic Plug-In.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)</em>,
pages 2421–2436. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark&nbsp;D Plumbley, and Wenwu Wang.
2023.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Text-to-Audio Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">arXiv</em>, abs/2309.08051.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi&nbsp;Victoria Lin, and others.
2022.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">arXiv</em>, abs/2205.01068.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Huaixiu&nbsp;Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, E.&nbsp;Chi,
Quoc&nbsp;V. Le, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Take a Step Back: Evoking Reasoning via Abstraction in
Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">arXiv</em>, abs/2310.06117.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji&nbsp;Ma, Jing Lu, Jianmo Ni,
Xuanhui Wang, and Michael Bendersky. 2023.

</span>
<span class="ltx_bibblock">Rankt5: Fine-Tuning T5 for Text Ranking with Ranking
Losses.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Information Retrieval</em>.
ACM.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header" data-bs-theme="dark"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
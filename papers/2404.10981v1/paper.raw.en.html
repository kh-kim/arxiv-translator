<html lang="en" data-theme="dark"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>A Survey on Retrieval-Augmented Text Generation for Large Language Models</title>
<!--Generated on Wed May  1 15:57:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.10981v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2404.10981v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="Dark mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2404.10981v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2404.10981v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S1" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>RAG Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS1" title="In 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Basic RAG Workflow</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS1.SSS1" title="In 2.1 Basic RAG Workflow ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Indexing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS1.SSS2" title="In 2.1 Basic RAG Workflow ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS1.SSS3" title="In 2.1 Basic RAG Workflow ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2" title="In 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>RAG Paradigm</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS1" title="In 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Pre-Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS1.Px1" title="In 2.2.1 Pre-Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Indexing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS1.Px2" title="In 2.2.1 Pre-Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Query Manipulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS1.Px3" title="In 2.2.1 Pre-Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Data Modification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS2" title="In 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS2.Px1" title="In 2.2.2 Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Search &amp; Ranking</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS3" title="In 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Post-Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS3.Px1" title="In 2.2.3 Post-Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Re-Ranking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS3.Px2" title="In 2.2.3 Post-Retrieval ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Filtering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS4" title="In 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.4 </span>Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS4.Px1" title="In 2.2.4 Generation ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Enhancing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.SS2.SSS4.Px2" title="In 2.2.4 Generation ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Customization</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S3" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Pre-Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S3.SS1" title="In 3 Pre-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Indexing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S3.SS2" title="In 3 Pre-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Query Manipulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S3.SS3" title="In 3 Pre-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Data Modification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S4" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S4.SS1" title="In 4 Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Search &amp; Ranking</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S5" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Post-Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S5.SS1" title="In 5 Post-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Re-Ranking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S5.SS2" title="In 5 Post-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Filtering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S6" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Generation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S6.SS1" title="In 6 Generation ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Enhancing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S6.SS2" title="In 6 Generation ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Customization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S7" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Comparisons of RAG</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S7.SS1" title="In 7 Comparisons of RAG ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>The Comprehensive Summary of RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S7.SS2" title="In 7 Comparisons of RAG ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Retriever and Generator</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S8" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Evaluation in RAG</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S8.SS1" title="In 8 Evaluation in RAG ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Retrieval-based Aspect</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S8.SS2" title="In 8 Evaluation in RAG ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Generation-based Aspect</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Future Directions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS1" title="In 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1 </span>Retrieval Quality</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS1.SSS0.Px1" title="In 9.1 Retrieval Quality ‣ 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Differentiable Search Indices</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS1.SSS0.Px2" title="In 9.1 Retrieval Quality ‣ 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Generative Models for Search</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS1.SSS0.Px3" title="In 9.1 Retrieval Quality ‣ 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Fine-tuning Pre-trained Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS1.SSS0.Px4" title="In 9.1 Retrieval Quality ‣ 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title">Noise Power</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S9.SS2" title="In 9 Future Directions ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2 </span>Multimodal RAG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S10" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S11" title="In A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div class="package-alerts ltx_document" role="status" aria-label="Conversion errors have been found">
      <button aria-label="Dismiss alert" onclick="closePopup()">
          <span aria-hidden="true"><svg role="presentation" width="20" height="20" viewBox="0 0 44 44" aria-hidden="true" focusable="false">
          <path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
          <path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
          </svg></span>
      </button>
      <p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
          <ul arial-label="Unsupported packages used in this paper">
              <li>failed: inconsolata</li>,<li>failed: tabularray</li>,<li>failed: forest</li>
          </ul>
      <p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
    </div><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: arXiv.org perpetual non-exclusive license</a><div id="watermark-tr">arXiv:2404.10981v1 [cs.IR] 17 Apr 2024</div></div>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">A Survey on Retrieval-Augmented Text Generation for Large Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yizheng Huang 
<br class="ltx_break">York University 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">hyz@yorku.ca</span>
<br class="ltx_break"><span class="ltx_ERROR undefined" id="id2.2.id2">\And</span>Jimmy X. Huang 
<br class="ltx_break">York University 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">jhuang@yorku.ca</span>
<br class="ltx_break">
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id4.id1">Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG’s evolution and discusses the field’s progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">A Survey on Retrieval-Augmented Text Generation for Large Language Models</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<br class="ltx_break ltx_centering">
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Yizheng Huang</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">York University</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.3.3.1.1">hyz@yorku.ca</span></span></span>
</span>
</span></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_text ltx_inline-block" id="p1.1.2.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.2.1.1.1.1.1">Jimmy X. Huang</span></span></span>
<span class="ltx_tr" id="p1.1.2.2.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.2.2.1">York University</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.2.1.3.3.1.1">jhuang@yorku.ca</span></span></span>
</span>
</span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<br class="ltx_break ltx_centering">
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The advent of ChatGPT has significantly impacted both academia and industry due to its interactive capabilities and widespread application, establishing itself as a leading artificial intelligence tool <cite class="ltx_cite ltx_citemacro_citep">(Laskar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib57" title="">2023</a>; Jahan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib45" title="">2023</a>; Huang and Huang, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib40" title="">2024</a>)</cite>. At the core of ChatGPT is the large language model (LLM) GPT-4, as detailed by <cite class="ltx_cite ltx_citemacro_citep">(OpenAI et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib76" title="">2023</a>)</cite>, which has seen numerous enhancements to its predecessors, showcasing exceptional abilities in a variety of Natural Language Processing (NLP) tasks <cite class="ltx_cite ltx_citemacro_citep">(Laskar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib58" title="">2020</a>)</cite>. Despite these advancements, the adoption of LLMs has highlighted several critical issues primarily due to their reliance on extensive datasets. This reliance restricts their ability to incorporate new information post-training, leading to three primary challenges. First, the focus on broad and general data to maximize accessibility and applicability results in subpar performance in specialized areas. Second, the rapid creation of online data, combined with the significant resources required for data annotation and model training, hinders LLMs’ ability to stay updated. Third, LLMs are susceptible to generating convincing yet inaccurate responses, known as “hallucinations”, which can mislead users.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="70" id="S1.F1.g1" src="extracted/2404.10981v1/RAG_example.png" width="281">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of RAG benefits ChatGPT resolves questions that cannot be answered beyond the scope of the training data and generates correct results.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Addressing these challenges is crucial for LLMs to be effectively utilized across various domains. A promising solution is the integration of Retrieval-Augmented Generation (RAG) technology, which supplements models by fetching external data in response to queries, thus ensuring more accurate and current outputs. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates how RAG can enable ChatGPT to provide precise answers beyond its initial training data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Since its introduction by Lewis et al. <cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>)</cite> in 2020, RAG technology has undergone significant advancements, particularly influenced by ChatGPT’s success. However, there is a noticeable gap in the literature regarding a thorough analysis of RAG’s mechanisms and the progress made by subsequent studies. Furthermore, the field is characterized by diverse research focuses and the use of ambiguous terminology for similar methods, leading to confusion. This paper aims to clarify these aspects by offering a structured overview of RAG, categorizing various methods, and delivering an in-depth understanding of this research area. This survey will primarily focus on textual applications of RAG, reflecting the current emphasis of research efforts in this area.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">RAG combines retrieval methods and advanced deep learning to address two main questions: effectively retrieving relevant information and generating accurate responses. The workflow of RAG is outlined in Section 2, categorizing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. These sections, from 3 to 6, provide an in-depth analysis of the technologies within these phases. Section 7 offers summaries of the reviewed studies, along with the retrievers and generators utilized. Section 8 details the evaluation methodologies for RAG. Section 9 explores future research directions, concentrating on text-based studies and extending to image and multimodal data considerations. The conclusion is presented in Section 10.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The contributions of this paper are threefold: This paper offers a comprehensive framework for understanding the RAG domain, identifying areas for improvement and challenges for future research. It provides a detailed analysis of RAG’s core technologies, examining their strengths in addressing retrieval and generation. Additionally, it introduces the evaluation methods used in RAG research, highlighting current challenges and suggesting promising directions for future studies.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="246" id="S1.F2.g1" src="extracted/2404.10981v1/RAG_framework.png" width="598">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An unified RAG framework with basic workflow and paradigm.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>RAG Framework</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The hallucinations are largely attributed to LLMs’ inability to access up-to-date information. This limitation stems from the models’ reliance on their training datasets. RAG proposes a solution to this issue by supplementing the LLM’s training data with current information from external sources through a retrieval model, thereby enabling the generation of accurate responses. RAG presents a more cost-effective alternative to the extensive training and fine-tuning processes typically required for LLMs. It allows for the dynamic incorporation of fresh information via traditional retrieval methods or pre-trained LMs, without the need to directly integrate this new data into the LLM. This feature makes RAG both flexible and scalable, facilitating its application across different LLMs for various purposes. The information retrieved through RAG is derived from real-world data, authored by humans, which not only simplifies the generation process but also increases the reliability of the generated responses. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> represents the unified RAG framework with basic workflow and paradigm.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Research by Khandelwal et al. <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>)</cite> demonstrates that accessing relevant information from the training dataset itself can significantly improve LLM performance, highlighting the effectiveness of RAG. Over time, RAG has evolved from a means of providing supplementary information to enabling multiple interactions between the retrieval and generation components. This involves conducting several rounds of retrieval to refine the accuracy of the information retrieved and iteratively improve the quality of the generated output. Platforms such as LangChain<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.langchain.com</span></span></span> and LlamaIndex<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://www.llamaindex.ai</span></span></span> have modularized the RAG approach, enhancing its adaptability and expanding its range of applications. Despite these platforms employing diverse methodologies to tackle different aspects of RAG—from multiple search iterations to iterative generation—they maintain adherence to the fundamental RAG workflow. This consistency is crucial for understanding their operation and pinpointing opportunities for further development.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Basic RAG Workflow</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The foundational workflow of RAG begins with the creation of an index comprising external sources. This index serves as the basis for retrieving relevant information through a retriever model based on a specific query. The final step involves a generator model, which combines the retrieved information with the query to produce the desired output.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Indexing</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Efficient retrieval begins with comprehensive indexing, where data preparation is key. This stage involves text normalization processes such as tokenization, stemming, and the removal of stop words to enhance the text’s suitability for indexing <cite class="ltx_cite ltx_citemacro_citep">(Manning et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib72" title="">2008</a>)</cite>. Text segments are then organized into sentences or paragraphs to facilitate more focused searches, allowing for the pinpointing of segments containing pertinent keywords. The integration of deep learning has revolutionized indexing through the use of pretrained LMs for generating semantic vector representations of texts. These vectors are stored, enabling rapid and precise retrieval from extensive data collections, significantly enhancing retrieval efficiency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Retrieval</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">While traditional retrieval methods, such as the BM25 algorithm <cite class="ltx_cite ltx_citemacro_citep">(Hancock-Beaulieu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib33" title="">1996</a>)</cite>, focus on term frequency and presence for document ranking, they often overlook the semantic information of queries. Current strategies leverage pretrained LMs like BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib22" title="">2019</a>)</cite>, which capture the semantic essence of queries more effectively. These models improve search accuracy by considering synonyms and the structure of phrases, thereby refining document ranking through the detection of semantic similarities. This is typically achieved by measuring vector distances between documents and queries, combining traditional retrieval metrics with semantic understanding to yield search results that are both relevant and aligned with user intent.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Generation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">The generation phase is tasked with producing text that is both relevant to the query and reflective of the information found in the retrieved documents. The usual method involves concatenating the query with the retrieved information, which is then fed into an LLM for text generation <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib62" title="">2022</a>)</cite>. Although ensuring the generated text’s alignment and accuracy with the retrieved content presents challenges, it is also essential to strike a balance between adhering closely to the source material and infusing the output with creativity. The generated text should accurately convey the information from the retrieved documents and align with the query’s intent, while also offering the flexibility to introduce new insights or perspectives not explicitly contained within the retrieved data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>RAG Paradigm</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The RAG paradigm organizes research within the domain, offering a straightforward yet robust framework to enhance LLM performance. Central to RAG is its search mechanism, crucial for generating high-quality outcomes. Therefore, this paradigm is structured into four main phases from a retrieval perspective: pre-retrieval, retrieval, post-retrieval, and generation. Both single-hop and multi-hop retrieval approaches, encompassing iterative retrieve-generate cycles, follow this four-phase structure. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S2.F3" title="Figure 3 ‣ Customization ‣ 2.2.4 Generation ‣ 2.2 RAG Paradigm ‣ 2 RAG Framework ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> is the taxonomy tree of RAG’s core techniques.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Pre-Retrieval</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">The pre-retrieval phase of retrieval-augmented generation lays the foundation for successful data and query preparation, ensuring efficient information retrieval. This phase includes essential tasks to prepare for effective data access.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Indexing</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p1.1">The process starts with indexing, which establishes an organized system to enable fast and accurate retrieval of information. The specificity of indexing depends on the task and data type. For example, sentence-level indexing is beneficial for question-answering systems to precisely locate answers, while document-level indexing is more appropriate for summarizing documents to understand their main concepts and ideas.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Query Manipulation</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p1.1">After indexing, query manipulation is performed to adjust user queries for a better match with the indexed data. This involves query reformulation <cite class="ltx_cite ltx_citemacro_citep">(Jansen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib46" title="">2009</a>; Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib110" title="">2020</a>)</cite>, which rewrites the query to align more closely with the user’s intention; query expansion <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib37" title="">2013</a>)</cite>, which extends the query to capture more relevant results through synonyms or related terms; and query normalization, which resolves differences in spelling or terminology for consistent query matching.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Data Modification</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p1.1">Data modification is also critical in enhancing retrieval efficiency. This step includes preprocessing techniques like removing irrelevant or redundant information to improve the quality of results and enriching the data with additional information such as metadata to boost the relevance and diversity of the retrieved content <cite class="ltx_cite ltx_citemacro_citep">(Bevilacqua et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib3" title="">2022a</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Retrieval</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Search &amp; Ranking</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS2.Px1.p1.1">The retrieval stage is the combination of search and ranking. It focuses on selecting and prioritizing documents from a dataset to enhance the quality of the generation model’s outputs. This stage employs search algorithms to navigate through the indexed data, finding documents that match a user’s query. After identifying relevant documents, the process of initially ranking these documents starts to sort them according to their relevance to the query.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Post-Retrieval</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">The post-retrieval phase serves to refine the initially retrieved documents to improve the quality of text generation. This phase consists of re-ranking and filtering, each aimed at optimizing the document selection for the final generation task.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">Re-Ranking</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS3.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS3.Px1.p1.1">In the re-ranking step, the documents previously retrieved are reassessed, scored, and reorganized. The objective is to more accurately highlight the documents most relevant to the query and diminish the importance of the less relevant ones. This step involves incorporating additional metrics and external knowledge sources to enhance precision. In this context, pre-trained models with superior accuracy but lower efficiency can be effectively employed due to the limited set of candidate documents available <cite class="ltx_cite ltx_citemacro_citep">(Huang and Hu, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib39" title="">2009</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">Filtering</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS3.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS3.Px2.p1.1">Filtering aims to remove documents that fail to meet specified quality or relevance standards. This can be done through several approaches, such as establishing a minimum relevance score threshold to exclude documents below a certain relevance level. Furthermore, the use of feedback from users or prior relevance evaluations assists in adjusting the filtering process, guaranteeing that only the most relevant documents are retained for text generation <cite class="ltx_cite ltx_citemacro_citep">(Khattab and Zaharia, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib55" title="">2020</a>; Huang and Huang, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib41" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>Generation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS4.p1">
<p class="ltx_p" id="S2.SS2.SSS4.p1.1">The generation stage is a crucial component of the RAG process, responsible for leveraging retrieved information to enhance the quality of the generated response. This stage encompasses several sub-steps aimed at producing content that is readable, engaging, and informative.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS4.Px1">
<h5 class="ltx_title ltx_title_paragraph">Enhancing</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS4.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS4.Px1.p1.1">At the heart of the generation phase is the enhancement step, where the objective is to merge the retrieved information with the user’s query to create a coherent and relevant response. This includes the process of elaboration, adding extra details to the retrieved content to enrich it. Efforts are focused on improving the output’s quality by increasing its clarity, coherence, and stylistic appeal through methods such as rephrasing and restructuring. Information from various sources is combined to offer a comprehensive perspective, and verification is conducted to ensure the accuracy and relevance of the content.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS4.Px2">
<h5 class="ltx_title ltx_title_paragraph">Customization</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS4.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS4.Px2.p1.1">Customization is an optional step, involving the adjustment of content to align with the user’s specific preferences or the context of the request. This tailoring includes adapting the content to meet the needs of the target audience or the format in which it will be presented and condensing the information to succinctly convey the essence of the content. The process also entails creating summaries or abstracts that emphasize the key points or arguments, ensuring the output is both informative and concise.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.F3.1" style="width:433.6pt;height:385.6pt;vertical-align:-380.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-99.2pt,1.1pt) scale(0.685985672853256,0.685985672853256) ;"><span class="ltx_ERROR undefined" id="S2.F3.1.1">{forest}</span>
<p class="ltx_p" id="S2.F3.1.2">for tree=
grow=south,
growth parent anchor=south,
parent anchor=south,
child anchor=north,
draw,
minimum height = 1cm,
rounded corners=10pt,
drop shadow,
node options=align=center,,
text width=3cm,
l sep=10mm,
s sep=3mm,
edge=very thick, draw=black, tier/.wrap pgfmath arg=tier #1level(),
edge path=
[<span class="ltx_ERROR undefined" id="S2.F3.1.2.1">\forestoption</span>edge, very thick]
(!u.parent anchor) – +(0,-15pt) -|
(.child anchor)<span class="ltx_ERROR undefined" id="S2.F3.1.2.2">\forestoption</span>edge label;
,

[RAG, root, for tree=parent anchor=south
[Pre-Retrieval, xnode, for tree=parent anchor=south
[Indexing, tnode, for tree=parent anchor=south
[REALM <cite class="ltx_cite ltx_citemacro_citep">(Guu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib31" title="">2020</a>)</cite>; kNN-LMs <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>)</cite>; RAG <cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>)</cite>; Webgpt <cite class="ltx_cite ltx_citemacro_citep">(Nakano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib73" title="">2021</a>)</cite>; RETRO <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib6" title="">2022</a>)</cite>; MEMWALKER <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib8" title="">2023a</a>)</cite>; Atlas <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>
, pnode]]
[Query Manipulation, tnode, for tree=parent anchor=south
[Webgpt <cite class="ltx_cite ltx_citemacro_citep">(Nakano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib73" title="">2021</a>)</cite>; DSP <cite class="ltx_cite ltx_citemacro_citep">(Khattab et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib54" title="">2022</a>)</cite>; COK <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib63" title="">2023</a>)</cite>; IRCOT <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib98" title="">2023</a>)</cite>; Query2doc <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib102" title="">2023a</a>)</cite>; Step-Back <cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib116" title="">2023</a>)</cite>; PROMPTAGATOR <cite class="ltx_cite ltx_citemacro_cite">Dai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib21" title="">2023</a>)</cite>; KnowledGPT <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib103" title="">2023b</a>)</cite>; Rewrite-Retrieve-Read <cite class="ltx_cite ltx_citemacro_cite">Ma et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>; FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib48" title="">2023</a>)</cite>
, pnode]]
[Data Modification, tnode, for tree=parent anchor=south
[RA-DIT <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib66" title="">2023b</a>)</cite>; RECITE <cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib92" title="">2023</a>)</cite>; UPRISE <cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib15" title="">2023a</a>)</cite>; GENREAD <cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib111" title="">2023a</a>)</cite>; KnowledGPT <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib103" title="">2023b</a>)</cite>
, pnode] ]
]
[Retrieval, xnode, for tree=parent anchor=south
[Search &amp; Ranking, tnode, for tree=parent anchor=south
[REALM <cite class="ltx_cite ltx_citemacro_citep">(Guu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib31" title="">2020</a>)</cite>; kNN-LMs <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>)</cite>; RAG <cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>)</cite>; FiD <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib43" title="">2021</a>)</cite>; Webgpt <cite class="ltx_cite ltx_citemacro_citep">(Nakano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib73" title="">2021</a>)</cite>; RETRO <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib6" title="">2022</a>)</cite>; ITRG <cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib26" title="">2023</a>)</cite>; RA-DIT <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib66" title="">2023b</a>)</cite>; SURGE <cite class="ltx_cite ltx_citemacro_citep">(Kang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib51" title="">2023</a>)</cite>; PRCA <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib107" title="">2023a</a>)</cite>; AAR <cite class="ltx_cite ltx_citemacro_cite">Yu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib113" title="">2023b</a>)</cite>; ITER-RETGEN <cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib90" title="">2023</a>)</cite>; UPRISE <cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib15" title="">2023a</a>)</cite>; MEMWALKER <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib8" title="">2023a</a>)</cite>; Atlas <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>; FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib48" title="">2023</a>)</cite>
, pnode] ]]
[Post-Retrieval, xnode, for tree=parent anchor=south
[Re-Ranking, tnode, for tree=parent anchor=south
[Re2G <cite class="ltx_cite ltx_citemacro_citep">(Glass et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib29" title="">2022</a>)</cite>; DSP <cite class="ltx_cite ltx_citemacro_citep">(Khattab et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib54" title="">2022</a>)</cite>; COK <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib63" title="">2023</a>)</cite>; FiD-TF <cite class="ltx_cite ltx_citemacro_citep">(Berchansky et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib2" title="">2023</a>)</cite>; ITER-RETGEN <cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib90" title="">2023</a>)</cite>; PROMPTAGATOR <cite class="ltx_cite ltx_citemacro_cite">Dai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib21" title="">2023</a>)</cite>; Selfmem <cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib16" title="">2023b</a>)</cite>; DKS-RAC <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib38" title="">2023</a>)</cite>; In-Context RALM <cite class="ltx_cite ltx_citemacro_citep">(Ram et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib83" title="">2023</a>)</cite>; Fid-light <cite class="ltx_cite ltx_citemacro_citep">(Hofstätter et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib34" title="">2023</a>)</cite>
, pnode]]
[Filtering, tnode, for tree=parent anchor=south
[Webgpt <cite class="ltx_cite ltx_citemacro_citep">(Nakano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib73" title="">2021</a>)</cite>; Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib1" title="">2023</a>)</cite>; FiD-TF <cite class="ltx_cite ltx_citemacro_citep">(Berchansky et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib2" title="">2023</a>)</cite>; PROMPTAGATOR <cite class="ltx_cite ltx_citemacro_cite">Dai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib21" title="">2023</a>)</cite>; RECOMP <cite class="ltx_cite ltx_citemacro_citep">(Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib106" title="">2023</a>)</cite>; DKS-RAC <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib38" title="">2023</a>)</cite>
, pnode] ] ]
[Generation, xnode, for tree=parent anchor=south
[Enhancing, tnode, for tree=parent anchor=south
[FiD <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib43" title="">2021</a>)</cite>; Webgpt <cite class="ltx_cite ltx_citemacro_citep">(Nakano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib73" title="">2021</a>)</cite>; DSP <cite class="ltx_cite ltx_citemacro_citep">(Khattab et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib54" title="">2022</a>)</cite>; IRCOT <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib98" title="">2023</a>)</cite>; ITRG <cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib26" title="">2023</a>)</cite>; RA-DIT <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib66" title="">2023b</a>)</cite>; PRCA <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib107" title="">2023a</a>)</cite>; RECITE <cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib92" title="">2023</a>)</cite>; UPRISE <cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib15" title="">2023a</a>)</cite>; GENREAD <cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib111" title="">2023a</a>)</cite>; Selfmem <cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib16" title="">2023b</a>)</cite>; MEMWALKER <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib8" title="">2023a</a>)</cite>; Atlas <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>
, pnode]]
[Customization, tnode, for tree=parent anchor=south
[PKG <cite class="ltx_cite ltx_citemacro_citep">(Luo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib69" title="">2023</a>)</cite>; Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib1" title="">2023</a>)</cite>; SURGE <cite class="ltx_cite ltx_citemacro_citep">(Kang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib51" title="">2023</a>)</cite>; REPLUG <cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib91" title="">2023</a>)</cite>
, pnode] ]]
]</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Taxonomy tree of RAG’s core techniques</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Pre-Retrieval</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Indexing</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The integration of the k-nearest neighbor (kNN) algorithm with pre-trained neural LMs, as demonstrated in kNN-LMs <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>)</cite>, represents significant progress in language modeling. This method employs a datastore created from collections of texts, enabling the dynamic retrieval of contextually relevant examples to improve perplexity without necessitating additional training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Known for its efficiency, FAISS <cite class="ltx_cite ltx_citemacro_citep">(Johnson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib49" title="">2021</a>)</cite> has been adopted in many studies for indexing purposes <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>; Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>; Khattab et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib54" title="">2022</a>)</cite>. Some research integrates enhancements like the Hierarchical Navigable Small World (HNSW) approximation <cite class="ltx_cite ltx_citemacro_cite">Malkov and Yashunin (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib71" title="">2020</a>)</cite> to achieve faster retrieval <cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>)</cite>. In addition, alternative tools like utilizing the Bing API <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.microsoft.com/en-us/bing/apis/bing-web-search-api</span></span></span> for indexing based on actual user search histories as outlined in Webgpt <cite class="ltx_cite ltx_citemacro_citep">(Nakano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib73" title="">2021</a>)</cite>, illustrate the variety of indexing techniques under investigation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Furthermore, MEMWALKER <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib8" title="">2023a</a>)</cite> introduces an innovative method to overcome the limitations of context window size in LLMs by creating a memory tree from the input text. This tree is formed by initially segmenting the text into smaller pieces and then summarizing these segments into a hierarchical structure of summary nodes, facilitating efficient indexing and management of large volumes of information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Query Manipulation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Studies such as FiD <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib43" title="">2021</a>)</cite>, COK<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib63" title="">2023</a>)</cite>, and Query2doc <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib102" title="">2023a</a>)</cite> emphasize the significance of creating new queries or refining existing ones to achieve more pertinent retrieval results. These research efforts highlight the necessity of efficiently gathering evidence from multiple passages and tailoring queries to suit various knowledge sources, whether structured or unstructured. Techniques ranging from the creation of pseudo-documents to enhance queries have shown to bolster retrieval performance across diverse information retrieval datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Further exploration into query manipulation has been conducted by Step-Back <cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib116" title="">2023</a>)</cite> and PROMPTAGATOR <cite class="ltx_cite ltx_citemacro_citep">(Dai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib21" title="">2023</a>)</cite>, which focus on abstracting high-level concepts or utilizing LLMs for prompt-based query generation. These strategies strive to better align queries with the retrieval system’s functionality by rephrasing tasks into more generalized versions or crafting task-specific queries from limited examples. Such methodologies enhance the consistency between queries and indexed data, facilitating the retrieval of more pertinent and insightful information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Moreover, KnowledGPT <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib103" title="">2023b</a>)</cite> and Rewrite-Retrieve-Read <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite> introduce approaches for query manipulation through “program of thought” prompting and innovative query rewriting techniques. KnowledGPT innovates by generating code to interface with knowledge bases, converting user queries into structured search commands. In contrast, Rewrite-Retrieve-Read utilizes a trainable compact LM for query reformulation, adjusting them to more effectively reflect the user’s intent and context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Lastly, FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib48" title="">2023</a>)</cite> presents a strategy based on confidence for query formulation, which focuses on crafting queries that precisely reflect the information needs. This method incorporates the use of generated sentences or fragments thereof as a foundation for search queries. By opting to directly use sentences, obscuring tokens of low confidence, or formulating explicit questions, this approach aims to boost the efficiency of the retrieval process, ensuring that the retrieved information faithfully satisfies the requirements of the generation process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data Modification</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">RA-DIT <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib66" title="">2023b</a>)</cite> and RECITE <cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib92" title="">2023</a>)</cite> emphasize enhancements through internal data modifications. RA-DIT distinguishes between fine-tuning datasets for LLMs and retrievers, aiming to bolster the LLM’s contextual comprehension and the retriever’s ability to align with queries. RECITE, on the other hand, utilizes passage hints and synthetic question-passage pairs to increase the variety and relevance of its generated recitations and responses. This approach seeks to broaden the model’s knowledge base and improve its response accuracy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">UPRISE <cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib15" title="">2023a</a>)</cite> and GENREAD <cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib111" title="">2023a</a>)</cite> target the refinement of external data. UPRISE converts raw task data into a structured format and refines the selection of prompts to enhance retrieval outcomes. In contrast, the Clustering-Based Prompts method employed by GENREAD generates documents from questions and clusters them to eliminate irrelevant data, enriching the input with varied contextual insights. This technique aims to improve the performance of the generative model by providing it with a richer set of information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Furthermore, KnowledGPT <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib103" title="">2023b</a>)</cite> is dedicated to augmenting raw text data with structured, semantically rich information through entity linking. This enrichment process not only structures the data more cohesively and makes it more amenable to queries but also boosts the model’s retrieval efficiency. It leverages precise, linked knowledge to enhance the model’s understanding and its ability to generate relevant responses, thereby improving its overall performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Retrieval</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Search &amp; Ranking</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Atlas <cite class="ltx_cite ltx_citemacro_citep">(Izacard et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib44" title="">2023</a>)</cite> investigates few-shot learning approaches, including Attention Distillation and Perplexity Distillation, to steer the retriever toward retrieving more relevant documents. IRCOT <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib98" title="">2023</a>)</cite> integrates retrieval with reasoning to improve the effectiveness of retrieval. SURGE <cite class="ltx_cite ltx_citemacro_citep">(Kang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib51" title="">2023</a>)</cite> employs a subgraph retriever to extract relevant subgraphs from a knowledge graph, while AAR <cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib113" title="">2023b</a>)</cite> modifies search preferences to help LLMs in fetching pertinent documents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">PRCA <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib107" title="">2023a</a>)</cite> focuses on employing domain-specific abstractive summarization to extract relevant and context-rich information from documents, using a supervised learning strategy to prioritize content crucial for accurate query responses. Meanwhile, MEMWALKER <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib8" title="">2023a</a>)</cite> leverages an internal search and ranking mechanism in the constructed memory tree to identify pertinent information for long-context question answering. Additionally, the Confidence-based Active Retrieval approach of FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib48" title="">2023</a>)</cite> dynamically triggers information retrieval based on the confidence levels of generated sentences, utilizing the insight that low-confidence tokens signal a need for external knowledge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Post-Retrieval</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Re-Ranking</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Re2G <cite class="ltx_cite ltx_citemacro_citep">(Glass et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib29" title="">2022</a>)</cite> introduces a sequence-pair classification approach for re-ranking, utilizing a BERT transformer to simultaneously analyze the query and passage. This interaction model, employing cross-attention between sequences, offers a contrast to the representation model typically used in initial retrieval phases. PROMPTAGATOR <cite class="ltx_cite ltx_citemacro_citep">(Dai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib21" title="">2023</a>)</cite> also employs a cross-attention model for re-scoring. Its “Lift Yourself Up” strategy iteratively selects the best candidate from a pool for further generation rounds, progressively improving content quality via self-generated content.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Re-ranking is also a significant focus of In-Context RALM <cite class="ltx_cite ltx_citemacro_citep">(Ram et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib83" title="">2023</a>)</cite>. Two approaches to reranking are explored: zero-shot reranking using language models and predictive reranking through trained models. This step is aimed at refining the selection of documents based on their expected utility for improving language model performance. ITER-RETGEN <cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib90" title="">2023</a>)</cite>, in particular, leverages knowledge distillation from the re-ranker to the dense retriever, fine-tuning retrieval efforts based on relevance signals from LLM outputs. This optimization of the retrieval model aims to more accurately capture query nuances, thereby improving document selection.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">DKS-RAC <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib38" title="">2023</a>)</cite> presents the Dense Knowledge Similarity (DKS) for aligning the knowledge between answers and retrieved passages at the sequence level. This approach is categorized under re-ranking due to its direct impact on passage selection based on knowledge similarity, refining the match between queries and documents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">FiD-light <cite class="ltx_cite ltx_citemacro_citep">(Hofstätter et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib34" title="">2023</a>)</cite> introduces a listwise autoregressive re-ranking method that employs source pointers to optimize the ranking order. This method maintains a link between the generated text and source passages, enabling a more structured generation process. By incorporating textual citations within the model’s output as pointers to relevant information sources, this approach facilitates an organized retrieval and generation process, enhancing the overall coherence and relevance of the generated content.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Filtering</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">COK <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib63" title="">2023</a>)</cite> presents the Progressive Rationale Correction technique, aimed at iteratively refining rationales with retrieved knowledge. This method constitutes a continuous optimization process, significantly enhancing the relevance and quality of information used in content generation. Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib1" title="">2023</a>)</cite> introduces a self-reflection mechanism to efficiently filter out irrelevant content. By employing critique tokens, this approach evaluates the relevance, supportiveness, and utility of retrieved passages, ensuring the integration of only high-quality information into the content generation process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Additionally, FiD-TF <cite class="ltx_cite ltx_citemacro_citep">(Berchansky et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib2" title="">2023</a>)</cite> and RECOMP <cite class="ltx_cite ltx_citemacro_citep">(Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib106" title="">2023</a>)</cite> are dedicated to the removal of irrelevant or redundant tokens and information from retrieved documents. FiD-TF employs a dynamic mechanism to identify and eliminate unnecessary tokens, enhancing the efficiency of information processing. RECOMP, on the other hand, compresses documents into concise summaries, focusing on selecting only the most pertinent content for the generation process. These methods streamline the content generation workflow by ensuring that only relevant and supportive information is utilized, thereby improving the overall quality and relevance of the generated content.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S5.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.1" style="width:433.6pt;height:255.6pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-364.4pt,214.4pt) scale(0.373036932542922,0.373036932542922) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1.1">Research</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.2.1">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.3.1">Retrieval Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.4.1">Multi-hop</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.5" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.5.1">Training</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S5.T1.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.6.1">Pre-Retrieval</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.7.1">Retrieval</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T1.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.8.1">Post-Retrieval</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S5.T1.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.9.1">Generation</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.1.1">Internal</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.2.1">External</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.3.1">Indexing</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.4.1">Query Manipulation</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.5.1">Data Modification</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.6.1">Search &amp; Ranking</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.7"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.7.1">Re-Ranking</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.8"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.8.1">Filtering</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.9"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.9.1">Enhancing</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.2.10"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.2.2.10.1">Customization</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.1">REALM <cite class="ltx_cite ltx_citemacro_citep">(Guu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib31" title="">2020</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.2">2020</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.3.3.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.1">kNN-LMs <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.2">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.4.4.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.1">RAG <cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.2">2020</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.5.5.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.1">FiD <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib43" title="">2021</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.2">2021</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.6.6.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.1">Webgpt <cite class="ltx_cite ltx_citemacro_citep">(Nakano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib73" title="">2021</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.2">2021</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.7">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.12">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.7.7.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.1">Re2G <cite class="ltx_cite ltx_citemacro_citep">(Glass et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib29" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.2">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.8.8.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.1">RETRO <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib6" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.2">2022</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.9.9.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.1">DSP <cite class="ltx_cite ltx_citemacro_citep">(Khattab et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib54" title="">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.2">2022</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.10.10.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.1">COK <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib63" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.11.11.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.1">IRCOT <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib98" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.12.12.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.1">ITRG <cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib26" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.13.13.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.1">PKG <cite class="ltx_cite ltx_citemacro_citep">(Luo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib69" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.14.14.14">✓</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.1">RA-DIT <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib66" title="">2023b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.9">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.15.15.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.1">Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib1" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.12">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.16.16.14">✓</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.17.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.1">SURGE <cite class="ltx_cite ltx_citemacro_citep">(Kang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib51" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.17.17.14">✓</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.18.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.1">FiD-TF <cite class="ltx_cite ltx_citemacro_citep">(Berchansky et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib2" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.11">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.12">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.18.18.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.19.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.1">PRCA <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib107" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.19.19.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.20.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.1">REPLUG <cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib91" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.13"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.20.20.14">✓</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.21.21">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.1">AAR <cite class="ltx_cite ltx_citemacro_cite">Yu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib113" title="">2023b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.21.21.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.22.22">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.1">Query2doc <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib102" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.22.22.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.23.23">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.1">Step-Back <cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib116" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.23.23.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.24.24">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.1">ITER-RETGEN <cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib90" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.10">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.24.24.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.25.25">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.1">RECITE <cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib92" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.9">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.25.25.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.26.26">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.1">PROMPTAGATOR <cite class="ltx_cite ltx_citemacro_cite">Dai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib21" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.11">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.12">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.26.26.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.27.27">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.1">UPRISE <cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib15" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.9">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.27.27.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.28.28">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.1">GENREAD <cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib111" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.2">2023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.3">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.4"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.8"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.9">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.28.28.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.29.29">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.1">KnowledGPT <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib103" title="">2023b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.8">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.9">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.29.29.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.30.30">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.1">Selfmem <cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib16" title="">2023b</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.30.30.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.31.31">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.1">MEMWALKER <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib8" title="">2023a</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.10">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.12"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.13">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.31.31.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.32.32">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.1">RECOMP <cite class="ltx_cite ltx_citemacro_citep">(Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib106" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.11"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.12">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.32.32.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.33.33">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.1">Rewrite-Retrieve-Read <cite class="ltx_cite ltx_citemacro_cite">Ma et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.8">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.10"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.11"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.33.33.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.34.34">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.1">Atlas <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.6">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.7">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.9"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.10">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.34.34.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.35.35">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.1">DKS-RAC <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib38" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.5">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.6">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.11">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.12">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.35.35.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.36.36">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.1">In-Context RALM <cite class="ltx_cite ltx_citemacro_citep">(Ram et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib83" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.5"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.36.36.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.37.37">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.1">Fid-light <cite class="ltx_cite ltx_citemacro_citep">(Hofstätter et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib34" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.2">2023</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.5">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.6"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.7"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.8"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.11">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.12"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.13"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T1.1.1.37.37.14"></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.38.38">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.1">FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib48" title="">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.2">2023</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.3"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.4">✓</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.5"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.6"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.7"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.8">✓</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.9"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.10">✓</td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.11"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.12"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.13"></td>
<td class="ltx_td ltx_border_b ltx_border_r ltx_border_t" id="S5.T1.1.1.38.38.14"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The comprehensive summary of RAG studies. A ✓in the “Multi-hop” column signifies that the research involves multiple search rounds. Similarly, a ✓in the “Training” column indicates that the study included training phases. It is important to note that in this context, “Training” encompasses both initial model training and fine-tuning processes.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Generation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Enhancing</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">DSP <cite class="ltx_cite ltx_citemacro_citep">(Khattab et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib54" title="">2022</a>)</cite> introduces a framework designed to generate multiple retrieval queries to summarize and answer questions, drawing upon information aggregated from various passages. This framework employs CombSUM <cite class="ltx_cite ltx_citemacro_citep">(Fox and Shaw, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib27" title="">1994</a>)</cite> to calculate a cumulative probability score for passages across different retrieval lists, facilitating the compilation of a comprehensive response from multiple sources.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">PRCA <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib107" title="">2023a</a>)</cite> outlines a Reward-Driven Stage, wherein the distilled context is refined based on feedback from the generator. Utilizing reinforcement learning, this stage adjusts the parameters of PRCA according to the rewards received for providing relevant context. The objective is to fine-tune the extracted context to meet the specific requirements of the generator, thereby optimizing the generation process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">REPLUG <cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib91" title="">2023</a>)</cite> proposes a method for prepending retrieved documents to the input context before the final prediction by the black-box LM. It introduces an ensemble strategy to encode retrieved documents in parallel, overcoming the limitations of LM context length and enhancing accuracy through the allocation of increased computational resources. This approach improves the generation process by ensuring that the LM has access to a broader range of relevant information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">RECITE <cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib92" title="">2023</a>)</cite> implements a self-consistency technique, which involves generating multiple recitations independently and employing a plurality/majority vote system to determine the most appropriate answer. This method is designed to increase the reliability and accuracy of the answers, thereby improving the quality and credibility of the output.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Customization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">The PKG framework, introduced by <cite class="ltx_cite ltx_citemacro_citep">(Luo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib69" title="">2023</a>)</cite>, represents an approach to customizing the output of LMs. By generating background knowledge internally using a pre-trained model, PKG eliminates the need for traditional external retrieval processes. This method directly integrates domain- or task-specific knowledge into the generation step, significantly enhancing the LM’s capacity to produce responses that are specifically tailored to the given context or requirements.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib1" title="">2023</a>)</cite> offers a strategy that incorporates reflection tokens within a customizable decoding algorithm. This technique permits dynamic adjustment of the model’s retrieval and generation behaviors based on the specific task, facilitating more versatile response generation. Depending on the requirements, this approach can be tuned for accuracy or creativity, providing flexibility in generating outputs that meet diverse needs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">SURGE <cite class="ltx_cite ltx_citemacro_citep">(Kang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib51" title="">2023</a>)</cite> achieves customization through the application of graph-text contrastive learning. This method ensures that the generated dialogue responses are in tight alignment with the knowledge contained in the retrieved subgraph, yielding responses that are specific, relevant, and deeply rooted in the dialogue context. By maintaining consistency between the retrieved knowledge and the generated text, SURGE is capable of producing outputs that precisely reflect the detailed knowledge of the subgraph, enhancing the relevance and specificity of the responses.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T2.1" style="width:433.6pt;height:4525.6pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-209.5pt,2186.1pt) scale(0.50859446856813,0.50859446856813) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T2.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.1.1">Research</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.2.1">Year</span></th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T2.1.1.1.1.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.1.1.3.1">
<span class="ltx_p" id="S6.T2.1.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.3.1.1.1">Retriever</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T2.1.1.1.1.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.1.1.4.1">
<span class="ltx_p" id="S6.T2.1.1.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.1.1.1.1.4.1.1.1">Generator</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.2.1.1">REALM <cite class="ltx_cite ltx_citemacro_citep">(Guu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib31" title="">2020</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.2.1.2">2020</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.2.1.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.2.1.3.1">
<span class="ltx_p" id="S6.T2.1.1.2.1.3.1.1">BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib22" title="">2019</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.2.1.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.2.1.4.1">
<span class="ltx_p" id="S6.T2.1.1.2.1.4.1.1">Transformers <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib99" title="">2017</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.3.2.1">kNN-LMs <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib53" title="">2020</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.3.2.2">2020</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.3.2.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.3.2.3.1">
<span class="ltx_p" id="S6.T2.1.1.3.2.3.1.1">FAISS <cite class="ltx_cite ltx_citemacro_citep">(Johnson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib49" title="">2021</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.3.2.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.3.2.4.1">
<span class="ltx_p" id="S6.T2.1.1.3.2.4.1.1">Transformers</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.4.3.1">RAG <cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib60" title="">2020b</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.4.3.2">2020</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.4.3.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.4.3.3.1">
<span class="ltx_p" id="S6.T2.1.1.4.3.3.1.1">DPR <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib52" title="">2020</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.4.3.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.4.3.4.1">
<span class="ltx_p" id="S6.T2.1.1.4.3.4.1.1">BART-Large <cite class="ltx_cite ltx_citemacro_citep">(Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib59" title="">2020a</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.5.4.1">FiD <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib43" title="">2021</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.5.4.2">2021</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.5.4.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.5.4.3.1">
<span class="ltx_p" id="S6.T2.1.1.5.4.3.1.1">BM25 <cite class="ltx_cite ltx_citemacro_citep">(Robertson and Zaragoza, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib86" title="">2009</a>)</cite>, DPR</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.5.4.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.5.4.4.1">
<span class="ltx_p" id="S6.T2.1.1.5.4.4.1.1">T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib82" title="">2020</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.6.5.1">Webgpt <cite class="ltx_cite ltx_citemacro_citep">(Nakano et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib73" title="">2021</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.6.5.2">2021</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.6.5.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.6.5.3.1">
<span class="ltx_p" id="S6.T2.1.1.6.5.3.1.1">Bing</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.6.5.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.6.5.4.1">
<span class="ltx_p" id="S6.T2.1.1.6.5.4.1.1">GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib7" title="">2020</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.7.6.1">Re2G <cite class="ltx_cite ltx_citemacro_citep">(Glass et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib29" title="">2022</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.7.6.2">2022</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.7.6.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.7.6.3.1">
<span class="ltx_p" id="S6.T2.1.1.7.6.3.1.1">BM25, DPR</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.7.6.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.7.6.4.1">
<span class="ltx_p" id="S6.T2.1.1.7.6.4.1.1">BART</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.8.7.1">RETRO <cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib6" title="">2022</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.8.7.2">2022</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.8.7.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.8.7.3.1">
<span class="ltx_p" id="S6.T2.1.1.8.7.3.1.1">BERT</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.8.7.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.8.7.4.1">
<span class="ltx_p" id="S6.T2.1.1.8.7.4.1.1">Transformer</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.9.8.1">DSP <cite class="ltx_cite ltx_citemacro_citep">(Khattab et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib54" title="">2022</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.9.8.2">2022</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.9.8.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.9.8.3.1">
<span class="ltx_p" id="S6.T2.1.1.9.8.3.1.1">ColBERTv2 <cite class="ltx_cite ltx_citemacro_citep">(Khattab and Zaharia, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib55" title="">2020</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.9.8.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.9.8.4.1">
<span class="ltx_p" id="S6.T2.1.1.9.8.4.1.1">GPT-3.5 (text-davinci-002)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.10.9.1">COK <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib63" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.10.9.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.10.9.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.10.9.3.1">
<span class="ltx_p" id="S6.T2.1.1.10.9.3.1.1">LLaMA2-7B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib97" title="">2023b</a>)</cite>, ChatGPT (gpt-3.5-turbo-0613)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.10.9.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.10.9.4.1">
<span class="ltx_p" id="S6.T2.1.1.10.9.4.1.1">ChatGPT (gpt-3.5-turbo-0613)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.11.10.1">IRCOT <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib98" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.11.10.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.11.10.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.11.10.3.1">
<span class="ltx_p" id="S6.T2.1.1.11.10.3.1.1">BM25</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.11.10.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.11.10.4.1">
<span class="ltx_p" id="S6.T2.1.1.11.10.4.1.1">GPT-3 (code-davinci-002), Flan-T5&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chung et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib18" title="">2022</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.12.11.1">ITRG <cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib26" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.12.11.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.12.11.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.12.11.3.1">
<span class="ltx_p" id="S6.T2.1.1.12.11.3.1.1">Atlas <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.12.11.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.12.11.4.1">
<span class="ltx_p" id="S6.T2.1.1.12.11.4.1.1">LLaMA 33B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib96" title="">2023a</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.13.12.1">PKG <cite class="ltx_cite ltx_citemacro_citep">(Luo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib69" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.13.12.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.13.12.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.13.12.3.1">
<span class="ltx_p" id="S6.T2.1.1.13.12.3.1.1">LLaMa-7B</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.13.12.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.13.12.4.1">
<span class="ltx_p" id="S6.T2.1.1.13.12.4.1.1">InstructGPT-3.5 (text-davinic-002) <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib77" title="">2022</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.14.13.1">RA-DIT <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib66" title="">2023b</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.14.13.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.14.13.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.14.13.3.1">
<span class="ltx_p" id="S6.T2.1.1.14.13.3.1.1">DRAGON+ <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib65" title="">2023a</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.14.13.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.14.13.4.1">
<span class="ltx_p" id="S6.T2.1.1.14.13.4.1.1">LLama</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.15.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.15.14.1">Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib1" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.15.14.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.15.14.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.15.14.3.1">
<span class="ltx_p" id="S6.T2.1.1.15.14.3.1.1">Contriever <cite class="ltx_cite ltx_citemacro_citep">(Izacard et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib42" title="">2022</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.15.14.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.15.14.4.1">
<span class="ltx_p" id="S6.T2.1.1.15.14.4.1.1">Llama2 (7B and 13B) , GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib76" title="">2023</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.16.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.16.15.1">SURGE <cite class="ltx_cite ltx_citemacro_citep">(Kang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib51" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.16.15.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.16.15.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.16.15.3.1">
<span class="ltx_p" id="S6.T2.1.1.16.15.3.1.1">Graph Neural Networks (GNN) <cite class="ltx_cite ltx_citemacro_citep">(Hamilton, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib32" title="">2020</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.16.15.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.16.15.4.1">
<span class="ltx_p" id="S6.T2.1.1.16.15.4.1.1">Transformers</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.17.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.17.16.1">FiD-TF <cite class="ltx_cite ltx_citemacro_citep">(Berchansky et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib2" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.17.16.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.17.16.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.17.16.3.1">
<span class="ltx_p" id="S6.T2.1.1.17.16.3.1.1">BM25, Sentence Transformers</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.17.16.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.17.16.4.1">
<span class="ltx_p" id="S6.T2.1.1.17.16.4.1.1">T5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.18.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.18.17.1">PRCA <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib107" title="">2023a</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.18.17.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.18.17.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.18.17.3.1">
<span class="ltx_p" id="S6.T2.1.1.18.17.3.1.1">BM25, DPR, Contriver, SimCSE <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib28" title="">2021</a>)</cite>, SBERT <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib85" title="">2019</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.18.17.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.18.17.4.1">
<span class="ltx_p" id="S6.T2.1.1.18.17.4.1.1">T5-large, Phoenix-7B <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib14" title="">2023d</a>)</cite>, Vicuna-7B <cite class="ltx_cite ltx_citemacro_citep">(Peng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib79" title="">2023</a>)</cite>, ChatGLM <cite class="ltx_cite ltx_citemacro_citep">(Du et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib24" title="">2022</a>)</cite>, GPT-3.5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.19.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.19.18.1">REPLUG <cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib91" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.19.18.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.19.18.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.19.18.3.1">
<span class="ltx_p" id="S6.T2.1.1.19.18.3.1.1">Contriever</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.19.18.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.19.18.4.1">
<span class="ltx_p" id="S6.T2.1.1.19.18.4.1.1">GPT-3</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.20.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.20.19.1">AAR <cite class="ltx_cite ltx_citemacro_cite">Yu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib113" title="">2023b</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.20.19.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.20.19.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.20.19.3.1">
<span class="ltx_p" id="S6.T2.1.1.20.19.3.1.1">ANCE <cite class="ltx_cite ltx_citemacro_citep">(Xiong et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib105" title="">2021</a>)</cite>,&nbsp;Contriever</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.20.19.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.20.19.4.1">
<span class="ltx_p" id="S6.T2.1.1.20.19.4.1.1">Flan-T5, InstructGPT</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.21.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.21.20.1">Query2doc <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib102" title="">2023a</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.21.20.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.21.20.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.21.20.3.1">
<span class="ltx_p" id="S6.T2.1.1.21.20.3.1.1">BM25, DPR</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.21.20.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.21.20.4.1">
<span class="ltx_p" id="S6.T2.1.1.21.20.4.1.1">GPT-3 (text-davinci-003)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.22.21">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.22.21.1">Step-Back <cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib116" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.22.21.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.22.21.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.22.21.3.1">
<span class="ltx_p" id="S6.T2.1.1.22.21.3.1.1">PaLM-2L <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib17" title="">2023</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.22.21.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.22.21.4.1">
<span class="ltx_p" id="S6.T2.1.1.22.21.4.1.1">PaLM-2L, GPT-4</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.23.22">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.23.22.1">ITER-RETGEN <cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib90" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.23.22.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.23.22.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.23.22.3.1">
<span class="ltx_p" id="S6.T2.1.1.23.22.3.1.1">Contriever</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.23.22.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.23.22.4.1">
<span class="ltx_p" id="S6.T2.1.1.23.22.4.1.1">InstructGPT (text-davinci-003), Llama-2</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.24.23">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.24.23.1">RECITE <cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib92" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.24.23.2">2023</th>
<td class="ltx_td ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.24.23.3" style="width:142.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.24.23.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.24.23.4.1">
<span class="ltx_p" id="S6.T2.1.1.24.23.4.1.1">PaLM, UL2 <cite class="ltx_cite ltx_citemacro_citep">(Tay et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib93" title="">2023</a>)</cite>, OPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib115" title="">2022</a>)</cite>, Codex <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib11" title="">2021</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.25.24">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.25.24.1">PROMPTAGATOR <cite class="ltx_cite ltx_citemacro_citep">(Dai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib21" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.25.24.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.25.24.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.25.24.3.1">
<span class="ltx_p" id="S6.T2.1.1.25.24.3.1.1">T5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.25.24.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.25.24.4.1">
<span class="ltx_p" id="S6.T2.1.1.25.24.4.1.1">FLAN</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.26.25">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.26.25.1">UPRISE <cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib15" title="">2023a</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.26.25.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.26.25.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.26.25.3.1">
<span class="ltx_p" id="S6.T2.1.1.26.25.3.1.1">GPT-Neo-2.7B <cite class="ltx_cite ltx_citemacro_citep">(Black et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib5" title="">2021</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.26.25.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.26.25.4.1">
<span class="ltx_p" id="S6.T2.1.1.26.25.4.1.1">BLOOM-7.1B <cite class="ltx_cite ltx_citemacro_citep">(Workshop et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib104" title="">2022</a>)</cite>, OPT-66B, GPT-3-175B</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.27.26">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.27.26.1">GENREAD <cite class="ltx_cite ltx_citemacro_citep">(Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib111" title="">2023a</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.27.26.2">2023</th>
<td class="ltx_td ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.27.26.3" style="width:142.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.27.26.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.27.26.4.1">
<span class="ltx_p" id="S6.T2.1.1.27.26.4.1.1">InstructGPT</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.28.27">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.28.27.1">KnowledGPT <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib103" title="">2023b</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.28.27.2">2023</th>
<td class="ltx_td ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.28.27.3" style="width:142.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.28.27.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.28.27.4.1">
<span class="ltx_p" id="S6.T2.1.1.28.27.4.1.1">GPT-4</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.29.28">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.29.28.1">Selfmem <cite class="ltx_cite ltx_citemacro_citep">(Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib16" title="">2023b</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.29.28.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.29.28.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.29.28.3.1">
<span class="ltx_p" id="S6.T2.1.1.29.28.3.1.1">BM25</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.29.28.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.29.28.4.1">
<span class="ltx_p" id="S6.T2.1.1.29.28.4.1.1">XGLM <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib67" title="">2022</a>)</cite>, XLM-Rbase <cite class="ltx_cite ltx_citemacro_citep">(Conneau et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib19" title="">2020</a>)</cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.30.29">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.30.29.1">MEMWALKER <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib8" title="">2023a</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.30.29.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.30.29.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.30.29.3.1">
<span class="ltx_p" id="S6.T2.1.1.30.29.3.1.1">LLaMA-2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.30.29.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.30.29.4.1">
<span class="ltx_p" id="S6.T2.1.1.30.29.4.1.1">LLaMA-2</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.31.30">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.31.30.1">RECOMP <cite class="ltx_cite ltx_citemacro_citep">(Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib106" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.31.30.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.31.30.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.31.30.3.1">
<span class="ltx_p" id="S6.T2.1.1.31.30.3.1.1">BM25</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.31.30.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.31.30.4.1">
<span class="ltx_p" id="S6.T2.1.1.31.30.4.1.1">T5-Large</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.32.31">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.32.31.1">Rewrite-Retrieve-Read <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.32.31.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.32.31.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.32.31.3.1">
<span class="ltx_p" id="S6.T2.1.1.32.31.3.1.1">Bing</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.32.31.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.32.31.4.1">
<span class="ltx_p" id="S6.T2.1.1.32.31.4.1.1">T5-Large, ChatGPT(gpt-3.5-turbo), Vicuna-13B</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.33.32">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.33.32.1">Atlas <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib70" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.33.32.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.33.32.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.33.32.3.1">
<span class="ltx_p" id="S6.T2.1.1.33.32.3.1.1">Contriever</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.33.32.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.33.32.4.1">
<span class="ltx_p" id="S6.T2.1.1.33.32.4.1.1">T5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.34.33">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.34.33.1">DKS-RAC <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib38" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.34.33.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.34.33.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.34.33.3.1">
<span class="ltx_p" id="S6.T2.1.1.34.33.3.1.1">DPR</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.34.33.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.34.33.4.1">
<span class="ltx_p" id="S6.T2.1.1.34.33.4.1.1">BART</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.35.34">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.35.34.1">In-Context RALM <cite class="ltx_cite ltx_citemacro_citep">(Ram et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib83" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.35.34.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.35.34.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.35.34.3.1">
<span class="ltx_p" id="S6.T2.1.1.35.34.3.1.1">BM25, BERT-base, Contriever, Spider <cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib84" title="">2022</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.35.34.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.35.34.4.1">
<span class="ltx_p" id="S6.T2.1.1.35.34.4.1.1">GPT-2, GPT-Neo, GPT-J <cite class="ltx_cite ltx_citemacro_citep">(Wang and Komatsuzaki, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib101" title="">2021</a>)</cite>, OPT, and LLaMA</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.36.35">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.36.35.1">Fid-light <cite class="ltx_cite ltx_citemacro_citep">(Hofstätter et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib34" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T2.1.1.36.35.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.36.35.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.36.35.3.1">
<span class="ltx_p" id="S6.T2.1.1.36.35.3.1.1">GTR-Base <cite class="ltx_cite ltx_citemacro_citep">(Ni et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib74" title="">2022</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S6.T2.1.1.36.35.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.36.35.4.1">
<span class="ltx_p" id="S6.T2.1.1.36.35.4.1.1">T5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T2.1.1.37.36">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.1.1.37.36.1">FLARE <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib48" title="">2023</a>)</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S6.T2.1.1.37.36.2">2023</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" id="S6.T2.1.1.37.36.3" style="width:142.3pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.37.36.3.1">
<span class="ltx_p" id="S6.T2.1.1.37.36.3.1.1">BM25, Bing</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" id="S6.T2.1.1.37.36.4" style="width:170.7pt;">
<span class="ltx_inline-block ltx_align_top" id="S6.T2.1.1.37.36.4.1">
<span class="ltx_p" id="S6.T2.1.1.37.36.4.1.1">GPT-3.5 (text-davinci-003)</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The summary of Retrievers and Generators. The retrieval models and pre-trained language models explicitly mentioned in these studies have been recorded.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Comparisons of RAG</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>The Comprehensive Summary of RAG</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S5.T1" title="Table 1 ‣ 5.2 Filtering ‣ 5 Post-Retrieval ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> presents a detailed analysis of the RAG studies discussed in this paper. The analysis shows that the majority of these studies have utilized external data sources to enrich the content of LLMs. A preference for multiple-hop over single-hop retrieval was noted, indicating that iterative search rounds generally yield superior results. In other words, most methods employ dense retrieval to secure higher quality candidate documents. Compared to modifying datasets in the pre-retrieval stage, more studies focus on manipulating the query to improve retrieval performance. Additionally, there is a significant emphasis on optimizing the retrieval phase, highlighting its crucial role in the research. However, there seems to be a scarcity of studies concentrating on customization in the generation stage, pointing to this as a potential area for future exploration. Overall, while the goal of RAG is to enhance the response quality of LLMs, greater efforts have been directed towards improving retrieval aspects.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Retriever and Generator</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">In RAG, the retriever and the generator are the primary components. Table <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S6.T2" title="Table 2 ‣ 6.2 Customization ‣ 6 Generation ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the retrievers and generators used in the studies discussed in this paper. It is clear from the table that while most generators utilize advanced language models, a significant number of retrievers still employ the traditional BM25 due to its efficiency. The method of retrieval is a crucial aspect in RAG, highlighting the importance of exploring ways to enhance retrieval performance without compromising efficiency. Similarly, not many studies have adopted powerful LLMs such as LLaMA2, GPT-3.5, or GPT-4 as their generators. LLMs like T5 remain popular, yet fundamental models like BERT and Transformers are rarely used in 2023. Compared to generators, it is evident that not many IR-based LLMs are used in retrievers, indicating a promising direction for developing such models in the future.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S7.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S7.T3.1" style="width:433.6pt;height:131.3pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-287.1pt,86.7pt) scale(0.430278599735059,0.430278599735059) ;">
<table class="ltx_tabular ltx_align_middle" id="S7.T3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T3.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S7.T3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S7.T3.1.1.1.1.1.1">Evaluation Framework</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S7.T3.1.1.1.1.2.1">Aspects</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S7.T3.1.1.1.1.3.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S7.T3.1.1.1.1.4.1">Metrics</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S7.T3.1.1.1.1.5.1">Datasets</span></td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S7.T3.1.1.2.2.1" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.2.2.1.1">RAGAS <cite class="ltx_cite ltx_citemacro_citep">(Shahul et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib89" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.2.2.2" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.2.2.2.1">Quality of RAG Systems</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.2.2.3">Context Relevance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.2.2.4">Extracted Sentences / Total Sentences</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.2.2.5" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.2.2.5.1">WikiEval <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://huggingface.co/datasets/explodinggradients/WikiEval</span></span></span></span></td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.3.3.1">Answer Relevance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.3.3.2">Average Cosine Similarity</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.4.4.1">Faithfulness</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.4.4.2">Supported Statements / Total Statements</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S7.T3.1.1.5.5.1" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.5.5.1.1">ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib87" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.5.5.2" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.5.5.2.1">Improving RAGAS</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.5.5.3">Context Relevance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.5.5.4" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.5.5.4.1">Confidence Intervals</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.5.5.5" rowspan="3"><span class="ltx_text" id="S7.T3.1.1.5.5.5.1">
<span class="ltx_tabular ltx_align_middle" id="S7.T3.1.1.5.5.5.1.1">
<span class="ltx_tr" id="S7.T3.1.1.5.5.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.5.5.5.1.1.1.1">KILT <cite class="ltx_cite ltx_citemacro_citep">(Petroni et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib80" title="">2021</a>)</cite></span></span>
<span class="ltx_tr" id="S7.T3.1.1.5.5.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.5.5.5.1.1.2.1">SuperGLUE <cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib100" title="">2019</a>)</cite></span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.6.6.1">Answer Relevance</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.7.7.1">Answer Faithfulness</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S7.T3.1.1.8.8.1" rowspan="2"><span class="ltx_text" id="S7.T3.1.1.8.8.1.1">RECALL <cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib68" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.8.8.2" rowspan="2"><span class="ltx_text" id="S7.T3.1.1.8.8.2.1">Counterfactual Robustness</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.8.8.3">Response Quality</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.8.8.4">
<table class="ltx_tabular ltx_align_middle" id="S7.T3.1.1.8.8.4.1">
<tbody><tr class="ltx_tr" id="S7.T3.1.1.8.8.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.8.8.4.1.1.1">Accuracy (QA)</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.8.8.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.8.8.4.1.2.1">BLEU, ROUGE-L (Generation)</td>
</tr>
</tbody></table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.8.8.5" rowspan="2"><span class="ltx_text" id="S7.T3.1.1.8.8.5.1">
<span class="ltx_tabular ltx_align_middle" id="S7.T3.1.1.8.8.5.1.1">
<span class="ltx_tr" id="S7.T3.1.1.8.8.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.8.8.5.1.1.1.1">EventKG <cite class="ltx_cite ltx_citemacro_citep">(Gottschalk and Demidova, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib30" title="">2018</a>)</cite></span></span>
<span class="ltx_tr" id="S7.T3.1.1.8.8.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.8.8.5.1.1.2.1">UJ <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib36" title="">2022</a>)</cite></span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.9.9.1">Robustness</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.9.9.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T3.1.1.9.9.2.1">
<tbody><tr class="ltx_tr" id="S7.T3.1.1.9.9.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.9.9.2.1.1.1">Misleading Rate (QA)</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.9.9.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.9.9.2.1.2.1">Mistake Reappearance Rate (Generation)</td>
</tr>
</tbody></table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S7.T3.1.1.10.10.1" rowspan="4"><span class="ltx_text" id="S7.T3.1.1.10.10.1.1">RGB <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib10" title="">2023b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T3.1.1.10.10.2" rowspan="4"><span class="ltx_text" id="S7.T3.1.1.10.10.2.1">Impact of RAG on LLMs</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.10.10.3">Noise Robustness</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.10.10.4">Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T3.1.1.10.10.5" rowspan="4"><span class="ltx_text" id="S7.T3.1.1.10.10.5.1">Synthetic Dataset including English and Chinese</span></td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.11.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.11.11.1">Negative Rejection</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.11.11.2">Rejection Rate</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.12.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.12.12.1">Information Integration</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.12.12.2">Accuracy</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.13.13">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T3.1.1.13.13.1">Counterfactual Robustness</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S7.T3.1.1.13.13.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T3.1.1.13.13.2.1">
<tbody><tr class="ltx_tr" id="S7.T3.1.1.13.13.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.13.13.2.1.1.1">Error Detection Rate</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.13.13.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S7.T3.1.1.13.13.2.1.2.1">Error Correction Rate</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The Comparison of Different RAG Evaluation Frameworks</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Evaluation in RAG</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">To understand the effectiveness of LMs in generating more accurate, relevant, and robust responses by leveraging external knowledge, the evaluation of RAG systems has become a significant research area. With the popularity of dialogue-based interactions, recent works have been focused on assessing the performance of RAG models on such downstream tasks using established metrics like Exact Match (EM) and F1 scores. Furthermore, a wide array of datasets has been utilized for this purpose, including TriviaQA <cite class="ltx_cite ltx_citemacro_citep">(Joshi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib50" title="">2017</a>)</cite>, HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib108" title="">2018</a>)</cite>, FEVER <cite class="ltx_cite ltx_citemacro_citep">(Thorne et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib95" title="">2018</a>)</cite>, Natural Questions <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib56" title="">2019</a>)</cite>, Wizard of Wikipedia <cite class="ltx_cite ltx_citemacro_citep">(Dinan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib23" title="">2019</a>)</cite>, and T-REX <cite class="ltx_cite ltx_citemacro_citep">(ElSahar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib25" title="">2018</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">However, evaluation solely from the perspective of downstream tasks falls short in addressing the evolving needs of RAG development. Recent research has introduced various frameworks and benchmarks that aim to evaluate these systems across multiple dimensions, including the quality of the generated text, the relevance of retrieved documents, and the model’s resilience to misinformation, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#S7.T3" title="Table 3 ‣ 7.2 Retriever and Generator ‣ 7 Comparisons of RAG ‣ A Survey on Retrieval-Augmented Text Generation for Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>. These evaluations focus on assessing specific capabilities such as noise robustness, negative prompting, information integration, and counterfactual robustness, highlighting the complex challenges faced by RAG systems in practical applications. The continuous development of evaluation frameworks and metrics is crucial for advancing the field, broadening the applicability of RAG systems, and ensuring they meet the demands of a complex and evolving information landscape.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Retrieval-based Aspect</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.SS1.p1">
<p class="ltx_p" id="S8.SS1.p1.1">In information retrieval, the quality of search results is typically evaluated using standard metrics such as Mean Average Precision (MAP), Precision, Reciprocal Rank, and Normalized Discounted Cumulative Gain (NDCG) <cite class="ltx_cite ltx_citemacro_citep">(Radlinski and Craswell, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib81" title="">2010</a>; Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib85" title="">2019</a>; Nogueira et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib75" title="">2019</a>)</cite>. These metrics primarily assess the relevance of retrieved documents to a given query.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S8.SS1.p2">
<p class="ltx_p" id="S8.SS1.p2.1">Retrieval-based Metrics in RAG focus on the effectiveness of retrieving relevant information to support generation tasks. These include Accuracy, which measures the precision of retrieved documents in providing correct information for answering queries, and Rejection Rate <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib10" title="">2023b</a>)</cite>, assessing a system’s ability to decline answering when no relevant information is found. Additionally, Error Detection Rate <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib10" title="">2023b</a>)</cite> evaluates the model’s capability to identify and disregard incorrect or misleading information from retrieved documents. Context Relevance is another essential metric, assessing the pertinence of the retrieved documents to the query. It’s vital to ensure the information used to generate responses is directly related to the query’s context. Faithfulness <cite class="ltx_cite ltx_citemacro_citep">(Shahul et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib89" title="">2023</a>)</cite> measures the accuracy with which the generated content reflects the information in the retrieved documents, ensuring that the generation process with no misinformation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span>Generation-based Aspect</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.SS2.p1">
<p class="ltx_p" id="S8.SS2.p1.1">Evaluating the quality of text produced by LLMs involves analyzing their performance on various downstream tasks using standard metrics. These metrics assess linguistic quality, coherence, accuracy, and the extent to which the generated text reflects ground-truth data. Linguistic quality and coherence are evaluated through metrics such as BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib78" title="">2002</a>)</cite>, which measures fluency and similarity to human-produced text, and ROUGE-L <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib64" title="">2004</a>)</cite>, which quantifies the overlap with reference summaries to gauge the text’s capacity to encapsulate main ideas and phrases. Accuracy and overlap with ground-truth data are gauged using metrics like EM and F1 Score, which respectively determine the percentage of answers that are entirely correct and offer a balanced assessment of precision and recall in retrieving relevant answers while minimizing inaccuracies.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S8.SS2.p2">
<p class="ltx_p" id="S8.SS2.p2.1">Beyond these standard metrics, the evaluation may also incorporate task-specific criteria and novel metrics tailored to particular applications. For instance, in dialogue generation, perplexity and entropy are used to evaluate response diversity and naturalness. Additionally, metrics such as Misleading Rate and Mistake Reappearance Rate <cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib68" title="">2023</a>)</cite> gauge a model’s ability to avoid misinformation and inaccuracies. Other specialized metrics include Answer Relevance <cite class="ltx_cite ltx_citemacro_citep">(Shahul et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib89" title="">2023</a>)</cite>, assessing the precision of responses to queries; Kendall’s tau <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib87" title="">2023</a>)</cite>, for evaluating the accuracy of RAG system rankings; Micro-F1 <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib87" title="">2023</a>)</cite>, which fine-tunes accuracy evaluation in tasks with multiple correct answers; and Prediction Accuracy, directly measuring the alignment of generated answers with expected responses, thereby offering a direct insight into a system’s effectiveness in generating accurate content.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Future Directions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S9.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span>Retrieval Quality</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS1.p1">
<p class="ltx_p" id="S9.SS1.p1.1">The integration of RAG into LLMs faces significant hurdles due to the vast amounts of unreliable information on the internet, including fake news. This presents a challenge for accurately retrieving useful knowledge, leading to the unreliable generation of responses by LLMs. As a result, LLMs may generate content based on incorrect information, undermining their reliability. Recent research efforts are directed towards enhancing retrieval methods to improve the efficiency, scalability, and effectiveness of LLMs in generating accurate and reliable responses.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S9.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Differentiable Search Indices</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S9.SS1.SSS0.Px1.p1.1"><cite class="ltx_cite ltx_citemacro_citep">(Tay et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib94" title="">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Bevilacqua et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib4" title="">2022b</a>)</cite> developed differentiable search indices that integrate the retrieval process within a Transformer model, enabling direct mapping of text queries to document identifiers. These approaches offer superior performance and potential for more efficient and scalable retrieval.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Generative Models for Search</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S9.SS1.SSS0.Px2.p1.1">GERE <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib9" title="">2022a</a>)</cite> can directly generate document titles and evidence sentences for fact-verification tasks. PARADE <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib61" title="">2024</a>)</cite> is a method for document reranking that aggregates passage representations into a unified document relevance score. Both of them demonstrate significant improvements in retrieval quality over traditional methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Fine-tuning Pre-trained Language Models</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S9.SS1.SSS0.Px3.p1.1">RankT5 <cite class="ltx_cite ltx_citemacro_citep">(Zhuang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib117" title="">2023</a>)</cite> is a model that fine-tunes the T5 framework specifically for text ranking. It leverages ranking losses to optimize performance metrics and exhibits promising zero-shot performance on out-of-domain data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S9.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Noise Power</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S9.SS1.SSS0.Px4.p1.1"><cite class="ltx_cite ltx_citemacro_cite">Cuconasu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib20" title="">2024</a>)</cite> provide a comprehensive analysis of the impact of IR components on RAG systems, revealing that the inclusion of irrelevant documents can significantly improve accuracy. It challenges conventional retrieval strategies and underscores the potential for developing specialized approaches that integrate retrieval with language generation models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S9.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span>Multimodal RAG</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S9.SS2.p1">
<p class="ltx_p" id="S9.SS2.p1.1">The multimodal RAG domain has experienced significant growth, highlighting a pivotal advancement at the confluence of text and visual comprehension. The introduction of MuRAG <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib12" title="">2022b</a>)</cite> marked a breakthrough by amalgamating textual and visual information for language generation, establishing a new standard for multimodal datasets. This model showcased the efficacy of utilizing a multimodal memory system to boost the accuracy in question-answering and reasoning tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S9.SS2.p2">
<p class="ltx_p" id="S9.SS2.p2.1">After MuRAG, studies such as REVEAL <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib35" title="">2023</a>)</cite> and Re-Imagen <cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib13" title="">2023c</a>)</cite> have focused on enhancing visual question answering and text-to-image generation. They achieved this through the incorporation of dynamic retrieval mechanisms and the improvement of image fidelity, respectively. These advancements laid the groundwork for further models by researchers like Sarto et al. <cite class="ltx_cite ltx_citemacro_citep">(Sarto et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib88" title="">2022</a>)</cite> for image captioning, and Yuan et al. <cite class="ltx_cite ltx_citemacro_citep">(Yuan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib114" title="">2023</a>)</cite> for text-to-audio generation, broadening the scope of RAG’s application across different modalities and improving the quality and realism of the generated outputs. Furthermore, Re-ViLM <cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2404.10981v1#bib.bib109" title="">2023b</a>)</cite> refined image captioning capabilities through a retrieval-augmented visual language model. By fine-tuning model parameters and implementing innovative filtering strategies, it has made strides in producing more precise and contextually appropriate captions. By tapping into external resources, these models have provided significant enhancements over traditional benchmarks, highlighting the advantage of integrating diverse sources of knowledge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Conclusions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">In this paper, we have presented a comprehensive framework for understanding the RAG domain, highlighting its significance in enhancing the capabilities of LLMs. Through a structured overview of RAG, categorizing various methods, and an in-depth analysis of its core technologies and evaluation methods, this study illuminates the path for future research. It identifies crucial areas for improvement and outlines potential directions for advancing RAG applications, especially in textual contexts. This survey aims to elucidate the core concepts of the RAG field from a retrieval perspective, and it is intended to facilitate further exploration and innovation in the accurate retrieval and generation of information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S11">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>Limitations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S11.p1">
<p class="ltx_p" id="S11.p1.1">This survey comprehensively examines existing RAG models, summarizing their core techniques into four main steps from a retrieval perspective. It recognizes that some methods may encompass multiple steps and that decoupling these steps could potentially obscure their intrinsic connections. Nevertheless, the primary objective is to simplify the complexity of the approach, clearly delineating the specific problems it addresses. This allows for a clearer identification of areas ripe for further optimization and improvement. Despite the thorough investigation, the rapid evolution of the field and page limits mean that certain aspects might not have been fully analyzed and explored, or recent developments could have been missed. While the paper references evaluation methods that can aid in the development of RAG, it also acknowledges mature tools like LangChain and LlamaIndex as useful resources. However, the focus of this survey is not on detailing the evaluation pipeline or how these tools are specifically used, but rather on illustrating how evaluation aspects can support the advancement of RAG. This choice highlights an area for future work, emphasizing the importance of methodological clarity and the application of evaluation tools in refining and enhancing RAG models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada and the York Research Chairs (YRC) program.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock">Self-RAG: Learning to Retrieve, Generate, and Critique
through Self-Reflection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv</em>, abs/2310.11511.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berchansky et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat.
2023.

</span>
<span class="ltx_bibblock">Optimizing Retrieval-augmented Reader Models via Token
Elimination.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em>, pages 1506–1524.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bevilacqua et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Michele Bevilacqua, Giuseppe Ottaviano, Patrick S.&nbsp;H. Lewis, Scott Yih,
Sebastian Riedel, and Fabio Petroni. 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2022/hash/cd88d62a2063fdaf7ce6f9068fb15dcd-Abstract-Conference.html" title="">Autoregressive search engines: Generating substrings as document
identifiers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Advances in Neural Information Processing Systems 35: Annual
Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New
Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bevilacqua et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Michele Bevilacqua, Giuseppe Ottaviano, Patrick S.&nbsp;H. Lewis, Scott Yih,
Sebastian Riedel, and Fabio Petroni. 2022b.

</span>
<span class="ltx_bibblock">Autoregressive Search Engines: Generating Substrings as
Document Identifiers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.5281/zenodo.5297715" title="">GPT-Neo: Large Scale
Autoregressive Language Modeling with Mesh-Tensorflow</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
Rutherford, Katie Millican, George van&nbsp;den Driessche, Jean-Baptiste Lespiau,
Bogdan Damoc, Aidan Clark, Diego de&nbsp;Las Casas, Aurelia Guy, Jacob Menick,
Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin
Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon
Osindero, Karen Simonyan, Jack&nbsp;W. Rae, Erich Elsen, and Laurent Sifre. 2022.

</span>
<span class="ltx_bibblock">Improving Language Models by Retrieving from Trillions of
Tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">International Conference on Machine Learning
(ICML)</em>, pages 2206–2240.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>, volume abs/2005.14165.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz.
2023a.

</span>
<span class="ltx_bibblock">Walking Down the Memory Maze: Beyond Context Limit through
Interactive Reading.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv</em>, abs/2310.05029.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng.
2022a.

</span>
<span class="ltx_bibblock">Gere: Generative Evidence Retrieval for Fact Verification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval</em>.
ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le&nbsp;Sun. 2023b.

</span>
<span class="ltx_bibblock">Benchmarking large language models in retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv</em>, abs/2309.01431.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de&nbsp;Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, and others. 2021.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv</em>, abs/2107.03374.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenhu Chen, Hexiang Hu, Xi&nbsp;Chen, Pat Verga, and William Cohen.
2022b.

</span>
<span class="ltx_bibblock">Murag: Multimodal Retrieval-Augmented Generator for Open
Question Answering over Images and Text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William&nbsp;W. Cohen.
2023c.

</span>
<span class="ltx_bibblock">Re-Imagen: Retrieval-Augmented Text-to-Image Generator.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023d)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen,
Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, and others.
2023d.

</span>
<span class="ltx_bibblock">Phoenix: Democratizing chatgpt across languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv</em>, abs/2304.10453.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing
Wang, Hao Sun, Furu Wei, Weiwei Deng, and Qi&nbsp;Zhang. 2023a.

</span>
<span class="ltx_bibblock">Uprise: Universal Prompt Retrieval for Improving Zero-Shot
Evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em>, pages 12318–12337.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xin Cheng, Di&nbsp;Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan.
2023b.

</span>
<span class="ltx_bibblock">Lift Yourself Up: Retrieval-augmented Text Generation with
Self-Memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Thirty-seventh Conference on Neural Information
Processing Systems</em>, volume abs/2305.02437.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian
Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
Abhishek Rao, Parker Barnes, Yi&nbsp;Tay, Noam Shazeer, Vinodkumar Prabhakaran,
Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
Dohan, Shivani Agrawal, Mark Omernick, Andrew&nbsp;M. Dai,
Thanumalayan&nbsp;Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
2023.

</span>
<span class="ltx_bibblock">Palm: Scaling Language Modeling with Pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Journal of Machine Learning Research (JMLR)</em>,
24:240:1–240:113.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Le&nbsp;Hou, S.&nbsp;Longpre, Barret Zoph, Yi&nbsp;Tay, W.&nbsp;Fedus, Eric Li,
Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, S.&nbsp;Gu,
Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter,
Sharan Narang, Gaurav Mishra, Adams&nbsp;Wei Yu, Vincent Zhao, Yanping Huang,
Andrew&nbsp;M. Dai, Hongkun Yu, Slav Petrov, E.&nbsp;Chi, J.&nbsp;Dean, Jacob Devlin, Adam
Roberts, Denny Zhou, Quoc&nbsp;V. Le, and Jason Wei. 2022.

</span>
<span class="ltx_bibblock">Scaling Instruction-Finetuned Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv</em>, abs/2210.11416.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock">Unsupervised Cross-lingual Representation Learning at Scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics</em>, pages 8440–8451.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuconasu et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare
Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024.

</span>
<span class="ltx_bibblock">The Power of Noise: Redefining Retrieval for RAG Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv</em>, abs/2401.14887.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhuyun Dai, Vincent&nbsp;Y. Zhao, Ji&nbsp;Ma, Yi&nbsp;Luan, Jianmo Ni, Jing Lu, Anton Bakalov,
Kelvin Guu, Keith&nbsp;B. Hall, and Ming-Wei Chang. 2023.

</span>
<span class="ltx_bibblock">Promptagator: Few-shot Dense Retrieval From 8 Examples.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Bert: Pre-training of Deep Bidirectional Transformers for
Language Understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2019 Conference of the North</em>, pages
4171–4186. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinan et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason
Weston. 2019.

</span>
<span class="ltx_bibblock">Wizard of Wikipedia: Knowledge-Powered Conversational Agents.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
Jie Tang. 2022.

</span>
<span class="ltx_bibblock">Glm: General Language Model Pretraining with Autoregressive
Blank Infilling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)</em>.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ElSahar et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier,
Jonathon&nbsp;S. Hare, Frédérique Laforest, and Elena Simperl. 2018.

</span>
<span class="ltx_bibblock">T-REx: A Large Scale Alignment of Natural Language with
Knowledge Base Triples.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">International Conference on Language Resources and
Evaluation (LREC)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2023.

</span>
<span class="ltx_bibblock">Retrieval-generation synergy augmented large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv</em>, abs/2310.05149.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fox and Shaw (1994)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edward&nbsp;A. Fox and Joseph&nbsp;A. Shaw. 1994.

</span>
<span class="ltx_bibblock">Combination of multiple searches.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">TREC-2: Text retrieval conference</em>, 500215, pages 105–108.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.

</span>
<span class="ltx_bibblock">Simcse: Simple Contrastive Learning of Sentence Embeddings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing</em>, pages 6894–6910.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glass et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Michael Glass, Gaetano Rossiello, Md&nbsp;Faisal&nbsp;Mahbub Chowdhury, Ankita Naik,
Pengshan Cai, and Alfio Gliozzo. 2022.

</span>
<span class="ltx_bibblock">Re2g: Retrieve, Rerank, Generate.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies</em>, pages 2701–2715. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gottschalk and Demidova (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Simon Gottschalk and Elena Demidova. 2018.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">EventKG: A Multilingual Event-Centric Temporal
Knowledge Graph</em>.

</span>
<span class="ltx_bibblock">Springer International Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.

</span>
<span class="ltx_bibblock">Retrieval Augmented Language Model Pre-Training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">International Conference on Machine Learning
(ICML)</em>, pages 3929–3938.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamilton (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
William&nbsp;L. Hamilton. 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Graph representation learning</em>.

</span>
<span class="ltx_bibblock">Springer International Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hancock-Beaulieu et&nbsp;al. (1996)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Micheline Hancock-Beaulieu, Mike Gatford, Xiangji Huang, Stephen&nbsp;E.
Robertson, Steve Walker, and P.&nbsp;W. Williams. 1996.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://trec.nist.gov/pubs/trec5/papers/city.procpaper.ps.gz" title="">Okapi at TREC-5</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of The Fifth Text REtrieval Conference, TREC
1996, Gaithersburg, Maryland, USA, November 20-22, 1996</em>, volume 500-238 of
<em class="ltx_emph ltx_font_italic" id="bib.bib33.2.2">NIST Special Publication</em>. National Institute of Standards and
Technology (NIST).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hofstätter et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock">Fid-light: Efficient and effective retrieval-augmented text
generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Information Retrieval</em>,
pages 1437–1447.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun,
Cordelia Schmid, David&nbsp;A. Ross, and Alireza Fathi. 2023.

</span>
<span class="ltx_bibblock">Reveal: Retrieval-Augmented Visual-Language Pre-Training
with Multi-Source Multimodal Knowledge Memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">2023 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, pages 23369–23379. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang, Jinjun Xiong, and Wen-mei Hwu.
2022.

</span>
<span class="ltx_bibblock">Understanding Jargon: Combining Extraction and Generation for
Definition Modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing</em>. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2013)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jimmy&nbsp;Xiangji Huang, Jun Miao, and Ben He. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/J.IPM.2012.08.002" title="">High performance
query expansion using adaptive co-training</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Inf. Process. Manag.</em>, 49(2):441–453.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenyu Huang, Mirella Lapata, Pavlos Vougiouklis, Nikos Papasarantopoulos, and
Jeff&nbsp;Z Pan. 2023.

</span>
<span class="ltx_bibblock">Retrieval Augmented Generation with Rich Answer Encoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proc. of IJCNLP-AACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Hu (2009)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiangji Huang and Qinmin Hu. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/1571941.1571995" title="">A bayesian learning
approach to promoting diversity in ranking for biomedical information
retrieval</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 32nd Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval, SIGIR
2009, Boston, MA, USA, July 19-23, 2009</em>, pages 307–314. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Huang (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yizheng Huang and Jimmy Huang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2402.11203" title="">Exploring chatgpt
for next-generation information retrieval: Opportunities and challenges</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">CoRR</em>, abs/2402.11203.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Huang (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yizheng Huang and Jimmy&nbsp;X. Huang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3233/FAIA230385" title="">Diversified prior
knowledge enhanced general language model for biomedical information
retrieval</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">ECAI 2023 - 26th European Conference on Artificial
Intelligence, September 30 - October 4, 2023, Kraków, Poland -
Including 12th Conference on Prestigious Applications of Intelligent Systems
(PAIS 2023)</em>, volume 372 of <em class="ltx_emph ltx_font_italic" id="bib.bib41.2.2">Frontiers in Artificial Intelligence and
Applications</em>, pages 1109–1115. IOS Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr
Bojanowski, Armand Joulin, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock">Unsupervised Dense Information Retrieval with Contrastive
Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Transactions on Machine Learning Research (TMLR)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2021.

</span>
<span class="ltx_bibblock">Leveraging Passage Retrieval with Generative Models for
Open Domain Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 16th Conference of the European
Chapter of the Association for Computational Linguistics: Main
Volume</em>, pages 874–880. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick S.&nbsp;H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio
Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2023.

</span>
<span class="ltx_bibblock">Atlas: Few-shot Learning with Retrieval Augmented Language
Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Journal of Machine Learning Research (JMLR)</em>, 24:251:1–251:43.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jahan et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Israt Jahan, Md. Tahmid&nbsp;Rahman Laskar, Chun Peng, and Jimmy&nbsp;Xiangji Huang.
2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2306.04504" title="">Evaluation of
chatgpt on biomedical tasks: A zero-shot comparison with fine-tuned
generative transformers</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CoRR</em>, abs/2306.04504.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jansen et&nbsp;al. (2009)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bernard&nbsp;J. Jansen, Danielle&nbsp;L. Booth, and Amanda Spink. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1002/ASI.21071" title="">Patterns of query
reformulation during web searching</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">J. Assoc. Inf. Sci. Technol.</em>, 60(7):1358–1371.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
Yejin Bang, Andrea Madotto, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3571730" title="">Survey of hallucination in
natural language generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">ACM Comput. Surv.</em>, 55(12):248:1–248:38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengbao Jiang, Frank&nbsp;F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu,
Yiming Yang, Jamie Callan, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock">Active Retrieval Augmented Generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Conference on Empirical Methods in Natural Language
Processing (EMNLP)</em>, pages 7969–7992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TBDATA.2019.2921572" title="">Billion-scale
similarity search with gpus</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">IEEE Transactions on Big Data</em>, 7(3):535–547.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock">Triviaqa: A Large Scale Distantly Supervised Challenge
Dataset for Reading Comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)</em>,
pages 1601–1611. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Minki Kang, Jin&nbsp;Myung Kwak, Jinheon Baek, and Sung&nbsp;Ju Hwang. 2023.

</span>
<span class="ltx_bibblock">Knowledge Graph-Augmented Language Models for
Knowledge-Grounded Dialogue Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv</em>, abs/2305.18846.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S.&nbsp;H. Lewis, Ledell Wu,
Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock">Dense Passage Retrieval for Open-Domain Question
Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Conference on Empirical Methods in Natural Language
Processing (EMNLP)</em>, pages 6769–6781.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
2020.

</span>
<span class="ltx_bibblock">Generalization through Memorization: Nearest Neighbor Language
Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Khattab, Keshav Santhanam, Xiang&nbsp;Lisa Li, David Leo&nbsp;Wright Hall, Percy
Liang, Christopher Potts, and M.&nbsp;Zaharia. 2022.

</span>
<span class="ltx_bibblock">Demonstrate-Search-Predict: Composing retrieval and language
models for knowledge-intensive NLP.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv</em>, abs/2212.14024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab and Zaharia (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Omar Khattab and Matei Zaharia. 2020.

</span>
<span class="ltx_bibblock">Colbert - Efficient and Effective Passage Search via
Contextualized Late Interaction over BERT.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 43rd International ACM SIGIR
Conference on Research and Development in Information Retrieval</em>,
pages 39–48. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang,
Andrew&nbsp;M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock">Natural Questions: A Benchmark for Question Answering
Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Transactions of the Association for Computational Linguistics</em>,
7:453–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laskar et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Md. Tahmid&nbsp;Rahman Laskar, M.&nbsp;Saiful Bari, Mizanur Rahman, Md&nbsp;Amran&nbsp;Hossen
Bhuiyan, Shafiq Joty, and Jimmy&nbsp;Xiangji Huang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2305.18486" title="">A systematic study
and comprehensive evaluation of chatgpt on benchmark datasets</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">CoRR</em>, abs/2305.18486.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laskar et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Md. Tahmid&nbsp;Rahman Laskar, Enamul Hoque, and Jimmy&nbsp;X. Huang. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-030-47358-7_35" title="">Query focused
abstractive summarization via incorporating query relevance and transfer
learning with transformer models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Advances in Artificial Intelligence - 33rd Canadian
Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON,
Canada, May 13-15, 2020, Proceedings</em>, volume 12109 of <em class="ltx_emph ltx_font_italic" id="bib.bib58.2.2">Lecture Notes in
Computer Science</em>, pages 342–348. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a.

</span>
<span class="ltx_bibblock">Bart: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics</em>, pages 7871–7880.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Patrick S.&nbsp;H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020b.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP
Tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. 2024.

</span>
<span class="ltx_bibblock">Parade: Passage Representation Aggregation forDocument
Reranking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">ACM Transactions on Information Systems</em>, 42(2):1–26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022.

</span>
<span class="ltx_bibblock">A Survey on Retrieval-Augmented Text Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv</em>, abs/2202.01110.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xingxuan Li, Ruochen Zhao, Yew&nbsp;Ken Chia, Bosheng Ding, Shafiq&nbsp;R. Joty, Soujanya
Poria, and Lidong Bing. 2023.

</span>
<span class="ltx_bibblock">Chain-of-Knowledge: Grounding Large Language Models via
Dynamic Knowledge Adapting over Heterogeneous Sources.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W04-1013" title="">ROUGE: A package for
automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Text Summarization Branches Out</em>, pages 74–81, Barcelona,
Spain. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad,
Wen-tau Yih, and Xilun Chen. 2023a.

</span>
<span class="ltx_bibblock">How to Train Your Dragon: Diverse Augmentation Towards
Generalizable Dense Retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Findings of the Association for Computational
Linguistics: EMNLP 2023</em>, pages 6385–6400. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James,
Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, and others.
2023b.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv</em>, abs/2310.01352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen,
Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth
Pasunuru, Sam Shleifer, Punit&nbsp;Singh Koura, Vishrav Chaudhary, Brian
O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona
Diab, Veselin Stoyanov, and Xian Li. 2022.

</span>
<span class="ltx_bibblock">Few-shot Learning with Multilingual Generative Language
Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing</em>. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie
Zhou, and Xu&nbsp;Sun. 2023.

</span>
<span class="ltx_bibblock">Recall: A Benchmark for LLMs Robustness against External
Counterfactual Knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">arXiv</em>, abs/2311.08147.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ziyang Luo, Can Xu, Pu&nbsp;Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin,
and Daxin Jiang. 2023.

</span>
<span class="ltx_bibblock">Augmented Large Language Models with Parametric Knowledge
Guiding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv</em>, abs/2305.04757.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock">Query Rewriting in Retrieval-Augmented Large Language
Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em>, pages 5303–5315.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malkov and Yashunin (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yu&nbsp;A. Malkov and D.&nbsp;A. Yashunin. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TPAMI.2018.2889473" title="">Efficient and
robust approximate nearest neighbor search using hierarchical navigable small
world graphs</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 42(4):824–836.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manning et&nbsp;al. (2008)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher&nbsp;D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Introduction to Information Retrieval</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and
others. 2021.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv</em>, abs/2112.09332.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo&nbsp;Hernandez Abrego, Ji&nbsp;Ma,
Vincent Zhao, Yi&nbsp;Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.

</span>
<span class="ltx_bibblock">Large Dual Encoders Are Generalizable Retrievers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing</em>, pages 9844–9855.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nogueira et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019.

</span>
<span class="ltx_bibblock">Multi-stage document ranking with BERT.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">CoRR</em>, abs/1910.14424.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
Florencia&nbsp;Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom,
Paul Baltescu, Haiming Bao, Mo&nbsp;Bavarian, Jeff Belgum, Irwan Bello, Jake
Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg
Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany
Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis
Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben
Chess, Chester Cho, Casey Chu, Hyung&nbsp;Won Chung, Dave Cummings, Jeremiah
Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien
Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien
Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
Simón&nbsp;Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges,
Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross,
Shixiang&nbsp;Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen
He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,
Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost
Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang,
Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan,
Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish&nbsp;Shirish Keskar,
Tabarak Khan, Logan Kilpatrick, Jong&nbsp;Wook Kim, Christina Kim, Yongjik Kim,
Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz
Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade
Leung, Daniel Levy, Chak&nbsp;Ming Li, Rachel Lim, Molly Lin, Stephanie Lin,
Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim
Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott&nbsp;Mayer McKinney, Christine McLeavey,
Paul McMillan, Jake McNeil, and others. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">PREPRINT</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll&nbsp;L. Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul&nbsp;F. Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et&nbsp;al. (2002)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for
automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the 40th Annual Meeting on Association for
Computational Linguistics</em>, ACL ’02, page 311–318, USA. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.

</span>
<span class="ltx_bibblock">Instruction tuning with gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">arXiv</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani,
Nicola&nbsp;De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean
Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel.
2021.

</span>
<span class="ltx_bibblock">Kilt: a Benchmark for Knowledge Intensive Language Tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies</em>, pages 2523–2544. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radlinski and Craswell (2010)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Filip Radlinski and Nick Craswell. 2010.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/1835449.1835560" title="">Comparing the
sensitivity of information retrieval metrics</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Proceedings of the 33rd International ACM SIGIR Conference
on Research and Development in Information Retrieval</em>, SIGIR ’10, page
667–674, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam&nbsp;M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified
Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Journal of Machine Learning Research (JMLR)</em>, 21:140:1–140:67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin
Leyton-Brown, and Yoav Shoham. 2023.

</span>
<span class="ltx_bibblock">In-Context Retrieval-Augmented Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">Transactions of the Association for Computational Linguistics</em>,
11:1316–1331.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. 2022.

</span>
<span class="ltx_bibblock">Learning to Retrieve Passages without Supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence Embeddings using Siamese
BERT-Networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP)</em>, pages 3980–3990. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Zaragoza (2009)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephen Robertson and Hugo Zaragoza. 2009.

</span>
<span class="ltx_bibblock">The Probabilistic Relevance Framework: Bm25 and Beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">Foundations and Trends® in Information
Retrieval</em>, 3(4):333–389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saad-Falcon et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jon Saad-Falcon, O.&nbsp;Khattab, Christopher Potts, and Matei Zaharia. 2023.

</span>
<span class="ltx_bibblock">Ares: An Automated Evaluation Framework for
Retrieval-Augmented Generation Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">arXiv</em>, abs/2311.09476.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarto et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 2022.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Transformer for Image Captioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">International Conference on Content-based Multimedia
Indexing</em>. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shahul et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
ES&nbsp;Shahul, Jithin James, Luis&nbsp;Espinosa Anke, and S.&nbsp;Schockaert. 2023.

</span>
<span class="ltx_bibblock">Ragas: Automated Evaluation of Retrieval Augmented
Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">arXiv</em>, abs/2309.15217.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen.
2023.

</span>
<span class="ltx_bibblock">Enhancing Retrieval-Augmented Large Language Models with
Iterative Retrieval-Generation Synergy.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Findings of the Association for Computational
Linguistics: EMNLP 2023</em>, pages 9248–9274. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis,
Luke Zettlemoyer, and Wen-tau Yih. 2023.

</span>
<span class="ltx_bibblock">Replug: Retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">arXiv</em>, abs/2301.12652.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiqing Sun, Xuezhi Wang, Yi&nbsp;Tay, Yiming Yang, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Recitation-Augmented Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Tay, Mostafa Dehghani, Vinh&nbsp;Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang,
Hyung&nbsp;Won Chung, Dara Bahri, Tal Schuster, Huaixiu&nbsp;Steven Zheng, Denny Zhou,
Neil Houlsby, and Donald Metzler. 2023.

</span>
<span class="ltx_bibblock">Ul2: Unifying Language Learning Paradigms.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen
Qin, Kai Hui, Zhe Zhao, Jai&nbsp;Prakash Gupta, Tal Schuster, William&nbsp;W. Cohen,
and Donald Metzler. 2022.

</span>
<span class="ltx_bibblock">Transformer Memory as a Differentiable Search Index.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.
2018.

</span>
<span class="ltx_bibblock">Fever: a Large-scale Dataset for Fact Extraction and
VERification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers)</em>, pages
809–819. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal
Azhar, and others. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">arXiv</em>, abs/2302.13971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
and others. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">arxiv</em>, abs/2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.
2023.

</span>
<span class="ltx_bibblock">Interleaving Retrieval with Chain-of-Thought Reasoning for
Knowledge-Intensive Multi-Step Questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)</em>,
pages 10014–10037. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam&nbsp;M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is All you Need.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">Neural Information Processing Systems</em>, pages
5998–6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
Felix Hill, Omer Levy, and Samuel&nbsp;R. Bowman. 2019.

</span>
<span class="ltx_bibblock">Superglue: A Stickier Benchmark for General-Purpose
Language Understanding Systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">Conference on Neural Information Processing Systems
(NeurIPS)</em>, pages 3261–3275.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Komatsuzaki (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ben Wang and Aran Komatsuzaki. 2021.

</span>
<span class="ltx_bibblock">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/kingoflolz/mesh-transformer-jax" title="">https://github.com/kingoflolz/mesh-transformer-jax</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, and Furu Wei. 2023a.

</span>
<span class="ltx_bibblock">Query2doc: Query Expansion with Large Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em>, pages 9414–9423.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xintao Wang, Qian Yang, Yongting Qiu, Jiaqing Liang, Qi&nbsp;He, Zhouhong Gu,
Yanghua Xiao, and W.&nbsp;Wang. 2023b.

</span>
<span class="ltx_bibblock">Knowledgpt: Enhancing Large Language Models with Retrieval
and Storage Access on Knowledge Bases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">arXiv</em>, abs/2308.11761.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Workshop et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
BigScience Workshop, Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie
Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha
Luccioni, François Yvon, and others. 2022.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">arXiv</em>, abs/2211.05100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lee Xiong, Chenyan Xiong, Ye&nbsp;Li, Kwok-Fung Tang, Jialin Liu, Paul&nbsp;N. Bennett,
Junaid Ahmed, and Arnold Overwijk. 2021.

</span>
<span class="ltx_bibblock">Approximate Nearest Neighbor Negative Contrastive Learning
for Dense Text Retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.

</span>
<span class="ltx_bibblock">Recomp: Improving Retrieval-Augmented LMs with Compression
and Selective Augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">arXiv</em>, abs/2310.04408.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and
Jing Xiao. 2023a.

</span>
<span class="ltx_bibblock">Prca: Fitting Black-Box Large Language Models for
Retrieval Question Answering via Pluggable Reward-Driven
Contextual Adapter.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em>, pages 5364–5375.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
Salakhutdinov, and Christopher&nbsp;D. Manning. 2018.

</span>
<span class="ltx_bibblock">Hotpotqa: A Dataset for Diverse, Explainable Multi-hop
Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing</em>, pages 2369–2380.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang,
Linxi Fan, Zhiding Yu, Shiyi Lan, Bo&nbsp;Li, Mohammad Shoeybi, Ming-Yu Liu, Yuke
Zhu, Bryan Catanzaro, Chaowei Xiao, and Anima Anandkumar. 2023b.

</span>
<span class="ltx_bibblock">Re-ViLM: Retrieval-Augmented Visual Language Model for
Zero and Few-Shot Image Captioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">Findings of the Association for Computational
Linguistics: EMNLP 2023</em>, pages 11844–11857. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul&nbsp;N. Bennett, Jianfeng Gao,
and Zhiyuan Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3397271.3401323" title="">Few-shot generative
conversational query rewriting</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">Proceedings of the 43rd International ACM SIGIR
conference on research and development in Information Retrieval, SIGIR
2020, Virtual Event, China, July 25-30, 2020</em>, pages 1933–1936. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal,
Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023a.

</span>
<span class="ltx_bibblock">Generate rather than Retrieve: Large Language Models are
Strong Context Generators.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and
Meng Jiang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3512467" title="">A survey of
knowledge-enhanced text generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">ACM Comput. Surv.</em>, 54(11s):227:1–227:38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. 2023b.

</span>
<span class="ltx_bibblock">Augmentation-Adapted Retriever Improves Generalization of
Language Models as Generic Plug-In.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers)</em>,
pages 2421–2436. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark&nbsp;D Plumbley, and Wenwu Wang.
2023.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Text-to-Audio Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">arXiv</em>, abs/2309.08051.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi&nbsp;Victoria Lin, and others.
2022.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">arXiv</em>, abs/2205.01068.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Huaixiu&nbsp;Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, E.&nbsp;Chi,
Quoc&nbsp;V. Le, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Take a Step Back: Evoking Reasoning via Abstraction in
Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">arXiv</em>, abs/2310.06117.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji&nbsp;Ma, Jing Lu, Jianmo Ni,
Xuanhui Wang, and Michael Bendersky. 2023.

</span>
<span class="ltx_bibblock">Rankt5: Fine-Tuning T5 for Text Ranking with Ranking
Losses.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Information Retrieval</em>.
ACM.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header" data-bs-theme="dark"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
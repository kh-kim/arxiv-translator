<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Multi-task Item-attribute Graph Pre-training for Strict Cold-start Item Recommendation\n' +
      '\n' +
      'YUWEI CAO\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'LIANGWEI YANG\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'University of Illinois Chicago\n' +
      '\n' +
      ' USA\n' +
      '\n' +
      'USA\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Recommendation systems suffer in the _strict cold-start (SCS)_ scenario, where the user-item interactions are entirely unavailable. The well-established, dominating identity (ID)-based approaches completely fail to work. Cold-start recommenders, on the other hand, leverage item contents (brand, title, descriptions, etc.) to map the new items to the existing ones. However, the existing SCS recommenders explore item contents in _coarse-grained_ manners that introduce noise or information loss. Moreover, informative data sources other than item contents, such as users\' purchase sequences and review texts, are largely ignored. In this work, we explore the role of the _fine-grained_ item attributes in bridging the gaps between the existing and the SCS items and pre-train a knowledgeable item-attribute graph for SCS item recommendation. Our proposed framework, ColdGPT, models item-attribute correlations into an item-attribute graph by extracting fine-grained attributes from item contents. ColdGPT then transfers knowledge into the item-attribute graph from various available data sources, i.e., item contents, historical purchase sequences, and review texts of the existing items, via multi-task learning. To facilitate the positive transfer, ColdGPT designs specific submodules according to the natural forms of the data sources and proposes to coordinate the multiple pre-training tasks via unified alignment-and-uniformity losses. Our pre-trained item-attribute graph acts as an implicit, extendable item embedding matrix, which enables the SCS item embeddings to be easily acquired by inserting these items into the item-attribute graph and propagating their attributes\' embeddings. We carefully process three public datasets, i.e., Yelp, Amazon-home, and Amazon-sports, to guarantee the SCS setting for evaluation. Extensive experiments show that ColdGPT consistently outperforms the existing SCS recommenders by large margins and even surpasses models that are pre-trained on 75 - 224 times more, cross-domain data on two out of four datasets. Our code and pre-processed datasets for SCS evaluations are publicly available to help future SCS studies.\n' +
      '\n' +
      '[] **Information systems \\(\\rightarrow\\) Data mining: Collaborative filtering.**\n' +
      '\n' +
      '[] **Information systems \\(\\rightarrow\\) Data mining: Collaborative filtering.**\n' +
      '\n' +
      '[] **Additional Key Words and Phrases: Strict Cold-start Recommendation, Graph Pre-training, Multi-task Learning**\n' +
      '\n' +
      '[] **ACM Reference Format:**\n' +
      '\n' +
      'Yuwei Cao, Liangwei Yang, Chen Wang, Zhiwei Liu, Hao Peng, Chenyu You, and Philip S. Yu. 2023. Multi-task Item-attribute Graph Pre-training for Strict Cold-start Item Recommendation. In _RecSys \'23: ACM Conference on Recommender Systems, September 18-22, 2023, Singapore_. ACM, New York, NY, USA, 18 pages. [https://doi.org/](https://doi.org/)\n' +
      '\n' +
      '## 1. Introduction\n' +
      '\n' +
      'Recommendation systems suffer in cold-start scenarios where the well-established, dominating identity (ID)-based approaches deteriorate (Wang et al., 2018; Wang et al., 2019). The problem is especially severe in the zero-shot, _strict cold-start (SCS)_(Wang et al., 2019) setting. In this work, we propose to address SCS item recommendation, i.e., recommend new items that are unseen during training without relying on any historical rating records of these new items. This is to be differentiated from the _few-shot cold-start_ item recommendation (Wang et al., 2019), in which the new items\' rating records, though inaccessible during training and/or pre-training, are available for inference and/or fine-tuning.\n' +
      '\n' +
      'Recommending cold-start items is of great importance. As shown in Figure 1 (a), SCS and few-shot cold-start items (e.g., festival new releases) account for up to 4% and 22% of the monthly Amazon-home purchases in the year 2017, respectively. Cold-start recommendation is an essential task, yet is under-explored (Wang et al., 2019). Few studies address SCS item recommendation, as it is extremely difficult: without rating records, we have little knowledge about new items, merely _item contents_, i.e., auxiliary features such as price, brand, descriptions, etc. Intrinsically, one would resort to transferring knowledge from the warm-start items, which are associated with abundant information (historical rating scores and review texts, in addition to item contents). As such, existing SCS recommenders (Beng et al., 2017; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019) strive to reconstruct the preference embeddings of the users and items from their content embeddings. In addition, owing to the rapid advance of NLP, recent methods such as UniSRec (Wang et al., 2019) and P5 (Wang et al., 2019) adapt pre-trained language models (PLMs) (Wang et al., 2019; Wang et al., 2019) for recommendation and are capable of accommodating SCS items. However, existing methods merely explore the available data sources in _coarse-grained_ manners. Specifically, methods that reconstruct the preference embeddings ignore both the review texts and the item correlations contained in the users\' rating sequences. UniSRec (Wang et al., 2019) simply concatenates item contents and truncates the information due to the input length limitations of PLMs. P5 (Wang et al., 2019) distorts various data sources, including the numerical rating sequences, into natural language sequences to fit its PLM backbone (Wang et al., 2019). Such coarse-grained processing tends to mix useful information with noise and hinders the preference characterization ability of models.\n' +
      '\n' +
      'Hence, this paper investigates how to leverage the information for SCS items in a _fine-grained_ manner. Specifically, we study how to effectively harness the item _attributes_, i.e., phrases that describe the items, to characterize proper item information for the recommendation, especially for those SCS items. We observe that though new items are\n' +
      '\n' +
      'Figure 1. The distribution of Amazon-home purchases in 2017 (a) and an example of our proposed item-attribute graph (b). In (a), ‘SCS’, ‘few-shot CS’, and ‘Other’ count the purchases of the SCS, few-shot cold-start (items that have been bought \\(>1\\) and \\(\\leq 20\\) times), and the other items, respectively. In (b), the LHS shows a pre-trained item-attribute graph and the RHS shows the updated item-attribute graph with SCS restaurant C and its new attribute, ‘Family-owned’, inserted.\n' +
      '\n' +
      'ever-emerging, their attributes are relatively stable. We demonstrate in Table 1 that \\(2,867\\) test items in the Home-SCS dataset introduce only one \\(1\\) new attribute. Therefore, if we train a recommendation model via the attributes of items, it is also able to recommend SCS items. In Figure 1 (b), for example, a user that likes restaurant A may be interested in a new restaurant C that shares the same style (\'Italian\') and serves similar foods (\'Pizza\'). Based on this intuition, we propose to pre-train our model on a bipartite item-attribute graph for SCS item recommendation. The goal is to transfer item-related knowledge from available data resources into attribute embeddings. Informative SCS item embeddings can then be easily acquired upon the arrival of these items -- insert the SCS items along with the new attributes into the item-attribute graph and propagate their neighbors\' embeddings. For example, in Figure 1 (b), restaurant C\'s embedding is obtained by fusing the embeddings of its attributes, i.e., \'Italian\', \'Pizza\', and \'Family-owned\'. The first two contain pre-trained knowledge related to the existing restaurants while the last one introduces information unique to restaurant C. Arguably, our proposed item-attribute graph is an implicit, extendable counterpart of the traditional explicit, fixed item ID embedding matrix in the SCS setting: it allows encoding items with only their attributes.\n' +
      '\n' +
      'Pre-training a knowledgeable item-attribute graph requires properly addressing three challenges. **1) Construction of the item-attribute graph.** On the one hand, mining fine-grained item attributes from the item contents is non-trivial. The item contents can be long and noisy, containing all-caps sentences, URLs, special characters, etc. We carefully pre-process them and extract noun phrases (which better describe the items than the raw item contents, as demonstrated in Section 2.2) to serve as high-quality item attributes. On the other hand, review texts reflect users\' subjective perception of the items\' attributes and serve as valuable supplements to the objective item contents. However, due to the high volume and complexity of the review texts, using them in straightforward manners is costly and ineffective (empirically verified in Section 3). Inspired by studies in phrase-level sentiment analysis [74], we extract phrases with sentiments and model the item correlations reflected by them. For example, in Figure 1 (b), subjective descriptions such as \'Seasonal appetizer\' helps to reveal the subtle correlations between restaurants A and B.\n' +
      '\n' +
      '**2) Transferring knowledge from various available data sources**, i.e., item contents, historical rating scores, and review texts. To preserve the above information, we design submodules according to their natural forms for further exploration. Specifically, the item contents, after modeled into the aforementioned item-attribute graph, are then embedded with PLM and graph neural network (GNN) [25, 53] to capture the underlying natural language semantics and higher-order structural information. The historical rating scores are modeled with a Transformer [52]-based sequential submodule to extract item-item correlations. The review texts are mined in a manner similar to the item contents but with a separate submodule.\n' +
      '\n' +
      '**3) Simultaneously incorporating all knowledge.** We conduct multi-task learning (MTL) [4] with self-supervised pre-training tasks specifically designed for these sub-modules to transfer knowledge into the item-attribute graph. We observe negative transfer [4] between the tasks, which leads to sub-optimal performance when adopting the commonly used loss terms for the different sub-modules (detailed in Section 3.3). We hypothesize that negative knowledge transfer results from the inconsistency of different objective functions. Thus, we propose to integrate knowledge from different tasks in a unified manner. Specifically, we propose to coordinate embeddings learned from different tasks via consistent _alignment_[59] losses. In this way, knowledge from multiple tasks is effectively shared. However, enforcing the alignment of multi-task knowledge without regularization induces the embedding collapse problem [23, 59] as the optimization process degenerates to a trivial solution. Hence, we devise a multi-task _uniformity_[59] regularization objective, which enables those embeddings to be uniformly distributed on a common hyper-sphere. As such, knowledge can be effectively fused and positively transferred. Differing from recent studies [56] that leverage alignment-and-uniformity for collaborative filtering, we are the first work to demonstrate the extraordinary ability of multi-task alignment-and-uniformity in knowledge transferring. Additionally, we address the discrepancies among various pre-training tasks and investigate how to simultaneously characterize the item-attribute, item-item, and item-review correlations via a multi-task training paradigm.\n' +
      '\n' +
      'Our proposed unified task losses greatly benefit the multi-task pre-training, as demonstrated in Section 3.3. To further facilitate the positive transfer, we carefully tune hyperparameters including the task weights, learning rate, and regularization weight, as recent MTL studies (Zhu et al., 2020; Wang et al., 2021) suggest that such an approach essentially outperforms the ad-hoc MTL methods (Zhu et al., 2020; Wang et al., 2021). We name our proposed framework as an item-attribute graph pre-training based SCS item recommender, i.e., ColdGPT (GPT stands for graph pre-training, which is to be differentiated from generative pre-trained transformer (Beng et al., 2019)). We carefully process three public datasets, i.e., Yelp, Amazon-home, and Amazon-sports, to guarantee the SCS setting for ColdGPT\'s evaluation. Extensive experiments show that ColdGPT outperforms the existing cold-start recommenders consistently by large margins and even surpasses models that are pre-trained on high-volume, cross-domain data on two out of four datasets.\n' +
      '\n' +
      'The contributions of our work are: 1) we address item recommendation in the SCS setting, which is extremely challenging and under-explored. Compared to the previous SCS methods, we consider more information, i.e., sequential item correlations and review texts, in addition to item contents. 2) we explore item contents in a fine-grained manner like no previous recommenders and explicitly model the item attributes to bridge the gaps between the existing and the SCS items. Our pre-trained item-attribute graph serves as an implicit, extendable counterpart of the traditional item ID embedding matrix in the SCS setting and allows us to easily encode items using their attributes. 3) we effectively transfer knowledge from various data sources, i.e., item contents, rating scores, and review texts of the existing items, into the item-attribute graph. Our proposed framework, ColdGPT, adopts submodules that are specifically designed with respect to the natural forms of various data sources. The multi-task pre-training paradigm of ColdGPT is the first to demonstrate the extraordinary ability of multi-task alignment-and-uniformity in positive knowledge transferring. 4) we evaluate ColdGPT in the SCS setting, which is either overlooked or under-explored by the previous work in their evaluations. Extensive experiments show the superior performance of ColdGPT over various baselines and the effectiveness of ColdGPT\'s components. To help future SCS studies, our code and datasets carefully processed to keep the SCS setting are publicly available1.\n' +
      '\n' +
      'Footnote 1: [https://github.com/YuweiCao-UIC/ColdGPT](https://github.com/YuweiCao-UIC/ColdGPT)\n' +
      '\n' +
      '## 2. Methodology\n' +
      '\n' +
      'In this section, we pre-train an item-attribute graph that enables efficient and effective inference of SCS item embedding for solving SCS item recommendation. Figure 2 shows the overall architecture of the proposed ColdGPT framework. We first formulate the task in Section 2.1. Section 2.2 presents the construction of the item-attribute graph. Section 2.3 illustrates how the item-attribute graph is pre-trained by transferring knowledge from various available data sources. Section 2.4 introduces how SCS item embeddings can be easily acquired using the pre-trained item-attribute graph.\n' +
      '\n' +
      '### SCS Item Recommendation\n' +
      '\n' +
      'Let \\(\\mathcal{U}\\), \\(\\mathcal{I}_{old}\\), and \\(\\mathcal{I}_{SCS}\\) denote the user, the existing item, and the SCS item set, respectively, and \\(\\mathcal{I}_{old}\\cap\\mathcal{I}_{SCS}=\\emptyset\\). In the task of SCS item recommendation, we are given 1) \\(\\mathcal{Y}_{old}=\\{(u,i)|u\\in\\mathcal{U},i\\in\\mathcal{I}_{old}\\}\\), which is a set of observed user-item interactions (optionally, each interaction comes with a textual review) and 2) \\(\\mathcal{C}=\\{C_{i}|i\\in\\mathcal{I}_{old}\\cup\\mathcal{I}_{SCS}\\}\\), which is a set of item contents and the item contents \\(\\mathcal{C}_{i}\\) of item \\(i\\) include _title_, _brand_, _category_, _description field_, etc., depending on the specific dataset. Our goal is to learn a model \\(f_{\\mathcal{O}}\\) that recommends \\(\\mathcal{I}_{SCS}\\) to \\(\\mathcal{U}\\), i.e., \\(f_{\\mathcal{O}}(C,\\mathcal{Y}_{old})=\\hat{\\mathcal{Y}}_{SCS}=\\{(u,i)|u\\in \\mathcal{U},i\\in\\mathcal{I}_{SCS}\\}\\) and we want the recommendations to be as accurate as possible. I.e., \\(maximize(|\\hat{\\mathcal{Y}}_{SCS}\\cap\\mathcal{Y}_{SCS}|)\\), where \\(\\mathcal{Y}_{SCS}=\\{(u,i)|u\\in\\mathcal{U},i\\in\\mathcal{I}_{SCS},u\\text{ subsequently interacts with }i\\}\\).\n' +
      '\n' +
      '### Item-attribute Graph Construction\n' +
      '\n' +
      'The main challenge in constructing the proposed item-attribute graph is determining the attributes given the item contents, i.e., fields such as _brand, category, description_, etc. Considering their essential role as media in bridging the items, the attributes should only contain terms that reflect the property of the items. As shown in Figure 2 (a), fields such as brand are short and precise. Therefore, they can be directly added to the set of attributes, denoted as \\(\\mathcal{A}\\). For long, noisy fields, however, pre-processing is necessary. We observe that compared to the raw sentences, the noun phrases better describe the items with less irrelevant information. For example, the raw description field of the sample item contents in Figure 2 (a) contains words that are irrelevant to the item, such as \'whether\' and \'you\'re\'. There is also noise like hashtags and URLs. In contrast, the noun phrases contained in the description field, i.e. \'gardening supplies\', \'holiday decorations\', and \'black-and-gray plastic container\', precisely describe the item with much less redundant information. Therefore, we extract noun phrases to serve as item attributes and add these attributes into \\(\\mathcal{A}\\). We first filter out the noise and non-human-readable contents such as URLs, HTML tags, and special characters, and then leverage the NLP toolkit 2 for noun phrases extraction. As shown in Figure 2 (b), the construction of the proposed item-attribute graph \\(\\mathcal{G}\\) then becomes straightforward - we treat a set of all the old items as well as all their acquired\n' +
      '\n' +
      'Figure 2. The overall architecture of our proposed ColdGPT framework. The blue, red, and purple nodes indicate the items, attributes, and review terms, respectively. (a) shows the extraction of the fine-grained attributes. (b) shows the construction of the item-attribute graph. (c) shows the multi-task pre-training of the item-attribute graph, in which the parameters of the PLM (grey rectangle) are fixed. (d) shows SCS item embedding with the pre-trained item-attribute graph.\n' +
      '\n' +
      'attributes as nodes and then adding non-directed edges between each item and its attributes. I.e., the node set of \\(\\mathcal{G}\\) is \\(\\mathcal{V}=\\mathcal{I}_{old}\\cup\\mathcal{A}\\) and the edge set of \\(\\mathcal{G}\\) is \\(\\mathcal{E}=\\{(i,a)|i\\in\\mathcal{I}_{old},a\\in\\mathcal{A},i\\text{ has attribute }a\\}\\).\n' +
      '\n' +
      '### Item-attribute Graph Pre-training\n' +
      '\n' +
      'Figure 2 (c) shows the multi-task pre-training of the proposed item-attribute graph. The pre-training process aims to fuse knowledge from the various available data sources, i.e., item contents \\(\\mathcal{C}\\), historical rating scores, and review texts of the old items \\(\\mathcal{Y}_{old}\\), into the item-attribute graph. To facilitate knowledge transfer, we design specific submodules concerning the natural forms of the data sources as well as pre-training tasks with unified losses that consistently adhere to the property of alignment-and-uniformity (Srivastava et al., 2017), as introduced in Section 2.3.1 - 2.3.3. The pre-training is then conducted in an MTL manner, as detailed in Section 2.3.4.\n' +
      '\n' +
      '#### 2.3.1. Task 1: Item-attribute Correlation Modeling\n' +
      '\n' +
      'The first pre-training task integrates the natural language semantics of the attributes and the higher-order item-attribute correlations into the item-attribute graph (more specifically, into the representations of its nodes). We adopt PLMs, _e.g._, BERT (Devlin et al., 2017), and GNNs, _e.g._, GCN (Kipf and Welling, 2017) for knowledge extraction and then design alignment-and-uniformity-based loss terms for knowledge transferring.\n' +
      '\n' +
      'As shown in Figure 2 (c), we input the attributes and the items (each item is represented by the concatenation of its attributes) into PLM to get their natural language embeddings. We input these embeddings into an interpreter, i.e., a multi-layer perceptron (MLP), to filter out any irrelevant information and map them into a lower dimensional space for easier downstream modeling. We then propagate the mapped embeddings over the item-attribute graph via a GNN to learn higher-order item-attribute correlations. The forward propagation follows:\n' +
      '\n' +
      '\\[h_{1}(x)=\\text{GNN}_{1}(\\text{Interpreter}(\\text{PLM}(x))),x\\in\\mathcal{I}_{ old}\\cup\\mathcal{A}. \\tag{1}\\]\n' +
      '\n' +
      'The RHS of the above equation, referred to as the encoder of task 1, encodes the item-attribute graph. It serves as an implicit, extendable embedding matrix and is also shared either entirely or partially by the other pre-training tasks (discussed in the following sections).\n' +
      '\n' +
      'Instead of adopting InfoNCE (Han et al., 2017) or BPR (Kipf and Welling, 2017) losses that are commonly used in Graph Pre-training (Kipf and Welling, 2017), we design the first pre-training task with self-supervised loss that adhere to the alignment and uniformity properties (Srivastava et al., 2017), which are favorable in contrastive learning and essential for alleviating the sparsity issue. Specifically, task 1 minimizes \\(\\mathcal{L}_{1}\\) as:\n' +
      '\n' +
      '\\[\\mathcal{L}_{1}=\\mathcal{L}_{a1}+\\lambda\\mathcal{L}_{u1}, \\tag{2}\\]\n' +
      '\n' +
      '\\[\\mathcal{L}_{a1}=\\mathop{\\mathbb{E}}_{(i,a)\\sim p_{\\text{pos},\\text{ia}}}\\parallel h _{1}(i)-h_{1}(a)\\parallel^{2}, \\tag{3}\\]\n' +
      '\n' +
      '\\[\\mathcal{L}_{u1}=\\log\\mathop{\\mathbb{E}}_{(i,i^{\\prime})\\sim p_{\\text{item}} }e^{-2\\left\\|h_{1}(i)-h_{1}(i^{\\prime})\\right\\|^{2}}/2+\\log\\mathop{\\mathbb{E} }_{(a,a^{\\prime})\\sim p_{\\text{attr}}}e^{-2\\left\\|h_{1}(a)-h_{1}(a^{\\prime}) \\right\\|^{2}}/2. \\tag{4}\\]\n' +
      '\n' +
      '\\(\\mathcal{L}_{a1}\\) and \\(\\mathcal{L}_{u1}\\) are alignment and uniformity loss terms, respectively. \\(p_{\\text{pos}\\_\\text{ia}}\\) denotes the distribution of the positive item-attribute pairs. \\(p_{\\text{item}}\\) and \\(p_{\\text{attr}}\\) denote the data distributions of the items and the attributes. In practice, we use in-batch \\((i,a)\\), \\((i,i^{\\prime})\\), and \\((a,a^{\\prime})\\) pairs as these instances are consistent with the actual data distributions, i.e., \\(p_{\\text{pos}\\_\\text{ia}}\\), \\(p_{\\text{item}}\\), and \\(p_{\\text{attr}}\\). Intuitively, \\(\\mathcal{L}_{a1}\\) aligns the features of each item with the features of its attributes to capture their correlations while \\(\\mathcal{L}_{u1}\\) highlights such correlations by pushing each item/attribute away from the remaining items/attributes.\n' +
      '\n' +
      '#### 2.3.2. Task 2: Item-item Correlation Modeling\n' +
      '\n' +
      'Pre-training task 2 seeks to fuse the correlations between the old items into the items-attribute graph. The users\' historical purchase sequences can reflect the correlations between these items, i.e., two items are considered correlated if they were purchased by the same user. Following this intuition, we leverage a Transformer-based sequential encoder to model the historical rating sequences, as shown in Figure 2 (c). The architecture of our sequential encoder is similar to [47] (we choose its bidirectional self-attention over the unidirectional self-attention adopted by other sequential encoders [24], as the former is recently shown to perform better with proper configuration [39]). Different from [47], which randomly initializes the item embeddings, we use the encoder of task \\(1\\) (Section 2.3.1) to serve as an implicit and extendable embedding matrix. The forward propagation follows:\n' +
      '\n' +
      '\\[h_{2}(u)=\\text{FFN}(\\text{BiAttn}(\\text{mask}(h_{1}(u))+p(u))), \\tag{5}\\]\n' +
      '\n' +
      'where \\(u\\) denotes a user\'s historical purchase sequence, which is a sequence of items. \\(h_{1}(u)\\) is a sequence of initial embeddings of the items in \\(u\\), acquired using the encoder of task \\(1\\) (Equation 1). \\(\\text{mask}(\\cdot)\\) randomly masks out some items in \\(u\\), i.e., replace their embeddings with an embedding that represents a special token [mask]. \\(p(u)\\) encodes the positional information of the items in \\(u\\). BiAttn and FFN denote the stacks of multi-head self-attention layers and point-wise feed-forward layers, which are the same as in BERT4Rec. \\(h_{2}(u)\\) is a sequence of embeddings of the items in \\(u\\), enhanced with sequence-level item-item correlations.\n' +
      '\n' +
      'Pre-training task \\(2\\) is then designed to predict the masked items given the remaining contextual items in the sequences. Instead of leveraging the cross-entropy loss, which is widely adopted by the existing sequential encoders including BERT4Rec, we design loss terms to adhere to the alignment and uniformity properties [59]. These loss terms are consistent with the losses of the other pre-training tasks (Section 2.3.1 and 2.3.3), which help the tasks to cooperate with each other in multi-task pre-training (illustrated in Section 2.3.4). The loss of task \\(2\\) is defined as follows:\n' +
      '\n' +
      '\\[\\mathcal{L}_{2}=\\mathcal{L}_{a2}+\\lambda\\mathcal{L}_{u2}, \\tag{6}\\]\n' +
      '\n' +
      '\\[\\mathcal{L}_{a2}=\\mathop{\\mathbb{E}}_{i\\sim p_{\\text{mask}}}\\parallel h_{1}( i)-h_{2}(i)\\parallel^{2},\\quad\\mathcal{L}_{u2}=\\log\\mathop{\\mathbb{E}}_{(i,i^{ \\prime})-p_{\\text{harm}}}e^{-2\\left\\lVert h_{1}(i)-h_{1}(i^{\\prime})\\right\\rVert ^{2}}/2. \\tag{7}\\]\n' +
      '\n' +
      '\\(\\mathcal{L}_{a2}\\) is the alignment loss term. The uniformity loss term \\(\\mathcal{L}_{u2}\\) follows Equation 4 but with only the first term as the attributes are not involved in this task. \\(p_{\\text{mask}}\\) denotes the distribution of the masked items and is approximated by using the in-batch instances. Intuitively, \\(\\mathcal{L}_{a2}\\) aligns \\(h_{2}(i)\\), which encodes the item-item correlations underneath the purchase sequences, with \\(h_{1}(i)\\), which are the item embeddings in the item-attribute graph. In this way, the item-item correlations are fused into the item-attribute graph.\n' +
      '\n' +
      '#### 2.3.3. Task 3: Item-review Correlation Modeling\n' +
      '\n' +
      'Review texts contain important subjective observations made by the users and describe the subtle aspects of the items. However, review texts are ignored by the existing cold-start recommenders [18, 64] as they are high in volume, long, and noisy, and thus are difficult to explore. Our pre-training task 3 seeks to transfer knowledge from the review texts of the existing items in an efficient and effective manner. Specifically, as shown in Figure 2 (a), we pre-process the review texts to extract phrases with sentiments. We first leverage phrase-level sentiment analysis [74] tool kits 3\\({}^{,}\\)4 and get a set of (noun phrase, opinion word, sentiment score) tuples (the set is formally referred to as context-dependent sentiment lexicon). Concerning the huge size of the set caused by the diverse opinion words, we further filter the tuples and only keep the sentiments and the noun phrases. E.g., (lock, easy to open, 1) is mapped into \'good lock\' and (hinge, broken, -1) is mapped into \'bad hinge\'. We refer to the results as the _review terms_, to be distinguished from the item attributes. Review terms that are too common or too rare are filtered out to simplify the graph and avoid introducing less informative item correlations. The remaining review terms, along with their correlated items, form a bipartite item-review term graph in a manner similar to how the item-attribute graph is constructed (Section 2.2). The items and review terms are embedded with the PLM and Interpreter of task 1 and then propagated over the item-review term graph via GNN layers. The forward propagation of task 3, as shown in Figure 2 (c), follows:\n' +
      '\n' +
      '\\[h_{3}(x)=\\text{GNN}_{3}(\\text{Interpreter}(\\text{PLM}(x))),x\\in\\mathcal{I}_{ old}\\cup\\mathcal{R}, \\tag{8}\\]\n' +
      '\n' +
      'where \\(\\mathcal{R}\\) denotes a set of all the review terms. GNN\\({}_{3}\\) does not share parameters with GNN\\({}_{1}\\) since it aims to model the higher-order item-review term correlations instead of the item-attribute correlations. Note how the encoders of task 3 and task 1 partially overlap, by using the same PLM and Interpreter, to enable knowledge sharing.\n' +
      '\n' +
      'The loss of task 3 follows the forms of the losses of the previous two tasks and is defined as:\n' +
      '\n' +
      '\\[\\mathcal{L}_{3}=\\mathcal{L}_{a3}+\\lambda\\mathcal{L}_{u3}, \\tag{9}\\]\n' +
      '\n' +
      '\\[\\mathcal{L}_{a3}=\\mathop{\\mathbb{E}}_{(i,t^{\\prime})\\sim p_{\\text{pos},i}} \\parallel h_{3}(i)-h_{3}(r)\\parallel^{2}, \\tag{10}\\]\n' +
      '\n' +
      '\\[\\mathcal{L}_{u3}=\\log\\mathop{\\mathbb{E}}_{(i,t^{\\prime})\\sim p_{\\text{item}} }e^{-2\\left\\|h_{3}(i)-h_{3}(t^{\\prime})\\right\\|^{2}}/2+\\log\\mathop{\\mathbb{E} }_{(r,r^{\\prime})\\sim p_{\\text{review}}}e^{-2\\left\\|h_{3}(r)-h_{3}(r^{\\prime} )\\right\\|^{2}}/2. \\tag{11}\\]\n' +
      '\n' +
      '\\(\\mathcal{L}_{a3}\\) is the alignment loss term. The uniformity loss term \\(\\mathcal{L}_{u3}\\) follows Equation 4 but its second term contrasts review terms other than attributes. \\(p_{\\text{review}}\\) is the distribution of the review terms and is approximated with in-batch \\((r,r^{\\prime})\\) pairs. \\(p_{\\text{pos}\\_ir}\\) is the distribution of the positive item-review term pairs and is approximated by using the in-batch instances. Intuitively, \\(\\mathcal{L}_{a3}\\) aligns the features of each item with the features of its review terms to capture their correlations. Such correlations are fused into the item-attribute graph through the shared parameters of the encoders of task 3 and task 1.\n' +
      '\n' +
      '#### 2.3.4. Multi-task Item-attribute Graph Pre-training\n' +
      '\n' +
      'To pre-train a knowledgeable item-attribute graph, ColdGPT leverages MTL and conducts the pre-training tasks introduced in Sections 2.3.1 - 2.3.3 concurrently. The encoder of task 1, which encodes the item-attribute graph, is shared by task 2 and partially by task 3, thus enabling a fusion of the knowledge learned in all three pre-training tasks into the item-attribute graph.\n' +
      '\n' +
      'It is essential, however, for the pre-training tasks to cooperate with each other and facilitate positive transfer among themselves. As shown in Sections 2.3.1 - 2.3.3, we design unified losses for the pre-training tasks. Despite the extensive studies on contrastive learning (Zhu et al., 2017; Wang et al., 2018; Wang et al., 2019; Wang et al., 2019), our design stands out for its novelty as it aligns and contrasts various embedding spaces within a MTL scenario. Our losses help to fuse knowledge from the various data sources into the item-attribute graph and mitigate the negative transfer between the tasks observed when adopting heterogeneous task losses (empirically shown in Section 3.3). To further facilitate the positive transfer, we adopt unitary scalarization, i.e., simply minimizes the weighted sum of the task losses as it is shown, by recent studies in MTL (Zhu et al., 2018; Wang et al., 2019), to outperform ad-hoc multi-task optimization algorithms when hyperparameters such as the task weights, the regularization term weight, and the learning rate, are properly tuned (we carefully tune the hyperparameters in Section 3.3). The overall training objective is:\n' +
      '\n' +
      '\\[\\mathcal{L}=\\sum_{i=1}^{K}\\mathbf{w}_{i}\\mathcal{L}_{i}, \\tag{12}\\]\n' +
      '\n' +
      'where \\(K=3\\) denotes the number of pre-training tasks. \\(\\mathbf{w}>0\\) are the weights of the tasks.\n' +
      '\n' +
      '### SCS Item Embedding with Pre-trained Item-attribute Graph\n' +
      '\n' +
      'Given the pre-trained item-attribute graph, the embeddings of the SCS items can be easily acquired upon the arrival of these items and rely only on their item contents, as shown in Figure 2 (d). Specifically, we first extract the attributes of the SCS items from their item contents. The SCS items \\(I_{SCS}\\), along with new item attributes that are unseen during the pre-training (denoted as \\(\\mathcal{A}_{SCS}\\)) are then inserted into the pre-trained item-attribute graph. Formally, the node set of the item-attribute graph, after the update, is \\(\\mathcal{V}=\\mathcal{I}_{old}\\cup\\mathcal{I}_{SCS}\\cup\\mathcal{A}\\cup\\mathcal{ A}_{SCS}\\) while the edge set after the update is \\(\\mathcal{E}=\\{(i,a)|i\\in\\mathcal{I}_{old}\\cup\\mathcal{I}_{SCS},a\\in\\mathcal{A} \\cup\\mathcal{A}_{SCS},i\\text{ has attribute }a\\}\\). We then conduct forward propagation over the updated item-attribute graph using the pre-trained encoder of task 1, shown in Equation 1, to get the embeddings of all the items and attributes, including the SCS ones. The learned item embeddings thus fuse the item-attribute, item-item, and item-review term correlations captured during the multi-task pre-training. Note that this process does not involve any fine-tuning as the SCS scenario assumes no interactions of the SCS items are available for fine-tuning.\n' +
      '\n' +
      'The acquired embeddings provide a solution for the SCS item recommendation task. Specifically, a prediction on a user\'s preference for an SCS item can be made by calculating the dot product of their embeddings (note that using the dot productions between the items\' and users\' embeddings for preference estimation is commonly seen in recommendation studies (Kumar et al., 2018)). The users\' embeddings, on the other hand, can be calculated by averaging the embeddings of the items that are purchased by them. Note that compared to cold-start recommenders that require large amounts of sampling (Wang et al., 2019) and querying (Kumar et al., 2018) during inference, our approach requires simple updates and propagation over the item-attribute graph, thus is much more efficient, as shown in Section 3.2.\n' +
      '\n' +
      '## 3. Experiment\n' +
      '\n' +
      'We evaluate the proposed ColdGPT in the SCS setting. Section 3.1 reports the experimental setup. Section 3.2 compares ColdGPT to various baselines for top-K SCS item recommendation. Section 3.3 verifies the effectiveness of ColdGPT\'s pre-training tasks and studies the effects of changing losses as well as hyperparameters. Section 3.4 visualizes our proposed item-attribute graph and qualitatively shows how attributes help bridge the items.\n' +
      '\n' +
      '### SCS Experimental Setup\n' +
      '\n' +
      '#### 3.1.1. SCS Datasets\n' +
      '\n' +
      'We experiment on three public real-world datasets, i.e., Yelp5, Amazon-home, and Amazon-sports6, which are commonly adopted in previous recommendation studies (Kumar et al., 2018; Wang et al., 2019). To guarantee the SCS setting, we carefully process these datasets.\n' +
      '\n' +
      'Footnote 5: [https://www.yelp.com/dataset](https://www.yelp.com/dataset)\n' +
      '\n' +
      'Footnote 6: [https://nijianmo.github.io/amazon/#subsets](https://nijianmo.github.io/amazon/#subsets)\n' +
      '\n' +
      'For Yelp and Amazon-home, we first follow previous works (Wang et al., 2019) and filter out users and items with too few (\\(<\\)20 for Yelp and \\(<\\)15 for Amazon-home) interactions. We further filter out items with insufficient item contents, as recommenders rely on item contents to make SCS item recommendations. Specifically, items that have less than five attributes are filtered out. Next, we sort the items based on the lengths of their historical purchase records. After sorting, we split the items by 9:1 for training and testing, so that the warm items are used for training. We leave out the last 5% of the training data as a validation set for hyper-parameter tuning. Finally, interactions that involve users unseen in the training set are removed from the validation and test sets.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|l|l l l l|l l l l|l l l} \\hline \\hline SCS Dataset & \\#Users & \\#Items & Train & Val & Test & \\#Attrs & Train & Val & Test & \\#Ratings & Train & Val & Test \\\\ \\hline Yelp-SCS & 4,901 & 2,639 & 2,258 & 118 & 263 & 2,087 & 2,035 & 9 & 43 & 193,320 & 185,097 & 2,731 & 5,492 \\\\ Home-SCS & 96,420 & 28,672 & 24,515 & 1,290 & 2,867 & 24,879 & 24,877 & 1 & 1 & 1,343,374 & 1,277,815 & 21,454 & 44,105 \\\\ Sports-SCS-1 & 81,778 & 28,316 & 26,483 & 1,393 & 440 & 12,154 & 12,130 & 0 & 24 & 464,335 & 456,050 & 6,745 & 1,540 \\\\ Sports-SCS-2 & 81,778 & 28,332 & 26,483 & 1,393 & 456 & 12,154 & 12,130 & 0 & 24 & 464,483 & 456,050 & 6,745 & 1,688 \\\\ Sports-SCS-3 & 81,778 & 28,344 & 26,483 & 1,393 & 468 & 12,155 & 12,130 & 0 & 25 & 464,376 & 456,050 & 6,745 & 1,581 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1. SCS Dataset statistics.\n' +
      '\n' +
      'The Amazon-sports dataset is processed differently with the aim of comparing ColdGPT to the pre-trained P5 model (Gupta et al., 2019). P5 is pre-trained on three large datasets, including the old version of the Amazon-sports dataset 7 and only able to predict the preferences of users seen during its pre-training. Given this, we explore the new version of the Amazon-sports dataset, which is partially overlapped with the old version, for test set construction. Specifically, we first randomly sample a set of items that are in the new version while not in the old version of Amazon-sports. These are considered candidate SCS items. We then extract the interactions of these items from the new version and remove the interactions that involve users that do not exist in the old version. The remaining interactions form a test set, in which all users are seen, and all items are unseen by the pre-trained P5 model. In this manner, we sample three test sets, each containing \\(\\sim\\)450 SCS items. The training and validation sets are then constructed from the old version of Amazon-sports in the same manner as Yelp and Amazon-home datasets.\n' +
      '\n' +
      'Footnote 7: [https://jmcauley.ucsd.edu/data/amazon/](https://jmcauley.ucsd.edu/data/amazon/)\n' +
      '\n' +
      'We denote the processed datasets as Yelp-SCS, Home-SCS, and Sports-SCS-1/2/3, respectively. Table 1 summarizes the statistics of these datasets. Note that for each processed dataset, the SCS setting is guaranteed as the interactions that involve the validation and test items are completely removed from the training set.\n' +
      '\n' +
      '#### 3.1.2. SCS Models\n' +
      '\n' +
      'We compare ColdGPT to content-based methods, Cold-start recommenders, and pre-training-based methods that accommodate SCS items. Content-based baselines are: 1) **BERT**(Chen et al., 2019). As a PLM, it enables a direct acquisition of the embeddings of the items by taking the concatenations of their attributes as input. 2) **BERT+R**. To study the effects of leveraging the review texts in naive manners, for each old item, we further get a set of the BERT-based embeddings of all its reviews, then calculate the average of the attribute-based embedding and the review embeddings for the item\'s final embedding. 3) **AFM**(Wang et al., 2019), which is a variant of FM and leverages the attention mechanism to model the interactions of the input features. For Cold-start recommenders, we consider: 4) **DropoutNet**(Wang et al., 2019), which adopts a two-tower neural network and applies dropout to input mini-batches to reconstruct preference embeddings from content embeddings. 5) **Heater**(Wang et al., 2019), which adopts a randomized training mechanism for better reconstruction. For pre-training-based methods, we consider: 6) **IDCF-HY**(Wang et al., 2019), which is the SCS extension of the graph- and meta-learning-based IDCF (Wang et al., 2019) and relies on contents other than interactions for adaptation. 7) **GPT-GNN**(Wang et al., 2019), which is a generative graph pre-training method. 8) **UniSRec**(Chen et al., 2019), which embeds items with PLM and extends sequential recommenders (Wang et al., 2019; Wang et al., 2020) to the inductive setting. 9) **P5**(Gupta et al., 2019), which adopts a PLM backbone, i.e., T5 (Wang et al., 2019) and further pre-trains it by distorting recommendation data into natural language sequences. Furthermore, we compare to 10) the pre-trained, cross-domain version of UniSRec, denoted as **UniSRec-PT**. UniSRec-PT leverages knowledge transferred from other domains (five Amazon datasets, i.e., Food, CDs, Kindle, Movies, and Home, with 14,029,229 interactions in total). Note that UniSRec-PT is not considered as a baseline since it is pre-trained on large datasets (75-220 times larger than the pre-training data of ColdGPT) from different domains while ColdGPT only leverages data from the target domain.\n' +
      '\n' +
      '#### 3.1.3. SCS Experiment Setting\n' +
      '\n' +
      'For ColdGPT, we set the batch size to 512, the embedding dimension to 64, the learning rate to 0.005, the pre-training tasks\' weights **w** to [0.6, 0.2, 0.2], and the regularization term weight \\(\\lambda\\) to 0.6, the PLM to SBERT (Wang et al., 2019) (the effects of changing the hyperparameters are observed in Section 3.3). We set the number of convolutional layers to 1 for GNN\\({}_{1}\\) and GNN\\({}_{3}\\). For the BERT4Rec component in the submodule for our pre-training task 2, we set the maximum sequence length to 100, the number of self-attention layers to 1, the number of attention heads to 1, and the item masking probability to 0.2. When the BERT4Rec component is not involved (as in some of the ablation studies in 3.3), we adopt early stopping with a patience of 50 epochs using the pre-training loss (Equation 12) as an indicator.\n' +
      '\n' +
      'Otherwise, we follow (Srivastava et al., 2017) and adopt early stopping with a patience of 200 epochs. Consider the large volume of the review texts, we randomly sample 100 reviews per item to construct the item-review term graph for our pre-training task 3. As mentioned in Section 2.4, we calculate the users\' embeddings by averaging the embeddings of their purchased items. We then predict a user\'s preference for an item by calculating the dot product of their embeddings. For AFM, we adopt user embedding, item embedding, and rating score as the input features. For DropoutNet and Heater, we pre-train a Matric Factorization (MF) model on the training data to provide the required user/item collaborative embedding. The model structure and hyperparameter setting are consistent with the original paper. For IDCF-HY, UnisRec, and GPT-GNN, we adopt the experiment settings as reported in their original papers. For UnisRec, we acquire a user\'s embedding from its universal sequence representation module, and a test item\'s embedding from its MoE-enhanced adaptor then calculate the dot product of these embeddings for the predicted rating. For P5, we adopt the pre-trained version 8. Note we only report P5\'s performance on the Sports-SCS datasets as the pre-trained P5 model for the other two datasets is unavailable. We query P5 with the \'Z-3\' template. One query predicts only one interaction, therefore, we query all the test items for each user before ranking. For BERT and BERT+R, we adopt Hugging Face pre-trained models, i.e., \'bert-large-cased\'. The prediction process is consistent with ColdGPT after the acquisition of the item embeddings. We use a 12-core Intel Xeon CPU E5-2620 v3@2.40GHz with 64GB RAM and 3\\(\\times\\)NVIDIA GeForce GTX 1080 Ti GPU and report the mean over 5 runs for all experiments.\n' +
      '\n' +
      'Footnote 8: [https://huggingface.co/makitankaze/P5_sports_base](https://huggingface.co/makitankaze/P5_sports_base)\n' +
      '\n' +
      '#### 3.1.4. Evaluation Metrics\n' +
      '\n' +
      'Following the previous recommendation studies (Kang et al., 2018; Wang et al., 2019), we adopt two widely used metrics, i.e., NDCG and Recall. We report Recall@N and NDCG@N, where N is set to 5, 20, and 40.\n' +
      '\n' +
      '### SCS Item Recommendation\n' +
      '\n' +
      'Table 2 summarizes the top-K SCS item recommendation results. The observations are: 1) The proposed ColdGPT consistently outperforms the best baseline, i.e., UniSRC, by large margins. For example, ColdGPT achieves up to 64% higher NDCGs and up to 48% higher Recalls than UnisRec. Note that unlike UnisRec, which leverages the item contents in coarse manners (concatenate the fields then truncate the result), ColdGPT extracts and models fine-grained item attributes. This shows that fine-grained item-attribute correlations are essential for SCS item recommendation and the proposed item-attribute graph as well as multi-task pre-training framework effectively explore such correlations. 2) Compared to merely using the item contents, incorporating the rating score sequences of the existing items further improves performance. Specifically, models that incorporate sequential information, i.e., ColdGPT and UniSRC, outperform the other methods. This show that the item-item correlations contained in the historical rating sequences help to reveal the correlations between the existing items and the SCS ones. 3) Simple content-based methods, i.e., BERT and AFM, perform better than or on par with some cold-start and pre-training-based recommenders, i.e., DropoutNet, Heater, IDCF-HY, GPT-GNN, and P5. This shows that these recommenders fail to further capture the correlations between the SCS items and the old items. The reason is, they explore the available data sources in insufficient or coarse manners (e.g., DropoutNet, Heater, IDCF-HY, and GPT-GNN ignore the sequential correlations between the items, while P5 distorts the rating score sequences to natural language) and thus model noisy and unreliable item correlations. In contrast, our proposed ColdGPT constructs and pre-trains a fine-grained, knowledgeable item-attribute graph and efficiently mitigates the gap between the existing and the SCS items. This again justifies the importance of incorporating more information sources and exploring the information sources in fine-grained other than naive or coarse manners. 4) Although review texts contain valuable information, leveraging the in naive manners does not help. Specifically,\n' +
      '\n' +
      'BERT+R performs on par with BERT. We discuss more on proper ways to leverage review texts in Section 3.3. 5) The inference of ColdGPT is efficient. For example, the inferences of IDCF-HY and P5 on Sports-SCS-3 take hours as they require large amounts of sampling and querying, respectively. In contrast, ColdGPT takes < 60 seconds as it simply needs to insert the SCS items and attributes into the pre-trained item-attribute graph and then propagate.\n' +
      '\n' +
      'Table 3 compares ColdGPT to UnisRec-PT, which is pre-trained on 76-224 times more, cross-domain data. Despite being greatly disadvantaged by its scarce pre-training data, ColdGPT outperforms UnisRec-PT on Sports-SCS-1 and Sports-SCS-2 while performing on par on Sports-SCS-3. This shows the effectiveness and data efficiency of ColdGPT. We\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l|c c c c c c c c c|c} \\hline \\hline Dataset & Metric & BERT & BERT+R & AFM & DropoutNet & Heater & IDCF-HY & GPT-GNN & P5 & UniRec & ColdGPT & \\(\\Delta\\) (\\%) \\\\ \\hline \\multirow{4}{*}{Yelp} & NDCG@5 & 0.0125 & 0.0128 & 0.0132 & 0.0127 & 0.0128 & 0.0113 & 0.0032 & / & 0.0161 & **0.0265** & 64.60 \\\\  & NDCG@20 & 0.0391 & 0.0297 & 0.0308 & 0.0304 & 0.0301 & 0.0292 & 0.0091 & / & 0.0383 & **0.0556** & 45.17 \\\\  & NDCG@40 & 0.0480 & 0.0473 & 0.0504 & 0.0488 & 0.0480 & 0.0459 & 0.0157 & / & 0.0573 & **0.0863** & 50.61 \\\\ \\cline{2-13} -SCS & Recall@5 & 0.0182 & 0.0179 & 0.0170 & 0.0172 & 0.0179 & 0.0161 & 0.0042 & / & 0.0257 & **0.0371** & 44.36 \\\\  & Recall@20 & 0.0756 & 0.0743 & 0.0729 & 0.0734 & 0.0743 & 0.0736 & 0.0234 & / & 0.0989 & **0.1282** & 29.63 \\\\  & Recall@40 & 0.1541 & 0.1504 & 0.1566 & 0.1495 & 0.1485 & 0.1436 & 0.0515 & / & 0.1816 & **0.2354** & 29.63 \\\\ \\hline \\multirow{4}{*}{Home} & NDCG@5 & 0.0010 & 0.0010 & 0.0011 & 0.0010 & 0.0011 & / & 0.0009 & / & 0.0064 & **0.0095** & 48.44 \\\\  & NDCG@20 & 0.0027 & 0.0025 & 0.0026 & 0.0025 & / & 0.0056 & / & 0.0112 & **0.0166** & 48.21 \\\\ \\cline{2-13}  & DCG@40 & 0.0042 & 0.0037 & 0.0041 & 0.0041 & 0.0040 & / & 0.0121 & / & 0.0148 & **0.0215** & 45.27 \\\\ \\cline{2-13} -SCS & Recall@5 & 0.0017 & 0.0016 & 0.0018 & 0.0016 & 0.0018 & / & 0.0019 & / & 0.0094 & **0.0139** & 47.87 \\\\  & Recall@20 & 0.0073 & 0.0070 & 0.0070 & 0.0070 & 0.0071 & / & 0.0179 & / & 0.0263 & **0.0383** & 45.63 \\\\  & Recall@40 & 0.0144 & 0.0135 & 0.0139 & 0.0140 & 0.0134 & / & 0.0483 & / & 0.0426 & **0.0609** & 26.08 \\\\ \\hline \\multirow{4}{*}{Sports} & NDCG@5 & 0.0052 & 0.0051 & 0.0044 & 0.0082 & 0.0054 & 0.0049 & 0.0000 & 0.0047 & 0.0303 & **0.0324** & 6.93 \\\\  & NDCG@20 & 0.0154 & 0.0154 & 0.0135 & 0.0190 & 0.0152 & 0.0147 & 0.0000 & 0.0182 & 0.0613 & **0.0663** & 8.16 \\\\  & NDCG@40 & 0.0264 & 0.0259 & 0.0283 & 0.0274 & 0.0244 & 0.0252 & 0.0004 & 0.0231 & 0.0830 & **0.0884** & 6.51 \\\\ \\cline{2-13}  & Recall@5 & 0.0075 & 0.0076 & 0.0092 & 0.0124 & 0.0097 & 0.0072 & 0.0000 & 0.0080 & 0.0528 & **0.0571** & 8.14 \\\\  & Recall@20 & 0.0448 & 0.0446 & 0.0411 & 0.0505 & 0.0446 & 0.0446 & 0.0000 & 0.0473 & 0.1624 & **0.1760** & 8.37 \\\\ \\cline{2-13}  & Recall@40 & 0.0981 & 0.0970 & 0.0102 & 0.0910 & 0.0889 & 0.0971 & 0.0021 & 0.0744 & 0.2666 & **0.2831** & 6.19 \\\\ \\hline \\multirow{4}{*}{Sports} & NDCG@5 & 0.0043 & 0.0046 & 0.0050 & 0.0051 & 0.0067 & 0.0039 & 0.0002 & 0.0064 & 0.0271 & **0.0297** & 9.59 \\\\  & NDCG@20 & 0.0121 & 0.0113 & 0.0168 & 0.0114 & 0.0158 & 0.0107 & 0.0022 & 0.0148 & 0.0636 & **0.0660** & 3.77 \\\\ \\cline{2-13}  & NDCG@40 & 0.0203 & 0.0258 & 0.0283 & 0.0196 & 0.0269 & 0.0185 & 0.0037 & 0.0232 & 0.0828 & **0.0904** & 9.84 \\\\ \\cline{2-13} -SCS-2 & Recall@5 & 0.0073 & 0.0071 & 0.0092 & 0.0069 & 0.0066 & 0.0088 & 0.0003 & 0.0098 & 0.0455 & **0.0504** & 10.77 \\\\  & Recall@20 & 0.0347 & 0.0329 & 0.0497 & 0.0303 & 0.0421 & 0.0311 & 0.0075 & 0.0386 & 0.1621 & **0.1788** & 10.30 \\\\  & Recall@40 & 0.0747 & 0.0706 & 0.1062 & 0.0701 & 0.0964 & 0.0688 & 0.0151 & 0.0822 & 0.2528 & **0.2974** & 17.64 \\\\ \\hline \\multirow{4}{*}{Sports} & NDCG@5 & 0.0055 & 0.0051 & 0.0032 & 0.0032 & 0.0059 & 0.0050 & 0.0000 & 0.0056 & 0.0358 & **0.0359** & 0.28 \\\\  & NDCG@20 & 0.0117 & 0.0112 & 0.0107 & 0.0108 & 0.0113 & 0.0110 & 0.0002 & 0.0138 & 0.0628 & **0.0692** & 10.19 \\\\ \\cline{1-1} \\cline{2-13}  & NDCG@40 & 0.0210 & 0.0195 & 0.0197 & 0.0209 & 0.0224 & 0.0192 & 0.0005 & 0.0229 & 0.0826 & **0.0924** & 11.86 \\\\ \\cline{1-1} \\cline{2-13} -SCS-3 & Recall@5 & 0.0073 & 0.0068 & 0.0056 & 0.0066 & 0.0102 & 0.0066 & 0.0000 & 0.0086 & 0.0565 & **0.0569** & 0.71 \\\\ \\cline{1-1} \\cline{2-13}  & Recall@20 & 0.0308 & 0.0298 & 0.0336 & 0.0345 & 0.0289 & 0.0268 & 0.0007 & 0.0367 & 0.1515 & **0.1758** & 16.04 \\\\ \\cline{1-1} \\cline{2-recognize, on the other hand, that cross-domain knowledge transferring may further improve ColdGPT\'s performance and leave the exploration to future work (Section 5).\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      'We study the effects of changing the task weights \\(\\mathbf{w}\\), task 3 data, regularization term weight \\(\\lambda\\), learning rate (lr), PLM, and loss function of ColdGPT. Table 4 and Figure 3 present the results. As summarized in Figure 3 (a) and (b), incorporating pre-training tasks 2 and 3 on top of task 1 helps. Comparing lines 1 - 14 and line 24 further shows that the three tasks are able to cooperate with each other during multi-task pre-training and perform the best with the weights set to [0.6, 0.2, 0.2]. In line 15, we replace the review terms in task 3 with noun phrases, i.e., we remove sentiments from the original review terms. Line 15 performs less well than line 24, indicating that the sentiments of the users help to better model item correlations and should be considered when exploring the subjective review texts. Comparing the results in lines 16 - 21 and line 24 indicates that setting \\(\\lambda\\) and lr to 0.6 and 0.005, respectively, provides the best performance. Comparing line 22 to line 24 shows that ColdGPT works despite the choice of PLM and ColdGPT still presents competitive results after changing its PLM from SBERT to BERT. Changing the pre-training task losses lead to a considerable drop in performance, as shown in line 23. Instead of adopting the proposed alignment-based loss terms, line 23 adopts InfoNCE loss, which is commonly used by graph pre-training methods [20; 41] for tasks 1 and 3. For task 2, line 23 follows the original design of BERT4Rec [47] and adopts cross-entropy loss. Line 23 also adopts Euclidean norms other than uniformity-based regularization terms. These changes cause the pre-training tasks to conflict with each other and exploding gradients are observed. As shown in Figure 3 (c) and (d), the loss of line 23 fluctuates severely between 0 and 30,000, while the loss of line 12 steadily drops to around -5. This shows that the unified, alignment-and-uniformity-based pre-training task losses of ColdGPT effectively facilitate the positive transfer. Also, note that line 23 performs on par with while the rest lines outperform the best baseline, i.e., UnisRec (Table 2). For example, lines 2, 4, 6, and 10 show that ColdGPT outperforms UnisRec when using the same data sources (i.e., without\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c c c c c} \\hline \\hline  & Factor & Value & NDCG@5 & NDCG@20 & NDCG@40 & Recall@5 & Recall@20 & Recall@40 \\\\ \\hline\n' +
      '1 & [1, 0.0, 0.0] & 0.0227 & 0.0454 & 0.0721 & 0.0312 & 0.1015 & 0.2102 \\\\\n' +
      '2 & [0.8, 0.2, 0.0] & 0.0242 & 0.0546 & 0.0796 & 0.0342 & 0.1223 & 0.2341 \\\\\n' +
      '3 & [0.8, 0.0, 0.2] & 0.0258 & 0.0548 & 0.0819 & 0.0335 & 0.1242 & 0.2388 \\\\\n' +
      '4 & [0.6, 0.4, 0.0] & 0.0250 & 0.0542 & 0.0805 & 0.0328 & 0.1241 & 0.2366 \\\\\n' +
      '5 & [0.6, 0.0, 0.4] & 0.0263 & 0.0538 & 0.0800 & 0.0353 & 0.1223 & 0.2331 \\\\\n' +
      '6 & [0.4, 0.6, 0.0] & 0.0246 & 0.0503 & 0.0810 & 0.0328 & 0.1143 & 0.2345 \\\\\n' +
      '7 & task weights (\\(\\mathbf{w}\\)) & [0.4, 0.4, 0.2] & 0.0257 & 0.0521 & 0.0768 & 0.0336 & 0.1171 & 0.2225 \\\\\n' +
      '8 & [0.4, 0.2, 0.4] & 0.0251 & 0.0522 & 0.0797 & 0.0318 & 0.1166 & 0.2367 \\\\\n' +
      '9 & [0.4, 0.0, 0.6] & 0.0261 & 0.0538 & 0.0793 & 0.0339 & 0.1231 & 0.2302 \\\\\n' +
      '10 & [0.2, 0.8, 0.0] & 0.0230 & 0.0458 & 0.0710 & 0.0311 & 0.1039 & 0.2101 \\\\\n' +
      '11 & [0.2, 0.6, 0.2] & 0.0228 & 0.0462 & 0.0714 & 0.0307 & 0.1028 & 0.2096 \\\\\n' +
      '12 & [0.2, 0.4, 0.4] & 0.0255 & 0.0523 & 0.0784 & 0.0347 & 0.1192 & 0.2301 \\\\\n' +
      '13 & [0.2, 0.2, 0.6] & 0.0213 & 0.0489 & 0.0741 & 0.0278 & 0.1158 & 0.2228 \\\\\n' +
      '14 & [0.2, 0.0, 0.8] & 0.0251 & 0.0481 & 0.0738 & 0.0335 & 0.1065 & 0.2166 \\\\ \\hline\n' +
      '15 & task 3 & w/o sentiments & 0.0217 & 0.0510 & 0.0790 & 0.0293 & 0.1227 & 0.2340 \\\\ \\hline\n' +
      '16 & & 0.2 & 0.0225 & 0.0498 & 0.0639 & 0.0317 & 0.0991 & 0.2005 \\\\\n' +
      '17 & regularization weight (\\(\\lambda\\)) & 0.4 & 0.0258 & 0.0505 & 0.0780 & 0.0370 & 0.1152 & 0.2327 \\\\\n' +
      '18 & & 0.8 & 0.0262 & 0.0558 & 0.0853 & 0.0368 & 0.1208 & 0.2274 \\\\ \\hline\n' +
      '19 & & 0.010 & 0.0221 & 0.0495 & 0.0777 & 0.0315 & 0.1194 & 0.2291 \\\\\n' +
      '20 & learning rate (lr) & 0.008 & 0.0255 & 0.0553 & 0.0851 & 0.0362 & 0.1269 & 0.2341 \\\\\n' +
      '21 & & 0.003 & 0.0246 & 0.0516 & 0.0812 & **0.0371** & 0.1232 & 0.2300 \\\\ \\hline\n' +
      '22 & PLM & BERT & 0.0255 & **0.0559** & 0.0862 & 0.0370 & 0.1277 & **0.2457** \\\\ \\hline\n' +
      '23 & loss function & InfoNCE + CE & 0.0146 & 0.0355 & 0.0608 & 0.0215 & 0.0990 & 0.1941 \\\\ \\hline\n' +
      '24 & Default setting (Section 3.13) & **0.0265** & 0.0556 & **0.0863** & **0.0371** & **0.1282** & 0.2354 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4. Ablation study of ColdGPT on the Yelp-SC5 dataset. In line 24, The results under the default setting (\\(\\mathbf{w}\\)=[0.6, 0.2, 0.2], \\(\\lambda\\) =0.6, \\(lr\\) =0.005, PLM = SBERT, loss function = Equation 12) are copied from Table 2. The best results are boldfaced and the second-best ones are underlined.\n' +
      '\n' +
      'incorporating the review texts) as the later. Line 1 further shows that ColdGPT outperforms UnisRec using even less data (i.e., merely relying on the item contents). This again verifies ColdGPT\'s effectiveness.\n' +
      '\n' +
      '### Item-attribute Graph Visualization\n' +
      '\n' +
      'We visualize the Pearson correlation coefficients between the pair-wise distance (measured by calculating the cosine similarity between the item embeddings learned via ColdGPT) and attributes\' similarity of the item pairs, as shown in Figure 4. We visualize all item pairs, SCS-existing item pairs, and SCS-SCS item pairs in Figure 4 (a), (b) and (c), respectively. We can observe the following. 1) For all item pairs, the Pearson correlation is positive and significant at the 0.01 level. This shows that the item embeddings learned by ColdGPT capture the correlations between the items - for the two items in each pair, the more attributes they share, the closer their embeddings are. 2) For SCS-existing item pairs, the Pearson correlation is also positive and significant at the 0.01 level, indicating that ColdGPT effectively transfers attribute-related information to the new items and connects the new items to the old items. 3) For SCS-SCS item pairs, the Pearson correlation is also positive and significant at the 0.01 level. It shows even if two items are both strictly cold start items, ColdGPT can generate positively correlated embeddings with respect to their shared attributes. 4) The order of Pearson coefficient between the item embedding similarity and the attribute similarity on all item pairs (0.3973) > SCS-existing item pairs (0.3541) > SCS-SCS pairs (0.3285). This is justifiable since all the existing items are explicitly trained to align with the attributes, which makes the Pearson coefficients on all item pairs higher. SCS items, unseen during pre-training, are not directly aligned with the attributes, which makes the coefficient on SCS-existing items pairs relatively lower. It will be even lower if two items are both unseen during the pre-training stage.\n' +
      '\n' +
      'Figure 4. Pearson correlation coefficients between the ColdGPT embedding cosine similarity and the attribute Jaccard similarity of the item pairs.\n' +
      '\n' +
      'Figure 3. The changes in NDCG@5 when increasing the weights of task 2 (a) and task 3 (b), the loss values of lines 23 (c) and 24 (d) in Table 4. In (a), we fix the weight of task 3 to 0.0 and gradually increase the weight of task 2. Similarly, in (b), we gradually increase the weight of task 3. ‘default’ refers to the default setting (line 24 in Table 4). In (c) and (d), the x-axis shows the training epochs.\n' +
      '\n' +
      '## 4. Related Work\n' +
      '\n' +
      '**Cold-start Recommenders.** Recommendation methods suffer from the out-of-distribution problem in cold-start scenarios where the well-established, dominating ID-based approaches deteriorate (Zhou et al., 2018; Zhang et al., 2019). The existing cold-start recommenders mainly aim at the few-shot cold-start setting and require a few interactions of the test items for fine-tuning or adaptation. Methods based on meta-learning (Zhou et al., 2018; Zhang et al., 2019; Zhang et al., 2019; Zhang et al., 2019; Zhang et al., 2019; Zhang et al., 2019; Zhang et al., 2019; Zhang et al., 2020; Zhang et al., 2021) are commonly seen. To address the SCS setting, in which the interactions of the test items are completely inaccessible, content-based methods such as wide&deep (Chen et al., 2018), Factorization Machines (FM) (Zhou et al., 2018) and its variants (Zhou et al., 2019) can be applied, as they rely merely on the contents (auxiliary user-profiles and item descriptions) of the users and items for making recommendations. Specific types of cold-start recommenders are also able to work in the SCS setting. These methods typically reconstruct the preference (Collaborative Filtering-based) embeddings of the users and items from their content (auxiliary feature-based) embeddings. E.g., (Zhou et al., 2019) applies dropout to input mini-batches for such reconstruction. (Zhou et al., 2019) adopts a randomized training mechanism to facilitate the effectiveness of the reconstruction. (Chen et al., 2018) formalizes the reconstruction as a regression task and minimizes the MSE loss between the two types of embeddings. IDCF-HY, the SCS extension of (Zhou et al., 2019), uses meta-learning and relies on contents other than interactions for adaptation. (Zhou et al., 2019), on the other hand, leverages contrastive learning. (Zhou et al., 2019), (Zhou et al., 2020), and (Zhou et al., 2021) instead relies on Autoencoder or Variational Autoencoder (VAE). (Zhou et al., 2020) further recognizes that the distributions of the warm and the cold domains are different and leverages Stein Path for alignment. Thanks to the advancement in NLP, pre-training-based methods that adapt PLMs (Chen et al., 2018; Zhang et al., 2020) for recommendation are attracting increasing attention. Specifically, UniSRec (Li et al., 2020) leverages PLM (Chen et al., 2018) as an item encoder and extends a popular ID-based sequential recommender (Zhou et al., 2020) to work in both few-shot and strict cold-start scenarios. P5 (Chen et al., 2020) adopts a PLM backbone (Zhou et al., 2020) and distorts various forms of data, including historical purchases and reviews, into natural language sequences, to further pre-train the backbone (Note P5 makes recommendations only to users seen during pre-training). ColdGPT differs from the existing cold start recommenders in that: 1) It considers more information. In addition to item contents, ColdGPT further incorporates sequential item correlations lie in the users\' purchase histories and subjective item attributes contained in the review texts. 2) It exploits the various information sources more effectively. ColdGPT models the fine-grained attributes rather than the noisy raw fields. It designs specific submodules and tasks to fully explore information sources of different forms while adopting unified loss terms and a MTL framework to coordinate them.\n' +
      '\n' +
      '**Graph-based Recommenders.** Graphs are expressive in capturing higher-level structural information, thus are widely adopted by recommendation studies. Recent graph-based recommenders (Zhou et al., 2019; Zhang et al., 2020) typically construct user-item bipartite graphs and then capture user-item correlations by propagating their embeddings using GNNs (Zhou et al., 2019; Zhang et al., 2020). Some studies further enrich the user-item graphs with knowledge graphs (KGs) (Chen et al., 2018; Zhang et al., 2019; Zhang et al., 2020; Zhang et al., 2021) and social relations (Zhou et al., 2019; Zhang et al., 2020). There also exist recommenders (Zhou et al., 2019; Zhang et al., 2020; Zhang et al., 2021; Zhang et al., 2021) that leverage graph pre-training to enhance the learning on the user-item graphs. (Chen et al., 2020) improves the zero-shot item recommendation via pre-training a product knowledge graph. (Zhang et al., 2020) pre-trains user and item graphs to involve side information. (Zhou et al., 2020) pre-trains an implicit user-item graph to decide meta latent for future matching. (Zhou et al., 2020) contrasts different views of user-item graphs with the aim of improving robustness. Our proposed ColdGPT, unlike the previous studies, pre-trains a bipartite item-attribute graph under the motivation of leveraging the fine-grained attributes to bridge the gaps between the SCS and the existing items.\n' +
      '\n' +
      '**Graph Pre-training.** Graph pre-training transfers knowledge using easily accessible information such as the existing labels (Zhou et al., 2019) or self-supervision signals (Zhou et al., 2019; Zhang et al., 2020; Zhang et al., 2021) for more informative graph representation learning. The knowledge transfer can happen within the same domain (Zhou et al., 2019; Zhang et al., 2020; Zhang et al., 2021; Zhang et al., 2021; Zhang et al., 2021; Zhang et al., 2021; Zhang et al., 2021) or across domains (Zhang et al., 2021; Zhang et al., 2021). Early skip-gram based methods (Zhou et al., 2019; Zhang et al., 2021), inspired by Word2vec (Zhou et al., 2020), capture neighborhood similarities. Later methods transfer higher-ordergraph structural and semantic knowledge by pre-training GNNs. They design supervised pre-training tasks such as node type prediction (Krizhevsky et al., 2014), unsupervised tasks such as centrality score ranking (Krizhevsky et al., 2014), and self-supervised tasks such as masked edge prediction (Krizhevsky et al., 2014) and subgraph comparison (Krizhevsky et al., 2014). ColdGPT transfers knowledge from various data sources related to the target domain. We note, nonetheless, that cross-domain knowledge transfer may further benefit the target domain and leave the exploration to future work. Note that studies that pre-train GNNs typically fine-tune them concerning the gaps between the pre-training and the downstream tasks. E.g., (Krizhevsky et al., 2014) learns how to fine-tune during pre-training while (Krizhevsky et al., 2014) customizes prompt tuning (Krizhevsky et al., 2014) for GNNs. ColdGPT, on the other hand, does not involve fine-tuning as it addresses the SCS scenario that assumes no downstream task labels are available for fine-tuning. Also note that studies that leverage graph pre-training for the recommendation has been addressed in the last subsection.\n' +
      '\n' +
      '**Multi-task Learning.** Multi-Task Learning (MTL) (Beng et al., 2015) enables knowledge transfer between different tasks and modalities and thus has been broadly adopted in pre-training deep models (Krizhevsky et al., 2014). MTL studies strive to address the commonly observed negative transfer issue (tasks interfere with each other in harmful manners) by removing the conflicts between the per-task gradients (Krizhevsky et al., 2014; Krizhevsky et al., 2014; Krizhevsky et al., 2014), reweighting task losses in deterministic manners (Beng et al., 2015), and conducting multi-objective optimization (Krizhevsky et al., 2016). However, recent studies (Krizhevsky et al., 2016; Krizhevsky et al., 2016) show that simply optimizing a weighted average of the task losses, with careful regularization and tuning of the scalarization weights and hyperparameters, performs on par with these ad-hoc methods while avoiding the added complexity and overhead. In other words, empirically inspecting the combinations of the hyperparameters is preferable to applying ad-hoc MTL methods. We, therefore, carefully tune the task weights, regularization term weight, and learning rate of ColdGPT (Section 3.3). Note we also extend the idea of alignment-and-uniformity to MTL and design unified loss terms to eliminate the discrepancies between the pre-training tasks caused by the heterogeneous submodules (we empirically verify the effectiveness of our approach in Section) 3.3.\n' +
      '\n' +
      '## 5. Conclusion and Future Work\n' +
      '\n' +
      'We address SCS item recommendation using fine-grained item attributes. We propose a multi-task pre-training framework, ColdGPT, to transfer knowledge from various available data sources into an item-attribute graph. To fully explore the data sources, ColdGPT designs submodules according to their natural forms. To facilitate the positive transfer, ColdGPT coordinates the multiple pre-training tasks via unified alignment-and-uniformity losses. The pre-trained item-attribute graph enables an easy acquisition of informative SCS item embeddings. We carefully process three public datasets to guarantee the SCS setting for evaluation. Extensive experiments show that ColdGPT consistently outperforms the existing cold-start recommenders by large margins and even surpasses models that are pre-trained on 75-224 times more data on two out of four datasets. Qualitative studies show ColdGPT effectively connects the SCS items to the existing ones via their shared attributes. In the future, we plan to extend ColdGPT for cross-domain SCS item recommendation. We note that pre-training on large-scale, cross-domain datasets such as Amazon may further improve ColdGPT\'s performance. Another future direction would be extending ColdGPT for the few-shot cold start and non-cold start scenarios, where a few or abundant interactions of the test items are accessible. Ideally, the extended ColdGPT would incorporate these available interactions to enrich the pre-trained item-attribute graph.\n' +
      '\n' +
      '## 6. Acknowledgments\n' +
      '\n' +
      'This paper was supported by the National Key R&D Program of China through grant 2022YFB3104703, NSFC through grant 62002007, Natural Science Foundation of Beijing Municipality through grant 4222030, and S&T Program of Hebei through grant 21340301D. Philip S. Yu was supported by NSF under grants III-1763325, III1909323, III-2106758, and SaTC-1930941.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '* Qian et al. (2020) Tieyun Qian, Yile Liang, et al. 2020. Attribute graph neural networks for strict cold start recommendation. _IEEE TKDE_ (2020).\n' +
      '* Qiu et al. (2020) Jiezhong Qiu, Qibin Chen, et al. 2020. Gcc: Graph contrastive coding for graph neural network pre-training. In _Proc. SIGKDD_.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._ 21, 140 (2020), 1-67.\n' +
      '* Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In _Proc. EMNLP_.\n' +
      '* Rendle (2010) Steffen Rendle. 2010. Factorization machines. In _Proc. ICDM_. IEEE.\n' +
      '* Rendle et al. (2009) Steffen Rendle, Christoph Freudenthaler, et al. 2009. BPR: Bayesian personalized ranking from implicit feedback. In _Proc. UAI_.\n' +
      '* Sener and Koltun (2018) Ozan Sener and Vladlen Koltun. 2018. Multi-task learning as multi-objective optimization. _Proc. NeurIPS_.\n' +
      '* Sun et al. (2019) Fei Sun, Jun Liu, et al. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In _Proc. CIKM_.\n' +
      '* Sun et al. (2022) Mingchen Sun, Kaixiong Zhou, et al. 2022. Gppt: Graph pre-training and prompt tuning to generalize graph neural networks. In _Proc. SIGKDD_.\n' +
      '* Tang et al. (2015) Jian Tang, Meng Qu, et al. 2015. Line: Large-scale information network embedding. In _Proc. TheWebConf_.\n' +
      '* Vamsi et al. (2022) Aribandi Vamsi, Yi Tay, et al. 2022. Ext5: Towards extreme multi-task scaling for transfer learning. _Proc. ICLR_.\n' +
      '* Vartak et al. (2017) Manasi Vartak, Arvind Thiagarajan, et al. 2017. A meta-learning perspective on cold-start recommendations for items. In _Proc. NeurIPS_.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, et al. 2017. Attention is all you need. _Proc. NeurIPS_.\n' +
      '* Velickovic et al. (2018) Petar Velickovic, Guillem Cucurull, et al. 2018. Graph attention networks. In _Proc. ICLR_.\n' +
      '* Volkovs et al. (2017) Maksims Volkovs, Guangwei Yu, and Tomi Poutanen. 2017. Dropoutnet: Addressing cold start in recommender systems. In _Proc. NeurIPS_.\n' +
      '* Wang et al. (2021) Chen Wang, Yueqing Liang, Zhiwei Liu, Tao Zhang, and S Yu Philip. 2021. Pre-training graph neural network for cross domain recommendation. In _2021 IEEE Third International Conference on Cognitive Machine Intelligence (CogM)_. IEEE, 140-145.\n' +
      '* Wang et al. (2022) Chenyang Wang, Yuanqing Yu, et al. 2022. Towards Representation Alignment and Uniformity in Collaborative Filtering. In _Proc. SIGKDD_.\n' +
      '* Wang et al. (2019) Hongwei Wang, Miao Zhao, et al. 2019. Knowledge graph convolutional networks for recommender systems. In _Proc. TheWebConf_.\n' +
      '* Wang et al. (2022) Shen Wang, Liangwei Yang, et al. 2022. MetaKRec: Collaborative Meta-Knowledge Enhanced Recommender System. In _Proc. BigData_. IEEE.\n' +
      '* Wang and Isola (2020) Tongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In _Proc. ICML_.\n' +
      '* Wang et al. (2019) Xiang Wang, Xiangnan He, et al. 2019. Kgat: Knowledge graph attention network for recommendation. In _Proc. SIGKDD_.\n' +
      '* Wang et al. (2019) Xiang Wang, Xiangnan He, et al. 2019. Neural graph collaborative filtering. In _Proc. SIGIR_.\n' +
      '* Wei et al. (2021) Yinwei Wei, Xiang Wang, et al. 2021. Contrastive learning for cold-start recommendation. In _Proc. Multimedia_.\n' +
      '* Wu et al. (2021) Jiancan Wu, Xiang Wang, et al. 2021. Self-supervised graph learning for recommendation. In _Proc. SIGIR_.\n' +
      '* Wu et al. (2021) Qitian Wu, Hengrui Zhang, et al. 2021. Towards open-world recommendation: An inductive model-based collaborative filtering approach. In _Proc. ICML_.\n' +
      '* Xiao et al. (2017) Jun Xiao, Hao Ye, et al. 2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. In _Proc. IJCAI_.\n' +
      '* Xin et al. (2022) Derrick Xin, Behrooz Ghorbani, et al. 2022. Do Current Multi-Task Optimization Methods in Deep Learning Even Help? _Proc. NeurIPS_.\n' +
      '* Yang et al. (2021) Liangwei Yang, Zhiwei Liu, et al. 2021. Consisrec: Enhancing gpn for social recommendation via consistent neighbor aggregation. In _Proc. SIGIR_.\n' +
      '* You et al. (2022) Chenyu You, Weicheng Dai, et al. 2022. Mine your own anatomy: Revisiting medical image segmentation with extremely limited labels. _arXiv preprint arXiv:2209.13476_ (2022).\n' +
      '* You et al. (2023) Chenyu You, Weicheng Dai, et al. 2023. ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. _arXiv preprint arXiv:2304.02689_ (2023).\n' +
      '* You et al. (2023) Chenyu You, Weicheng Dai, et al. 2023. Bootstrapping semi-supervised medical image segmentation with anatomical-aware contrastive distillation. In _Proc. IPML_. Springer.\n' +
      '* You et al. (2023) Chenyu You, Weicheng Dai, and other. 2023. Rethinking semi-supervised medical image segmentation: A variance-reduction perspective. _arXiv preprint arXiv:2302.01735_ (2023).\n' +
      '* Yu et al. (2020) Tianhe Yu, Saurabh Kumar, et al. 2020. Gradient surgery for multi-task learning. _Proc. NeurIPS_.\n' +
      '* Yuan et al. (2023) Zheng Yuan, Fajie Yuan, et al. 2023. Where to Go Next for Recommender Systems? ID- vs. Modality-based recommender models revisied. In _Proc. SIGIR_.\n' +
      '* Zhang et al. (2014) Yongfeng Zhang, Haochen Zhang, et al. 2014. Do users rate or review? Boost phrase-level sentiment labeling with review-level sentiment classification. In _Proc. SIGIR_.\n' +
      '* Zhao et al. (2022) Xu Zhao, Yi Ren, et al. 2022. Improving Item Cold-start Recommendation via Model-agnostic Conditional Variational Autoencoder. In _Proc. SIGIR_.\n' +
      '* Zheng et al. (2022) Wenqing Zheng, Edward W Huang, et al. 2022. Cold brew: Distilling graph node representations with incomplete or missing neighborhoods. In _Proc. ICLR_.\n' +
      '* Zhu et al. (2021) Yongchun Zhu, Ruobing Xie, et al. 2021. Learning to warm up cold item embeddings for cold-start recommendation with meta scaling and shifting networks. In _Proc. SIGIR_.\n' +
      '* Zhu et al. (2020) Ziwei Zhu, Shahin Sefati, et al. 2020. Recommendation for new users and new items via randomized training and mixture-of-experts transformation. In _Proc. SIGIR_.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
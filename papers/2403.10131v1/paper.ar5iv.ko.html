<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.10131] RAFT: Adapting Language Model to Domain Specific RAG</title><meta property="og:description" content="Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm.
When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., ti…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RAFT: Adapting Language Model to Domain Specific RAG">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="RAFT: Adapting Language Model to Domain Specific RAG">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.10131">

<!--Generated on Fri Apr  5 14:06:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">RAFT: Adapting Language Model to Domain Specific RAG</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianjun Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shishir G. Patil
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Naman Jain
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sheng Shen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matei Zaharia
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ion Stoica
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joseph E. Gonzalez
</span></span>
</div>

<h1 class="ltx_title ltx_title_document">RAFT: Adapting Language Model to Domain Specific RAG</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianjun Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shishir G. Patil
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Naman Jain
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sheng Shen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matei Zaharia
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ion Stoica
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joseph E. Gonzalez
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">텍스트 데이터의 대규모 코퍼라에 대한 대규모 언어 모델(LLM) 사전 훈련은 이제 표준 패러다임이다. 이러한 LLM을 많은 다운스트림 애플리케이션에 사용할 때 RAG 기반 촉진 또는 미세 조정을 통해 사전 학습된 모델에 새로운 지식(예: 시간 중요 뉴스 또는 개인 도메인 지식)을 추가로 베이크하는 것이 일반적이다. 그러나 이러한 새로운 지식을 얻기 위한 모델의 최적 방법론은 여전히 미해결 문제로 남아 있다. 본 논문에서는 "오픈 북" 인 도메인 환경에서 모델의 질문에 대한 답변 능력을 향상시키는 훈련 레시피인 RFT(Retrieval Augmented Fine Tuning)를 제시한다. RAFT에서 질문과 검색된 문서 집합이 주어지면, 우리는 질문에 답하는 데 도움이 되지 않는 문서를 무시하도록 모델을 훈련시키고, 이를 우리는 문서 주의를 분산시킨다. RAFT는 질문에 답하는 데 도움이 될 관련 문서의 올바른 순서를 그대로 인용함으로써 이를 달성한다. 이것은 RAFT의 사고 연쇄형 반응과 결합되어 모델의 추론 능력을 향상시키는 데 도움이 된다. 도메인별 RAG에서 RAFT는 PubMed, HotpotQA 및 고릴라 데이터 세트에 걸쳐 모델의 성능을 일관되게 개선하여 사전 훈련된 LLM을 도메인 내 RAG로 개선하기 위한 사후 훈련 레시피를 제시한다. RAFT의 코드와 데모는 <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ShishirPatil/gorilla" target="_blank" title="">https://github.com/ShishirPatil/gorilla</a>에서 open-sourced이다.</p>
<figure id="S0.F1" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.10131/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="83" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.3.1.1" style="font-size:90%;">Figure 1</span>:</span><span class="ltx_text ltx_font_bold" id="S0.F1.4.2" style="font-size:90%;">How best to prepare for a Exam? <span class="ltx_text ltx_font_medium" id="S0.F1.4.2.1">(a) Fine-tuning 기반 접근법은 입력 문서를 직접 "메모리"하거나 문서를 참조하지 않고 응답 연습 QA를 통해 "studying"을 구현합니다. (b) 대안적으로, 인컨텍스트 검색 방법들은 고정된 도메인에서 제공되는 학습 기회를 활용하지 못하고, 공부하지 않고 오픈북 시험을 보는 것과 동등하다. 이러한 접근법은 도메인 내 학습을 활용하지만 오픈북 테스트를 준비하지 못한다. 대조적으로, 우리의 접근법 (c) RAFT는 시뮬레이션된 불완전한 검색 설정에서 문서를 참조하면서 질문-답변 쌍으로 미세 조정을 활용함으로써 오픈 북 시험 설정을 효과적으로 준비한다. </span></span></figcaption>
</figure>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\minted@def@optcl</span>
<p class="ltx_p" id="p1.2">envname-P envname#1</p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_typewriter">tianjunz@berkeley.edu, shishirpatil@berkeley.edu</span></p>
<p id="id1.p1.2" class="ltx_p ltx_align_center">UC Berkeley</p>
</div>
</div>
<div id="p3" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">광범위한 양의 공개 데이터에 대해 훈련된 대용량 언어 모델 LLM은 광범위한 일반 지식 추론 작업  <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib5" title="">2020</a>; Wei et al., <a class="ltx_ref" href="#bib.bib52" title="">2022</a>)</cite>에서 상당한 발전을 이루었다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">그러나, 점점 더 많은 LLMs들이 특정 소프트웨어 프레임워크들에 대한 코드 완성에서부터 특정 문서 컬렉션들(예를 들어, 법적 또는 의학적 문서들)에 대한 질의 응답에 이르는 태스크들을 지원하기 위해 전문 도메인들에서 채용되고 있다. 이러한 설정에서 일반적인 지식 추론은 덜 중요하지만 대신 주어진 문서 세트를 기반으로 정확도를 최대화하는 것이 주요 목표이다. 실제로, LLM을 특수 도메인(예를 들어, 트레이닝 컷오프 후에 구성된 최근 뉴스, 기업 사설 문서 또는 프로그램 리소스)에 적응시키는 것은 많은 새로운 애플리케이션들<cite class="ltx_cite ltx_citemacro_citep">(Vu et al., <a class="ltx_ref" href="#bib.bib49" title="">2023</a>; Lazaridou et al., <a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite>에 필수적이며, 이 작업의 초점이다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">이 논문은 다음 질문 – <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">전문 도메인에서 사전 훈련된 LLMs for Retrieval Augmented Generation (RAG)를 적응시키는 방법에 대해 연구합니다. </em></p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">LLM을 특화 도메인에 적용할 때, 우리는 검색-증강 생성(RAG)을 통한 상황 내 학습과 감독 미세 조정이라는 두 가지 후보를 고려한다. RAG 기반 메서드는 LLM이 질문에 답할 때 문서를 참조할 수 있도록 합니다. 그러나 이러한 방법들은 고정된 도메인 설정과 테스트 문서에 대한 조기 접근에 의해 제공되는 학습 기회를 활용하지 못한다. 또는 감독 미세 조정은 문서에서 보다 일반적인 패턴을 배우고 최종 작업 및 사용자 선호도에 더 잘 정렬할 수 있는 기회를 제공한다. 그러나 기존의 미세 조정 기반 방법은 테스트 시 문서를 활용하지 못하거나(RAG를 통합하지 않음) 학습 중 검색 프로세스의 불완전성을 설명하지 못한다.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">우리는 공개 시험에 비유할 수 있다. 기존의 인맥 검색 방법은 공부를 하지 않고 오픈북 시험을 보는 것과 같다. 대안적으로, 기존의 미세-조정 기반 접근법들은 입력 문서들을 직접 "기억하기" <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al., <a class="ltx_ref" href="#bib.bib55" title="">2023</a>)</cite>하거나 문서를 참조하지 않고 연습 질문들 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib51" title="">2022</a>)</cite>에 응답함으로써 "학습"을 구현한다. 이러한 접근법은 도메인 내 학습을 활용하지만 테스트 설정의 공개 문서 특성에 대비하지 못한다.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p" id="S1.p6.1">본 논문에서는 지도 미세 조정(supervised fine-tuning, SFT)과 검색 증강 생성( retrieval augmented generation, RAG)을 결합하는 방법에 대해 연구한다. 우리는 새로운 적응 전략인 검색 증강 미세 조정(RAFT)을 제안한다. RAFT는 도메인 지식을 통합하는 동시에 도메인 내 RAG 성능을 개선하기 위해 LLM을 미세 조정하는 문제를 구체적으로 다룬다. RAFT는 모델이 미세 조정을 통해 도메인 특정 지식을 학습할 수 있도록 할 뿐만 아니라 부정확한 검색에 대한 견고성을 보장하는 것을 목표로 한다. 이것은 제기된 질문(즉시), 검색된 도메인 특정 문서 및 적절한 답변 간의 역학을 이해하도록 모델을 훈련함으로써 달성된다. 다시 우리의 비유로 돌아가서, 우리의 접근법은 관련되고 관련이 없는 검색 문서를 인식하여 공개 도서 시험을 공부하는 것과 유사하다.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p class="ltx_p" id="S1.p7.2">RAFT에서는 문서(들)(D*)에서 질문(Q)에 답하도록 모델을 훈련하여 답변(A*)을 생성하는데, 여기서 A*는 연쇄-of-thought <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib52" title="">2022</a>; Anthropic, <a class="ltx_ref" href="#bib.bib2" title="">2023</a>)</cite>를 포함하고, 분산기 문서가 있는 경우(<math alttext="D_{k}" class="ltx_Math" display="inline" id="S1.p7.1.m1.1"><semantics id="S1.p7.1.m1.1a"><msub id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml"><mi id="S1.p7.1.m1.1.1.2" xref="S1.p7.1.m1.1.1.2.cmml">D</mi><mi id="S1.p7.1.m1.1.1.3" xref="S1.p7.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><apply id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p7.1.m1.1.1.1.cmml" xref="S1.p7.1.m1.1.1">subscript</csymbol><ci id="S1.p7.1.m1.1.1.2.cmml" xref="S1.p7.1.m1.1.1.2">𝐷</ci><ci id="S1.p7.1.m1.1.1.3.cmml" xref="S1.p7.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">D_{k}</annotation></semantics></math>)에 학습한다. 방법론을 <a class="ltx_ref" href="#S3" title="3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">3</span></a> 절에서 자세히 설명하고, <a class="ltx_ref" href="#S5" title="5 RAFT Generalizes to Top-K RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">5</span></a> 절에서 열차 및 시험 시간에 주의 분산기 문서 수(<math alttext="k" class="ltx_Math" display="inline" id="S1.p7.2.m2.1"><semantics id="S1.p7.2.m2.1a"><mi id="S1.p7.2.m2.1.1" xref="S1.p7.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p7.2.m2.1b"><ci id="S1.p7.2.m2.1.1.cmml" xref="S1.p7.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.2.m2.1c">k</annotation></semantics></math>)에 대한 민감도를 분석한다. RAFT는 PubMed <cite class="ltx_cite ltx_citemacro_citep">(Dernoncourt &amp; Lee, <a class="ltx_ref" href="#bib.bib10" title="">2017</a>)</cite>, HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib57" title="">2018</a>)</cite> 및 HuggingFace Hub, Torch Hub 및 Tensorflow Hub 고릴라 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(Patil et al., <a class="ltx_ref" href="#bib.bib38" title="">2023</a>)</cite>에 걸쳐 RAG 유무에 관계없이 Supervised-finetuning을 일관되게 능가하며 도메인 내 RAG에 대해 사전 훈련된 LLM을 개선하기 위한 새롭고 간단한 기술을 제시한다.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>LLMs for Open-Book Exam</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p" id="S2.p1.1">우리의 목표를 더 잘 이해하기 위해, 우리는 시험을 준비하는 실제 환경에서 LLM을 훈련하는 것의 비유를 확장한다.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Closed-Book Exam</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">비공개 도서 시험은 종종 LLM이 시험 중에 질문에 답하기 위한 추가 문서 또는 참조에 액세스할 수 없는 시나리오를 지칭한다. LLM의 경우, 이는 예를 들어 LLM을 챗봇으로 사용하는 시나리오와 동일하다. 이 시나리오에서 LLM은 사전 훈련 및 감독된 미세 조정 중에 구워진 지식에서 파생되어 프롬프트에 응답한다.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Open Book Exam</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">대조적으로, 우리는 오픈 북 시험 설정을 LLM이 외부 정보 소스(예를 들어, 웹사이트 또는 책 챕터)를 참조할 수 있는 시나리오에 비유한다. 이러한 시나리오에서 일반적으로 LLM은 프롬프트에 추가된 'k' 문서(또는 문서의 특정 세그먼트)를 검색하는 검색기와 페어링됩니다. LLM이 "새로운 지식"에 대한 접근 권한을 얻는 것은 검색된 이러한 문서를 통해서만 가능하다. 결과적으로 범용 LLM으로 훈련되는 이러한 환경에서 LLM의 성능은 리트리버의 품질과 리트리버가 가장 관련된 정보를 얼마나 정확하게 식별할 수 있는지에 크게 달려 있다고 주장한다.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Domain Specific Open-Book Exam</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">본 논문에서는 도메인 특화 오픈북 시험이라고 불리는 일반 오픈북 시험보다 좁지만 점점 대중화되는 도메인에 초점을 맞추었다. 도메인 특정 공개 도서 시험에서 우리는 LLM이 테스트될 도메인 - 추론에 사용됨 - 을 알고 있다. LLM은 이 특정 도메인의 모든 정보를 사용하여 프롬프트에 응답할 수 있으며, 이 정보는 미세 조정되었습니다. 도메인 특정 예시의 예는 기업 문서, 최신 뉴스, 조직에 속하는 코드 리포지토리 등을 포함한다. 이러한 모든 시나리오에서 LLM은 질문에 응답하는 데 사용되며, 그 답변은 문서 모음(작은 실용적인 영역) 내에서 찾을 수 있다. 검색 기술 자체는 (정확도에 영향을 미칠 수 있지만) 메커니즘에 거의 또는 전혀 영향을 미치지 않는다. 본 논문에서는 이러한 도메인별 오픈북 설정과 미리 학습된 LLM을 이 도메인에 적응시키는 방법에 대해 주로 연구한다. 예를 들어, 다양한 수의 검색 문서 및 분산 장치에 보다 강건하게 하는 방법을 포함한다.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>RAFT</h2>

<figure id="S3.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.10131/assets/figures/RAFT.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.4.1.1" style="font-size:90%;">Figure 2</span>:</span><span class="ltx_text ltx_font_bold" id="S3.F2.5.2" style="font-size:90%;">Overview of our RAFT method. <span class="ltx_text ltx_font_medium" id="S3.F2.5.2.1"> 왼쪽 상단 그림은 LLMs를 <span class="ltx_text ltx_font_italic" id="S3.F2.5.2.1">reading</span> 솔루션을 암기와 읽기 모두의 혼합물인 리트리버 출력을 기반으로 모델이 훈련되는 표준 RAG 설정과 대조적으로 긍정 및 부정 문서 집합에 적응시키는 접근법을 묘사한다. 테스트 시간에 모든 방법은 컨텍스트에서 상위 k개의 검색된 문서와 함께 표준 RAG 설정을 따릅니다. </span></span></figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">이 섹션에서는 도메인 특정 오픈 북 테스트를 위한 LLM을 훈련하는 새로운 방법인 RAFT를 제시한다. 먼저 감독 미세 조정에 대한 고전적인 기술을 소개하고 실험을 통한 주요 테이크아웃을 소개한다. 그런 다음 일반적인 명령어 튜닝을 수정한 RAFT를 소개한다. 마지막으로, 우리는 후기 섹션에서 기대할 수 있는 실험의 개요를 제공한다.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Supervised Finetuning</span></p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p" id="S3.p3.3">질의 응답 데이터 세트에 대한 감독 미세 조정(SFT) 설정을 고려합니다. 제형은 질문(<math alttext="Q" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">Q</annotation></semantics></math>) 및 대응하는 답변(<math alttext="A" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">A</annotation></semantics></math>) 쌍의 세트가 도출되거나 이미 이용 가능한 데이터 세트(<math alttext="D" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">D</annotation></semantics></math>)로 구성된다. 클래식 SFT 설정에서 모델은 사전 훈련 중 또는 SFT 훈련 단계에서 얻은 지식을 기반으로 질문에 답하는 능력을 향상시키도록 훈련된다. 이렇게 훈련된 모델은 또한 검색 증강 생성(RAG) 설정으로 테스트 시간에 사용될 수 있으며, 여기서 프롬프트에 추가 문서가 도입되어 모델이 질문에 답하는 것을 도울 수 있다. 이는 다음과 같이 나타낼 수 있다:</p>
</div>
<div id="S3.p4" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.3" class="ltx_p">Train: <math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="{\mathbf{Q}}" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><mi id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">{\mathbf{Q}}</annotation></semantics></math> <math id="S3.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S3.I1.i1.p1.2.m2.1a"><mo stretchy="false" id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><ci id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">\to</annotation></semantics></math> <math id="S3.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="{\mathbf{A}}" display="inline"><semantics id="S3.I1.i1.p1.3.m3.1a"><mi id="S3.I1.i1.p1.3.m3.1.1" xref="S3.I1.i1.p1.3.m3.1.1.cmml">𝐀</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.1b"><ci id="S3.I1.i1.p1.3.m3.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1">𝐀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.1c">{\mathbf{A}}</annotation></semantics></math></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.3" class="ltx_p">0-shot Inference: <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="{\mathbf{Q}}" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">{\mathbf{Q}}</annotation></semantics></math> <math id="S3.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S3.I1.i2.p1.2.m2.1a"><mo stretchy="false" id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">\to</annotation></semantics></math> <math id="S3.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="{\mathbf{A}}" display="inline"><semantics id="S3.I1.i2.p1.3.m3.1a"><mi id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml">𝐀</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><ci id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">𝐀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">{\mathbf{A}}</annotation></semantics></math></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.4" class="ltx_p">RAG Inference: <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="{\mathbf{Q}}" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mi id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><ci id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">{\mathbf{Q}}</annotation></semantics></math> + <math id="S3.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="{\mathbf{D}}" display="inline"><semantics id="S3.I1.i3.p1.2.m2.1a"><mi id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml">𝐃</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><ci id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">𝐃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">{\mathbf{D}}</annotation></semantics></math> <math id="S3.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S3.I1.i3.p1.3.m3.1a"><mo stretchy="false" id="S3.I1.i3.p1.3.m3.1.1" xref="S3.I1.i3.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m3.1b"><ci id="S3.I1.i3.p1.3.m3.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m3.1c">\to</annotation></semantics></math> <math id="S3.I1.i3.p1.4.m4.1" class="ltx_Math" alttext="{\mathbf{A}}" display="inline"><semantics id="S3.I1.i3.p1.4.m4.1a"><mi id="S3.I1.i3.p1.4.m4.1.1" xref="S3.I1.i3.p1.4.m4.1.1.cmml">𝐀</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.4.m4.1b"><ci id="S3.I1.i3.p1.4.m4.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1">𝐀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.4.m4.1c">{\mathbf{A}}</annotation></semantics></math></p>
</div>
</li>
</ul>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">RAFT</span></p>
</div>
<div id="S3.p6" class="ltx_para">
<p class="ltx_p" id="S3.p6.13">검색 인식 미세 조정(Retrieval Aware Fine-Tuning, RAFT)은 도메인 내 RAG와 동등한 도메인 특정 오픈 북 설정에 대한 모델을 맞춤화하기 위해 미세 조정 데이터를 준비하는 새로운 레시피를 제시한다. RAFT에서, 우리는 각 데이터 포인트가 질문(<math alttext="Q" class="ltx_Math" display="inline" id="S3.p6.1.m1.1"><semantics id="S3.p6.1.m1.1a"><mi id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><ci id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">Q</annotation></semantics></math>), 문서 세트(<math alttext="D_{k}" class="ltx_Math" display="inline" id="S3.p6.2.m2.1"><semantics id="S3.p6.2.m2.1a"><msub id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml"><mi id="S3.p6.2.m2.1.1.2" xref="S3.p6.2.m2.1.1.2.cmml">D</mi><mi id="S3.p6.2.m2.1.1.3" xref="S3.p6.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><apply id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p6.2.m2.1.1.1.cmml" xref="S3.p6.2.m2.1.1">subscript</csymbol><ci id="S3.p6.2.m2.1.1.2.cmml" xref="S3.p6.2.m2.1.1.2">𝐷</ci><ci id="S3.p6.2.m2.1.1.3.cmml" xref="S3.p6.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">D_{k}</annotation></semantics></math>) 및 문서 중 하나(<math alttext="D^{*}" class="ltx_Math" display="inline" id="S3.p6.4.m4.1"><semantics id="S3.p6.4.m4.1a"><msup id="S3.p6.4.m4.1.1" xref="S3.p6.4.m4.1.1.cmml"><mi id="S3.p6.4.m4.1.1.2" xref="S3.p6.4.m4.1.1.2.cmml">D</mi><mo id="S3.p6.4.m4.1.1.3" xref="S3.p6.4.m4.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p6.4.m4.1b"><apply id="S3.p6.4.m4.1.1.cmml" xref="S3.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p6.4.m4.1.1.1.cmml" xref="S3.p6.4.m4.1.1">superscript</csymbol><ci id="S3.p6.4.m4.1.1.2.cmml" xref="S3.p6.4.m4.1.1.2">𝐷</ci><times id="S3.p6.4.m4.1.1.3.cmml" xref="S3.p6.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.4.m4.1c">D^{*}</annotation></semantics></math>)로부터 생성된 대응하는 체인 사상 스타일 답변(<math alttext="A^{*}" class="ltx_Math" display="inline" id="S3.p6.3.m3.1"><semantics id="S3.p6.3.m3.1a"><msup id="S3.p6.3.m3.1.1" xref="S3.p6.3.m3.1.1.cmml"><mi id="S3.p6.3.m3.1.1.2" xref="S3.p6.3.m3.1.1.2.cmml">A</mi><mo id="S3.p6.3.m3.1.1.3" xref="S3.p6.3.m3.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p6.3.m3.1b"><apply id="S3.p6.3.m3.1.1.cmml" xref="S3.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p6.3.m3.1.1.1.cmml" xref="S3.p6.3.m3.1.1">superscript</csymbol><ci id="S3.p6.3.m3.1.1.2.cmml" xref="S3.p6.3.m3.1.1.2">𝐴</ci><times id="S3.p6.3.m3.1.1.3.cmml" xref="S3.p6.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.3.m3.1c">A^{*}</annotation></semantics></math>)을 포함하도록 훈련 데이터를 준비한다. 우리는 두 가지 유형의 문서, 즉 질문에 대한 답을 추론할 수 있는 문서(<math alttext="D*" class="ltx_math_unparsed" display="inline" id="S3.p6.5.m5.1"><semantics id="S3.p6.5.m5.1a"><mrow id="S3.p6.5.m5.1b"><mi id="S3.p6.5.m5.1.1">D</mi><mo id="S3.p6.5.m5.1.2" lspace="0.222em">∗</mo></mrow><annotation encoding="application/x-tex" id="S3.p6.5.m5.1c">D*</annotation></semantics></math>)와 답과 관련된 정보를 포함하지 않는 ‘distractor’ 문서(<math alttext="D_{i}" class="ltx_Math" display="inline" id="S3.p6.6.m6.1"><semantics id="S3.p6.6.m6.1a"><msub id="S3.p6.6.m6.1.1" xref="S3.p6.6.m6.1.1.cmml"><mi id="S3.p6.6.m6.1.1.2" xref="S3.p6.6.m6.1.1.2.cmml">D</mi><mi id="S3.p6.6.m6.1.1.3" xref="S3.p6.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.6.m6.1b"><apply id="S3.p6.6.m6.1.1.cmml" xref="S3.p6.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p6.6.m6.1.1.1.cmml" xref="S3.p6.6.m6.1.1">subscript</csymbol><ci id="S3.p6.6.m6.1.1.2.cmml" xref="S3.p6.6.m6.1.1.2">𝐷</ci><ci id="S3.p6.6.m6.1.1.3.cmml" xref="S3.p6.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.6.m6.1c">D_{i}</annotation></semantics></math>)를 구별한다. 구현 세부 사항으로서, ‘oracle’ 문서는 단일 문서일 필요는 없지만, HotpotQA<cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib57" title="">2018</a>)</cite>에서와 같이 둘 이상의 문서가 될 수 있다. 그런 다음 데이터 세트에서 질문의 <math alttext="P" class="ltx_Math" display="inline" id="S3.p6.7.m7.1"><semantics id="S3.p6.7.m7.1a"><mi id="S3.p6.7.m7.1.1" xref="S3.p6.7.m7.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.p6.7.m7.1b"><ci id="S3.p6.7.m7.1.1.cmml" xref="S3.p6.7.m7.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.7.m7.1c">P</annotation></semantics></math> 분수(<math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.p6.8.m8.1"><semantics id="S3.p6.8.m8.1a"><msub id="S3.p6.8.m8.1.1" xref="S3.p6.8.m8.1.1.cmml"><mi id="S3.p6.8.m8.1.1.2" xref="S3.p6.8.m8.1.1.2.cmml">q</mi><mi id="S3.p6.8.m8.1.1.3" xref="S3.p6.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.8.m8.1b"><apply id="S3.p6.8.m8.1.1.cmml" xref="S3.p6.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p6.8.m8.1.1.1.cmml" xref="S3.p6.8.m8.1.1">subscript</csymbol><ci id="S3.p6.8.m8.1.1.2.cmml" xref="S3.p6.8.m8.1.1.2">𝑞</ci><ci id="S3.p6.8.m8.1.1.3.cmml" xref="S3.p6.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.8.m8.1c">q_{i}</annotation></semantics></math>)에 대해 분산기 문서(<math alttext="d_{k-1}" class="ltx_Math" display="inline" id="S3.p6.10.m10.1"><semantics id="S3.p6.10.m10.1a"><msub id="S3.p6.10.m10.1.1" xref="S3.p6.10.m10.1.1.cmml"><mi id="S3.p6.10.m10.1.1.2" xref="S3.p6.10.m10.1.1.2.cmml">d</mi><mrow id="S3.p6.10.m10.1.1.3" xref="S3.p6.10.m10.1.1.3.cmml"><mi id="S3.p6.10.m10.1.1.3.2" xref="S3.p6.10.m10.1.1.3.2.cmml">k</mi><mo id="S3.p6.10.m10.1.1.3.1" xref="S3.p6.10.m10.1.1.3.1.cmml">−</mo><mn id="S3.p6.10.m10.1.1.3.3" xref="S3.p6.10.m10.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p6.10.m10.1b"><apply id="S3.p6.10.m10.1.1.cmml" xref="S3.p6.10.m10.1.1"><csymbol cd="ambiguous" id="S3.p6.10.m10.1.1.1.cmml" xref="S3.p6.10.m10.1.1">subscript</csymbol><ci id="S3.p6.10.m10.1.1.2.cmml" xref="S3.p6.10.m10.1.1.2">𝑑</ci><apply id="S3.p6.10.m10.1.1.3.cmml" xref="S3.p6.10.m10.1.1.3"><minus id="S3.p6.10.m10.1.1.3.1.cmml" xref="S3.p6.10.m10.1.1.3.1"></minus><ci id="S3.p6.10.m10.1.1.3.2.cmml" xref="S3.p6.10.m10.1.1.3.2">𝑘</ci><cn id="S3.p6.10.m10.1.1.3.3.cmml" type="integer" xref="S3.p6.10.m10.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.10.m10.1c">d_{k-1}</annotation></semantics></math>)와 함께 oracle 문서(<math alttext="d_{i}^{*}" class="ltx_Math" display="inline" id="S3.p6.9.m9.1"><semantics id="S3.p6.9.m9.1a"><msubsup id="S3.p6.9.m9.1.1" xref="S3.p6.9.m9.1.1.cmml"><mi id="S3.p6.9.m9.1.1.2.2" xref="S3.p6.9.m9.1.1.2.2.cmml">d</mi><mi id="S3.p6.9.m9.1.1.2.3" xref="S3.p6.9.m9.1.1.2.3.cmml">i</mi><mo id="S3.p6.9.m9.1.1.3" xref="S3.p6.9.m9.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.p6.9.m9.1b"><apply id="S3.p6.9.m9.1.1.cmml" xref="S3.p6.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p6.9.m9.1.1.1.cmml" xref="S3.p6.9.m9.1.1">superscript</csymbol><apply id="S3.p6.9.m9.1.1.2.cmml" xref="S3.p6.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p6.9.m9.1.1.2.1.cmml" xref="S3.p6.9.m9.1.1">subscript</csymbol><ci id="S3.p6.9.m9.1.1.2.2.cmml" xref="S3.p6.9.m9.1.1.2.2">𝑑</ci><ci id="S3.p6.9.m9.1.1.2.3.cmml" xref="S3.p6.9.m9.1.1.2.3">𝑖</ci></apply><times id="S3.p6.9.m9.1.1.3.cmml" xref="S3.p6.9.m9.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.9.m9.1c">d_{i}^{*}</annotation></semantics></math>)를 유지한다. 데이터 세트에서 질문의 <math alttext="(1-P)" class="ltx_Math" display="inline" id="S3.p6.11.m11.1"><semantics id="S3.p6.11.m11.1a"><mrow id="S3.p6.11.m11.1.1.1" xref="S3.p6.11.m11.1.1.1.1.cmml"><mo id="S3.p6.11.m11.1.1.1.2" stretchy="false" xref="S3.p6.11.m11.1.1.1.1.cmml">(</mo><mrow id="S3.p6.11.m11.1.1.1.1" xref="S3.p6.11.m11.1.1.1.1.cmml"><mn id="S3.p6.11.m11.1.1.1.1.2" xref="S3.p6.11.m11.1.1.1.1.2.cmml">1</mn><mo id="S3.p6.11.m11.1.1.1.1.1" xref="S3.p6.11.m11.1.1.1.1.1.cmml">−</mo><mi id="S3.p6.11.m11.1.1.1.1.3" xref="S3.p6.11.m11.1.1.1.1.3.cmml">P</mi></mrow><mo id="S3.p6.11.m11.1.1.1.3" stretchy="false" xref="S3.p6.11.m11.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.11.m11.1b"><apply id="S3.p6.11.m11.1.1.1.1.cmml" xref="S3.p6.11.m11.1.1.1"><minus id="S3.p6.11.m11.1.1.1.1.1.cmml" xref="S3.p6.11.m11.1.1.1.1.1"></minus><cn id="S3.p6.11.m11.1.1.1.1.2.cmml" type="integer" xref="S3.p6.11.m11.1.1.1.1.2">1</cn><ci id="S3.p6.11.m11.1.1.1.1.3.cmml" xref="S3.p6.11.m11.1.1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.11.m11.1c">(1-P)</annotation></semantics></math> 분수의 경우(<math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.p6.12.m12.1"><semantics id="S3.p6.12.m12.1a"><msub id="S3.p6.12.m12.1.1" xref="S3.p6.12.m12.1.1.cmml"><mi id="S3.p6.12.m12.1.1.2" xref="S3.p6.12.m12.1.1.2.cmml">q</mi><mi id="S3.p6.12.m12.1.1.3" xref="S3.p6.12.m12.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.12.m12.1b"><apply id="S3.p6.12.m12.1.1.cmml" xref="S3.p6.12.m12.1.1"><csymbol cd="ambiguous" id="S3.p6.12.m12.1.1.1.cmml" xref="S3.p6.12.m12.1.1">subscript</csymbol><ci id="S3.p6.12.m12.1.1.2.cmml" xref="S3.p6.12.m12.1.1.2">𝑞</ci><ci id="S3.p6.12.m12.1.1.3.cmml" xref="S3.p6.12.m12.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.12.m12.1c">q_{i}</annotation></semantics></math>), 오라클 문서는 포함하지 않고 산만기 문서(<math alttext="d_{k}" class="ltx_Math" display="inline" id="S3.p6.13.m13.1"><semantics id="S3.p6.13.m13.1a"><msub id="S3.p6.13.m13.1.1" xref="S3.p6.13.m13.1.1.cmml"><mi id="S3.p6.13.m13.1.1.2" xref="S3.p6.13.m13.1.1.2.cmml">d</mi><mi id="S3.p6.13.m13.1.1.3" xref="S3.p6.13.m13.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.13.m13.1b"><apply id="S3.p6.13.m13.1.1.cmml" xref="S3.p6.13.m13.1.1"><csymbol cd="ambiguous" id="S3.p6.13.m13.1.1.1.cmml" xref="S3.p6.13.m13.1.1">subscript</csymbol><ci id="S3.p6.13.m13.1.1.2.cmml" xref="S3.p6.13.m13.1.1.2">𝑑</ci><ci id="S3.p6.13.m13.1.1.3.cmml" xref="S3.p6.13.m13.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.13.m13.1c">d_{k}</annotation></semantics></math>)만 포함한다. 그런 다음 표준 지도 학습(SFT) 기법을 사용하여 언어 모델을 미세 조정하고 제공된 문서 및 질문에서 답변을 생성하도록 학습한다. Fig. <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">2</span></a>는 RAFT에 대한 상위 레벨 설계 주체를 예시한다.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p class="ltx_p" id="S3.p7.1">본 논문에서는 <em class="ltx_emph ltx_font_italic" id="S3.p7.1.1">i.e., in-domain</em>에서 학습한 문서 집합에서 더 나은 RAG를 수행하도록 모델을 학습한다. 어떤 경우에는 오라클 문서를 제거함으로써, 우리는 모델이 문맥에서 답을 도출하는 대신 답을 암기하도록 강요하고 있다. RAFT에 대한 학습 데이터는 다음과 같으며, 학습 데이터의 예는 Fig. <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
<div id="S3.p8" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.8" class="ltx_p"><math id="S3.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="{\mathbf{P}}" display="inline"><semantics id="S3.I2.i1.p1.1.m1.1a"><mi id="S3.I2.i1.p1.1.m1.1.1" xref="S3.I2.i1.p1.1.m1.1.1.cmml">𝐏</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.1.m1.1b"><ci id="S3.I2.i1.p1.1.m1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1">𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.1.m1.1c">{\mathbf{P}}</annotation></semantics></math> % of data: <math id="S3.I2.i1.p1.2.m2.1" class="ltx_Math" alttext="{\mathbf{Q}}" display="inline"><semantics id="S3.I2.i1.p1.2.m2.1a"><mi id="S3.I2.i1.p1.2.m2.1.1" xref="S3.I2.i1.p1.2.m2.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.2.m2.1b"><ci id="S3.I2.i1.p1.2.m2.1.1.cmml" xref="S3.I2.i1.p1.2.m2.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.2.m2.1c">{\mathbf{Q}}</annotation></semantics></math> + <math id="S3.I2.i1.p1.3.m3.1" class="ltx_Math" alttext="{\mathbf{D}}^{*}" display="inline"><semantics id="S3.I2.i1.p1.3.m3.1a"><msup id="S3.I2.i1.p1.3.m3.1.1" xref="S3.I2.i1.p1.3.m3.1.1.cmml"><mi id="S3.I2.i1.p1.3.m3.1.1.2" xref="S3.I2.i1.p1.3.m3.1.1.2.cmml">𝐃</mi><mo id="S3.I2.i1.p1.3.m3.1.1.3" xref="S3.I2.i1.p1.3.m3.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.3.m3.1b"><apply id="S3.I2.i1.p1.3.m3.1.1.cmml" xref="S3.I2.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.3.m3.1.1.1.cmml" xref="S3.I2.i1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.I2.i1.p1.3.m3.1.1.2.cmml" xref="S3.I2.i1.p1.3.m3.1.1.2">𝐃</ci><times id="S3.I2.i1.p1.3.m3.1.1.3.cmml" xref="S3.I2.i1.p1.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.3.m3.1c">{\mathbf{D}}^{*}</annotation></semantics></math> + <math id="S3.I2.i1.p1.4.m4.1" class="ltx_Math" alttext="{\mathbf{D}}_{2}" display="inline"><semantics id="S3.I2.i1.p1.4.m4.1a"><msub id="S3.I2.i1.p1.4.m4.1.1" xref="S3.I2.i1.p1.4.m4.1.1.cmml"><mi id="S3.I2.i1.p1.4.m4.1.1.2" xref="S3.I2.i1.p1.4.m4.1.1.2.cmml">𝐃</mi><mn id="S3.I2.i1.p1.4.m4.1.1.3" xref="S3.I2.i1.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.4.m4.1b"><apply id="S3.I2.i1.p1.4.m4.1.1.cmml" xref="S3.I2.i1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.4.m4.1.1.1.cmml" xref="S3.I2.i1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.4.m4.1.1.2.cmml" xref="S3.I2.i1.p1.4.m4.1.1.2">𝐃</ci><cn type="integer" id="S3.I2.i1.p1.4.m4.1.1.3.cmml" xref="S3.I2.i1.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.4.m4.1c">{\mathbf{D}}_{2}</annotation></semantics></math> + <math id="S3.I2.i1.p1.5.m5.1" class="ltx_Math" alttext="\dots" display="inline"><semantics id="S3.I2.i1.p1.5.m5.1a"><mi mathvariant="normal" id="S3.I2.i1.p1.5.m5.1.1" xref="S3.I2.i1.p1.5.m5.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.5.m5.1b"><ci id="S3.I2.i1.p1.5.m5.1.1.cmml" xref="S3.I2.i1.p1.5.m5.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.5.m5.1c">\dots</annotation></semantics></math> + <math id="S3.I2.i1.p1.6.m6.1" class="ltx_Math" alttext="{\mathbf{D}}_{k}" display="inline"><semantics id="S3.I2.i1.p1.6.m6.1a"><msub id="S3.I2.i1.p1.6.m6.1.1" xref="S3.I2.i1.p1.6.m6.1.1.cmml"><mi id="S3.I2.i1.p1.6.m6.1.1.2" xref="S3.I2.i1.p1.6.m6.1.1.2.cmml">𝐃</mi><mi id="S3.I2.i1.p1.6.m6.1.1.3" xref="S3.I2.i1.p1.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.6.m6.1b"><apply id="S3.I2.i1.p1.6.m6.1.1.cmml" xref="S3.I2.i1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.6.m6.1.1.1.cmml" xref="S3.I2.i1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.6.m6.1.1.2.cmml" xref="S3.I2.i1.p1.6.m6.1.1.2">𝐃</ci><ci id="S3.I2.i1.p1.6.m6.1.1.3.cmml" xref="S3.I2.i1.p1.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.6.m6.1c">{\mathbf{D}}_{k}</annotation></semantics></math> <math id="S3.I2.i1.p1.7.m7.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S3.I2.i1.p1.7.m7.1a"><mo stretchy="false" id="S3.I2.i1.p1.7.m7.1.1" xref="S3.I2.i1.p1.7.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.7.m7.1b"><ci id="S3.I2.i1.p1.7.m7.1.1.cmml" xref="S3.I2.i1.p1.7.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.7.m7.1c">\to</annotation></semantics></math> <math id="S3.I2.i1.p1.8.m8.1" class="ltx_math_unparsed" alttext="{\mathbf{A}}*" display="inline"><semantics id="S3.I2.i1.p1.8.m8.1a"><mrow id="S3.I2.i1.p1.8.m8.1b"><mi id="S3.I2.i1.p1.8.m8.1.1">𝐀</mi><mo lspace="0.222em" id="S3.I2.i1.p1.8.m8.1.2">∗</mo></mrow><annotation encoding="application/x-tex" id="S3.I2.i1.p1.8.m8.1c">{\mathbf{A}}*</annotation></semantics></math></p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.8" class="ltx_p">(<math id="S3.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="1-{\mathbf{P}}" display="inline"><semantics id="S3.I2.i2.p1.1.m1.1a"><mrow id="S3.I2.i2.p1.1.m1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.cmml"><mn id="S3.I2.i2.p1.1.m1.1.1.2" xref="S3.I2.i2.p1.1.m1.1.1.2.cmml">1</mn><mo id="S3.I2.i2.p1.1.m1.1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.1.cmml">−</mo><mi id="S3.I2.i2.p1.1.m1.1.1.3" xref="S3.I2.i2.p1.1.m1.1.1.3.cmml">𝐏</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.m1.1b"><apply id="S3.I2.i2.p1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1"><minus id="S3.I2.i2.p1.1.m1.1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1.1"></minus><cn type="integer" id="S3.I2.i2.p1.1.m1.1.1.2.cmml" xref="S3.I2.i2.p1.1.m1.1.1.2">1</cn><ci id="S3.I2.i2.p1.1.m1.1.1.3.cmml" xref="S3.I2.i2.p1.1.m1.1.1.3">𝐏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.m1.1c">1-{\mathbf{P}}</annotation></semantics></math>) % of data: <math id="S3.I2.i2.p1.2.m2.1" class="ltx_Math" alttext="{\mathbf{Q}}" display="inline"><semantics id="S3.I2.i2.p1.2.m2.1a"><mi id="S3.I2.i2.p1.2.m2.1.1" xref="S3.I2.i2.p1.2.m2.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.2.m2.1b"><ci id="S3.I2.i2.p1.2.m2.1.1.cmml" xref="S3.I2.i2.p1.2.m2.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.2.m2.1c">{\mathbf{Q}}</annotation></semantics></math> + <math id="S3.I2.i2.p1.3.m3.1" class="ltx_Math" alttext="{\mathbf{D}}_{1}" display="inline"><semantics id="S3.I2.i2.p1.3.m3.1a"><msub id="S3.I2.i2.p1.3.m3.1.1" xref="S3.I2.i2.p1.3.m3.1.1.cmml"><mi id="S3.I2.i2.p1.3.m3.1.1.2" xref="S3.I2.i2.p1.3.m3.1.1.2.cmml">𝐃</mi><mn id="S3.I2.i2.p1.3.m3.1.1.3" xref="S3.I2.i2.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.3.m3.1b"><apply id="S3.I2.i2.p1.3.m3.1.1.cmml" xref="S3.I2.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.p1.3.m3.1.1.1.cmml" xref="S3.I2.i2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.I2.i2.p1.3.m3.1.1.2.cmml" xref="S3.I2.i2.p1.3.m3.1.1.2">𝐃</ci><cn type="integer" id="S3.I2.i2.p1.3.m3.1.1.3.cmml" xref="S3.I2.i2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.3.m3.1c">{\mathbf{D}}_{1}</annotation></semantics></math> + <math id="S3.I2.i2.p1.4.m4.1" class="ltx_Math" alttext="{\mathbf{D}}_{2}" display="inline"><semantics id="S3.I2.i2.p1.4.m4.1a"><msub id="S3.I2.i2.p1.4.m4.1.1" xref="S3.I2.i2.p1.4.m4.1.1.cmml"><mi id="S3.I2.i2.p1.4.m4.1.1.2" xref="S3.I2.i2.p1.4.m4.1.1.2.cmml">𝐃</mi><mn id="S3.I2.i2.p1.4.m4.1.1.3" xref="S3.I2.i2.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.4.m4.1b"><apply id="S3.I2.i2.p1.4.m4.1.1.cmml" xref="S3.I2.i2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.p1.4.m4.1.1.1.cmml" xref="S3.I2.i2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.I2.i2.p1.4.m4.1.1.2.cmml" xref="S3.I2.i2.p1.4.m4.1.1.2">𝐃</ci><cn type="integer" id="S3.I2.i2.p1.4.m4.1.1.3.cmml" xref="S3.I2.i2.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.4.m4.1c">{\mathbf{D}}_{2}</annotation></semantics></math> + <math id="S3.I2.i2.p1.5.m5.1" class="ltx_Math" alttext="\dots" display="inline"><semantics id="S3.I2.i2.p1.5.m5.1a"><mi mathvariant="normal" id="S3.I2.i2.p1.5.m5.1.1" xref="S3.I2.i2.p1.5.m5.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.5.m5.1b"><ci id="S3.I2.i2.p1.5.m5.1.1.cmml" xref="S3.I2.i2.p1.5.m5.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.5.m5.1c">\dots</annotation></semantics></math> + <math id="S3.I2.i2.p1.6.m6.1" class="ltx_Math" alttext="{\mathbf{D}}_{k}" display="inline"><semantics id="S3.I2.i2.p1.6.m6.1a"><msub id="S3.I2.i2.p1.6.m6.1.1" xref="S3.I2.i2.p1.6.m6.1.1.cmml"><mi id="S3.I2.i2.p1.6.m6.1.1.2" xref="S3.I2.i2.p1.6.m6.1.1.2.cmml">𝐃</mi><mi id="S3.I2.i2.p1.6.m6.1.1.3" xref="S3.I2.i2.p1.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.6.m6.1b"><apply id="S3.I2.i2.p1.6.m6.1.1.cmml" xref="S3.I2.i2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.p1.6.m6.1.1.1.cmml" xref="S3.I2.i2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.I2.i2.p1.6.m6.1.1.2.cmml" xref="S3.I2.i2.p1.6.m6.1.1.2">𝐃</ci><ci id="S3.I2.i2.p1.6.m6.1.1.3.cmml" xref="S3.I2.i2.p1.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.6.m6.1c">{\mathbf{D}}_{k}</annotation></semantics></math> <math id="S3.I2.i2.p1.7.m7.1" class="ltx_Math" alttext="\to" display="inline"><semantics id="S3.I2.i2.p1.7.m7.1a"><mo stretchy="false" id="S3.I2.i2.p1.7.m7.1.1" xref="S3.I2.i2.p1.7.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.7.m7.1b"><ci id="S3.I2.i2.p1.7.m7.1.1.cmml" xref="S3.I2.i2.p1.7.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.7.m7.1c">\to</annotation></semantics></math> <math id="S3.I2.i2.p1.8.m8.1" class="ltx_math_unparsed" alttext="{\mathbf{A}}*" display="inline"><semantics id="S3.I2.i2.p1.8.m8.1a"><mrow id="S3.I2.i2.p1.8.m8.1b"><mi id="S3.I2.i2.p1.8.m8.1.1">𝐀</mi><mo lspace="0.222em" id="S3.I2.i2.p1.8.m8.1.2">∗</mo></mrow><annotation encoding="application/x-tex" id="S3.I2.i2.p1.8.m8.1c">{\mathbf{A}}*</annotation></semantics></math></p>
</div>
</li>
</ul>
</div>
<div id="S3.p9" class="ltx_para">
<p class="ltx_p" id="S3.p9.1">이어서, 테스트 시나리오에 대해, 모델은 RAG 파이프라인에 의해 검색된 Q 및 top-k 문서들을 제공한다. RAFT는 사용된 리트리버와 무관합니다.</p>
</div>
<div id="S3.p10" class="ltx_para">
<p class="ltx_p" id="S3.p10.1">학습 품질을 높이는 핵심 요소는 제공된 답변을 설명하기 위해 연쇄 사상과 같은 추론 프로세스를 생성하는 것이다.RAFT 접근법은 유사하다: 우리는 완전한 추론 체인을 만들고 추가로 출처를 명확하게 인용하면 질문에 대한 모델의 정확도가 향상된다는 것을 보여준다. 를 더 포함할 수 있다. <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">3</span></a> 이 설정을 설명한다. 이러한 방식으로 학습 데이터를 생성하는 것은 질문, 컨텍스트 및 검증된 답변으로 모델을 제시한 다음 원래의 컨텍스트를 적절하게 참조하는 추론 체인을 형성하도록 요청하는 것을 포함한다.</p>
</div>
<div id="S3.p11" class="ltx_para">
<p class="ltx_p" id="S3.p11.1">실험의 모든 데이터 세트에 대해 위에서 설명한 기술을 사용하여 답변을 생성한다. 고릴라 APIBench 데이터 세트에는 이미 답변에 추론이 포함되어 있습니다. 우리는 그림의 생성 단계의 예를 제공한다. <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">3</span></a>의 상세한 추론 답변은 <span class="ltx_text ltx_font_typewriter" id="S3.p11.1.1">##begin_quote##</span> 및 <span class="ltx_text ltx_font_typewriter" id="S3.p11.1.2">##end_quote##</span> 내부의 원래 컨텍스트에서 인용문을 포함할 뿐만 아니라 인용문을 기반으로 결론에 도달하는 방법에 대한 자세한 설명입니다. 우리는 상세한 추론 문단을 추가하는 것이 실험 섹션에서 모델의 성능을 높이는 데 도움이 된다는 것을 보여준다.</p>
</div>
<figure id="S3.F3" class="ltx_figure ltx_align_center">
<div id="S3.F3.2" class="ltx_listing ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,CnxcdGV4dGJme1F1ZXN0aW9uOn18IFRoZSBPYmVyb2kgZmFtaWx5IGlzIHBhcnQgb2YgYSBob3RlbCBjb21wYW55IHRoYXQgaGFzIGEgaGVhZCBvZmZpY2UgaW4gd2hhdCBjaXR5PwpccGFyfFx0ZXh0YmZ7Y29udGV4dDp9fCBbVGhlIE9iZXJvaSBmYW1pbHkgaXMgYW4gSW5kaWFuIGZhbWlseSB0aGF0IGlzIGZhbW91cyBmb3IgaXRzIGludm9sdmVtZW50IGluIGhvdGVscywgbmFtZWx5IHRocm91Z2ggVGhlIE9iZXJvaSBHcm91cF0uLi5bSXQgaXMgbG9jYXRlZCBpbiB0aGUgY2l0eSBjZW50ZXIgb2YgSmFrYXJ0YSwgbmVhciBNZWdhIEt1bmluZ2FuLCBhZGphY2VudCB0byB0aGUgc2lzdGVyIEpXIE1hcnJpb3R0IEhvdGVsLiBJdCBpcyBvcGVyYXRlZCBieSBUaGUgUml0ei1DYXJsdG9uIEhvdGVsIENvbXBhbnkuIFRoZSBjb21wbGV4IGhhcyB0d28gdG93ZXJzIHRoYXQgY29tcHJpc2UgYSBob3RlbCBhbmQgdGhlIEFpcmxhbmdnYSBBcGFydG1lbnQgcmVzcGVjdGl2ZWx5XS4uLltUaGUgT2Jlcm9pIEdyb3VwIGlzIGEgaG90ZWwgY29tcGFueSB3aXRoIGl0cyBoZWFkIG9mZmljZSBpbiBEZWxoaS5dClxwYXJ8XHRleHRiZntJbnN0cnVjdGlvbjp9fCBHaXZlbiB0aGUgcXVlc3Rpb24sIGNvbnRleHQsIGFuZCBhbnN3ZXIgYWJvdmUsIHByb3ZpZGUgYSBsb2dpY2FsIHJlYXNvbmluZyBmb3IgdGhhdCBhbnN3ZXIuIFBsZWFzZSB1c2UgdGhlIGZvcm1hdCBvZjogIyNSZWFzb246IHtyZWFzb259ICMjQW5zd2VyOiB7YW5zd2VyfS4KXHBhci0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tClxwYXJ8XHRleHRiZntDb1QgQW5zd2VyOn18ICMjUmVhc29uOiBUaGUgZG9jdW1lbnQgIyNiZWdpbl9xdW90ZSMjIFRoZSBPYmVyb2kgZmFtaWx5IGlzIGFuIEluZGlhbiBmYW1pbHkgdGhhdCBpcyBmYW1vdXMgZm9yIGl0cyBpbnZvbHZlbWVudCBpbiBob3RlbHMsIG5hbWVseSB0aHJvdWdoIFRoZSBPYmVyb2kgR3JvdXAuICMjZW5kX3F1b3RlIyMgZXN0YWJsaXNoZXMgdGhhdCB0aGUgT2Jlcm9pIGZhbWlseSBpcyBpbnZvbHZlZCBpbiB0aGUgT2Jlcm9pIGdyb3VwLCBhbmQgdGhlIGRvY3VtZW50ICMjYmVnaW5fcXVvdGUjIyBUaGUgT2Jlcm9pIEdyb3VwIGlzIGEgaG90ZWwgY29tcGFueSB3aXRoIGl0cyBoZWFkIG9mZmljZSBpbiBEZWxoaS4gIyNlbmRfcXVvdGUjIyBlc3RhYmxpc2hlcyB0aGUgaGVhZCBvZmZpY2Ugb2YgVGhlIE9iZXJvaSBHcm91cC4gVGhlcmVmb3JlLCB0aGUgT2Jlcm9pIGZhbWlseSBpcyBwYXJ0IG9mIGEgaG90ZWwgY29tcGFueSB3aG9zZSBoZWFkIG9mZmljZSBpcyBpbiBEZWxoaS4gIyNBbnN3ZXI6IERlbGhpClxwYXI=" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
</div>
<div id="lstnumberx2" class="ltx_listingline">|\<span id="lstnumberx2.1" class="ltx_text ltx_lst_identifier">textbf</span>{<span id="lstnumberx2.2" class="ltx_text ltx_lst_identifier">Question</span>:}|<span id="lstnumberx2.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.4" class="ltx_text ltx_lst_identifier">The</span><span id="lstnumberx2.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.6" class="ltx_text ltx_lst_identifier">Oberoi</span><span id="lstnumberx2.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.8" class="ltx_text ltx_lst_identifier">family</span><span id="lstnumberx2.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.10" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx2.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.12" class="ltx_text ltx_lst_identifier">part</span><span id="lstnumberx2.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.14" class="ltx_text ltx_lst_identifier">of</span><span id="lstnumberx2.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.16" class="ltx_text ltx_lst_identifier">a</span><span id="lstnumberx2.17" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.18" class="ltx_text ltx_lst_identifier">hotel</span><span id="lstnumberx2.19" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.20" class="ltx_text ltx_lst_identifier">company</span><span id="lstnumberx2.21" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.22" class="ltx_text ltx_lst_identifier">that</span><span id="lstnumberx2.23" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.24" class="ltx_text ltx_lst_identifier">has</span><span id="lstnumberx2.25" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.26" class="ltx_text ltx_lst_identifier">a</span><span id="lstnumberx2.27" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.28" class="ltx_text ltx_lst_identifier">head</span><span id="lstnumberx2.29" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.30" class="ltx_text ltx_lst_identifier">office</span><span id="lstnumberx2.31" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.32" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx2.33" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.34" class="ltx_text ltx_lst_identifier">what</span><span id="lstnumberx2.35" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.36" class="ltx_text ltx_lst_identifier">city</span>?
</div>
<div id="lstnumberx3" class="ltx_listingline">\<span id="lstnumberx3.1" class="ltx_text ltx_lst_identifier">par</span>|\<span id="lstnumberx3.2" class="ltx_text ltx_lst_identifier">textbf</span>{<span id="lstnumberx3.3" class="ltx_text ltx_lst_identifier">context</span>:}|<span id="lstnumberx3.4" class="ltx_text ltx_lst_space"> </span>[<span id="lstnumberx3.5" class="ltx_text ltx_lst_identifier">The</span><span id="lstnumberx3.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.7" class="ltx_text ltx_lst_identifier">Oberoi</span><span id="lstnumberx3.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.9" class="ltx_text ltx_lst_identifier">family</span><span id="lstnumberx3.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.11" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx3.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.13" class="ltx_text ltx_lst_identifier">an</span><span id="lstnumberx3.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.15" class="ltx_text ltx_lst_identifier">Indian</span><span id="lstnumberx3.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.17" class="ltx_text ltx_lst_identifier">family</span><span id="lstnumberx3.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.19" class="ltx_text ltx_lst_identifier">that</span><span id="lstnumberx3.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.21" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx3.22" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.23" class="ltx_text ltx_lst_identifier">famous</span><span id="lstnumberx3.24" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.25" class="ltx_text ltx_lst_identifier">for</span><span id="lstnumberx3.26" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.27" class="ltx_text ltx_lst_identifier">its</span><span id="lstnumberx3.28" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.29" class="ltx_text ltx_lst_identifier">involvement</span><span id="lstnumberx3.30" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.31" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx3.32" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.33" class="ltx_text ltx_lst_identifier">hotels</span>,<span id="lstnumberx3.34" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.35" class="ltx_text ltx_lst_identifier">namely</span><span id="lstnumberx3.36" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.37" class="ltx_text ltx_lst_identifier">through</span><span id="lstnumberx3.38" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.39" class="ltx_text ltx_lst_identifier">The</span><span id="lstnumberx3.40" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.41" class="ltx_text ltx_lst_identifier">Oberoi</span><span id="lstnumberx3.42" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.43" class="ltx_text ltx_lst_identifier">Group</span>]…[<span id="lstnumberx3.44" class="ltx_text ltx_lst_identifier">It</span><span id="lstnumberx3.45" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.46" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx3.47" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.48" class="ltx_text ltx_lst_identifier">located</span><span id="lstnumberx3.49" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.50" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx3.51" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.52" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx3.53" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.54" class="ltx_text ltx_lst_identifier">city</span><span id="lstnumberx3.55" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.56" class="ltx_text ltx_lst_identifier">center</span><span id="lstnumberx3.57" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.58" class="ltx_text ltx_lst_identifier">of</span><span id="lstnumberx3.59" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.60" class="ltx_text ltx_lst_identifier">Jakarta</span>,<span id="lstnumberx3.61" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.62" class="ltx_text ltx_lst_identifier">near</span><span id="lstnumberx3.63" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.64" class="ltx_text ltx_lst_identifier">Mega</span><span id="lstnumberx3.65" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.66" class="ltx_text ltx_lst_identifier">Kuningan</span>,<span id="lstnumberx3.67" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.68" class="ltx_text ltx_lst_identifier">adjacent</span><span id="lstnumberx3.69" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.70" class="ltx_text ltx_lst_identifier">to</span><span id="lstnumberx3.71" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.72" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx3.73" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.74" class="ltx_text ltx_lst_identifier">sister</span><span id="lstnumberx3.75" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.76" class="ltx_text ltx_lst_identifier">JW</span><span id="lstnumberx3.77" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.78" class="ltx_text ltx_lst_identifier">Marriott</span><span id="lstnumberx3.79" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.80" class="ltx_text ltx_lst_identifier">Hotel</span>.<span id="lstnumberx3.81" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.82" class="ltx_text ltx_lst_identifier">It</span><span id="lstnumberx3.83" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.84" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx3.85" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.86" class="ltx_text ltx_lst_identifier">operated</span><span id="lstnumberx3.87" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.88" class="ltx_text ltx_lst_identifier">by</span><span id="lstnumberx3.89" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.90" class="ltx_text ltx_lst_identifier">The</span><span id="lstnumberx3.91" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.92" class="ltx_text ltx_lst_identifier">Ritz</span>-<span id="lstnumberx3.93" class="ltx_text ltx_lst_identifier">Carlton</span><span id="lstnumberx3.94" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.95" class="ltx_text ltx_lst_identifier">Hotel</span><span id="lstnumberx3.96" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.97" class="ltx_text ltx_lst_identifier">Company</span>.<span id="lstnumberx3.98" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.99" class="ltx_text ltx_lst_identifier">The</span><span id="lstnumberx3.100" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.101" class="ltx_text ltx_lst_identifier">complex</span><span id="lstnumberx3.102" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.103" class="ltx_text ltx_lst_identifier">has</span><span id="lstnumberx3.104" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.105" class="ltx_text ltx_lst_identifier">two</span><span id="lstnumberx3.106" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.107" class="ltx_text ltx_lst_identifier">towers</span><span id="lstnumberx3.108" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.109" class="ltx_text ltx_lst_identifier">that</span><span id="lstnumberx3.110" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.111" class="ltx_text ltx_lst_identifier">comprise</span><span id="lstnumberx3.112" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.113" class="ltx_text ltx_lst_identifier">a</span><span id="lstnumberx3.114" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.115" class="ltx_text ltx_lst_identifier">hotel</span><span id="lstnumberx3.116" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.117" class="ltx_text ltx_lst_identifier">and</span><span id="lstnumberx3.118" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.119" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx3.120" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.121" class="ltx_text ltx_lst_identifier">Airlangga</span><span id="lstnumberx3.122" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.123" class="ltx_text ltx_lst_identifier">Apartment</span><span id="lstnumberx3.124" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.125" class="ltx_text ltx_lst_identifier">respectively</span>]…[<span id="lstnumberx3.126" class="ltx_text ltx_lst_identifier">The</span><span id="lstnumberx3.127" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.128" class="ltx_text ltx_lst_identifier">Oberoi</span><span id="lstnumberx3.129" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.130" class="ltx_text ltx_lst_identifier">Group</span><span id="lstnumberx3.131" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.132" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx3.133" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.134" class="ltx_text ltx_lst_identifier">a</span><span id="lstnumberx3.135" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.136" class="ltx_text ltx_lst_identifier">hotel</span><span id="lstnumberx3.137" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.138" class="ltx_text ltx_lst_identifier">company</span><span id="lstnumberx3.139" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.140" class="ltx_text ltx_lst_identifier">with</span><span id="lstnumberx3.141" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.142" class="ltx_text ltx_lst_identifier">its</span><span id="lstnumberx3.143" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.144" class="ltx_text ltx_lst_identifier">head</span><span id="lstnumberx3.145" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.146" class="ltx_text ltx_lst_identifier">office</span><span id="lstnumberx3.147" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.148" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx3.149" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.150" class="ltx_text ltx_lst_identifier">Delhi</span>.]
</div>
<div id="lstnumberx4" class="ltx_listingline">\<span id="lstnumberx4.1" class="ltx_text ltx_lst_identifier">par</span>|\<span id="lstnumberx4.2" class="ltx_text ltx_lst_identifier">textbf</span>{<span id="lstnumberx4.3" class="ltx_text ltx_lst_identifier">Instruction</span>:}|<span id="lstnumberx4.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.5" class="ltx_text ltx_lst_identifier">Given</span><span id="lstnumberx4.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.7" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx4.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.9" class="ltx_text ltx_lst_identifier">question</span>,<span id="lstnumberx4.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.11" class="ltx_text ltx_lst_identifier">context</span>,<span id="lstnumberx4.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.13" class="ltx_text ltx_lst_identifier">and</span><span id="lstnumberx4.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.15" class="ltx_text ltx_lst_identifier">answer</span><span id="lstnumberx4.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.17" class="ltx_text ltx_lst_identifier">above</span>,<span id="lstnumberx4.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.19" class="ltx_text ltx_lst_identifier">provide</span><span id="lstnumberx4.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.21" class="ltx_text ltx_lst_identifier">a</span><span id="lstnumberx4.22" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.23" class="ltx_text ltx_lst_identifier">logical</span><span id="lstnumberx4.24" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.25" class="ltx_text ltx_lst_identifier">reasoning</span><span id="lstnumberx4.26" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.27" class="ltx_text ltx_lst_identifier">for</span><span id="lstnumberx4.28" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.29" class="ltx_text ltx_lst_identifier">that</span><span id="lstnumberx4.30" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.31" class="ltx_text ltx_lst_identifier">answer</span>.<span id="lstnumberx4.32" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.33" class="ltx_text ltx_lst_identifier">Please</span><span id="lstnumberx4.34" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.35" class="ltx_text ltx_lst_identifier">use</span><span id="lstnumberx4.36" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.37" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx4.38" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.39" class="ltx_text ltx_lst_identifier">format</span><span id="lstnumberx4.40" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.41" class="ltx_text ltx_lst_identifier">of</span>:<span id="lstnumberx4.42" class="ltx_text ltx_lst_space"> </span>##<span id="lstnumberx4.43" class="ltx_text ltx_lst_identifier">Reason</span>:<span id="lstnumberx4.44" class="ltx_text ltx_lst_space"> </span>{<span id="lstnumberx4.45" class="ltx_text ltx_lst_identifier">reason</span>}<span id="lstnumberx4.46" class="ltx_text ltx_lst_space"> </span>##<span id="lstnumberx4.47" class="ltx_text ltx_lst_identifier">Answer</span>:<span id="lstnumberx4.48" class="ltx_text ltx_lst_space"> </span>{<span id="lstnumberx4.49" class="ltx_text ltx_lst_identifier">answer</span>}.
</div>
<div id="lstnumberx5" class="ltx_listingline">\<span id="lstnumberx5.1" class="ltx_text ltx_lst_identifier">par</span>————————————————————————————–
</div>
<div id="lstnumberx6" class="ltx_listingline">\<span id="lstnumberx6.1" class="ltx_text ltx_lst_identifier">par</span>|\<span id="lstnumberx6.2" class="ltx_text ltx_lst_identifier">textbf</span>{<span id="lstnumberx6.3" class="ltx_text ltx_lst_identifier">CoT</span><span id="lstnumberx6.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.5" class="ltx_text ltx_lst_identifier">Answer</span>:}|<span id="lstnumberx6.6" class="ltx_text ltx_lst_space"> </span>##<span id="lstnumberx6.7" class="ltx_text ltx_lst_identifier">Reason</span>:<span id="lstnumberx6.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.9" class="ltx_text ltx_lst_identifier">The</span><span id="lstnumberx6.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.11" class="ltx_text ltx_lst_identifier">document</span><span id="lstnumberx6.12" class="ltx_text ltx_lst_space"> </span>##<span id="lstnumberx6.13" class="ltx_text ltx_lst_identifier">begin_quote</span>##<span id="lstnumberx6.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.15" class="ltx_text ltx_lst_identifier">The</span><span id="lstnumberx6.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.17" class="ltx_text ltx_lst_identifier">Oberoi</span><span id="lstnumberx6.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.19" class="ltx_text ltx_lst_identifier">family</span><span id="lstnumberx6.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.21" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx6.22" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.23" class="ltx_text ltx_lst_identifier">an</span><span id="lstnumberx6.24" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.25" class="ltx_text ltx_lst_identifier">Indian</span><span id="lstnumberx6.26" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.27" class="ltx_text ltx_lst_identifier">family</span><span id="lstnumberx6.28" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.29" class="ltx_text ltx_lst_identifier">that</span><span id="lstnumberx6.30" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.31" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx6.32" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.33" class="ltx_text ltx_lst_identifier">famous</span><span id="lstnumberx6.34" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.35" class="ltx_text ltx_lst_identifier">for</span><span id="lstnumberx6.36" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.37" class="ltx_text ltx_lst_identifier">its</span><span id="lstnumberx6.38" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.39" class="ltx_text ltx_lst_identifier">involvement</span><span id="lstnumberx6.40" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.41" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx6.42" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.43" class="ltx_text ltx_lst_identifier">hotels</span>,<span id="lstnumberx6.44" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.45" class="ltx_text ltx_lst_identifier">namely</span><span id="lstnumberx6.46" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.47" class="ltx_text ltx_lst_identifier">through</span><span id="lstnumberx6.48" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.49" class="ltx_text ltx_lst_identifier">The</span><span id="lstnumberx6.50" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.51" class="ltx_text ltx_lst_identifier">Oberoi</span><span id="lstnumberx6.52" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.53" class="ltx_text ltx_lst_identifier">Group</span>.<span id="lstnumberx6.54" class="ltx_text ltx_lst_space"> </span>##<span id="lstnumberx6.55" class="ltx_text ltx_lst_identifier">end_quote</span>##<span id="lstnumberx6.56" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.57" class="ltx_text ltx_lst_identifier">establishes</span><span id="lstnumberx6.58" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.59" class="ltx_text ltx_lst_identifier">that</span><span id="lstnumberx6.60" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.61" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx6.62" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.63" class="ltx_text ltx_lst_identifier">Oberoi</span><span id="lstnumberx6.64" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.65" class="ltx_text ltx_lst_identifier">family</span><span id="lstnumberx6.66" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.67" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx6.68" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.69" class="ltx_text ltx_lst_identifier">involved</span><span id="lstnumberx6.70" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.71" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx6.72" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.73" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx6.74" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.75" class="ltx_text ltx_lst_identifier">Oberoi</span><span id="lstnumberx6.76" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.77" class="ltx_text ltx_lst_identifier">group</span>,<span id="lstnumberx6.78" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.79" class="ltx_text ltx_lst_identifier">and</span><span id="lstnumberx6.80" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.81" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx6.82" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.83" class="ltx_text ltx_lst_identifier">document</span><span id="lstnumberx6.84" class="ltx_text ltx_lst_space"> </span>##<span id="lstnumberx6.85" class="ltx_text ltx_lst_identifier">begin_quote</span>##<span id="lstnumberx6.86" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.87" class="ltx_text ltx_lst_identifier">The</span><span id="lstnumberx6.88" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.89" class="ltx_text ltx_lst_identifier">Oberoi</span><span id="lstnumberx6.90" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.91" class="ltx_text ltx_lst_identifier">Group</span><span id="lstnumberx6.92" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.93" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx6.94" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.95" class="ltx_text ltx_lst_identifier">a</span><span id="lstnumberx6.96" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.97" class="ltx_text ltx_lst_identifier">hotel</span><span id="lstnumberx6.98" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.99" class="ltx_text ltx_lst_identifier">company</span><span id="lstnumberx6.100" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.101" class="ltx_text ltx_lst_identifier">with</span><span id="lstnumberx6.102" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.103" class="ltx_text ltx_lst_identifier">its</span><span id="lstnumberx6.104" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.105" class="ltx_text ltx_lst_identifier">head</span><span id="lstnumberx6.106" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.107" class="ltx_text ltx_lst_identifier">office</span><span id="lstnumberx6.108" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.109" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx6.110" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.111" class="ltx_text ltx_lst_identifier">Delhi</span>.<span id="lstnumberx6.112" class="ltx_text ltx_lst_space"> </span>##<span id="lstnumberx6.113" class="ltx_text ltx_lst_identifier">end_quote</span>##<span id="lstnumberx6.114" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.115" class="ltx_text ltx_lst_identifier">establishes</span><span id="lstnumberx6.116" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.117" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx6.118" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.119" class="ltx_text ltx_lst_identifier">head</span><span id="lstnumberx6.120" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.121" class="ltx_text ltx_lst_identifier">office</span><span id="lstnumberx6.122" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.123" class="ltx_text ltx_lst_identifier">of</span><span id="lstnumberx6.124" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.125" class="ltx_text ltx_lst_identifier">The</span><span id="lstnumberx6.126" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.127" class="ltx_text ltx_lst_identifier">Oberoi</span><span id="lstnumberx6.128" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.129" class="ltx_text ltx_lst_identifier">Group</span>.<span id="lstnumberx6.130" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.131" class="ltx_text ltx_lst_identifier">Therefore</span>,<span id="lstnumberx6.132" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.133" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx6.134" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.135" class="ltx_text ltx_lst_identifier">Oberoi</span><span id="lstnumberx6.136" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.137" class="ltx_text ltx_lst_identifier">family</span><span id="lstnumberx6.138" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.139" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx6.140" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.141" class="ltx_text ltx_lst_identifier">part</span><span id="lstnumberx6.142" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.143" class="ltx_text ltx_lst_identifier">of</span><span id="lstnumberx6.144" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.145" class="ltx_text ltx_lst_identifier">a</span><span id="lstnumberx6.146" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.147" class="ltx_text ltx_lst_identifier">hotel</span><span id="lstnumberx6.148" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.149" class="ltx_text ltx_lst_identifier">company</span><span id="lstnumberx6.150" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.151" class="ltx_text ltx_lst_identifier">whose</span><span id="lstnumberx6.152" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.153" class="ltx_text ltx_lst_identifier">head</span><span id="lstnumberx6.154" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.155" class="ltx_text ltx_lst_identifier">office</span><span id="lstnumberx6.156" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.157" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx6.158" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.159" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx6.160" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.161" class="ltx_text ltx_lst_identifier">Delhi</span>.<span id="lstnumberx6.162" class="ltx_text ltx_lst_space"> </span>##<span id="lstnumberx6.163" class="ltx_text ltx_lst_identifier">Answer</span>:<span id="lstnumberx6.164" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.165" class="ltx_text ltx_lst_identifier">Delhi</span>
</div>
<div id="lstnumberx7" class="ltx_listingline">\<span id="lstnumberx7.1" class="ltx_text ltx_lst_identifier">par</span>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.3.1.1" style="font-size:90%;">Figure 3</span>:</span><span class="ltx_text" id="S3.F3.4.2" style="font-size:90%;">RAFT prompt to help LLM evaluate its own generated reasoning and answers, contrasting them with the correct reasoning and answers. LLM은 추론의 오류를 식별하고 개선을 위한 주요 통찰력을 추출하도록 촉구된다. 이 도면은 RAFT 알고리즘에서의 ‘GenerateExplanation’ 단계를 구체적으로 나타낸다(<a class="ltx_ref" href="#S3" title="3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a>). </span></figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">표 1</span>:</span><span class="ltx_text ltx_font_bold" id="S3.T1.4.2" style="font-size:90%;">RAFT는 모든 전문 도메인에 대한 RAG 성능을 향상시킵니다.<span class="ltx_text ltx_font_medium" id="S3.T1.4.2.1">: Across PubMed, HotpotQA, HuggingFace, Torch Hub 및 Tensorflow Hub, 우리는 도메인 특정 Finetuning이 기본 모델의 성능을 크게 향상시키지만 RAFT는 RAG가 있거나 없는 기존 도메인 특정 Finetuning 방법을 일관되게 능가합니다. 이는 문맥을 가지고 모델을 훈련시킬 필요성을 시사한다. 모델과 LLaMA 미세 조정 레시피를 비교하고 참조용으로 GPT-3.5를 제공한다. </span></span></figcaption>
<table id="S3.T1.5" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T1.5.1" class="ltx_tr">
<td id="S3.T1.5.1.1" class="ltx_td ltx_align_top ltx_border_tt"></td>
<td id="S3.T1.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T1.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.1.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T1.5.1.2.1.1.1" class="ltx_text"></span> <span id="S3.T1.5.1.2.1.1.2" class="ltx_text ltx_align_center">
<span id="S3.T1.5.1.2.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.5.1.2.1.1.2.1.1" class="ltx_tr">
<span id="S3.T1.5.1.2.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PubMed</span></span>
</span></span><span id="S3.T1.5.1.2.1.1.3" class="ltx_text ltx_align_center"></span></span>
</span>
</td>
<td id="S3.T1.5.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T1.5.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.1.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T1.5.1.3.1.1.1" class="ltx_text"></span> <span id="S3.T1.5.1.3.1.1.2" class="ltx_text ltx_align_center">
<span id="S3.T1.5.1.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.5.1.3.1.1.2.1.1" class="ltx_tr">
<span id="S3.T1.5.1.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">HotpotQA</span></span>
</span></span><span id="S3.T1.5.1.3.1.1.3" class="ltx_text ltx_align_center"></span></span>
</span>
</td>
<td id="S3.T1.5.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T1.5.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T1.5.1.4.1.1.1" class="ltx_text"></span> <span id="S3.T1.5.1.4.1.1.2" class="ltx_text ltx_align_center">
<span id="S3.T1.5.1.4.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.5.1.4.1.1.2.1.1" class="ltx_tr">
<span id="S3.T1.5.1.4.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">HuggingFace</span></span>
</span></span><span id="S3.T1.5.1.4.1.1.3" class="ltx_text ltx_align_center"></span></span>
</span>
</td>
<td id="S3.T1.5.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T1.5.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.1.5.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T1.5.1.5.1.1.1" class="ltx_text"></span> <span id="S3.T1.5.1.5.1.1.2" class="ltx_text ltx_align_center">
<span id="S3.T1.5.1.5.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.5.1.5.1.1.2.1.1" class="ltx_tr">
<span id="S3.T1.5.1.5.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Torch Hub</span></span>
</span></span><span id="S3.T1.5.1.5.1.1.3" class="ltx_text ltx_align_center"></span></span>
</span>
</td>
<td id="S3.T1.5.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.T1.5.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.1.6.1.1" class="ltx_p" style="width:71.1pt;"><span id="S3.T1.5.1.6.1.1.1" class="ltx_text"></span> <span id="S3.T1.5.1.6.1.1.2" class="ltx_text ltx_align_center">
<span id="S3.T1.5.1.6.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.5.1.6.1.1.2.1.1" class="ltx_tr">
<span id="S3.T1.5.1.6.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">TensorFlow Hub</span></span>
</span></span><span id="S3.T1.5.1.6.1.1.3" class="ltx_text ltx_align_center"></span></span>
</span>
</td>
</tr>
<tr id="S3.T1.5.2" class="ltx_tr">
<td id="S3.T1.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.2.1.1.1" class="ltx_p" style="width:85.4pt;">GPT-3.5 + RAG</span>
</span>
</td>
<td id="S3.T1.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.2.2.1.1" class="ltx_p" style="width:48.4pt;">71.60</span>
</span>
</td>
<td id="S3.T1.5.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.2.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T1.5.2.3.1.1.1" class="ltx_text ltx_font_bold">41.5</span></span>
</span>
</td>
<td id="S3.T1.5.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.2.4.1.1" class="ltx_p" style="width:56.9pt;">29.08</span>
</span>
</td>
<td id="S3.T1.5.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.2.5.1.1" class="ltx_p" style="width:48.4pt;">60.21</span>
</span>
</td>
<td id="S3.T1.5.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.2.6.1.1" class="ltx_p" style="width:71.1pt;">65.59</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.3" class="ltx_tr">
<td id="S3.T1.5.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.3.1.1.1" class="ltx_p" style="width:85.4pt;">LLaMA2-7B</span>
</span>
</td>
<td id="S3.T1.5.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.3.2.1.1" class="ltx_p" style="width:48.4pt;">56.5</span>
</span>
</td>
<td id="S3.T1.5.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.3.3.1.1" class="ltx_p" style="width:48.4pt;">0.54</span>
</span>
</td>
<td id="S3.T1.5.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.3.4.1.1" class="ltx_p" style="width:56.9pt;">0.22</span>
</span>
</td>
<td id="S3.T1.5.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.3.5.1.1" class="ltx_p" style="width:48.4pt;">0</span>
</span>
</td>
<td id="S3.T1.5.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.5.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.3.6.1.1" class="ltx_p" style="width:71.1pt;">0</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.4" class="ltx_tr">
<td id="S3.T1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.4.1.1.1" class="ltx_p" style="width:85.4pt;">LLaMA2-7B + RAG</span>
</span>
</td>
<td id="S3.T1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.4.2.1.1" class="ltx_p" style="width:48.4pt;">58.8</span>
</span>
</td>
<td id="S3.T1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.4.3.1.1" class="ltx_p" style="width:48.4pt;">0.03</span>
</span>
</td>
<td id="S3.T1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.4.4.1.1" class="ltx_p" style="width:56.9pt;">26.43</span>
</span>
</td>
<td id="S3.T1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.4.5.1.1" class="ltx_p" style="width:48.4pt;">08.60</span>
</span>
</td>
<td id="S3.T1.5.4.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.4.6.1.1" class="ltx_p" style="width:71.1pt;">43.06</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.5" class="ltx_tr">
<td id="S3.T1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.1.1.1" class="ltx_p" style="width:85.4pt;">DSF</span>
</span>
</td>
<td id="S3.T1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.2.1.1" class="ltx_p" style="width:48.4pt;">59.7</span>
</span>
</td>
<td id="S3.T1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.3.1.1" class="ltx_p" style="width:48.4pt;">6.38</span>
</span>
</td>
<td id="S3.T1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.4.1.1" class="ltx_p" style="width:56.9pt;">61.06</span>
</span>
</td>
<td id="S3.T1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.5.1.1" class="ltx_p" style="width:48.4pt;">84.94</span>
</span>
</td>
<td id="S3.T1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.6.1.1" class="ltx_p" style="width:71.1pt;">86.56</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.6" class="ltx_tr">
<td id="S3.T1.5.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.6.1.1.1" class="ltx_p" style="width:85.4pt;">DSF + RAG</span>
</span>
</td>
<td id="S3.T1.5.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.6.2.1.1" class="ltx_p" style="width:48.4pt;">71.6</span>
</span>
</td>
<td id="S3.T1.5.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.6.3.1.1" class="ltx_p" style="width:48.4pt;">4.41</span>
</span>
</td>
<td id="S3.T1.5.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.6.4.1.1" class="ltx_p" style="width:56.9pt;">42.59</span>
</span>
</td>
<td id="S3.T1.5.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.6.5.1.1" class="ltx_p" style="width:48.4pt;">82.80</span>
</span>
</td>
<td id="S3.T1.5.6.6" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.5.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.6.6.1.1" class="ltx_p" style="width:71.1pt;">60.29</span>
</span>
</td>
</tr>
<tr id="S3.T1.5.7" class="ltx_tr">
<td id="S3.T1.5.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T1.5.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.7.1.1.1" class="ltx_p" style="width:85.4pt;">RAFT (LLaMA2-7B)</span>
</span>
</td>
<td id="S3.T1.5.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T1.5.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.7.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T1.5.7.2.1.1.1" class="ltx_text ltx_font_bold">73.30</span></span>
</span>
</td>
<td id="S3.T1.5.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T1.5.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.7.3.1.1" class="ltx_p" style="width:48.4pt;">35.28</span>
</span>
</td>
<td id="S3.T1.5.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T1.5.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.7.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T1.5.7.4.1.1.1" class="ltx_text ltx_font_bold">74.00</span></span>
</span>
</td>
<td id="S3.T1.5.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T1.5.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.7.5.1.1" class="ltx_p" style="width:48.4pt;"><span id="S3.T1.5.7.5.1.1.1" class="ltx_text ltx_font_bold">84.95</span></span>
</span>
</td>
<td id="S3.T1.5.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S3.T1.5.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.7.6.1.1" class="ltx_p" style="width:71.1pt;"><span id="S3.T1.5.7.6.1.1.1" class="ltx_text ltx_font_bold">86.86</span></span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1">우리는 다양한 기준선과 비교하여 RAFT가 얼마나 잘 수행되는지 연구하기 위해 실험을 설계한다. RAFT-7B 모델(LlaMA-2의 미세 조정 버전)은 도메인 특정 미세 조정 모델 및 RAG를 사용한 범용 모델보다 도메인 문서로부터 정보를 읽고 추출하는 데 더 우수하다는 것을 발견했다. 절제술로서, 우리는 또한 모델이 사고 연쇄 반응으로 학습하는 것이 얼마나 중요한지 보여준다. 이 섹션에서는 먼저 실험에 사용한 모든 데이터 세트를 소개하고 벤치마킹하는 모든 기본 모델/미세 조정 기술을 소개한다.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p1.1">실험에서는 다음 데이터 세트를 사용하여 모델과 모든 기준선을 평가합니다. 위키피디아, 코딩/API 문서, 의료 문서에 대한 질의 응답을 포함한 인기 있는 도메인과 다양한 도메인을 나타내기 위해 이러한 데이터 세트를 선택했다.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S4.I1.i1.p1.1">NQ(Natural Questions) <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al., <a class="ltx_ref" href="#bib.bib21" title="">2019</a>)</cite>, Trivia QA<cite class="ltx_cite ltx_citemacro_citep">(Joshi et al., <a class="ltx_ref" href="#bib.bib18" title="">2017</a>)</cite> 및 HotpotQA<cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="#bib.bib57" title="">2018</a>)</cite>는 주로 상식(예: 영화, 스포츠 등)에 초점을 맞춘 위키피디아 기반의 개방형 질의응답이다.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S4.I1.i2.p1.1">HuggingFace, Torch Hub, TensorFlow Hub는 고릴라 논문에서 제안한 APIBench<cite class="ltx_cite ltx_citemacro_citep">(Patil et al., <a class="ltx_ref" href="#bib.bib38" title="">2023</a>)</cite>에서 나온 것이다. 이러한 벤치마크는 설명서를 기반으로 올바른 기능 및 실행 가능한 API 호출을 생성하는 방법을 측정합니다.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S4.I1.i3.p1.1">PubMed QA<cite class="ltx_cite ltx_citemacro_citep">(Jin et al., <a class="ltx_ref" href="#bib.bib17" title="">2019</a>)</cite>는 생의학-연구 질문-응답에만 맞춰진 질문-응답 데이터 세트이다. 주로 주어진 문서 세트를 기반으로 의학 및 생물학 질문에 답하는 데 중점을 둡니다.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p2.1">데이터 세트의 첫 번째 범주(NQ, 트리비아 QA 및 핫팟 QA)는 비교적 일반적인 도메인인 반면 후자의 두 도메인은 매우 도메인 특정 문서에 있다.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">실험을 위해 다음과 같은 기준선을 고려합니다.</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="S4.I2.i1.p1.1">0-샷 프롬프트가 있는 LlaMA2-7B-채팅 모델: 이것은 QA 작업에 일반적으로 사용되는 명령어-미세 조정 모델이며, 여기서 우리는 명확하게 작성된 지침을 제공하지만 참조 문서는 제공하지 않는다.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="S4.I2.i2.p1.1">RAG가 있는 LlaMA2-7B-chat 모델(Llama2 + RAG): 이전 설정과 유사하지만 여기에 참조 문서를 포함한다. 이것은 도메인별 QA 작업을 처리할 때 인기 있는 기술이다.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="S4.I2.i3.p1.1">도메인 특정 Finetuning with 0-shot prompting (DSF): 표준 감독 finetuning 수행, 문맥상 문서 없음. 우리는 모델의 응답 스타일을 정렬하고 도메인 컨텍스트에 익숙해지는 것이 대부분 유용하다는 것을 발견했다.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p class="ltx_p" id="S4.I2.i4.p1.1">도메인 특정 파인튜닝 with RAG(DSF + RAG): 도메인 특정 파인튜닝 모델을 RAG를 사용하여 외부 지식으로 구현합니다. 따라서 모델이 알지 못하는 '지식'에 대해서는 여전히 맥락을 참조할 수 있다.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p1.1">위의 데이터 세트와 기준선을 사용하여 모델 RAFT를 평가하고 탭에서 RAFT의 효과를 입증한다. <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">1</span></a>. 우리는 RAFT가 지속적으로 그리고 상당히 기준선을 능가한다는 것을 안다. 기본 Llama-2 명령어 조정 모델과 비교하여 RAG를 사용한 RAFT는 정보 추출과 산만 장치에 대한 견고성 측면에서 훨씬 더 우수하다. 이득은 핫팟 QA에서 35.25%, 토치 허브 평가에서 76.35%까지 클 수 있습니다. 특정 데이터 세트에 대한 DSF와 비교하여, 우리 모델은 문제를 해결하기 위해 제공된 컨텍스트에 더 잘 의존한다. RAFT는 HotpotQA 및 HuggingFace 데이터 세트(HotpotQA 30.87%, HuggingFace 31.41%)와 같은 작업에서 훨씬 더 잘한다. PubMed QA의 경우 이진 예/아니오 질문이기 때문에 모델을 DSF + RAG와 비교할 때 상당한 이득을 관찰하지 못한다. 훨씬 더 크고 더 나은 모델 GPT-3.5와 비교하더라도 RAFT는 상당한 이점을 보여준다.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p2.1">전반적으로, RAG가 있거나 없는 LLaMA-7B 모델은 응답 스타일이 그라운드 트루스와 일치하지 않아 성능이 좋지 않다. 도메인 특정 튜닝을 적용하여 성능을 크게 향상시켰습니다. 이 과정을 통해 모델은 적절한 답변 스타일을 학습하고 채택할 수 있다. 그러나 도메인 특정 미세 조정(DSF) 모델에 RAG를 도입하는 것은 항상 더 나은 결과로 이어지지 않는다. 이는 모델이 컨텍스트 처리 및 유용한 정보 추출에 대한 훈련이 부족함을 나타낼 수 있다. 제안하는 기법인 RAFT를 접목하여 응답 스타일과 필요한 응답 스타일을 일치시킬 뿐만 아니라 문서 처리 능력을 향상시킬 수 있도록 학습한다. 결과적으로, 우리의 접근법은 다른 모든 접근법보다 우수하다.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Effect of CoT</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p1.1">우리는 또한 모델의 성능을 향상시키는 데 있어 연쇄적 사고 접근법의 효과를 평가하기 위한 분석을 수행한다. <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.3 Effect of CoT ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">2</span></a>에 나타낸 바와 같이, 단순히 질문에 대한 답변을 제공하는 것이 항상 적절한 것은 아닐 수 있다. 이러한 접근은 손실의 급격한 감소로 이어져 훈련 과정이 분기될 수 있다. 모델을 해답에 안내할 뿐만 아니라 모델의 이해를 풍부하게 하는 추론 체인을 통합하면 전체 정확도를 향상시킬 수 있다. 우리의 실험에서 연쇄 사고를 통합하면 훈련 견고성이 크게 향상된다. 우리는 GPT-4-1106을 사용하여 우리의 연쇄적 사고 프롬프트를 생성하고 그림 <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">3</span></a>에서 사용한 프롬프트의 예를 포함한다.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>:</span><span class="ltx_text ltx_font_bold" id="S4.T2.4.2" style="font-size:90%;">Ablation on Chain-of-Thought<span class="ltx_text ltx_font_medium" id="S4.T2.4.2.1">:</span></span></figcaption>
The numbers of RAFT&nbsp; and RAFT&nbsp; without CoT. Results on various datasets show that adding CoT can significantly improve the performance of the finetuned model. With a gain of 9.66% and 14.93% on the Hotpot QA and HuggingFace datasets respectively.</span></span></figcaption>
<table id="S4.T2.5" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T2.5.1" class="ltx_tr">
<td id="S4.T2.5.1.1" class="ltx_td ltx_align_top ltx_border_tt"></td>
<td id="S4.T2.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T2.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.1.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T2.5.1.2.1.1.1" class="ltx_text"></span> <span id="S4.T2.5.1.2.1.1.2" class="ltx_text ltx_align_center">
<span id="S4.T2.5.1.2.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.5.1.2.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.5.1.2.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PubMed</span></span>
</span></span><span id="S4.T2.5.1.2.1.1.3" class="ltx_text ltx_align_center"></span></span>
</span>
</td>
<td id="S4.T2.5.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T2.5.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.1.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T2.5.1.3.1.1.1" class="ltx_text"></span> <span id="S4.T2.5.1.3.1.1.2" class="ltx_text ltx_align_center">
<span id="S4.T2.5.1.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.5.1.3.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.5.1.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">HotpotQA</span></span>
</span></span><span id="S4.T2.5.1.3.1.1.3" class="ltx_text ltx_align_center"></span></span>
</span>
</td>
<td id="S4.T2.5.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T2.5.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.1.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T2.5.1.4.1.1.1" class="ltx_text"></span> <span id="S4.T2.5.1.4.1.1.2" class="ltx_text ltx_align_center">
<span id="S4.T2.5.1.4.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.5.1.4.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.5.1.4.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">HuggingFace</span></span>
</span></span><span id="S4.T2.5.1.4.1.1.3" class="ltx_text ltx_align_center"></span></span>
</span>
</td>
<td id="S4.T2.5.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T2.5.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.1.5.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T2.5.1.5.1.1.1" class="ltx_text"></span> <span id="S4.T2.5.1.5.1.1.2" class="ltx_text ltx_align_center">
<span id="S4.T2.5.1.5.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.5.1.5.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.5.1.5.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Torch Hub</span></span>
</span></span><span id="S4.T2.5.1.5.1.1.3" class="ltx_text ltx_align_center"></span></span>
</span>
</td>
<td id="S4.T2.5.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T2.5.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.1.6.1.1" class="ltx_p" style="width:71.1pt;"><span id="S4.T2.5.1.6.1.1.1" class="ltx_text"></span> <span id="S4.T2.5.1.6.1.1.2" class="ltx_text ltx_align_center">
<span id="S4.T2.5.1.6.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.5.1.6.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.5.1.6.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">TensorFlow Hub</span></span>
</span></span><span id="S4.T2.5.1.6.1.1.3" class="ltx_text ltx_align_center"></span></span>
</span>
</td>
</tr>
<tr id="S4.T2.5.2" class="ltx_tr">
<td id="S4.T2.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.2.1.1.1" class="ltx_p" style="width:85.4pt;">RAFT w.o CoT</span>
</span>
</td>
<td id="S4.T2.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.2.2.1.1" class="ltx_p" style="width:48.4pt;">68.30</span>
</span>
</td>
<td id="S4.T2.5.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.2.3.1.1" class="ltx_p" style="width:48.4pt;">25.62</span>
</span>
</td>
<td id="S4.T2.5.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.5.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.2.4.1.1" class="ltx_p" style="width:56.9pt;">59.07</span>
</span>
</td>
<td id="S4.T2.5.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.5.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.2.5.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T2.5.2.5.1.1.1" class="ltx_text ltx_font_bold">86.56</span></span>
</span>
</td>
<td id="S4.T2.5.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T2.5.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.2.6.1.1" class="ltx_p" style="width:71.1pt;">83.21</span>
</span>
</td>
</tr>
<tr id="S4.T2.5.3" class="ltx_tr">
<td id="S4.T2.5.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T2.5.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.3.1.1.1" class="ltx_p" style="width:85.4pt;">RAFT</span>
</span>
</td>
<td id="S4.T2.5.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T2.5.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.3.2.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T2.5.3.2.1.1.1" class="ltx_text ltx_font_bold">73.30</span></span>
</span>
</td>
<td id="S4.T2.5.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T2.5.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.3.3.1.1" class="ltx_p" style="width:48.4pt;"><span id="S4.T2.5.3.3.1.1.1" class="ltx_text ltx_font_bold">35.28</span></span>
</span>
</td>
<td id="S4.T2.5.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T2.5.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.3.4.1.1" class="ltx_p" style="width:56.9pt;"><span id="S4.T2.5.3.4.1.1.1" class="ltx_text ltx_font_bold">74.00</span></span>
</span>
</td>
<td id="S4.T2.5.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T2.5.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.3.5.1.1" class="ltx_p" style="width:48.4pt;">84.95</span>
</span>
</td>
<td id="S4.T2.5.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T2.5.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.5.3.6.1.1" class="ltx_p" style="width:71.1pt;"><span id="S4.T2.5.3.6.1.1.1" class="ltx_text ltx_font_bold">86.86</span></span>
</span>
</td>
</tr>
</tbody></table>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Qualitative Analysis</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS4.p1.1">도메인 특정 미세 조정(DSF) 접근법에 비해 RAFT의 잠재적인 이점을 설명하기 위해 그림 <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.4 Qualitative Analysis ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">4</span></a>에서 비교 예를 제시한다. 이 예는 DSF 모델이 시나리오 작성자의 신원을 묻는 질문에 의해 혼란스러워지는 시나리오를 정성적으로 보여준다. 정확한 이름을 제공하는 대신, 그것은 시나리오 작가가 쓴 영화 중 하나를 잘못 인용한다. 대조적으로, RAFT 모델은 질문에 정확하게 대답한다. 이러한 불일치는 질문-응답 쌍만으로 모델을 훈련하는 것이 제공된 문서로부터 관련 컨텍스트를 도출하는 능력을 손상시킬 수 있음을 시사한다. 비교는 텍스트를 효과적으로 처리하는 모델의 능력을 보존하고 향상시키기 위해 표준 수업 조정과 상황 이해를 훈련 데이터 세트에 통합하는 것의 중요성을 강조한다.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><svg id="S4.F4.pic1" class="ltx_picture" height="45341.08" overflow="visible" version="1.1" width="45341.08"><g transform="translate(0,45341.08) matrix(1 0 0 -1 0 0) translate(22670.54,0) translate(0,22670.54)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><clipPath id="pgfcp13"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 5.91 L 0 298.57 C 0 301.83 2.64 304.48 5.91 304.48 L 594.09 304.48 C 597.36 304.48 600 301.83 600 298.57 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z"></path></clipPath><g fill-rule="evenodd"><g fill="#0000BF" fill-opacity="1.0"><path d="M 0 5.91 L 0 298.57 C 0 301.83 2.64 304.48 5.91 304.48 L 594.09 304.48 C 597.36 304.48 600 301.83 600 298.57 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z M 1.97 5.91 L 1.97 274.56 L 598.03 274.56 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 274.56 L 598.03 274.56 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 15.81 283.44)"><foreignObject width="568.39" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span id="S4.F4.pic1.3.3.3.3.1.1" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:410.8pt;">
<span id="S4.F4.pic1.3.3.3.3.1.1.1" class="ltx_p"><span id="S4.F4.pic1.3.3.3.3.1.1.1.1" class="ltx_text ltx_font_bold">HotPot QA</span></span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 15.81 15.81)"><foreignObject width="568.39" height="244.91" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:410.8pt;">
<span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3" class="ltx_p"><span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1" class="ltx_text ltx_font_bold">Question:</span> What screenwriter with credits for “Evolution”[0/1879]e a film starring Nicolas Cage and Téa Leoni?</span>
<span id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_p"><span id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Documents:</span> <math id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\ldots" display="inline"><semantics id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\ldots</annotation></semantics></math> <span id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" class="ltx_text" style="background-color:#FFFF00;">David Weissman</span> is a <span id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" class="ltx_text" style="background-color:#FFFF00;">screenwriter</span> and director. His film credits include “The Family Man” (2000), <span id="S4.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4" class="ltx_text" style="background-color:#FFFF00;">“Evolution” (2001)</span>, and “When in Rome” (2010).</span>
<span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_p"><math id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\ldots" display="inline"><semantics id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1a"><mi mathvariant="normal" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1b"><ci id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1c">\ldots</annotation></semantics></math></span>
<span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4" class="ltx_p">The Family Man is a 2000 American romantic comedy-drama film directed by Brett Ratner, written by David Diamond and David Weissman, and starring <span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.1" class="ltx_text" style="background-color:#FFFF00;">Nicolas Cage</span> and <span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.2" class="ltx_text" style="background-color:#FFFF00;">Téa Leoni</span>. …</span>
<span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5" class="ltx_p">——————————————————————————————————————————————–</span>
<span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6" class="ltx_p"><span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.1" class="ltx_text ltx_font_bold">RAFT:</span> ##Reason: The screenwriter with credits for the film “Evolution,” starring Nicolas Cage and Téa Leoni, is David Weissman. This information is provided in the reference documents which mention David Weissman as a screenwriter with film credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome” (2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer: <span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.2" class="ltx_text" style="background-color:#B3FFB3;">David Weissman</span> ✓</span>
<span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7" class="ltx_p"><span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.1" class="ltx_text ltx_font_bold">DSF:</span> <span id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.2" class="ltx_text" style="background-color:#FFB3B3;">The Family Man</span> ✗</span>
</span></foreignObject></g></g></g></svg>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.3.1.1" style="font-size:90%;">Figure 4</span>:</span><span class="ltx_text ltx_font_bold" id="S4.F4.4.2" style="font-size:90%;">Comparison of RAFT  and DSF<span class="ltx_text ltx_font_medium" id="S4.F4.4.2.1">: We prompt RAFT  and DSF fine-tuned models on the HotpotQA dataset. 우리는 DSF 모델이 문맥에서 잘못된 정보를 추출한다는 것을 알 수 있다. 질문에 대해, 누가 시나리오 작가인지, 그것은 영화명으로 응답한다. RAFT는 정확하게 결과를 얻을 수 있다. </span></span></figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Should we train the LLM always with the oracle context for RAG?</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p1.1">대규모 언어 모델(LLM)이 항상 검색 증강 생성(RAG)을 위한 오라클 컨텍스트로 훈련되어야 하는지에 대한 탐색에서, 우리는 핵심 질문을 다룬다: 훈련 데이터의 어떤 비율(p%)이 오라클 문서를 포함해야 하는가? 직관적으로, 컨텍스트(예를 들어, RAG 작업)로부터 정보를 읽고 추출하는 효과적인 훈련을 위해, 오라클 문서는 훈련 동안 항상 포함되어야 한다고 가정할 수 있다(P = 100%). 그러나 우리의 연구 결과는 맥락(P = 80%)에서 오라클 문서 없이 훈련 데이터의 일부를 통합하는 것이 RAG 작업에 대한 모델의 성능을 향상시키는 것으로 보인다는 이러한 가정에 도전한다.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS5.p2.1">Fig. <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.5 Should we train the LLM always with the oracle context for RAG? ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">5</span></a>는 오라클 문서를 포함해야 하는 훈련 인스턴스의 백분율을 나타내는 하이퍼파라미터 P%에 대한 우리의 조사를 제시한다. 우리의 분석에서는 최적 비율이 40%, 60% 및 100% 범위의 수치로 데이터 세트에 따라 다르다는 것을 보여준다. 이는 때때로 올바른 해당 컨텍스트 없이 LLM을 교육하는 것이 문서와 관련된 질문에 답하는 다운스트림 작업에 도움이 될 수 있음을 나타냅니다. 훈련 설정에서는 오라클 문서와 함께 4개의 산만기 문서를 포함하고 테스트 시간에는 오라클 문서에 4개의 산만기를 제공하여 이 형식을 유지한다. 우리의 연구 결과는 도메인 특정 RAG 작업의 경우 컨텍스트에 오라클 문서가 없는 특정 비율의 훈련 데이터를 포함하는 것이 유리하다는 것을 시사한다.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2403.10131/assets/x2.png" id="S4.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="323" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2403.10131/assets/x3.png" id="S4.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="323" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2403.10131/assets/x4.png" id="S4.F5.3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="323" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.8.2.1" style="font-size:90%;">Figure 5</span>:</span><span class="ltx_text ltx_font_bold" id="S4.F5.5.1" style="font-size:90%;">얼마나 많은 Golden documents to involve? <span class="ltx_text ltx_font_medium" id="S4.F5.5.1.1">우리는 하이퍼파라미터 <math alttext="P\%" class="ltx_Math" display="inline" id="S4.F5.5.1.1.m1.1"><semantics id="S4.F5.5.1.1.m1.1b"><mrow id="S4.F5.5.1.1.m1.1.1" xref="S4.F5.5.1.1.m1.1.1.cmml"><mi id="S4.F5.5.1.1.m1.1.1.2" xref="S4.F5.5.1.1.m1.1.1.2.cmml">P</mi><mo id="S4.F5.5.1.1.m1.1.1.1" xref="S4.F5.5.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.5.1.1.m1.1c"><apply id="S4.F5.5.1.1.m1.1.1.cmml" xref="S4.F5.5.1.1.m1.1.1"><csymbol cd="latexml" id="S4.F5.5.1.1.m1.1.1.1.cmml" xref="S4.F5.5.1.1.m1.1.1.1">percent</csymbol><ci id="S4.F5.5.1.1.m1.1.1.2.cmml" xref="S4.F5.5.1.1.m1.1.1.2">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.5.1.1.m1.1d">P\%</annotation></semantics></math>를 연구하는데, 이는 트레이닝 데이터의 어떤 부분이 그 문맥에서 oracle 문서(들)를 포함하는지를 나타낸다. NQ, TQA 및 HotpotQA에 대한 결과는 맥락에 오라클 문서가 없는 데이터의 일부를 혼합하는 것이 도메인 내 RAG에 도움이 된다는 것을 시사한다. </span></span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>RAFT Generalizes to Top-K RAG</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.1">다양한 벤치마크에서 RAFT의 성능을 입증한 후, 우리는 이제 또 다른 중요한 문제를 연구한다: 평가 중에 Top-k 리트리버 증강 생성(RAG) 결과로 증강될 때 RAFT의 분산기 문서의 수가 모델의 성능에 어떻게 영향을 미치는가? 이전 연구에서는 관련 없는 텍스트에 대한 LLM의 취약성을 강조했습니다(<cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="#bib.bib43" title="">2023a</a>; Weston &amp; Sukhbaatar, <a class="ltx_ref" href="#bib.bib53" title="">2023</a>; Liu et al., <a class="ltx_ref" href="#bib.bib29" title="">2023b</a>)</cite> 연구 참조). 이 문제는 높은 리콜을 보장하기 위해 top-k RAG가 테스트 시간에 자주 사용되기 때문에 LLM + RAG에 특히 중요하다. 그러한 시나리오는 모델이 관련 정보에만 초점을 맞추어 관련 없는 내용을 식별하고 무시할 수 있는 능력을 필요로 한다.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Making Model Robust to top-K RAG</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p1.1">검색 파이프라인 내에서 관련 없는 텍스트를 걸러내는 큰 언어 모델(LLM)의 능력을 향상시키는 문제를 해결하기 위해, 우리의 분석은 오라클(고관련) 문서만으로 훈련하는 것이 무의식적으로 모델의 관련 없는 정보를 분별하고 무시하는 능력을 감소시킬 수 있다는 것을 보여주었다. 이를 해결하기 위해, 우리의 알고리즘인 RAFT는 오라클 문서와 무관한 문서들을 통합하는 전략을 채택한다. 이 방법론은 학습 과정 전반에 걸쳐 통합할 부정(관련되지 않은) 문서의 이상적인 부분을 조사하고 이 학습 접근법이 테스트 단계에서 검색 증강 생성(RAG)이 직면하는 다양한 양의 문서에 얼마나 잘 적응하는지 평가하도록 촉구한다. 우리의 목표는 관련 정보와 관련 없는 정보 사이의 균형을 개선하여 관련 콘텐츠를 식별하고 활용하는 모델의 효율성을 강화하는 것이다. Sec<a class="ltx_ref" href="#S4.SS5" title="4.5 Should we train the LLM always with the oracle context for RAG? ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">4.5</span></a>는 훈련 데이터의 P%가 산만기를 포함해야 하는 것을 살펴보았지만 이 섹션에서는 테스트 시간 시나리오를 연구한다.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Training with Negative Documents</span> 검색된 문서의 관련 없는 텍스트에 대한 큰 언어 모델(LLMs)의 견고성을 향상시키기 위해 골든(highly relevant) 문서와 분산기(irrelevant) 문서를 모두 통합하는 파인튜닝 접근법을 채택했다. 모델은 다양한 수의 분산기 문서로 훈련되었지만, <math alttext="p" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">p</annotation></semantics></math>와 혼동되지 않도록 리트리버에서 얻은 top-k 문서를 사용하여 일관되게 평가되었다.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p3.2">우리의 연구 결과는 그림 1에 자세히 설명되어 있다. <a class="ltx_ref" href="#S5.F6" title="Figure 6 ‣ 5.1 Making Model Robust to top-K RAG ‣ 5 RAFT Generalizes to Top-K RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">6</span></a>는 오라클 문서만으로 미세 조정하면 더 많은 수의 산만기 문서를 포함하는 구성에 비해 성능이 떨어지는 경우가 많다는 것을 보여준다. 그림에서 볼 수 있듯이, 자연 질의에 대한 더 나은 성능은 <math alttext="D^{*}+3D" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><msup id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2.2" xref="S5.SS1.p3.1.m1.1.1.2.2.cmml">D</mi><mo id="S5.SS1.p3.1.m1.1.1.2.3" xref="S5.SS1.p3.1.m1.1.1.2.3.cmml">∗</mo></msup><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">+</mo><mrow id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml"><mn id="S5.SS1.p3.1.m1.1.1.3.2" xref="S5.SS1.p3.1.m1.1.1.3.2.cmml">3</mn><mo id="S5.SS1.p3.1.m1.1.1.3.1" lspace="0em" rspace="0em" xref="S5.SS1.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p3.1.m1.1.1.3.3" xref="S5.SS1.p3.1.m1.1.1.3.3.cmml">D</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><plus id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></plus><apply id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.1.1.2.1.cmml" xref="S5.SS1.p3.1.m1.1.1.2">superscript</csymbol><ci id="S5.SS1.p3.1.m1.1.1.2.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2.2">𝐷</ci><times id="S5.SS1.p3.1.m1.1.1.2.3.cmml" xref="S5.SS1.p3.1.m1.1.1.2.3"></times></apply><apply id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3"><times id="S5.SS1.p3.1.m1.1.1.3.1.cmml" xref="S5.SS1.p3.1.m1.1.1.3.1"></times><cn id="S5.SS1.p3.1.m1.1.1.3.2.cmml" type="integer" xref="S5.SS1.p3.1.m1.1.1.3.2">3</cn><ci id="S5.SS1.p3.1.m1.1.1.3.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">D^{*}+3D</annotation></semantics></math>로 훈련하는 것이고, Hotpot QA가 있는 <math alttext="D^{*}+1D" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mrow id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><msup id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml"><mi id="S5.SS1.p3.2.m2.1.1.2.2" xref="S5.SS1.p3.2.m2.1.1.2.2.cmml">D</mi><mo id="S5.SS1.p3.2.m2.1.1.2.3" xref="S5.SS1.p3.2.m2.1.1.2.3.cmml">∗</mo></msup><mo id="S5.SS1.p3.2.m2.1.1.1" xref="S5.SS1.p3.2.m2.1.1.1.cmml">+</mo><mrow id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3.cmml"><mn id="S5.SS1.p3.2.m2.1.1.3.2" xref="S5.SS1.p3.2.m2.1.1.3.2.cmml">1</mn><mo id="S5.SS1.p3.2.m2.1.1.3.1" lspace="0em" rspace="0em" xref="S5.SS1.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="S5.SS1.p3.2.m2.1.1.3.3" xref="S5.SS1.p3.2.m2.1.1.3.3.cmml">D</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><plus id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1.1"></plus><apply id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p3.2.m2.1.1.2.1.cmml" xref="S5.SS1.p3.2.m2.1.1.2">superscript</csymbol><ci id="S5.SS1.p3.2.m2.1.1.2.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2.2">𝐷</ci><times id="S5.SS1.p3.2.m2.1.1.2.3.cmml" xref="S5.SS1.p3.2.m2.1.1.2.3"></times></apply><apply id="S5.SS1.p3.2.m2.1.1.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3"><times id="S5.SS1.p3.2.m2.1.1.3.1.cmml" xref="S5.SS1.p3.2.m2.1.1.3.1"></times><cn id="S5.SS1.p3.2.m2.1.1.3.2.cmml" type="integer" xref="S5.SS1.p3.2.m2.1.1.3.2">1</cn><ci id="S5.SS1.p3.2.m2.1.1.3.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">D^{*}+1D</annotation></semantics></math> 문서이다. 이 통찰력은 우리의 알고리즘인 RAFT에 특히 유익했다. 실험에서는 일반적으로 1개의 오라클 문서와 4개의 산만기 문서로 구성된 훈련 설정을 사용한다. 이 접근법은 적절한 정보를 효과적으로 분별하고 우선순위를 정할 수 있는 능력을 얻으면서도 모델이 산만해지지 않도록 균형을 맞춘다.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Generalization to a variable number of test-time documents. </span> 다양한 양의 테스트 시간 문서가 모델의 성능에 미치는 영향을 조사하기 위해 연구를 확장했습니다. 특히, 다양한 수의 분산기 문서로 훈련된 모델이 테스트 시간에 제시된 문서 수의 변화에 어떻게 반응하는지 평가하는 데 중점을 두었다.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p5.1">결과는 그림 1에 나와 있다. <a class="ltx_ref" href="#S5.F6" title="Figure 6 ‣ 5.1 Making Model Robust to top-K RAG ‣ 5 RAFT Generalizes to Top-K RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">6</span></a> 트레이닝 중에 산만기 문서를 포함하면 실제로 모델이 테스트 중에 접하게 되는 문서 수의 변동에 더 탄력적으로 작용한다는 것을 확인시켜준다. 테스트 시간 문서 번호의 변화에도 불구하고 일관된 성능을 유지하는 이 기능은 우리의 접근 방식인 RAFT의 견고성을 더욱 검증한다. 이 발견은 실제 애플리케이션에서 직면할 수 있는 다양한 시나리오에 대한 모델을 준비하기 위해 잘 보정된 훈련 환경의 중요성을 강조한다.</p>
</div>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2403.10131/assets/x5.png" id="S5.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="323" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="https://ar5iv.labs.arxiv.org/html/2403.10131/assets/x6.png" id="S5.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="316" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.6.1.1" style="font-size:90%;">그림 6</span>:</span><span class="ltx_text ltx_font_bold" id="S5.F6.7.2" style="font-size:90%;">Test-Time Documents Varying<span class="ltx_text ltx_font_medium" id="S5.F6.7.2.1">: Retriever가 제공할 수 있는 다양한 수의 테스트 시간 문서에 대한 강력한 RAFT 방법을 연구합니다. NQ에서는 4개의 문서로 훈련하는 것이 최상의 성능으로 이어지지만 HotpotQA에서는 2개의 문서로 훈련하는 것이 최적이다. 그러나 두 데이터 세트 모두에서 <em class="ltx_emph ltx_font_italic" id="S5.F6.7.2.1.1">oracle</em> 문서로 구성된 모든 데이터 세트를 사용한 훈련은 성능에 영향을 줍니다. </span></span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Works</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text ltx_font_bold" id="S6.p1.1.1">Retrieval-Augmented Language Models</span> RAG는 언어 모델링 <cite class="ltx_cite ltx_citemacro_citep">(Guu et al., <a class="ltx_ref" href="#bib.bib12" title="">2020</a>; Borgeaud et al., <a class="ltx_ref" href="#bib.bib4" title="">2022</a>; Khandelwal et al., <a class="ltx_ref" href="#bib.bib20" title="">2019</a>; Shi et al., <a class="ltx_ref" href="#bib.bib46" title="">2023d</a>; Lin et al., <a class="ltx_ref" href="#bib.bib27" title="">2023b</a>; Shi et al., <a class="ltx_ref" href="#bib.bib45" title="">2023c</a>; Asai et al., <a class="ltx_ref" href="#bib.bib3" title="">2023</a>; Xu et al., <a class="ltx_ref" href="#bib.bib56" title="">2023</a>; Wang et al., <a class="ltx_ref" href="#bib.bib50" title="">2023</a>)</cite> 및 오픈 도메인 질문 응답 <cite class="ltx_cite ltx_citemacro_citep">(Izacard et al., <a class="ltx_ref" href="#bib.bib15" title="">2023</a>; Lewis et al., <a class="ltx_ref" href="#bib.bib24" title="">2020</a>)</cite>를 포함하여 다양한 NLP 작업에 걸쳐 성능을 크게 향상시키면서 외부 지식 베이스에서 관련 정보를 제공하는 검색 모듈을 통합하여 언어 모델을 향상시킵니다. 이 통합은 검색 모듈이 외부 소스로부터 추가 컨텍스트를 제공하는 "검색 및 읽기" 패러다임을 따르며, 그 다음 LM은 최종 출력을 생성하는 데 사용한다. 검색 프로세스는 입력을 질의로 사용하여 문서를 가져오는 것을 포함하며, LM은 최종 예측을 위해 통합한다. 예를 들어 Atlas<cite class="ltx_cite ltx_citemacro_citep">(Izacard et al., <a class="ltx_ref" href="#bib.bib15" title="">2023</a>)</cite> fine-tunes T5 model with retriever, treating documents as latent variables, and RETRO<cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et al., <a class="ltx_ref" href="#bib.bib4" title="">2022</a>)</cite> modify the decoder-only architecture to include retrieved texts and conduct pre-training from scratch. kNN-LM <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et al., <a class="ltx_ref" href="#bib.bib20" title="">2019</a>)</cite>는 LM 다음 토큰 분포와 추론 시 검색된 토큰에서 계산된 분포를 보간합니다. <cite class="ltx_cite ltx_citemacro_citep">(Shi et al., <a class="ltx_ref" href="#bib.bib46" title="">2023d</a>; Ram et al., <a class="ltx_ref" href="#bib.bib41" title="">2023</a>)</cite>는 LM에 대한 블랙박스 액세스를 가정하고 off-the-shelf 또는 fine-tuned retriever와 결합합니다.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Memorization</span> 대형 신경 언어 모델 주변의 핵심 질문은 그들이 진정으로 "이해" 텍스트  <cite class="ltx_cite ltx_citemacro_citep">(Feldman, <a class="ltx_ref" href="#bib.bib11" title="">2020</a>; Power et al., <a class="ltx_ref" href="#bib.bib39" title="">2022</a>)</cite>인지 아니면 단순히 표면 패턴 암기  <cite class="ltx_cite ltx_citemacro_citep">(Carlini et al., <a class="ltx_ref" href="#bib.bib6" title="">2019</a>; Tänzer et al., <a class="ltx_ref" href="#bib.bib47" title="">2022</a>)</cite>인지에 관한 것이다. <cite class="ltx_cite ltx_citemacro_citep">(Feldman, <a class="ltx_ref" href="#bib.bib11" title="">2020</a>; Carlini et al., <a class="ltx_ref" href="#bib.bib6" title="">2019</a>, <a class="ltx_ref" href="#bib.bib8" title="">2022</a>)</cite>는 신경 모델의 암기 정도를 정량화하는 방법론을 개발한다. <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib5" title="">2020</a>; Power et al., <a class="ltx_ref" href="#bib.bib39" title="">2022</a>; Liu et al., <a class="ltx_ref" href="#bib.bib31" title="">2022b</a>)</cite>는 암기가 모델의 일반화 능력에 어떤 영향을 미치는지 추가로 탐구했다. 최근 <cite class="ltx_cite ltx_citemacro_citep">(Carlini et al., <a class="ltx_ref" href="#bib.bib7" title="">2021</a>; Shi et al., <a class="ltx_ref" href="#bib.bib44" title="">2023b</a>)</cite>의 중요한 작업은 언어 모델이 훈련 데이터를 암기하고 역류하는 능력을 입증하여 상당한 프라이버시 염려 <cite class="ltx_cite ltx_citemacro_citep">(Kandpal et al., <a class="ltx_ref" href="#bib.bib19" title="">2022</a>; Pan et al., <a class="ltx_ref" href="#bib.bib37" title="">2020</a>)</cite>를 제기하였다.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">LLMs</span> 최근 몇 년 동안 대규모 언어 모델(LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="#bib.bib5" title="">2020</a>; OpenAI, <a class="ltx_ref" href="#bib.bib35" title="">2023</a>; Workshop et al., <a class="ltx_ref" href="#bib.bib54" title="">2022</a>; Touvron et al., <a class="ltx_ref" href="#bib.bib48" title="">2023</a>, <a class="ltx_ref" href="#bib.bib48" title="">2023</a>; Anil et al., <a class="ltx_ref" href="#bib.bib1" title="">2023</a>)</cite> 개발에 빠른 진전이 있었다. 이러한 기반 모델을 다운스트림 작업에 적용하기 위해 미세 조정 <cite class="ltx_cite ltx_citemacro_citep">(Mishra et al., <a class="ltx_ref" href="#bib.bib33" title="">2021</a>; Sanh et al., <a class="ltx_ref" href="#bib.bib42" title="">2021</a>; Chung et al., <a class="ltx_ref" href="#bib.bib9" title="">2022</a>; Muennighoff et al., <a class="ltx_ref" href="#bib.bib34" title="">2023</a>; Zhou et al., <a class="ltx_ref" href="#bib.bib60" title="">2023b</a>; Lin et al., <a class="ltx_ref" href="#bib.bib27" title="">2023b</a>; Ji et al., <a class="ltx_ref" href="#bib.bib16" title="">2024</a>)</cite>가 널리 퍼진 접근법이 되었다. 전통적인 감독 미세 조정은 LLM을 어댑팅하는 데 필요한 비용 및 계산에 의해 제한될 수 있다. 이러한 문제를 해결하기 위해 Prompt Tuning <cite class="ltx_cite ltx_citemacro_citep">(Lester et al., <a class="ltx_ref" href="#bib.bib23" title="">2021</a>)</cite>, Prefix-Tuning <cite class="ltx_cite ltx_citemacro_citep">(Li &amp; Liang, <a class="ltx_ref" href="#bib.bib25" title="">2021</a>)</cite>, P-Tuning <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="#bib.bib30" title="">2022a</a>)</cite> 및 Low-Rank 기반 Fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="#bib.bib14" title="">2021</a>)</cite>와 같은 매개변수 효율적인 Fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Houlsby et al., <a class="ltx_ref" href="#bib.bib13" title="">2019</a>)</cite> 영역에 대한 연구가 주목을 받고 있다. 이러한 방법을 통해 LLMs은 도메인별 지식을 습득하고 질의 응답, 요약, 대화 생성과 같은 전문화된 작업에 적응할 수 있다. 미세조정의 또 다른 분기는 RLHF<cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="#bib.bib36" title="">2022</a>; Rafailov et al., <a class="ltx_ref" href="#bib.bib40" title="">2023</a>; Liu et al., <a class="ltx_ref" href="#bib.bib28" title="">2023a</a>; Zhang et al., <a class="ltx_ref" href="#bib.bib58" title="">2023</a>)</cite>를 통해 LL의 선호도를 인간과 정렬하기 위해 RL을 채택한다.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_bold" id="S6.p4.1.1">Finetuning for RAG</span> More recently, several papers has exploring the idea of finetuning a pretrained LLM to be better at RAG tasks <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="#bib.bib26" title="">2023a</a>; Wang et al., <a class="ltx_ref" href="#bib.bib50" title="">2023</a>; Xu et al., <a class="ltx_ref" href="#bib.bib56" title="">2023</a>; Liu et al., <a class="ltx_ref" href="#bib.bib32" title="">2024</a>)</cite>. 이러한 작업은 RAG에 대한 미세 조정 데이터 세트의 조합을 구성하고 이러한 작업에 대해 잘 수행할 수 있도록 모델을 훈련하는 데 중점을 둔다. 특히, 테스트 시간에서 도메인 또는 문서는 훈련 시간과 다를 수 있지만, 본 논문은 동일한 문서 집합에 대한 LLM 테스트에만 신경을 쓰는 약간 반대 시나리오를 연구한다.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p" id="S7.p1.1">RAFT는 "오픈 북" 설정에서 특정 도메인 내에서 질문에 응답하는 모델의 성능을 향상시키기 위해 설계된 훈련 전략이다. 이 기술은 선택된 문서 모음을 기반으로 한 질문 응답 작업에 대한 LLM에 대한 미세 조정 레시피를 보여준다. 우리는 주의를 산만하게 하는 문서와 함께 모델을 훈련하고, 데이터 세트를 컨텍스트에 오라클 문서가 부족하도록 조직하고, 관련 텍스트의 직접적인 인용을 사용하여 연쇄적인 방식으로 답변을 공식화하는 것과 같은 몇 가지 중요한 설계 결정을 정확하게 지적했습니다. PubMed, HotpotQA 및 고릴라 API Bench에 대한 우리의 평가는 RAFT의 중요한 잠재력을 강조한다. 향후 RAG(In-domain Retrieval Augmented Generation)는 산업 분야와 학술 분야 모두에서 지속적으로 관심을 가질 것으로 기대한다. 일반 RAG와 달리 본 연구는 LLM이 도메인별 지식을 사용하여 질문에 답하는 작업을 수행하는 실제 시나리오를 다룬다. 현재 추세와 일치하게, 우리의 연구 결과는 더 작고 미세 조정된 모델이 일반적인 LLM 대응물과 대조적으로 도메인별 질문 응답 작업에서 비교적 잘 수행할 수 있음을 시사한다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Anil, R., Dai, A.&nbsp;M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et&nbsp;al.

</span>
<span class="ltx_bibblock">Palm 2 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.10403</em>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Prompt engineering for claude’s long context window.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Self-rag: Learning to retrieve, generate, and critique through self-reflection.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.11511</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den&nbsp;Driessche, G.&nbsp;B., Lespiau, J.-B., Damoc, B., Clark, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pp.&nbsp; 2206–2240. PMLR, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.&nbsp;D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., and Song, D.

</span>
<span class="ltx_bibblock">The secret sharer: Evaluating and testing unintended memorization in neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">28th USENIX Security Symposium (USENIX Security 19)</em>, pp.&nbsp; 267–284, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et&nbsp;al.

</span>
<span class="ltx_bibblock">Extracting training data from large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">30th USENIX Security Symposium (USENIX Security 21)</em>, pp.&nbsp; 2633–2650, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C.

</span>
<span class="ltx_bibblock">Quantifying memorization across neural language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Chung, H.&nbsp;W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.11416</em>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dernoncourt &amp; Lee (2017)</span>
<span class="ltx_bibblock">
Dernoncourt, F. and Lee, J.&nbsp;Y.

</span>
<span class="ltx_bibblock">Pubmed 200k rct: a dataset for sequential sentence classification in medical abstracts.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.06071</em>, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feldman (2020)</span>
<span class="ltx_bibblock">
Feldman, V.

</span>
<span class="ltx_bibblock">Does learning require memorization? a short tale about a long tail.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing</em>, pp.&nbsp; 954–959, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.

</span>
<span class="ltx_bibblock">Retrieval augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pp.&nbsp; 3929–3938. PMLR, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De&nbsp;Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp.&nbsp; 2790–2799. PMLR, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Hu, E.&nbsp;J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.

</span>
<span class="ltx_bibblock">Atlas: Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 24(251):1–43, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://jmlr.org/papers/v24/23-0037.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://jmlr.org/papers/v24/23-0037.html</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Ji, C. C.-J., Mao, H., Yan, F., Shishir G.&nbsp;Patil, T.&nbsp;Z., Stoica, I., and Gonzalez, J.&nbsp;E.

</span>
<span class="ltx_bibblock">Gorilla openfunctions v2.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Jin, Q., Dhingra, B., Liu, Z., Cohen, W.&nbsp;W., and Lu, X.

</span>
<span class="ltx_bibblock">Pubmedqa: A dataset for biomedical research question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.06146</em>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Joshi, M., Choi, E., Weld, D.&nbsp;S., and Zettlemoyer, L.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1705.03551</em>, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kandpal et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Kandpal, N., Wallace, E., and Raffel, C.

</span>
<span class="ltx_bibblock">Deduplicating training data mitigates privacy risks in language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp.&nbsp; 10697–10707. PMLR, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M.

</span>
<span class="ltx_bibblock">Generalization through memorization: Nearest neighbor language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00172</em>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et&nbsp;al.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 7:453–466, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lazaridou et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N.

</span>
<span class="ltx_bibblock">Internet-augmented language models through few-shot prompting for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.05115</em>, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Lester, B., Al-Rfou, R., and Constant, N.

</span>
<span class="ltx_bibblock">The power of scale for parameter-efficient prompt tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.08691</em>, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et&nbsp;al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:9459–9474, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li &amp; Liang (2021)</span>
<span class="ltx_bibblock">
Li, X.&nbsp;L. and Liang, P.

</span>
<span class="ltx_bibblock">Prefix-tuning: Optimizing continuous prompts for generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00190</em>, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Lin, X.&nbsp;V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01352</em>, 2023a.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Lin, X.&nbsp;V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01352</em>, 2023b.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Liu, H., Sferrazza, C., and Abbeel, P.

</span>
<span class="ltx_bibblock">Chain of hindsight aligns language models with feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.02676</em>, 3, 2023a.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Liu, N.&nbsp;F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.03172</em>, 2023b.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., and Tang, J.

</span>
<span class="ltx_bibblock">P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pp.&nbsp; 61–68, 2022a.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Liu, Z., Kitouni, O., Nolte, N.&nbsp;S., Michaud, E., Tegmark, M., and Williams, M.

</span>
<span class="ltx_bibblock">Towards understanding grokking: An effective theory of representation learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:34651–34663, 2022b.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Liu, Z., Ping, W., Roy, R., Xu, P., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Chatqa: Building gpt-4 level conversational qa models.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.10225</em>, 2024.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Cross-task generalization via natural language crowdsourcing instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.08773</em>, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Le&nbsp;Scao, T., Bari, M.&nbsp;S., Shen, S., Yong, Z.&nbsp;X., Schoelkopf, H., Tang, X., Radev, D., Aji, A.&nbsp;F., Almubarak, K., Albanie, S., Alyafeai, Z., Webson, A., Raff, E., and Raffel, C.

</span>
<span class="ltx_bibblock">Crosslingual generalization through multitask finetuning.

</span>
<span class="ltx_bibblock">In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.&nbsp; 15991–16111, Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.acl-long.891</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.acl-long.891" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.acl-long.891</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:27730–27744, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Pan, X., Zhang, M., Ji, S., and Yang, M.

</span>
<span class="ltx_bibblock">Privacy risks of general-purpose language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">2020 IEEE Symposium on Security and Privacy (SP)</em>, pp.&nbsp; 1314–1331. IEEE, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patil et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Patil, S.&nbsp;G., Zhang, T., Wang, X., and Gonzalez, J.&nbsp;E.

</span>
<span class="ltx_bibblock">Gorilla: Large language model connected with massive apis.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15334</em>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Power et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Power, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V.

</span>
<span class="ltx_bibblock">Grokking: Generalization beyond overfitting on small algorithmic datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.02177</em>, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C.&nbsp;D., and Finn, C.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18290</em>, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham, Y.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.00083</em>, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Sanh, V., Webson, A., Raffel, C., Bach, S.&nbsp;H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T.&nbsp;L., Raja, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.08207</em>, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E.&nbsp;H., Schärli, N., and Zhou, D.

</span>
<span class="ltx_bibblock">Large language models can be easily distracted by irrelevant context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp.&nbsp; 31210–31227. PMLR, 2023a.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L.

</span>
<span class="ltx_bibblock">Detecting pretraining data from large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.16789</em>, 2023b.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Shi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V., Smith, N.&nbsp;A., Zettlemoyer, L., Yih, S., and Lewis, M.

</span>
<span class="ltx_bibblock">In-context pretraining: Language modeling beyond document boundaries.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.10638</em>, 2023c.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023d)</span>
<span class="ltx_bibblock">
Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.

</span>
<span class="ltx_bibblock">Replug: Retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12652</em>, 2023d.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tänzer et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Tänzer, M., Ruder, S., and Rei, M.

</span>
<span class="ltx_bibblock">Memorisation versus generalisation in pre-trained language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.&nbsp; 7564–7578, 2022.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Vu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le, Q., et&nbsp;al.

</span>
<span class="ltx_bibblock">Freshllms: Refreshing large language models with search engine augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03214</em>, 2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Wang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Instructretro: Instruction tuning post retrieval-augmented pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.07713</em>, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.&nbsp;A., Khashabi, D., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language models with self-generated instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.10560</em>, 2022.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.&nbsp;V., Zhou, D., et&nbsp;al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:24824–24837, 2022.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weston &amp; Sukhbaatar (2023)</span>
<span class="ltx_bibblock">
Weston, J. and Sukhbaatar, S.

</span>
<span class="ltx_bibblock">System 2 attention (is something you might need too).

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.11829</em>, 2023.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Workshop et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Workshop, B., Scao, T.&nbsp;L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A.&nbsp;S., Yvon, F., et&nbsp;al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05100</em>, 2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K.&nbsp;A., Oguz, B., et&nbsp;al.

</span>
<span class="ltx_bibblock">Effective long-context scaling of foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16039</em>, 2023.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Retrieval meets long context large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03025</em>, 2023.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.&nbsp;W., Salakhutdinov, R., and Manning, C.&nbsp;D.

</span>
<span class="ltx_bibblock">Hotpotqa: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.09600</em>, 2018.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhang, T., Liu, F., Wong, J., Abbeel, P., and Gonzalez, J.&nbsp;E.

</span>
<span class="ltx_bibblock">The wisdom of hindsight makes language models better instruction followers.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.05206</em>, 2023.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et&nbsp;al.

</span>
<span class="ltx_bibblock">Lima: Less is more for alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11206</em>, 2023a.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et&nbsp;al.

</span>
<span class="ltx_bibblock">Lima: Less is more for alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11206</em>, 2023b.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2403.10128" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2403.10131" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2403.10131">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.10131" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2403.10132" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 14:06:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
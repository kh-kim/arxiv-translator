<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>RAFT: Adapting Language Model to Domain Specific RAG</title>
<!--Generated on Fri Mar 15 09:29:36 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Machine Learning,  ICML" lang="en" name="keywords">
<base href="/html/2403.10131v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2403.10131v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2403.10131v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2403.10131v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S1" title="1 Introduction ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S2" title="2 LLMs for Open-Book Exam ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>LLMs for Open-Book Exam</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S2.SS0.SSS0.Px1" title="Closed-Book Exam ‣ 2 LLMs for Open-Book Exam ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title">Closed-Book Exam</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S2.SS0.SSS0.Px2" title="Open Book Exam ‣ 2 LLMs for Open-Book Exam ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title">Open Book Exam</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S2.SS0.SSS0.Px3" title="Domain Specific Open-Book Exam ‣ 2 LLMs for Open-Book Exam ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title">Domain Specific Open-Book Exam</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S3" title="3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>RAFT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S4" title="4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S4.SS1" title="4.1 Datasets ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S4.SS1.SSS0.Px1" title="Baselines ‣ 4.1 Datasets ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S4.SS2" title="4.2 Results ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S4.SS3" title="4.3 Effect of CoT ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Effect of CoT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S4.SS4" title="4.4 Qualitative Analysis ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Qualitative Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S4.SS5" title="4.5 Should we train the LLM always with the oracle context for RAG? ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Should we train the LLM always with the oracle context for RAG?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S5" title="5 RAFT Generalizes to Top-K RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>RAFT Generalizes to Top-K RAG</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S5.SS1" title="5.1 Making Model Robust to top-K RAG ‣ 5 RAFT Generalizes to Top-K RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Making Model Robust to top-K RAG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S6" title="6 Related Works ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S7" title="7 Conclusion ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2403.10131v1 [cs.CL] 15 Mar 2024</div></div>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">RAFT: Adapting Language Model to Domain Specific RAG</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianjun Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shishir G. Patil
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Naman Jain
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sheng Shen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matei Zaharia
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ion Stoica
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joseph E. Gonzalez
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<h1 class="ltx_title ltx_title_document">RAFT: Adapting Language Model to Domain Specific RAG</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianjun Zhang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shishir G. Patil
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Naman Jain
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sheng Shen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matei Zaharia
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ion Stoica
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joseph E. Gonzalez
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id1.id1">Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm.
When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or finetuning.
However, the optimal methodology for the model to gain such new knowledge remains an open question.
In this paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe that improves the model’s ability to answer questions in an "open-book" in-domain setting. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT’s chain-of-thought-style response helps improve the model’s ability to reason. In domain specific RAG, RAFT consistently improves the model’s performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT’s code and demo are open-sourced at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ShishirPatil/gorilla" title="">https://github.com/ShishirPatil/gorilla</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="150" id="S0.F1.g1" src="x1.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S0.F1.4.2" style="font-size:90%;">How best to prepare for an Exam?<span class="ltx_text ltx_font_medium" id="S0.F1.4.2.1">(a) Fine-tuning based approaches implement "studying" by either directly "memorizing" the input documents or answering practice QA without referencing the documents. (b) Alternatively, in-context retrieval methods fail to leverage the learning opportunity afforded by the fixed domain and are equivalent to taking an open-book exam without studying. While these approaches leverage in-domain learning, they fail to prepare for open-book tests. In contrast, our approach (c) RAFT leverages fine-tuning with question-answer pairs while referencing the documents in a simulated imperfect retrieval setting — thereby effectively preparing for the open-book exam setting. </span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<span class="ltx_ERROR undefined" id="id1">\minted@def@optcl</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">envname-P envname#1



























        
















  
















</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_align_center" id="p3">
<p class="ltx_p" id="p3.1"><span class="ltx_text ltx_font_typewriter" id="p3.1.1">tianjunz@berkeley.edu, shishirpatil@berkeley.edu</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="p3.2">UC Berkeley</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p4">
<br class="ltx_break">
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Trained on vast quantities of public data, Large Language Models LLMs have achieved significant advances in a wide range of general knowledge reasoning tasks &nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib5" title="">2020</a>; Wei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib52" title="">2022</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, increasingly LLMs are being employed in specialized domains to support tasks ranging from code completion for specific software frameworks to question answering on specific document collections (e.g., legal or medical documents).
In these
settings, general knowledge reasoning is less critical but instead, the primary goal is to maximize accuracy based on a given set of documents.
Indeed, adapting LLMs to the specialized domains (e.g., recent news, enterprise private documents, or program resources constructed after the training cutoff) is essential to many emerging applications&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib49" title="">2023</a>; Lazaridou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib22" title="">2022</a>)</cite> and is the focus of this work.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This paper studies the following question – <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">How to adapt pre-trained LLMs for Retrieval Augmented Generation (RAG) in specialized domains?</em></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">When it comes to adapting LLMs to specialized domains, we consider the following two candidates: in-context learning through Retrieval-Augmented Generation (RAG) and supervised fine-tuning.
RAG-based methods allow the LLM to reference the documents when answering questions.
However, these methods fail to leverage the learning opportunity afforded by the fixed domain setting and early access to the test documents.
Alternatively, supervised fine-tuning offers the opportunity to learn more general patterns in the documents and better align to end tasks and user preferences&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib59" title="">2023a</a>)</cite>.
However, existing fine-tuning based approaches either fail to leverage the documents at test time (don’t incorporate RAG) or fail to account for the imperfections in the retrieval process during training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We can draw an analogy to an open-book exam.
Existing in-context retrieval methods are equivalent to taking an open-book exam without studying.
Alternatively, existing fine-tuning based approaches implement “studying" by either directly “memorizing"&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Xiong et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib55" title="">2023</a>)</cite> the input documents or answering practice questions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib51" title="">2022</a>)</cite> without referencing the documents.
While these approaches leverage in-domain learning they fail to prepare for the open-book nature of test setting.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In this paper, we study how to combine supervised fine-tuning (SFT) with retrieval augmented generation (RAG).
We propose a novel adaptation strategy – Retrieval-Augmented Fine Tuning (RAFT).
RAFT specifically addresses the challenge of fine-tuning LLMs to incorporate domain knowledge while also improving in-domain RAG performance.
RAFT aims to not only enable models to learn domain specific knowledge through fine-tuning, but also to ensure robustness against inaccurate retrievals.
This is achieved by training the models to understand the dynamics between the question posed (prompt), the domain specific documents retrieved, and the appropriate answer.
Going back to our analogy, our approach
is analogous to studying for an open-book exam by recognizing relevant, and irrelevant retrieved documents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.2">In RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate an answer (A*), where A* includes chain-of-thought&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib52" title="">2022</a>; Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib2" title="">2023</a>)</cite>, and in the presence of distractor documents (<math alttext="D_{k}" class="ltx_Math" display="inline" id="S1.p7.1.m1.1"><semantics id="S1.p7.1.m1.1a"><msub id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml"><mi id="S1.p7.1.m1.1.1.2" xref="S1.p7.1.m1.1.1.2.cmml">D</mi><mi id="S1.p7.1.m1.1.1.3" xref="S1.p7.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><apply id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p7.1.m1.1.1.1.cmml" xref="S1.p7.1.m1.1.1">subscript</csymbol><ci id="S1.p7.1.m1.1.1.2.cmml" xref="S1.p7.1.m1.1.1.2">𝐷</ci><ci id="S1.p7.1.m1.1.1.3.cmml" xref="S1.p7.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">D_{k}</annotation><annotation encoding="application/x-llamapun" id="S1.p7.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>). We explain the methodology in detail in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S3" title="3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">3</span></a> and analyze the sensitivity to the number of distractor documents (<math alttext="k" class="ltx_Math" display="inline" id="S1.p7.2.m2.1"><semantics id="S1.p7.2.m2.1a"><mi id="S1.p7.2.m2.1.1" xref="S1.p7.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p7.2.m2.1b"><ci id="S1.p7.2.m2.1.1.cmml" xref="S1.p7.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S1.p7.2.m2.1d">italic_k</annotation></semantics></math>) at train- and test- time in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S5" title="5 RAFT Generalizes to Top-K RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">5</span></a>. RAFT consistently outperforms Supervised-finetuning both with- and without- RAG across PubMed&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dernoncourt &amp; Lee, <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib10" title="">2017</a>)</cite>, HotpotQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib57" title="">2018</a>)</cite>, and HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Patil et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib38" title="">2023</a>)</cite>, presenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>LLMs for Open-Book Exam</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">To understand our goal better, we expand on our analogy between training an LLM in the real-world setting of preparing for an exam.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Closed-Book Exam</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">A closed book exam often refers to a scenario where the LLMs do not have access to any additional documents or references to answer the questions during the exam.
For LLMs, this is equivalent to the scenario, for example, in which the LLM is used as a chatbot. In this scenario, the LLM draws from the knowledge baked in during pre-training and supervised finetuning to respond to the prompt.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Open Book Exam</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">In contrast, we liken the open-book exam setting to the scenario in which the LLM can refer to external sources of information (e.g., a website or a book chapter).
In such scenarios, typically, the LLM is paired with a retriever which retrieves ‘k’ documents (or specific segments of the document) which are appended to the prompt. It is only through these documents retrieved that the LLM gains access to “new knowledge”.
As a result, we argue that the LLM’s performance in these settings, where it is trained as a general-purpose LLM is largely dependent on the quality of the retriever and how accurately the retriever can identify the most relevant piece of information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Domain Specific Open-Book Exam</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">In this paper, we focused on a narrower but increasingly popular domain than the general open book exam, called the domain specific open book exam. In domain specific open book exams, we know apriori the domain in which the LLM will be tested – used for inference. The LLM can respond to the prompt using use any and all information from this specific domain, which it has been fine-tuned on.
Examples of domain specific examples include enterprise documents, latest news, code repositories belonging to an organization, etc.
In all these scenarios, the LLM will be used to respond to the questions, whose answers can be found within a collection of documents (a small practical domain). The retrieval technique itself has little to no impact on the mechanism (though it may impact the accuracy).
This paper mainly studies this, domain specific open-book setting and how to adapt a pretrained LLM to this specific domain, including how to make it more robust to a varying number of retrieved documents and distractors.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>RAFT</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="220" id="S3.F2.g1" src="extracted/5471482/figures/RAFT.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.4.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2.5.2" style="font-size:90%;">Overview of our RAFT method.<span class="ltx_text ltx_font_medium" id="S3.F2.5.2.1"> The top-left figure depicts our approach of adapting LLMs to <span class="ltx_text ltx_font_italic" id="S3.F2.5.2.1.1">reading</span> solution from a set of positive and negative documents in contrast to standard RAG setup where models are trained based on the retriever outputs, which is a mixture of both memorization and reading. At test time, all methods follow the standard RAG setting, provided with a top-k retrieved documents in the context.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we present RAFT, a novel way of training LLMs for domain specific open-book exams. We first introduce the classical technique of supervised fine-tuning, followed by the key takeaways from our experiments. Then, we introduce RAFT&nbsp;, a modified version of general instruction tuning.
Lastly, we provide an overview of the experiments to expect in the later sections.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Supervised Finetuning</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.3">Consider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The formulation consists of the Dataset (<math alttext="D" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_D</annotation></semantics></math>) from which a set of Question (<math alttext="Q" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_Q</annotation></semantics></math>) and corresponding answer (<math alttext="A" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_A</annotation></semantics></math>) pairs are derived or already available. In the classical SFT setting, the model is trained to improve its ability to answer the questions based on its knowledge - obtained either during pre-training, or during the SFT training phase. The model so trained can also be used at test-time with the Retrieval Augmented Generation (RAG) setting, where additional documents can be introduced in the prompt to help the model answer the question. This can be represented as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p4">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.3">Train: <math alttext="{\mathbf{Q}}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><mi id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">{\mathbf{Q}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">bold_Q</annotation></semantics></math> <math alttext="\to" class="ltx_Math" display="inline" id="S3.I1.i1.p1.2.m2.1"><semantics id="S3.I1.i1.p1.2.m2.1a"><mo id="S3.I1.i1.p1.2.m2.1.1" stretchy="false" xref="S3.I1.i1.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><ci id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.2.m2.1d">→</annotation></semantics></math> <math alttext="{\mathbf{A}}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.3.m3.1"><semantics id="S3.I1.i1.p1.3.m3.1a"><mi id="S3.I1.i1.p1.3.m3.1.1" xref="S3.I1.i1.p1.3.m3.1.1.cmml">𝐀</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.1b"><ci id="S3.I1.i1.p1.3.m3.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1">𝐀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.1c">{\mathbf{A}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.3.m3.1d">bold_A</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.3">0-shot Inference: <math alttext="{\mathbf{Q}}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><mi id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">{\mathbf{Q}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">bold_Q</annotation></semantics></math> <math alttext="\to" class="ltx_Math" display="inline" id="S3.I1.i2.p1.2.m2.1"><semantics id="S3.I1.i2.p1.2.m2.1a"><mo id="S3.I1.i2.p1.2.m2.1.1" stretchy="false" xref="S3.I1.i2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.2.m2.1d">→</annotation></semantics></math> <math alttext="{\mathbf{A}}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.3.m3.1"><semantics id="S3.I1.i2.p1.3.m3.1a"><mi id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml">𝐀</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><ci id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">𝐀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">{\mathbf{A}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.3.m3.1d">bold_A</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.4">RAG Inference: <math alttext="{\mathbf{Q}}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.m1.1"><semantics id="S3.I1.i3.p1.1.m1.1a"><mi id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><ci id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">{\mathbf{Q}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.m1.1d">bold_Q</annotation></semantics></math> + <math alttext="{\mathbf{D}}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.2.m2.1"><semantics id="S3.I1.i3.p1.2.m2.1a"><mi id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml">𝐃</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><ci id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">𝐃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">{\mathbf{D}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.2.m2.1d">bold_D</annotation></semantics></math> <math alttext="\to" class="ltx_Math" display="inline" id="S3.I1.i3.p1.3.m3.1"><semantics id="S3.I1.i3.p1.3.m3.1a"><mo id="S3.I1.i3.p1.3.m3.1.1" stretchy="false" xref="S3.I1.i3.p1.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m3.1b"><ci id="S3.I1.i3.p1.3.m3.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m3.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.3.m3.1d">→</annotation></semantics></math> <math alttext="{\mathbf{A}}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.4.m4.1"><semantics id="S3.I1.i3.p1.4.m4.1a"><mi id="S3.I1.i3.p1.4.m4.1.1" xref="S3.I1.i3.p1.4.m4.1.1.cmml">𝐀</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.4.m4.1b"><ci id="S3.I1.i3.p1.4.m4.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1">𝐀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.4.m4.1c">{\mathbf{A}}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.4.m4.1d">bold_A</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">RAFT</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.13">Retrieval Aware Fine-Tuning (RAFT), presents a novel recipe to prepare fine-tuning data to tailor the models for domain specific open-book settings, equivalent to in-domain RAG In RAFT, we prepare the training data such that each data point contains a question (<math alttext="Q" class="ltx_Math" display="inline" id="S3.p6.1.m1.1"><semantics id="S3.p6.1.m1.1a"><mi id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><ci id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">Q</annotation><annotation encoding="application/x-llamapun" id="S3.p6.1.m1.1d">italic_Q</annotation></semantics></math>), a set of documents (<math alttext="D_{k}" class="ltx_Math" display="inline" id="S3.p6.2.m2.1"><semantics id="S3.p6.2.m2.1a"><msub id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml"><mi id="S3.p6.2.m2.1.1.2" xref="S3.p6.2.m2.1.1.2.cmml">D</mi><mi id="S3.p6.2.m2.1.1.3" xref="S3.p6.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><apply id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p6.2.m2.1.1.1.cmml" xref="S3.p6.2.m2.1.1">subscript</csymbol><ci id="S3.p6.2.m2.1.1.2.cmml" xref="S3.p6.2.m2.1.1.2">𝐷</ci><ci id="S3.p6.2.m2.1.1.3.cmml" xref="S3.p6.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">D_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>), and a corresponding Chain-of-though style answer (<math alttext="A^{*}" class="ltx_Math" display="inline" id="S3.p6.3.m3.1"><semantics id="S3.p6.3.m3.1a"><msup id="S3.p6.3.m3.1.1" xref="S3.p6.3.m3.1.1.cmml"><mi id="S3.p6.3.m3.1.1.2" xref="S3.p6.3.m3.1.1.2.cmml">A</mi><mo id="S3.p6.3.m3.1.1.3" xref="S3.p6.3.m3.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p6.3.m3.1b"><apply id="S3.p6.3.m3.1.1.cmml" xref="S3.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p6.3.m3.1.1.1.cmml" xref="S3.p6.3.m3.1.1">superscript</csymbol><ci id="S3.p6.3.m3.1.1.2.cmml" xref="S3.p6.3.m3.1.1.2">𝐴</ci><times id="S3.p6.3.m3.1.1.3.cmml" xref="S3.p6.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.3.m3.1c">A^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.3.m3.1d">italic_A start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math>) generated from one of the document (<math alttext="D^{*}" class="ltx_Math" display="inline" id="S3.p6.4.m4.1"><semantics id="S3.p6.4.m4.1a"><msup id="S3.p6.4.m4.1.1" xref="S3.p6.4.m4.1.1.cmml"><mi id="S3.p6.4.m4.1.1.2" xref="S3.p6.4.m4.1.1.2.cmml">D</mi><mo id="S3.p6.4.m4.1.1.3" xref="S3.p6.4.m4.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p6.4.m4.1b"><apply id="S3.p6.4.m4.1.1.cmml" xref="S3.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p6.4.m4.1.1.1.cmml" xref="S3.p6.4.m4.1.1">superscript</csymbol><ci id="S3.p6.4.m4.1.1.2.cmml" xref="S3.p6.4.m4.1.1.2">𝐷</ci><times id="S3.p6.4.m4.1.1.3.cmml" xref="S3.p6.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.4.m4.1c">D^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.4.m4.1d">italic_D start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math>). We differentiate between two types of documents: ‘oracle’ documents (<math alttext="D*" class="ltx_math_unparsed" display="inline" id="S3.p6.5.m5.1"><semantics id="S3.p6.5.m5.1a"><mrow id="S3.p6.5.m5.1b"><mi id="S3.p6.5.m5.1.1">D</mi><mo id="S3.p6.5.m5.1.2" lspace="0.222em">*</mo></mrow><annotation encoding="application/x-tex" id="S3.p6.5.m5.1c">D*</annotation><annotation encoding="application/x-llamapun" id="S3.p6.5.m5.1d">italic_D *</annotation></semantics></math>) i.e. the documents from which the answer to the question can be deduced, and ‘distractor’ documents (<math alttext="D_{i}" class="ltx_Math" display="inline" id="S3.p6.6.m6.1"><semantics id="S3.p6.6.m6.1a"><msub id="S3.p6.6.m6.1.1" xref="S3.p6.6.m6.1.1.cmml"><mi id="S3.p6.6.m6.1.1.2" xref="S3.p6.6.m6.1.1.2.cmml">D</mi><mi id="S3.p6.6.m6.1.1.3" xref="S3.p6.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.6.m6.1b"><apply id="S3.p6.6.m6.1.1.cmml" xref="S3.p6.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p6.6.m6.1.1.1.cmml" xref="S3.p6.6.m6.1.1">subscript</csymbol><ci id="S3.p6.6.m6.1.1.2.cmml" xref="S3.p6.6.m6.1.1.2">𝐷</ci><ci id="S3.p6.6.m6.1.1.3.cmml" xref="S3.p6.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.6.m6.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.6.m6.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>) that do not contain answer-relevant information. As an implementation detail, the ‘oracle’ document doesn’t need to be a single document, but can be more than one document, as is the case in HotpotQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib57" title="">2018</a>)</cite>. Then, for <math alttext="P" class="ltx_Math" display="inline" id="S3.p6.7.m7.1"><semantics id="S3.p6.7.m7.1a"><mi id="S3.p6.7.m7.1.1" xref="S3.p6.7.m7.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.p6.7.m7.1b"><ci id="S3.p6.7.m7.1.1.cmml" xref="S3.p6.7.m7.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.7.m7.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.p6.7.m7.1d">italic_P</annotation></semantics></math> fraction of the questions (<math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.p6.8.m8.1"><semantics id="S3.p6.8.m8.1a"><msub id="S3.p6.8.m8.1.1" xref="S3.p6.8.m8.1.1.cmml"><mi id="S3.p6.8.m8.1.1.2" xref="S3.p6.8.m8.1.1.2.cmml">q</mi><mi id="S3.p6.8.m8.1.1.3" xref="S3.p6.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.8.m8.1b"><apply id="S3.p6.8.m8.1.1.cmml" xref="S3.p6.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p6.8.m8.1.1.1.cmml" xref="S3.p6.8.m8.1.1">subscript</csymbol><ci id="S3.p6.8.m8.1.1.2.cmml" xref="S3.p6.8.m8.1.1.2">𝑞</ci><ci id="S3.p6.8.m8.1.1.3.cmml" xref="S3.p6.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.8.m8.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.8.m8.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>) in the dataset, we retain the oracle document (<math alttext="d_{i}^{*}" class="ltx_Math" display="inline" id="S3.p6.9.m9.1"><semantics id="S3.p6.9.m9.1a"><msubsup id="S3.p6.9.m9.1.1" xref="S3.p6.9.m9.1.1.cmml"><mi id="S3.p6.9.m9.1.1.2.2" xref="S3.p6.9.m9.1.1.2.2.cmml">d</mi><mi id="S3.p6.9.m9.1.1.2.3" xref="S3.p6.9.m9.1.1.2.3.cmml">i</mi><mo id="S3.p6.9.m9.1.1.3" xref="S3.p6.9.m9.1.1.3.cmml">*</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.p6.9.m9.1b"><apply id="S3.p6.9.m9.1.1.cmml" xref="S3.p6.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p6.9.m9.1.1.1.cmml" xref="S3.p6.9.m9.1.1">superscript</csymbol><apply id="S3.p6.9.m9.1.1.2.cmml" xref="S3.p6.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p6.9.m9.1.1.2.1.cmml" xref="S3.p6.9.m9.1.1">subscript</csymbol><ci id="S3.p6.9.m9.1.1.2.2.cmml" xref="S3.p6.9.m9.1.1.2.2">𝑑</ci><ci id="S3.p6.9.m9.1.1.2.3.cmml" xref="S3.p6.9.m9.1.1.2.3">𝑖</ci></apply><times id="S3.p6.9.m9.1.1.3.cmml" xref="S3.p6.9.m9.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.9.m9.1c">d_{i}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.9.m9.1d">italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math>) along with distractor documents (<math alttext="d_{k-1}" class="ltx_Math" display="inline" id="S3.p6.10.m10.1"><semantics id="S3.p6.10.m10.1a"><msub id="S3.p6.10.m10.1.1" xref="S3.p6.10.m10.1.1.cmml"><mi id="S3.p6.10.m10.1.1.2" xref="S3.p6.10.m10.1.1.2.cmml">d</mi><mrow id="S3.p6.10.m10.1.1.3" xref="S3.p6.10.m10.1.1.3.cmml"><mi id="S3.p6.10.m10.1.1.3.2" xref="S3.p6.10.m10.1.1.3.2.cmml">k</mi><mo id="S3.p6.10.m10.1.1.3.1" xref="S3.p6.10.m10.1.1.3.1.cmml">−</mo><mn id="S3.p6.10.m10.1.1.3.3" xref="S3.p6.10.m10.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p6.10.m10.1b"><apply id="S3.p6.10.m10.1.1.cmml" xref="S3.p6.10.m10.1.1"><csymbol cd="ambiguous" id="S3.p6.10.m10.1.1.1.cmml" xref="S3.p6.10.m10.1.1">subscript</csymbol><ci id="S3.p6.10.m10.1.1.2.cmml" xref="S3.p6.10.m10.1.1.2">𝑑</ci><apply id="S3.p6.10.m10.1.1.3.cmml" xref="S3.p6.10.m10.1.1.3"><minus id="S3.p6.10.m10.1.1.3.1.cmml" xref="S3.p6.10.m10.1.1.3.1"></minus><ci id="S3.p6.10.m10.1.1.3.2.cmml" xref="S3.p6.10.m10.1.1.3.2">𝑘</ci><cn id="S3.p6.10.m10.1.1.3.3.cmml" type="integer" xref="S3.p6.10.m10.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.10.m10.1c">d_{k-1}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.10.m10.1d">italic_d start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT</annotation></semantics></math>). For <math alttext="(1-P)" class="ltx_Math" display="inline" id="S3.p6.11.m11.1"><semantics id="S3.p6.11.m11.1a"><mrow id="S3.p6.11.m11.1.1.1" xref="S3.p6.11.m11.1.1.1.1.cmml"><mo id="S3.p6.11.m11.1.1.1.2" stretchy="false" xref="S3.p6.11.m11.1.1.1.1.cmml">(</mo><mrow id="S3.p6.11.m11.1.1.1.1" xref="S3.p6.11.m11.1.1.1.1.cmml"><mn id="S3.p6.11.m11.1.1.1.1.2" xref="S3.p6.11.m11.1.1.1.1.2.cmml">1</mn><mo id="S3.p6.11.m11.1.1.1.1.1" xref="S3.p6.11.m11.1.1.1.1.1.cmml">−</mo><mi id="S3.p6.11.m11.1.1.1.1.3" xref="S3.p6.11.m11.1.1.1.1.3.cmml">P</mi></mrow><mo id="S3.p6.11.m11.1.1.1.3" stretchy="false" xref="S3.p6.11.m11.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.11.m11.1b"><apply id="S3.p6.11.m11.1.1.1.1.cmml" xref="S3.p6.11.m11.1.1.1"><minus id="S3.p6.11.m11.1.1.1.1.1.cmml" xref="S3.p6.11.m11.1.1.1.1.1"></minus><cn id="S3.p6.11.m11.1.1.1.1.2.cmml" type="integer" xref="S3.p6.11.m11.1.1.1.1.2">1</cn><ci id="S3.p6.11.m11.1.1.1.1.3.cmml" xref="S3.p6.11.m11.1.1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.11.m11.1c">(1-P)</annotation><annotation encoding="application/x-llamapun" id="S3.p6.11.m11.1d">( 1 - italic_P )</annotation></semantics></math> fraction of the questions (<math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.p6.12.m12.1"><semantics id="S3.p6.12.m12.1a"><msub id="S3.p6.12.m12.1.1" xref="S3.p6.12.m12.1.1.cmml"><mi id="S3.p6.12.m12.1.1.2" xref="S3.p6.12.m12.1.1.2.cmml">q</mi><mi id="S3.p6.12.m12.1.1.3" xref="S3.p6.12.m12.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.12.m12.1b"><apply id="S3.p6.12.m12.1.1.cmml" xref="S3.p6.12.m12.1.1"><csymbol cd="ambiguous" id="S3.p6.12.m12.1.1.1.cmml" xref="S3.p6.12.m12.1.1">subscript</csymbol><ci id="S3.p6.12.m12.1.1.2.cmml" xref="S3.p6.12.m12.1.1.2">𝑞</ci><ci id="S3.p6.12.m12.1.1.3.cmml" xref="S3.p6.12.m12.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.12.m12.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.12.m12.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>) in the dataset, we include no oracle document and only include distractor documents (<math alttext="d_{k}" class="ltx_Math" display="inline" id="S3.p6.13.m13.1"><semantics id="S3.p6.13.m13.1a"><msub id="S3.p6.13.m13.1.1" xref="S3.p6.13.m13.1.1.cmml"><mi id="S3.p6.13.m13.1.1.2" xref="S3.p6.13.m13.1.1.2.cmml">d</mi><mi id="S3.p6.13.m13.1.1.3" xref="S3.p6.13.m13.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.13.m13.1b"><apply id="S3.p6.13.m13.1.1.cmml" xref="S3.p6.13.m13.1.1"><csymbol cd="ambiguous" id="S3.p6.13.m13.1.1.1.cmml" xref="S3.p6.13.m13.1.1">subscript</csymbol><ci id="S3.p6.13.m13.1.1.2.cmml" xref="S3.p6.13.m13.1.1.2">𝑑</ci><ci id="S3.p6.13.m13.1.1.3.cmml" xref="S3.p6.13.m13.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.13.m13.1c">d_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.13.m13.1d">italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>). We then fine-tune the language model using the standard supervised training (SFT) technique, training it to generate answers from the provided documents and questions. Fig.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S3.F2" title="Figure 2 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the high-level design principal for RAFT&nbsp;.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.1">We demonstrate that our approach trains the model to perform better RAG on the set of documents it is trained on <em class="ltx_emph ltx_font_italic" id="S3.p7.1.1">i.e., in-domain</em>. By removing the oracle documents in some instances, we are compelling the model to memorize answers instead of deriving them from the context. The training data for RAFT is as follows, and an example of training data can be seen in Fig.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S3.F3" title="Figure 3 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">3</span></a>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p8">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.8"><math alttext="{\mathbf{P}}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.1.m1.1"><semantics id="S3.I2.i1.p1.1.m1.1a"><mi id="S3.I2.i1.p1.1.m1.1.1" xref="S3.I2.i1.p1.1.m1.1.1.cmml">𝐏</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.1.m1.1b"><ci id="S3.I2.i1.p1.1.m1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1">𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.1.m1.1c">{\mathbf{P}}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.1.m1.1d">bold_P</annotation></semantics></math> % of data: <math alttext="{\mathbf{Q}}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.2.m2.1"><semantics id="S3.I2.i1.p1.2.m2.1a"><mi id="S3.I2.i1.p1.2.m2.1.1" xref="S3.I2.i1.p1.2.m2.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.2.m2.1b"><ci id="S3.I2.i1.p1.2.m2.1.1.cmml" xref="S3.I2.i1.p1.2.m2.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.2.m2.1c">{\mathbf{Q}}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.2.m2.1d">bold_Q</annotation></semantics></math> + <math alttext="{\mathbf{D}}^{*}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.3.m3.1"><semantics id="S3.I2.i1.p1.3.m3.1a"><msup id="S3.I2.i1.p1.3.m3.1.1" xref="S3.I2.i1.p1.3.m3.1.1.cmml"><mi id="S3.I2.i1.p1.3.m3.1.1.2" xref="S3.I2.i1.p1.3.m3.1.1.2.cmml">𝐃</mi><mo id="S3.I2.i1.p1.3.m3.1.1.3" xref="S3.I2.i1.p1.3.m3.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.3.m3.1b"><apply id="S3.I2.i1.p1.3.m3.1.1.cmml" xref="S3.I2.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.3.m3.1.1.1.cmml" xref="S3.I2.i1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.I2.i1.p1.3.m3.1.1.2.cmml" xref="S3.I2.i1.p1.3.m3.1.1.2">𝐃</ci><times id="S3.I2.i1.p1.3.m3.1.1.3.cmml" xref="S3.I2.i1.p1.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.3.m3.1c">{\mathbf{D}}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.3.m3.1d">bold_D start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math> + <math alttext="{\mathbf{D}}_{2}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.4.m4.1"><semantics id="S3.I2.i1.p1.4.m4.1a"><msub id="S3.I2.i1.p1.4.m4.1.1" xref="S3.I2.i1.p1.4.m4.1.1.cmml"><mi id="S3.I2.i1.p1.4.m4.1.1.2" xref="S3.I2.i1.p1.4.m4.1.1.2.cmml">𝐃</mi><mn id="S3.I2.i1.p1.4.m4.1.1.3" xref="S3.I2.i1.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.4.m4.1b"><apply id="S3.I2.i1.p1.4.m4.1.1.cmml" xref="S3.I2.i1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.4.m4.1.1.1.cmml" xref="S3.I2.i1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.4.m4.1.1.2.cmml" xref="S3.I2.i1.p1.4.m4.1.1.2">𝐃</ci><cn id="S3.I2.i1.p1.4.m4.1.1.3.cmml" type="integer" xref="S3.I2.i1.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.4.m4.1c">{\mathbf{D}}_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.4.m4.1d">bold_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> + <math alttext="\dots" class="ltx_Math" display="inline" id="S3.I2.i1.p1.5.m5.1"><semantics id="S3.I2.i1.p1.5.m5.1a"><mi id="S3.I2.i1.p1.5.m5.1.1" mathvariant="normal" xref="S3.I2.i1.p1.5.m5.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.5.m5.1b"><ci id="S3.I2.i1.p1.5.m5.1.1.cmml" xref="S3.I2.i1.p1.5.m5.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.5.m5.1c">\dots</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.5.m5.1d">…</annotation></semantics></math> + <math alttext="{\mathbf{D}}_{k}" class="ltx_Math" display="inline" id="S3.I2.i1.p1.6.m6.1"><semantics id="S3.I2.i1.p1.6.m6.1a"><msub id="S3.I2.i1.p1.6.m6.1.1" xref="S3.I2.i1.p1.6.m6.1.1.cmml"><mi id="S3.I2.i1.p1.6.m6.1.1.2" xref="S3.I2.i1.p1.6.m6.1.1.2.cmml">𝐃</mi><mi id="S3.I2.i1.p1.6.m6.1.1.3" xref="S3.I2.i1.p1.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.6.m6.1b"><apply id="S3.I2.i1.p1.6.m6.1.1.cmml" xref="S3.I2.i1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.6.m6.1.1.1.cmml" xref="S3.I2.i1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.6.m6.1.1.2.cmml" xref="S3.I2.i1.p1.6.m6.1.1.2">𝐃</ci><ci id="S3.I2.i1.p1.6.m6.1.1.3.cmml" xref="S3.I2.i1.p1.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.6.m6.1c">{\mathbf{D}}_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.6.m6.1d">bold_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> <math alttext="\to" class="ltx_Math" display="inline" id="S3.I2.i1.p1.7.m7.1"><semantics id="S3.I2.i1.p1.7.m7.1a"><mo id="S3.I2.i1.p1.7.m7.1.1" stretchy="false" xref="S3.I2.i1.p1.7.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.7.m7.1b"><ci id="S3.I2.i1.p1.7.m7.1.1.cmml" xref="S3.I2.i1.p1.7.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.7.m7.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.7.m7.1d">→</annotation></semantics></math> <math alttext="{\mathbf{A}}*" class="ltx_math_unparsed" display="inline" id="S3.I2.i1.p1.8.m8.1"><semantics id="S3.I2.i1.p1.8.m8.1a"><mrow id="S3.I2.i1.p1.8.m8.1b"><mi id="S3.I2.i1.p1.8.m8.1.1">𝐀</mi><mo id="S3.I2.i1.p1.8.m8.1.2" lspace="0.222em">*</mo></mrow><annotation encoding="application/x-tex" id="S3.I2.i1.p1.8.m8.1c">{\mathbf{A}}*</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.8.m8.1d">bold_A *</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.8">(<math alttext="1-{\mathbf{P}}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.1.m1.1"><semantics id="S3.I2.i2.p1.1.m1.1a"><mrow id="S3.I2.i2.p1.1.m1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.cmml"><mn id="S3.I2.i2.p1.1.m1.1.1.2" xref="S3.I2.i2.p1.1.m1.1.1.2.cmml">1</mn><mo id="S3.I2.i2.p1.1.m1.1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.1.cmml">−</mo><mi id="S3.I2.i2.p1.1.m1.1.1.3" xref="S3.I2.i2.p1.1.m1.1.1.3.cmml">𝐏</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.m1.1b"><apply id="S3.I2.i2.p1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1"><minus id="S3.I2.i2.p1.1.m1.1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1.1"></minus><cn id="S3.I2.i2.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.I2.i2.p1.1.m1.1.1.2">1</cn><ci id="S3.I2.i2.p1.1.m1.1.1.3.cmml" xref="S3.I2.i2.p1.1.m1.1.1.3">𝐏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.m1.1c">1-{\mathbf{P}}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.1.m1.1d">1 - bold_P</annotation></semantics></math>) % of data: <math alttext="{\mathbf{Q}}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.2.m2.1"><semantics id="S3.I2.i2.p1.2.m2.1a"><mi id="S3.I2.i2.p1.2.m2.1.1" xref="S3.I2.i2.p1.2.m2.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.2.m2.1b"><ci id="S3.I2.i2.p1.2.m2.1.1.cmml" xref="S3.I2.i2.p1.2.m2.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.2.m2.1c">{\mathbf{Q}}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.2.m2.1d">bold_Q</annotation></semantics></math> + <math alttext="{\mathbf{D}}_{1}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.3.m3.1"><semantics id="S3.I2.i2.p1.3.m3.1a"><msub id="S3.I2.i2.p1.3.m3.1.1" xref="S3.I2.i2.p1.3.m3.1.1.cmml"><mi id="S3.I2.i2.p1.3.m3.1.1.2" xref="S3.I2.i2.p1.3.m3.1.1.2.cmml">𝐃</mi><mn id="S3.I2.i2.p1.3.m3.1.1.3" xref="S3.I2.i2.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.3.m3.1b"><apply id="S3.I2.i2.p1.3.m3.1.1.cmml" xref="S3.I2.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.p1.3.m3.1.1.1.cmml" xref="S3.I2.i2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.I2.i2.p1.3.m3.1.1.2.cmml" xref="S3.I2.i2.p1.3.m3.1.1.2">𝐃</ci><cn id="S3.I2.i2.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.I2.i2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.3.m3.1c">{\mathbf{D}}_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.3.m3.1d">bold_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> + <math alttext="{\mathbf{D}}_{2}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.4.m4.1"><semantics id="S3.I2.i2.p1.4.m4.1a"><msub id="S3.I2.i2.p1.4.m4.1.1" xref="S3.I2.i2.p1.4.m4.1.1.cmml"><mi id="S3.I2.i2.p1.4.m4.1.1.2" xref="S3.I2.i2.p1.4.m4.1.1.2.cmml">𝐃</mi><mn id="S3.I2.i2.p1.4.m4.1.1.3" xref="S3.I2.i2.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.4.m4.1b"><apply id="S3.I2.i2.p1.4.m4.1.1.cmml" xref="S3.I2.i2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.p1.4.m4.1.1.1.cmml" xref="S3.I2.i2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.I2.i2.p1.4.m4.1.1.2.cmml" xref="S3.I2.i2.p1.4.m4.1.1.2">𝐃</ci><cn id="S3.I2.i2.p1.4.m4.1.1.3.cmml" type="integer" xref="S3.I2.i2.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.4.m4.1c">{\mathbf{D}}_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.4.m4.1d">bold_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> + <math alttext="\dots" class="ltx_Math" display="inline" id="S3.I2.i2.p1.5.m5.1"><semantics id="S3.I2.i2.p1.5.m5.1a"><mi id="S3.I2.i2.p1.5.m5.1.1" mathvariant="normal" xref="S3.I2.i2.p1.5.m5.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.5.m5.1b"><ci id="S3.I2.i2.p1.5.m5.1.1.cmml" xref="S3.I2.i2.p1.5.m5.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.5.m5.1c">\dots</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.5.m5.1d">…</annotation></semantics></math> + <math alttext="{\mathbf{D}}_{k}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.6.m6.1"><semantics id="S3.I2.i2.p1.6.m6.1a"><msub id="S3.I2.i2.p1.6.m6.1.1" xref="S3.I2.i2.p1.6.m6.1.1.cmml"><mi id="S3.I2.i2.p1.6.m6.1.1.2" xref="S3.I2.i2.p1.6.m6.1.1.2.cmml">𝐃</mi><mi id="S3.I2.i2.p1.6.m6.1.1.3" xref="S3.I2.i2.p1.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.6.m6.1b"><apply id="S3.I2.i2.p1.6.m6.1.1.cmml" xref="S3.I2.i2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.p1.6.m6.1.1.1.cmml" xref="S3.I2.i2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.I2.i2.p1.6.m6.1.1.2.cmml" xref="S3.I2.i2.p1.6.m6.1.1.2">𝐃</ci><ci id="S3.I2.i2.p1.6.m6.1.1.3.cmml" xref="S3.I2.i2.p1.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.6.m6.1c">{\mathbf{D}}_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.6.m6.1d">bold_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> <math alttext="\to" class="ltx_Math" display="inline" id="S3.I2.i2.p1.7.m7.1"><semantics id="S3.I2.i2.p1.7.m7.1a"><mo id="S3.I2.i2.p1.7.m7.1.1" stretchy="false" xref="S3.I2.i2.p1.7.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.7.m7.1b"><ci id="S3.I2.i2.p1.7.m7.1.1.cmml" xref="S3.I2.i2.p1.7.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.7.m7.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.7.m7.1d">→</annotation></semantics></math> <math alttext="{\mathbf{A}}*" class="ltx_math_unparsed" display="inline" id="S3.I2.i2.p1.8.m8.1"><semantics id="S3.I2.i2.p1.8.m8.1a"><mrow id="S3.I2.i2.p1.8.m8.1b"><mi id="S3.I2.i2.p1.8.m8.1.1">𝐀</mi><mo id="S3.I2.i2.p1.8.m8.1.2" lspace="0.222em">*</mo></mrow><annotation encoding="application/x-tex" id="S3.I2.i2.p1.8.m8.1c">{\mathbf{A}}*</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.8.m8.1d">bold_A *</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.p9">
<p class="ltx_p" id="S3.p9.1">Subsequently, for the test scenario, the model is provided with the Q and top-k documents retrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p10">
<p class="ltx_p" id="S3.p10.1">A key factor in enhancing training quality is the generation of a reasoning process, such as Chain-of-Thought, to explain the provided answers.RAFT&nbsp; approach is similar: we demonstrate that creating a full reasoning chain and in addition, clearly citing sources enhances the model’s accuracy in answering questions. In Fig.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S3.F3" title="Figure 3 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">3</span></a>, we illustrate this set-up. Generating the training data in this fashion, involves presenting the model with a question, context, and verified answers, and then requesting it to form a reasoning chain that appropriately references the original context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p11">
<p class="ltx_p" id="S3.p11.1">For all the datasets in our experiments, we generate the answers using the technique described above. Note that the Gorilla APIBench dataset, already includes reasoning in the answers.
We provide an example of the generation step in Fig.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S3.F3" title="Figure 3 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">3</span></a>, the detailed reasoning answer includes a citation from the original context inside <span class="ltx_text ltx_font_typewriter" id="S3.p11.1.1">##begin_quote##</span> and <span class="ltx_text ltx_font_typewriter" id="S3.p11.1.2">##end_quote##</span> as well as the detailed explanation on how to reach the conclusion based on the citations.
We demonstrate that adding detailed reasoning paragraphs helps boost the model’s performance in our experiment section.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure ltx_align_center" id="S3.F3">
<div class="ltx_listing ltx_lstlisting ltx_listing" id="S3.F3.2">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,CnxcdGV4dGJme1F1ZXN0aW9uOn18IFRoZSBPYmVyb2kgZmFtaWx5IGlzIHBhcnQgb2YgYSBob3RlbCBjb21wYW55IHRoYXQgaGFzIGEgaGVhZCBvZmZpY2UgaW4gd2hhdCBjaXR5PwpccGFyfFx0ZXh0YmZ7Y29udGV4dDp9fCBbVGhlIE9iZXJvaSBmYW1pbHkgaXMgYW4gSW5kaWFuIGZhbWlseSB0aGF0IGlzIGZhbW91cyBmb3IgaXRzIGludm9sdmVtZW50IGluIGhvdGVscywgbmFtZWx5IHRocm91Z2ggVGhlIE9iZXJvaSBHcm91cF0uLi5bSXQgaXMgbG9jYXRlZCBpbiB0aGUgY2l0eSBjZW50ZXIgb2YgSmFrYXJ0YSwgbmVhciBNZWdhIEt1bmluZ2FuLCBhZGphY2VudCB0byB0aGUgc2lzdGVyIEpXIE1hcnJpb3R0IEhvdGVsLiBJdCBpcyBvcGVyYXRlZCBieSBUaGUgUml0ei1DYXJsdG9uIEhvdGVsIENvbXBhbnkuIFRoZSBjb21wbGV4IGhhcyB0d28gdG93ZXJzIHRoYXQgY29tcHJpc2UgYSBob3RlbCBhbmQgdGhlIEFpcmxhbmdnYSBBcGFydG1lbnQgcmVzcGVjdGl2ZWx5XS4uLltUaGUgT2Jlcm9pIEdyb3VwIGlzIGEgaG90ZWwgY29tcGFueSB3aXRoIGl0cyBoZWFkIG9mZmljZSBpbiBEZWxoaS5dClxwYXJ8XHRleHRiZntJbnN0cnVjdGlvbjp9fCBHaXZlbiB0aGUgcXVlc3Rpb24sIGNvbnRleHQsIGFuZCBhbnN3ZXIgYWJvdmUsIHByb3ZpZGUgYSBsb2dpY2FsIHJlYXNvbmluZyBmb3IgdGhhdCBhbnN3ZXIuIFBsZWFzZSB1c2UgdGhlIGZvcm1hdCBvZjogIyNSZWFzb246IHtyZWFzb259ICMjQW5zd2VyOiB7YW5zd2VyfS4KXHBhci0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tClxwYXJ8XHRleHRiZntDb1QgQW5zd2VyOn18ICMjUmVhc29uOiBUaGUgZG9jdW1lbnQgIyNiZWdpbl9xdW90ZSMjIFRoZSBPYmVyb2kgZmFtaWx5IGlzIGFuIEluZGlhbiBmYW1pbHkgdGhhdCBpcyBmYW1vdXMgZm9yIGl0cyBpbnZvbHZlbWVudCBpbiBob3RlbHMsIG5hbWVseSB0aHJvdWdoIFRoZSBPYmVyb2kgR3JvdXAuICMjZW5kX3F1b3RlIyMgZXN0YWJsaXNoZXMgdGhhdCB0aGUgT2Jlcm9pIGZhbWlseSBpcyBpbnZvbHZlZCBpbiB0aGUgT2Jlcm9pIGdyb3VwLCBhbmQgdGhlIGRvY3VtZW50ICMjYmVnaW5fcXVvdGUjIyBUaGUgT2Jlcm9pIEdyb3VwIGlzIGEgaG90ZWwgY29tcGFueSB3aXRoIGl0cyBoZWFkIG9mZmljZSBpbiBEZWxoaS4gIyNlbmRfcXVvdGUjIyBlc3RhYmxpc2hlcyB0aGUgaGVhZCBvZmZpY2Ugb2YgVGhlIE9iZXJvaSBHcm91cC4gVGhlcmVmb3JlLCB0aGUgT2Jlcm9pIGZhbWlseSBpcyBwYXJ0IG9mIGEgaG90ZWwgY29tcGFueSB3aG9zZSBoZWFkIG9mZmljZSBpcyBpbiBEZWxoaS4gIyNBbnN3ZXI6IERlbGhpClxwYXI=">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
</div>
<div class="ltx_listingline" id="lstnumberx2">|\<span class="ltx_text ltx_lst_identifier" id="lstnumberx2.1">textbf</span>{<span class="ltx_text ltx_lst_identifier" id="lstnumberx2.2">Question</span>:}|<span class="ltx_text ltx_lst_space" id="lstnumberx2.3">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.4">The</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.5">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.6">Oberoi</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.7">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.8">family</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.9">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.10">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.11">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.12">part</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.13">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.14">of</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.15">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.16">a</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.17">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.18">hotel</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.19">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.20">company</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.21">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.22">that</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.23">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.24">has</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.25">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.26">a</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.27">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.28">head</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.29">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.30">office</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.31">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.32">in</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.33">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.34">what</span><span class="ltx_text ltx_lst_space" id="lstnumberx2.35">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx2.36">city</span>?
</div>
<div class="ltx_listingline" id="lstnumberx3">\<span class="ltx_text ltx_lst_identifier" id="lstnumberx3.1">par</span>|\<span class="ltx_text ltx_lst_identifier" id="lstnumberx3.2">textbf</span>{<span class="ltx_text ltx_lst_identifier" id="lstnumberx3.3">context</span>:}|<span class="ltx_text ltx_lst_space" id="lstnumberx3.4">&nbsp;</span>[<span class="ltx_text ltx_lst_identifier" id="lstnumberx3.5">The</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.6">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.7">Oberoi</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.8">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.9">family</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.10">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.11">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.12">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.13">an</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.14">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.15">Indian</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.16">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.17">family</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.18">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.19">that</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.20">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.21">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.22">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.23">famous</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.24">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.25">for</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.26">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.27">its</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.28">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.29">involvement</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.30">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.31">in</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.32">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.33">hotels</span>,<span class="ltx_text ltx_lst_space" id="lstnumberx3.34">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.35">namely</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.36">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.37">through</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.38">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.39">The</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.40">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.41">Oberoi</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.42">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.43">Group</span>]…[<span class="ltx_text ltx_lst_identifier" id="lstnumberx3.44">It</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.45">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.46">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.47">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.48">located</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.49">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.50">in</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.51">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.52">the</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.53">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.54">city</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.55">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.56">center</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.57">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.58">of</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.59">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.60">Jakarta</span>,<span class="ltx_text ltx_lst_space" id="lstnumberx3.61">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.62">near</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.63">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.64">Mega</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.65">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.66">Kuningan</span>,<span class="ltx_text ltx_lst_space" id="lstnumberx3.67">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.68">adjacent</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.69">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.70">to</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.71">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.72">the</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.73">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.74">sister</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.75">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.76">JW</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.77">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.78">Marriott</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.79">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.80">Hotel</span>.<span class="ltx_text ltx_lst_space" id="lstnumberx3.81">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.82">It</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.83">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.84">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.85">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.86">operated</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.87">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.88">by</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.89">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.90">The</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.91">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.92">Ritz</span>-<span class="ltx_text ltx_lst_identifier" id="lstnumberx3.93">Carlton</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.94">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.95">Hotel</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.96">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.97">Company</span>.<span class="ltx_text ltx_lst_space" id="lstnumberx3.98">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.99">The</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.100">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.101">complex</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.102">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.103">has</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.104">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.105">two</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.106">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.107">towers</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.108">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.109">that</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.110">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.111">comprise</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.112">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.113">a</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.114">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.115">hotel</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.116">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.117">and</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.118">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.119">the</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.120">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.121">Airlangga</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.122">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.123">Apartment</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.124">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.125">respectively</span>]…[<span class="ltx_text ltx_lst_identifier" id="lstnumberx3.126">The</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.127">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.128">Oberoi</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.129">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.130">Group</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.131">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.132">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.133">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.134">a</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.135">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.136">hotel</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.137">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.138">company</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.139">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.140">with</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.141">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.142">its</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.143">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.144">head</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.145">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.146">office</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.147">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.148">in</span><span class="ltx_text ltx_lst_space" id="lstnumberx3.149">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx3.150">Delhi</span>.]
</div>
<div class="ltx_listingline" id="lstnumberx4">\<span class="ltx_text ltx_lst_identifier" id="lstnumberx4.1">par</span>|\<span class="ltx_text ltx_lst_identifier" id="lstnumberx4.2">textbf</span>{<span class="ltx_text ltx_lst_identifier" id="lstnumberx4.3">Instruction</span>:}|<span class="ltx_text ltx_lst_space" id="lstnumberx4.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.5">Given</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.6">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.7">the</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.8">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.9">question</span>,<span class="ltx_text ltx_lst_space" id="lstnumberx4.10">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.11">context</span>,<span class="ltx_text ltx_lst_space" id="lstnumberx4.12">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.13">and</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.14">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.15">answer</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.16">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.17">above</span>,<span class="ltx_text ltx_lst_space" id="lstnumberx4.18">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.19">provide</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.20">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.21">a</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.22">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.23">logical</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.24">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.25">reasoning</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.26">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.27">for</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.28">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.29">that</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.30">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.31">answer</span>.<span class="ltx_text ltx_lst_space" id="lstnumberx4.32">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.33">Please</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.34">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.35">use</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.36">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.37">the</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.38">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.39">format</span><span class="ltx_text ltx_lst_space" id="lstnumberx4.40">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx4.41">of</span>:<span class="ltx_text ltx_lst_space" id="lstnumberx4.42">&nbsp;</span>##<span class="ltx_text ltx_lst_identifier" id="lstnumberx4.43">Reason</span>:<span class="ltx_text ltx_lst_space" id="lstnumberx4.44">&nbsp;</span>{<span class="ltx_text ltx_lst_identifier" id="lstnumberx4.45">reason</span>}<span class="ltx_text ltx_lst_space" id="lstnumberx4.46">&nbsp;</span>##<span class="ltx_text ltx_lst_identifier" id="lstnumberx4.47">Answer</span>:<span class="ltx_text ltx_lst_space" id="lstnumberx4.48">&nbsp;</span>{<span class="ltx_text ltx_lst_identifier" id="lstnumberx4.49">answer</span>}.
</div>
<div class="ltx_listingline" id="lstnumberx5">\<span class="ltx_text ltx_lst_identifier" id="lstnumberx5.1">par</span>————————————————————————————–
</div>
<div class="ltx_listingline" id="lstnumberx6">\<span class="ltx_text ltx_lst_identifier" id="lstnumberx6.1">par</span>|\<span class="ltx_text ltx_lst_identifier" id="lstnumberx6.2">textbf</span>{<span class="ltx_text ltx_lst_identifier" id="lstnumberx6.3">CoT</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.4">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.5">Answer</span>:}|<span class="ltx_text ltx_lst_space" id="lstnumberx6.6">&nbsp;</span>##<span class="ltx_text ltx_lst_identifier" id="lstnumberx6.7">Reason</span>:<span class="ltx_text ltx_lst_space" id="lstnumberx6.8">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.9">The</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.10">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.11">document</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.12">&nbsp;</span>##<span class="ltx_text ltx_lst_identifier" id="lstnumberx6.13">begin_quote</span>##<span class="ltx_text ltx_lst_space" id="lstnumberx6.14">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.15">The</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.16">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.17">Oberoi</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.18">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.19">family</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.20">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.21">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.22">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.23">an</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.24">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.25">Indian</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.26">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.27">family</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.28">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.29">that</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.30">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.31">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.32">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.33">famous</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.34">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.35">for</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.36">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.37">its</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.38">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.39">involvement</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.40">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.41">in</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.42">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.43">hotels</span>,<span class="ltx_text ltx_lst_space" id="lstnumberx6.44">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.45">namely</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.46">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.47">through</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.48">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.49">The</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.50">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.51">Oberoi</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.52">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.53">Group</span>.<span class="ltx_text ltx_lst_space" id="lstnumberx6.54">&nbsp;</span>##<span class="ltx_text ltx_lst_identifier" id="lstnumberx6.55">end_quote</span>##<span class="ltx_text ltx_lst_space" id="lstnumberx6.56">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.57">establishes</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.58">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.59">that</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.60">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.61">the</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.62">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.63">Oberoi</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.64">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.65">family</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.66">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.67">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.68">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.69">involved</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.70">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.71">in</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.72">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.73">the</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.74">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.75">Oberoi</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.76">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.77">group</span>,<span class="ltx_text ltx_lst_space" id="lstnumberx6.78">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.79">and</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.80">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.81">the</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.82">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.83">document</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.84">&nbsp;</span>##<span class="ltx_text ltx_lst_identifier" id="lstnumberx6.85">begin_quote</span>##<span class="ltx_text ltx_lst_space" id="lstnumberx6.86">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.87">The</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.88">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.89">Oberoi</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.90">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.91">Group</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.92">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.93">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.94">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.95">a</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.96">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.97">hotel</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.98">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.99">company</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.100">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.101">with</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.102">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.103">its</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.104">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.105">head</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.106">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.107">office</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.108">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.109">in</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.110">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.111">Delhi</span>.<span class="ltx_text ltx_lst_space" id="lstnumberx6.112">&nbsp;</span>##<span class="ltx_text ltx_lst_identifier" id="lstnumberx6.113">end_quote</span>##<span class="ltx_text ltx_lst_space" id="lstnumberx6.114">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.115">establishes</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.116">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.117">the</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.118">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.119">head</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.120">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.121">office</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.122">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.123">of</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.124">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.125">The</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.126">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.127">Oberoi</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.128">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.129">Group</span>.<span class="ltx_text ltx_lst_space" id="lstnumberx6.130">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.131">Therefore</span>,<span class="ltx_text ltx_lst_space" id="lstnumberx6.132">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.133">the</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.134">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.135">Oberoi</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.136">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.137">family</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.138">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.139">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.140">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.141">part</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.142">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.143">of</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.144">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.145">a</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.146">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.147">hotel</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.148">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.149">company</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.150">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.151">whose</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.152">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.153">head</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.154">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.155">office</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.156">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.157">is</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.158">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.159">in</span><span class="ltx_text ltx_lst_space" id="lstnumberx6.160">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.161">Delhi</span>.<span class="ltx_text ltx_lst_space" id="lstnumberx6.162">&nbsp;</span>##<span class="ltx_text ltx_lst_identifier" id="lstnumberx6.163">Answer</span>:<span class="ltx_text ltx_lst_space" id="lstnumberx6.164">&nbsp;</span><span class="ltx_text ltx_lst_identifier" id="lstnumberx6.165">Delhi</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">\<span class="ltx_text ltx_lst_identifier" id="lstnumberx7.1">par</span>
</div>
</div>
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.3.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.4.2" style="font-size:90%;">RAFT&nbsp;prompt to help LLM evaluate its own generated reasoning and answers, contrasting them with the correct reasoning and answers. The LLM is prompted to identify errors in its reasoning and extract key insights for improvement. This figure specifically represents the ‘GenerateExplanation‘ step in the RAFT&nbsp;algorithm&nbsp;(<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S3" title="3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>).</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text ltx_font_bold" id="S3.T1.4.2" style="font-size:90%;">RAFT improves RAG performance forall specialized domains<span class="ltx_text ltx_font_medium" id="S3.T1.4.2.1">: Across PubMed, HotpotQA, HuggingFace, Torch Hub, and Tensorflow Hub, we see that domain specific Finetuning improves significantly of the performance of the base model, but RAFT&nbsp; consistently outperforms the existing domain specific finetuning method with or without RAG. This suggests the need to train the model with context. We compare our model with LLaMA finetuning receipes, and provide GPT-3.5 for reference.</span></span></figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T1.5">
<tbody><tr class="ltx_tr" id="S3.T1.5.1">
<td class="ltx_td ltx_border_tt" id="S3.T1.5.1.1" style="width:85.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S3.T1.5.1.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.1.2.1"><span class="ltx_text ltx_align_center" id="S3.T1.5.1.2.1.1"></span> <span class="ltx_text ltx_align_center" id="S3.T1.5.1.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.5.1.2.1.2.1">
<span class="ltx_tr" id="S3.T1.5.1.2.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S3.T1.5.1.2.1.2.1.1.1">PubMed</span></span>
</span></span> <span class="ltx_text ltx_align_center" id="S3.T1.5.1.2.1.3"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S3.T1.5.1.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.1.3.1"><span class="ltx_text ltx_align_center" id="S3.T1.5.1.3.1.1"></span> <span class="ltx_text ltx_align_center" id="S3.T1.5.1.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.5.1.3.1.2.1">
<span class="ltx_tr" id="S3.T1.5.1.3.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S3.T1.5.1.3.1.2.1.1.1">HotpotQA</span></span>
</span></span> <span class="ltx_text ltx_align_center" id="S3.T1.5.1.3.1.3"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S3.T1.5.1.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.1.4.1"><span class="ltx_text ltx_align_center" id="S3.T1.5.1.4.1.1"></span> <span class="ltx_text ltx_align_center" id="S3.T1.5.1.4.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.5.1.4.1.2.1">
<span class="ltx_tr" id="S3.T1.5.1.4.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S3.T1.5.1.4.1.2.1.1.1">HuggingFace</span></span>
</span></span> <span class="ltx_text ltx_align_center" id="S3.T1.5.1.4.1.3"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S3.T1.5.1.5" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.1.5.1"><span class="ltx_text ltx_align_center" id="S3.T1.5.1.5.1.1"></span> <span class="ltx_text ltx_align_center" id="S3.T1.5.1.5.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.5.1.5.1.2.1">
<span class="ltx_tr" id="S3.T1.5.1.5.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S3.T1.5.1.5.1.2.1.1.1">Torch Hub</span></span>
</span></span> <span class="ltx_text ltx_align_center" id="S3.T1.5.1.5.1.3"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S3.T1.5.1.6" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.1.6.1"><span class="ltx_text ltx_align_center" id="S3.T1.5.1.6.1.1"></span> <span class="ltx_text ltx_align_center" id="S3.T1.5.1.6.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.5.1.6.1.2.1">
<span class="ltx_tr" id="S3.T1.5.1.6.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S3.T1.5.1.6.1.2.1.1.1">TensorFlow Hub</span></span>
</span></span> <span class="ltx_text ltx_align_center" id="S3.T1.5.1.6.1.3"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.2">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.2.1" style="width:85.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.2.1.1">GPT-3.5 + RAG</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.2.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.2.2.1">71.60</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.2.3" style="width:48.4pt;"><span class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" id="S3.T1.5.2.3.1">41.5</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.2.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.2.4.1">29.08</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.2.5" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.2.5.1">60.21</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.2.6" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.2.6.1">65.59</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.3">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.3.1" style="width:85.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.3.1.1">LLaMA2-7B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.3.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.3.2.1">56.5</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.3.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.3.3.1">0.54</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.3.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.3.4.1">0.22</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.3.5" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.3.5.1">0</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T1.5.3.6" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.3.6.1">0</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.4">
<td class="ltx_td ltx_align_justify" id="S3.T1.5.4.1" style="width:85.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.4.1.1">LLaMA2-7B + RAG</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.4.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.4.2.1">58.8</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.4.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.4.3.1">0.03</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.4.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.4.4.1">26.43</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.4.5" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.4.5.1">08.60</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.4.6" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.4.6.1">43.06</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.5">
<td class="ltx_td ltx_align_justify" id="S3.T1.5.5.1" style="width:85.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.5.1.1">DSF</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.5.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.5.2.1">59.7</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.5.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.5.3.1">6.38</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.5.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.5.4.1">61.06</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.5.5" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.5.5.1">84.94</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.5.6" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.5.6.1">86.56</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.6">
<td class="ltx_td ltx_align_justify" id="S3.T1.5.6.1" style="width:85.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.6.1.1">DSF + RAG</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.6.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.6.2.1">71.6</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.6.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.6.3.1">4.41</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.6.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.6.4.1">42.59</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.6.5" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.6.5.1">82.80</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S3.T1.5.6.6" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.6.6.1">60.29</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.7">
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T1.5.7.1" style="width:85.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.7.1.1">RAFT (LLaMA2-7B)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T1.5.7.2" style="width:48.4pt;"><span class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" id="S3.T1.5.7.2.1">73.30</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T1.5.7.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T1.5.7.3.1">35.28</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T1.5.7.4" style="width:56.9pt;"><span class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" id="S3.T1.5.7.4.1">74.00</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T1.5.7.5" style="width:48.4pt;"><span class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" id="S3.T1.5.7.5.1">84.95</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T1.5.7.6" style="width:71.1pt;"><span class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" id="S3.T1.5.7.6.1">86.86</span></td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We design our experiments to study how well RAFT&nbsp; performs compared to various baselines.
We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading and extracting information from in-domain documents, than domain specific finetuned model, and general-purpose model with RAG.
As an ablation, we also demonstrate how important it is for the model to learn with Chain-of-Thought responses.
In this section, we will first introduce all the datasets we used in the experiments, then all the baseline model/fine-tuning techniques that we benchmark against.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In our experiments, we use the following datasets to evaluate our model and all baselines.
We selected these datasets to represent both popular and diverse domains including Wikipedia, Coding/API documents, and question-answering on medical documents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Natural Questions (NQ)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib21" title="">2019</a>)</cite>, Trivia QA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Joshi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib18" title="">2017</a>)</cite> and HotpotQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib57" title="">2018</a>)</cite> are the open-domain question-answers based on Wikipedia, mainly focused on common knowledge (e.g., movies, sports, etc).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">HuggingFace, Torch Hub, and TensorFlow Hub are from the APIBench&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Patil et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib38" title="">2023</a>)</cite> proposed in the Gorilla paper. These benchmarks measure how to generate the correct, functional, and executable API calls based on the documentation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">PubMed QA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib17" title="">2019</a>)</cite> is a question-answering dataset tailored only for biomedical-research question-answering. It mainly focuses on answering medical and biology questions based on a given set of documents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Note that the first category of dataset (NQ, Trivia QA, and HotpotQA) is a relatively general domain whereas the latter two domains are on very domain specific documents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">We consider the following baselines for our experiments:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used instruction-finetuned model for QA tasks, where we provide clearly written instructions, but no reference documentation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting, except here we include reference documents. This is a popular technique when dealing with domain specific QA tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">domain specific Finetuning with 0-shot prompting (DSF): Performing standard supervised finetuning, without documents in context. We find that it mostly useful to align the answering style of the model as well as get familiar with the domain context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1">domain specific Finetuning with RAG (DSF + RAG): Equip a domain specific finetuned model with external knowledge using RAG. So, for the “knowledge” the model does not know, it can still refer to the context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Using the above datasets and baselines, we evaluate our model RAFT&nbsp; and demonstrate the effectiveness of RAFT&nbsp; in Tab.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S3.T1" title="Table 1 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">1</span></a>.
We see that RAFT&nbsp; consistently and significantly outperforms the baselines.
Compared with the base Llama-2 instruction-tuned model, RAFT&nbsp; with RAG does much better in terms of extracting information as well as being robust towards distractors.
The gain can be as big as 35.25% on Hotpot QA and 76.35% on Torch Hub evaluation.
Compared with DSF on the specific dataset, our model does better at relying on the provided context to solve the problem.
RAFT&nbsp; does much better on tasks like HotpotQA and HuggingFace datasets (30.87% on HotpotQA and 31.41% on HuggingFace).
Note that for PubMed QA, since it is a binary yes/no question, we don’t observe significant gains when we compare our model with DSF + RAG.
Even compared with a much larger and better model GPT-3.5, RAFT&nbsp; demonstrates significant advantages.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Overall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its answering style not aligning with the ground truth. By applying domain specific tuning, we significantly enhance its performance. This process enables the model to learn and adopt the appropriate style of answering. However, introducing RAG to a domain-specifically fine-tuned (DSF) model doesn’t invariably lead to better outcomes. This might indicate that the model lacks training in context processing and extracting useful information from it. By incorporating our method, RAFT&nbsp;, we train the model not only to match its answering style with that required but also to improve its document processing capabilities. Consequently, our approach outperforms all others.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Effect of CoT</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach in enhancing the model’s performance. As indicated in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S4.T2" title="Table 2 ‣ 4.3 Effect of CoT ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">2</span></a>, simply providing the answer to a question may not always be adequate. This approach can lead to a rapid decrease in loss, resulting in the training process to diverge. Incorporating a reasoning chain that not only guides the model to the answer but also enriches the model’s understanding can improve the overall accuracy. In our experiments, integrating the Chain-of-Thought significantly enhances training robustness. We employ GPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the prompt we used in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S3.F3" title="Figure 3 ‣ 3 RAFT ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">3</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T2.4.2" style="font-size:90%;">Ablation on Chain-of-Thought<span class="ltx_text ltx_font_medium" id="S4.T2.4.2.1">:
The numbers of RAFT&nbsp; and RAFT&nbsp; without CoT. Results on various datasets show that adding CoT can significantly improve the performance of the finetuned model. With a gain of 9.66% and 14.93% on the Hotpot QA and HuggingFace datasets respectively.</span></span></figcaption>
<table class="ltx_tabular ltx_align_middle" id="S4.T2.5">
<tbody><tr class="ltx_tr" id="S4.T2.5.1">
<td class="ltx_td ltx_border_tt" id="S4.T2.5.1.1" style="width:85.4pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S4.T2.5.1.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.1.2.1"><span class="ltx_text ltx_align_center" id="S4.T2.5.1.2.1.1"></span> <span class="ltx_text ltx_align_center" id="S4.T2.5.1.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.5.1.2.1.2.1">
<span class="ltx_tr" id="S4.T2.5.1.2.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T2.5.1.2.1.2.1.1.1">PubMed</span></span>
</span></span> <span class="ltx_text ltx_align_center" id="S4.T2.5.1.2.1.3"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S4.T2.5.1.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.1.3.1"><span class="ltx_text ltx_align_center" id="S4.T2.5.1.3.1.1"></span> <span class="ltx_text ltx_align_center" id="S4.T2.5.1.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.5.1.3.1.2.1">
<span class="ltx_tr" id="S4.T2.5.1.3.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T2.5.1.3.1.2.1.1.1">HotpotQA</span></span>
</span></span> <span class="ltx_text ltx_align_center" id="S4.T2.5.1.3.1.3"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S4.T2.5.1.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.1.4.1"><span class="ltx_text ltx_align_center" id="S4.T2.5.1.4.1.1"></span> <span class="ltx_text ltx_align_center" id="S4.T2.5.1.4.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.5.1.4.1.2.1">
<span class="ltx_tr" id="S4.T2.5.1.4.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T2.5.1.4.1.2.1.1.1">HuggingFace</span></span>
</span></span> <span class="ltx_text ltx_align_center" id="S4.T2.5.1.4.1.3"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S4.T2.5.1.5" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.1.5.1"><span class="ltx_text ltx_align_center" id="S4.T2.5.1.5.1.1"></span> <span class="ltx_text ltx_align_center" id="S4.T2.5.1.5.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.5.1.5.1.2.1">
<span class="ltx_tr" id="S4.T2.5.1.5.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T2.5.1.5.1.2.1.1.1">Torch Hub</span></span>
</span></span> <span class="ltx_text ltx_align_center" id="S4.T2.5.1.5.1.3"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S4.T2.5.1.6" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.1.6.1"><span class="ltx_text ltx_align_center" id="S4.T2.5.1.6.1.1"></span> <span class="ltx_text ltx_align_center" id="S4.T2.5.1.6.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.5.1.6.1.2.1">
<span class="ltx_tr" id="S4.T2.5.1.6.1.2.1.1">
<span class="ltx_td ltx_align_center" id="S4.T2.5.1.6.1.2.1.1.1">TensorFlow Hub</span></span>
</span></span> <span class="ltx_text ltx_align_center" id="S4.T2.5.1.6.1.3"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.2">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T2.5.2.1" style="width:85.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.2.1.1">RAFT w.o CoT</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T2.5.2.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.2.2.1">68.30</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T2.5.2.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.2.3.1">25.62</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T2.5.2.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.2.4.1">59.07</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T2.5.2.5" style="width:48.4pt;"><span class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" id="S4.T2.5.2.5.1">86.56</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S4.T2.5.2.6" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.2.6.1">83.21</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.3">
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S4.T2.5.3.1" style="width:85.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.3.1.1">RAFT</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S4.T2.5.3.2" style="width:48.4pt;"><span class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" id="S4.T2.5.3.2.1">73.30</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S4.T2.5.3.3" style="width:48.4pt;"><span class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" id="S4.T2.5.3.3.1">35.28</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S4.T2.5.3.4" style="width:56.9pt;"><span class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" id="S4.T2.5.3.4.1">74.00</span></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S4.T2.5.3.5" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S4.T2.5.3.5.1">84.95</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S4.T2.5.3.6" style="width:71.1pt;"><span class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" id="S4.T2.5.3.6.1">86.86</span></td>
</tr>
</tbody></table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Qualitative Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To illustrate the potential advantages of RAFT&nbsp; over the domain-specifically fine-tuned (DSF) approach, we present a comparative example in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S4.F4" title="Figure 4 ‣ 4.4 Qualitative Analysis ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">4</span></a>. This example qualitatively demonstrates a scenario where the DSF model becomes confused by a question asking for the identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one of the films written by the screenwriter. In contrast, the RAFT&nbsp; model accurately answers the question. This discrepancy suggests that training a model solely with question-answer pairs may impair its ability to derive relevant context from provided documents. The comparison underscores the importance of incorporating both standard instructional tuning and context comprehension into the training dataset to preserve and enhance the model’s ability to process text effectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F4">
<br class="ltx_break"><svg class="ltx_picture" height="45341.08" id="S4.F4.pic1" overflow="visible" version="1.1" width="45341.08"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,45341.08) matrix(1 0 0 -1 0 0) translate(0,-22670.54) translate(22670.54,0) translate(0,22670.54)"><clipPath id="pgfcp13"><path d="M -22670.54 -22670.54 L 22670.54 -22670.54 L 22670.54 22670.54 L -22670.54 22670.54 Z M 0 5.91 L 0 298.57 C 0 301.83 2.64 304.48 5.91 304.48 L 594.09 304.48 C 597.36 304.48 600 301.83 600 298.57 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z"></path></clipPath><g fill-rule="evenodd"><g fill="#0000BF" fill-opacity="1.000000"><path d="M 0 5.91 L 0 298.57 C 0 301.83 2.64 304.48 5.91 304.48 L 594.09 304.48 C 597.36 304.48 600 301.83 600 298.57 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z M 1.97 5.91 L 1.97 274.56 L 598.03 274.56 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.000000"><path d="M 1.97 5.91 L 1.97 274.56 L 598.03 274.56 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 15.81 283.44)"><foreignObject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="568.39">
<p class="ltx_p ltx_minipage ltx_align_bottom" id="S4.F4.pic1.3.3.3.3.2.2" style="width:410.8pt;"><span class="ltx_text ltx_font_bold" id="S4.F4.pic1.3.3.3.3.2.2.1">HotPot QA</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></foreignObject></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 15.81 15.81)"><foreignObject height="244.91" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="568.39"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1" style="width:410.8pt;">
<span class="ltx_para" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.3"><span class="ltx_text ltx_font_bold" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.3.1">Question:</span> What screenwriter with credits for “Evolution”[0/1879]e a film starring Nicolas Cage and Téa Leoni?</span>
<span class="ltx_p" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.1">Documents:</span> <math alttext="\ldots" class="ltx_Math" display="inline" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.m1.1"><semantics id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.m1.1a"><mi id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.m1.1.1" mathvariant="normal" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.m1.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.m1.1b"><ci id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.m1.1.1.cmml" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.m1.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.m1.1c">\ldots</annotation><annotation encoding="application/x-llamapun" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.m1.1d">…</annotation></semantics></math> <span class="ltx_text" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.2" style="background-color:#FFFF00;">David Weissman</span> is a <span class="ltx_text" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.3" style="background-color:#FFFF00;">screenwriter</span> and director. His film credits include “The Family Man” (2000), <span class="ltx_text" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.1.4" style="background-color:#FFFF00;">“Evolution” (2001)</span>, and “When in Rome” (2010).</span>
<span class="ltx_p" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2"><math alttext="\ldots" class="ltx_Math" display="inline" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2.m1.1"><semantics id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2.m1.1a"><mi id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2.m1.1.1" mathvariant="normal" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2.m1.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2.m1.1b"><ci id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2.m1.1.1.cmml" xref="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2.m1.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2.m1.1c">\ldots</annotation><annotation encoding="application/x-llamapun" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.2.m1.1d">…</annotation></semantics></math></span>
<span class="ltx_p" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.4">The Family Man is a 2000 American romantic comedy-drama film directed by Brett Ratner, written by David Diamond and David Weissman, and starring <span class="ltx_text" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.4.1" style="background-color:#FFFF00;">Nicolas Cage</span> and <span class="ltx_text" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.4.2" style="background-color:#FFFF00;">Téa Leoni</span>. …</span>
<span class="ltx_p" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.5">——————————————————————————————————————————————–</span>
<span class="ltx_p" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.6"><span class="ltx_text ltx_font_bold" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.6.1">RAFT:</span> ##Reason: The screenwriter with credits for the film “Evolution,” starring Nicolas Cage and Téa Leoni, is David Weissman. This information is provided in the reference documents which mention David Weissman as a screenwriter with film credits including “The Family Man” (2000), “Evolution” (2001), and “When in Rome” (2010). Therefore, the screenwriter for “Evolution” is David Weissman. ##Answer: <span class="ltx_text" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.6.2" style="background-color:#B3FFB3;">David Weissman</span> ✓</span>
<span class="ltx_p" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.7"><span class="ltx_text ltx_font_bold" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.7.1">DSF:</span> <span class="ltx_text" id="S4.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.p1.7.2" style="background-color:#FFB3B3;">The Family Man</span> ✗</span>
</span></span></foreignObject></g></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<br class="ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.3.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F4.4.2" style="font-size:90%;">Comparison of RAFT&nbsp; and DSF<span class="ltx_text ltx_font_medium" id="S4.F4.4.2.1">: We prompt RAFT&nbsp; and DSF fine-tuned models on the HotpotQA dataset. We can see that the DSF model extracts the wrong information from the context. For the question, who is the screenwriter, it responds with a film name. RAFT&nbsp; manages to get the result correctly .</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Should we train the LLM always with the oracle context for RAG?</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">In our exploration of whether large language models (LLMs) should always be trained with the oracle context for Retrieval-Augmented Generation (RAG), we address a key question: what proportion (p%) of the training data should include oracle documents? Intuitively, one might assume that for effective training in reading and extracting information from context (e.g., RAG tasks), the oracle document should always be included during training (P = 100%). However, our findings challenge this assumption: incorporating a portion of the training data without the oracle document in the context (P = 80%) appears to enhance the model’s performance on RAG tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">Fig.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S4.F5" title="Figure 5 ‣ 4.5 Should we train the LLM always with the oracle context for RAG? ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">5</span></a> presents our investigation into the hyperparameter P%, which represents the percentage of training instances that should include oracle documents. Our analysis reveals that the optimal proportion varies across datasets, with figures ranging from 40%, 60%, and 100%. This indicates that training your LLM without the correct corresponding context at times can be beneficial for the downstream task of answering questions related to the documents. In our training setup, we include four distractor documents alongside the oracle document, and at test time, we maintain this format by providing the oracle document with four distractors. Our findings suggest that, for domain specific RAG tasks, including a certain percentage of training data without the oracle documents in the context proves to be advantageous.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S4.F5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="581" id="S4.F5.1.g1" src="x2.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S4.F5.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="581" id="S4.F5.2.g1" src="x3.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S4.F5.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="581" id="S4.F5.3.g1" src="x4.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.8.2.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F5.5.1" style="font-size:90%;">How many golden documents to involve?<span class="ltx_text ltx_font_medium" id="S4.F5.5.1.1"> We study the hyperparameter <math alttext="P\%" class="ltx_Math" display="inline" id="S4.F5.5.1.1.m1.1"><semantics id="S4.F5.5.1.1.m1.1b"><mrow id="S4.F5.5.1.1.m1.1.1" xref="S4.F5.5.1.1.m1.1.1.cmml"><mi id="S4.F5.5.1.1.m1.1.1.2" xref="S4.F5.5.1.1.m1.1.1.2.cmml">P</mi><mo id="S4.F5.5.1.1.m1.1.1.1" mathvariant="normal" xref="S4.F5.5.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.5.1.1.m1.1c"><apply id="S4.F5.5.1.1.m1.1.1.cmml" xref="S4.F5.5.1.1.m1.1.1"><csymbol cd="latexml" id="S4.F5.5.1.1.m1.1.1.1.cmml" xref="S4.F5.5.1.1.m1.1.1.1">percent</csymbol><ci id="S4.F5.5.1.1.m1.1.1.2.cmml" xref="S4.F5.5.1.1.m1.1.1.2">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.5.1.1.m1.1d">P\%</annotation><annotation encoding="application/x-llamapun" id="S4.F5.5.1.1.m1.1e">italic_P %</annotation></semantics></math> which indicates what fraction of the training data contains the oracle document(s) in its context. Results on NQ, TQA and HotpotQA suggest that mixing a fraction of data that does not have the oracle document in its context is helpful for in-domain RAG.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>RAFT Generalizes to Top-K RAG</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">After demonstrating the performance of RAFT&nbsp; on various benchmarks, we now study another important problem: How does the number of distractor documents in RAFT&nbsp; affect the model’s performance when augmented with top-k retriever augmented generation (RAG) result during the evaluation?
Previous research has highlighted the vulnerability of LLMs to irrelevant text (see studies &nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib43" title="">2023a</a>; Weston &amp; Sukhbaatar, <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib53" title="">2023</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib29" title="">2023b</a>)</cite>).
This issue is particularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to ensure high recall.
Such a scenario necessitates the model to have the ability to discern and disregard irrelevant content, focusing solely on pertinent information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Making Model Robust to top-K RAG</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">To tackle the challenge of enhancing large language models’ (LLMs) ability to sift through irrelevant text within the retrieval pipeline, our analysis revealed that training solely with oracle (highly relevant) documents can inadvertently diminish the model’s ability to discern and disregard irrelevant information. To address this, our algorithm, RAFT&nbsp;, adopts a strategy that integrates oracle documents with a mix of irrelevant ones. This methodology prompts us to investigate the ideal fraction of negative (irrelevant) documents to incorporate throughout the training process and to assess how well this training approach adapts to different volumes of documents encountered by the Retrieval-Augmented Generation (RAG) during the test phase. Our aim is to refine the balance between relevant and irrelevant information to strengthen the model’s efficiency in identifying and utilizing pertinent content. Notice that Sec&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S4.SS5" title="4.5 Should we train the LLM always with the oracle context for RAG? ‣ 4 Evaluation ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">4.5</span></a> looked at what P% of training data should include distractors, while in this section, we study test-time scenarios.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Training with Negative Documents</span>
To enhance the robustness of large language models (LLMs) against irrelevant text in retrieved documents, we adopted a finetuning approach that incorporates both golden (highly relevant) documents and distractor (irrelevant) documents. The model was trained with varying numbers of distractor documents, but consistently evaluated using the top-k documents obtained from the retriever - not to be confused with <math alttext="p" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">italic_p</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.2">Our findings, detailed in Fig.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S5.F6" title="Figure 6 ‣ 5.1 Making Model Robust to top-K RAG ‣ 5 RAFT Generalizes to Top-K RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">6</span></a>, reveal that finetuning with only the oracle document frequently results in inferior performance compared to configurations that include a greater number of distractor documents.
As we can see in the figure, the better performance for Natural Questions is training with <math alttext="D^{*}+3D" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><msup id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2.2" xref="S5.SS1.p3.1.m1.1.1.2.2.cmml">D</mi><mo id="S5.SS1.p3.1.m1.1.1.2.3" xref="S5.SS1.p3.1.m1.1.1.2.3.cmml">*</mo></msup><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">+</mo><mrow id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml"><mn id="S5.SS1.p3.1.m1.1.1.3.2" xref="S5.SS1.p3.1.m1.1.1.3.2.cmml">3</mn><mo id="S5.SS1.p3.1.m1.1.1.3.1" xref="S5.SS1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p3.1.m1.1.1.3.3" xref="S5.SS1.p3.1.m1.1.1.3.3.cmml">D</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><plus id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></plus><apply id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.1.1.2.1.cmml" xref="S5.SS1.p3.1.m1.1.1.2">superscript</csymbol><ci id="S5.SS1.p3.1.m1.1.1.2.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2.2">𝐷</ci><times id="S5.SS1.p3.1.m1.1.1.2.3.cmml" xref="S5.SS1.p3.1.m1.1.1.2.3"></times></apply><apply id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3"><times id="S5.SS1.p3.1.m1.1.1.3.1.cmml" xref="S5.SS1.p3.1.m1.1.1.3.1"></times><cn id="S5.SS1.p3.1.m1.1.1.3.2.cmml" type="integer" xref="S5.SS1.p3.1.m1.1.1.3.2">3</cn><ci id="S5.SS1.p3.1.m1.1.1.3.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">D^{*}+3D</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">italic_D start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT + 3 italic_D</annotation></semantics></math> and it is <math alttext="D^{*}+1D" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mrow id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><msup id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml"><mi id="S5.SS1.p3.2.m2.1.1.2.2" xref="S5.SS1.p3.2.m2.1.1.2.2.cmml">D</mi><mo id="S5.SS1.p3.2.m2.1.1.2.3" xref="S5.SS1.p3.2.m2.1.1.2.3.cmml">*</mo></msup><mo id="S5.SS1.p3.2.m2.1.1.1" xref="S5.SS1.p3.2.m2.1.1.1.cmml">+</mo><mrow id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3.cmml"><mn id="S5.SS1.p3.2.m2.1.1.3.2" xref="S5.SS1.p3.2.m2.1.1.3.2.cmml">1</mn><mo id="S5.SS1.p3.2.m2.1.1.3.1" xref="S5.SS1.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S5.SS1.p3.2.m2.1.1.3.3" xref="S5.SS1.p3.2.m2.1.1.3.3.cmml">D</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><plus id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1.1"></plus><apply id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p3.2.m2.1.1.2.1.cmml" xref="S5.SS1.p3.2.m2.1.1.2">superscript</csymbol><ci id="S5.SS1.p3.2.m2.1.1.2.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2.2">𝐷</ci><times id="S5.SS1.p3.2.m2.1.1.2.3.cmml" xref="S5.SS1.p3.2.m2.1.1.2.3"></times></apply><apply id="S5.SS1.p3.2.m2.1.1.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3"><times id="S5.SS1.p3.2.m2.1.1.3.1.cmml" xref="S5.SS1.p3.2.m2.1.1.3.1"></times><cn id="S5.SS1.p3.2.m2.1.1.3.2.cmml" type="integer" xref="S5.SS1.p3.2.m2.1.1.3.2">1</cn><ci id="S5.SS1.p3.2.m2.1.1.3.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">D^{*}+1D</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m2.1d">italic_D start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT + 1 italic_D</annotation></semantics></math> documents with Hotpot QA.
This insight has been particularly beneficial for our algorithm, RAFT&nbsp;. In our experiments, we typically employ a training setup consisting of one oracle document alongside four distractor documents. This approach strikes a balance, ensuring the model is not overwhelmed by distractors while still gaining the ability to effectively discern and prioritize relevant information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">Generalization to a variable number of test-time documents.</span>
We extended our research to examine the impact of different quantities of test-time documents on the model’s performance. Specifically, our experiments focused on assessing how models, trained with varying numbers of distractor documents, respond to changes in the number of documents presented at test time.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">The results, illustrated in Fig.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#S5.F6" title="Figure 6 ‣ 5.1 Making Model Robust to top-K RAG ‣ 5 RAFT Generalizes to Top-K RAG ‣ RAFT: Adapting Language Model to Domain Specific RAG"><span class="ltx_text ltx_ref_tag">6</span></a>, confirm that the inclusion of distractor documents during training indeed makes the model more resilient to fluctuations in the number of documents encountered during testing. This ability to maintain consistent performance despite variations in test-time document numbers further validates the robustness of our approach, RAFT&nbsp;. This finding underscores the importance of a well-calibrated training environment to prepare the model for a range of scenarios it may encounter in real-world applications.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S5.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S5.F6.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="581" id="S5.F6.1.g1" src="x5.png" width="830">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S5.F6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="569" id="S5.F6.2.g1" src="x6.png" width="831">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.6.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F6.7.2" style="font-size:90%;">Test-Time Documents Varying<span class="ltx_text ltx_font_medium" id="S5.F6.7.2.1">: We study how robust RAFT&nbsp;is to varying numbers of test-time documents that a retriever might provide. In NQ, we find that training with 4 documents leads to the best performance, but training with 2 documents is optimal for HotpotQA. However, across both datasets, training with all datasets consisting of <em class="ltx_emph ltx_font_italic" id="S5.F6.7.2.1.1">oracle</em> documents hurts performance.</span></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Works</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text ltx_font_bold" id="S6.p1.1.1">Retrieval-Augmented Language Models</span> RAG enhances language models by integrating a retrieval module that sources relevant information from external knowledge bases, significantly improving performance across various NLP tasks, including language modeling&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Guu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib12" title="">2020</a>; Borgeaud et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib4" title="">2022</a>; Khandelwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib20" title="">2019</a>; Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib46" title="">2023d</a>; Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib27" title="">2023b</a>; Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib45" title="">2023c</a>; Asai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib3" title="">2023</a>; Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib56" title="">2023</a>; Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib50" title="">2023</a>)</cite> and open-domain question answering&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Izacard et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib15" title="">2023</a>; Lewis et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib24" title="">2020</a>)</cite>. This integration follows a “retrieve-and-read" paradigm where the retrieval module provides additional context from external sources, which the LM then uses to generate the final output. The retrieval process involves using the input as a query to fetch documents, which the LM incorporates for final predictions.
For instance, Atlas&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Izacard et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib15" title="">2023</a>)</cite> fine-tunes T5 models with the retriever, treating documents as latent variables, while RETRO&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Borgeaud et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib4" title="">2022</a>)</cite> modifies the decoder-only architecture to include retrieved texts and conducts pre-training from scratch. kNN-LM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib20" title="">2019</a>)</cite> interpolates between the LM’s next token distribution and distributions computed from retrieved tokens at inference. <cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib46" title="">2023d</a>; Ram et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib41" title="">2023</a>)</cite> assume black-box access to an LM and combine it with either off-the-shelf or fine-tuned retriever.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text ltx_font_bold" id="S6.p2.1.1">Memorization</span> A key question around large neural language models is whether they truly “understand” text &nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Feldman, <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib11" title="">2020</a>; Power et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib39" title="">2022</a>)</cite> or simply rely on surface pattern memorization &nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Carlini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib6" title="">2019</a>; Tänzer et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib47" title="">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citep">(Feldman, <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib11" title="">2020</a>; Carlini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib6" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib8" title="">2022</a>)</cite> develop methodologies to quantify the extent of memorization in neural models. <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib5" title="">2020</a>; Power et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib39" title="">2022</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib31" title="">2022b</a>)</cite> further explored how memorization impacts the models’ generalization capabilities. Recently, a seminal work by <cite class="ltx_cite ltx_citemacro_citep">(Carlini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib7" title="">2021</a>; Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib44" title="">2023b</a>)</cite> demonstrated the ability of language models to memorize and regurgitate training data, raising significant privacy concerns <cite class="ltx_cite ltx_citemacro_citep">(Kandpal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib19" title="">2022</a>; Pan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib37" title="">2020</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Finetuning of LLMs</span> Recent years have seen rapid progress in developing large-scale language models (LLMs)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib5" title="">2020</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib35" title="">2023</a>; Workshop et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib54" title="">2022</a>; Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib48" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib48" title="">2023</a>; Anil et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib1" title="">2023</a>)</cite>. To adapt these foundation models to downstream tasks, fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mishra et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib33" title="">2021</a>; Sanh et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib42" title="">2021</a>; Chung et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib9" title="">2022</a>; Muennighoff et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib34" title="">2023</a>; Zhou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib60" title="">2023b</a>; Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib27" title="">2023b</a>; Ji et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib16" title="">2024</a>)</cite> has become a prevalent approach. Traditional supervised fine-tuning may be limited by the cost and compute required for adapating LLMs. Addressing these challenges, research in the realm of parameter-efficient fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Houlsby et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib13" title="">2019</a>)</cite>, such as Prompt Tuning <cite class="ltx_cite ltx_citemacro_citep">(Lester et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib23" title="">2021</a>)</cite>, Prefix-Tuning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li &amp; Liang, <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib25" title="">2021</a>)</cite>, P-Tuning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib30" title="">2022a</a>)</cite> and Low-Rank based fine-tuning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib14" title="">2021</a>)</cite>, has gained traction. These methods enable LLMs to acquire domain-specific knowledge and adapt to specialized tasks such as question answering, summarization, and dialogue generation. Another branch of finetuning is through RLHF&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib36" title="">2022</a>; Rafailov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib40" title="">2023</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib28" title="">2023a</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib58" title="">2023</a>)</cite>, which adopts RL to align LLM’s preference with human.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1"><span class="ltx_text ltx_font_bold" id="S6.p4.1.1">Finetuning for RAG</span> More recently, several papers have been exploring the idea of finetuning a pretrained LLM to be better at RAG tasks&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib26" title="">2023a</a>; Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib50" title="">2023</a>; Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib56" title="">2023</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.10131v1#bib.bib32" title="">2024</a>)</cite>.
These works focus on constructing a combination of finetuning dataset for RAG and train a model to perform well on these tasks.
In particular, in their settings, at test time, the domain or documents can be different than the training time; whereas our paper studies a slightly opposite scenario where we only care about testing the LLM on the same set of documents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">RAFT is a training strategy designed to enhance the model’s performance in answering questions within a specific domain, in "open-book" settings. This technique demonstrates a fine-tuning recipe for LLMs for question-answering tasks based on a selected collection of documents. We have pinpointed several crucial design decisions, such as training the model alongside distractor documents, organizing the dataset so a portion lacks oracle documents in their context, and formulating answers in a chain-of-thought manner with direct quotations from the relevant text. Our evaluations on PubMed, HotpotQA, and Gorilla API Bench underline RAFT’s significant potential. Looking forward, we anticipate that in-domain Retrieval-Augmented Generation (RAG) will continue to gain interest within both industrial and academic spheres. Unlike general-RAG, our work addresses practical scenarios where LLMs are tasked with answering questions using domain-specific knowledge. Aligning with current trends, our findings suggest that smaller, fine-tuned models are capable of performing comparably well in domain-specific question-answering tasks, in contrast to their generic LLM counterparts.

</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anil, R., Dai, A.&nbsp;M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et&nbsp;al.

</span>
<span class="ltx_bibblock">Palm 2 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2305.10403</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Prompt engineering for claude’s long context window.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Self-rag: Learning to retrieve, generate, and critique through self-reflection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2310.11511</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den&nbsp;Driessche, G.&nbsp;B., Lespiau, J.-B., Damoc, B., Clark, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">International conference on machine learning</em>, pp.&nbsp; 2206–2240. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.&nbsp;D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., and Song, D.

</span>
<span class="ltx_bibblock">The secret sharer: Evaluating and testing unintended memorization in neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">28th USENIX Security Symposium (USENIX Security 19)</em>, pp.&nbsp; 267–284, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et&nbsp;al.

</span>
<span class="ltx_bibblock">Extracting training data from large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">30th USENIX Security Symposium (USENIX Security 21)</em>, pp.&nbsp; 2633–2650, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C.

</span>
<span class="ltx_bibblock">Quantifying memorization across neural language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">The Eleventh International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chung, H.&nbsp;W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2210.11416</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dernoncourt &amp; Lee (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dernoncourt, F. and Lee, J.&nbsp;Y.

</span>
<span class="ltx_bibblock">Pubmed 200k rct: a dataset for sequential sentence classification in medical abstracts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:1710.06071</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feldman (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Feldman, V.

</span>
<span class="ltx_bibblock">Does learning require memorization? a short tale about a long tail.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing</em>, pp.&nbsp; 954–959, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.

</span>
<span class="ltx_bibblock">Retrieval augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">International conference on machine learning</em>, pp.&nbsp; 3929–3938. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De&nbsp;Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Conference on Machine Learning</em>, pp.&nbsp; 2790–2799. PMLR, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hu, E.&nbsp;J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.

</span>
<span class="ltx_bibblock">Atlas: Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Journal of Machine Learning Research</em>, 24(251):1–43, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v24/23-0037.html" title="">http://jmlr.org/papers/v24/23-0037.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ji, C. C.-J., Mao, H., Yan, F., Shishir G.&nbsp;Patil, T.&nbsp;Z., Stoica, I., and Gonzalez, J.&nbsp;E.

</span>
<span class="ltx_bibblock">Gorilla openfunctions v2.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jin, Q., Dhingra, B., Liu, Z., Cohen, W.&nbsp;W., and Lu, X.

</span>
<span class="ltx_bibblock">Pubmedqa: A dataset for biomedical research question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1909.06146</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Joshi, M., Choi, E., Weld, D.&nbsp;S., and Zettlemoyer, L.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1705.03551</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kandpal et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kandpal, N., Wallace, E., and Raffel, C.

</span>
<span class="ltx_bibblock">Deduplicating training data mitigates privacy risks in language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">International Conference on Machine Learning</em>, pp.&nbsp; 10697–10707. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M.

</span>
<span class="ltx_bibblock">Generalization through memorization: Nearest neighbor language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:1911.00172</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et&nbsp;al.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Transactions of the Association for Computational Linguistics</em>, 7:453–466, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lazaridou et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N.

</span>
<span class="ltx_bibblock">Internet-augmented language models through few-shot prompting for open-domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2203.05115</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lester et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lester, B., Al-Rfou, R., and Constant, N.

</span>
<span class="ltx_bibblock">The power of scale for parameter-efficient prompt tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2104.08691</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et&nbsp;al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Advances in Neural Information Processing Systems</em>, 33:9459–9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li &amp; Liang (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Li, X.&nbsp;L. and Liang, P.

</span>
<span class="ltx_bibblock">Prefix-tuning: Optimizing continuous prompts for generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2101.00190</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lin, X.&nbsp;V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2310.01352</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lin, X.&nbsp;V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2310.01352</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Liu, H., Sferrazza, C., and Abbeel, P.

</span>
<span class="ltx_bibblock">Chain of hindsight aligns language models with feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2302.02676</em>, 3, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Liu, N.&nbsp;F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2307.03172</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., and Tang, J.

</span>
<span class="ltx_bibblock">P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pp.&nbsp; 61–68, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Liu, Z., Kitouni, O., Nolte, N.&nbsp;S., Michaud, E., Tegmark, M., and Williams, M.

</span>
<span class="ltx_bibblock">Towards understanding grokking: An effective theory of representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in Neural Information Processing Systems</em>, 35:34651–34663, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Liu, Z., Ping, W., Roy, R., Xu, P., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Chatqa: Building gpt-4 level conversational qa models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2401.10225</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Cross-task generalization via natural language crowdsourcing instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2104.08773</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Le&nbsp;Scao, T., Bari, M.&nbsp;S., Shen, S., Yong, Z.&nbsp;X., Schoelkopf, H., Tang, X., Radev, D., Aji, A.&nbsp;F., Almubarak, K., Albanie, S., Alyafeai, Z., Webson, A., Raff, E., and Raffel, C.

</span>
<span class="ltx_bibblock">Crosslingual generalization through multitask finetuning.

</span>
<span class="ltx_bibblock">In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.&nbsp; 15991–16111, Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2023.acl-long.891" title="">10.18653/v1/2023.acl-long.891</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.acl-long.891" title="">https://aclanthology.org/2023.acl-long.891</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Advances in Neural Information Processing Systems</em>, 35:27730–27744, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pan, X., Zhang, M., Ji, S., and Yang, M.

</span>
<span class="ltx_bibblock">Privacy risks of general-purpose language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">2020 IEEE Symposium on Security and Privacy (SP)</em>, pp.&nbsp; 1314–1331. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patil et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Patil, S.&nbsp;G., Zhang, T., Wang, X., and Gonzalez, J.&nbsp;E.

</span>
<span class="ltx_bibblock">Gorilla: Large language model connected with massive apis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2305.15334</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Power et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Power, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V.

</span>
<span class="ltx_bibblock">Grokking: Generalization beyond overfitting on small algorithmic datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2201.02177</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C.&nbsp;D., and Finn, C.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2305.18290</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham, Y.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2302.00083</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sanh, V., Webson, A., Raffel, C., Bach, S.&nbsp;H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T.&nbsp;L., Raja, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2110.08207</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E.&nbsp;H., Schärli, N., and Zhou, D.

</span>
<span class="ltx_bibblock">Large language models can be easily distracted by irrelevant context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">International Conference on Machine Learning</em>, pp.&nbsp; 31210–31227. PMLR, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L.

</span>
<span class="ltx_bibblock">Detecting pretraining data from large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2310.16789</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V., Smith, N.&nbsp;A., Zettlemoyer, L., Yih, S., and Lewis, M.

</span>
<span class="ltx_bibblock">In-context pretraining: Language modeling beyond document boundaries.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2310.10638</em>, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023d)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.

</span>
<span class="ltx_bibblock">Replug: Retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2301.12652</em>, 2023d.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tänzer et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tänzer, M., Ruder, S., and Rei, M.

</span>
<span class="ltx_bibblock">Memorisation versus generalisation in pre-trained language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.&nbsp; 7564–7578, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le, Q., et&nbsp;al.

</span>
<span class="ltx_bibblock">Freshllms: Refreshing large language models with search engine augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2310.03214</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Instructretro: Instruction tuning post retrieval-augmented pretraining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2310.07713</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.&nbsp;A., Khashabi, D., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language models with self-generated instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2212.10560</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.&nbsp;V., Zhou, D., et&nbsp;al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Advances in Neural Information Processing Systems</em>, 35:24824–24837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weston &amp; Sukhbaatar (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Weston, J. and Sukhbaatar, S.

</span>
<span class="ltx_bibblock">System 2 attention (is something you might need too).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2311.11829</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Workshop et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Workshop, B., Scao, T.&nbsp;L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A.&nbsp;S., Yvon, F., et&nbsp;al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2211.05100</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K.&nbsp;A., Oguz, B., et&nbsp;al.

</span>
<span class="ltx_bibblock">Effective long-context scaling of foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2309.16039</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Retrieval meets long context large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2310.03025</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.&nbsp;W., Salakhutdinov, R., and Manning, C.&nbsp;D.

</span>
<span class="ltx_bibblock">Hotpotqa: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:1809.09600</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhang, T., Liu, F., Wong, J., Abbeel, P., and Gonzalez, J.&nbsp;E.

</span>
<span class="ltx_bibblock">The wisdom of hindsight makes language models better instruction followers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2302.05206</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et&nbsp;al.

</span>
<span class="ltx_bibblock">Lima: Less is more for alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2305.11206</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et&nbsp;al.

</span>
<span class="ltx_bibblock">Lima: Less is more for alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2305.11206</em>, 2023b.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
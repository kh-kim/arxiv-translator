<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing\n' +
      '\n' +
      'Yucheng Hu\n' +
      '\n' +
      'East China University of Science and Technology\n' +
      '\n' +
      'huyc@mail.ecust.edu.cn\n' +
      '\n' +
      '&Yuxing Lu\n' +
      '\n' +
      'Peking University\n' +
      '\n' +
      'yxlu@613@gmail.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: [https://github.com/2471023025/RALM_Survey](https://github.com/2471023025/RALM_Survey).\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Natural Language Processing (NLP) is a significant focus within the realms of computer science and artificial intelligence, dedicated to the study of theoretical and methodological frameworks that enable effective communication between humans and computers using natural language. As a multidisciplinary field, NLP integrates linguistics, computer science, and mathematics with the aim of realizing the mutual transformation between human language and computer data. Its ultimate objective is to empower computers with the capability to process and "understand" natural language, thereby facilitating tasks such as automatic translation, text categorization, and sentiment analysis. The complexity of NLP is evident in the numerous steps it encompasses, including word segmentation, part-of-speech tagging, parsing, stemming, named entity recognition, and more, all of which contribute to the challenge of replicating human language understanding in artificial intelligence systems.\n' +
      '\n' +
      'Traditional natural language processing tasks typically employ statistic-based algorithms (Hogenboom et al., 2010)Serra et al. (2013)Aussenac-Gilles and Sorgel (2005) and deep learning algorithms such as convolutional neural network (CNN) (Yin et al., 2017), recurrent neural network (RNN) (Banerjee et al., 2019), long short-term memory network (LSTM) (Yao and Guan, 2018), and others. Recently, with the advent of the transformer architecture (Vaswani et al., 2017) as a leading representative of natural language processing, its popularity has grown significantly. The transformer architecture, as a prominent large language model (Lewis et al., 2019)Raffel et al. (2020) in the natural language processing domain, has consistently demonstrated enhanced performance, attracting the attention of an increasing number of researchers who are engaged in studying its capabilities.\n' +
      '\n' +
      'The most prevalent LMs nowadays are the GPT families (Radford et al., 2019) (Brown et al., 2020) (Achiam et al., 2023) and Bert families (Liu et al., 2019) (Devlin et al., 2018)(Sanh et al., 2019), which have been demonstrated to excel in a multitude of natural language processing tasks. Among these, the AutoEncoder language model is particularly adept at natural language understanding tasks,while the AutoRegressive language model is more suited to natural language generation tasks. While increasing parameters (Touvron et al., 2023b) and model tuning (Han et al., 2023) can enhance the performance of LLMs, the phenomenon of "hallucination" (Ji et al., 2023) persists. Furthermore, the limitations of LMs in effectively handling knowledge-intensive work (Feng et al., 2023) and their inability to promptly update their knowledge (Mousavi et al., 2024) are consistently apparent. Consequently, numerous researchers (Lewis et al., 2020) (Izacard and Grave, 2020b) (Khandelwal et al., 2019) have employed the technique of retrieval to obtain external knowledge, which can assist the language model in attaining enhanced performance in a multitude of tasks.\n' +
      '\n' +
      'Currently, there is a paucity of surveys on the use of retrieval augmentation to enhance the performance of LLMs. Zhao et al. (2023) provide a comprehensive overview of work on RAG for multimodality. Zhao et al. (2024a) concentrate on the utilisation of retrieval augmentation generation techniques for the Artificial Intelligence Generated Content (AIGC) domain. This article provides a comprehensive overview of recent RAG work, but it does not cover all relevant domains. Additionally, the article lacks sufficient detail to provide a comprehensive timeline of the overall development. Gao et al. (2023) investigate the enhancement of RAG for large models. This article summarizes some of the recent RAG work, but it introduces the retrievers and generators independently, which is not conducive to the upgrading and interactions with the components of subsequent work. Li et al. (2022b) focus on text generation only. The article has fewer figures and tables, and the content\n' +
      '\n' +
      'Figure 1: A general overview of this surveyâ€™s work\n' +
      '\n' +
      'is more abstract, which is not conducive to the reader\'s understanding.\n' +
      '\n' +
      'Also, surveys on RAG only tells half of the story in retrieval-augmented methods in NLP. Not only do tasks associated with NLG require retrieval enhancement techniques, but NLU tasks also necessitate external information. To date, there is a scarcity of comprehensive surveys that thoroughly review the application of augmented retrieval techniques across the spectrum of NLP. In order to improve the current situation, this paper presents the following contributions:\n' +
      '\n' +
      '(1) The article does not merely focus on the work related to RAG; it also places significant emphasis on RALM and aligns with the concept of NLP. The work related to generation aligns with NLG, while the rest of the work aligns with NLU.\n' +
      '\n' +
      '(2) The two components of RALM, the Retriever and the Language Model, are described in detail, and the different interaction modes of these two components are precisely defined for the first time.\n' +
      '\n' +
      '(3) A comprehensive overview of the RALM work schedule is provided, along with a summary of the common and novel applications of current RALM, accompanied by an analysis of the associated limitations. Potential solutions to these limitations are proposed, along with recommendations for future research directions.\n' +
      '\n' +
      'Figure 1 provides a general overview of the framework of RALM methods. The following is a summary of the paper: Section 2 defines RALM. Section 3 provides a detailed classification and summary of the work of retrievers in RALM. Section 4 provides a detailed classification and summary of the work of LMs in RALM. Section 5 provides a classification and summary of specific enhancements to RALM. Section 6 of RALM is a classification and summary of the sources of retrieved data. Section 7 is a summary of RALM applications. Section 8 is a summary of RALM evaluations and benchmarks. Finally, Section 9 is a discussion of the limitations of existing RALM and directions for future work.\n' +
      '\n' +
      '## 2 Definition\n' +
      '\n' +
      'Retrieval-Augmented Language Model (RALM) is the process of refining the output of the LM with retrieved information to obtain a satisfactory result for the user. This section provides a detailed definition of the different modes of RALM by categorising the ways in which the retriever interacts with the language model. The specific categorization of interactions can be seen in Figure 2. In addition, the development history of each interaction method can be seen in Figure 3. Assuming that \\(z\\) is the retrieved message, \\(x\\) is the input, \\(y\\) is the output, and \\(F()\\) is a function, either a language model or a data processing function, with \\(x\\) and \\(z\\) as independent variables, the basic architecture of RALM is defined as follows:\n' +
      '\n' +
      '\\[y=F(x,z) \\tag{1}\\]\n' +
      '\n' +
      '### Sequential Single Interaction\n' +
      '\n' +
      'The sequential single interaction process involves finding the Top-K relevant documents \\(z\\) to input \\(x\\) through a retriever \\(P_{\\eta}(z|x)\\), where \\(\\eta\\) is a parameter of the retriever. Subsequently, the language model \\(P_{\\theta}(y_{i}|x,z,y_{r})\\) receives input \\(x\\) along with relevant documents \\(z\\) and outputs the i-th token \\(y_{i}\\). Parameter \\(\\theta\\) is used, along with relevant output tokens \\(y_{r}\\). The number of relevant output tokens is related to the location and type of language model. The RALM for sequential single interaction is defined as follows:\n' +
      '\n' +
      '\\[y_{i}=LM(z,x,y_{r}) \\tag{2}\\]\n' +
      '\n' +
      'When RALM was first proposed, many researchers used this method because it aligned with their original ideas, particularly those of Lewis et al. (2020), Guu et al. (2020), and Izacard and Grave (2020).\n' +
      '\n' +
      '### Sequential Multiple Interactions\n' +
      '\n' +
      'As RALM technology develops, researchers have discovered that a single interaction is insufficient for long dialogue generation and solving multi-hop problems. Therefore, a method with multiple interactions between a retriever and a language model has been proposed. In this method, the researcher includes step s and typically has the language model generate the output first. When a retrieval technique is necessary, the outputted content is used for retrieval and the relevant formulas are expressed as follows:\n' +
      '\n' +
      '\\[y_{s}=LM(z,x|y_{<s}) \\tag{3}\\]\n' +
      '\n' +
      'where \\(y_{s}\\) represents the generated tokens at the current step \\(s\\). Among the researchers who have employed this method, the most renowned are Jiang et al. (2023), He et al. (2022), and Khattab et al. (2022).\n' +
      '\n' +
      '### Parallel Interaction\n' +
      '\n' +
      'In all of the previously mentioned approaches, the flow of information has a clear sequential structure, whether from the retriever to the language model or from the language model to the retriever. However, this sequential structure may not be optimal in all domains and may be less extensible, it is important to consider alternative approaches. Researchers have proposed a novel parallel structure in which the retriever and the language model work independently for the user input \\(x\\). The output \\(y\\) is then determined by weighted interpolation. \\(I()\\) is the interpolation function. The relevant equations are expressed as follows:\n' +
      '\n' +
      '\\[y=I(LM(x,y_{r}),z) \\tag{4}\\]\n' +
      '\n' +
      'the specific interpolation function is:\n' +
      '\n' +
      '\\[p(y|x)=\\lambda p_{R}(y|x)+(1-\\lambda)p_{LM}(y|x) \\tag{5}\\]\n' +
      '\n' +
      'where the retrieved output tokens are denoted by \\(p_{R}(y|x)\\), the language model generated output tokens are denoted by \\(p_{LM}(y|x)\\), and the weights are denoted by \\(\\lambda\\). Among the researchers who have employed this method, the most renowned are Khandelwal et al. (2019), Alon et al. (2022), and He et al. (2021).\n' +
      '\n' +
      '## 3 Retriever\n' +
      '\n' +
      'Retrievers play a crucial role in the RALM architecture. The information obtained through retrievers can significantly improve the accuracy of the LM. This section provides a summary of the retrieval methods commonly used in the RALM architecture. The retrieval methods are classified into four categories based on their methods and sources: Sparse Retrieval, Dense Retrieval, Internet Retrieval, and Hybrid Retrieval. Table 1 lists information about specific applications of retrievers in the RALM.\n' +
      '\n' +
      '### Sparse Retriever\n' +
      '\n' +
      'For a period of time following the proposal of the retrieval technique, sparse retrieval proves to be a straightforward and effective tool in solving problems, particularly those based on knowledge. One of the main advantages of sparse retrieval is its simplicity, which can be easily integrated into existing indexing systems due to the fewer dimensions involved. (Hambarde and Proenca, 2023)This is consistent with human cognitive processes. Additionally, sparse retrieval is easier to generalise and more efficient. Sparse retrieval used in RALM can be classified into two categories: Word Frequency and\n' +
      '\n' +
      'Figure 2: Three different ways the Retriever interacts with the LMSparse Vector Representation. The choice between the two depends on whether machine learning is used.\n' +
      '\n' +
      '#### 3.1.1 Word Frequency\n' +
      '\n' +
      'In the initial stage, individuals often use methods for retrieval that involve matching of relevant content, such as the TF-IDF Ramos et al. (2003) and BM25 Robertson et al. (1995) algorithms, which are considered classic and effective.\n' +
      '\n' +
      'The TF-IDF algorithm utilises term frequency (TF) and inverse document frequency (IDF) to represent relevance, which has the advantages of simplicity and speed, and even if the corpus is unchanged, the TF-IDF value for each word can be computed in advance. In RALM, Lazaridou et al. (2022) utilise the TF-IDF algorithm to match information obtained from user queries and calls to the Google search API. Hua and Wang (2018) also employ the algorithm to score the generated results. The BM25 represents an enhancement over the TF-IDF. It considers the user\'s query and calculates the relevance score as the weighted sum of the relevance of each query word to the document. The IDF algorithm is used to derive the weight of each word, but it is improved by two moderating factors to prevent the strength of the influence of a certain factor from being infinite. This is consistent with\n' +
      '\n' +
      'Figure 3: A roadmap of the three types of interactions. The purple areas represent work on Sequential Interaction RALM models, the red boxes signify work on Sequential Multiple Interactions RALMs models, and the yellow areas indicate work on Parallel Interaction RALM models.\n' +
      '\n' +
      'common sense. Due to its excellent generalisation capabilities, many Retrieval-Augmented Language Model (RaLM) architectures, particularly those oriented towards open domains, employ BM25 as a retrieval method, such as Jiang et al. (2023c), Ram et al. (2023) and Zhong et al. (2022).\n' +
      '\n' +
      '#### 3.1.2 Sparse Vector Representation\n' +
      '\n' +
      'It has become evident that simple term matching is no longer sufficient to meet the demand. Manual labelling can solve problems such as the synonym issue, but it is a resource-intensive method. With the rise of machine learning, sparse vectors are now used to represent words and retrieve them by calculating the distance between them. (Hambarde and Proenca, 2023) Sparse vector representation techniques differ from term matching methods in that they construct sparse vectors for queries and documents. The purpose of these representations is to capture the semantic essence of each input text, which places queries and documents in a latent space.\n' +
      '\n' +
      'Ram et al. (2021) utilised the fact that when given two paragraphs with the same repeat span, one was used to construct a query and the other as the retrieval target. The remaining paragraph in the document, which did not contain a repeat span, was used as a negative example and Ram\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|l|l|l|l|} \\hline \\multicolumn{1}{|c|}{ Category} & Technique & Year & Reference \\\\ \\hline \\multirow{10}{*}{Sparse Retrieval} & \\multirow{8}{*}{Word Frequency} & \\multirow{8}{*}{BM25(Robertson et al., 1995)} & 2023 & (Jiang et al., 2023c) \\\\ \\cline{3-5}  & & & 2023 & (Ram et al., 2023) \\\\ \\cline{3-5}  & & & 2022 & (Zhong et al., 2022) \\\\ \\cline{3-5}  & & & 2024 & (Cheng et al., 2024) \\\\ \\cline{3-5}  & & & 2020 & (Lazard and Ghare, 2020b) \\\\ \\cline{3-5}  & & & 2022 & (Mallen et al., 2022) \\\\ \\cline{3-5}  & & & 2024 & (Schield et al., 2024) \\\\ \\cline{3-5}  & & & 2023 & (Xu et al., 2023) \\\\ \\cline{3-5}  & & & 2022 & (Hle et al., 2022) \\\\ \\cline{3-5}  & & & 2023 & (Yu et al., 2023) \\\\ \\cline{3-5}  & & & 2022 & (Ado et al., 2022) \\\\ \\cline{3-5}  & & KNN search & 2022 & (Borgnod et al., 2022) \\\\ \\cline{3-5}  & & & 2019 & (Khandelwal et al., 2019) \\\\ \\cline{3-5}  & & GUD-IR(Madan et al., 2022) & 2022 & (Madan et al., 2022) \\\\ \\cline{3-5}  & & GAR(Mao et al., 2020) & 2020 & (Mao et al., 2020) \\\\ \\cline{3-5}  & & Spider(Ram et al., 2021) & 2023 & (Ram et al., 2023) \\\\ \\hline \\multirow{10}{*}{Dense Retrieval} & \\multirow{8}{*}{COLBERTY2(Santhanam et al., 2021)} & 2022 & (Izacard et al., 2022) \\\\ \\cline{3-5}  & & & 2021 & (Thulley et al., 2021) \\\\ \\cline{3-5}  & & & 2023 & (Holkatner et al., 2023) \\\\ \\cline{3-5}  & & & 2023 & (Sriwardhama et al., 2023) \\\\ \\cline{3-5}  & & & 2021 & (Sachan et al., 2021) \\\\ \\cline{3-5}  & & & 2020 & (Guan et al., 2020) \\\\ \\cline{3-5}  & & & 2020 & (Ram et al., 2023) \\\\ \\cline{3-5}  & & & 2020 & (Lazard and Ghare, 2020b) \\\\ \\cline{3-5}  & & & 2020 & (Karpukhin et al., 2020) \\\\ \\cline{3-5}  & & & 2020 & (Karpukhin et al., 2020) \\\\ \\cline{3-5}  & & & 2022 & (Mallen et al., 2022) \\\\ \\cline{3-5}  & & & 2020 & (Izacard and Ghare, 2020a) \\\\ \\cline{3-5}  & & & 2021 & (Izacard and Ghare, 2020b) \\\\ \\cline{3-5}  & & & 2021 & (Izacard and Ghare, 2020a) \\\\ \\cline{3-5}  & & & 2021 & (Izacard et al., 2019) \\\\ \\cline{3-5}  & & & 2021 & (He et al., 2021) \\\\ \\cline{3-5}  & & & 2023 & (Yoran et al., 2023) \\\\ \\cline{3-5}  & & & 2022 & (Mallen et al., 2022) \\\\ \\cline{3-5}  & & & 2021 & (Santharam et al., 2021) \\\\ \\hline \\multirow{10}{*}{Multimodal Retrieval} & MultiMedia(Chen et al., 2022b) & 2022 & (Chen et al., 2022b) \\\\ \\cline{3-5}  & & & 2022 & (Yasumga et al., 2022) \\\\ \\cline{3-5}  & & & 2022 & (Chen et al., 2022) \\\\ \\cline{3-5}  & & & 2022 & (Chen et al., 2022) \\\\ \\cline{3-5}  & & & 2022 & (Chen et al., 2022) \\\\ \\cline{3-5}  & & & 2022 & (Li et al., 2022a) \\\\ \\cline{3-5}  & & & 2022 & (Bittmann et al., 2022) \\\\ \\cline{3-5}  & & & 2023 & (Lin et al., 2023b) \\\\ \\cline{3-5}  & & & 2021 & (Shi et al., 2023b) \\\\ \\hline \\multirow{10}{*}{Internet Retrieval} & \\multirow{8}{*}{Hybrid Retrieval} & FLARE(Jiang et al., 2023c) & 2023 & (Jiang et al., 2023c) \\\\ \\cline{3-5}  & & & 2023 & (Yoran et al., 2023) \\\\ \\cline{3-5}  & & & 2021 & (Konelli et al., 2021) \\\\ \\cline{1-1} \\cline{3-5}  & & & 2021 & (Nakano et al., 2021) \\\\ \\hline \\multirow{10}{*}{Hybrid Retrieval} & \\multirow{8}{*}{Hybrid Retrieval} & Doc\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '2021) involves transferring data from a pre-trained, larger model to a smaller one, often using methods such as embedded matching. Research has even been conducted on data distillation using LMs as the technology has evolved.\n' +
      '\n' +
      'Shi et al. (2023) utilises knowledge distillation to divide the retrieval process into four distinct steps. Firstly, documents are retrieved and retrieval likelihoods are computed. Secondly, the retrieved documents are scored using a language model. Thirdly, the parameters of the retrieval model are updated by minimising the KL discrepancy between the retrieval likelihoods and the distribution of the language model scores. Finally, asynchronously updating of the indexes of the data is performed. Based on this technique, Lin et al. (2023) further improve the accuracy of knowledge distillation. They present a data distillation scheme that combines sentence truncation and query enhancement with incremental relevance label enhancement using multiple enhancers.\n' +
      '\n' +
      '### Internet Retrieval\n' +
      '\n' +
      'With the advancement of Internet search and sorting technology, some researchers have focused their search efforts on Internet retrieval, which is a plug-and-play approach. This approach allows non-specialists to benefit from RALM and is better suited to the open domain and generalisation. Another advantage of this retrieval model is that it does not require real-time updating of the database, but relies on updates from commercial search engines. However, despite the advantages of simplicity and convenience, there is a significant amount of irrelevant and even harmful information on the Internet that can hinder the work of RALM. If an effective screening mechanism is not implemented, the effectiveness of RALM will be significantly reduced.\n' +
      '\n' +
      'Unlike most studies (Yoran et al., 2023) (Nakano et al., 2021) that directly utilise commercial search engine APIs. Komeili et al. (2021) propose an alternative approach to using multiple commercial search engine APIs. They suggest using the Bing Search API to generate a list of URLs for each query. These URLs are then used as keys to look up their page content in a lookup table constructed from public crawl snapshots, which populates a set of pages for that query. In addition, the evaluation takes into account whether the URL is from the English Wikipedia. If so, the page title is extracted from the URL and the corresponding page is searched for in the Wikipedia dump.\n' +
      '\n' +
      '### Hybrid Retrieval\n' +
      '\n' +
      'As researchers gain a better understanding of the strengths and weaknesses of various retrieval techniques, they are increasingly opting to combine them, as described above. This is done in the hope of further exploiting the advantages of these techniques to improve the effectiveness and robustness of the RALM architecture.\n' +
      '\n' +
      'To tackle the issue of inaccurate Internet retrieval results, Lazaridou et al. (2022) proposed using the TF-IDF algorithm to score the retrieval results. They used each question q verbatim as a query and issued a call to Google Search via the Google Search API. For each question, they retrieved the top 20 URLs and parsed their HTML content to extract clean text, generating a set of documents D for each question q. To prevent irrelevant information from hindering the resolution of a user\'s query, Hu et al. (2023) designed a gating circuit. This circuit utilised a dual-encoder dot product to calculate similarity and a gating circuit based on term weights. Additionally, Boytsov et al. (2016) presented an approach that replaced term-based retrieval with k-Nearest Neighbors(kNN) search while combining a translation model and BM25 to improve retrieval performance. This approach enabled the model to take into account the semantic relationships between terms and traditional statistical weighting schemes, resulting in a more efficient retrieval system.\n' +
      '\n' +
      '## 4 Language Models\n' +
      '\n' +
      'Although humans used to rely solely on searching for information, the development of language models has revolutionised the field of natural language processing, making it more vibrant and creative. In contrast to LM, which employs solely the parameters derived from training to complete the task, RALM integrates the nonparametric memory acquired by the retriever with the parametric memory of LM itself to create a semiparametric memory, thereby enhancing the performance of the language model. In the RALM architecture, many researchers utilise off-the-shelf language models for evaluation. This section introduces the language models commonly used in RALM architectures and classifies them into three categories: AutoEncoder-language model, AutoRegressive language model and Encoder-Decoder model. Table 2 lists information about specific applications of LM in the RALM.\n' +
      '\n' +
      '### AutoEncoder Language Model\n' +
      '\n' +
      'The logical process of an AutoEncoder is that the original input (set to x) is weighted and mapped to y, which is then inversely weighted and mapped back to z. If through iterative training the loss function L(H) is minimised, i.e. z is as close to x as possible, i.e. x is perfectly reconstructed, then it can be said that forward weighting is a successful way of learning the key features of the input. AutoEncoder language models take their name from the Denoising AutoEncoder (DAE) (Vincent et al., 2008), which is used to predict tokens that are [masked] by contextual words (these [masked] words are actually noise added at the input, typical of thinking). DAE is a technique that involves adding random noise to the input layer of data. This helps to learn more robust features when using an unsupervised approach to pre-train the weights of a deep network in a hierarchical manner.\n' +
      '\n' +
      'Most of AutoEncoder language models are highly generalisable, unsupervised, and do not require data annotation. They can naturally incorporate contextual semantic information. However, the independence assumption introduced in the Pre-Training stage means that the correlation between\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|l|l|l|} \\hline Category & Technique & Year & Reference \\\\ \\hline \\multirow{3}{*}{AutoEncoder Language Model} & RoBERTaDevlin et al. (2018) & 2021 & Thulke et al. (2021) \\\\ \\cline{2-4}  & BERTLiu et al. (2019) & 2021 & Sachan et al. (2021) \\\\ \\hline \\multirow{9}{*}{AutoRegressive Language Model} & GPT-3.5 & 2023 & Jiang et al. (2023c) \\\\ \\cline{2-4}  & GPT-2Rafford et al. (2019) & 2023 & Khattab et al. (2022) \\\\ \\cline{2-4}  & GPT-NeoBlack et al. (2022) & 2022 & Mallen et al. (2022) \\\\ \\cline{2-4}  & chatGPT & 2023 & Feng et al. (2023) \\\\ \\cline{2-4}  & GPT Family & GPT-4Achiam et al. (2023) & 2023 & Asai et al. (2023) \\\\ \\cline{2-4}  & GPT-3Brown et al. (2020) & 2022 & Maoan et al. (2022) \\\\ \\cline{2-4}  & GPT-3Brown et al. (2020) & 2022 & He et al. (2022) \\\\ \\cline{2-4}  & GPTRafford et al. (2018) & 2023 & Shi et al. (2023b) \\\\ \\cline{2-4}  & GPT-J & 2024 & Schick et al. (2024) \\\\ \\cline{2-4}  & GPT-J & 2024 & Hu et al. (2024) \\\\ \\cline{2-4}  & \\multirow{3}{*}{Llama Family} & 2024 & Yan et al. (2024) \\\\ \\cline{2-4}  & & 2023 & Asai et al. (2023) \\\\ \\cline{2-4}  & & 2023 & Wang et al. (2023c) \\\\ \\cline{2-4}  & & 2023 & Yoran et al. (2023) \\\\ \\cline{2-4}  & & 2023 & Lin et al. (2023b) \\\\ \\cline{2-4}  & & 2023 & Luo et al. (2023a) \\\\ \\hline \\multirow{9}{*}{Others} & AlpacaDubois et al. (2024) & 2024 & Yan et al. (2024) \\\\ \\cline{2-4}  & OPTZhang et al. (2022) & 2023 & Ram et al. (2023) \\\\ \\cline{2-4}  & XGHMLin et al. (2022) & 2024 & Cheng et al. (2024) \\\\ \\cline{2-4}  & BLOOMWorkshop et al. (2022) & 2023 & Shi et al. (2023b) \\\\ \\cline{2-4}  & MistralJiang et al. (2023a) & 2024 & Hu et al. (2024) \\\\ \\hline \\multirow{9}{*}{Encoder-Decoder Language Model} & & 2022 & Izaard et al. (2022) \\\\ \\cline{2-4}  & & 2023 & Hofstatter et al. (2023) \\\\ \\cline{2-4}  & & 2021 & Sachan et al. (2021) \\\\ \\cline{2-4}  & & 2023 & Wang et al. (2023c) \\\\ \\cline{2-4}  & T5Raffel et al. (2020) & 2022 & Chen et al. (2022b) \\\\ \\cline{2-4}  & & 2021 & Singh et al. (2021) \\\\ \\cline{2-4}  & & 2020 & Izaard and Grave (2020a) \\\\ \\cline{2-4}  & & 2022 & Lazaridou et al. (2022) \\\\ \\cline{2-4}  & & 2023 & Hu et al. (2023) \\\\ \\cline{2-4}  & & 2021 & Thulke et al. (2021) \\\\ \\cline{2-4}  & & 2023 & Siwirandana et al. (2023) \\\\ \\cline{2-4}  & & 2019 & Lewis et al. (2019) \\\\ \\cline{2-4}  & BARTlLewis et al. (2019) & 2020 & Lewis et al. (2020) \\\\ \\cline{2-4}  & & 2023 & Yoran et al. (2023) \\\\ \\cline{2-4}  & & 2020 & Izaard and Grave (2020a) \\\\ \\cline{2-4}  & & 2022 & Lazaridou et al. (2022) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: Summary of LMs in RALM methods.\n' +
      '\n' +
      'predicted [MASK] is not considered. Additionally, the introduction of [Mask] as a special marker in the input to replace the original Token creates inconsistency between the data in the Pre-Training stage and the Fine-Tuning stage, where [Mask] is not present. Self-encoding language models are commonly used in RALM architectures for natural language understanding (NLU) tasks.\n' +
      '\n' +
      'As AutoEncoder language models excel at Natural Language Understanding (NLU) tasks, many RALM architectures(Thulke et al., 2021) (Cheng et al., 2024) (Sachan et al., 2021)utilise them for specific tasks, such as judgement. One of the most commonly used models is BERT and its improved versions.Devlin et al. (2018) proposed the BERT model, which was inspired by closed tasks(Taylor, 1953). RoBERTa(Liu et al., 2019) is trained using dynamic masking, full sentences without NSP loss, large mini-batches, and a larger byte-level BPE to address the lack of training of Bert\'s model. According to Jiang et al. (2020), BERT heavily relies on global self-attention blocks, resulting in a large memory footprint and computational cost. Although all attention heads query the entire input sequence, some only need to learn local dependencies, leading to computational redundancy. To address this issue, They proposed a new span-based dynamic convolution to replace these self-attention heads and directly model local dependencies. The new convolutional head, along with other self-attentive heads, forms a hybrid attention block. Furthermore, Sanh et al. (2019) was able to decrease the size of the BERT model by 40% while maintaining 97% of its language comprehension abilities and achieving a 60% increase in speed by implementing knowledge distillation during the pre-training phase.\n' +
      '\n' +
      '### AutoRegressive Language Model\n' +
      '\n' +
      'The primary purpose of an AutoRegressive language model is to predict the next word based on the preceding words. This is commonly known as left-to-right language modelling, where the token at the current time t is predicted based on the first t-1 tokens.\n' +
      '\n' +
      'This model has the advantage of being left-to-right, which is beneficial for generative natural language processing tasks like dialog generation and machine translation. AutoRegressive language models are well-suited to this process, making this model a popular choice for NLG tasks in the field of RALM. However, The information in question can be utilized only from the preceding or following text, and not in combination with both. OpenAI has made a notable impact on the field of research pertaining to autoregressive language models. Recently, Google has also made advancements in research on the model.\n' +
      '\n' +
      'The GPT family is one of the most common examples of AutoRegressive language models. It was first proposed by Radford et al. (2018), who identified a basic architecture of unsupervised pre-training followed by fine-tuning. Radford et al. (2019) later proposed zero-shot learning based on GPT. Later, Brown et al. (2020) proposed GPT-3 using an approach similar to Radford et al. (2019), which involved scaling up and abandoning fine-tuning. They also utilized alternately dense and locally banded sparse attentional patterns in the transformer layer, similar to the sparse transformer Child et al. (2019). There are also several related studies, such as GPT-NEO(Black et al., 2022) and ChatGPT, which use Reinforcement Learning from Human Feedback (RLHF). RLHF has significantly enhanced the accuracy of GPT models. Although ChatGPT is not open source, many researchers Jiang et al. (2023) (Khattab et al., 2022) still use its API for generative tasks in RALM. Recently, report on GPT-4(Achiam et al., 2023) have appeared, with the main focus on building a predictable and scalable deep learning stack dedicated to improving GPT-4\'s safety and alignment. Many researchers Asai et al. (2023) (Luo et al., 2023) have recently used GPT-4 to generate prompts for RALM.\n' +
      '\n' +
      'The Llama family is a well-known class of AutoRegressive language models. Llama(Touvron et al., 2023) was first proposed as a language model that uses only publicly available data. To improve training stability, they normalise the input of each transformer sub-layer instead of normalising the output. They use the RMSNorm normalisation function introduced by Zhang and Sennrich (2019) and replace the ReLU non-linearity with the SwiGLU activation function introduced by Shazeer (2020) to improve performance. Furthermore, the authors replaced the absolute position embedding with the rotational position embedding (RoPE) introduced by Su et al. (2024) at each layer of the network. Llama2(Touvron et al., 2023) used supervised fine-tuning, initial and iterative reward modelling and RLHF in their experiments. they also invented a new technique, Ghost Attention (GAtt), which helps to control the flow of dialogue in multiple turns. Qwen(Bai et al., 2023), based on Llama, the following adjustments were made: 1. The method of loose embedding was chosen instead of bundling the weights of input embedding and output projection to save memory cost. 2. The accuracy of the inverse frequency matrix was improved. 3. For most of the layers, bias was eliminated as per Chowdhery et al. (2023). However, bias was added in the QKV layer to enhance the model\'s extrapolation capability. The traditional layer normalization technique described in Ba et al. (2016) was replaced with RMSNorm. They chose SwiGLU as their activation function, which is a combination of Swish Ramachandran et al. (2017) and gated linear units (Dauphin et al., 2017). The dimension of the feed-forward network (FFN) is also reduced. Furthermore, Mistral 7b (Jiang et al., 2023) utilises Grouped Query Attention (GQA) to enhance inference speed and combines it with Sliding Window Attention (SWA) to efficiently process sequences of any length with reduced inference cost. These techniques demonstrate superior performance over Llama2.The Llama model is open source and uses publicly available data, providing researchers (Yan et al., 2024) (Asai et al., 2023) (Wang et al., 2023) with more opportunities to expand. As a result, many researchers use the Llama family as language models in the RALM architecture.\n' +
      '\n' +
      '### Encoder-Decoder Language Model\n' +
      '\n' +
      'Transformer(Vaswani et al., 2017) is an "encoder-decoder" architecture, which consists of encoders and decoders superimposed on multi-head self-attention modules. Among them, the input sequence is divided into two parts, the source sequence and the destination sequence. The former is input to the encoder and the latter is input to the decoder, and both sequences need to embed representation and add position information. The Transformer architecture enables parallel computation and the processing of entire text sequences simultaneously, resulting in a significant increase in model training and inference speed.\n' +
      '\n' +
      'Raffel et al. (2020) introduces a unified framework for converting all text-based language problems into text-to-text format. The aim is to explore the potential of transfer learning techniques for natural language processing. In contrast to the original transformer, a simplified version of layer normalization is used, where activations are rescaled without additional biases. After applying layer normalization, a residual skip connection (He et al., 2016) adds the input of each subcomponent to its output. Srivastava et al. (2014) is applied to the feed-forward network, the skip connections, the attentional weights, and the inputs and outputs of the entire stack. The T5 model has been widely used as a language model by many researchers, such as Hofstatter et al. (2023), Sachan et al. (2021), and Singh et al. (2021). Additionally, Chung et al. (2022) proposed instruction tuning as an approach to improve model performance. The study focused on three aspects: the number of scaling tasks, the size of the scaled model, and the fine-tuning of chain of thought data. The results showed that larger model sizes and more fine-tuning tasks significantly improved model performance. Additionally, the study found that chain of thought(CoT) significantly improves inference level. Wang et al. (2023) used this approach to tune T5 and apply it to the RALM architecture.\n' +
      '\n' +
      'BART(Lewis et al., 2019) is an Encoder-Decoder model that allows for arbitrary noise transformations, as the input to the encoder does not need to align with the output of the decoder. In this case, the document is corrupted by replacing the text span with mask symbols. For pre-training, the researchers proposed five models: Token Masking, Token Deletion, Text Infilling, Sentence Permutation, and Document Rotation. For fine-tuning, the encoder and decoder are fed an uncorrupted document, and the representation of the final hidden state from the decoder is used. Many researchers(Thulke et al., 2021) (Siriwardhana et al., 2023) (Lewis et al., 2019) have adopted BART as the language model in the RALM architecture due to its comprehensive and novel pre-training approach, which greatly enhances the model\'s robustness.\n' +
      '\n' +
      '## 5 RALM Enhancement\n' +
      '\n' +
      'This section describes how researchers in the RALM architecture improved the output quality by enhancing its components. We divided the improvement method into three parts: Retriever Enhancement, LM Enhancement, and Overall Enhancement. Figure 4 illustrates the categorization of enhancement methods.\n' +
      '\n' +
      '### Retriever Enhancement\n' +
      '\n' +
      'This section presents the researchers\' efforts on the retriever side, which include Retrieval Quality Control and Retrieval Timing Optimization.\n' +
      '\n' +
      '#### 5.1.1 Retrieval Quality Control\n' +
      '\n' +
      'Shi et al. (2023a) argue that retrieval can produce documents that not only fail to provide helpful information but can also compromise the quality of the language model output. As a result, many scholars in the field of RALM focus on improving the relevance between the retrieved content and the user\'s input to enhance the final output\'s quality.\n' +
      '\n' +
      'Lin et al. (2023b) propose an approach for instruction-tuning. They update the query encoder using a generalised LM supervised retrieval (LSR) (Shi et al., 2023b)training target that completes the computation through a combination of supervised tasks and unsupervised text. This enables the retriever to produce more contextually relevant results that are consistent with LLM preferences. Inspired by this instruction-tuning approach, Asai et al. (2023) proposed a more sophisticated model trained on an instruction-tracking dataset: SELF-RAG. As a result of their refinements, SELF-RAG can retrieve and select the best possible model outputs on demand through fine-grained self-reflection, making it broadly applicable, more robust, and controllable. In contrast to approaches that aim to enhance the quality of retrieved documents through the use of external models, such as natural language inference (Yoran et al., 2023) and summarization(Xu et al., 2023) models, SELF-RAG proposes entirely novel ideas. The model divides the retrieved document into parallel segments and compares their relevance. It then combines the most similar parts of the document. Yan et al. (2024) improves on SELF-RAG by designing a correction strategy to address inaccurate retriever results. They classify the information into three categories: CORRECT, INCORRECT, and AMBIGUOUS. If the information is CORRECT, the document is refined and filtered. If it is INCORRECT, the document is discarded and the web is searched for retrieval. The term AMBIGUOUS indicates a lack of confidence in the accuracy of a judgement. In this case, a combination of the two methods mentioned above will be used. Additionally, Wang et al. (2023c) proposed FILCO, a method for retrieving document content with sentence precision through three filters: STRING, lexical overlap, and conditional cross-mutual information (CXMI).\n' +
      '\n' +
      '#### 5.1.2 Retrieval Timing Optimization\n' +
      '\n' +
      'Researchers typically consider the timing of retrieval in two situations: when working on tasks that require multiple retrievals, such as long dialogue generation and multi-hop problems, or when it is impossible to find a suitable and relevant document. Using irrelevant documents can harm the accuracy of the output.\n' +
      '\n' +
      'A simple way to determine the timing of a retrieval is to adjust the retrieval steps. Ram et al. (2023) utilised an approach that involved a prefix encoding to adjust the runtime cost. The prefix encoding of the generated content was constantly recalculated. The choice of retrieval stride is a trade-off between runtime and performance. According to Toolformer (Schick et al., 2024), the search command can be used directly to retrieve useful information when the model needs to retrieve documentation help in the process of generating content. Inspired by this idea, Jiang et al. (2023c) propose two methods for determining the timing of retrieval. The first method involves interrupting the generation of the LM when it encounters a place where a retrieval needs to be performed and then performing the retrieval operation. The second method involves generating a temporary sentence in its entirety. If there is a low\n' +
      '\n' +
      'Figure 4: Classification of RALM enhancement methods.\n' +
      '\n' +
      'confidence marker in the sentence, the marker is masked and the rest of the sentence is used for retrieval. Yu et al. (2023) also employed LM to determine the timing of retrieval. However, instead of generating low-confidence markers using LM, they had LM score the output before and after retrieval.Mallen et al. (2022)\'s approach differed from the traditional method of having LMs generate low-confidence markers. Instead, they used Wikipedia page views as a measure of prevalence and converted knowledge triples of wiki data with varying levels of prevalence into natural language questions anchored to the original entity and relation types. This approach is more objective and avoids subjective evaluations. For tasks that required reasoning, both He et al. (2022) and Khattab et al. (2022) used chain of thought(CoT) to determine when to perform a retrieval.\n' +
      '\n' +
      '### LM Enhancement\n' +
      '\n' +
      'This section presents the researchers\' efforts in language modelling, including Pre-Generation Retrieval Processing, Structural Model Optimization, and Post-Generation Output Enhancement.\n' +
      '\n' +
      '#### 5.2.1 Pre-Generation Retrieval Processing\n' +
      '\n' +
      'The RALM architecture initially used a single document for retrieval augmentation. However, it was discovered that RALM\'s performance significantly improved when the number of retrieved paragraphs was increased. (Izacard and Grave, 2020) Therefore, they proposed a new method called Fusion-in-Decoder (FiD) which involves keeping the retriever unchanged, using the encoder in LM to encode the related documents one by one, and then connecting the related documents and giving them to the decoder for output. Then Hofstatter et al. (2023) improved on the FiD. They constrained the information flow from encoder to decoder. FiD-Light with reranking was also tuned via text source pointers to improve the topmost source accuracy. Izacard and Grave (2020) applied knowledge distillation to the FiD model, also known as FiD-KD, using cross-attention scores from a sequence-to-sequence reader to obtain synthetic targets for the retriever. Singh et al. (2021) proposed an enhancement approach that differs from knowledge distillation in that it uses an end-to-end training approach requiring fewer documents, training cycles, and no supervised initialization compared to FiD-KD.\n' +
      '\n' +
      '#### 5.2.2 Structural Model Optimization\n' +
      '\n' +
      'As language models continue to evolve at an accelerated pace, an increasing number of large models with high parameter counts and exceptional performance are emerging. Tuning the parameters and internal structure of these models has become increasingly difficult and inefficient, making instruction tuning more important than ever.\n' +
      '\n' +
      'FLAN(Chung et al., 2022) is one of the most systematic and comprehensive approaches among the many studies on instruction tuning. This approach fine-tunes the language model on the instruction-optimised dataset, scales the number of tasks and model size, and incorporates chain-of-thought data in the fine-tuning. Although the authors did not consider a specific approach to tuning instructions in RALM architecture, their work provides a valuable reference for future research. In the instruction fine-tuning of RALM, Lin et al. (2023) integrated in-context retrieval augmentation. This greatly reduces the likelihood of the language model being misled by irrelevant retrieval content. SAIL(Luo et al., 2023) builds language generation and instruction tracking capabilities on complex search results generated by internal and external search engines. Using a corpus of instruction tuning, they collect search results for each training case from different search APIs and domains, and construct a search-based training set containing a triplet of (instruction, grounding information, response). In contrast to training on instruction-tuned datasets, Madaan et al. (2022) and Lazaridou et al. (2022) propose to prompt large models directly from retrieved knowledge. Madaan et al. (2022) used GPT-3 to clarify memory pairings of recorded cases where the model misinterpreted the user\'s intention, as well as user feedback. This ensures that their system can generate enhanced prompts for each new query based on user feedback. In contrast, Lazaridou et al. (2022) uses few-shot prompts and answer reordering to improve inference computation.\n' +
      '\n' +
      '#### 5.2.3 Post-Generation Output Enhancement\n' +
      '\n' +
      'As defined in Section 2 on Parallel Interaction, this interaction is inspired by the K-Nearest Neighbor (KNN) LM (Khandelwal et al., 2019). It is a paradigmatic instance of RALM, wherein the LM is employed solely to enhance the outcomes. Since the proposal of KNN-LM, many researchers have worked to optimize the model. In this section, we will describe the landmark work in detail.\n' +
      '\n' +
      'The KNN-LM approach involves linearly interpolating the extended neural language model with the K-Nearest Neighbours in the pre-trained LM embedding space. Zhong et al. (2022) proposed different processing for three types of memories (local, long-term and external) and added training for in-batch tokens to KNN-LM. The proposed changes aim to improve the performance of the model. Unlike KNN-LM, which only uses memory units during training, TRIME Zhong et al. (2022) uses memory units during both testing and training. He et al. (2021) suggested that not all generated tokens need to be retrieved. Instead, a lightweight neural network can be trained to aid the KNN-LM in adaptive retrieval. Additionally, efficiency can be improved through database streamlining and dimension reduction. Alon et al. (2022) proposed RETOMATON, an unsupervised, weighted finite automaton built on top of the data store. RETOMATON is based on saving pointers between successive data store entries and clustering techniques. RETOMATON is more effective than ADAPTRETHe et al. (2021) in improving accuracy by utilizing remaining pointers during KNN retrieval. Even without KNN retrieval, interpolation operations can still be performed using the stored previous information in the pointers, unlike ADAPTRET which solely relies on the language model. Furthermore, RETOMATON is unsupervised, requiring no additional data for training, making it more data-efficient. Grave et al. (2016) proposed using continuous cache to improve the performance of KNN-LM. This involves storing past hidden activations and accessing them at the appropriate time by dot product with present hidden activation. Yogatama et al. (2021) utilise an extended short-term context by caching local hidden states and global long-term memory by retrieving a set of nearest-neighbour tokens at each time step. They also design a gating function to adaptively combine multiple sources of information for prediction. Compared to KNN-LM, this method uses dynamic weights and can handle cases where interpolation is not feasible, such as when the memory output is an image, video, or sound. Drozdov et al. (2022) proposed a method for adjusting the interpolation weights. The weights are dynamically adjusted based on the size of the region of overlap between the retrieved stored data and the assessment set, which reflects the quality of the retrieval.\n' +
      '\n' +
      '### Overall Enhancement\n' +
      '\n' +
      'This section presents the researchers\' efforts on the RALM architecture as a whole, including End-to-End Training and Build intermediate modules.\n' +
      '\n' +
      '#### 5.3.1 End-to-End Training\n' +
      '\n' +
      'Researchers have begun working on a method called end-to-end training, which aims to minimise manual intervention and focus solely on data. This method utilises deep learning and is becoming increasingly popular due to the growing amount of available data. During research on RALM architectures, many researchers tend to use end-to-end training methods to achieve better results.\n' +
      '\n' +
      'Lewis et al. (2020) and Guu et al. (2020) were among the first researchers to apply end-to-end training to the field of RALM. However, they differed in their approach. REALM Guu et al. (2020) used masked language training in the pre-training phase and included a retriever that can be trained end-to-end. In the fine-tuning phase, only the QA task was targeted while keeping the retriever frozen. On the other hand, RAG Lewis et al. (2020) used an already trained retriever, DPR, and only employed BART for partial end-to-end training. Similar to REALM, Sachan et al. (2021) present an unsupervised pre-training method that involves an inverse cloze task and masked salient spans. This is followed by supervised fine-tuning using question-context pairs. In addition, they find that the use of end-to-end trained retrievers resulted in a significant improvement in performance across tasks. Singh et al. (2021) apply end-to-end training to multi-document processing, in their proposed approach, the value of the latent variable, which represents the set of relevant documents for a given question, is estimated iteratively. This estimate is then used to update the parameters of the retriever and reader. Siriwardhana et al. (2023) describe the end-to-end optimization of RAG from previous studies and introduces an auxiliary training signal to incorporate more domain-specific knowledge. This signal forces RAG-end2end to reconstruct a given sentence by accessing relevant information in an external knowledge base. This approach has greatly improved domain adaptability.\n' +
      '\n' +
      '#### 5.3.2 Intermediate Modules\n' +
      '\n' +
      'Recently, some researchers have constructed an intermediate module to coordinate the activities of both the retriever and the language model due to space or black-box LLM constraints, without improving either.\n' +
      '\n' +
      'Cheng et al. (2024) present Selfmem, a model designed to tackle the issue of low corpus quality. Selfmem utilises a retrieval-enhanced generator to create an infinite pool of memory, which is then used by a memory selector to choose an output for subsequent generations. This approach enables the model to use its own output to enhance generation. Peng et al. (2023) propose an AI agent that formulates human system dialogue as a Markov Decision Process (MDP) described by a quintuple. The quintuple includes an infinitely large set of dialogue states, a collection of historical behaviours, a probability of state transfer, external rewards obtained, and a variable parameter.\n' +
      '\n' +
      '## 6 Data Sources\n' +
      '\n' +
      'This section will introduce some of the data sources commonly used in RALM and categorise them into structured and unstructured data. Figure 5 illustrates the categorization of data sources.\n' +
      '\n' +
      '### Structured Data\n' +
      '\n' +
      'Structured data includes various structures, such as tables and knowledge graphs. The benefit of this type of data is its clear structure, typically in tabular form, with each field precisely defined. It is appropriate for storing numbers, dates, text, and other data types. Structured data can be easily queried, analysed, and processed using a structured query language like SQL.\n' +
      '\n' +
      'Natural Questions(NQ) (Kwiatkowski et al., 2019) is a very well-known dataset in the NLU field.The given text describes a structured question and a corresponding Wikipedia page. The page is annotated with a long answer, typically a paragraph, and a short answer consisting of one or more entities. If there is no long or short answer, it is labelled as empty. Due to the reliability of the Google search engine and its vast amount of data, many scholars have used this dataset to train RALM, such as Wang et al. (2023c), Izacard et al. (2022) and Ram et al. (2023). HotpotQA(HQA) (Yang et al., 2018) stores information about multi-hop questions and provides sentence-level supporting facts needed for inference. The structure includes the paragraph, question, answer, and sentence number that supports the answer. This dataset has been used by many researchers, such as Wang et al. (2023c), Khattab et al. (2022), and Feng et al. (2024), to train RALM for multi-hop question answering. Another significant form of structured data is the Knowledge Graph. It is a data structure that primarily consists of triples of (entities, relationships, attributes). Some of the most frequently used datasets include Wikidata5M, WikiKG90Mv2,\n' +
      '\n' +
      'Figure 5: Classification of RALM data sources.\n' +
      '\n' +
      '-OpendialKG, and KOMODIS. All of these models Kang et al. (2023)Yu and Yang (2023)He et al. (2024) rely on knowledge graphs as a data source.\n' +
      '\n' +
      '### Unstructured Data\n' +
      '\n' +
      'Unstructured data, in contrast, does not have a clearly defined data structure and exists in various forms, including text, images, and audio. Due to its large and diverse nature, it is challenging to store and manage in traditional tabular form. Although it contains valuable information, it requires natural language processing, image recognition, and other technologies to parse and comprehend.\n' +
      '\n' +
      'Several RALM researchers, including Khattab et al. (2022), Wang et al. (2023), and Yang et al. (2023), have used this dataset as a source of data. The FEVER Thorne et al. (2018) dataset is mainly used for fact extraction and validation. Several RALM researchers, including Lewis et al. (2020), Wang et al. (2023), and Izacard et al. (2022), have used the factual text in this dataset as a source of data. In addition to unstructured text, there is also a significant amount of inherently less structured data, such as images, videos, and audio. Several common image datasets are available for use in research, including MNIST, CIFAR-10, Pascal VOC, and COCO. Many studies Chen et al. (2022)Hu et al. (2023)Yasunaga et al. (2022) in the field of RALM have utilized these datasets. Common audio datasets used in speech research include LJ Speech, JSUT, and RUSLAN. Many studies Yuan et al. (2024)Huang et al. (2023)Ghosh et al. (2024) in the field also rely on audio data as a primary source. Common video datasets used in research include HMDB, UCF101, and ASLAN. Many studies Chen et al. (2023)He et al. (2023)Yin et al. (2019) in the field of RALM utilize audio data as a source of information.\n' +
      '\n' +
      '## 7 Applications\n' +
      '\n' +
      'This section provides a summary of the downstream tasks that the RALM architecture primarily focuses on. The relevant application directions are categorized according to the requirements for model generation or comprehension. RALM on NLG indicates that the accomplishment of the task primarily depends on the generative capabilities. Conversely, RALM on NLU indicates that the accomplishment of the task primarily depends on the comprehension capabilities. Finally, RALM on both NLU and NLG indicates that the task is generally handled in two ways, one that relies primarily on comprehension capabilities and one that relies primarily on generative capabilities. Figure 6 illustrates the categorization of applications.\n' +
      '\n' +
      '### RALM on NLG Tasks\n' +
      '\n' +
      '#### 7.1.1 Machine Translation\n' +
      '\n' +
      'Machine translation, also known as automatic translation, is the process of converting one natural language (source language) into another natural language (target language) using a computer. It is a branch of computational linguistics, one of the ultimate goals of artificial intelligence, and has important scientific research value. Machine translation systems can be divided into two categories: rule-based and corpus-based. The former comprises a dictionary and a rule base, which collectively constitute the knowledge source. In contrast, the latter comprises a corpus that has been divided and labeled, and which does not require a dictionary or rules. Instead, it is based on statistical laws and most RALMs accomplish this task based on rules.\n' +
      '\n' +
      'The Selfmem Cheng et al. (2024) system employs two distinct language models for the machine translation task. The first is a trainable mini-model, which has been trained using a joint and bipartite approach, respectively. The second is a few-shot prompted LLM. Ultimately, Selfmem has demonstrated a notable enhancement in its performance across all four translation directions and for both training architectures. This outcome suggests that enhanced memory capabilities often result in superior generation outcomes. In order to achieve the best results, TRIME Zhong et al. (2022) used the IWSLT\'14 De-En baseline. Given that the task is sentence-level, the researchers did not use local memory and long-term memory, as there are few repetitive tokens in them. Instead, they used only external memory, which enabled them to beat the KNN-MT Khandelwal et al. (2020) in performance.\n' +
      '\n' +
      '#### 7.1.2 Math Teaching\n' +
      '\n' +
      'As the RALM architecture continues to evolve, an increasing number of potential application directions are being identified. Levonian Levonian et al. (2023) were inspired by RALM to apply this architecture to the domain of mathematics teaching and learning. To address the fact that the knowledge stored in the LLM may not match what is taught in schools, they used one of three prompted instructional conditions to generate responses to and evaluated basic math textbooks as a retrieval corpus.\n' +
      '\n' +
      '#### 7.1.3 Dialog Generation\n' +
      '\n' +
      'Dialog generation, in particular lengthy dialogue, is a challenging task. This is due to the necessity of not only ensuring that the language model possesses natural language processing capabilities, but also that the model is able to utilise context in order to satisfy the requirements of the dialogue.\n' +
      '\n' +
      'FILCO (Wang et al., 2023c) employs the Wikipedia dataset from the KILT benchmark, referred to as the "Wizard of Wikipedia" (WoW), to generate subsequent dialogue. This process involves basing the output on a Wikipedia article, with the input comprising the history of multiple rounds of dialogue. RA-DIT (Lin et al., 2023b) also employs the WoW dataset in the fine-tuning phase. As a result of the command tuning operation, the model outperforms Llama (Touvron et al., 2023a) and Llama-REPLUG (Shi et al., 2023b) with the same parameters for dialogue generation in the zero-shot condition. The incorporation of Selfmem (Cheng et al., 2024) into the retrieval-augmented generator markedly enhances the generation of dialogue, as a consequence of its remarkable flexibility. This is achieved by the direct optimisation of\n' +
      '\n' +
      'Figure 6: Classification of RALM applications.\n' +
      '\n' +
      'memory for the desired properties of diverse and information-rich dialogues. In contrast, SURGE Kang et al. (2023) employs the Knowledge Graph as a data source for the dialogue generation task, wherein each dialogue round comprises facts from a large-scale KG. In contrast to other related work Rony et al. (2022), they retrieve only contextually relevant subgraphs, thus avoiding the computational overheads and misleading models that can result from retrieving irrelevant data.\n' +
      '\n' +
      '### RALM on NLU Tasks\n' +
      '\n' +
      '#### 7.2.1 Slot Filling\n' +
      '\n' +
      'Slot filling is a technique employed in natural language processing for the purpose of recognizing and extracting specific information from user-supplied text or speech. In slot filling, the system defines a set of slots in advance, with each slot representing a specific information requirement. These requirements may include, but are not limited to, date, time, location, and so forth. Upon receipt of a user input in the form of text or speech, the system performs an analysis of the content, attempting to identify information that matches the predefined slots or classification labels Lu et al. (2023, 2023). This information is then populated into the corresponding slots for subsequent processing and response.\n' +
      '\n' +
      'KGI Glass et al. (2021) enhances dense channel retrieval through the utilization of hard negatives in dense indexing and implements a robust training process for retrieval enhancement generation. The retrieval enhancement is employed to enhance the effectiveness of the slot-filling task, thereby facilitating the generation of high-quality knowledge graphs by AI. The results demonstrate that the method achieves excellent performance in TREx and zsRE datasets and exhibits remarkable robustness in TACRED dataset.\n' +
      '\n' +
      '#### 7.2.2 Image Generation\n' +
      '\n' +
      'The process of text-to-image generation is a challenging one, requiring a model to demonstrate a high degree of natural language understanding and to convey this understanding through an image, in contrast to the typical format of textual data.\n' +
      '\n' +
      'In a pioneering study, Li et al. (2022) proposed the use of retrieval techniques to enhance the quality of text-to-image generation. They conducted a comparative analysis of the quality and quantity of the generated images with mainstream models on the CUB and COCO datasets. Their findings demonstrated that all models outperformed their contemporaries. In contrast, RE-IMAGEN Chen et al. (2022) focused on assisting the model in generating images of uncommon objects through retrieval. This approach ultimately led to the achievement of exceptionally high FID scores on the COCO and WikilImage datasets. Even more groundbreaking results were obtained on the authors\' own proposed EntityDrawBench benchmark, which encompasses a range of common and rare objects across multiple categories. RDM Blattmann et al. (2022), although trained in a similar manner to RE-IMAGEN, employs image features as the foundation for retrieval and is supplanted by user examples during the inference process. Consequently, RDM is capable of efficiently transferring the described artistic style to the generated images. Furthermore, in contrast to RE-IMAGEN, which employs image-text pairs for retrieval, KNN-Diffusion Sheynin et al. (2022) solely utilizes images for retrieval, resulting in a lower quality of results on the COCO dataset.\n' +
      '\n' +
      '#### 7.2.3 Fact checking\n' +
      '\n' +
      'Fact checking involves verifying a statement based on evidence. This task at hand involves a retrieval problem and a challenging implicit reasoning task. Furthermore, This task typically involves taking the statement as input and producing relevant document passages that prove or disprove the statement. Many RALM models get excellent performance because they come with their own retrievers. It is an important aspect of natural language understanding.\n' +
      '\n' +
      'RAG Lewis et al. (2020) uses the FEVER dataset to map labels (Supported, Refuted, NotE-noughInfo) to individual output tokens. It is trained directly using the declaration class, which is not supervised over the retrieved evidence, unlike other works. Atlas Izacard et al. (2022) employs few-shot learning to achieve performance comparable to previous studies in just 64-shot conditions. Furthermore, after training with the full dataset, it outperformed the best model Hoffmann et al. (2022) available at the time. FILCO Wang et al. (2023) approached the task of improving the quality of retrieved documents by using the FEVER dataset from the KILT base aggregation, which only included the supports and refutes tags. Accuracy was used as a metric.\n' +
      '\n' +
      '#### 7.2.4 Knowledge Graph Completion\n' +
      '\n' +
      'A multitude of previous tasks have employed structured data in the form of knowledge graphs, with knowledge graph completion representing a pervasive application. The conventional methodology (Chen et al., 2022) (Saxena et al., 2022) for completion entails defining the task as a sequence-to-sequence process, wherein incomplete triples and entities are transformed into text sequences. However, this approach is constrained by its reliance on implicit reasoning, which significantly constrains the utility of the knowledge graph itself. ReSKGC (Yu and Yang, 2023) proposes the integration of retrieval augmentation techniques with knowledge graph completion. This integration entails the selection of semantically relevant triples from the knowledge graph and their utilization as evidence to inform the generation of output through explicit reasoning. This model employs data from the Wikidata5M and WikiKG90Mv2 datasets, demonstrating superior performance compared to other existing work in a range of conditions.\n' +
      '\n' +
      '#### 7.2.5 Commonsense Reasoning\n' +
      '\n' +
      'Commonsense Reasoning is a challenging task for language models. In addition to exhibiting human-like thinking and reasoning patterns, these models must also be able to store a substantial amount of commonsense knowledge. However, the advent of RALM has made the second requirement less demanding, as retrieval techniques provide language models with access to nonparametric memories.\n' +
      '\n' +
      'FLARE (Jiang et al., 2023) uses StrategyQA, which contains a significant number of yes/no questions from a diverse range of sources. In addition, the authors request that the model provide the exact reasoning process and the final answer that determines the yes/no answer, ensuring that the answer matches the gold answer exactly. The incorporation of in-context samples into the retrieved content, along with training using data from the COPA, HellaSwag, and PIQA datasets, has resulted in the development of LLM-R (Wang et al., 2023) model that exhibits excellent performance. The fundamental concept of the ITER-RETGEN (Gao et al., 2022) model employs an iterative methodology to integrate retrievers and language models. The model was trained for the Commonsense Reasoning task using the StrategyQA dataset and achieved its optimal performance at seven iterations. In contrast, KG-BART (Shao et al., 2023) is designed to prioritize the Commonsense Reasoning task and employs knowledge graphs to enhance its performance in this area. This approach has proven effective in significantly improving the model\'s ability to complete the Commonsense Reasoning task, with performance approaching that of human beings under certain evaluation metrics.\n' +
      '\n' +
      '### RALM on both NLU and NLG tasks\n' +
      '\n' +
      '#### 7.3.1 Text Summarization\n' +
      '\n' +
      'Text summarization represents a crucial application of language modelling. In essence, text summarization is the process of generating concise and fluent summaries while maintaining the content and overall meaning of key information. Currently, two distinct types of this task exist: extractive summarization and abstractive summarization.\n' +
      '\n' +
      'RA-DIT (Lin et al., 2023) employs the CNN/DailyMail dataset to refine the language model component of the model, which demonstrates remarkable efficacy in the text summarization task due to the operation of command fine-tuning. In contrast, Self-Mem(Cheng et al., 2024) was trained using the XSum and BigPatent datasets. The authors observed that memory enhancement had a significantly larger effect on BigPatent than XSum. They hypothesize that this discrepancy is due to the inclusion of official patent documents in the BigPatent dataset, which exhibit considerable similarity. The LLM-R (Wang et al., 2023) model employs an in-context learning approach, integrating RALM, and utilizes the AESLC, AG News, and Gigaword datasets for text summarization training. The results demonstrate that LLM-R significantly outperforms both traditional and dense retrievers in the summarization task. RAMKG (Gao et al., 2022) extended the iterative training of the RALM architecture to the multilingual domain and employed two multilingual datasets, EcommerceMKP and AcademicMKP, for the training of summarization work, achieving the best results at that time.\n' +
      '\n' +
      '#### 7.3.2 Question Answering\n' +
      '\n' +
      'Question answering also includes generative and extractive forms. It is a common task in NLP that relies on domain-specific knowledge. RALMs can achieve better results than traditional language models by utilizing externally stored knowledge. Common question answering tasks include domain-specific QA, open-domain QA(ODQA), and multi-hop QA. In the medical field, large language models are commonly used, PKG(Luo et al., 2023) uses the relevant data from the MedMC-QA dataset,using the questions in the training set as input and the medical explanations as background knowledge, and the accuracy of the background knowledge generated by the model as an evaluation metric. HyKGE(Jiang et al., 2023) also targets question answering in the medical field, but uses a knowledge graph-enhanced approach. When targeting ODQA tasks, RAG (Lewis et al., 2020) considers questions and answers as input-output text pairs and trains by minimizing the negative log-likelihood of the answers. In contrast, ICRALM (Ram et al., 2023) exclusively performs the ODQA task using frozen LMs, which have not been enhanced by pre-training, fine-tuning, or retrieval, as well as the associated knowledge documents. Other models (Wang et al., 2023)(Lin et al., 2023)(Shi et al., 2023) were also trained for the ODQA task. In relation to the multi-hop QA task, FILCO (Wang et al., 2023) used their proposed filtered retrieval method to filter multiple documents. They validated their approach using the HotpotQA dataset. RR (He et al., 2022), on the other hand, used the Chain-of-Thought (CoT) approach to address the multi-hop problem. In addition, many other models (Jiang et al., 2023)(Khattab et al., 2022) deal with multi-hop problems.\n' +
      '\n' +
      '#### 7.3.3 Code Generation and Summarization\n' +
      '\n' +
      'Code generation (Romera-Paredes et al., 2024; Ye et al., 2024) and summarization (Nam et al., 2024) differ from ordinary text generation and summarization in terms of the target audience and processing. Code generation and summarization involves computer program code, which may require domain-specific syntactic and semantic understanding, in addition to higher requirements for NLU and NLG capabilities of language models.\n' +
      '\n' +
      'REDCODER (Parvez et al., 2021) initially identified potential candidate codes from existing code or abstract databases. The researchers retained 1.1 million unique lines of codes and abstracts as retrieved data through multiple channels. The final evaluation on the CodeXGLUE dataset demonstrated excellent performance across multiple programming languages and evaluation metrics. Another proposed enhancement was put forth by Zan et al. (2022), who utilized private libraries to enhance the quality of code generation. This involved first identifying the most suitable private libraries based on the API documentation through the APIRetriever component, and then utilizing the APIcoder for generation. This approach led to a notable improvement in the accuracy of the generated content.\n' +
      '\n' +
      'Liu et al. (2020) propose a novel attention-based dynamic graph to complement the source code for static graph representations and design a hybrid message-passing GNN to capture local and global structural information. This approach improves the accuracy of code summarization and ultimately yields superior performance over both mainstream retrievers and generators. The RACE (Shi et al., 2022) model employ a conventional RALM framework to consolidate code in five programming languages within the MCMD dataset, resulting in a 6 to 38 percent enhancement over all baseline models.\n' +
      '\n' +
      '## 8 Evaluation\n' +
      '\n' +
      'This section provides a summary of the evaluation approach and benchmarks for RALM. In Sections 6 and 7, we presented some evaluation criteria for large language model tasks as well as baselines. At the time of the initial proposal of the RALM architecture, the majority of researchers employed generalized benchmarks. However, as the RALM architecture evolved, there was a growing number of RALM-specific evaluation methods and baselines proposed. Table 3 demonstrates the details of each evaluation model.\n' +
      '\n' +
      'RAGAS (Es et al., 2023) employs the WikiEval Dataset to assess the faithfulness, answer relevance, and context relevance of RALMs. Faithfulness is defined as the degree to which responses align with the provided context. Answer relevance refers to the extent to which generated responses address the actual question posed. Context relevance is gauged by the degree to which retrieved context is centralized and devoid of irrelevant information. Additionally, the researchers utilize the prompt gpt-3.5-turbo-16k model to automate the evaluation process. RGB (Chen et al., 2024) developed a bilingual Chinese and English evaluation system employing three evaluation metrics: accuracy, rejection rate, and error detection rate. These metrics were utilized to assess the noise robustness, negative rejection, information integration, and counterfactual robustness of the data sources, which were articles processed by LM and retrieved through Google\'s API. CRUD-RAG (Lyu et al., 2024) considers the impact of retrieval components and the construction of external knowledge bases that have not been previously considered by researchers. A dataset was generated using a large model to evaluate the Create, Read, Update, and Delete (summarization) capabilities of RALM through four evaluation metrics: ROUGE, BLEU, bertScore, and RAGQuestEval. In addition, ARES (Saad-Falcon et al., 2023) employs datasets generated by the LM, but utilizes a lightweight LM to determine the quality of individual RALM components and utilizes human-labeled data points for prediction-powered inference. The RALM\'s context is evaluated using the KILT and SuperGLUE benchmarks, with Relevance, Answer Faithfulness, and Answer Relevance being the relevant criteria.\n' +
      '\n' +
      'In addition to the general assessment of RALM, there has been some work focusing on the assessment of specific details and domains. RECALL (Liu et al., 2023) employs the EventKG and UJ datasets to incorporate inaccurate information into its existing data set. It then determines whether RALM is susceptible to being misled by such inaccurate information through two tasks: question answering and text generation. Xiong et al. (2024) concentrated on the medical domain and proposed MIRAGE, which integrates data from five datasets, including MMLU-Med, to evaluate the zero-shot learning, multi-choice evaluation, retrieval-augmented generation, and question-only retrieval ideation capabilities of medical RALMs. Ultimately, they also discovered the log-linear scaling property and the "lost-in-the-middle" effect in the medical domain.\n' +
      '\n' +
      '## 9 Disscussion\n' +
      '\n' +
      'This section is devoted to an analysis of the limitations of existing RALM architectures and a description of potential future developments. Figure 7 summarizes the limitations of existing RALMs and our proposed solutions.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'This section presents a summary and analysis of some of the limitations of existing RALMs.\n' +
      '\n' +
      '#### 9.1.1 Poor Robustness\n' +
      '\n' +
      'Robustness is a crucial aspect to be considered in all systems. RALM systems, despite exhibiting performance benefits in several domains, introduce a multitude of uncertainties to the architecture due to the incorporation of retrieval as a technique. As elucidated by Hu et al. (2024), through exceedingly simple prefix attacks, not only can the relevance and accuracy of RALM output be diminished, but even the retrieval strategy of the retriever can be altered. Consequently, in addition to utilising various\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|c|c|c|c|c|c|} \\hline Reference & \\begin{tabular}{c} RAGAS \\\\ Es et al., 2023) \\\\ \\end{tabular} & \\begin{tabular}{c} RGB \\\\ Chen et al., 2024) \\\\ Chen et al., 2024) \\\\ \\end{tabular} & \\begin{tabular}{c} CRUD-RAG \\\\ Lyu et al., 2024) \\\\ \\end{tabular} & \\begin{tabular}{c} ARES \\\\ Saad-Falcon et al., 2023) \\\\ \\end{tabular} & \\begin{tabular}{c} MIRAGE \\\\ Xiao et al., 2024) \\\\ \\end{tabular} & \\begin{tabular}{c} RECALL \\\\ Liu et al., 2023) \\\\ \\end{tabular} \\\\ \\hline Dataset & WikiEval & \\multicolumn{4}{c|}{LLM-generated} & \\begin{tabular}{c} MMLU-Med \\\\ MedQA-US \\\\ MedMCQA \\\\ PubMedQA \\\\ BioASQ-Y/N \\\\ \\end{tabular} & \n' +
      '\\begin{tabular}{c} EventKG \\\\ UJ \\\\ \\end{tabular} \\\\ \\hline Target & \\multicolumn{4}{c|}{Retrieval Quality: Generation Quality} & Generation Quality \\\\ \\hline Context & \\(\\checkmark\\) & & & \\(\\checkmark\\) & & \\\\ Relevance & \\(\\checkmark\\) & & & \\(\\checkmark\\) & & \\\\ \\hline Faithfulness & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) \\\\ \\hline Answer & \\(\\checkmark\\) & & & \\(\\checkmark\\) & & \\\\ Relevance & \\(\\checkmark\\) & & & \\(\\checkmark\\) & & \\\\ \\hline Noise & & \\(\\checkmark\\) & & & & \\\\ Robustness & & & & & & \\\\ \\hline Information & & \\(\\checkmark\\) & \\(\\checkmark\\) & & \\(\\checkmark\\) & \\\\ Integration & & & & & & \\\\ \\hline Negative & & \\(\\checkmark\\) & & & & \\\\ Rejection & & & & & & \\\\ \\hline Counterfactual & & \\(\\checkmark\\) & \\(\\checkmark\\) & & & \\(\\checkmark\\) \\\\ Robustness & & & & & & \\\\ \\hline Error & & & \\(\\checkmark\\) & & & \\\\ Correction & & & & & \\\\ \\hline Summarization & & & \\(\\checkmark\\) & & & \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Summary of evaluation methods in RALM.\n' +
      '\n' +
      'retrieval enhancement techniques to enhance the performance of LMs, researchers should also take care to minimise the effects of factually inaccurate data, information that is not relevant to problem solving and even some harmful hints and prefixes on LMs.\n' +
      '\n' +
      '#### 9.1.2 Poor Quality of Retrieval Results\n' +
      '\n' +
      'A significant number of researchers (Yan et al., 2024) (Asai et al., 2023) engaged in the endeavour of enhancing retrieval efficacy have asserted that although their proposed models are demonstrably beneficial in optimising the quality of the output, there is as yet no assurance that the retrieval outcomes can be entirely aligned with the LM. Particularly when using the Internet as a retrieval tool, the quality of Internet sources can vary widely, and merging this data without proper consideration can introduce noise or misleading information into the resulting output.\n' +
      '\n' +
      '#### 9.1.3 Overspending\n' +
      '\n' +
      'While existing RALMs (Siriwardhana et al., 2023) (Guu et al., 2020) (Borgeaud et al., 2022) can greatly improve the performance of LMs in various domains, some of them require extensive model changes as well as complex pre-training and fine-tuning operations, which greatly increases the time and space overhead and also reduces the scalability of RALMs. In addition, as the scale of retrieval increases, so does the complexity of storing and accessing the data sources. As a result, researchers must weigh the benefits of modifying the model against the costs.\n' +
      '\n' +
      '#### 9.1.4 Few Applications\n' +
      '\n' +
      'Although many RALMs have greatly improved the performance of LMs in various domains, there has not been much improvement from an application perspective, and RALMs are still doing some of the routine work that was done in the early days of LMs, e.g., question answering, summarizing (Luo et al., 2023) (Jiang et al., 2023) (Alon et al., 2022). Although there have been some very interesting application directions recently, such as math teaching (Levonian et al., 2023), slot filling (Glass et al., 2021), etc., this is not enough. A technology always needs to be actually used to fully prove its value, and RALM is no exception.\n' +
      '\n' +
      '### Future Prospects\n' +
      '\n' +
      'This section suggests some possible directions for the future development of RALM, based mainly on the limitations mentioned in Section 9.1.\n' +
      '\n' +
      '#### 9.2.1 Improve Robustness\n' +
      '\n' +
      'Some scholars have mentioned possible ways to improve model robustness in the future work section of their papers, such as explicit self-reflection and fine-grained attribution (Asai et al., 2023). In\n' +
      '\n' +
      'Figure 7: Summary of limitations of current RALM models and future prospects.\n' +
      '\n' +
      'contrast to these works, Hu et al. (2024) proposed a method called Gradient Guided Prompt Perturbation (GGPP), a way to perturb the RALM, which was experimentally found to be effective in improving the situation by utilizing the SAT probe (Yuksekgonul et al., 2023) and activation (ACT) classifier. A method is proposed to detect this perturbation by prompting the internal state of the perturbed RALM. In addition, by proposing and improving the evaluation method of RALM and the related baseline can also help improve the robustness of the model, Chen et al. (2024) made a series of evaluation system for RALM by focusing on the robustness.\n' +
      '\n' +
      '#### 9.2.2 Improve Retrieval Quality\n' +
      '\n' +
      'Improving the quality of retrieval can be considered in two parts: improving the quality of the dataset used for retrieval, and improving the performance of the retrieval technique. Nowadays, many data sets are given to LLM to generate relevant content, and since LLM itself has "hallucination", certain means must be adopted to ensure the accuracy of the data, such as using human beings to supervise the refinement (Chen et al., 2024). In addition, due to the wide range of information sources on the Internet, it is obviously not enough to rely solely on search engines for screening, so it is necessary to improve the retrieval technology, such as the use of BM25 (Luo et al., 2023) or TF-IDF (Lazaridou et al., 2022) algorithms for further re-ranking.\n' +
      '\n' +
      '#### 9.2.3 Weigh Expenses and Benefits\n' +
      '\n' +
      'Reducing the overhead can be considered from three perspectives: first, some plug-and-play intermediate modules can be designed, e.g., CRAG (Yan et al., 2024), Selfmem (Cheng et al., 2024), AI agent (Peng et al., 2023), or some deployment solutions, e.g., LangChain, Llama Index, so that there is no need to make targeted improvements for each model. Second, Internet retrieval can be utilized to reduce the overhead of the retriever, but attention needs to be paid to the data relevance mentioned earlier. Finally, In-context learning can be employed to reduce the overhead associated with improving LMs, e.g., ICRALM (Ram et al., 2023).\n' +
      '\n' +
      '#### 9.2.4 Expand Applications\n' +
      '\n' +
      'In the contemporary era, the application of LLM has been expanded to encompass a multitude of domains, whereas the application direction of RALM remains relatively limited. To address this limitation, researchers must not only consider the existing application areas of LLM but also leverage the distinctive strengths of RALM, which excels in addressing problems closely related to knowledge and experience. Additionally, they should integrate RALM with other advanced technologies and utilize it to overcome the challenges associated with them. This paper presents several illustrative examples, including decision support, search engine, and recommendation system.\n' +
      '\n' +
      '## 10 Conclusion\n' +
      '\n' +
      'The integration of RALMs represents a significant advance in the capabilities of NLP systems. This survey has provided an extensive review of RALMs, highlighting their architecture, applications, and the challenges they face. RALMs enhance language models by retrieving and integrating external knowledge, leading to improved performance across a variety of NLP tasks, including translation, dialogue generation, and knowledge graph completion.\n' +
      '\n' +
      'Despite their successes, RALMs encounter several limitations. Notably, their robustness against adversarial inputs, the quality of retrieval results, the computational costs associated with their deployment, and a lack of diversity in application domains have been identified as areas requiring further attention. To address these, the research community has proposed several strategies, such as improving the evaluation methods, refining retrieval techniques, and exploring cost-effective solutions that maintain a balance between performance and efficiency.\n' +
      '\n' +
      'In the future, the advancement of RALMs will depend on the enhancement of their robustness, the improvement of retrieval quality, and the expansion of their application scope. By incorporating more sophisticated techniques and integrating RALMs with other AI technologies, these models can be leveraged to address an even broader spectrum of challenges. The ongoing research and development in this field are expected to result in more resilient, efficient, and versatile RALMs, thereby pushing the boundaries of what is achievable in NLP and beyond. As RALMs continue to evolve, they hold the promise of enabling AI systems with deeper understanding and more human-like language capabilities, thereby opening up new possibilities in a wide range of fields.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_.\n' +
      '* Adolphs et al. (2021) Leonard Adolphs, Benjamin Boerschinger, Christian Buck, Michelle Chen Huebscher, Massimiliano Ciaramita, Lasse Espeholt, Thomas Hofmann, Yannic Kilcher, Sascha Rothe, Pier Giuseppe Sessa, et al. 2021. Boosting search engines with interactive agents. _arXiv preprint arXiv:2109.00527_.\n' +
      '* Alon et al. (2022) Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022. Neuro-symbolic language modeling with automaton-augmented retrieval. In _International Conference on Machine Learning_, pages 468-485. PMLR.\n' +
      '* Asai et al. (2023) Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. _arXiv preprint arXiv:2310.11511_.\n' +
      '* Aussenac-Gilles and Sorgel (2005) Nathalie Aussenac-Gilles and Dagobert Sorgel. 2005. Text analysis for ontology and terminology engineering. _Applied Ontology_, 1(1):35-46.\n' +
      '* Ba et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. _arXiv preprint arXiv:1607.06450_.\n' +
      '* Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. _arXiv preprint arXiv:2309.16609_.\n' +
      '* Banerjee et al. (2019) Imon Banerjee, Yuan Ling, Matthew C Chen, Sadid A Hasan, Curtis P Langlotz, Nathaniel Moradzadeh, Brian Chapman, Timothy Amrhein, David Mong, Daniel L Rubin, et al. 2019. Comparative effectiveness of convolutional neural network (cnn) and recurrent neural network (rnn) architectures for radiology text report classification. _Artificial intelligence in medicine_, 97:79-88.\n' +
      '* Black et al. (2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. _arXiv preprint arXiv:2204.06745_.\n' +
      '* Blattmann et al. (2022) Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Muller, and Bjorn Ommer. 2022. Retrieval-augmented diffusion models. _Advances in Neural Information Processing Systems_, 35:15309-15324.\n' +
      '* Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving from trilions of tokens. In _International conference on machine learning_, pages 2206-2240. PMLR.\n' +
      '* Boytsov et al. (2016) Leonid Boytsov, David Novak, Yury Malkov, and Eric Nyberg. 2016. Off the beaten path: Let\'s replace term-based retrieval with k-nn search. In _Proceedings of the 25th ACM international on conference on information and knowledge management_, pages 1099-1108.\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901.\n' +
      '* Chen et al. (2022a) Chen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam. 2022a. Knowledge is flat: A seq2seq generative framework for various knowledge graph completion. _arXiv preprint arXiv:2209.07299_.\n' +
      '* Chen et al. (2024) Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented generation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 17754-17762.\n' +
      '* Chen et al. (2023) Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Hongyang Chao, and Tao Mei. 2023. Retrieval augmented convolutional encoder-decoder networks for video captioning. _ACM Transactions on Multimedia Computing, Communications and Applications_, 19(1s):1-24.\n' +
      '* Chen et al. (2022b) Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W Cohen. 2022b. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. _arXiv preprint arXiv:2210.02928_.\n' +
      '* Chen et al. (2022c) Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. 2022c. Re-imagen: Retrieval-augmented text-to-image generator. _arXiv preprint arXiv:2209.14491_.\n' +
      '* Cheng et al. (2024) Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2024. Lift yourself up: Retrieval-augmented text generation with self-memory. _Advances in Neural Information Processing Systems_, 36.\n' +
      '* Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. _arXiv preprint arXiv:1904.10509_.\n' +
      '* Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. _Journal of Machine Learning Research_, 24(240):1-113.\n' +
      '* Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_.\n' +
      '* Chen et al. (2020)Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. Language modeling with gated convolutional networks. In _International conference on machine learning_, pages 933-941. PMLR.\n' +
      '* Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_.\n' +
      '* Drozdov et al. (2022) Andrew Drozdov, Shufan Wang, Razieh Rahimi, Andrew McCallum, Hamed Zamani, and Mohit Iyyer. 2022. You can\'t pick your neighbors, or can you? when and how to rely on retrieval in the \\(k\\) nn-lm. _arXiv preprint arXiv:2210.15859_.\n' +
      '* Dubois et al. (2024) Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. 2024. Alpacaafarm: A simulation framework for methods that learn from human feedback. _Advances in Neural Information Processing Systems_, 36.\n' +
      '* Es et al. (2023) Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023. Ragas: Automated evaluation of retrieval augmented generation. _arXiv preprint arXiv:2309.15217_.\n' +
      '* Feng et al. (2023) Chao Feng, Xinyu Zhang, and Zichu Fei. 2023. Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs. _arXiv preprint arXiv:2309.03118_.\n' +
      '* Feng et al. (2024) Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2024. Retrieval-generation synergy augmented large language models. In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 11661-11665. IEEE.\n' +
      '* Gao et al. (2022) Yifan Gao, Qingyu Yin, Zheng Li, Rui Meng, Tong Zhao, Bing Yin, Irwin King, and Michael R Lyu. 2022. Retrieval-augmented multilingual keyphrase generation with retriever-generator iterative training. _arXiv preprint arXiv:2205.10471_.\n' +
      '* Gao et al. (2023) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. _arXiv preprint arXiv:2312.10997_.\n' +
      '* Ghosh et al. (2024) Sreyan Ghosh, Sonal Kumar, Chandra Kiran Reddy Evuru, Ramani Duraiswami, and Dinesh Manocha. 2024. Recap: retrieval-augmented audio captioning. In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1161-1165. IEEE.\n' +
      '* Glass et al. (2021) Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, and Alfio Gliozzo. 2021. Robust retrieval augmented generation for zero-shot slot filling. _arXiv preprint arXiv:2108.13934_.\n' +
      '* Gou et al. (2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey. _International Journal of Computer Vision_, 129(6):1789-1819.\n' +
      '* Grave et al. (2016) Edouard Grave, Armand Joulin, and Nicolas Usunier. 2016. Improving neural language models with a continuous cache. _arXiv preprint arXiv:1612.04426_.\n' +
      '* Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In _International conference on machine learning_, pages 3929-3938. PMLR.\n' +
      '* Hambarde and Proenca (2023) Kailash A Hambarde and Hugo Proenca. 2023. Information retrieval: recent advances and beyond. _IEEE Access_.\n' +
      '* Han et al. (2023) Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. 2023. Imagebind-llm: Multi-modality instruction tuning. _arXiv preprint arXiv:2309.03905_.\n' +
      '* He et al. (2022) Hangfeng He, Hongming Zhang, and Dan Roth. 2022. Rethinking with retrieval: Faithful large language model inference. _arXiv preprint arXiv:2301.00303_.\n' +
      '* He et al. (2021) Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2021. Efficient nearest neighbor language models. _arXiv preprint arXiv:2109.04212_.\n' +
      '* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778.\n' +
      '* He et al. (2024) Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. _arXiv preprint arXiv:2402.07630_.\n' +
      '* He et al. (2023) Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. 2023. Animate-a-story: Storytelling with retrieval-augmented video generation. _arXiv preprint arXiv:2307.06940_.\n' +
      '* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_.\n' +
      '* Hofstatter et al. (2023) Sebastian Hofstatter, Jiccao Chen, Karthik Raman, and Hamed Zamani. 2023. Fid-light: Efficient and effective retrieval-augmented text generation. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1437-1447.\n' +
      '* H. H. et al. (2016)Frederik Hogenboom, Flavius Frasincar, and Uzay Kaymak. 2010. An overview of approaches to extract information from natural language corpora. _Information Foraging Lab_, 69.\n' +
      '* Hu et al. (2024) Zhibo Hu, Chen Wang, Yanfeng Shu, Liming Zhu, et al. 2024. Prompt perturbation in retrieval-augmented generation based large language models. _arXiv preprint arXiv:2402.07179_.\n' +
      '* Hu et al. (2023) Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A Ross, and Alireza Fathi. 2023. Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 23369-23379.\n' +
      '* Hua and Wang (2018) Xinyu Hua and Lu Wang. 2018. Neural argument generation augmented with externally retrieved evidence. _arXiv preprint arXiv:1805.10254_.\n' +
      '* Huang et al. (2023) Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. 2023. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. In _International Conference on Machine Learning_, pages 13916-13932. PMLR.\n' +
      '* Izacard et al. (2021) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. _arXiv preprint arXiv:2112.09118_.\n' +
      '* Izacard and Grave (2020a) Gautier Izacard and Edouard Grave. 2020a. Distilling knowledge from reader to retriever for question answering. _arXiv preprint arXiv:2012.04584_.\n' +
      '* Izacard and Grave (2020b) Gautier Izacard and Edouard Grave. 2020b. Leveraging passage retrieval with generative models for open domain question answering. _arXiv preprint arXiv:2007.01282_.\n' +
      '* Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Atlas: Few-shot learning with retrieval augmented language models. _arXiv preprint arXiv:2208.03299_.\n' +
      '* Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. _ACM Computing Surveys_, 55(12):1-38.\n' +
      '* Jiang et al. (2023a) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023a. Mistral 7b. _arXiv preprint arXiv:2310.06825_.\n' +
      '* Jiang et al. (2023b) Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu, Yue Fang, Zhiyuan Wang, Jinyi Tang, Hongxin Ding, Xu Chu, Junfeng Zhao, et al. 2023b. Think and retrieval: A hypothesis knowledge graph enhanced medical large language models. _arXiv preprint arXiv:2312.15883_.\n' +
      '* Jiang et al. (2020) Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023c. Active retrieval augmented generation. _arXiv preprint arXiv:2305.06983_.\n' +
      '* Jiang et al. (2020) Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. 2020. Convbert: Improving bert with span-based dynamic convolution. _Advances in Neural Information Processing Systems_, 33:12837-12848.\n' +
      '* Kang et al. (2023) Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. 2023. Knowledge graph-augmented language models for knowledge-grounded dialogue generation. _arXiv preprint arXiv:2305.18846_.\n' +
      '* Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. _arXiv preprint arXiv:2004.04906_.\n' +
      '* Khandelwal et al. (2020) Urvash Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Nearest neighbor machine translation. _arXiv preprint arXiv:2010.00710_.\n' +
      '* Khandelwal et al. (2019) Urvash Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. _arXiv preprint arXiv:1911.00172_.\n' +
      '* Khattab et al. (2022) Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. _arXiv preprint arXiv:2212.14024_.\n' +
      '* Komeili et al. (2021) Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2021. Internet-augmented dialogue generation. _arXiv preprint arXiv:2107.07566_.\n' +
      '* Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:453-466.\n' +
      '* Lazaridou et al. (2022) Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internet-augmented language models through few-shot prompting for open-domain question answering. _arXiv preprint arXiv:2203.05115_.\n' +
      '* Lee et al. (2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. _arXiv preprint arXiv:1906.00300_.\n' +
      '* Liu et al. (2019)Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka Gade, Owen Henkel, Mille-Ellen Postle, and Wanli Xing. 2023. Retrieval-augmented generation to improve mau question-answering: Trade-offs between groundedness and human preference. _arXiv preprint arXiv:2310.03184_.\n' +
      '* Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. _arXiv preprint arXiv:1910.13461_.\n' +
      '* Lewis et al. (2019) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459-9474.\n' +
      '* Li et al. (2022a) Bowen Li, Philip HS Torr, and Thomas Lukasiewicz. 2022a. Memory-driven text-to-image generation. _arXiv preprint arXiv:2208.07022_.\n' +
      '* Li et al. (2022b) Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022b. A survey on retrieval-augmented text generation. _arXiv preprint arXiv:2202.01110_.\n' +
      '* Lin et al. (2023a) Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023a. How to train your dragon: Diverse augmentation towards generalizable dense retrieval. _arXiv preprint arXiv:2302.07452_.\n' +
      '* Lin et al. (2023b) Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvay, Mike Lewis, et al. 2023b. Ra-dit: Retrieval-augmented dual instruction tuning. _arXiv preprint arXiv:2310.01352_.\n' +
      '* Lin et al. (2022) Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. 2022. Few-shot learning with multilingual generative language models. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 9019-9052.\n' +
      '* Liu et al. (2020) Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, and Yang Liu. 2020. Retrieval-augmented generation for code summarization via hybrid gnn. _arXiv preprint arXiv:2006.05405_.\n' +
      '* Liu et al. (2023) Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. Recall: A benchmark for l1ms robustness against external counterfactual knowledge. _arXiv preprint arXiv:2311.08147_.\n' +
      '* Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_.\n' +
      '* Lu et al. (2023a) Yuxing Lu, Xiaohong Liu, Zongxin Du, Yuanxu Gao, and Guangyu Wang. 2023a. Medkpl: a heterogeneous knowledge enhanced prompt learning framework for transferable diagnosis. _Journal of Biomedical Informatics_, 143:104417.\n' +
      '* Lu et al. (2023b) Yuxing Lu, Xukai Zhao, and Jinzhuo Wang. 2023b. Medical knowledge-enhanced prompt learning for diagnosis classification from clinical text. In _Proceedings of the 5th Clinical Natural Language Processing Workshop_, pages 278-288.\n' +
      '* Luo et al. (2023a) Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, and James Glass. 2023a. Sail: Search-augmented instruction learning. _arXiv preprint arXiv:2305.15225_.\n' +
      '* Luo et al. (2023b) Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023b. Augmented large language models with parametric knowledge guiding. _arXiv preprint arXiv:2305.04757_.\n' +
      '* Lyu et al. (2024) Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, and Enhong Chen. 2024. Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models. _arXiv preprint arXiv:2401.17043_.\n' +
      '* Madaan et al. (2022) Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022. Memory-assisted prompt editing to improve gpt-3 after deployment. _arXiv preprint arXiv:2201.06009_.\n' +
      '* Mallen et al. (2022) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. _arXiv preprint arXiv:2212.10511_.\n' +
      '* Mao et al. (2020) Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020. Generation-augmented retrieval for open-domain question answering. _arXiv preprint arXiv:2009.08553_.\n' +
      '* Mousavi et al. (2024) Seyed M. Mousavi, Simone Alghisi, and Giuseppe Riccardi. 2024. Is your llm outdated? benchmarking l1ms & alignment algorithms for time-sensitive knowledge. _arXiv preprint arXiv:2404.08700_.\n' +
      '* Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback, 2021. _URL https://arxiv. org/abs/2112.09332_.\n' +
      '* Nam et al. (2024) Daye Nam, Andrew Macevan, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an llm to help with code understanding. In _Proceedings of the IEEE/ACM 46th International Conference on Software Engineering_, pages 1-13.\n' +
      '* Nemi et al. (2019)Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021. Large dual encoders are generalizable retrievers. _arXiv preprint arXiv:2112.07899_.\n' +
      '* Parvez et al. (2021) Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval augmented code generation and summarization. _arXiv preprint arXiv:2108.11601_.\n' +
      '* Peng et al. (2023) Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. _arXiv preprint arXiv:2302.12813_.\n' +
      '* Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.\n' +
      '* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9.\n' +
      '* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1-67.\n' +
      '* Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. _Transactions of the Association for Computational Linguistics_, 11:1316-1331.\n' +
      '* Ram et al. (2021) Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. 2021. Learning to retrieve passages without supervision. _arXiv preprint arXiv:2112.07708_.\n' +
      '* Ramachandran et al. (2017) Prajit Ramachandran, Barret Zoph, and Quoc V Le. 2017. Searching for activation functions. _arXiv preprint arXiv:1710.05941_.\n' +
      '* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3.\n' +
      '* Ramos et al. (2003) Juan Ramos et al. 2003. Using tf-idf to determine word relevance in document queries. In _Proceedings of the first instructional conference on machine learning_, volume 242, pages 29-48. Citeseer.\n' +
      '* Robertson et al. (1995) Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at trec-3. _Nist Special Publication Sp_, 109:109.\n' +
      '* Romera-Paredes et al. (2021) Bernardino Romera-Paredes, Mohammadam Barrekata, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. 2024. Mathematical discoveries from program search with large language models. _Nature_, 625(7995):468-475.\n' +
      '* Rony et al. (2022) Md Rashad Al Hasan Rony, Ricardo Usbeck, and Jens Lehmann. 2022. Dialokg: Knowledge-structure aware task-oriented dialogue generation. _arXiv preprint arXiv:2204.09149_.\n' +
      '* Saad-Falcon et al. (2023) Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2023. Ares: An automated evaluation framework for retrieval-augmented generation systems. _arXiv preprint arXiv:2311.09476_.\n' +
      '* Sachan et al. (2021) Devendra Singh Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L Hamilton, and Bryan Catanzaro. 2021. End-to-end training of neural retrievers for open-domain question answering. _arXiv preprint arXiv:2101.00408_.\n' +
      '* Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. _arXiv preprint arXiv:1910.01108_.\n' +
      '* Santhanam et al. (2021) Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021. Colbertv2: Effective and efficient retrieval via lightweight late interaction. _arXiv preprint arXiv:2112.01488_.\n' +
      '* Saxena et al. (2022) Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. 2022. Sequence-to-sequence knowledge graph completion and question answering. _arXiv preprint arXiv:2203.10321_.\n' +
      '* Schick et al. (2024) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Railenau, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. _Advances in Neural Information Processing Systems_, 36.\n' +
      '* Serra et al. (2013) Ivo Serra, Rosario Girardi, and Paulo Novais. 2013. Parnt: a statistic based approach to extract non-taxonomic relationships of ontologies from text. In _2013 10th International Conference on Information Technology: New Generations_, pages 561-566. IEEE.\n' +
      '* Shao et al. (2023) Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. _arXiv preprint arXiv:2305.15294_.\n' +
      '* Shazeer (2020) Noam Shazeer. 2020. Glu variants improve transformer. _arXiv preprint arXiv:2002.05202_.\n' +
      '* Sheynin et al. (2021) Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and YanivTaigman. 2022. Knn-diffusion: Image generation via large-scale retrieval. _arXiv preprint arXiv:2204.02849_.\n' +
      '* Shi et al. (2022) Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun. 2022. Race: Retrieval-augmented commit message generation. _arXiv preprint arXiv:2203.02700_.\n' +
      '* Shi et al. (2023a) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Scharli, and Denny Zhou. 2023a. Large language models can be easily distracted by irrelevant context. In _International Conference on Machine Learning_, pages 31210-31227. PMLR.\n' +
      '* Shi et al. (2023b) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023b. Replug: Retrieval-augmented black-box language models. _arXiv preprint arXiv:2301.12652_.\n' +
      '* Singh et al. (2021) Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. 2021. End-to-end training of multi-document reader and retriever for open-domain question answering. _Advances in Neural Information Processing Systems_, 34:25968-25981.\n' +
      '* Siriwardhana et al. (2023) Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering. _Transactions of the Association for Computational Linguistics_, 11:1-17.\n' +
      '* Srivastava et al. (2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. _The journal of machine learning research_, 15(1):1929-1958.\n' +
      '* Su et al. (2024) Jianlin Su, Murtafha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063.\n' +
      '* Taylor (1953) Wilson L Taylor. 1953. "cloze procedure": A new tool for measuring readability. _Journalism quarterly_, 30(4):415-433.\n' +
      '* Thorne et al. (2018) James Thorne, Andreas Vlachos, Christos Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: a large-scale dataset for fact extraction and verification. _arXiv preprint arXiv:1803.05355_.\n' +
      '* Thulke et al. (2021) David Thulke, Nico Dhaheim, Christian Dugast, and Hermann Ney. 2021. Efficient retrieval augmented generation from unstructured knowledge for task-oriented dialog. _arXiv preprint arXiv:2102.04643_.\n' +
      '* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothele Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. _Advances in neural information processing systems_, 30.\n' +
      '* Vincent et al. (2008) Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In _Proceedings of the 25th international conference on Machine learning_, pages 1096-1103.\n' +
      '* Wang et al. (2023a) Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan Catanzaro. 2023a. Instructretro: Instruction tuning post retrieval-augmented pretraining. _arXiv preprint arXiv:2310.07713_.\n' +
      '* Wang et al. (2023b) Liang Wang, Nan Yang, and Furu Wei. 2023b. Learning to retrieve in-context examples for large language models. _arXiv preprint arXiv:2307.07164_.\n' +
      '* Wang et al. (2023c) Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023c. Learning to filter context for retrieval-augmented generation. _arXiv preprint arXiv:2311.08377_.\n' +
      '* Workshop et al. (2022) BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_.\n' +
      '* Xiong et al. (2024) Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking retrieval-augmented generation for medicine. _arXiv preprint arXiv:2402.13178_.\n' +
      '* Xu et al. (2023) Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Improving retrieval-augmented lms with compression and selective augmentation. _arXiv preprint arXiv:2310.04408_.\n' +
      '* Yan et al. (2024) Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. _arXiv preprint arXiv:2401.15884_.\n' +
      '* Yang et al. (2023) Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. 2023. Prca: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter. _arXiv preprint arXiv:2310.18347_.\n' +
      '* Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. _arXiv preprint arXiv:1809.09600_.\n' +
      '* Zhang et al. (2020)* Yao and Guan (2018) Lirong Yao and Yazhuo Guan. 2018. An improved lstm structure for natural language processing. In _2018 IEEE international conference of safety produce information (IICSPI)_, pages 565-569. IEEE.\n' +
      '* Yasunaga et al. (2022) Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2022. Retrieval-augmented multimodal language modeling. _arXiv preprint arXiv:2211.12561_.\n' +
      '* Ye et al. (2024) Haoran Ye, Jiarui Wang, Zhiguang Cao, and Guojie Song. 2024. Reevo: Large language models as hyper-heuristics with reflective evolution. _arXiv preprint arXiv:2402.01145_.\n' +
      '* Yin et al. (2019) Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. 2019. Memory augmented deep recurrent neural network for video question answering. _IEEE transactions on neural networks and learning systems_, 31(9):3159-3167.\n' +
      '* Yin et al. (2017) Wenpeng Yin, Katharina Kann, Mo Yu, and Hinrich Schutze. 2017. Comparative study of cnn and rnn for natural language processing. _arXiv preprint arXiv:1702.01923_.\n' +
      '* Yogatama et al. (2021) Dani Yogatama, Cyprien de Masson d\'Autume, and Lingpeng Kong. 2021. Adaptive semiparametric language models. _Transactions of the Association for Computational Linguistics_, 9:362-373.\n' +
      '* Yoran et al. (2023) Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making retrieval-augmented language models robust to irrelevant context. _arXiv preprint arXiv:2310.01558_.\n' +
      '* Yu and Yang (2023) Donghan Yu and Yiming Yang. 2023. Retrieval-enhanced generative model for large-scale knowledge graph completion. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 2334-2338.\n' +
      '* Yu et al. (2023) Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. 2023. Improving language models via plug-and-play retrieval feedback. _arXiv preprint arXiv:2305.14002_.\n' +
      '* Yuan et al. (2024) Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D Plumbley, and Wenwu Wang. 2024. Retrieval-augmented text-to-audio generation. In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 581-585. IEEE.\n' +
      '* Yukeskgonul et al. (2023) Mert Yukeskgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, and Besmira Nushi. 2023. Attention satisfies: A constraint-satisfaction lens on factual errors of language models. _arXiv preprint arXiv:2309.15098_.\n' +
      '* Zan et al. (2022) Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2022. When language model meets private library. _arXiv preprint arXiv:2210.17236_.\n' +
      '* Zhang and Sennrich (2019) Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. _Advances in Neural Information Processing Systems_, 32.\n' +
      '* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_.\n' +
      '* Zhao et al. (2024a) Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024a. Retrieval-augmented generation for ai-generated content: A survey. _arXiv preprint arXiv:2402.19473_.\n' +
      '* Zhao et al. (2023) Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al. 2023. Retrieving multimodal information for augmented generation: A survey. _arXiv preprint arXiv:2303.10868_.\n' +
      '* Zhao et al. (2024b) Weichen Zhao, Yuxing Lu, Ge Jiao, and Yuan Yang. 2024b. Concentrated reasoning and unified reconstruction for multi-modal media manipulation. In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 8190-8194. IEEE.\n' +
      '* Zhao et al. (2024c) Weichen Zhao, Yuxing Lu, Ge Jiao, and Yuan Yang. 2024c. Dual-color granularity alignment for text-based person search. In _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 8075-8079. IEEE.\n' +
      '* Zhong et al. (2022) Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. _arXiv preprint arXiv:2205.12674_.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>